{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract OAI metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_oad_metadata(full_path):\n",
    "    namespaces = {\n",
    "        'oai': 'http://www.openarchives.org/OAI/2.0/',\n",
    "        'arxiv': 'http://arxiv.org/OAI/arXiv/'\n",
    "    }\n",
    "                \n",
    "    oai_metadata = []\n",
    "    # read xml file by line\n",
    "    with open(full_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            xml_info = ET.fromstring(line)\n",
    "        \n",
    "            # target on record element\n",
    "            if xml_info.tag == '{http://www.openarchives.org/OAI/2.0/}record':\n",
    "                # get header info\n",
    "                header = xml_info.find('oai:header', namespaces)\n",
    "                identifier = header.find('oai:identifier', namespaces).text\n",
    "                datestamp = header.find('oai:datestamp', namespaces).text\n",
    "                setSpec = header.find('oai:setSpec', namespaces).text\n",
    "\n",
    "                # get metadata\n",
    "                metadata = xml_info.find('oai:metadata', namespaces)\n",
    "                arxiv = metadata.find('arxiv:arXiv', namespaces)\n",
    "                \n",
    "                # get arXiv info\n",
    "                arxiv_id = arxiv.find('arxiv:id', namespaces).text\n",
    "                created = arxiv.find('arxiv:created', namespaces).text\n",
    "                updated = arxiv.find('arxiv:updated', namespaces).text if arxiv.find('arxiv:updated', namespaces) is not None else None\n",
    "                \n",
    "                # get authors info\n",
    "                authors = []\n",
    "                for author in arxiv.findall('arxiv:authors/arxiv:author', namespaces):\n",
    "                    keyname = author.find('arxiv:keyname', namespaces).text\n",
    "                    forenames = author.find('arxiv:forenames', namespaces).text if author.find('arxiv:forenames', namespaces) is not None else ''\n",
    "                    suffix = author.find('arxiv:suffix', namespaces)\n",
    "                    suffix_text = suffix.text if suffix is not None else ''\n",
    "                    authors.append(f\"{forenames} {keyname} {suffix_text}\".strip())\n",
    "                \n",
    "                # get title, abstract, etc\n",
    "                title = arxiv.find('arxiv:title', namespaces).text\n",
    "                categories = arxiv.find('arxiv:categories', namespaces).text.split(' ')\n",
    "                comments = arxiv.find('arxiv:comments', namespaces).text if arxiv.find('arxiv:comments', namespaces) is not None else None\n",
    "                journal_ref = arxiv.find('arxiv:journal-ref', namespaces)\n",
    "                journal_ref_text = journal_ref.text if journal_ref is not None else None\n",
    "                doi = arxiv.find('arxiv:doi', namespaces)\n",
    "                doi_text = doi.text if doi is not None else None\n",
    "                license = arxiv.find('arxiv:license', namespaces).text\n",
    "                abstract = arxiv.find('arxiv:abstract', namespaces).text\n",
    "\n",
    "                # construct dict\n",
    "                record_data = {\n",
    "                    \"identifier\": identifier,\n",
    "                    \"datestamp\": datestamp,\n",
    "                    \"setSpec\": setSpec,\n",
    "                    \"arxiv_id\": arxiv_id,\n",
    "                    \"created\": created,\n",
    "                    \"updated\": updated,\n",
    "                    \"authors\": authors,\n",
    "                    \"title\": title,\n",
    "                    \"categories\": categories,\n",
    "                    \"comments\": comments,\n",
    "                    \"journal_ref\": journal_ref_text,\n",
    "                    \"doi\": doi_text,\n",
    "                    \"license\": license,\n",
    "                    \"abstract\": abstract\n",
    "                }\n",
    "                oai_metadata.append(record_data)\n",
    "    return oai_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DB_PATH ='/home/jiezi/Code/Github/TrendingPapers/data/'\n",
    "DB_NAME = 'trending_papers.db'\n",
    "OAI_PAPER_TBL_NM = \"oai_paper_pool\"  # table for preprint paper metadata (batch trhough OAI)\n",
    "OAI_PAPER_TBL_KEY = 'identifier'   # PK column for OAI_PAPER_TBL_NM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def sqlite_connect(db_name):\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        return conn\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error connecting to database: {e}\")\n",
    "        return None\n",
    "\n",
    "def df_to_sqlite(\n",
    "        df, \n",
    "        table_name, \n",
    "        db_name, \n",
    "        id_key=None,\n",
    "        if_exists='append' \n",
    "        ):   \n",
    "    \"\"\"import pandas DataFrame to SQLite database\n",
    "    Args:\n",
    "        :param pd.DataFrame df: DataFrame to import\n",
    "        :param str table_name: table name to import to\n",
    "        :param str db_name: database name\n",
    "        :param str id_key: primary key for the table\n",
    "        :param str if_exists: 'append' or 'replace'\n",
    "    Returns:\n",
    "        :returns: None\n",
    "    Note:\n",
    "        - If 'id_key' is provided, the function will check for existing records in the database and only insert new records.\n",
    "        - If 'if_exists' is set to 'replace', the function will replace the existing table with the new data.\n",
    "        - If 'if_exists' is set to 'append', the function will append the new data to the existing table.\n",
    "        - The code would automatically neglect columns that are not in the table.\n",
    "        - The code would set the value of missing columns to None.\n",
    "        - Automatically create table if not exist.\n",
    "    \"\"\"\n",
    "    conn = sqlite_connect(db_name)\n",
    "    if conn:\n",
    "        df_converted = df.copy()\n",
    "\n",
    "        try:\n",
    "            # Check if the table exists\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(f\"SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}'\")\n",
    "            table_exists = cursor.fetchone() is not None\n",
    "\n",
    "            # 1. Identify and Convert Dict/List-of-Dict Columns to JSON\n",
    "            # This block of code must be placed before creating the table\n",
    "            for col in df_converted.columns:\n",
    "                if df_converted[col].dtype == 'object':\n",
    "                    df_converted[col] = df_converted[col].apply(lambda x: json.dumps(x, ensure_ascii=False) if any(isinstance(x, (dict, list)) for x in df_converted[col].dropna()) else str(x))\n",
    "\n",
    "            # Create table if it doesn't exist\n",
    "            if not table_exists:\n",
    "                create_table_from_df(conn, df_converted, table_name, id_key)\n",
    "\n",
    "            # Get the list of columns in the existing table\n",
    "            cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "            table_columns = {row[1] for row in cursor.fetchall()} # Using a set for faster lookup\n",
    "\n",
    "            if id_key and table_exists:\n",
    "                # Fetch existing IDs from the database\n",
    "                cursor.execute(f\"SELECT DISTINCT {id_key} FROM {table_name}\")\n",
    "                existing_ids = {row[0] for row in cursor.fetchall()}\n",
    "\n",
    "                # Filter out rows with IDs that already exist\n",
    "                df_converted = df_converted[~df_converted[id_key].isin(existing_ids)]\n",
    "\n",
    "            if df_converted.empty and table_exists:\n",
    "                print(f\"No new records to insert into '{table_name}' (based on '{id_key}').\")\n",
    "                return\n",
    "            \n",
    "            # --- Modification: Keep only relevant columns ---\n",
    "            if table_exists:\n",
    "                df_converted = df_converted.loc[:, df_converted.columns.isin(table_columns)]\n",
    "\n",
    "            # Add missing columns to the DataFrame and set values to None\n",
    "            if table_exists:\n",
    "                for col in table_columns:\n",
    "                    if col not in df_converted.columns:\n",
    "                        df_converted[col] = None\n",
    "\n",
    "            # Reorder DataFrame columns to match the table's column order\n",
    "            # Convert table_columns set back to a list for ordering\n",
    "            if table_exists:\n",
    "                df_converted = df_converted[list(table_columns)]\n",
    "            \n",
    "            # 2. Explicitly define SQLite types if needed\n",
    "            dtype_mapping = {}\n",
    "            for col_name, col_type in df_converted.dtypes.items():\n",
    "                if col_name == id_key:\n",
    "                    dtype_mapping[col_name] = \"TEXT PRIMARY KEY\"  # Assuming ID key is text\n",
    "                elif 'int' in str(col_type):\n",
    "                    dtype_mapping[col_name]  = \"INTEGER\"\n",
    "                elif 'float' in str(col_type):\n",
    "                    dtype_mapping[col_name]  = \"REAL\"\n",
    "                else:\n",
    "                    dtype_mapping[col_name]  = \"TEXT\"\n",
    "\n",
    "            df_converted.to_sql(\n",
    "                table_name, conn, if_exists=if_exists, index=False,\n",
    "                dtype=dtype_mapping\n",
    "                )\n",
    "            # df_converted.to_sql(table_name, conn, if_exists=if_exists, index=False)\n",
    "            print(f\"Data successfully written to table '{table_name}' in '{db_name}'\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error writing to database: {e}\")\n",
    "            print(f\"Error writing to database: {e}\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "def create_table_from_df(conn, df, table_name, id_key):\n",
    "    \"\"\"Creates a table in the SQLite database based on the DataFrame structure.\"\"\"\n",
    "    columns_sql = []\n",
    "    for col_name, col_type in df.dtypes.items():\n",
    "        if col_name == id_key:\n",
    "            sql_type = \"TEXT PRIMARY KEY\"  # Assuming ID key is text\n",
    "        elif 'int' in str(col_type):\n",
    "            sql_type = \"INTEGER\"\n",
    "        elif 'float' in str(col_type):\n",
    "            sql_type = \"REAL\"\n",
    "        else:\n",
    "            sql_type = \"TEXT\"\n",
    "        columns_sql.append(f'\"{col_name}\" {sql_type}') # Wrapping column names in double quotes\n",
    "\n",
    "    create_table_sql = f\"CREATE TABLE {table_name} ({', '.join(columns_sql)})\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(create_table_sql)\n",
    "    conn.commit()\n",
    "    print(f\"Table '{table_name}' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_list_of_dicts(data, key):\n",
    "    \"\"\"dedup list of dicts based on given key. Keep only the first item.\n",
    "    \"\"\"\n",
    "    seen = set()  # 使用集合来高效地检查是否已经遇到过某个值\n",
    "    deduplicated_data = []\n",
    "    for item in data:\n",
    "        value = item[key]\n",
    "        if value not in seen:\n",
    "            seen.add(value)\n",
    "            deduplicated_data.append(item)\n",
    "    return deduplicated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to table 'oai_paper_pool' in '/home/jiezi/Code/Github/TrendingPapers/data/trending_papers.db'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "CURRENT_DT = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "full_path = \"../data/stat_2025-02-05_2025-02-06.xml\"\n",
    "oai_metadata = extract_oad_metadata(full_path)\n",
    "cleaned_oai_metadata = deduplicate_list_of_dicts(oai_metadata, OAI_PAPER_TBL_KEY)\n",
    "df = pd.DataFrame(cleaned_oai_metadata)\n",
    "df['insert_dt'] = 'new'\n",
    "df_to_sqlite(\n",
    "    df, \n",
    "    table_name = OAI_PAPER_TBL_NM, \n",
    "    db_name = os.path.join(DB_PATH, DB_NAME),\n",
    "    if_exists = 'append', \n",
    "    id_key = OAI_PAPER_TBL_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface API Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import requests\n",
    "from requests.adapters import Retry, HTTPAdapter\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "from json_repair import repair_json  # https://github.com/mangiucugna/json_repair/\n",
    "from firecrawl import FirecrawlApp  # pip install firecrawl-py https://github.com/mendableai/firecrawl\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "_useragent_list = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36 Edg/111.0.1661.62',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/111.0'\n",
    "]\n",
    "\n",
    "class HuggingFaceKit:\n",
    "    def __init__(self, max_retries_cnt=3, firecrawl_api_key=None):\n",
    "        self.base_url = \"https://huggingface.co/api/daily_papers\"\n",
    "        self.headers = {\n",
    "            \"User-Agent\": random.choice(_useragent_list)\n",
    "        }\n",
    "        self.session = requests.Session()\n",
    "        retry_strategy = Retry(\n",
    "            total=max_retries_cnt,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            backoff_factor=1,\n",
    "            allowed_methods=[\"GET\"]\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        self.session.mount(\"https://\", adapter)\n",
    "        self.session.mount(\"http://\", adapter)\n",
    "\n",
    "        if firecrawl_api_key is not None:\n",
    "            self.firecrawl = FirecrawlApp(api_key=firecrawl_api_key)\n",
    "\n",
    "\n",
    "    def fetch_daily_papers(self, date_str=None, max_cnt=None):\n",
    "        logger.info(f\"正在获取 {date_str} 的论文数据\")\n",
    "        \n",
    "        # 构建API URL\n",
    "        if date_str is not None and max_cnt is not None:\n",
    "            url = f\"{self.base_url}?date={date_str}&limit={max_cnt}\"\n",
    "        elif date_str is not None:\n",
    "            url = f\"{self.base_url}?date={date_str}\"\n",
    "        elif max_cnt is not None:\n",
    "            url = f\"{self.base_url}?limit={max_cnt}\"\n",
    "        else:\n",
    "            url = self.base_url \n",
    "\n",
    "        try:\n",
    "            response = self.session.get(url, headers=self.headers, timeout=10) # 添加 timeout\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            if not data:\n",
    "                print(\"No data received from API.\")\n",
    "\n",
    "            # Debug: Print keys of the first paper\n",
    "            if data: \n",
    "                hf_paper_dicts = [item.get('paper') for item in data]\n",
    "                return hf_paper_dicts\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        except requests.exceptions.RequestException as e: # 捕获更具体的 requests 异常\n",
    "            print(f\"Error fetching papers through API: {e}\\nSwitch to FireCrawl:\\n\")\n",
    "            try:\n",
    "                response = self.firecrawl.scrape_url(url=url, params={\n",
    "                    'formats': [ 'markdown', 'links' ],\n",
    "                    'excludeTags': [ '.ad', 'script', '#footer' ]\n",
    "                })\n",
    "                md = response.get('markdown').replace(\"\\\\n\", \" \").replace(\"\\\\\", \"\")\n",
    "                data = json.loads(repair_json(md))\n",
    "                if data:\n",
    "                    hf_paper_dicts = []\n",
    "                    for item in data:\n",
    "                        paper_metadata = item.get('paper')\n",
    "                        rvsd_paper_metadata = {{'\\\\_id':'_id'}.get(key, key): \n",
    "                                               value for key, value in paper_metadata.items()}\n",
    "                        hf_paper_dicts.append(rvsd_paper_metadata)\n",
    "                    return hf_paper_dicts\n",
    "                else:\n",
    "                    return []\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error: {e}\")\n",
    "                return []\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 16:51:31,743 - INFO - 正在获取 None 的论文数据\n",
      "2025-02-10 16:51:32,377 - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectionResetError(104, 'Connection reset by peer')': /api/daily_papers\n",
      "2025-02-10 16:51:34,653 - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectionResetError(104, 'Connection reset by peer')': /api/daily_papers\n",
      "2025-02-10 16:51:38,966 - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectionResetError(104, 'Connection reset by peer')': /api/daily_papers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching papers through API: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/daily_papers (Caused by ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')))\n",
      "Switch to FireCrawl:\n",
      "\n",
      "Successfully fetched 50 papers.\n"
     ]
    }
   ],
   "source": [
    "hf_kit = HuggingFaceKit(max_retries_cnt=3, firecrawl_api_key='fc-2c61a9560a0c409da739a7fd128d0ce1')\n",
    "papers = hf_kit.fetch_daily_papers()\n",
    "if papers:\n",
    "    print(f\"Successfully fetched {len(papers)} papers.\")\n",
    "else:\n",
    "    print(\"Failed to fetch papers after multiple retries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '2502.04689',\n",
       "  'authors': [{'_id': '67a9b911b1f5eece682d7961',\n",
       "    'name': 'Yuwei Yin',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a9b911b1f5eece682d7962',\n",
       "    'name': 'Giuseppe Carenini',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-07T06:30:33.000Z',\n",
       "  'title': 'ARR: Question Answering with Large Language Models via Analyzing,  Retrieving, and Reasoning',\n",
       "  'summary': 'Large language models (LLMs) achieve remarkable performance on challenging benchmarks that are often structured as multiple-choice question-answering (QA) tasks. Zero-shot Chain-of-Thought (CoT) prompting enhances reasoning in LLMs but provides only vague and generic guidance (\"think step by step\"). This paper introduces ARR, an intuitive and effective zero-shot prompting method that explicitly incorporates three key steps in QA solving: analyzing the intent of the question, retrieving relevant information, and reasoning step by step. Comprehensive experiments across diverse and challenging QA tasks demonstrate that ARR consistently improves the Baseline (without ARR prompting) and outperforms CoT. Ablation and case studies further validate the positive contributions of each component: analyzing, retrieving, and reasoning. Notably, intent analysis plays a vital role in ARR. Additionally, extensive evaluations across various model sizes, LLM series, and generation settings solidify the effectiveness, robustness, and generalizability of ARR.',\n",
       "  'upvotes': 1,\n",
       "  'discussionId': '67a9b911b1f5eece682d798c'},\n",
       " {'id': '2502.05003',\n",
       "  'authors': [{'_id': '67a9b1a69a99341e859c488d',\n",
       "    'user': {'_id': '623753b5eddd7763adc9346a',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/623753b5eddd7763adc9346a/rcpQAKZNrkn1-tMtraQBX.jpeg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Andrei Panferov',\n",
       "     'user': 'BlackSamorez',\n",
       "     'type': 'user'},\n",
       "    'name': 'Andrei Panferov',\n",
       "    'status': 'extracted_confirmed',\n",
       "    'statusLastChangedAt': '2025-02-10T08:09:18.686Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a9b1a69a99341e859c488e', 'name': 'Jiale Chen', 'hidden': False},\n",
       "   {'_id': '67a9b1a69a99341e859c488f',\n",
       "    'name': 'Soroush Tabesh',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a9b1a69a99341e859c4890',\n",
       "    'name': 'Roberto L. Castro',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a9b1a69a99341e859c4891',\n",
       "    'name': 'Mahdi Nikdan',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a9b1a69a99341e859c4892',\n",
       "    'name': 'Dan Alistarh',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-07T15:23:34.000Z',\n",
       "  'title': 'QuEST: Stable Training of LLMs with 1-Bit Weights and Activations',\n",
       "  'summary': 'One approach to reducing the massive costs of large language models (LLMs) is the use of quantized or sparse representations for training or deployment. While post-training compression methods are very popular, the question of obtaining even more accurate compressed models by directly training over such representations, i.e., Quantization-Aware Training (QAT), is still open: for example, a recent study (arXiv:2411.04330v2) put the \"optimal\" bit-width at which models can be trained using QAT, while staying accuracy-competitive with standard FP16/BF16 precision, at 8-bits weights and activations.  We advance this state-of-the-art via a new method called QuEST, which is Pareto-competitive with FP16, i.e., it provides better accuracy at lower model size, while training models with weights and activations in 4-bits or less. Moreover, QuEST allows stable training with 1-bit weights and activations. QuEST achieves this by improving two key aspects of QAT methods: (1) accurate and fast quantization of the (continuous) distributions of weights and activations via Hadamard normalization and MSE-optimal fitting; (2) a new trust gradient estimator based on the idea of explicitly minimizing the error between the noisy gradient computed over quantized states and the \"true\" (but unknown) full-precision gradient. Experiments on Llama-type architectures show that QuEST induces stable scaling laws across the entire range of hardware-supported precisions, and can be extended to sparse representations. We provide GPU kernel support showing that models produced by QuEST can be executed efficiently. Our code is available at https://github.com/IST-DASLab/QuEST.',\n",
       "  'upvotes': 6,\n",
       "  'discussionId': '67a9b1a79a99341e859c48c7'},\n",
       " {'id': '2502.03738',\n",
       "  'authors': [{'_id': '67a8d049406cb5a65f847eb1',\n",
       "    'name': 'Feng Wang',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a8d049406cb5a65f847eb2', 'name': 'Yaodong Yu', 'hidden': False},\n",
       "   {'_id': '67a8d049406cb5a65f847eb3',\n",
       "    'name': 'Guoyizhe Wei',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a8d049406cb5a65f847eb4', 'name': 'Wei Shao', 'hidden': False},\n",
       "   {'_id': '67a8d049406cb5a65f847eb5', 'name': 'Yuyin Zhou', 'hidden': False},\n",
       "   {'_id': '67a8d049406cb5a65f847eb6', 'name': 'Alan Yuille', 'hidden': False},\n",
       "   {'_id': '67a8d049406cb5a65f847eb7', 'name': 'Cihang Xie', 'hidden': False}],\n",
       "  'publishedAt': '2025-02-06T03:01:38.000Z',\n",
       "  'title': 'Scaling Laws in Patchification: An Image Is Worth 50,176 Tokens And More',\n",
       "  'summary': 'Since the introduction of Vision Transformer (ViT), patchification has long been regarded as a de facto image tokenization approach for plain visual architectures. By compressing the spatial size of images, this approach can effectively shorten the token sequence and reduce the computational cost of ViT-like plain architectures. In this work, we aim to thoroughly examine the information loss caused by this patchification-based compressive encoding paradigm and how it affects visual understanding. We conduct extensive patch size scaling experiments and excitedly observe an intriguing scaling law in patchification: the models can consistently benefit from decreased patch sizes and attain improved predictive performance, until it reaches the minimum patch size of 1x1, i.e., pixel tokenization. This conclusion is broadly applicable across different vision tasks, various input scales, and diverse architectures such as ViT and the recent Mamba models. Moreover, as a by-product, we discover that with smaller patches, task-specific decoder heads become less critical for dense prediction. In the experiments, we successfully scale up the visual sequence to an exceptional length of 50,176 tokens, achieving a competitive test accuracy of 84.6% with a base-sized model on the ImageNet-1k benchmark. We hope this study can provide insights and theoretical foundations for future works of building non-compressive vision models. Code is available at https://github.com/wangf3014/Patch_Scaling.',\n",
       "  'upvotes': 0,\n",
       "  'discussionId': '67a8d04a406cb5a65f847ed3'},\n",
       " {'id': '2502.03512',\n",
       "  'authors': [{'_id': '67a9a7cb6be3ca4a7ede471e',\n",
       "    'name': 'Amitava Das',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a9a7cb6be3ca4a7ede471f',\n",
       "    'name': 'Yaswanth Narsupalli',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a9a7cb6be3ca4a7ede4720',\n",
       "    'name': 'Gurpreet Singh',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a9a7cb6be3ca4a7ede4721', 'name': 'Vinija Jain', 'hidden': False},\n",
       "   {'_id': '67a9a7cb6be3ca4a7ede4722', 'name': 'Vasu Sharma', 'hidden': False},\n",
       "   {'_id': '67a9a7cb6be3ca4a7ede4723',\n",
       "    'name': 'Suranjana Trivedy',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a9a7cb6be3ca4a7ede4724', 'name': 'Aman Chadha', 'hidden': False},\n",
       "   {'_id': '67a9a7cb6be3ca4a7ede4725', 'name': 'Amit Sheth', 'hidden': False}],\n",
       "  'publishedAt': '2025-02-05T18:46:20.000Z',\n",
       "  'title': 'YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing  Multi-Objective Optimization based DPO for Text-to-Image Alignment',\n",
       "  'summary': 'Precise alignment in Text-to-Image (T2I) systems is crucial to ensure that generated visuals not only accurately encapsulate user intents but also conform to stringent ethical and aesthetic benchmarks. Incidents like the Google Gemini fiasco, where misaligned outputs triggered significant public backlash, underscore the critical need for robust alignment mechanisms. In contrast, Large Language Models (LLMs) have achieved notable success in alignment. Building on these advancements, researchers are eager to apply similar alignment techniques, such as Direct Preference Optimization (DPO), to T2I systems to enhance image generation fidelity and reliability.  We present YinYangAlign, an advanced benchmarking framework that systematically quantifies the alignment fidelity of T2I systems, addressing six fundamental and inherently contradictory design objectives. Each pair represents fundamental tensions in image generation, such as balancing adherence to user prompts with creative modifications or maintaining diversity alongside visual coherence. YinYangAlign includes detailed axiom datasets featuring human prompts, aligned (chosen) responses, misaligned (rejected) AI-generated outputs, and explanations of the underlying contradictions.',\n",
       "  'upvotes': 1,\n",
       "  'discussionId': '67a9a7cf6be3ca4a7ede47d5'},\n",
       " {'id': '2502.05178',\n",
       "  'authors': [{'_id': '67a99dfe98423dca45d8f659',\n",
       "    'name': 'Yue Zhao',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a99dfe98423dca45d8f65a', 'name': 'Fuzhao Xue', 'hidden': False},\n",
       "   {'_id': '67a99dfe98423dca45d8f65b', 'name': 'Scott Reed', 'hidden': False},\n",
       "   {'_id': '67a99dfe98423dca45d8f65c', 'name': 'Linxi Fan', 'hidden': False},\n",
       "   {'_id': '67a99dfe98423dca45d8f65d', 'name': 'Yuke Zhu', 'hidden': False},\n",
       "   {'_id': '67a99dfe98423dca45d8f65e', 'name': 'Jan Kautz', 'hidden': False},\n",
       "   {'_id': '67a99dfe98423dca45d8f65f', 'name': 'Zhiding Yu', 'hidden': False},\n",
       "   {'_id': '67a99dfe98423dca45d8f660',\n",
       "    'name': 'Philipp Krähenbühl',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a99dfe98423dca45d8f661',\n",
       "    'name': 'De-An Huang',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-07T18:59:57.000Z',\n",
       "  'title': 'QLIP: Text-Aligned Visual Tokenization Unifies Auto-Regressive  Multimodal Understanding and Generation',\n",
       "  'summary': 'We introduce Quantized Language-Image Pretraining (QLIP), a visual tokenization method that combines state-of-the-art reconstruction quality with state-of-the-art zero-shot image understanding. QLIP trains a binary-spherical-quantization-based autoencoder with reconstruction and language-image alignment objectives. We are the first to show that the two objectives do not need to be at odds. We balance the two loss terms dynamically during training and show that a two-stage training pipeline effectively mixes the large-batch requirements of image-language pre-training with the memory bottleneck imposed by the reconstruction objective. We validate the effectiveness of QLIP for multimodal understanding and text-conditioned image generation with a single model. Specifically, QLIP serves as a drop-in replacement for the visual encoder for LLaVA and the image tokenizer for LlamaGen with comparable or even better performance. Finally, we demonstrate that QLIP enables a unified mixed-modality auto-regressive model for understanding and generation.',\n",
       "  'upvotes': 1,\n",
       "  'discussionId': '67a99dfe98423dca45d8f691'},\n",
       " {'id': '2502.04376',\n",
       "  'authors': [{'_id': '67a998fe495b23306cdbf51d',\n",
       "    'name': 'Lingxiang Hu',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a998fe495b23306cdbf51e', 'name': 'Shurun Yuan', 'hidden': False},\n",
       "   {'_id': '67a998fe495b23306cdbf51f',\n",
       "    'name': 'Xiaoting Qin',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a998fe495b23306cdbf520', 'name': 'Jue Zhang', 'hidden': False},\n",
       "   {'_id': '67a998fe495b23306cdbf521', 'name': 'Qingwei Lin', 'hidden': False},\n",
       "   {'_id': '67a998fe495b23306cdbf522',\n",
       "    'name': 'Dongmei Zhang',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a998fe495b23306cdbf523',\n",
       "    'name': 'Saravan Rajmohan',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a998fe495b23306cdbf524', 'name': 'Qi Zhang', 'hidden': False}],\n",
       "  'publishedAt': '2025-02-05T16:25:43.000Z',\n",
       "  'title': 'MEETING DELEGATE: Benchmarking LLMs on Attending Meetings on Our Behalf',\n",
       "  'summary': 'In contemporary workplaces, meetings are essential for exchanging ideas and ensuring team alignment but often face challenges such as time consumption, scheduling conflicts, and inefficient participation. Recent advancements in Large Language Models (LLMs) have demonstrated their strong capabilities in natural language generation and reasoning, prompting the question: can LLMs effectively delegate participants in meetings? To explore this, we develop a prototype LLM-powered meeting delegate system and create a comprehensive benchmark using real meeting transcripts. Our evaluation reveals that GPT-4/4o maintain balanced performance between active and cautious engagement strategies. In contrast, Gemini 1.5 Pro tends to be more cautious, while Gemini 1.5 Flash and Llama3-8B/70B display more active tendencies. Overall, about 60% of responses address at least one key point from the ground-truth. However, improvements are needed to reduce irrelevant or repetitive content and enhance tolerance for transcription errors commonly found in real-world settings. Additionally, we implement the system in practical settings and collect real-world feedback from demos. Our findings underscore the potential and challenges of utilizing LLMs as meeting delegates, offering valuable insights into their practical application for alleviating the burden of meetings.',\n",
       "  'upvotes': 1,\n",
       "  'discussionId': '67a99900495b23306cdbf57e'},\n",
       " {'id': '2502.05163',\n",
       "  'authors': [{'_id': '67a9604851169a582d14c113',\n",
       "    'name': 'Yihe Deng',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a9604851169a582d14c114', 'name': 'Yu Yang', 'hidden': False},\n",
       "   {'_id': '67a9604851169a582d14c115',\n",
       "    'name': 'Junkai Zhang',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a9604851169a582d14c116', 'name': 'Wei Wang', 'hidden': False},\n",
       "   {'_id': '67a9604851169a582d14c117', 'name': 'Bo Li', 'hidden': False}],\n",
       "  'publishedAt': '2025-02-07T18:45:03.000Z',\n",
       "  'title': 'DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM  Guardrails',\n",
       "  'summary': 'The rapid advancement of large language models (LLMs) has increased the need for guardrail models to ensure responsible use, particularly in detecting unsafe and illegal content. While substantial safety data exist in English, multilingual guardrail modeling remains underexplored due to the scarcity of open-source safety data in other languages. To address this gap, we propose a novel two-player Reinforcement Learning (RL) framework, where a generator and a guardrail model co-evolve adversarially to produce high-quality synthetic data for multilingual guardrail training. We theoretically formalize this interaction as a two-player game, proving convergence to a Nash equilibrium. Empirical evaluations show that our model ours outperforms state-of-the-art models, achieving nearly 10% improvement over LlamaGuard3 (8B) on English benchmarks while being 4.5x faster at inference with a significantly smaller model (0.5B). We achieve substantial advancements in multilingual safety tasks, particularly in addressing the imbalance for lower-resource languages in a collected real dataset. Ablation studies emphasize the critical role of synthetic data generation in bridging the imbalance in open-source data between English and other languages. These findings establish a scalable and efficient approach to synthetic data generation, paving the way for improved multilingual guardrail models to enhance LLM safety. Code, model, and data will be open-sourced at https://github.com/yihedeng9/DuoGuard.',\n",
       "  'upvotes': 11,\n",
       "  'discussionId': '67a9604951169a582d14c14d'},\n",
       " {'id': '2502.05179',\n",
       "  'authors': [{'_id': '67a9901cc0310368e2488929',\n",
       "    'name': 'Shilong Zhang',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a9901cc0310368e248892a', 'name': 'Wenbo Li', 'hidden': False},\n",
       "   {'_id': '67a9901cc0310368e248892b', 'name': 'Shoufa Chen', 'hidden': False},\n",
       "   {'_id': '67a9901cc0310368e248892c',\n",
       "    'name': 'Chongjian Ge',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a9901cc0310368e248892d', 'name': 'Peize Sun', 'hidden': False},\n",
       "   {'_id': '67a9901cc0310368e248892e', 'name': 'Yida Zhang', 'hidden': False},\n",
       "   {'_id': '67a9901cc0310368e248892f', 'name': 'Yi Jiang', 'hidden': False},\n",
       "   {'_id': '67a9901cc0310368e2488930', 'name': 'Zehuan Yuan', 'hidden': False},\n",
       "   {'_id': '67a9901cc0310368e2488931', 'name': 'Binyue Peng', 'hidden': False},\n",
       "   {'_id': '67a9901cc0310368e2488932', 'name': 'Ping Luo', 'hidden': False}],\n",
       "  'publishedAt': '2025-02-07T18:59:59.000Z',\n",
       "  'title': 'FlashVideo:Flowing Fidelity to Detail for Efficient High-Resolution  Video Generation',\n",
       "  'summary': 'DiT diffusion models have achieved great success in text-to-video generation, leveraging their scalability in model capacity and data scale. High content and motion fidelity aligned with text prompts, however, often require large model parameters and a substantial number of function evaluations (NFEs). Realistic and visually appealing details are typically reflected in high resolution outputs, further amplifying computational demands especially for single stage DiT models. To address these challenges, we propose a novel two stage framework, FlashVideo, which strategically allocates model capacity and NFEs across stages to balance generation fidelity and quality. In the first stage, prompt fidelity is prioritized through a low resolution generation process utilizing large parameters and sufficient NFEs to enhance computational efficiency. The second stage establishes flow matching between low and high resolutions, effectively generating fine details with minimal NFEs. Quantitative and visual results demonstrate that FlashVideo achieves state-of-the-art high resolution video generation with superior computational efficiency. Additionally, the two-stage design enables users to preview the initial output before committing to full resolution generation, thereby significantly reducing computational costs and wait times as well as enhancing commercial viability .',\n",
       "  'upvotes': 5,\n",
       "  'discussionId': '67a9901ec0310368e24889c2'},\n",
       " {'id': '2502.04507',\n",
       "  'authors': [{'_id': '67a98cd1b8b21202c9004628',\n",
       "    'name': 'Peiyuan Zhang',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a98cd1b8b21202c9004629', 'name': 'Yongqi Chen', 'hidden': False},\n",
       "   {'_id': '67a98cd1b8b21202c900462a', 'name': 'Runlong Su', 'hidden': False},\n",
       "   {'_id': '67a98cd1b8b21202c900462b',\n",
       "    'name': 'Hangliang Ding',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a98cd1b8b21202c900462c', 'name': 'Ion Stoica', 'hidden': False},\n",
       "   {'_id': '67a98cd1b8b21202c900462d',\n",
       "    'name': 'Zhenghong Liu',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a98cd1b8b21202c900462e', 'name': 'Hao Zhang', 'hidden': False}],\n",
       "  'publishedAt': '2025-02-06T21:17:09.000Z',\n",
       "  'title': 'Fast Video Generation with Sliding Tile Attention',\n",
       "  'summary': 'Diffusion Transformers (DiTs) with 3D full attention power state-of-the-art video generation, but suffer from prohibitive compute cost -- when generating just a 5-second 720P video, attention alone takes 800 out of 945 seconds of total inference time. This paper introduces sliding tile attention (STA) to address this challenge. STA leverages the observation that attention scores in pretrained video diffusion models predominantly concentrate within localized 3D windows. By sliding and attending over the local spatial-temporal region, STA eliminates redundancy from full attention. Unlike traditional token-wise sliding window attention (SWA), STA operates tile-by-tile with a novel hardware-aware sliding window design, preserving expressiveness while being hardware-efficient. With careful kernel-level optimizations, STA offers the first efficient 2D/3D sliding-window-like attention implementation, achieving 58.79% MFU. Precisely, STA accelerates attention by 2.8-17x over FlashAttention-2 (FA2) and 1.6-10x over FlashAttention-3 (FA3). On the leading video DiT, HunyuanVideo, STA reduces end-to-end latency from 945s (FA3) to 685s without quality degradation, requiring no training. Enabling finetuning further lowers latency to 268s with only a 0.09% drop on VBench.',\n",
       "  'upvotes': 28,\n",
       "  'discussionId': '67a98cd7b8b21202c90047c5'},\n",
       " {'id': '2502.05176',\n",
       "  'authors': [{'_id': '67a9889dc1fbde5146aba8b1',\n",
       "    'name': 'Chung-Ho Wu',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a9889dc1fbde5146aba8b2',\n",
       "    'name': 'Yang-Jung Chen',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a9889dc1fbde5146aba8b3',\n",
       "    'name': 'Ying-Huan Chen',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a9889dc1fbde5146aba8b4',\n",
       "    'name': 'Jie-Ying Lee',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a9889dc1fbde5146aba8b5', 'name': 'Bo-Hsu Ke', 'hidden': False},\n",
       "   {'_id': '67a9889dc1fbde5146aba8b6',\n",
       "    'name': 'Chun-Wei Tuan Mu',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a9889dc1fbde5146aba8b7',\n",
       "    'name': 'Yi-Chuan Huang',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a9889dc1fbde5146aba8b8',\n",
       "    'name': 'Chin-Yang Lin',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a9889dc1fbde5146aba8b9',\n",
       "    'name': 'Min-Hung Chen',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a9889dc1fbde5146aba8ba', 'name': 'Yen-Yu Lin', 'hidden': False},\n",
       "   {'_id': '67a9889dc1fbde5146aba8bb', 'name': 'Yu-Lun Liu', 'hidden': False}],\n",
       "  'publishedAt': '2025-02-07T18:59:55.000Z',\n",
       "  'title': 'AuraFusion360: Augmented Unseen Region Alignment for Reference-based  360° Unbounded Scene Inpainting',\n",
       "  'summary': 'Three-dimensional scene inpainting is crucial for applications from virtual reality to architectural visualization, yet existing methods struggle with view consistency and geometric accuracy in 360{deg} unbounded scenes. We present AuraFusion360, a novel reference-based method that enables high-quality object removal and hole filling in 3D scenes represented by Gaussian Splatting. Our approach introduces (1) depth-aware unseen mask generation for accurate occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot method for accurate initial point placement without requiring additional training, and (3) SDEdit-based detail enhancement for multi-view coherence. We also introduce 360-USID, the first comprehensive dataset for 360{deg} unbounded scene inpainting with ground truth. Extensive experiments demonstrate that AuraFusion360 significantly outperforms existing methods, achieving superior perceptual quality while maintaining geometric accuracy across dramatic viewpoint changes. See our project page for video results and the dataset at https://kkennethwu.github.io/aurafusion360/.',\n",
       "  'upvotes': 16,\n",
       "  'discussionId': '67a988a4c1fbde5146abaa3b'},\n",
       " {'id': '2502.04896',\n",
       "  'authors': [{'_id': '67a983ea9b72585dd12587fb',\n",
       "    'name': 'Shoufa Chen',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a983ea9b72585dd12587fc',\n",
       "    'name': 'Chongjian Ge',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a983ea9b72585dd12587fd', 'name': 'Yuqi Zhang', 'hidden': False},\n",
       "   {'_id': '67a983ea9b72585dd12587fe', 'name': 'Yida Zhang', 'hidden': False},\n",
       "   {'_id': '67a983ea9b72585dd12587ff', 'name': 'Fengda Zhu', 'hidden': False},\n",
       "   {'_id': '67a983ea9b72585dd1258800', 'name': 'Hao Yang', 'hidden': False},\n",
       "   {'_id': '67a983ea9b72585dd1258801',\n",
       "    'name': 'Hongxiang Hao',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a983ea9b72585dd1258802', 'name': 'Hui Wu', 'hidden': False},\n",
       "   {'_id': '67a983ea9b72585dd1258803', 'name': 'Zhichao Lai', 'hidden': False},\n",
       "   {'_id': '67a983ea9b72585dd1258804', 'name': 'Yifei Hu', 'hidden': False},\n",
       "   {'_id': '67a983ea9b72585dd1258805',\n",
       "    'name': 'Ting-Che Lin',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a983ea9b72585dd1258806',\n",
       "    'name': 'Shilong Zhang',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a983ea9b72585dd1258807', 'name': 'Fu Li', 'hidden': False},\n",
       "   {'_id': '67a983ea9b72585dd1258808', 'name': 'Chuan Li', 'hidden': False},\n",
       "   {'_id': '67a983ea9b72585dd1258809', 'name': 'Xing Wang', 'hidden': False},\n",
       "   {'_id': '67a983ea9b72585dd125880a',\n",
       "    'name': 'Yanghua Peng',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a983ea9b72585dd125880b', 'name': 'Peize Sun', 'hidden': False},\n",
       "   {'_id': '67a983ea9b72585dd125880c', 'name': 'Ping Luo', 'hidden': False},\n",
       "   {'_id': '67a983ea9b72585dd125880d', 'name': 'Yi Jiang', 'hidden': False},\n",
       "   {'_id': '67a983ea9b72585dd125880e', 'name': 'Zehuan Yuan', 'hidden': False},\n",
       "   {'_id': '67a983ea9b72585dd125880f',\n",
       "    'name': 'Bingyue Peng',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a983ea9b72585dd1258810',\n",
       "    'name': 'Xiaobing Liu',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-07T13:03:55.000Z',\n",
       "  'title': 'Goku: Flow Based Video Generative Foundation Models',\n",
       "  'summary': 'This paper introduces Goku, a state-of-the-art family of joint image-and-video generation models leveraging rectified flow Transformers to achieve industry-leading performance. We detail the foundational elements enabling high-quality visual generation, including the data curation pipeline, model architecture design, flow formulation, and advanced infrastructure for efficient and robust large-scale training. The Goku models demonstrate superior performance in both qualitative and quantitative evaluations, setting new benchmarks across major tasks. Specifically, Goku achieves 0.76 on GenEval and 83.65 on DPG-Bench for text-to-image generation, and 84.85 on VBench for text-to-video tasks. We believe that this work provides valuable insights and practical advancements for the research community in developing joint image-and-video generation models.',\n",
       "  'upvotes': 10,\n",
       "  'discussionId': '67a983ee9b72585dd125890f'},\n",
       " {'id': '2502.04363',\n",
       "  'authors': [{'_id': '67a98180d0dc1ed664297368',\n",
       "    'name': 'Bosung Kim',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a98180d0dc1ed664297369', 'name': 'Kyuhwan Lee', 'hidden': False},\n",
       "   {'_id': '67a98180d0dc1ed66429736a', 'name': 'Isu Jeong', 'hidden': False},\n",
       "   {'_id': '67a98180d0dc1ed66429736b',\n",
       "    'name': 'Jungmin Cheon',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a98180d0dc1ed66429736c', 'name': 'Yeojin Lee', 'hidden': False},\n",
       "   {'_id': '67a98180d0dc1ed66429736d', 'name': 'Seulki Lee', 'hidden': False}],\n",
       "  'publishedAt': '2025-02-05T05:42:29.000Z',\n",
       "  'title': 'On-device Sora: Enabling Diffusion-Based Text-to-Video Generation for  Mobile Devices',\n",
       "  'summary': 'We present On-device Sora, a first pioneering solution for diffusion-based on-device text-to-video generation that operates efficiently on smartphone-grade devices. Building on Open-Sora, On-device Sora applies three novel techniques to address the challenges of diffusion-based text-to-video generation on computation- and memory-limited mobile devices. First, Linear Proportional Leap (LPL) reduces the excessive denoising steps required in video diffusion through an efficient leap-based approach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensive token-processing computation in attention layers by merging consecutive tokens along the temporal dimension. Third, Concurrent Inference with Dynamic Loading (CI-DL) dynamically partitions large models into smaller blocks and loads them into memory for concurrent model inference, effectively addressing the challenges of limited device memory. We implement On-device Sora on the iPhone 15 Pro, and the experimental evaluations demonstrate that it is capable of generating high-quality videos on the device, comparable to those produced by Open-Sora running on high-end GPUs. These results show that On-device Sora enables efficient and high-quality video generation on resource-constrained mobile devices, expanding accessibility, ensuring user privacy, reducing dependence on cloud infrastructure, and lowering associated costs. We envision the proposed On-device Sora as a significant first step toward democratizing state-of-the-art generative technologies, enabling video generation capabilities on commodity mobile and embedded devices. The code implementation is publicly available at an GitHub repository: https://github.com/eai-lab/On-device-Sora.',\n",
       "  'upvotes': 3,\n",
       "  'discussionId': '67a98185d0dc1ed664297491'},\n",
       " {'id': '2502.04520',\n",
       "  'authors': [{'_id': '67a97eea96d822bc6e13a1bb',\n",
       "    'name': 'Letian Peng',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97eea96d822bc6e13a1bc', 'name': 'Chenyang An', 'hidden': False},\n",
       "   {'_id': '67a97eea96d822bc6e13a1bd', 'name': 'Shibo Hao', 'hidden': False},\n",
       "   {'_id': '67a97eea96d822bc6e13a1be',\n",
       "    'name': 'Chengyu Dong',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97eea96d822bc6e13a1bf',\n",
       "    'name': 'Jingbo Shang',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-06T21:44:30.000Z',\n",
       "  'title': \"Linear Correlation in LM's Compositional Generalization and  Hallucination\",\n",
       "  'summary': 'The generalization of language models (LMs) is undergoing active debates, contrasting their potential for general intelligence with their struggles with basic knowledge composition (e.g., reverse/transition curse). This paper uncovers the phenomenon of linear correlations in LMs during knowledge composition. For explanation, there exists a linear transformation between certain related knowledge that maps the next token prediction logits from one prompt to another, e.g., \"X lives in the city of\" rightarrow \"X lives in the country of\" for every given X. This mirrors the linearity in human knowledge composition, such as Paris rightarrow France. Our findings indicate that the linear transformation is resilient to large-scale fine-tuning, generalizing updated knowledge when aligned with real-world relationships, but causing hallucinations when it deviates. Empirical results suggest that linear correlation can serve as a potential identifier of LM\\'s generalization. Finally, we show such linear correlations can be learned with a single feedforward network and pre-trained vocabulary representations, indicating LM generalization heavily relies on the latter.',\n",
       "  'upvotes': 5,\n",
       "  'discussionId': '67a97eea96d822bc6e13a1e7'},\n",
       " {'id': '2502.05171',\n",
       "  'authors': [{'_id': '67a97e27495b23306cd5ea56',\n",
       "    'name': 'Jonas Geiping',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97e27495b23306cd5ea57',\n",
       "    'name': 'Sean McLeish',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97e27495b23306cd5ea58', 'name': 'Neel Jain', 'hidden': False},\n",
       "   {'_id': '67a97e27495b23306cd5ea59',\n",
       "    'name': 'John Kirchenbauer',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97e27495b23306cd5ea5a',\n",
       "    'name': 'Siddharth Singh',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97e27495b23306cd5ea5b',\n",
       "    'name': 'Brian R. Bartoldson',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97e27495b23306cd5ea5c',\n",
       "    'name': 'Bhavya Kailkhura',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97e27495b23306cd5ea5d',\n",
       "    'name': 'Abhinav Bhatele',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97e27495b23306cd5ea5e',\n",
       "    'name': 'Tom Goldstein',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-07T18:55:02.000Z',\n",
       "  'title': 'Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth  Approach',\n",
       "  'summary': 'We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.',\n",
       "  'upvotes': 4,\n",
       "  'discussionId': '67a97e29495b23306cd5eae5'},\n",
       " {'id': '2502.04728',\n",
       "  'authors': [{'_id': '67a97d1c02da0cdf059cb0d8',\n",
       "    'name': 'Zhouliang Yu',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97d1c02da0cdf059cb0d9', 'name': 'Yuhuan Yuan', 'hidden': False},\n",
       "   {'_id': '67a97d1c02da0cdf059cb0da', 'name': 'Tim Z. Xiao', 'hidden': False},\n",
       "   {'_id': '67a97d1c02da0cdf059cb0db',\n",
       "    'name': 'Fuxiang Frank Xia',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97d1c02da0cdf059cb0dc', 'name': 'Jie Fu', 'hidden': False},\n",
       "   {'_id': '67a97d1c02da0cdf059cb0dd', 'name': 'Ge Zhang', 'hidden': False},\n",
       "   {'_id': '67a97d1c02da0cdf059cb0de', 'name': 'Ge Lin', 'hidden': False},\n",
       "   {'_id': '67a97d1c02da0cdf059cb0df',\n",
       "    'name': 'Weiyang Liu',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-07T07:52:25.000Z',\n",
       "  'title': 'Generating Symbolic World Models via Test-time Scaling of Large Language  Models',\n",
       "  'summary': 'Solving complex planning problems requires Large Language Models (LLMs) to explicitly model the state transition to avoid rule violations, comply with constraints, and ensure optimality-a task hindered by the inherent ambiguity of natural language. To overcome such ambiguity, Planning Domain Definition Language (PDDL) is leveraged as a planning abstraction that enables precise and formal state descriptions. With PDDL, we can generate a symbolic world model where classic searching algorithms, such as A*, can be seamlessly applied to find optimal plans. However, directly generating PDDL domains with current LLMs remains an open challenge due to the lack of PDDL training data. To address this challenge, we propose to scale up the test-time computation of LLMs to enhance their PDDL reasoning capabilities, thereby enabling the generation of high-quality PDDL domains. Specifically, we introduce a simple yet effective algorithm, which first employs a Best-of-N sampling approach to improve the quality of the initial solution and then refines the solution in a fine-grained manner with verbalized machine learning. Our method outperforms o1-mini by a considerable margin in the generation of PDDL domain, achieving over 50% success rate on two tasks (i.e., generating PDDL domains from natural language description or PDDL problems). This is done without requiring additional training. By taking advantage of PDDL as state abstraction, our method is able to outperform current state-of-the-art methods on almost all competition-level planning tasks.',\n",
       "  'upvotes': 3,\n",
       "  'discussionId': '67a97d1d02da0cdf059cb11a'},\n",
       " {'id': '2502.04403',\n",
       "  'authors': [{'_id': '67a97c7542d4d2f92ee57d20',\n",
       "    'name': 'David Abel',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97c7542d4d2f92ee57d21',\n",
       "    'name': 'André Barreto',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97c7542d4d2f92ee57d22',\n",
       "    'name': 'Michael Bowling',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97c7542d4d2f92ee57d23', 'name': 'Will Dabney', 'hidden': False},\n",
       "   {'_id': '67a97c7542d4d2f92ee57d24', 'name': 'Shi Dong', 'hidden': False},\n",
       "   {'_id': '67a97c7542d4d2f92ee57d25',\n",
       "    'name': 'Steven Hansen',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97c7542d4d2f92ee57d26',\n",
       "    'name': 'Anna Harutyunyan',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97c7542d4d2f92ee57d27',\n",
       "    'name': 'Khimya Khetarpal',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97c7542d4d2f92ee57d28', 'name': 'Clare Lyle', 'hidden': False},\n",
       "   {'_id': '67a97c7542d4d2f92ee57d29',\n",
       "    'name': 'Razvan Pascanu',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97c7542d4d2f92ee57d2a',\n",
       "    'name': 'Georgios Piliouras',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97c7542d4d2f92ee57d2b',\n",
       "    'name': 'Doina Precup',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97c7542d4d2f92ee57d2c',\n",
       "    'name': 'Jonathan Richens',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97c7542d4d2f92ee57d2d',\n",
       "    'name': 'Mark Rowland',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97c7542d4d2f92ee57d2e', 'name': 'Tom Schaul', 'hidden': False},\n",
       "   {'_id': '67a97c7542d4d2f92ee57d2f',\n",
       "    'name': 'Satinder Singh',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-06T08:34:57.000Z',\n",
       "  'title': 'Agency Is Frame-Dependent',\n",
       "  'summary': \"Agency is a system's capacity to steer outcomes toward a goal, and is a central topic of study across biology, philosophy, cognitive science, and artificial intelligence. Determining if a system exhibits agency is a notoriously difficult question: Dennett (1989), for instance, highlights the puzzle of determining which principles can decide whether a rock, a thermostat, or a robot each possess agency. We here address this puzzle from the viewpoint of reinforcement learning by arguing that agency is fundamentally frame-dependent: Any measurement of a system's agency must be made relative to a reference frame. We support this claim by presenting a philosophical argument that each of the essential properties of agency proposed by Barandiaran et al. (2009) and Moreno (2018) are themselves frame-dependent. We conclude that any basic science of agency requires frame-dependence, and discuss the implications of this claim for reinforcement learning.\",\n",
       "  'upvotes': 6,\n",
       "  'discussionId': '67a97c7642d4d2f92ee57d77'},\n",
       " {'id': '2502.04404',\n",
       "  'authors': [{'_id': '67a97bc5500b3bcf5babc5e8',\n",
       "    'name': 'Xiao-Wen Yang',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97bc5500b3bcf5babc5e9', 'name': 'Xuan-Yi Zhu', 'hidden': False},\n",
       "   {'_id': '67a97bc5500b3bcf5babc5ea', 'name': 'Wen-Da Wei', 'hidden': False},\n",
       "   {'_id': '67a97bc5500b3bcf5babc5eb',\n",
       "    'name': 'Ding-Chu Zhang',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97bc5500b3bcf5babc5ec',\n",
       "    'name': 'Jie-Jing Shao',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97bc5500b3bcf5babc5ed', 'name': 'Zhi Zhou', 'hidden': False},\n",
       "   {'_id': '67a97bc5500b3bcf5babc5ee', 'name': 'Lan-Zhe Guo', 'hidden': False},\n",
       "   {'_id': '67a97bc5500b3bcf5babc5ef', 'name': 'Yu-Feng Li', 'hidden': False}],\n",
       "  'publishedAt': '2025-02-06T08:52:43.000Z',\n",
       "  'title': 'Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of  Language Models',\n",
       "  'summary': \"The integration of slow-thinking mechanisms into large language models (LLMs) offers a promising way toward achieving Level 2 AGI Reasoners, as exemplified by systems like OpenAI's o1. However, several significant challenges remain, including inefficient overthinking and an overreliance on auxiliary reward models. We point out that these limitations stem from LLMs' inability to internalize the search process, a key component of effective reasoning. A critical step toward addressing this issue is enabling LLMs to autonomously determine when and where to backtrack, a fundamental operation in traditional search algorithms. To this end, we propose a self-backtracking mechanism that equips LLMs with the ability to backtrack during both training and inference. This mechanism not only enhances reasoning ability but also efficiency by transforming slow-thinking processes into fast-thinking through self-improvement. Empirical evaluations demonstrate that our proposal significantly enhances the reasoning capabilities of LLMs, achieving a performance gain of over 40 percent compared to the optimal-path supervised fine-tuning method. We believe this study introduces a novel and promising pathway for developing more advanced and robust Reasoners.\",\n",
       "  'upvotes': 1,\n",
       "  'discussionId': '67a97bc7500b3bcf5babc64e'},\n",
       " {'id': '2502.05173',\n",
       "  'authors': [{'_id': '67a97a47174028234b74f687',\n",
       "    'name': 'Xilin Wei',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97a47174028234b74f688', 'name': 'Xiaoran Liu', 'hidden': False},\n",
       "   {'_id': '67a97a47174028234b74f689', 'name': 'Yuhang Zang', 'hidden': False},\n",
       "   {'_id': '67a97a47174028234b74f68a', 'name': 'Xiaoyi Dong', 'hidden': False},\n",
       "   {'_id': '67a97a47174028234b74f68b', 'name': 'Pan Zhang', 'hidden': False},\n",
       "   {'_id': '67a97a47174028234b74f68c', 'name': 'Yuhang Cao', 'hidden': False},\n",
       "   {'_id': '67a97a47174028234b74f68d', 'name': 'Jian Tong', 'hidden': False},\n",
       "   {'_id': '67a97a47174028234b74f68e',\n",
       "    'name': 'Haodong Duan',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97a47174028234b74f68f', 'name': 'Qipeng Guo', 'hidden': False},\n",
       "   {'_id': '67a97a47174028234b74f690', 'name': 'Jiaqi Wang', 'hidden': False},\n",
       "   {'_id': '67a97a47174028234b74f691', 'name': 'Xipeng Qiu', 'hidden': False},\n",
       "   {'_id': '67a97a47174028234b74f692', 'name': 'Dahua Lin', 'hidden': False}],\n",
       "  'publishedAt': '2025-02-07T18:56:04.000Z',\n",
       "  'title': 'VideoRoPE: What Makes for Good Video Rotary Position Embedding?',\n",
       "  'summary': 'While Rotary Position Embedding (RoPE) and its variants are widely adopted for their long-context capabilities, the extension of the 1D RoPE to video, with its complex spatio-temporal structure, remains an open challenge. This work first introduces a comprehensive analysis that identifies four key characteristics essential for the effective adaptation of RoPE to video, which have not been fully considered in prior work. As part of our analysis, we introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors) task, which adds periodic distractors into V-NIAH. The V-NIAH-D task demonstrates that previous RoPE variants, lacking appropriate temporal dimension allocation, are easily misled by distractors. Based on our analysis, we introduce VideoRoPE, with a 3D structure designed to preserve spatio-temporal relationships. VideoRoPE features low-frequency temporal allocation to mitigate periodic oscillations, a diagonal layout to maintain spatial symmetry, and adjustable temporal spacing to decouple temporal and spatial indexing. VideoRoPE consistently surpasses previous RoPE variants, across diverse downstream tasks such as long video retrieval, video understanding, and video hallucination. Our code will be available at https://github.com/Wiselnn570/VideoRoPE{https://github.com/Wiselnn570/VideoRoPE}.',\n",
       "  'upvotes': 22,\n",
       "  'discussionId': '67a97a4a174028234b74f707'},\n",
       " {'id': '2502.04350',\n",
       "  'authors': [{'_id': '67a97a77d163c9e6ea2bdb85',\n",
       "    'name': 'Yongchao Chen',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a97a77d163c9e6ea2bdb86', 'name': 'Yilun Hao', 'hidden': False},\n",
       "   {'_id': '67a97a77d163c9e6ea2bdb87', 'name': 'Yueying Liu', 'hidden': False},\n",
       "   {'_id': '67a97a77d163c9e6ea2bdb88', 'name': 'Yang Zhang', 'hidden': False},\n",
       "   {'_id': '67a97a77d163c9e6ea2bdb89', 'name': 'Chuchu Fan', 'hidden': False}],\n",
       "  'publishedAt': '2025-02-04T15:53:59.000Z',\n",
       "  'title': 'CodeSteer: Symbolic-Augmented Language Models via Code/Text Guidance',\n",
       "  'summary': 'Existing methods fail to effectively steer Large Language Models (LLMs) between textual reasoning and code generation, leaving symbolic computing capabilities underutilized. We introduce CodeSteer, an effective method for guiding LLM code/text generation. We construct a comprehensive benchmark SymBench comprising 37 symbolic tasks with adjustable complexity and also synthesize datasets of 12k multi-round guidance/generation trajectories and 5.5k guidance comparison pairs. We fine-tune the Llama-3-8B model with a newly designed multi-round supervised fine-tuning (SFT) and direct preference optimization (DPO). The resulting model, CodeSteerLLM, augmented with the proposed symbolic and self-answer checkers, effectively guides the code/text generation of larger models. Augmenting GPT-4o with CodeSteer raises its average performance score from 53.3 to 86.4, even outperforming the existing best LLM OpenAI o1 (82.7), o1-preview (74.8), and DeepSeek R1 (76.8) across all 37 tasks (28 seen, 9 unseen). Trained for GPT-4o, CodeSteer demonstrates superior generalizability, providing an average 41.8 performance boost on Claude, Mistral, and GPT-3.5. CodeSteer-guided LLMs fully harness symbolic computing to maintain strong performance on highly complex tasks. Models, Datasets, and Codes are available at https://github.com/yongchao98/CodeSteer-v1.0.',\n",
       "  'upvotes': 2,\n",
       "  'discussionId': '67a97a79d163c9e6ea2bdc0c'},\n",
       " {'id': '2502.04320',\n",
       "  'authors': [{'_id': '67a6431d0fdd5543151da7d2',\n",
       "    'user': {'_id': '62d757a22d32f0bff5710596',\n",
       "     'avatarUrl': '/avatars/f3e05ea4fb853420923e04b6bf3a1a6e.svg',\n",
       "     'isPro': True,\n",
       "     'fullname': 'Alec Helbling',\n",
       "     'user': 'helblazer811',\n",
       "     'type': 'user'},\n",
       "    'name': 'Alec Helbling',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-08T13:56:13.003Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a6431d0fdd5543151da7d3',\n",
       "    'user': {'_id': '64f8b03f83807928d25e766f',\n",
       "     'avatarUrl': '/avatars/68fd4ee967a1673a1d78a7581be8b3da.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Tuna Han Salih Meral',\n",
       "     'user': 'tmeral',\n",
       "     'type': 'user'},\n",
       "    'name': 'Tuna Han Salih Meral',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-08T13:56:15.868Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a6431d0fdd5543151da7d4', 'name': 'Ben Hoover', 'hidden': False},\n",
       "   {'_id': '67a6431d0fdd5543151da7d5',\n",
       "    'name': 'Pinar Yanardag',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a6431d0fdd5543151da7d6',\n",
       "    'name': 'Duen Horng Chau',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-06T18:59:00.000Z',\n",
       "  'title': 'ConceptAttention: Diffusion Transformers Learn Highly Interpretable  Features',\n",
       "  'summary': 'Do the rich representations of multi-modal diffusion transformers (DiTs) exhibit unique properties that enhance their interpretability? We introduce ConceptAttention, a novel method that leverages the expressive power of DiT attention layers to generate high-quality saliency maps that precisely locate textual concepts within images. Without requiring additional training, ConceptAttention repurposes the parameters of DiT attention layers to produce highly contextualized concept embeddings, contributing the major discovery that performing linear projections in the output space of DiT attention layers yields significantly sharper saliency maps compared to commonly used cross-attention mechanisms. Remarkably, ConceptAttention even achieves state-of-the-art performance on zero-shot image segmentation benchmarks, outperforming 11 other zero-shot interpretability methods on the ImageNet-Segmentation dataset and on a single-class subset of PascalVOC. Our work contributes the first evidence that the representations of multi-modal DiT models like Flux are highly transferable to vision tasks like segmentation, even outperforming multi-modal foundation models like CLIP.',\n",
       "  'upvotes': 26,\n",
       "  'discussionId': '67a643200fdd5543151da869'},\n",
       " {'id': '2502.00473',\n",
       "  'authors': [{'_id': '67a5f635c20315f5e3f16f62',\n",
       "    'user': {'_id': '66348bf4e1555067669870fa',\n",
       "     'avatarUrl': '/avatars/8b8bbc7dff7d9a0a02b0960084bc95ab.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': '白立忱',\n",
       "     'user': 'Indulge-Bai',\n",
       "     'type': 'user'},\n",
       "    'name': 'Lichen Bai',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-07T12:56:27.462Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5f635c20315f5e3f16f63',\n",
       "    'name': 'Masashi Sugiyama',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5f635c20315f5e3f16f64', 'name': 'Zeke Xie', 'hidden': False}],\n",
       "  'publishedAt': '2025-02-01T16:00:08.000Z',\n",
       "  'title': 'Weak-to-Strong Diffusion with Reflection',\n",
       "  'summary': 'The goal of diffusion generative models is to align the learned distribution with the real data distribution through gradient score matching. However, inherent limitations in training data quality, modeling strategies, and architectural design lead to inevitable gap between generated outputs and real data. To reduce this gap, we propose Weak-to-Strong Diffusion (W2SD), a novel framework that utilizes the estimated difference between existing weak and strong models (i.e., weak-to-strong difference) to approximate the gap between an ideal model and a strong model. By employing a reflective operation that alternates between denoising and inversion with weak-to-strong difference, we theoretically understand that W2SD steers latent variables along sampling trajectories toward regions of the real data distribution. W2SD is highly flexible and broadly applicable, enabling diverse improvements through the strategic selection of weak-to-strong model pairs (e.g., DreamShaper vs. SD1.5, good experts vs. bad experts in MoE). Extensive experiments demonstrate that W2SD significantly improves human preference, aesthetic quality, and prompt adherence, achieving SOTA performance across various modalities (e.g., image, video), architectures (e.g., UNet-based, DiT-based, MoE), and benchmarks. For example, Juggernaut-XL with W2SD can improve with the HPSv2 winning rate up to 90% over the original results. Moreover, the performance gains achieved by W2SD markedly outweigh its additional computational overhead, while the cumulative improvements from different weak-to-strong difference further solidify its practical utility and deployability.',\n",
       "  'upvotes': 19,\n",
       "  'discussionId': '67a5f638c20315f5e3f17086'},\n",
       " {'id': '2502.00988',\n",
       "  'authors': [{'_id': '67a5e076b94446dfc848533b',\n",
       "    'name': 'Kanika Goswami',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5e076b94446dfc848533c',\n",
       "    'name': 'Puneet Mathur',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5e076b94446dfc848533d', 'name': 'Ryan Rossi', 'hidden': False},\n",
       "   {'_id': '67a5e076b94446dfc848533e',\n",
       "    'user': {'_id': '62c5947524171688a9feb992',\n",
       "     'avatarUrl': '/avatars/5a151713b9eae8dc566f5957acee3475.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Franck Dernoncourt',\n",
       "     'user': 'Franck-Dernoncourt',\n",
       "     'type': 'user'},\n",
       "    'name': 'Franck Dernoncourt',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-07T10:30:03.421Z',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-03T02:00:29.000Z',\n",
       "  'title': 'PlotGen: Multi-Agent LLM-based Scientific Data Visualization via  Multimodal Feedback',\n",
       "  'summary': 'Scientific data visualization is pivotal for transforming raw data into comprehensible visual representations, enabling pattern recognition, forecasting, and the presentation of data-driven insights. However, novice users often face difficulties due to the complexity of selecting appropriate tools and mastering visualization techniques. Large Language Models (LLMs) have recently demonstrated potential in assisting code generation, though they struggle with accuracy and require iterative debugging. In this paper, we propose PlotGen, a novel multi-agent framework aimed at automating the creation of precise scientific visualizations. PlotGen orchestrates multiple LLM-based agents, including a Query Planning Agent that breaks down complex user requests into executable steps, a Code Generation Agent that converts pseudocode into executable Python code, and three retrieval feedback agents - a Numeric Feedback Agent, a Lexical Feedback Agent, and a Visual Feedback Agent - that leverage multimodal LLMs to iteratively refine the data accuracy, textual labels, and visual correctness of generated plots via self-reflection. Extensive experiments show that PlotGen outperforms strong baselines, achieving a 4-6 percent improvement on the MatPlotBench dataset, leading to enhanced user trust in LLM-generated visualizations and improved novice productivity due to a reduction in debugging time needed for plot errors.',\n",
       "  'upvotes': 5,\n",
       "  'discussionId': '67a5e077b94446dfc8485375'},\n",
       " {'id': '2501.19085',\n",
       "  'authors': [{'_id': '67a5b65fe7798ca5b7473a45',\n",
       "    'user': {'_id': '663486a1f64712540644cb68',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/663486a1f64712540644cb68/YZFR41ERY6UrC6rCC6Nan.jpeg',\n",
       "     'isPro': True,\n",
       "     'fullname': 'Alessandro',\n",
       "     'user': 'Devy1',\n",
       "     'type': 'user'},\n",
       "    'name': 'Alessandro Giagnorio',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-07T09:57:46.514Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5b65fe7798ca5b7473a46',\n",
       "    'user': {'_id': '65a7cb0fc5ffe1d019a21cb3',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/GShcO1DwVNzlIUr3n1ifi.jpeg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Alberto Martín López',\n",
       "     'user': 'AML14',\n",
       "     'type': 'user'},\n",
       "    'name': 'Alberto Martin-Lopez',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:46:32.306Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5b65fe7798ca5b7473a47',\n",
       "    'user': {'_id': '6638bea59e57161faac814e7',\n",
       "     'avatarUrl': '/avatars/91375b88945af50e51b7229a789a31b8.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Gabriele Bavota',\n",
       "     'user': 'gbavota',\n",
       "     'type': 'user'},\n",
       "    'name': 'Gabriele Bavota',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:46:38.124Z',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-01-31T12:23:28.000Z',\n",
       "  'title': 'Enhancing Code Generation for Low-Resource Languages: No Silver Bullet',\n",
       "  'summary': \"The advent of Large Language Models (LLMs) has significantly advanced the field of automated code generation. LLMs rely on large and diverse datasets to learn syntax, semantics, and usage patterns of programming languages. For low-resource languages (i.e., niche programming languages characterized by the scarcity of training data), the limited availability of such data hampers the models' ability to generalize effectively, resulting in poorer code generation performance as compared to high-resource languages. For this reason, there is a quest for techniques able to close this performance gap. We present an empirical study investigating the effectiveness of several approaches for boosting LLMs' performance on low-resource languages, namely: (i) a classic fine-tuning, which is however capped in size by the scarcity of training data; (ii) three variants of in-context learning, with prompts crafted to provide the LLM with additional information about the low-resource language (e.g., few-shot examples showcasing features of the targeted language); and (iii) a pre-training objective teaching the model how to translate between high- and low-resource languages. The context of our study are two low-resource languages (R and Racket) and six LLMs having different architectures and sizes. Our findings reveal that a fine-tuning is usually the best choice for smaller LLMs, possibly due to the fact that even a small dataset is sufficient to train their limited number of parameters. With the increase in size of the models, in-context learning becomes more and more effective, representing a safe and cheap bet (i.e., it always helps, but with different magnitudes). Differently, very large LLMs may deteriorate their performance on low-resource languages when fine-tuning is performed, possibly due to the lack of enough data needed to effectively update their weights.\",\n",
       "  'upvotes': 4,\n",
       "  'discussionId': '67a5b660e7798ca5b7473a6b'},\n",
       " {'id': '2502.00989',\n",
       "  'authors': [{'_id': '67a5c7601e6db426653ebc3d',\n",
       "    'name': 'Kanika Goswami',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5c7601e6db426653ebc3e',\n",
       "    'user': {'_id': '65c16444d4c3b8dff2f0d78d',\n",
       "     'avatarUrl': '/avatars/4ed764c1657bd260d2a12ba61c111062.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Puneet Mathur',\n",
       "     'user': 'puneetm',\n",
       "     'type': 'user'},\n",
       "    'name': 'Puneet Mathur',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:47:20.093Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5c7601e6db426653ebc3f',\n",
       "    'user': {'_id': '62a3ab83e4dd6252344d27cd',\n",
       "     'avatarUrl': '/avatars/7ca8510f70a58dc207b104240e30c35c.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Ryan A. Rossi',\n",
       "     'user': 'ryanrossi',\n",
       "     'type': 'user'},\n",
       "    'name': 'Ryan Rossi',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:47:32.496Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5c7601e6db426653ebc40',\n",
       "    'user': {'_id': '62c5947524171688a9feb992',\n",
       "     'avatarUrl': '/avatars/5a151713b9eae8dc566f5957acee3475.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Franck Dernoncourt',\n",
       "     'user': 'Franck-Dernoncourt',\n",
       "     'type': 'user'},\n",
       "    'name': 'Franck Dernoncourt',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-07T10:30:50.575Z',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-03T02:00:51.000Z',\n",
       "  'title': 'ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual  Attribution',\n",
       "  'summary': 'Large Language Models (LLMs) can perform chart question-answering tasks but often generate unverified hallucinated responses. Existing answer attribution methods struggle to ground responses in source charts due to limited visual-semantic context, complex visual-text alignment requirements, and difficulties in bounding box prediction across complex layouts. We present ChartCitor, a multi-agent framework that provides fine-grained bounding box citations by identifying supporting evidence within chart images. The system orchestrates LLM agents to perform chart-to-table extraction, answer reformulation, table augmentation, evidence retrieval through pre-filtering and re-ranking, and table-to-chart mapping. ChartCitor outperforms existing baselines across different chart types. Qualitative user studies show that ChartCitor helps increase user trust in Generative AI by providing enhanced explainability for LLM-assisted chart QA and enables professionals to be more productive.',\n",
       "  'upvotes': 7,\n",
       "  'discussionId': '67a5c7621e6db426653ebc8a'},\n",
       " {'id': '2502.04313',\n",
       "  'authors': [{'_id': '67a5b9107897c8f5406155e0',\n",
       "    'user': {'_id': '6506832221ac448013f94995',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6506832221ac448013f94995/sVUI1JV4Dxan5l-MqNze4.jpeg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Shashwat Goel',\n",
       "     'user': 'shash42',\n",
       "     'type': 'user'},\n",
       "    'name': 'Shashwat Goel',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:39:36.508Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5b9107897c8f5406155e1',\n",
       "    'user': {'_id': '6728c6113d35dd53cfe9f30c',\n",
       "     'avatarUrl': '/avatars/7f93b9d41446cce382f63c78ca5059a1.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Joschka Strüber',\n",
       "     'user': 'Klingspor',\n",
       "     'type': 'user'},\n",
       "    'name': 'Joschka Struber',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:39:43.250Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5b9107897c8f5406155e2',\n",
       "    'user': {'_id': '671b49503fd1d03dc69194b0',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/tnkR0j1VaWClUcumXcgjQ.png',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Ilze Amanda Auzina',\n",
       "     'user': 'iaa01',\n",
       "     'type': 'user'},\n",
       "    'name': 'Ilze Amanda Auzina',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:40:04.242Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5b9107897c8f5406155e3',\n",
       "    'name': 'Karuna K Chandra',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5b9107897c8f5406155e4',\n",
       "    'user': {'_id': '67a6a4b7f379cef464950268',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yaMsT2jYsj6mvvYU1gvO_.jpeg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'ponnurangam kumaraguru',\n",
       "     'user': 'pk-profgiri',\n",
       "     'type': 'user'},\n",
       "    'name': 'Ponnurangam Kumaraguru',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-08T13:56:22.133Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5b9107897c8f5406155e5',\n",
       "    'user': {'_id': '61dc997715b47073db1620dc',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1641847245435-61dc997715b47073db1620dc.jpeg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Douwe Kiela',\n",
       "     'user': 'douwekiela',\n",
       "     'type': 'user'},\n",
       "    'name': 'Douwe Kiela',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:40:22.153Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5b9107897c8f5406155e6',\n",
       "    'user': {'_id': '6464a0d41683d3c81f51924a',\n",
       "     'avatarUrl': '/avatars/bfa89f568302fa34a641e0d8744bf8b5.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Ameya Prabhu',\n",
       "     'user': 'AmeyaPrabhu',\n",
       "     'type': 'user'},\n",
       "    'name': 'Ameya Prabhu',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:40:34.763Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5b9107897c8f5406155e7',\n",
       "    'name': 'Matthias Bethge',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5b9107897c8f5406155e8',\n",
       "    'user': {'_id': '63d86dbf3130cadcaf8bdd11',\n",
       "     'avatarUrl': '/avatars/29d79a0c6dcec01111ef192fecd0fa7a.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Jonas Geiping',\n",
       "     'user': 'JonasGeiping',\n",
       "     'type': 'user'},\n",
       "    'name': 'Jonas Geiping',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:40:49.233Z',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-06T18:56:01.000Z',\n",
       "  'title': 'Great Models Think Alike and this Undermines AI Oversight',\n",
       "  'summary': 'As Language Model (LM) capabilities advance, evaluating and supervising them at scale is getting harder for humans. There is hope that other language models can automate both these tasks, which we refer to as \"AI Oversight\". We study how model similarity affects both aspects of AI oversight by proposing a probabilistic metric for LM similarity based on overlap in model mistakes. Using this metric, we first show that LLM-as-a-judge scores favor models similar to the judge, generalizing recent self-preference results. Then, we study training on LM annotations, and find complementary knowledge between the weak supervisor and strong student model plays a crucial role in gains from \"weak-to-strong generalization\". As model capabilities increase, it becomes harder to find their mistakes, and we might defer more to AI oversight. However, we observe a concerning trend -- model mistakes are becoming more similar with increasing capabilities, pointing to risks from correlated failures. Our work underscores the importance of reporting and correcting for model similarity, especially in the emerging paradigm of AI oversight.',\n",
       "  'upvotes': 21,\n",
       "  'discussionId': '67a5b9137897c8f540615673'},\n",
       " {'id': '2502.04322',\n",
       "  'authors': [{'_id': '67a5a9357415f9155e9b4b58',\n",
       "    'name': 'Yik Siu Chan',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5a9357415f9155e9b4b59',\n",
       "    'user': {'_id': '64698ed0dcbb937d56b9dd02',\n",
       "     'avatarUrl': '/avatars/835ce9bf6e2cd1d4b7a709cf41a884e2.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Edward Ri',\n",
       "     'user': 'narutatsuri',\n",
       "     'type': 'user'},\n",
       "    'name': 'Narutatsu Ri',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-07T09:57:58.519Z',\n",
       "    'hidden': True},\n",
       "   {'_id': '67a5a9357415f9155e9b4b5a',\n",
       "    'user': {'_id': '64bf072bae436c8813494ba3',\n",
       "     'avatarUrl': '/avatars/afb96d2bbf90411f4b1a030ebebff300.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Yuxin Xiao',\n",
       "     'user': 'YuxinXiao',\n",
       "     'type': 'user'},\n",
       "    'name': 'Yuxin Xiao',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-07T09:58:00.910Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5a9357415f9155e9b4b5b',\n",
       "    'name': 'Marzyeh Ghassemi',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-06T18:59:02.000Z',\n",
       "  'title': 'Speak Easy: Eliciting Harmful Jailbreaks from LLMs with Simple  Interactions',\n",
       "  'summary': 'Despite extensive safety alignment efforts, large language models (LLMs) remain vulnerable to jailbreak attacks that elicit harmful behavior. While existing studies predominantly focus on attack methods that require technical expertise, two critical questions remain underexplored: (1) Are jailbroken responses truly useful in enabling average users to carry out harmful actions? (2) Do safety vulnerabilities exist in more common, simple human-LLM interactions? In this paper, we demonstrate that LLM responses most effectively facilitate harmful actions when they are both actionable and informative--two attributes easily elicited in multi-step, multilingual interactions. Using this insight, we propose HarmScore, a jailbreak metric that measures how effectively an LLM response enables harmful actions, and Speak Easy, a simple multi-step, multilingual attack framework. Notably, by incorporating Speak Easy into direct request and jailbreak baselines, we see an average absolute increase of 0.319 in Attack Success Rate and 0.426 in HarmScore in both open-source and proprietary LLMs across four safety benchmarks. Our work reveals a critical yet often overlooked vulnerability: Malicious users can easily exploit common interaction patterns for harmful intentions.',\n",
       "  'upvotes': 3,\n",
       "  'discussionId': '67a5a9367415f9155e9b4bbb'},\n",
       " {'id': '2502.03032',\n",
       "  'authors': [{'_id': '67a59c4e7ffacd843a56404a',\n",
       "    'user': {'_id': '634c5f8cfb80cc6bcaf42c03',\n",
       "     'avatarUrl': '/avatars/1f37db0e70cbaf9707f4c8cbcee37ca0.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Daniil Laptev',\n",
       "     'user': 'dlaptev',\n",
       "     'type': 'user'},\n",
       "    'name': 'Daniil Laptev',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-07T09:58:04.546Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a59c4e7ffacd843a56404b',\n",
       "    'user': {'_id': '60b364e7f88532cd79eaff7b',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1654185363389-60b364e7f88532cd79eaff7b.jpeg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Nikita Balagansky',\n",
       "     'user': 'elephantmipt',\n",
       "     'type': 'user'},\n",
       "    'name': 'Nikita Balagansky',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-07T09:58:02.693Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a59c4e7ffacd843a56404c',\n",
       "    'name': 'Yaroslav Aksenov',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a59c4e7ffacd843a56404d',\n",
       "    'user': {'_id': '62a9c8edc19f92ae443ab37f',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Daniil Gavrilov',\n",
       "     'user': 'kefirski',\n",
       "     'type': 'user'},\n",
       "    'name': 'Daniil Gavrilov',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-07T09:58:06.718Z',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-05T09:39:34.000Z',\n",
       "  'title': 'Analyze Feature Flow to Enhance Interpretation and Steering in Language  Models',\n",
       "  'summary': 'We introduce a new approach to systematically map features discovered by sparse autoencoder across consecutive layers of large language models, extending earlier work that examined inter-layer feature links. By using a data-free cosine similarity technique, we trace how specific features persist, transform, or first appear at each stage. This method yields granular flow graphs of feature evolution, enabling fine-grained interpretability and mechanistic insights into model computations. Crucially, we demonstrate how these cross-layer feature maps facilitate direct steering of model behavior by amplifying or suppressing chosen features, achieving targeted thematic control in text generation. Together, our findings highlight the utility of a causal, cross-layer interpretability framework that not only clarifies how features develop through forward passes but also provides new means for transparent manipulation of large language models.',\n",
       "  'upvotes': 53,\n",
       "  'discussionId': '67a59c4f7ffacd843a56408f'},\n",
       " {'id': '2502.04235',\n",
       "  'authors': [{'_id': '67a56af6d7c26c7497a86308',\n",
       "    'user': {'_id': '64b764bffdb702b3d8640610',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64b764bffdb702b3d8640610/lpHg0AX_NOmzw-ZxeOa1s.png',\n",
       "     'isPro': False,\n",
       "     'fullname': 'haoxintong',\n",
       "     'user': 'haoxintong',\n",
       "     'type': 'user'},\n",
       "    'name': 'Xintong Hao',\n",
       "    'status': 'extracted_confirmed',\n",
       "    'statusLastChangedAt': '2025-02-07T04:41:11.249Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a56af6d7c26c7497a86309',\n",
       "    'user': {'_id': '645604eebabbbbd3486dc615',\n",
       "     'avatarUrl': '/avatars/17a5ca8274e2bfc8f183a4af9878a930.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'shenke',\n",
       "     'user': 'shenke18',\n",
       "     'type': 'user'},\n",
       "    'name': 'Ke Shen',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-07T12:56:30.330Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a56af6d7c26c7497a8630a',\n",
       "    'name': 'Chenggang Li',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-06T17:19:55.000Z',\n",
       "  'title': 'MAGA: MAssive Genre-Audience Reformulation to Pretraining Corpus  Expansion',\n",
       "  'summary': \"Despite the remarkable capabilities of large language models across various tasks, their continued scaling faces a critical challenge: the scarcity of high-quality pretraining data. While model architectures continue to evolve, the natural language data struggles to scale up. To tackle this bottleneck, we propose MAssive Genre-Audience~(MAGA) reformulation method, which systematic synthesizes diverse, contextually-rich pretraining data from existing corpus. This work makes three main contributions: (1) We propose MAGA reformulation method, a lightweight and scalable approach for pretraining corpus expansion, and build a 770B tokens MAGACorpus. (2) We evaluate MAGACorpus with different data budget scaling strategies, demonstrating consistent improvements across various model sizes (134M-13B), establishing the necessity for next-generation large-scale synthetic pretraining language models. (3) Through comprehensive analysis, we investigate prompt engineering's impact on synthetic training collapse and reveal limitations in conventional collapse detection metrics using validation losses. Our work shows that MAGA can substantially expand training datasets while maintaining quality, offering a reliably pathway for scaling models beyond data limitations.\",\n",
       "  'upvotes': 16,\n",
       "  'discussionId': '67a56af8d7c26c7497a86359'},\n",
       " {'id': '2502.04328',\n",
       "  'authors': [{'_id': '67a586fad177de2eeba7de7b',\n",
       "    'user': {'_id': '64f001bfabd9fb1914398bd5',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64f001bfabd9fb1914398bd5/9teH82hkBI4csIz_WQh5q.jpeg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'liuzuyan',\n",
       "     'user': 'Zuyan',\n",
       "     'type': 'user'},\n",
       "    'name': 'Zuyan Liu',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-07T09:58:10.679Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a586fad177de2eeba7de7c',\n",
       "    'user': {'_id': '652965773a416e1f2173443b',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Yuhao Dong',\n",
       "     'user': 'THUdyh',\n",
       "     'type': 'user'},\n",
       "    'name': 'Yuhao Dong',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:37:45.556Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a586fad177de2eeba7de7d', 'name': 'Jiahui Wang', 'hidden': False},\n",
       "   {'_id': '67a586fad177de2eeba7de7e',\n",
       "    'user': {'_id': '62ab1ac1d48b4d8b048a3473',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Ziwei Liu',\n",
       "     'user': 'liuziwei7',\n",
       "     'type': 'user'},\n",
       "    'name': 'Ziwei Liu',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:38:14.852Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a586fad177de2eeba7de7f',\n",
       "    'user': {'_id': '63673bb9d0ee6e2662be0ec1',\n",
       "     'avatarUrl': '/avatars/1b8976785d64bc4e3f7159ccdb7f06c5.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Qingqiao Hu',\n",
       "     'user': 'WinstonHu',\n",
       "     'type': 'user'},\n",
       "    'name': 'Winston Hu',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:38:23.468Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a586fad177de2eeba7de80',\n",
       "    'user': {'_id': '66c44203ea476bea05e9fcd7',\n",
       "     'avatarUrl': '/avatars/b061eebec609446e669f5ad6365959f9.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'lu',\n",
       "     'user': 'jiwenlu',\n",
       "     'type': 'user'},\n",
       "    'name': 'Jiwen Lu',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:38:29.456Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a586fad177de2eeba7de81',\n",
       "    'user': {'_id': '63e4865354f51ea342d45d78',\n",
       "     'avatarUrl': '/avatars/2e7eccc878751331ca8b282f53e38899.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Yongming Rao',\n",
       "     'user': 'raoyongming',\n",
       "     'type': 'user'},\n",
       "    'name': 'Yongming Rao',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:38:35.766Z',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-06T18:59:55.000Z',\n",
       "  'title': 'Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive  Modality Alignment',\n",
       "  'summary': 'Recent advances in large language models, particularly following GPT-4o, have sparked increasing interest in developing omni-modal models capable of understanding more modalities. While some open-source alternatives have emerged, there is still a notable lag behind specialized single-modality models in performance. In this paper, we present Ola, an Omni-modal language model that achieves competitive performance across image, video, and audio understanding compared to specialized counterparts. The core design of Ola lies in its progressive modality alignment strategy that extends the supporting modality of the language model progressively. Our training pipeline begins with the most distinct modalities: image and text, then gradually expands the skill sets of the model using speech data that connects language and audio knowledge, and video data that connects all modalities. The progressive learning pipeline also enables us to maintain a relatively small size of the cross-modal alignment data, making developing omni-modal from existing vision-language models easy and less costly. Moreover, to unlock an advanced interactive experience like GPT-4o, we further design a sentence-wise decoding solution for streaming speech generation. Extensive experiments demonstrate that Ola surpasses existing open omni-modal LLMs across all modalities while achieving highly competitive performance compared to state-of-the-art specialized models of similar sizes. We aim to make Ola a fully open omni-modal understanding solution to advance future research in this emerging field. Model weights, code, and data are open-sourced at https://github.com/Ola-Omni/Ola.',\n",
       "  'upvotes': 19,\n",
       "  'discussionId': '67a586fbd177de2eeba7deae'},\n",
       " {'id': '2502.03621',\n",
       "  'authors': [{'_id': '67a59e5298f41a0460ee5282',\n",
       "    'user': {'_id': '6301d8324ccccaa23d3864f4',\n",
       "     'avatarUrl': '/avatars/148b1b1d1460e26f03a1f2ce0feacf78.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Danah Yatim',\n",
       "     'user': 'DanahY',\n",
       "     'type': 'user'},\n",
       "    'name': 'Danah Yatim',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:37:14.464Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a59e5298f41a0460ee5283',\n",
       "    'user': {'_id': '62627f3c02cd5952e013c843',\n",
       "     'avatarUrl': '/avatars/1d76689d75d670630b6fa0307309c31f.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Rafail Fridman',\n",
       "     'user': 'RafailFridman',\n",
       "     'type': 'user'},\n",
       "    'name': 'Rafail Fridman',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:37:21.549Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a59e5298f41a0460ee5284',\n",
       "    'user': {'_id': '62e29044a133a252b5cf70b2',\n",
       "     'avatarUrl': '/avatars/6d09ddcba9bc47c309150a8d77815891.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Omer Bar-Tal',\n",
       "     'user': 'omerbartal',\n",
       "     'type': 'user'},\n",
       "    'name': 'Omer Bar-Tal',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:37:28.382Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a59e5298f41a0460ee5285',\n",
       "    'user': {'_id': '631cddec68f7da9ad24f6fc7',\n",
       "     'avatarUrl': '/avatars/7d4f1ce805e5889ca6594bd4a93f2583.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Tali Dekel',\n",
       "     'user': 'talidekel',\n",
       "     'type': 'user'},\n",
       "    'name': 'Tali Dekel',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:37:34.275Z',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-05T21:14:55.000Z',\n",
       "  'title': 'DynVFX: Augmenting Real Videos with Dynamic Content',\n",
       "  'summary': 'We present a method for augmenting real-world videos with newly generated dynamic content. Given an input video and a simple user-provided text instruction describing the desired content, our method synthesizes dynamic objects or complex scene effects that naturally interact with the existing scene over time. The position, appearance, and motion of the new content are seamlessly integrated into the original footage while accounting for camera motion, occlusions, and interactions with other dynamic objects in the scene, resulting in a cohesive and realistic output video. We achieve this via a zero-shot, training-free framework that harnesses a pre-trained text-to-video diffusion transformer to synthesize the new content and a pre-trained Vision Language Model to envision the augmented scene in detail. Specifically, we introduce a novel inference-based method that manipulates features within the attention mechanism, enabling accurate localization and seamless integration of the new content while preserving the integrity of the original scene. Our method is fully automated, requiring only a simple user instruction. We demonstrate its effectiveness on a wide range of edits applied to real-world videos, encompassing diverse objects and scenarios involving both camera and object motion.',\n",
       "  'upvotes': 25,\n",
       "  'discussionId': '67a59e5798f41a0460ee5389'},\n",
       " {'id': '2502.03639',\n",
       "  'authors': [{'_id': '67a59193f86e1b9d7ae7cd55',\n",
       "    'user': {'_id': '65a47b4d60cc6b04c9ebb0ff',\n",
       "     'avatarUrl': '/avatars/b35ae99eab95e95a327c30b6d3ad6c83.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Yunuo Chen',\n",
       "     'user': 'yunuoch',\n",
       "     'type': 'user'},\n",
       "    'name': 'Yunuo Chen',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:50:19.710Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a59193f86e1b9d7ae7cd56',\n",
       "    'user': {'_id': '63f54aa73aa49d8cb97b84bc',\n",
       "     'avatarUrl': '/avatars/c73c5870039611ab9162daad46a1ba20.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'junli cao',\n",
       "     'user': 'jlcao2',\n",
       "     'type': 'user'},\n",
       "    'name': 'Junli Cao',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:50:25.916Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a59193f86e1b9d7ae7cd57',\n",
       "    'user': {'_id': '66b01ee8e53bbad918362856',\n",
       "     'avatarUrl': '/avatars/293529589a91dd7a95909d66727db224.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Anil Kag',\n",
       "     'user': 'anilkagak2',\n",
       "     'type': 'user'},\n",
       "    'name': 'Anil Kag',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:50:32.227Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a59193f86e1b9d7ae7cd58',\n",
       "    'user': {'_id': '636c0c1a15cd58e915bb8139',\n",
       "     'avatarUrl': '/avatars/7c675ac6a7d303d3425e498c4e939eb0.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Vidit Goel',\n",
       "     'user': 'vidit98',\n",
       "     'type': 'user'},\n",
       "    'name': 'Vidit Goel',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:50:37.829Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a59193f86e1b9d7ae7cd59',\n",
       "    'name': 'Sergei Korolev',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a59193f86e1b9d7ae7cd5a',\n",
       "    'user': {'_id': '655683727be68c0961673f45',\n",
       "     'avatarUrl': '/avatars/cddca36c041fa04860a4d42c0feaa07f.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Chenfanfu Jiang',\n",
       "     'user': 'cffjiang',\n",
       "     'type': 'user'},\n",
       "    'name': 'Chenfanfu Jiang',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:50:50.619Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a59193f86e1b9d7ae7cd5b',\n",
       "    'name': 'Sergey Tulyakov',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a59193f86e1b9d7ae7cd5c', 'name': 'Jian Ren', 'hidden': False}],\n",
       "  'publishedAt': '2025-02-05T21:49:06.000Z',\n",
       "  'title': 'Towards Physical Understanding in Video Generation: A 3D Point  Regularization Approach',\n",
       "  'summary': 'We present a novel video generation framework that integrates 3-dimensional geometry and dynamic awareness. To achieve this, we augment 2D videos with 3D point trajectories and align them in pixel space. The resulting 3D-aware video dataset, PointVid, is then used to fine-tune a latent diffusion model, enabling it to track 2D objects with 3D Cartesian coordinates. Building on this, we regularize the shape and motion of objects in the video to eliminate undesired artifacts, eg, nonphysical deformation. Consequently, we enhance the quality of generated RGB videos and alleviate common issues like object morphing, which are prevalent in current video models due to a lack of shape awareness. With our 3D augmentation and regularization, our model is capable of handling contact-rich scenarios such as task-oriented videos. These videos involve complex interactions of solids, where 3D information is essential for perceiving deformation and contact. Furthermore, our model improves the overall quality of video generation by promoting the 3D consistency of moving objects and reducing abrupt changes in shape and motion.',\n",
       "  'upvotes': 7,\n",
       "  'discussionId': '67a59195f86e1b9d7ae7cd97'},\n",
       " {'id': '2502.04299',\n",
       "  'authors': [{'_id': '67a591234020a3bfdb8cb2e5',\n",
       "    'user': {'_id': '64770e86d7cf39f2e937ae9a',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/64770e86d7cf39f2e937ae9a/pLqGg2z1KzQxCGpMwds-9.jpeg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Jinbo Xing',\n",
       "     'user': 'Doubiiu',\n",
       "     'type': 'user'},\n",
       "    'name': 'Jinbo Xing',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:43:19.911Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a591234020a3bfdb8cb2e6', 'name': 'Long Mai', 'hidden': False},\n",
       "   {'_id': '67a591234020a3bfdb8cb2e7',\n",
       "    'user': {'_id': '6372fab1bd1595ae66a62543',\n",
       "     'avatarUrl': '/avatars/783bdae07b2663eebeea4c7919a87c91.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Cusuh Ham',\n",
       "     'user': 'cusuh',\n",
       "     'type': 'user'},\n",
       "    'name': 'Cusuh Ham',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:43:42.324Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a591234020a3bfdb8cb2e8',\n",
       "    'user': {'_id': '644a717e75fce8ebef4e4955',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/zLga4NZBohFPlv50dcAo9.png',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Jiahui Huang',\n",
       "     'user': 'heiwang1997',\n",
       "     'type': 'user'},\n",
       "    'name': 'Jiahui Huang',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:44:01.034Z',\n",
       "    'hidden': True},\n",
       "   {'_id': '67a591234020a3bfdb8cb2e9',\n",
       "    'user': {'_id': '633bd831d5935998f74c4156',\n",
       "     'avatarUrl': '/avatars/feb4976ad10dd678ccad2652acf8a611.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Aniruddha Mahapatra',\n",
       "     'user': 'aniruddha26398',\n",
       "     'type': 'user'},\n",
       "    'name': 'Aniruddha Mahapatra',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:44:08.344Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a591234020a3bfdb8cb2ea', 'name': 'Chi-Wing Fu', 'hidden': False},\n",
       "   {'_id': '67a591234020a3bfdb8cb2eb',\n",
       "    'user': {'_id': '65574f0fc4865c852d5eec15',\n",
       "     'avatarUrl': '/avatars/1e03db4f2de4959dee620c577fbbb063.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Tien-Tsin Wong',\n",
       "     'user': 'ttwong',\n",
       "     'type': 'user'},\n",
       "    'name': 'Tien-Tsin Wong',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:44:26.835Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a591234020a3bfdb8cb2ec', 'name': 'Feng Liu', 'hidden': False}],\n",
       "  'publishedAt': '2025-02-06T18:41:04.000Z',\n",
       "  'title': 'MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video  Generation',\n",
       "  'summary': 'This paper presents a method that allows users to design cinematic video shots in the context of image-to-video generation. Shot design, a critical aspect of filmmaking, involves meticulously planning both camera movements and object motions in a scene. However, enabling intuitive shot design in modern image-to-video generation systems presents two main challenges: first, effectively capturing user intentions on the motion design, where both camera movements and scene-space object motions must be specified jointly; and second, representing motion information that can be effectively utilized by a video diffusion model to synthesize the image animations. To address these challenges, we introduce MotionCanvas, a method that integrates user-driven controls into image-to-video (I2V) generation models, allowing users to control both object and camera motions in a scene-aware manner. By connecting insights from classical computer graphics and contemporary video generation techniques, we demonstrate the ability to achieve 3D-aware motion control in I2V synthesis without requiring costly 3D-related training data. MotionCanvas enables users to intuitively depict scene-space motion intentions, and translates them into spatiotemporal motion-conditioning signals for video diffusion models. We demonstrate the effectiveness of our method on a wide range of real-world image content and shot-design scenarios, highlighting its potential to enhance the creative workflows in digital content creation and adapt to various image and video editing applications.',\n",
       "  'upvotes': 12,\n",
       "  'discussionId': '67a5912b4020a3bfdb8cb4d5'},\n",
       " {'id': '2502.02358',\n",
       "  'authors': [{'_id': '67a43546f6caedc30f9d8c71',\n",
       "    'user': {'_id': '659faf1d874e583fed79d09b',\n",
       "     'avatarUrl': '/avatars/178a18686426908b9496ce71f6550655.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Ziyan Guo',\n",
       "     'user': 'ZiyanGuo',\n",
       "     'type': 'user'},\n",
       "    'name': 'Ziyan Guo',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-06T14:15:01.599Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a43546f6caedc30f9d8c72',\n",
       "    'user': {'_id': '65fbc3c6f52ac1107f5b1677',\n",
       "     'avatarUrl': '/avatars/b8373c039c3d978510b89d057bd9b5e8.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Zeyu Hu',\n",
       "     'user': 'zeyuhu',\n",
       "     'type': 'user'},\n",
       "    'name': 'Zeyu Hu',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:38:48.101Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a43546f6caedc30f9d8c73', 'name': 'Na Zhao', 'hidden': False},\n",
       "   {'_id': '67a43546f6caedc30f9d8c74', 'name': 'De Wen Soh', 'hidden': False}],\n",
       "  'publishedAt': '2025-02-04T14:43:26.000Z',\n",
       "  'title': 'MotionLab: Unified Human Motion Generation and Editing via the  Motion-Condition-Motion Paradigm',\n",
       "  'summary': 'Human motion generation and editing are key components of computer graphics and vision. However, current approaches in this field tend to offer isolated solutions tailored to specific tasks, which can be inefficient and impractical for real-world applications. While some efforts have aimed to unify motion-related tasks, these methods simply use different modalities as conditions to guide motion generation. Consequently, they lack editing capabilities, fine-grained control, and fail to facilitate knowledge sharing across tasks. To address these limitations and provide a versatile, unified framework capable of handling both human motion generation and editing, we introduce a novel paradigm: Motion-Condition-Motion, which enables the unified formulation of diverse tasks with three concepts: source motion, condition, and target motion. Based on this paradigm, we propose a unified framework, MotionLab, which incorporates rectified flows to learn the mapping from source motion to target motion, guided by the specified conditions. In MotionLab, we introduce the 1) MotionFlow Transformer to enhance conditional generation and editing without task-specific modules; 2) Aligned Rotational Position Encoding} to guarantee the time synchronization between source motion and target motion; 3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for effective multi-task learning and knowledge sharing across tasks. Notably, our MotionLab demonstrates promising generalization capabilities and inference efficiency across multiple benchmarks for human motion. Our code and additional video results are available at: https://diouo.github.io/motionlab.github.io/.',\n",
       "  'upvotes': 14,\n",
       "  'discussionId': '67a43547f6caedc30f9d8c9b'},\n",
       " {'id': '2502.03544',\n",
       "  'authors': [{'_id': '67a589ebb16fabcdd2dea1eb',\n",
       "    'name': 'Yuri Chervonyi',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a589ebb16fabcdd2dea1ec',\n",
       "    'name': 'Trieu H. Trinh',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a589ebb16fabcdd2dea1ed',\n",
       "    'name': 'Miroslav Olšák',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a589ebb16fabcdd2dea1ee',\n",
       "    'name': 'Xiaomeng Yang',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a589ebb16fabcdd2dea1ef',\n",
       "    'name': 'Hoang Nguyen',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a589ebb16fabcdd2dea1f0',\n",
       "    'user': {'_id': '60cc0c3494ab6115ab6ecf12',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1623985197562-noauth.jpeg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Marcelo Menegali',\n",
       "     'user': 'mmenegali',\n",
       "     'type': 'user'},\n",
       "    'name': 'Marcelo Menegali',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:41:38.940Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a589ebb16fabcdd2dea1f1',\n",
       "    'name': 'Junehyuk Jung',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a589ebb16fabcdd2dea1f2', 'name': 'Vikas Verma', 'hidden': False},\n",
       "   {'_id': '67a589ebb16fabcdd2dea1f3', 'name': 'Quoc V. Le', 'hidden': False},\n",
       "   {'_id': '67a589ebb16fabcdd2dea1f4',\n",
       "    'user': {'_id': '65ee0b97306927c125d65779',\n",
       "     'avatarUrl': '/avatars/637129308a95efdf8faac9fb81a66589.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Thang Luong',\n",
       "     'user': 'lmthang',\n",
       "     'type': 'user'},\n",
       "    'name': 'Thang Luong',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:42:08.011Z',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-05T19:02:03.000Z',\n",
       "  'title': 'Gold-medalist Performance in Solving Olympiad Geometry with  AlphaGeometry2',\n",
       "  'summary': 'We present AlphaGeometry2, a significantly improved version of AlphaGeometry introduced in Trinh et al. (2024), which has now surpassed an average gold medalist in solving Olympiad geometry problems. To achieve this, we first extend the original AlphaGeometry language to tackle harder problems involving movements of objects, and problems containing linear equations of angles, ratios, and distances. This, together with other additions, has markedly improved the coverage rate of the AlphaGeometry language on International Math Olympiads (IMO) 2000-2024 geometry problems from 66% to 88%. The search process of AlphaGeometry2 has also been greatly improved through the use of Gemini architecture for better language modeling, and a novel knowledge-sharing mechanism that combines multiple search trees. Together with further enhancements to the symbolic engine and synthetic data generation, we have significantly boosted the overall solving rate of AlphaGeometry2 to 84% for all geometry problems over the last 25 years, compared to 54% previously. AlphaGeometry2 was also part of the system that achieved silver-medal standard at IMO 2024 https://dpmd.ai/imo-silver. Last but not least, we report progress towards using AlphaGeometry2 as a part of a fully automated system that reliably solves geometry problems directly from natural language input.',\n",
       "  'upvotes': 34,\n",
       "  'discussionId': '67a589ecb16fabcdd2dea259'},\n",
       " {'id': '2502.04128',\n",
       "  'authors': [{'_id': '67a5894db16fabcdd2de5459',\n",
       "    'user': {'_id': '645f172d7c6bff8577353d1a',\n",
       "     'avatarUrl': '/avatars/a83682e1343809257b082b78d58c582a.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'ZhenYE',\n",
       "     'user': 'ZhenYe234',\n",
       "     'type': 'user'},\n",
       "    'name': 'Zhen Ye',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-07T09:58:08.787Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5894db16fabcdd2de545a', 'name': 'Xinfa Zhu', 'hidden': False},\n",
       "   {'_id': '67a5894db16fabcdd2de545b',\n",
       "    'name': 'Chi-Min Chan',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5894db16fabcdd2de545c',\n",
       "    'name': 'Xinsheng Wang',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5894db16fabcdd2de545d', 'name': 'Xu Tan', 'hidden': False},\n",
       "   {'_id': '67a5894db16fabcdd2de545e', 'name': 'Jiahe Lei', 'hidden': False},\n",
       "   {'_id': '67a5894db16fabcdd2de545f', 'name': 'Yi Peng', 'hidden': False},\n",
       "   {'_id': '67a5894db16fabcdd2de5460', 'name': 'Haohe Liu', 'hidden': False},\n",
       "   {'_id': '67a5894db16fabcdd2de5461', 'name': 'Yizhu Jin', 'hidden': False},\n",
       "   {'_id': '67a5894db16fabcdd2de5462', 'name': 'Zheqi DAI', 'hidden': False},\n",
       "   {'_id': '67a5894db16fabcdd2de5463',\n",
       "    'name': 'Hongzhan Lin',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5894db16fabcdd2de5464', 'name': 'Jianyi Chen', 'hidden': False},\n",
       "   {'_id': '67a5894db16fabcdd2de5465', 'name': 'Xingjian Du', 'hidden': False},\n",
       "   {'_id': '67a5894db16fabcdd2de5466', 'name': 'Liumeng Xue', 'hidden': False},\n",
       "   {'_id': '67a5894db16fabcdd2de5467', 'name': 'Yunlin Chen', 'hidden': False},\n",
       "   {'_id': '67a5894db16fabcdd2de5468', 'name': 'Zhifei Li', 'hidden': False},\n",
       "   {'_id': '67a5894db16fabcdd2de5469', 'name': 'Lei Xie', 'hidden': False},\n",
       "   {'_id': '67a5894db16fabcdd2de546a',\n",
       "    'name': 'Qiuqiang Kong',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5894db16fabcdd2de546b', 'name': 'Yike Guo', 'hidden': False},\n",
       "   {'_id': '67a5894db16fabcdd2de546c',\n",
       "    'user': {'_id': '6628adb14277eae0da5eee28',\n",
       "     'avatarUrl': '/avatars/6cb41b80cc5e014e455dfc2a22682e64.svg',\n",
       "     'isPro': True,\n",
       "     'fullname': 'HKUST Audio',\n",
       "     'user': 'HKUST-Audio',\n",
       "     'type': 'user'},\n",
       "    'name': 'Wei Xue',\n",
       "    'status': 'extracted_pending',\n",
       "    'statusLastChangedAt': '2025-02-07T04:17:17.888Z',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-06T15:04:00.000Z',\n",
       "  'title': 'Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based  Speech Synthesis',\n",
       "  'summary': 'Recent advances in text-based large language models (LLMs), particularly in the GPT series and the o1 model, have demonstrated the effectiveness of scaling both training-time and inference-time compute. However, current state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring separate models (e.g., diffusion models after LLM), complicating the decision of whether to scale a particular model during training or testing. This work makes the following contributions: First, we explore the scaling of train-time and inference-time compute for speech synthesis. Second, we propose a simple framework Llasa for speech synthesis that employs a single-layer vector quantizer (VQ) codec and a single Transformer architecture to fully align with standard LLMs such as Llama. Our experiments reveal that scaling train-time compute for Llasa consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns. Furthermore, from the perspective of scaling inference-time compute, we employ speech understanding models as verifiers during the search, finding that scaling inference-time compute shifts the sampling modes toward the preferences of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy. In addition, we released the checkpoint and training code for our TTS model (1B, 3B, 8B) and codec model publicly available.',\n",
       "  'upvotes': 15,\n",
       "  'discussionId': '67a5894db16fabcdd2de54d3'},\n",
       " {'id': '2502.04270',\n",
       "  'authors': [{'_id': '67a5882fa8e877ef10b8d1fd',\n",
       "    'user': {'_id': '664187fa1cd689758847f44b',\n",
       "     'avatarUrl': '/avatars/501ed1d5bcffd7466fd8b8c8d3b758f0.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Yunzhen Feng',\n",
       "     'user': 'Coolfyz',\n",
       "     'type': 'user'},\n",
       "    'name': 'Yunzhen Feng',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:49:26.187Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5882fa8e877ef10b8d1fe',\n",
       "    'user': {'_id': '625de0717341c641426e7932',\n",
       "     'avatarUrl': '/avatars/9deb06fc565a80002c3ae75c6f4cd9e7.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Ariel Kwiatkowski',\n",
       "     'user': 'RedTachyon',\n",
       "     'type': 'user'},\n",
       "    'name': 'Ariel Kwiatkowski',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:49:33.321Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5882fa8e877ef10b8d1ff',\n",
       "    'user': {'_id': '6424123d3fa01ecba6fd94e8',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/B-1YSkTJMVBBQDX3WVxIL.jpeg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Kunhao Zheng',\n",
       "     'user': 'Kunhao',\n",
       "     'type': 'user'},\n",
       "    'name': 'Kunhao Zheng',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:49:41.052Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5882fa8e877ef10b8d200',\n",
       "    'user': {'_id': '65ce30e06da01df536eded5a',\n",
       "     'avatarUrl': '/avatars/04c32cba7a3bbaf9ea5dee88c96cf87b.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Julia Kempe',\n",
       "     'user': 'Knykny',\n",
       "     'type': 'user'},\n",
       "    'name': 'Julia Kempe',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:49:48.009Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5882fa8e877ef10b8d201',\n",
       "    'user': {'_id': '66494b428d50b4b0efceab9c',\n",
       "     'avatarUrl': '/avatars/ac7293aafaf15759d53cf62f4e1ae874.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Yaqi Duan',\n",
       "     'user': 'duanyq',\n",
       "     'type': 'user'},\n",
       "    'name': 'Yaqi Duan',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:49:54.220Z',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-06T18:09:00.000Z',\n",
       "  'title': 'PILAF: Optimal Human Preference Sampling for Reward Modeling',\n",
       "  'summary': 'As large language models increasingly drive real-world applications, aligning them with human values becomes paramount. Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique, translating preference data into reward models when oracle human values remain inaccessible. In practice, RLHF mostly relies on approximate reward models, which may not consistently guide the policy toward maximizing the underlying human values. We propose Policy-Interpolated Learning for Aligned Feedback (PILAF), a novel response sampling strategy for preference labeling that explicitly aligns preference learning with maximizing the underlying oracle reward. PILAF is theoretically grounded, demonstrating optimality from both an optimization and a statistical perspective. The method is straightforward to implement and demonstrates strong performance in iterative and online RLHF settings where feedback curation is critical.',\n",
       "  'upvotes': 8,\n",
       "  'discussionId': '67a58830a8e877ef10b8d226'},\n",
       " {'id': '2502.03860',\n",
       "  'authors': [{'_id': '67a5880c886a1e223b1d57ec',\n",
       "    'user': {'_id': '63e08acbf351dc0745749d56',\n",
       "     'avatarUrl': '/avatars/8e2d5ce9db5bd8008ac2ad80f6025553.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Bo Pang',\n",
       "     'user': 'bpucla',\n",
       "     'type': 'user'},\n",
       "    'name': 'Bo Pang',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:46:17.484Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5880c886a1e223b1d57ed',\n",
       "    'user': {'_id': '63a3ff69f91ad3ea5703841d',\n",
       "     'avatarUrl': '/avatars/69227c4bce01d33747c1377b6f9672db.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Hanze Dong',\n",
       "     'user': 'hendrydong',\n",
       "     'type': 'user'},\n",
       "    'name': 'Hanze Dong',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:46:02.107Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5880c886a1e223b1d57ee',\n",
       "    'user': {'_id': '631983d5cb116eab31df5821',\n",
       "     'avatarUrl': '/avatars/6a42c842a9439241ead2ace1d79fc32c.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Jiacheng Xu',\n",
       "     'user': 'jcxu',\n",
       "     'type': 'user'},\n",
       "    'name': 'Jiacheng Xu',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:45:54.892Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5880c886a1e223b1d57ef',\n",
       "    'name': 'Silvio Savarese',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5880c886a1e223b1d57f0',\n",
       "    'user': {'_id': '649bc93758d8b19de0c7785f',\n",
       "     'avatarUrl': '/avatars/3ed9473aee23d99f4ee949d3705089ea.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Yingbo Zhou',\n",
       "     'user': 'yingbozhou',\n",
       "     'type': 'user'},\n",
       "    'name': 'Yingbo Zhou',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:45:09.887Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a5880c886a1e223b1d57f1',\n",
       "    'user': {'_id': '649dbcc4e0fff1ed099dc80a',\n",
       "     'avatarUrl': '/avatars/c87c273ca628dbcddccbf1ee19b2ce33.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Caiming Xiong',\n",
       "     'user': 'cxiong',\n",
       "     'type': 'user'},\n",
       "    'name': 'Caiming Xiong',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:45:02.543Z',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-06T08:19:59.000Z',\n",
       "  'title': 'BOLT: Bootstrap Long Chain-of-Thought in Language Models without  Distillation',\n",
       "  'summary': \"Large language models (LLMs), such as o1 from OpenAI, have demonstrated remarkable reasoning capabilities. o1 generates a long chain-of-thought (LongCoT) before answering a question. LongCoT allows LLMs to analyze problems, devise plans, reflect, and backtrack effectively. These actions empower LLM to solve complex problems. After the release of o1, many teams have attempted to replicate its LongCoT and reasoning capabilities. In terms of methods, they primarily rely on knowledge distillation with data from existing models with LongCoT capacities (e.g., OpenAI-o1, Qwen-QwQ, DeepSeek-R1-Preview), leaving significant uncertainties on systematically developing such reasoning abilities. In terms of data domains, these works focus narrowly on math while a few others include coding, limiting their generalizability. This paper introduces a novel approach to enable LLM's LongCoT capacity without distillation from o1-like models or expensive human annotations, where we bootstrap LongCoT (BOLT) from a standard instruct model. BOLT involves three stages: 1) LongCoT data bootstrapping with in-context learning on a standard instruct model; 2) LongCoT supervised finetuning; 3) online training to further refine LongCoT capacities. In BOLT, only a few in-context examples need to be constructed during the bootstrapping stage; in our experiments, we created 10 examples, demonstrating the feasibility of this approach. We use Llama-3.1-70B-Instruct to bootstrap LongCoT and apply our method to various model scales (7B, 8B, 70B). We achieve impressive performance on a variety of benchmarks, Arena-Hard, MT-Bench, WildBench, ZebraLogic, MATH500, which evaluate diverse task-solving and reasoning capabilities.\",\n",
       "  'upvotes': 16,\n",
       "  'discussionId': '67a5880e886a1e223b1d58ca'},\n",
       " {'id': '2502.04306',\n",
       "  'authors': [{'_id': '67a57f334e50b2956b13f4e0',\n",
       "    'user': {'_id': '6730dc8df84c8aac97451e57',\n",
       "     'avatarUrl': '/avatars/4f2cf5363b17744daca41d2a18ddfeb8.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Yinjie Wang',\n",
       "     'user': 'yinjiewang',\n",
       "     'type': 'user'},\n",
       "    'name': 'Yinjie Wang',\n",
       "    'status': 'extracted_pending',\n",
       "    'statusLastChangedAt': '2025-02-07T03:34:13.176Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a57f334e50b2956b13f4e1', 'name': 'Ling Yang', 'hidden': False},\n",
       "   {'_id': '67a57f334e50b2956b13f4e2',\n",
       "    'user': {'_id': '6338790e76421c054310c96b',\n",
       "     'avatarUrl': '/avatars/112e3d88d155bc998a89fef6f33af64d.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Guohao Li',\n",
       "     'user': 'lightaime',\n",
       "     'type': 'user'},\n",
       "    'name': 'Guohao Li',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:43:05.909Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a57f334e50b2956b13f4e3',\n",
       "    'user': {'_id': '6599415e8c8ac79295e0b5e3',\n",
       "     'avatarUrl': '/avatars/85500bc8d2cd51444adcc19b1f8db313.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Mengdi Wang',\n",
       "     'user': 'Edify-Kd2024',\n",
       "     'type': 'user'},\n",
       "    'name': 'Mengdi Wang',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:42:45.269Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a57f334e50b2956b13f4e4',\n",
       "    'name': 'Bryon Aragam',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-06T18:47:49.000Z',\n",
       "  'title': 'ScoreFlow: Mastering LLM Agent Workflows via Score-based Preference  Optimization',\n",
       "  'summary': 'Recent research has leveraged large language model multi-agent systems for complex problem-solving while trying to reduce the manual effort required to build them, driving the development of automated agent workflow optimization methods. However, existing methods remain inflexible due to representational limitations, a lack of adaptability, and poor scalability when relying on discrete optimization techniques. We address these challenges with ScoreFlow, a simple yet high-performance framework that leverages efficient gradient-based optimization in a continuous space. ScoreFlow incorporates Score-DPO, a novel variant of the direct preference optimization method that accounts for quantitative feedback. Across six benchmarks spanning question answering, coding, and mathematical reasoning, ScoreFlow achieves an 8.2% improvement over existing baselines. Moreover, it empowers smaller models to outperform larger ones with lower inference costs. Project: https://github.com/Gen-Verse/ScoreFlow',\n",
       "  'upvotes': 15,\n",
       "  'discussionId': '67a57f354e50b2956b13f53d'},\n",
       " {'id': '2502.04153',\n",
       "  'authors': [{'_id': '67a57b1fdea89ffe80d9fe56',\n",
       "    'user': {'_id': '66c89152d33e34fbc29497d7',\n",
       "     'avatarUrl': '/avatars/bbddabf6532393951c4759e5915a065b.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'KaikaiAn',\n",
       "     'user': 'kkk-an',\n",
       "     'type': 'user'},\n",
       "    'name': 'Kaikai An',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-07T09:58:18.320Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a57b1fdea89ffe80d9fe57', 'name': 'Li Sheng', 'hidden': False},\n",
       "   {'_id': '67a57b1fdea89ffe80d9fe58',\n",
       "    'user': {'_id': '650eba9555dc1e841746f132',\n",
       "     'avatarUrl': '/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Ganqu Cui',\n",
       "     'user': 'ganqu',\n",
       "     'type': 'user'},\n",
       "    'name': 'Ganqu Cui',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:31:05.333Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a57b1fdea89ffe80d9fe59',\n",
       "    'user': {'_id': '637c99bbfe115289cfedfb44',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/637c99bbfe115289cfedfb44/344NN9KKF_XXTlVYaGaMW.png',\n",
       "     'isPro': False,\n",
       "     'fullname': 'ssz',\n",
       "     'user': 'ssz1111',\n",
       "     'type': 'user'},\n",
       "    'name': 'Shuzheng Si',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-07T09:58:16.229Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a57b1fdea89ffe80d9fe5a', 'name': 'Ning Ding', 'hidden': False},\n",
       "   {'_id': '67a57b1fdea89ffe80d9fe5b', 'name': 'Yu Cheng', 'hidden': False},\n",
       "   {'_id': '67a57b1fdea89ffe80d9fe5c',\n",
       "    'name': 'Baobao Chang',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-06T15:39:16.000Z',\n",
       "  'title': 'UltraIF: Advancing Instruction Following from the Wild',\n",
       "  'summary': 'Instruction-following made modern large language models (LLMs) helpful assistants. However, the key to taming LLMs on complex instructions remains mysterious, for that there are huge gaps between models trained by open-source community and those trained by leading companies. To bridge the gap, we propose a simple and scalable approach UltraIF for building LLMs that can follow complex instructions with open-source data. UltraIF first decomposes real-world user prompts into simpler queries, constraints, and corresponding evaluation questions for the constraints. Then, we train an UltraComposer to compose constraint-associated prompts with evaluation questions. This prompt composer allows us to synthesize complicated instructions as well as filter responses with evaluation questions. In our experiment, for the first time, we successfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5 instruction-following benchmarks without any benchmark information, using only 8B model as response generator and evaluator. The aligned model also achieved competitive scores on other benchmarks. Moreover, we also show that UltraIF could further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating broader use cases for the method. Our code will be available at https://github.com/kkk-an/UltraIF.',\n",
       "  'upvotes': 19,\n",
       "  'discussionId': '67a57b1fdea89ffe80d9fe93'},\n",
       " {'id': '2502.04295',\n",
       "  'authors': [{'_id': '67a57d32bc587f5b57a3f24f',\n",
       "    'name': 'Yuanye Liu',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a57d32bc587f5b57a3f250',\n",
       "    'user': {'_id': '62abdf657b037eafffc48808',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1655430982462-noauth.jpeg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Jiahang Xu',\n",
       "     'user': 'Jiahang',\n",
       "     'type': 'user'},\n",
       "    'name': 'Jiahang Xu',\n",
       "    'status': 'extracted_pending',\n",
       "    'statusLastChangedAt': '2025-02-07T03:25:39.760Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a57d32bc587f5b57a3f251',\n",
       "    'user': {'_id': '62b0009c72043b05d29492b2',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/62b0009c72043b05d29492b2/NqRkX2YLhlfOLvYysa7dD.png',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Li Lyna Zhang',\n",
       "     'user': 'lynazhang',\n",
       "     'type': 'user'},\n",
       "    'name': 'Li Lyna Zhang',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:47:55.350Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a57d32bc587f5b57a3f252', 'name': 'Qi Chen', 'hidden': False},\n",
       "   {'_id': '67a57d32bc587f5b57a3f253', 'name': 'Xuan Feng', 'hidden': False},\n",
       "   {'_id': '67a57d32bc587f5b57a3f254', 'name': 'Yang Chen', 'hidden': False},\n",
       "   {'_id': '67a57d32bc587f5b57a3f255',\n",
       "    'name': 'Zhongxin Guo',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a57d32bc587f5b57a3f256', 'name': 'Yuqing Yang', 'hidden': False},\n",
       "   {'_id': '67a57d32bc587f5b57a3f257', 'name': 'Cheng Peng', 'hidden': False}],\n",
       "  'publishedAt': '2025-02-06T18:36:44.000Z',\n",
       "  'title': 'Beyond Prompt Content: Enhancing LLM Performance via Content-Format  Integrated Prompt Optimization',\n",
       "  'summary': 'Large Language Models (LLMs) have shown significant capability across various tasks, with their real-world effectiveness often driven by prompt design. While recent research has focused on optimizing prompt content, the role of prompt formatting, a critical but often overlooked dimension, has received limited systematic investigation. In this paper, we introduce Content-Format Integrated Prompt Optimization (CFPO), an innovative methodology that jointly optimizes both prompt content and formatting through an iterative refinement process. CFPO leverages natural language mutations to explore content variations and employs a dynamic format exploration strategy that systematically evaluates diverse format options. Our extensive evaluations across multiple tasks and open-source LLMs demonstrate that CFPO demonstrates measurable performance improvements compared to content-only optimization methods. This highlights the importance of integrated content-format optimization and offers a practical, model-agnostic approach to enhancing LLM performance. Code will be available at https://github.com/HenryLau7/CFPO.',\n",
       "  'upvotes': 7,\n",
       "  'discussionId': '67a57d33bc587f5b57a3f29d'},\n",
       " {'id': '2502.04296',\n",
       "  'authors': [{'_id': '67a57a4637e2abc28667ec1b',\n",
       "    'user': {'_id': '63151385b031f7b1c7c0871c',\n",
       "     'avatarUrl': '/avatars/0088eb929866face5f95218943e3f478.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Lirui Wang',\n",
       "     'user': 'liruiw',\n",
       "     'type': 'user'},\n",
       "    'name': 'Lirui Wang',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:51:11.182Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a57a4637e2abc28667ec1c', 'name': 'Kevin Zhao', 'hidden': False},\n",
       "   {'_id': '67a57a4637e2abc28667ec1d',\n",
       "    'user': {'_id': '6747a05a736eaadf2eec50ff',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/mF6_-m3GRm5OfG2HDNorC.jpeg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Chaoqi Liu',\n",
       "     'user': 'chaoqi-liu',\n",
       "     'type': 'user'},\n",
       "    'name': 'Chaoqi Liu',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:51:27.451Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a57a4637e2abc28667ec1e',\n",
       "    'user': {'_id': '63e58e3a006a775275e59e41',\n",
       "     'avatarUrl': '/avatars/75262a35b27a2ae1939df9118120d99e.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Xinlei Chen',\n",
       "     'user': 'endernewton',\n",
       "     'type': 'user'},\n",
       "    'name': 'Xinlei Chen',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-07T10:51:33.905Z',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-06T18:38:26.000Z',\n",
       "  'title': 'Learning Real-World Action-Video Dynamics with Heterogeneous Masked  Autoregression',\n",
       "  'summary': 'We propose Heterogeneous Masked Autoregression (HMA) for modeling action-video dynamics to generate high-quality data and evaluation in scaling robot learning. Building interactive video world models and policies for robotics is difficult due to the challenge of handling diverse settings while maintaining computational efficiency to run in real time. HMA uses heterogeneous pre-training from observations and action sequences across different robotic embodiments, domains, and tasks. HMA uses masked autoregression to generate quantized or soft tokens for video predictions. ourshort achieves better visual fidelity and controllability than the previous robotic video generation models with 15 times faster speed in the real world. After post-training, this model can be used as a video simulator from low-level action inputs for evaluating policies and generating synthetic data. See this link https://liruiw.github.io/hma for more information.',\n",
       "  'upvotes': 5,\n",
       "  'discussionId': '67a57a4737e2abc28667ec58'},\n",
       " {'id': '2502.00226',\n",
       "  'authors': [{'_id': '67a3d37e2d9a08978848c657',\n",
       "    'user': {'_id': '63eff09f4a788ed1dd863b09',\n",
       "     'avatarUrl': '/avatars/b557d83cf6d6b3b46dfbe9b7727ae16d.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Jun',\n",
       "     'user': 'oldteacherjoy',\n",
       "     'type': 'user'},\n",
       "    'name': 'Jun Xing',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-06T14:15:09.857Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a3d37e2d9a08978848c658',\n",
       "    'name': 'Mayur Bhatia',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a3d37e2d9a08978848c659',\n",
       "    'name': 'Sahil Phulwani',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a3d37e2d9a08978848c65a',\n",
       "    'name': 'Darshan Suresh',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a3d37e2d9a08978848c65b',\n",
       "    'name': 'Rafik Matta',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-01-31T23:47:02.000Z',\n",
       "  'title': 'HackerRank-ASTRA: Evaluating Correctness & Consistency of Large Language  Models on cross-domain multi-file project problems',\n",
       "  'summary': 'Evaluating the real-world applicability of large language models (LLMs) provides valuable insights for their development and use in software development tasks. Existing benchmarks often focus on standalone coding problems or specific libraries, overlooking multi-file, project-based scenarios and lacking a rigorous evaluation of consistency. The HackerRank-ASTRA Benchmark introduces project-based coding problems that mirror real-world scenarios. It evaluates model consistency through 32 runs (k = 32) and median standard deviation while incorporating taxonomy-level analysis to assess sub-skill capabilities. Initial evaluations on 65 problems show that the top three models -- o1, o1-preview, and Claude-3.5-Sonnet-1022 -- achieved comparable average scores of 75%, with no statistically significant differences in performance. Notably, Claude-3.5-Sonnet-1022 demonstrated the highest consistency across problems, with low variability (SD = 0.0497), which was statistically significant compared to other models, highlighting its reliability for real-world software development tasks.',\n",
       "  'upvotes': 0,\n",
       "  'discussionId': '67a3d37f2d9a08978848c6b0'},\n",
       " {'id': '2502.02421',\n",
       "  'authors': [{'_id': '67a4fc2450641c7d60cead58',\n",
       "    'user': {'_id': '64d516ba80d47a6b76fc1015',\n",
       "     'avatarUrl': '/avatars/e520825f2ac9ff047844496ae2dad7d6.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Amin Heyrani Nobari',\n",
       "     'user': 'ahn1376',\n",
       "     'type': 'user'},\n",
       "    'name': 'Amin Heyrani Nobari',\n",
       "    'status': 'extracted_pending',\n",
       "    'statusLastChangedAt': '2025-02-06T18:15:01.447Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a4fc2450641c7d60cead59',\n",
       "    'name': 'Kaveh Alimohammadi',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a4fc2450641c7d60cead5a',\n",
       "    'name': 'Ali ArjomandBigdeli',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a4fc2450641c7d60cead5b',\n",
       "    'name': 'Akash Srivastava',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a4fc2450641c7d60cead5c', 'name': 'Faez Ahmed', 'hidden': False},\n",
       "   {'_id': '67a4fc2450641c7d60cead5d',\n",
       "    'name': 'Navid Azizan',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-04T15:42:03.000Z',\n",
       "  'title': 'Activation-Informed Merging of Large Language Models',\n",
       "  'summary': 'Model merging, a method that combines the parameters and embeddings of multiple fine-tuned large language models (LLMs), offers a promising approach to enhance model performance across various tasks while maintaining computational efficiency. This paper introduces Activation-Informed Merging (AIM), a technique that integrates the information from the activation space of LLMs into the merging process to improve performance and robustness. AIM is designed as a flexible, complementary solution that is applicable to any existing merging method. It aims to preserve critical weights from the base model, drawing on principles from continual learning~(CL) and model compression. Utilizing a task-agnostic calibration set, AIM selectively prioritizes essential weights during merging. We empirically demonstrate that AIM significantly enhances the performance of merged models across multiple benchmarks. Our findings suggest that considering the activation-space information can provide substantial advancements in the model merging strategies for LLMs with up to 40% increase in benchmark performance.',\n",
       "  'upvotes': 3,\n",
       "  'discussionId': '67a4fc2550641c7d60ceada5'},\n",
       " {'id': '2502.00306',\n",
       "  'authors': [{'_id': '67a4d341784a1ad88b6110a0',\n",
       "    'name': 'Ali Naseh',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a4d341784a1ad88b6110a1',\n",
       "    'user': {'_id': '66c6763f2402eab42a0ed395',\n",
       "     'avatarUrl': '/avatars/a6f47924d3a705dd350327f9814eb77e.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Yuefeng Peng',\n",
       "     'user': 'yfp16443',\n",
       "     'type': 'user'},\n",
       "    'name': 'Yuefeng Peng',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-06T16:14:27.273Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a4d341784a1ad88b6110a2',\n",
       "    'user': {'_id': '647a1010ffe1b559f5418534',\n",
       "     'avatarUrl': '/avatars/fed1a8dbd1090d8f48dc6c2d321a6212.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Anshuman Suri',\n",
       "     'user': 'iamgroot42',\n",
       "     'type': 'user'},\n",
       "    'name': 'Anshuman Suri',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-06T16:06:34.501Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a4d341784a1ad88b6110a3',\n",
       "    'name': 'Harsh Chaudhari',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a4d341784a1ad88b6110a4', 'name': 'Alina Oprea', 'hidden': False},\n",
       "   {'_id': '67a4d341784a1ad88b6110a5',\n",
       "    'name': 'Amir Houmansadr',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-01T04:01:18.000Z',\n",
       "  'title': 'Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented  Generation',\n",
       "  'summary': \"Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to generate grounded responses by leveraging external knowledge databases without altering model parameters. Although the absence of weight tuning prevents leakage via model parameters, it introduces the risk of inference adversaries exploiting retrieved documents in the model's context. Existing methods for membership inference and data extraction often rely on jailbreaking or carefully crafted unnatural queries, which can be easily detected or thwarted with query rewriting techniques common in RAG systems. In this work, we present Interrogation Attack (IA), a membership inference technique targeting documents in the RAG datastore. By crafting natural-text queries that are answerable only with the target document's presence, our approach demonstrates successful inference with just 30 queries while remaining stealthy; straightforward detectors identify adversarial prompts from existing methods up to ~76x more frequently than those generated by our attack. We observe a 2x improvement in TPR@1%FPR over prior inference attacks across diverse RAG configurations, all while costing less than $0.02 per document inference.\",\n",
       "  'upvotes': 3,\n",
       "  'discussionId': '67a4d342784a1ad88b6110d9'},\n",
       " {'id': '2502.02928',\n",
       "  'authors': [{'_id': '67a4213c54bfb820ffb26f4a',\n",
       "    'user': {'_id': '65eef9ce7443c09267513796',\n",
       "     'avatarUrl': '/avatars/62547f99130557f54093b2ff4d6c9c24.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Muntasir Adnan',\n",
       "     'user': 'adnaan525',\n",
       "     'type': 'user'},\n",
       "    'name': 'Muntasir Adnan',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-06T14:15:07.849Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a4213c54bfb820ffb26f4b',\n",
       "    'user': {'_id': '659fa5d10183046e16b6f993',\n",
       "     'avatarUrl': '/avatars/744497ac023d9ebc21e0c297d4f15fca.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Zhiwei Xu',\n",
       "     'user': 'zhiwei555',\n",
       "     'type': 'user'},\n",
       "    'name': 'Zhiwei Xu',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-06T16:13:47.448Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a4213c54bfb820ffb26f4c',\n",
       "    'user': {'_id': '6510df53469c325dc4dc69a5',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NBydVZ-3cc8ORQpQeQCf_.png',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Carlos Kuhn',\n",
       "     'user': 'CarlosKuhn',\n",
       "     'type': 'user'},\n",
       "    'name': 'Carlos C. N. Kuhn',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-06T16:13:58.336Z',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-05T06:43:40.000Z',\n",
       "  'title': 'Large Language Model Guided Self-Debugging Code Generation',\n",
       "  'summary': 'Automated code generation is gaining significant importance in intelligent computer programming and system deployment. However, current approaches often face challenges in computational efficiency and lack robust mechanisms for code parsing and error correction. In this work, we propose a novel framework, PyCapsule, with a simple yet effective two-agent pipeline and efficient self-debugging modules for Python code generation. PyCapsule features sophisticated prompt inference, iterative error handling, and case testing, ensuring high generation stability, safety, and correctness. Empirically, PyCapsule achieves up to 5.7% improvement of success rate on HumanEval, 10.3% on HumanEval-ET, and 24.4% on BigCodeBench compared to the state-of-art methods. We also observe a decrease in normalized success rate given more self-debugging attempts, potentially affected by limited and noisy error feedback in retention. PyCapsule demonstrates broader impacts on advancing lightweight and efficient code generation for artificial intelligence systems.',\n",
       "  'upvotes': 8,\n",
       "  'discussionId': '67a4213d54bfb820ffb26f75'},\n",
       " {'id': '2502.02671',\n",
       "  'authors': [{'_id': '67a495ce0f2d0f0303a3af71',\n",
       "    'user': {'_id': '6262880c5eb4fa93219f0064',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/6262880c5eb4fa93219f0064/6yyBvRK4Oh7OhjaaweaVN.jpeg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Daniil Tiapkin',\n",
       "     'user': 'dtiapkin',\n",
       "     'type': 'user'},\n",
       "    'name': 'Daniil Tiapkin',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-06T14:14:19.738Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a495ce0f2d0f0303a3af72',\n",
       "    'name': 'Daniele Calandriello',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a495ce0f2d0f0303a3af73',\n",
       "    'user': {'_id': '65afb7dbdd6bdfd73cd8e609',\n",
       "     'avatarUrl': '/avatars/b21069bc2d7ee4cc1508008e3c8ade64.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Johan Ferret',\n",
       "     'user': 'ferretj',\n",
       "     'type': 'user'},\n",
       "    'name': 'Johan Ferret',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-06T16:12:15.183Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a495ce0f2d0f0303a3af74',\n",
       "    'user': {'_id': '66328157b270ae503e91339b',\n",
       "     'avatarUrl': '/avatars/ea7a52060f5360f523ca28e137e85e33.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Sarah Perrin',\n",
       "     'user': 'Sper42',\n",
       "     'type': 'user'},\n",
       "    'name': 'Sarah Perrin',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-06T16:12:39.678Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a495ce0f2d0f0303a3af75',\n",
       "    'name': 'Nino Vieillard',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a495ce0f2d0f0303a3af76',\n",
       "    'user': {'_id': '63c94ede00104ea998de19a6',\n",
       "     'avatarUrl': '/avatars/273959d87f0c67747588cf0700d64039.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Alexandre Rame',\n",
       "     'user': 'alexrame',\n",
       "     'type': 'user'},\n",
       "    'name': 'Alexandre Ramé',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-06T16:12:53.881Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a495ce0f2d0f0303a3af77',\n",
       "    'user': {'_id': '66d093b681e0683bca48bed6',\n",
       "     'avatarUrl': '/avatars/cc1fbccb0b6aa93d648bcbdf9c3a35e1.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Mathieu Blondel',\n",
       "     'user': 'mblondel',\n",
       "     'type': 'user'},\n",
       "    'name': 'Mathieu Blondel',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-06T16:13:01.896Z',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-04T19:26:28.000Z',\n",
       "  'title': 'On Teacher Hacking in Language Model Distillation',\n",
       "  'summary': \"Post-training of language models (LMs) increasingly relies on the following two stages: (i) knowledge distillation, where the LM is trained to imitate a larger teacher LM, and (ii) reinforcement learning from human feedback (RLHF), where the LM is aligned by optimizing a reward model. In the second RLHF stage, a well-known challenge is reward hacking, where the LM over-optimizes the reward model. Such phenomenon is in line with Goodhart's law and can lead to degraded performance on the true objective. In this paper, we investigate whether a similar phenomenon, that we call teacher hacking, can occur during knowledge distillation. This could arise because the teacher LM is itself an imperfect approximation of the true distribution. To study this, we propose a controlled experimental setup involving: (i) an oracle LM representing the ground-truth distribution, (ii) a teacher LM distilled from the oracle, and (iii) a student LM distilled from the teacher. Our experiments reveal the following insights. When using a fixed offline dataset for distillation, teacher hacking occurs; moreover, we can detect it by observing when the optimization process deviates from polynomial convergence laws. In contrast, employing online data generation techniques effectively mitigates teacher hacking. More precisely, we identify data diversity as the key factor in preventing hacking. Overall, our findings provide a deeper understanding of the benefits and limitations of distillation for building robust and efficient LMs.\",\n",
       "  'upvotes': 11,\n",
       "  'discussionId': '67a495d00f2d0f0303a3afde'},\n",
       " {'id': '2502.01154',\n",
       "  'authors': [{'_id': '67a4609af2e553c1d0da914d',\n",
       "    'name': 'Yu-Ling Hsu',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a4609af2e553c1d0da914e',\n",
       "    'user': {'_id': '608abf1272b50b02c4b02865',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1619708309549-608abf1272b50b02c4b02865.jpeg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Hsuan Su',\n",
       "     'user': 'jacksukk',\n",
       "     'type': 'user'},\n",
       "    'name': 'Hsuan Su',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-06T14:14:29.721Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a4609af2e553c1d0da914f',\n",
       "    'name': 'Shang-Tse Chen',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-03T08:44:24.000Z',\n",
       "  'title': 'Jailbreaking with Universal Multi-Prompts',\n",
       "  'summary': 'Large language models (LLMs) have seen rapid development in recent years, revolutionizing various applications and significantly enhancing convenience and productivity. However, alongside their impressive capabilities, ethical concerns and new types of attacks, such as jailbreaking, have emerged. While most prompting techniques focus on optimizing adversarial inputs for individual cases, resulting in higher computational costs when dealing with large datasets. Less research has addressed the more general setting of training a universal attacker that can transfer to unseen tasks. In this paper, we introduce JUMP, a prompt-based method designed to jailbreak LLMs using universal multi-prompts. We also adapt our approach for defense, which we term DUMP. Experimental results demonstrate that our method for optimizing universal multi-prompts outperforms existing techniques.',\n",
       "  'upvotes': 7,\n",
       "  'discussionId': '67a4609bf2e553c1d0da9181'},\n",
       " {'id': '2502.01105',\n",
       "  'authors': [{'_id': '67a45c85e73ad243c0b9529e',\n",
       "    'name': 'Yiren Song',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a45c85e73ad243c0b9529f',\n",
       "    'user': {'_id': '6729d1fed3ec5370cb035901',\n",
       "     'avatarUrl': '/avatars/50f7ce9c635148df76d1c63ebf3efa38.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': '1',\n",
       "     'user': 'DANNY621',\n",
       "     'type': 'user'},\n",
       "    'name': 'Danze Chen',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-07T09:58:20.740Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a45c85e73ad243c0b952a0',\n",
       "    'user': {'_id': '63a55320ce5763e06f78519c',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1671779060549-noauth.jpeg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Mike Shou',\n",
       "     'user': 'mikeshou',\n",
       "     'type': 'user'},\n",
       "    'name': 'Mike Zheng Shou',\n",
       "    'status': 'extracted_pending',\n",
       "    'statusLastChangedAt': '2025-02-06T06:54:02.195Z',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-03T06:49:58.000Z',\n",
       "  'title': 'LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion  Transformer',\n",
       "  'summary': \"Generating cognitive-aligned layered SVGs remains challenging due to existing methods' tendencies toward either oversimplified single-layer outputs or optimization-induced shape redundancies. We propose LayerTracer, a diffusion transformer based framework that bridges this gap by learning designers' layered SVG creation processes from a novel dataset of sequential design operations. Our approach operates in two phases: First, a text-conditioned DiT generates multi-phase rasterized construction blueprints that simulate human design workflows. Second, layer-wise vectorization with path deduplication produces clean, editable SVGs. For image vectorization, we introduce a conditional diffusion mechanism that encodes reference images into latent tokens, guiding hierarchical reconstruction while preserving structural integrity. Extensive experiments demonstrate LayerTracer's superior performance against optimization-based and neural baselines in both generation quality and editability, effectively aligning AI-generated vectors with professional design cognition.\",\n",
       "  'upvotes': 15,\n",
       "  'discussionId': '67a45c8ae73ad243c0b953ea'},\n",
       " {'id': '2502.03275',\n",
       "  'authors': [{'_id': '67a448b69ca42c642a723a7d',\n",
       "    'name': 'DiJia Su',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a448b69ca42c642a723a7e',\n",
       "    'user': {'_id': '6467bc59b990713c50339d2d',\n",
       "     'avatarUrl': '/avatars/064aba45e37040f7b1de8f76169f5174.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Hanlin Zhu',\n",
       "     'user': 'hanlinzhu',\n",
       "     'type': 'user'},\n",
       "    'name': 'Hanlin Zhu',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-06T16:10:24.407Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a448b69ca42c642a723a7f',\n",
       "    'user': {'_id': '6481333f8c6a3b8f11fc4114',\n",
       "     'avatarUrl': '/avatars/15c194296506b32e3f218530382c9f78.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Yingchen Xu',\n",
       "     'user': 'xuyingchen',\n",
       "     'type': 'user'},\n",
       "    'name': 'Yingchen Xu',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-06T16:09:40.111Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a448b69ca42c642a723a80',\n",
       "    'user': {'_id': '653b306986b88947d5cacfa4',\n",
       "     'avatarUrl': '/avatars/21ebd4daf35ec67c7d5f9b0a53628b00.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Jiantao Jiao',\n",
       "     'user': 'nexus-jt-llm',\n",
       "     'type': 'user'},\n",
       "    'name': 'Jiantao Jiao',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-06T16:09:29.786Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a448b69ca42c642a723a81',\n",
       "    'user': {'_id': '6344cf73ee1504dbcd5bdfe7',\n",
       "     'avatarUrl': '/avatars/6dd2bf1f9c5679e5c8c85d62c9836aac.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Yuandong Tian',\n",
       "     'user': 'tydsh',\n",
       "     'type': 'user'},\n",
       "    'name': 'Yuandong Tian',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-06T16:09:19.670Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a448b69ca42c642a723a82',\n",
       "    'user': {'_id': '64d27579dafee18faf9308ac',\n",
       "     'avatarUrl': '/avatars/8914a47244017c3541d3d5ac5b2d0372.svg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Qinqing Zheng',\n",
       "     'user': 'goodsleep',\n",
       "     'type': 'user'},\n",
       "    'name': 'Qinqing Zheng',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-06T16:09:11.277Z',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-05T15:33:00.000Z',\n",
       "  'title': 'Token Assorted: Mixing Latent and Text Tokens for Improved Language  Model Reasoning',\n",
       "  'summary': 'Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks.',\n",
       "  'upvotes': 11,\n",
       "  'discussionId': '67a448b89ca42c642a723ac6'},\n",
       " {'id': '2502.03387',\n",
       "  'authors': [{'_id': '67a445ccbdd74b63b4e52a7d',\n",
       "    'name': 'Yixin Ye',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a445ccbdd74b63b4e52a7e',\n",
       "    'user': {'_id': '643581a4f3b08e267d990499',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/643581a4f3b08e267d990499/KRhB-48W4IPuB0bX16Ahj.png',\n",
       "     'isPro': False,\n",
       "     'fullname': 'Zhen Huang',\n",
       "     'user': 'ZhenHuang',\n",
       "     'type': 'user'},\n",
       "    'name': 'Zhen Huang',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-06T14:37:03.643Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a445ccbdd74b63b4e52a7f', 'name': 'Yang Xiao', 'hidden': False},\n",
       "   {'_id': '67a445ccbdd74b63b4e52a80',\n",
       "    'user': {'_id': '64bb5f9d8e051085bace4d1e',\n",
       "     'avatarUrl': '/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg',\n",
       "     'isPro': True,\n",
       "     'fullname': 'Ethan Chern',\n",
       "     'user': 'ethanchern',\n",
       "     'type': 'user'},\n",
       "    'name': 'Ethan Chern',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-06T14:36:45.045Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a445ccbdd74b63b4e52a81',\n",
       "    'user': {'_id': '65900d4ff5a209eeac08b463',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/65900d4ff5a209eeac08b463/PJNNBRJIk1qR24oaRLTex.jpeg',\n",
       "     'isPro': False,\n",
       "     'fullname': 'shijie xia',\n",
       "     'user': 'seven-cat',\n",
       "     'type': 'user'},\n",
       "    'name': 'Shijie Xia',\n",
       "    'status': 'claimed_verified',\n",
       "    'statusLastChangedAt': '2025-02-06T14:14:59.334Z',\n",
       "    'hidden': False},\n",
       "   {'_id': '67a445ccbdd74b63b4e52a82',\n",
       "    'user': {'_id': '6144a0c4ff1146bbd84d9865',\n",
       "     'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/1661715958139-6144a0c4ff1146bbd84d9865.png',\n",
       "     'isPro': True,\n",
       "     'fullname': 'Pengfei Liu',\n",
       "     'user': 'Pengfei',\n",
       "     'type': 'user'},\n",
       "    'name': 'Pengfei Liu',\n",
       "    'status': 'admin_assigned',\n",
       "    'statusLastChangedAt': '2025-02-06T14:36:38.049Z',\n",
       "    'hidden': False}],\n",
       "  'publishedAt': '2025-02-05T17:23:45.000Z',\n",
       "  'title': 'LIMO: Less is More for Reasoning',\n",
       "  'summary': 'We present a fundamental discovery that challenges our understanding of how complex reasoning emerges in large language models. While conventional wisdom suggests that sophisticated reasoning tasks demand extensive training data (>100,000 examples), we demonstrate that complex mathematical reasoning abilities can be effectively elicited with surprisingly few examples. Through comprehensive experiments, our proposed model LIMO demonstrates unprecedented performance in mathematical reasoning. With merely 817 curated training samples, LIMO achieves 57.1% accuracy on AIME and 94.8% on MATH, improving from previous SFT-based models\\' 6.5% and 59.2% respectively, while only using 1% of the training data required by previous approaches. LIMO demonstrates exceptional out-of-distribution generalization, achieving 40.5% absolute improvement across 10 diverse benchmarks, outperforming models trained on 100x more data, challenging the notion that SFT leads to memorization rather than generalization. Based on these results, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning capabilities can emerge through minimal but precisely orchestrated demonstrations of cognitive processes. This hypothesis posits that the elicitation threshold for complex reasoning is determined by two key factors: (1) the completeness of the model\\'s encoded knowledge foundation during pre-training, and (2) the effectiveness of post-training examples as \"cognitive templates\" that show the model how to utilize its knowledge base to solve complex reasoning tasks. To facilitate reproducibility and future research in data-efficient reasoning, we release LIMO as a comprehensive open-source suite at https://github.com/GAIR-NLP/LIMO.',\n",
       "  'upvotes': 41,\n",
       "  'discussionId': '67a445cdbdd74b63b4e52af7'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jiezi4ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
