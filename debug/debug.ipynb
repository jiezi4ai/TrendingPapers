{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract OAI metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_oad_metadata(full_path):\n",
    "    namespaces = {\n",
    "        'oai': 'http://www.openarchives.org/OAI/2.0/',\n",
    "        'arxiv': 'http://arxiv.org/OAI/arXiv/'\n",
    "    }\n",
    "                \n",
    "    oai_metadata = []\n",
    "    # read xml file by line\n",
    "    with open(full_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            xml_info = ET.fromstring(line)\n",
    "        \n",
    "            # target on record element\n",
    "            if xml_info.tag == '{http://www.openarchives.org/OAI/2.0/}record':\n",
    "                # get header info\n",
    "                header = xml_info.find('oai:header', namespaces)\n",
    "                identifier = header.find('oai:identifier', namespaces).text\n",
    "                datestamp = header.find('oai:datestamp', namespaces).text\n",
    "                setSpec = header.find('oai:setSpec', namespaces).text\n",
    "\n",
    "                # get metadata\n",
    "                metadata = xml_info.find('oai:metadata', namespaces)\n",
    "                arxiv = metadata.find('arxiv:arXiv', namespaces)\n",
    "                \n",
    "                # get arXiv info\n",
    "                arxiv_id = arxiv.find('arxiv:id', namespaces).text\n",
    "                created = arxiv.find('arxiv:created', namespaces).text\n",
    "                updated = arxiv.find('arxiv:updated', namespaces).text if arxiv.find('arxiv:updated', namespaces) is not None else None\n",
    "                \n",
    "                # get authors info\n",
    "                authors = []\n",
    "                for author in arxiv.findall('arxiv:authors/arxiv:author', namespaces):\n",
    "                    keyname = author.find('arxiv:keyname', namespaces).text\n",
    "                    forenames = author.find('arxiv:forenames', namespaces).text if author.find('arxiv:forenames', namespaces) is not None else ''\n",
    "                    suffix = author.find('arxiv:suffix', namespaces)\n",
    "                    suffix_text = suffix.text if suffix is not None else ''\n",
    "                    authors.append(f\"{forenames} {keyname} {suffix_text}\".strip())\n",
    "                \n",
    "                # get title, abstract, etc\n",
    "                title = arxiv.find('arxiv:title', namespaces).text\n",
    "                categories = arxiv.find('arxiv:categories', namespaces).text.split(' ')\n",
    "                comments = arxiv.find('arxiv:comments', namespaces).text if arxiv.find('arxiv:comments', namespaces) is not None else None\n",
    "                journal_ref = arxiv.find('arxiv:journal-ref', namespaces)\n",
    "                journal_ref_text = journal_ref.text if journal_ref is not None else None\n",
    "                doi = arxiv.find('arxiv:doi', namespaces)\n",
    "                doi_text = doi.text if doi is not None else None\n",
    "                license = arxiv.find('arxiv:license', namespaces).text\n",
    "                abstract = arxiv.find('arxiv:abstract', namespaces).text\n",
    "\n",
    "                # construct dict\n",
    "                record_data = {\n",
    "                    \"identifier\": identifier,\n",
    "                    \"datestamp\": datestamp,\n",
    "                    \"setSpec\": setSpec,\n",
    "                    \"arxiv_id\": arxiv_id,\n",
    "                    \"created\": created,\n",
    "                    \"updated\": updated,\n",
    "                    \"authors\": authors,\n",
    "                    \"title\": title,\n",
    "                    \"categories\": categories,\n",
    "                    \"comments\": comments,\n",
    "                    \"journal_ref\": journal_ref_text,\n",
    "                    \"doi\": doi_text,\n",
    "                    \"license\": license,\n",
    "                    \"abstract\": abstract\n",
    "                }\n",
    "                oai_metadata.append(record_data)\n",
    "    return oai_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DB_PATH ='/home/jiezi/Code/Github/TrendingPapers/data/'\n",
    "DB_NAME = 'trending_papers.db'\n",
    "OAI_PAPER_TBL_NM = \"oai_paper_pool\"  # table for preprint paper metadata (batch trhough OAI)\n",
    "OAI_PAPER_TBL_KEY = 'identifier'   # PK column for OAI_PAPER_TBL_NM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def sqlite_connect(db_name):\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_name)\n",
    "        return conn\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error connecting to database: {e}\")\n",
    "        return None\n",
    "\n",
    "def df_to_sqlite(\n",
    "        df, \n",
    "        table_name, \n",
    "        db_name, \n",
    "        id_key=None,\n",
    "        if_exists='append' \n",
    "        ):   \n",
    "    \"\"\"import pandas DataFrame to SQLite database\n",
    "    Args:\n",
    "        :param pd.DataFrame df: DataFrame to import\n",
    "        :param str table_name: table name to import to\n",
    "        :param str db_name: database name\n",
    "        :param str id_key: primary key for the table\n",
    "        :param str if_exists: 'append' or 'replace'\n",
    "    Returns:\n",
    "        :returns: None\n",
    "    Note:\n",
    "        - If 'id_key' is provided, the function will check for existing records in the database and only insert new records.\n",
    "        - If 'if_exists' is set to 'replace', the function will replace the existing table with the new data.\n",
    "        - If 'if_exists' is set to 'append', the function will append the new data to the existing table.\n",
    "        - The code would automatically neglect columns that are not in the table.\n",
    "        - The code would set the value of missing columns to None.\n",
    "        - Automatically create table if not exist.\n",
    "    \"\"\"\n",
    "    conn = sqlite_connect(db_name)\n",
    "    if conn:\n",
    "        df_converted = df.copy()\n",
    "\n",
    "        try:\n",
    "            # Check if the table exists\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(f\"SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}'\")\n",
    "            table_exists = cursor.fetchone() is not None\n",
    "\n",
    "            # 1. Identify and Convert Dict/List-of-Dict Columns to JSON\n",
    "            # This block of code must be placed before creating the table\n",
    "            for col in df_converted.columns:\n",
    "                if df_converted[col].dtype == 'object':\n",
    "                    df_converted[col] = df_converted[col].apply(lambda x: json.dumps(x, ensure_ascii=False) if any(isinstance(x, (dict, list)) for x in df_converted[col].dropna()) else str(x))\n",
    "\n",
    "            # Create table if it doesn't exist\n",
    "            if not table_exists:\n",
    "                create_table_from_df(conn, df_converted, table_name, id_key)\n",
    "\n",
    "            # Get the list of columns in the existing table\n",
    "            cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "            table_columns = {row[1] for row in cursor.fetchall()} # Using a set for faster lookup\n",
    "\n",
    "            if id_key and table_exists:\n",
    "                # Fetch existing IDs from the database\n",
    "                cursor.execute(f\"SELECT DISTINCT {id_key} FROM {table_name}\")\n",
    "                existing_ids = {row[0] for row in cursor.fetchall()}\n",
    "\n",
    "                # Filter out rows with IDs that already exist\n",
    "                df_converted = df_converted[~df_converted[id_key].isin(existing_ids)]\n",
    "\n",
    "            if df_converted.empty and table_exists:\n",
    "                print(f\"No new records to insert into '{table_name}' (based on '{id_key}').\")\n",
    "                return\n",
    "            \n",
    "            # --- Modification: Keep only relevant columns ---\n",
    "            if table_exists:\n",
    "                df_converted = df_converted.loc[:, df_converted.columns.isin(table_columns)]\n",
    "\n",
    "            # Add missing columns to the DataFrame and set values to None\n",
    "            if table_exists:\n",
    "                for col in table_columns:\n",
    "                    if col not in df_converted.columns:\n",
    "                        df_converted[col] = None\n",
    "\n",
    "            # Reorder DataFrame columns to match the table's column order\n",
    "            # Convert table_columns set back to a list for ordering\n",
    "            if table_exists:\n",
    "                df_converted = df_converted[list(table_columns)]\n",
    "            \n",
    "            # 2. Explicitly define SQLite types if needed\n",
    "            dtype_mapping = {}\n",
    "            for col_name, col_type in df_converted.dtypes.items():\n",
    "                if col_name == id_key:\n",
    "                    dtype_mapping[col_name] = \"TEXT PRIMARY KEY\"  # Assuming ID key is text\n",
    "                elif 'int' in str(col_type):\n",
    "                    dtype_mapping[col_name]  = \"INTEGER\"\n",
    "                elif 'float' in str(col_type):\n",
    "                    dtype_mapping[col_name]  = \"REAL\"\n",
    "                else:\n",
    "                    dtype_mapping[col_name]  = \"TEXT\"\n",
    "\n",
    "            df_converted.to_sql(\n",
    "                table_name, conn, if_exists=if_exists, index=False,\n",
    "                dtype=dtype_mapping\n",
    "                )\n",
    "            # df_converted.to_sql(table_name, conn, if_exists=if_exists, index=False)\n",
    "            print(f\"Data successfully written to table '{table_name}' in '{db_name}'\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error writing to database: {e}\")\n",
    "            print(f\"Error writing to database: {e}\")\n",
    "        finally:\n",
    "            conn.close()\n",
    "\n",
    "def create_table_from_df(conn, df, table_name, id_key):\n",
    "    \"\"\"Creates a table in the SQLite database based on the DataFrame structure.\"\"\"\n",
    "    columns_sql = []\n",
    "    for col_name, col_type in df.dtypes.items():\n",
    "        if col_name == id_key:\n",
    "            sql_type = \"TEXT PRIMARY KEY\"  # Assuming ID key is text\n",
    "        elif 'int' in str(col_type):\n",
    "            sql_type = \"INTEGER\"\n",
    "        elif 'float' in str(col_type):\n",
    "            sql_type = \"REAL\"\n",
    "        else:\n",
    "            sql_type = \"TEXT\"\n",
    "        columns_sql.append(f'\"{col_name}\" {sql_type}') # Wrapping column names in double quotes\n",
    "\n",
    "    create_table_sql = f\"CREATE TABLE {table_name} ({', '.join(columns_sql)})\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(create_table_sql)\n",
    "    conn.commit()\n",
    "    print(f\"Table '{table_name}' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_list_of_dicts(data, key):\n",
    "    \"\"\"dedup list of dicts based on given key. Keep only the first item.\n",
    "    \"\"\"\n",
    "    seen = set()  # 使用集合来高效地检查是否已经遇到过某个值\n",
    "    deduplicated_data = []\n",
    "    for item in data:\n",
    "        value = item[key]\n",
    "        if value not in seen:\n",
    "            seen.add(value)\n",
    "            deduplicated_data.append(item)\n",
    "    return deduplicated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "CURRENT_DT = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "full_path = \"../data/stat_2025-02-05_2025-02-06.xml\"\n",
    "oai_metadata = extract_oad_metadata(full_path)\n",
    "cleaned_oai_metadata = deduplicate_list_of_dicts(oai_metadata, OAI_PAPER_TBL_KEY)\n",
    "df = pd.DataFrame(cleaned_oai_metadata)\n",
    "df['insert_dt'] = 'new'\n",
    "df_to_sqlite(\n",
    "    df, \n",
    "    table_name = OAI_PAPER_TBL_NM, \n",
    "    db_name = os.path.join(DB_PATH, DB_NAME),\n",
    "    if_exists = 'append', \n",
    "    id_key = OAI_PAPER_TBL_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huggingface API Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import requests\n",
    "from requests.adapters import Retry, HTTPAdapter\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "from json_repair import repair_json  # https://github.com/mangiucugna/json_repair/\n",
    "from firecrawl import FirecrawlApp  # pip install firecrawl-py https://github.com/mendableai/firecrawl\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "_useragent_list = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36 Edg/111.0.1661.62',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/111.0'\n",
    "]\n",
    "\n",
    "class HuggingFaceKit:\n",
    "    def __init__(self, max_retries_cnt=3, firecrawl_api_key=None):\n",
    "        self.base_url = \"https://huggingface.co/api/daily_papers\"\n",
    "        self.headers = {\n",
    "            \"User-Agent\": random.choice(_useragent_list)\n",
    "        }\n",
    "        self.session = requests.Session()\n",
    "        retry_strategy = Retry(\n",
    "            total=max_retries_cnt,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            backoff_factor=1,\n",
    "            allowed_methods=[\"GET\"]\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        self.session.mount(\"https://\", adapter)\n",
    "        self.session.mount(\"http://\", adapter)\n",
    "\n",
    "        if firecrawl_api_key is not None:\n",
    "            self.firecrawl = FirecrawlApp(api_key=firecrawl_api_key)\n",
    "\n",
    "\n",
    "    def fetch_daily_papers(self, date_str=None, max_cnt=None):\n",
    "        logger.info(f\"正在获取 {date_str} 的论文数据\")\n",
    "        \n",
    "        # 构建API URL\n",
    "        if date_str is not None and max_cnt is not None:\n",
    "            url = f\"{self.base_url}?date={date_str}&limit={max_cnt}\"\n",
    "        elif date_str is not None:\n",
    "            url = f\"{self.base_url}?date={date_str}\"\n",
    "        elif max_cnt is not None:\n",
    "            url = f\"{self.base_url}?limit={max_cnt}\"\n",
    "        else:\n",
    "            url = self.base_url \n",
    "\n",
    "        try:\n",
    "            response = self.session.get(url, headers=self.headers, timeout=10) # 添加 timeout\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            if not data:\n",
    "                print(\"No data received from API.\")\n",
    "\n",
    "            # Debug: Print keys of the first paper\n",
    "            if data: \n",
    "                hf_paper_dicts = [item.get('paper') for item in data]\n",
    "                return hf_paper_dicts\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "        except requests.exceptions.RequestException as e: # 捕获更具体的 requests 异常\n",
    "            print(f\"Error fetching papers through API: {e}\\nSwitch to FireCrawl:\\n\")\n",
    "            try:\n",
    "                response = self.firecrawl.scrape_url(url=url, params={\n",
    "                    'formats': [ 'markdown', 'links' ],\n",
    "                    'excludeTags': [ '.ad', 'script', '#footer' ]\n",
    "                })\n",
    "                md = response.get('markdown').replace(\"\\\\n\", \" \").replace(\"\\\\\", \"\")\n",
    "                data = json.loads(repair_json(md))\n",
    "                if data:\n",
    "                    hf_paper_dicts = []\n",
    "                    for item in data:\n",
    "                        paper_metadata = item.get('paper')\n",
    "                        rvsd_paper_metadata = {{'\\\\_id':'_id'}.get(key, key): \n",
    "                                               value for key, value in paper_metadata.items()}\n",
    "                        hf_paper_dicts.append(rvsd_paper_metadata)\n",
    "                    return hf_paper_dicts\n",
    "                else:\n",
    "                    return []\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error: {e}\")\n",
    "                return []\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_kit = HuggingFaceKit(max_retries_cnt=3, firecrawl_api_key=)\n",
    "papers = hf_kit.fetch_daily_papers()\n",
    "if papers:\n",
    "    print(f\"Successfully fetched {len(papers)} papers.\")\n",
    "else:\n",
    "    print(\"Failed to fetch papers after multiple retries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jiezi4ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
