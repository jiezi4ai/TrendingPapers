<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:0907.2412</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>0907.2412</id><created>2009-07-14</created><updated>2025-02-01</updated><authors><author><keyname>Hammerich</keyname><forenames>Edwin</forenames></author></authors><title>Design of Pulse Shapes Based on Sampling with Gaussian Prefilter</title><categories>cs.IT math.IT</categories><comments>4 pages, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two new pulse shapes for communications are presented. The first pulse shape generates a set of pulses without intersymbol interferenc (ISI) or is ISI-free for short. In the neighbourhood of the origin it is similar in shape to the classical cardinal sine function but is of exponential decay at infinity. This pulse shape is identical to the interpolating function of a recent sampling theorem with Gaussian prefilter. The second pulse shape is obtained from the first pulse shape by spectral factorization. Besides being also of exponential decay at infinity, it has a causal appearance since it is of superexponential decay for negative times. It is closely related to the orthonormal generating function considered earlier by Unser in the context of shift-invariant spaces. This pulse shape is not ISI-free but it generates a set of orthonormal pulses. The second pulse shape may also be used to define a receive matched filter so that at the filter output the ISI-free pulses of the first kind are recovered. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:1705.00290</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>1705.00290</id><created>2017-04-30</created><updated>2025-02-01</updated><authors><author><keyname>Baskar</keyname><forenames>A.</forenames></author><author><keyname>Sreejith</keyname><forenames>A. V.</forenames></author><author><keyname>Thinniyam</keyname><forenames>R. S.</forenames></author></authors><title>Modulo quantifiers over functional vocabularies extending addition</title><categories>cs.LO</categories><comments>There is a bug in the proof</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that first order logic (FO) and first order logic extended with modulo counting quantifiers (FOMOD) over purely functional vocabularies which extend addition, satisfy the Crane beach property (CBP) if the logic satisfies a normal form (called positional normal form). This not only shows why logics over the addition vocabulary have the CBP but also gives new CBP results, for example for the vocabulary which extends addition with the exponentiation function. The above results can also be viewed from the perspective of circuit complexity. Showing the existence of regular languages not definable in FOMOD[&lt;, +, *] is equivalent to the separation of the circuit complexity classes ACC0 and NC1 . Our theorem shows that a weaker logic , namely, FOMOD[&lt;,+,2^x] cannot define all regular languages. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:1910.13398</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>1910.13398</id><created>2019-10-29</created><updated>2025-01-31</updated><authors><author><keyname>Lin</keyname><forenames>Wu</forenames></author><author><keyname>Khan</keyname><forenames>Mohammad Emtiyaz</forenames></author><author><keyname>Schmidt</keyname><forenames>Mark</forenames></author></authors><title>Stein's Lemma for the Reparameterization Trick with Exponential Family   Mixtures</title><categories>stat.ML cs.LG</categories><comments>fixed some typos in the main text</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Stein's method (Stein, 1973; 1981) is a powerful tool for statistical applications and has significantly impacted machine learning. Stein's lemma plays an essential role in Stein's method. Previous applications of Stein's lemma either required strong technical assumptions or were limited to Gaussian distributions with restricted covariance structures. In this work, we extend Stein's lemma to exponential-family mixture distributions, including Gaussian distributions with full covariance structures. Our generalization enables us to establish a connection between Stein's lemma and the reparameterization trick to derive gradients of expectations of a large class of functions under weak assumptions. Using this connection, we can derive many new reparameterizable gradient identities that go beyond the reach of existing works. For example, we give gradient identities when the expectation is taken with respect to Student's t-distribution, skew Gaussian, exponentially modified Gaussian, and normal inverse Gaussian. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2002.05308</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2002.05308</id><created>2020-02-12</created><updated>2025-02-01</updated><authors><author><keyname>Kato</keyname><forenames>Masahiro</forenames></author><author><keyname>Ishihara</keyname><forenames>Takuya</forenames></author><author><keyname>Honda</keyname><forenames>Junya</forenames></author><author><keyname>Narita</keyname><forenames>Yusuke</forenames></author></authors><title>Efficient Adaptive Experimental Design for Average Treatment Effect   Estimation</title><categories>stat.ML cs.LG econ.EM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study how to efficiently estimate average treatment effects (ATEs) using adaptive experiments. In adaptive experiments, experimenters sequentially assign treatments to experimental units while updating treatment assignment probabilities based on past data. We start by defining the efficient treatment-assignment probability, which minimizes the semiparametric efficiency bound for ATE estimation. Our proposed experimental design estimates and uses the efficient treatment-assignment probability to assign treatments. At the end of the proposed design, the experimenter estimates the ATE using a newly proposed Adaptive Augmented Inverse Probability Weighting (A2IPW) estimator. We show that the asymptotic variance of the A2IPW estimator using data from the proposed design achieves the minimized semiparametric efficiency bound. We also analyze the estimator's finite-sample properties and develop nonparametric and nonasymptotic confidence intervals that are valid at any round of the proposed design. These anytime valid confidence intervals allow us to conduct rate-optimal sequential hypothesis testing, allowing for early stopping and reducing necessary sample size. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2103.01648</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2103.01648</id><created>2021-03-02</created><updated>2022-04-25</updated><authors><author><keyname>González</keyname><forenames>Mario</forenames></author><author><keyname>Almansa</keyname><forenames>Andrés</forenames></author><author><keyname>Tan</keyname><forenames>Pauline</forenames></author></authors><title>Solving Inverse Problems by Joint Posterior Maximization with   Autoencoding Prior</title><categories>stat.ML cs.CV cs.LG eess.IV math.OC</categories><comments>arXiv admin note: text overlap with arXiv:1911.06379</comments><report-no>https://hal.science/hal-03151455/</report-no><journal-ref>SIAM Journal on Imaging Sciences, Vol. 15, Iss. 2 (2022)   10.1137/21M140225X</journal-ref><doi>10.1137/21M140225X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we address the problem of solving ill-posed inverse problems in imaging where the prior is a variational autoencoder (VAE). Specifically we consider the decoupled case where the prior is trained once and can be reused for many different log-concave degradation models without retraining. Whereas previous MAP-based approaches to this problem lead to highly non-convex optimization algorithms, our approach computes the joint (space-latent) MAP that naturally leads to alternate optimization algorithms and to the use of a stochastic encoder to accelerate computations. The resulting technique (JPMAP) performs Joint Posterior Maximization using an Autoencoding Prior. We show theoretical and experimental evidence that the proposed objective function is quite close to bi-convex. Indeed it satisfies a weak bi-convexity property which is sufficient to guarantee that our optimization scheme converges to a stationary point. We also highlight the importance of correctly training the VAE using a denoising criterion, in order to ensure that the encoder generalizes well to out-of-distribution images, without affecting the quality of the generative model. This simple modification is key to providing robustness to the whole procedure. Finally we show how our joint MAP methodology relates to more common MAP approaches, and we propose a continuation scheme that makes use of our JPMAP algorithm to provide more robust MAP estimates. Experimental results also show the higher quality of the solutions obtained by our JPMAP approach with respect to other non-convex MAP approaches which more often get stuck in spurious local optima. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2103.04715</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2103.04715</id><created>2021-03-08</created><updated>2022-01-12</updated><authors><author><keyname>Laumont</keyname><forenames>Rémi</forenames></author><author><keyname>de Bortoli</keyname><forenames>Valentin</forenames></author><author><keyname>Almansa</keyname><forenames>Andrés</forenames></author><author><keyname>Delon</keyname><forenames>Julie</forenames></author><author><keyname>Durmus</keyname><forenames>Alain</forenames></author><author><keyname>Pereyra</keyname><forenames>Marcelo</forenames></author></authors><title>Bayesian imaging using Plug &amp; Play priors: when Langevin meets Tweedie</title><categories>stat.ME cs.CV eess.IV math.ST stat.ML stat.TH</categories><report-no>https://hal.science/hal-03161400/</report-no><msc-class>65K10, 65K05, 65D18, 62F15, 62C10, 68Q25, 68U10, 90C26</msc-class><journal-ref>SIAM Journal on Imaging Sciences, Volume 15, Issue 2 (2022)</journal-ref><doi>10.1137/21M1406349</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the seminal work of Venkatakrishnan et al. in 2013, Plug &amp; Play (PnP) methods have become ubiquitous in Bayesian imaging. These methods derive Minimum Mean Square Error (MMSE) or Maximum A Posteriori (MAP) estimators for inverse problems in imaging by combining an explicit likelihood function with a prior that is implicitly defined by an image denoising algorithm. The PnP algorithms proposed in the literature mainly differ in the iterative schemes they use for optimisation or for sampling. In the case of optimisation schemes, some recent works guarantee the convergence to a fixed point, albeit not necessarily a MAP estimate. In the case of sampling schemes, to the best of our knowledge, there is no known proof of convergence. There also remain important open questions regarding whether the underlying Bayesian models and estimators are well defined, well-posed, and have the basic regularity properties required to support these numerical schemes. To address these limitations, this paper develops theory, methods, and provably convergent algorithms for performing Bayesian inference with PnP priors. We introduce two algorithms: 1) PnP-ULA (Unadjusted Langevin Algorithm) for Monte Carlo sampling and MMSE inference; and 2) PnP-SGD (Stochastic Gradient Descent) for MAP inference. Using recent results on the quantitative convergence of Markov chains, we establish detailed convergence guarantees for these two algorithms under realistic assumptions on the denoising operators used, with special attention to denoisers based on deep neural networks. We also show that these algorithms approximately target a decision-theoretically optimal Bayesian model that is well-posed. The proposed algorithms are demonstrated on several canonical problems such as image deblurring, inpainting, and denoising, where they are used for point estimation as well as for uncertainty visualisation and quantification. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2104.14433</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2104.14433</id><created>2021-04-21</created><authors><author><keyname>Bhatasana</keyname><forenames>Meghavin</forenames></author><author><keyname>Marconnet</keyname><forenames>Amy</forenames></author></authors><title>Machine-Learning Assisted Optimization Strategies for Phase Change   Materials Embedded within Electronic Packages</title><categories>cs.CE cs.ET cs.LG</categories><comments>13 pages, 8 figures, 7 tables</comments><journal-ref>Applied Thermal Engineering, vol. 199, p. 117384, 2021</journal-ref><doi>10.1016/j.applthermaleng.2021.117384</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Leveraging the latent heat of phase change materials (PCMs) can reduce the peak temperatures and transient variations in temperature in electronic devices. But as the power levels increase, the thermal conduction pathway from the heat source to the heat sink limits the effectiveness of these systems. In this work, we evaluate embedding the PCM within the silicon device layer of an electronic device to minimize the thermal resistance between the source and the PCM to minimize this thermal resistance and enhance the thermal performance of the device. The geometry and material properties of the embedded PCM regions are optimized using a combination of parametric and machine learning algorithms. For a fixed geometry, considering commercially available materials, Solder 174 significantly outperforms other organic and metallic PCMs. Also with a fixed geometry, the optimal melting points to minimize the peak temperature is higher than the optimal melting point to minimize the amplitude of the transient temperature oscillation, and both optima increase with increasing heater power. Extending beyond conventional optimization strategies, genetic algorithms and particle swarm optimization with and without neural network surrogate models are used to enable optimization of many geometric and material properties. For the test case evaluated, the optimized geometries and properties are similar between all ML-assisted algorithms, but the computational time depends on the technique. Ultimately, the optimized design with embedded phase change materials reduces the maximum temperature rise by 19% and the fluctuations by up to 88% compared to devices without PCM. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2105.04733</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2105.04733</id><created>2021-05-10</created><updated>2023-07-11</updated><authors><author><keyname>Sulimany</keyname><forenames>Kfir</forenames></author><author><keyname>Pelc</keyname><forenames>Guy</forenames></author><author><keyname>Dudkiewicz</keyname><forenames>Rom</forenames></author><author><keyname>Korenblit</keyname><forenames>Simcha</forenames></author><author><keyname>Eisenberg</keyname><forenames>Hagai S.</forenames></author><author><keyname>Bromberg</keyname><forenames>Yaron</forenames></author><author><keyname>Ben-Or</keyname><forenames>Michael</forenames></author></authors><title>High-dimensional coherent one-way quantum key distribution</title><categories>quant-ph cs.CR cs.IT math.IT physics.optics</categories><journal-ref>npj Quantum Information 11, 16 (2025)</journal-ref><doi>10.1038/s41534-025-00965-7</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  High-dimensional quantum key distribution (QKD) offers secure communication, with secure key rates that surpass those achievable by QKD protocols utilizing two-dimensional encoding. However, existing high-dimensional QKD protocols require additional experimental resources, such as multiport interferometers and multiple detectors, thus raising the cost of practical high-dimensional systems and limiting their use. Here, we present and analyze a novel protocol for arbitrary-dimensional QKD, that requires only the hardware of a standard two-dimensional system. We provide security proofs against individual attacks and coherent attacks, setting an upper and lower bound on the secure key rates. Then, we test the new high-dimensional protocol in a standard two-dimensional QKD system over a 40 km fiber link. The new protocol yields a two-fold enhancement of the secure key rate compared to the standard two-dimensional coherent one-way protocol, without introducing any hardware modifications to the system. This work, therefore, holds great potential to enhance the performance of already deployed time-bin QKD systems through a software update alone. Furthermore, its applications extend across different encoding schemes of QKD qudits. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2107.05087</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2107.05087</id><created>2021-07-11</created><updated>2025-02-02</updated><authors><author><keyname>Mathew</keyname><forenames>Joshua</forenames></author><author><keyname>Tian</keyname><forenames>Xin</forenames></author><author><keyname>Wu</keyname><forenames>Min</forenames></author><author><keyname>Wong</keyname><forenames>Chau-Wai</forenames></author></authors><title>Remote Blood Oxygen Estimation From Videos Using Neural Networks</title><categories>cs.LG cs.CV eess.IV</categories><comments>Published in IEEE Journal of Biomedical and Health Informatics</comments><journal-ref>"Remote Blood Oxygen Estimation From Videos Using Neural   Networks," in IEEE Journal of Biomedical and Health Informatics, vol. 27, no.   8, pp. 3710-3720, Aug. 2023</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Blood oxygen saturation (SpO$_2$) is an essential indicator of respiratory functionality and is receiving increasing attention during the COVID-19 pandemic. Clinical findings show that it is possible for COVID-19 patients to have significantly low SpO$_2$ before any obvious symptoms. The prevalence of cameras has motivated researchers to investigate methods for monitoring SpO$_2$ using videos. Most prior schemes involving smartphones are contact-based: They require a fingertip to cover the phone's camera and the nearby light source to capture re-emitted light from the illuminated tissue. In this paper, we propose the first convolutional neural network based noncontact SpO$_2$ estimation scheme using smartphone cameras. The scheme analyzes the videos of a participant's hand for physiological sensing, which is convenient and comfortable, and can protect their privacy and allow for keeping face masks on. We design our neural network architectures inspired by the optophysiological models for SpO$_2$ measurement and demonstrate the explainability by visualizing the weights for channel combination. Our proposed models outperform the state-of-the-art model that is designed for contact-based SpO$_2$ measurement, showing the potential of our proposed method to contribute to public health. We also analyze the impact of skin type and the side of a hand on SpO$_2$ estimation performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2107.06095</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2107.06095</id><created>2021-07-08</created><authors><author><keyname>Krane</keyname><forenames>Patrick</forenames></author><author><keyname>Nash</keyname><forenames>Austin L.</forenames></author><author><keyname>Ziviani</keyname><forenames>Davide</forenames></author><author><keyname>Braun</keyname><forenames>James E.</forenames></author><author><keyname>Marconnet</keyname><forenames>Amy M.</forenames></author><author><keyname>Jain</keyname><forenames>Neera</forenames></author></authors><title>Dynamic Modeling and Control of a Two-Reactor Metal Hydride Energy   Storage System</title><categories>eess.SY cs.SY</categories><comments>42 pages; 14 Figures</comments><journal-ref>Applied Energy, Volume 325, 1 November 2022, 119836</journal-ref><doi>10.1016/j.apenergy.2022.119836</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Metal hydrides have been studied for use in energy storage, hydrogen storage, and air-conditioning (A/C) systems. A common architecture for A/C and energy storage systems is two metal hydride reactors connected to each other so that hydrogen can flow between them, allowing for cyclic use of the hydrogen. This paper presents a nonlinear dynamic model and multivariate control strategy of such a system. Each reactor is modelled as a shell-and-tube heat exchanger connected to a circulating fluid, and a compressor drives hydrogen flow between the reactors. We further develop a linear state-space version of this model integrated with a model predictive controller to determine the fluid mass flow rates and compressor pressure difference required to achieve desired heat transfer rates between the metal hydride and the fluid. A series of case studies demonstrates that this controller can track desired heat transfer rates in each reactor, even in the presence of time-varying circulating fluid inlet temperatures, thereby enabling the use of a two-reactor system for energy storage or integration with a heat pump. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2108.03418</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2108.03418</id><created>2021-08-07</created><updated>2025-02-02</updated><authors><author><keyname>Lai</keyname><forenames>Qiuxia</forenames></author><author><keyname>Li</keyname><forenames>Yu</forenames></author><author><keyname>Zeng</keyname><forenames>Ailing</forenames></author><author><keyname>Liu</keyname><forenames>Minhao</forenames></author><author><keyname>Sun</keyname><forenames>Hanqiu</forenames></author><author><keyname>Xu</keyname><forenames>Qiang</forenames></author></authors><title>Information Bottleneck Approach to Spatial Attention Learning</title><categories>cs.CV</categories><comments>Accepted to IJCAI 2021; Update supplementary</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The selective visual attention mechanism in the human visual system (HVS) restricts the amount of information to reach visual awareness for perceiving natural scenes, allowing near real-time information processing with limited computational capacity [Koch and Ullman, 1987]. This kind of selectivity acts as an 'Information Bottleneck (IB)', which seeks a trade-off between information compression and predictive accuracy. However, such information constraints are rarely explored in the attention mechanism for deep neural networks (DNNs). In this paper, we propose an IB-inspired spatial attention module for DNN structures built for visual recognition. The module takes as input an intermediate representation of the input image, and outputs a variational 2D attention map that minimizes the mutual information (MI) between the attention-modulated representation and the input, while maximizing the MI between the attention-modulated representation and the task label. To further restrict the information bypassed by the attention map, we quantize the continuous attention scores to a set of learnable anchor values during training. Extensive experiments show that the proposed IB-inspired spatial attention mechanism can yield attention maps that neatly highlight the regions of interest while suppressing backgrounds, and bootstrap standard DNN structures for visual recognition tasks (e.g., image classification, fine-grained recognition, cross-domain classification). The attention maps are interpretable for the decision making of the DNNs as verified in the experiments. Our code is available at https://github.com/ashleylqx/AIB.git. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2110.03155</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2110.03155</id><created>2021-10-06</created><updated>2025-02-01</updated><authors><author><keyname>Sun</keyname><forenames>Ke</forenames></author><author><keyname>Zhao</keyname><forenames>Yingnan</forenames></author><author><keyname>Shi</keyname><forenames>Enze</forenames></author><author><keyname>Wang</keyname><forenames>Yafei</forenames></author><author><keyname>Yan</keyname><forenames>Xiaodong</forenames></author><author><keyname>Jiang</keyname><forenames>Bei</forenames></author><author><keyname>Kong</keyname><forenames>Linglong</forenames></author></authors><title>The Benefits of Being Categorical Distributional: Uncertainty-aware   Regularized Exploration in Reinforcement Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The remarkable empirical performance of distributional reinforcement learning (RL) has garnered increasing attention to understanding its theoretical advantages over classical RL. By decomposing the categorical distributional loss commonly employed in distributional RL, we find that the potential superiority of distributional RL can be attributed to a derived distribution-matching entropy regularization. This less-studied entropy regularization aims to capture additional knowledge of return distribution beyond only its expectation, contributing to an augmented reward signal in policy optimization. In contrast to the vanilla entropy regularization in MaxEnt RL, which explicitly encourages exploration by promoting diverse actions, the novel entropy regularization derived from categorical distributional loss implicitly updates policies to align the learned policy with (estimated) environmental uncertainty. Finally, extensive experiments substantiate the significance of this uncertainty-aware regularization from distributional RL on the empirical benefits over classical RL. Our study offers a new perspective from the exploration to explain the intrinsic benefits of adopting distributional learning in RL. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2110.05942</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2110.05942</id><created>2021-10-11</created><updated>2025-01-31</updated><authors><author><keyname>Lin</keyname><forenames>Tianrong</forenames></author></authors><title>Resolution of The Linear-Bounded Automata Question</title><categories>cs.CC cs.FL</categories><comments>[v20] some typos fixed; 1 figure, 23 pages</comments><msc-class>68Q15, 68Q17, 03D15</msc-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper resolves a famous and longstanding open question in automata theory, i.e., the {\it linear-bounded automata question} (or shortly, LBA question), which can also be phrased succinctly in the language of computational complexity theory as $$ {\rm NSPACE}[n]\overset{?}{=}{\rm DSPACE}[n]. $$ In fact, we prove a more general result that $$ {\rm DSPACE}[S(n)]\subsetneqq {\rm NSPACE}[S(n)] $$ where $S(n)\geq n$ is a space-constructible function. Our proof technique is based on diagonalization against deterministic $S(n)$ space-bounded Turing machines with a universal nondeterministic Turing machine and on other novel and interesting new techniques. Our proof also implies the following consequences, which resolve some famous open questions in complexity theory:   (1). ${\rm DSPACE}[n]\subsetneqq {\rm NSPACE}[n]$;   (2). $L\subsetneqq NL$;   (3). $L\subsetneqq P$;   (4). There exists no deterministic Turing machine working in $O(\log n)$ space deciding the $st$-connectivity question (STCON). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2201.06133</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2201.06133</id><created>2022-01-16</created><authors><author><keyname>Laumont</keyname><forenames>Rémi</forenames></author><author><keyname>de Bortoli</keyname><forenames>Valentin</forenames></author><author><keyname>Almansa</keyname><forenames>Andrés</forenames></author><author><keyname>Delon</keyname><forenames>Julie</forenames></author><author><keyname>Durmus</keyname><forenames>Alain</forenames></author><author><keyname>Pereyra</keyname><forenames>Marcelo</forenames></author></authors><title>On Maximum-a-Posteriori estimation with Plug &amp; Play priors and   stochastic gradient descent</title><categories>stat.ML cs.CV cs.LG eess.IV math.OC</categories><report-no>https://hal.science/hal-03348735/</report-no><msc-class>65K10 (Primary) 65K05, 62F15, 62C10, 68Q25, 68U10, 90C26 (Secondary)   65K10, 65K05, 62F15, 62C10, 68Q25, 68U10, 90C26</msc-class><journal-ref>Journal of Mathematical Imaging and Vision, Volume 65, Pages   140-163 (2023)</journal-ref><doi>10.1007/s10851-022-01134-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian methods to solve imaging inverse problems usually combine an explicit data likelihood function with a prior distribution that explicitly models expected properties of the solution. Many kinds of priors have been explored in the literature, from simple ones expressing local properties to more involved ones exploiting image redundancy at a non-local scale. In a departure from explicit modelling, several recent works have proposed and studied the use of implicit priors defined by an image denoising algorithm. This approach, commonly known as Plug &amp; Play (PnP) regularisation, can deliver remarkably accurate results, particularly when combined with state-of-the-art denoisers based on convolutional neural networks. However, the theoretical analysis of PnP Bayesian models and algorithms is difficult and works on the topic often rely on unrealistic assumptions on the properties of the image denoiser. This papers studies maximum-a-posteriori (MAP) estimation for Bayesian models with PnP priors. We first consider questions related to existence, stability and well-posedness, and then present a convergence proof for MAP computation by PnP stochastic gradient descent (PnP-SGD) under realistic assumptions on the denoiser used. We report a range of imaging experiments demonstrating PnP-SGD as well as comparisons with other PnP schemes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2201.10838</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2201.10838</id><created>2022-01-26</created><updated>2025-02-01</updated><authors><author><keyname>Chiang</keyname><forenames>John</forenames></author></authors><title>Privacy-Preserving Logistic Regression Training with A Faster Gradient   Variant</title><categories>cs.CR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training logistic regression over encrypted data has been a compelling approach in addressing security concerns for several years. In this paper, we introduce an efficient gradient variant, called $quadratic$ $gradient$, for privacy-preserving logistic regression training. We enhance Nesterov's Accelerated Gradient (NAG), Adaptive Gradient Algorithm (Adagrad) and Adam algorithms by incorporating their quadratic gradients and evaluate these improved algorithms on various datasets. Experimental results demonstrate that the enhanced algorithms achieve significantly improved convergence speed compared to traditional first-order gradient methods. Moreover, we applied the enhanced NAG method to implement homomorphic logistic regression training, achieving comparable results within just 4 iterations. There is a great chance that the quadratic gradient approach could integrate first-order gradient descent/ascent algorithms with the second-order Newton-Raphson methods, and that it could be applied to a wide range of numerical optimization problems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2202.03356</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2202.03356</id><created>2022-02-07</created><updated>2025-02-01</updated><authors><author><keyname>Zhao</keyname><forenames>Liangyu</forenames></author><author><keyname>Pal</keyname><forenames>Siddharth</forenames></author><author><keyname>Chugh</keyname><forenames>Tapan</forenames></author><author><keyname>Wang</keyname><forenames>Weiyang</forenames></author><author><keyname>Fantl</keyname><forenames>Jason</forenames></author><author><keyname>Basu</keyname><forenames>Prithwish</forenames></author><author><keyname>Khoury</keyname><forenames>Joud</forenames></author><author><keyname>Krishnamurthy</keyname><forenames>Arvind</forenames></author></authors><title>Efficient Direct-Connect Topologies for Collective Communications</title><categories>cs.NI cs.DC cs.LG</categories><comments>NSDI '25</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of distilling efficient network topologies for collective communications. We provide an algorithmic framework for constructing direct-connect topologies optimized for the latency vs. bandwidth trade-off associated with the workload. Our approach synthesizes many different topologies and schedules for a given cluster size and degree and then identifies the appropriate topology and schedule for a given workload. Our algorithms start from small, optimal base topologies and associated communication schedules and use techniques that can be iteratively applied to derive much larger topologies and schedules. Additionally, we incorporate well-studied large-scale graph topologies into our algorithmic framework by producing efficient collective schedules for them using a novel polynomial-time algorithm. Our evaluation uses multiple testbeds and large-scale simulations to demonstrate significant performance benefits from our derived topologies and schedules. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2203.05022</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2203.05022</id><created>2022-02-05</created><updated>2025-02-02</updated><authors><author><keyname>Ming</keyname><forenames>Gao</forenames></author></authors><title>A proof of P != NP (New symmetric encryption algorithm against any   linear attacks and differential attacks)</title><categories>cs.CC cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  P vs NP problem is the most important unresolved problem in the field of computational complexity. Its impact has penetrated into all aspects of algorithm design, especially in the field of cryptography. The security of cryptographic algorithms based on short keys depends on whether P is equal to NP. In fact, Shannon[1] strictly proved that the one-time-pad system meets unconditional security, but because the one-time-pad system requires the length of key to be at least the length of plaintext, how to transfer the key is a troublesome problem that restricts the use of the one-time-pad system in practice. Cryptography algorithms used in practice are all based on short key, and the security of the short key mechanism is ultimately based on "one-way" assumption, that is, it is assumed that a one-way function exists. In fact, the existence of one-way function can directly lead to the important conclusion P != NP. In this paper, we originally constructed a short-key block cipher algorithm. The core feature of this algorithm is that for any block, when a plaintext-ciphertext pair is known, any key in the key space can satisfy the plaintext-ciphertext pair, that is, for each block, the plaintext-ciphertext pair and the key are independence, and the independence between blocks is also easy to construct. This feature is completely different from all existing short-key cipher algorithms. Based on the above feature, we construct a problem and theoretically prove that the problem satisfies the properties of one-way functions, thereby solving the problem of the existence of one-way functions, that is, directly proving that P != NP. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2204.00486</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2204.00486</id><created>2022-04-01</created><updated>2025-02-01</updated><authors><author><keyname>Wang</keyname><forenames>Yuxuan</forenames></author><author><keyname>Gao</keyname><forenames>Difei</forenames></author><author><keyname>Yu</keyname><forenames>Licheng</forenames></author><author><keyname>Lei</keyname><forenames>Stan Weixian</forenames></author><author><keyname>Feiszli</keyname><forenames>Matt</forenames></author><author><keyname>Shou</keyname><forenames>Mike Zheng</forenames></author></authors><title>GEB+: A Benchmark for Generic Event Boundary Captioning, Grounding and   Retrieval</title><categories>cs.CV</categories><comments>Updated in Jan. 2025, In Proceedings of the European Conference on   Computer Vision 2022 [ECCV 2022], 27 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Cognitive science has shown that humans perceive videos in terms of events separated by the state changes of dominant subjects. State changes trigger new events and are one of the most useful among the large amount of redundant information perceived. However, previous research focuses on the overall understanding of segments without evaluating the fine-grained status changes inside. In this paper, we introduce a new dataset called Kinetic-GEB+. The dataset consists of over 170k boundaries associated with captions describing status changes in the generic events in 12K videos. Upon this new dataset, we propose three tasks supporting the development of a more fine-grained, robust, and human-like understanding of videos through status changes. We evaluate many representative baselines in our dataset, where we also design a new TPD (Temporal-based Pairwise Difference) Modeling method for visual difference and achieve significant performance improvements. Besides, the results show there are still formidable challenges for current methods in the utilization of different granularities, representation of visual difference, and the accurate localization of status changes. Further analysis shows that our dataset can drive developing more powerful methods to understand status changes and thus improve video level comprehension. The dataset including both videos and boundaries is available at https://yuxuan-w.github.io/GEB-plus/ </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2207.01789</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2207.01789</id><created>2022-07-04</created><updated>2024-07-08</updated><authors><author><keyname>Zhang</keyname><forenames>Richard Y.</forenames></author></authors><title>Improved Global Guarantees for the Nonconvex Burer--Monteiro   Factorization via Rank Overparameterization</title><categories>math.OC cs.LG stat.ML</categories><journal-ref>Mathematical Programming, 2024</journal-ref><doi>10.1007/s10107-024-02160-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider minimizing a twice-differentiable, $L$-smooth, and $\mu$-strongly convex objective $\phi$ over an $n\times n$ positive semidefinite matrix $M\succeq0$, under the assumption that the minimizer $M^{\star}$ has low rank $r^{\star}\ll n$. Following the Burer--Monteiro approach, we instead minimize the nonconvex objective $f(X)=\phi(XX^{T})$ over a factor matrix $X$ of size $n\times r$. This substantially reduces the number of variables from $O(n^{2})$ to as few as $O(n)$ and also enforces positive semidefiniteness for free, but at the cost of giving up the convexity of the original problem. In this paper, we prove that if the search rank $r\ge r^{\star}$ is overparameterized by a \emph{constant factor} with respect to the true rank $r^{\star}$, namely as in $r&gt;\frac{1}{4}(L/\mu-1)^{2}r^{\star}$, then despite nonconvexity, local optimization is guaranteed to globally converge from any initial point to the global optimum. This significantly improves upon a previous rank overparameterization threshold of $r\ge n$, which we show is sharp in the absence of smoothness and strong convexity, but would increase the number of variables back up to $O(n^{2})$. Conversely, without rank overparameterization, we prove that such a global guarantee is possible if and only if $\phi$ is almost perfectly conditioned, with a condition number of $L/\mu&lt;3$. Therefore, we conclude that a small amount of overparameterization can lead to large improvements in theoretical guarantees for the nonconvex Burer--Monteiro factorization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2207.08392</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2207.08392</id><created>2022-07-18</created><updated>2025-02-01</updated><authors><author><keyname>Tas</keyname><forenames>Ertem Nusret</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author><author><keyname>Gai</keyname><forenames>Fangyu</forenames></author><author><keyname>Kannan</keyname><forenames>Sreeram</forenames></author><author><keyname>Maddah-Ali</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Yu</keyname><forenames>Fisher</forenames></author></authors><title>Bitcoin-Enhanced Proof-of-Stake Security: Possibilities and   Impossibilities</title><categories>cs.CR</categories><comments>In IEEE Symposium on Security and Privacy 2023</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bitcoin is the most secure blockchain in the world, supported by the immense hash power of its Proof-of-Work miners. Proof-of-Stake chains are energy-efficient, have fast finality but face several security issues: susceptibility to non-slashable long-range safety attacks, low liveness resilience and difficulty to bootstrap from low token valuation. We show that these security issues are inherent in any PoS chain without an external trusted source, and propose a new protocol, Babylon, where an off-the-shelf PoS protocol checkpoints onto Bitcoin to resolve these issues. An impossibility result justifies the optimality of Babylon. A use case of Babylon is to reduce the stake withdrawal delay: our experimental results show that this delay can be reduced from weeks in existing PoS chains to less than 5 hours using Babylon, at a transaction cost of less than 10K USD per annum for posting the checkpoints onto Bitcoin. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2207.08878</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2207.08878</id><created>2022-07-18</created><updated>2025-02-02</updated><authors><author><keyname>Liu</keyname><forenames>Jingxiao</forenames></author><author><keyname>Wei</keyname><forenames>Yujie</forenames></author><author><keyname>Chen</keyname><forenames>Bingqing</forenames></author><author><keyname>Noh</keyname><forenames>Hae Young</forenames></author></authors><title>A hierarchical semantic segmentation framework for computer vision-based   bridge damage detection</title><categories>cs.CV</categories><journal-ref>SMART STRUCTURES AND SYSTEMS 31(4):325-334, 2023</journal-ref><doi>10.12989/sss.2023.31.4.325</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer vision-based damage detection using remote cameras and unmanned aerial vehicles (UAVs) enables efficient and low-cost bridge health monitoring that reduces labor costs and the needs for sensor installation and maintenance. By leveraging recent semantic image segmentation approaches, we are able to find regions of critical structural components and recognize damage at the pixel level using images as the only input. However, existing methods perform poorly when detecting small damages (e.g., cracks and exposed rebars) and thin objects with limited image samples, especially when the components of interest are highly imbalanced. To this end, this paper introduces a semantic segmentation framework that imposes the hierarchical semantic relationship between component category and damage types. For example, certain concrete cracks only present on bridge columns and therefore the non-column region will be masked out when detecting such damages. In this way, the damage detection model could focus on learning features from possible damaged regions only and avoid the effects of other irrelevant regions. We also utilize multi-scale augmentation that provides views with different scales that preserves contextual information of each image without losing the ability of handling small and thin objects. Furthermore, the proposed framework employs important sampling that repeatedly samples images containing rare components (e.g., railway sleeper and exposed rebars) to provide more data samples, which addresses the imbalanced data challenge. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2207.09560</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2207.09560</id><created>2022-07-19</created><updated>2025-02-01</updated><authors><author><keyname>Bennouna</keyname><forenames>Amine</forenames></author><author><keyname>Van Parys</keyname><forenames>Bart</forenames></author><author><keyname>Lucas</keyname><forenames>Ryan</forenames></author></authors><title>Holistic Robust Data-Driven Decisions</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The design of data-driven formulations for machine learning and decision-making with good out-of-sample performance is a key challenge. The observation that good in-sample performance does not guarantee good out-of-sample performance is generally known as overfitting. Practical overfitting can typically not be attributed to a single cause but is caused by several factors simultaneously. We consider here three overfitting sources: (i) statistical error as a result of working with finite sample data, (ii) data noise, which occurs when the data points are measured only with finite precision, and finally, (iii) data misspecification in which a small fraction of all data may be wholly corrupted. Although existing data-driven formulations may be robust against one of these three sources in isolation, they do not provide holistic protection against all overfitting sources simultaneously. We design a novel data-driven formulation that guarantees such holistic protection and is computationally viable. Our distributionally robust optimization formulation can be interpreted as a novel combination of a Kullback-Leibler and L\'evy-Prokhorov robust optimization formulation. In the context of classification and regression problems, we show that several popular regularized and robust formulations naturally reduce to a particular case of our proposed novel formulation. Finally, we apply the proposed HR formulation to two real-life applications and study it alongside several benchmarks: (1) training neural networks on healthcare data, where we analyze various robustness and generalization properties in the presence of noise, labeling errors, and scarce data, (2) a portfolio selection problem with real stock data, and analyze the risk/return tradeoff under the natural severe distribution shift of the application. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2208.04851</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2208.04851</id><created>2022-08-09</created><updated>2025-02-02</updated><authors><author><keyname>Chaumont-Frelet</keyname><forenames>T.</forenames></author><author><keyname>Dolean</keyname><forenames>V.</forenames></author><author><keyname>Ingremeau</keyname><forenames>M.</forenames></author></authors><title>Efficient approximation of high-frequency Helmholtz solutions by   Gaussian coherent states</title><categories>math.NA cs.NA math.AP</categories><report-no>hal-03747290</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce new finite-dimensional spaces specifically designed to approximate the solutions to high-frequency Helmholtz problems with smooth variable coefficients in dimension $d$. These discretization spaces are spanned by Gaussian coherent states, that have the key property to be localised in phase space. We carefully select the Gaussian coherent states spanning the approximation space by exploiting the (known) micro-localisation properties of the solution. For a large class of source terms (including plane-wave scattering problems), this choice leads to discrete spaces that provide a uniform approximation error for all wavenumber $k$ with a number of degrees of freedom scaling as $k^{d-1/2}$, which we rigorously establish. In comparison, for discretization spaces based on (piecewise) polynomials, the number of degrees of freedom has to scale at least as $k^d$ to achieve the same property. These theoretical results are illustrated by one-dimensional numerical examples, where the proposed discretization spaces are coupled with a least-squares variational formulation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2209.01870</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2209.01870</id><created>2022-09-05</created><updated>2025-02-02</updated><authors><author><keyname>Wang</keyname><forenames>Lianyu</forenames></author><author><keyname>Wang</keyname><forenames>Meng</forenames></author><author><keyname>Zhang</keyname><forenames>Daoqiang</forenames></author><author><keyname>Fu</keyname><forenames>Huazhu</forenames></author></authors><title>Unsupervised Domain Adaptation via Style-Aware Self-intermediate Domain</title><categories>cs.CV cs.AI</categories><comments>13 pages, 7 figures</comments><msc-class>68T07</msc-class><acm-class>I.2.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised domain adaptation (UDA) has attracted considerable attention, which transfers knowledge from a label-rich source domain to a related but unlabeled target domain. Reducing inter-domain differences has always been a crucial factor to improve performance in UDA, especially for tasks where there is a large gap between source and target domains. To this end, we propose a novel style-aware feature fusion method (SAFF) to bridge the large domain gap and transfer knowledge while alleviating the loss of class-discriminative information. Inspired by the human transitive inference and learning ability, a novel style-aware self-intermediate domain (SSID) is investigated to link two seemingly unrelated concepts through a series of intermediate auxiliary synthesized concepts. Specifically, we propose a novel learning strategy of SSID, which selects samples from both source and target domains as anchors, and then randomly fuses the object and style features of these anchors to generate labeled and style-rich intermediate auxiliary features for knowledge transfer. Moreover, we design an external memory bank to store and update specified labeled features to obtain stable class features and class-wise style features. Based on the proposed memory bank, the intra- and inter-domain loss functions are designed to improve the class recognition ability and feature compatibility, respectively. Meanwhile, we simulate the rich latent feature space of SSID by infinite sampling and the convergence of the loss function by mathematical theory. Finally, we conduct comprehensive experiments on commonly used domain adaptive benchmarks to evaluate the proposed SAFF, and the experimental results show that the proposed SAFF can be easily combined with different backbone networks and obtain better performance as a plug-in-plug-out module. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2210.08599</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2210.08599</id><created>2022-10-16</created><updated>2025-02-01</updated><authors><author><keyname>Shin</keyname><forenames>Sungho</forenames></author><author><keyname>Na</keyname><forenames>Sen</forenames></author><author><keyname>Anitescu</keyname><forenames>Mihai</forenames></author></authors><title>Near-Optimal Performance of Stochastic Model Predictive Control</title><categories>math.OC cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a dynamic regret analysis for stochastic model predictive control (SMPC) in linear systems with quadratic performance index and additive and multiplicative uncertainties. Under a finite support assumption, the problem can be cast as a finite-dimensional quadratic program, but the problem becomes quickly intractable as the problem size grows exponentially in the horizon length. SMPC aims to compute approximate solutions by solving a sequence of problems with truncated prediction horizons and committing the solution in a receding-horizon fashion. While this approach is widely used in practice, its performance relative to the optimal solution is not well understood. This article reports for the first time a rigorous near-optimal performance guarantee of SMPC: Under stabilizability and detectability conditions, the dynamic regret of SMPC is exponentially small in the prediction horizon length, allowing SMPC to achieve near-optimal performance at a substantially reduced computational expense. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2210.16334</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2210.16334</id><created>2022-10-28</created><authors><author><keyname>Sadr</keyname><forenames>Alireza Vafaei</forenames></author><author><keyname>Bassett</keyname><forenames>Bruce A.</forenames></author><author><keyname>Sekyi</keyname><forenames>Emmanuel</forenames></author></authors><title>Learning to Detect Interesting Anomalies</title><categories>cs.LG astro-ph.IM cs.AI hep-ex physics.data-an</categories><comments>10 pages, 7 figures</comments><msc-class>68T07 (Primary) 68T05, 68T09 (Secondary)</msc-class><acm-class>I.1.2; I.1.4</acm-class><journal-ref>RAS Techniques and Instruments, 2, Issue 1, 586, 2023</journal-ref><doi>10.1093/rasti/rzad032</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anomaly detection algorithms are typically applied to static, unchanging, data features hand-crafted by the user. But how does a user systematically craft good features for anomalies that have never been seen? Here we couple deep learning with active learning -- in which an Oracle iteratively labels small amounts of data selected algorithmically over a series of rounds -- to automatically and dynamically improve the data features for efficient outlier detection. This approach, AHUNT, shows excellent performance on MNIST, CIFAR10, and Galaxy-DESI data, significantly outperforming both standard anomaly detection and active learning algorithms with static feature spaces. Beyond improved performance, AHUNT also allows the number of anomaly classes to grow organically in response to Oracle's evaluations. Extensive ablation studies explore the impact of Oracle question selection strategy and loss function on performance. We illustrate how the dynamic anomaly class taxonomy represents another step towards fully personalized rankings of different anomaly classes that reflect a user's interests, allowing the algorithm to learn to ignore statistically significant but uninteresting outliers (e.g., noise). This should prove useful in the era of massive astronomical datasets serving diverse sets of users who can only review a tiny subset of the incoming data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2211.01174</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2211.01174</id><created>2022-11-02</created><authors><author><keyname>Lu</keyname><forenames>Zhuheng</forenames></author><author><keyname>Zhang</keyname><forenames>Peng</forenames></author><author><keyname>Dai</keyname><forenames>Yuewei</forenames></author><author><keyname>Li</keyname><forenames>Weiqing</forenames></author><author><keyname>Su</keyname><forenames>Zhiyong</forenames></author></authors><title>Hypergraph Convolutional Network based Weakly Supervised Point Cloud   Semantic Segmentation with Scene-Level Annotations</title><categories>cs.CV cs.GR</categories><journal-ref>Neurocomputing, Volume 620, 1 March 2025, 129264</journal-ref><doi>10.1016/j.neucom.2024.129264</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Point cloud segmentation with scene-level annotations is a promising but challenging task. Currently, the most popular way is to employ the class activation map (CAM) to locate discriminative regions and then generate point-level pseudo labels from scene-level annotations. However, these methods always suffer from the point imbalance among categories, as well as the sparse and incomplete supervision from CAM. In this paper, we propose a novel weighted hypergraph convolutional network-based method, called WHCN, to confront the challenges of learning point-wise labels from scene-level annotations. Firstly, in order to simultaneously overcome the point imbalance among different categories and reduce the model complexity, superpoints of a training point cloud are generated by exploiting the geometrically homogeneous partition. Then, a hypergraph is constructed based on the high-confidence superpoint-level seeds which are converted from scene-level annotations. Secondly, the WHCN takes the hypergraph as input and learns to predict high-precision point-level pseudo labels by label propagation. Besides the backbone network consisting of spectral hypergraph convolution blocks, a hyperedge attention module is learned to adjust the weights of hyperedges in the WHCN. Finally, a segmentation network is trained by these pseudo point cloud labels. We comprehensively conduct experiments on the ScanNet and S3DIS segmentation datasets. Experimental results demonstrate that the proposed WHCN is effective to predict the point labels with scene annotations, and yields state-of-the-art results in the community. The source code is available at http://zhiyongsu.github.io/Project/WHCN.html. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2212.01713</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2212.01713</id><created>2022-12-03</created><updated>2024-01-30</updated><authors><author><keyname>Zhang</keyname><forenames>Junxue</forenames></author><author><keyname>Cheng</keyname><forenames>Xiaodian</forenames></author><author><keyname>Yang</keyname><forenames>Liu</forenames></author><author><keyname>Hu</keyname><forenames>Jinbin</forenames></author><author><keyname>Liu</keyname><forenames>Ximeng</forenames></author><author><keyname>Chen</keyname><forenames>Kai</forenames></author></authors><title>SoK: Fully Homomorphic Encryption Accelerators</title><categories>cs.CR cs.AR</categories><journal-ref>ACM Computing Survey, 2024, Volume: 26, Issue: 12</journal-ref><doi>10.1145/3676955</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fully Homomorphic Encryption~(FHE) is a key technology enabling privacy-preserving computing. However, the fundamental challenge of FHE is its inefficiency, due primarily to the underlying polynomial computations with high computation complexity and extremely time-consuming ciphertext maintenance operations. To tackle this challenge, various FHE accelerators have recently been proposed by both research and industrial communities. This paper takes the first initiative to conduct a systematic study on the 14 FHE accelerators -- cuHE/cuFHE, nuFHE, HEAT, HEAX, HEXL, HEXL-FPGA, 100$\times$, F1, CraterLake, BTS, ARK, Poseidon, FAB and TensorFHE. We first make our observations on the evolution trajectory of these existing FHE accelerators to establish a qualitative connection between them. Then, we perform testbed evaluations of representative open-source FHE accelerators to provide a quantitative comparison on them. Finally, with the insights learned from both qualitative and quantitative studies, we discuss potential directions to inform the future design and implementation for FHE accelerators. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2301.09522</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2301.09522</id><created>2023-01-23</created><updated>2025-02-02</updated><authors><author><keyname>Wu</keyname><forenames>Dengyu</forenames></author><author><keyname>Jin</keyname><forenames>Gaojie</forenames></author><author><keyname>Yu</keyname><forenames>Han</forenames></author><author><keyname>Yi</keyname><forenames>Xinping</forenames></author><author><keyname>Huang</keyname><forenames>Xiaowei</forenames></author></authors><title>Optimising Event-Driven Spiking Neural Network with Regularisation and   Cutoff</title><categories>cs.CV</categories><journal-ref>Frontiers in Neuroscience, 19 (2025)</journal-ref><doi>10.3389/fnins.2025.1522788</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Spiking neural network (SNN), as the next generation of artificial neural network (ANN), offer a closer mimicry of natural neural networks and hold promise for significant improvements in computational efficiency. However, the current SNN is trained to infer over a fixed duration, overlooking the potential of dynamic inference in SNN. In this paper, we strengthen the marriage between SNN and event-driven processing with a proposal to consider a cutoff in SNN, which can terminate SNN anytime during inference to achieve efficient inference. Two novel optimisation techniques are presented to achieve inference efficient SNN: a Top-K cutoff and a regularisation.The proposed regularisation influences the training process, optimising SNN for the cutoff, while the Top-K cutoff technique optimises the inference phase. We conduct an extensive set of experiments on multiple benchmark frame-based datasets, such asCIFAR10/100, Tiny-ImageNet, and event-based datasets, including CIFAR10-DVS, N-Caltech101 and DVS128 Gesture. The experimental results demonstrate the effectiveness of our techniques in both ANN-to-SNN conversion and direct training, enabling SNNs to require 1.76 to 2.76x fewer timesteps for CIFAR-10, while achieving 1.64 to 1.95x fewer timesteps across all event-based datasets, with near-zero accuracy loss. These findings affirms the compatibility and potential benefits of our techniques in enhancing accuracy and reducing inference latency when integrated with existing methods. Code available: https://github.com/Dengyu-Wu/SNNCutoff </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2303.14581</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2303.14581</id><created>2023-03-25</created><authors><author><keyname>Cohen</keyname><forenames>Joseph</forenames></author><author><keyname>Huan</keyname><forenames>Xun</forenames></author><author><keyname>Ni</keyname><forenames>Jun</forenames></author></authors><title>Shapley-based Explainable AI for Clustering Applications in Fault   Diagnosis and Prognosis</title><categories>cs.LG cs.SY eess.SP eess.SY</categories><comments>23 pages with 8 figures</comments><journal-ref>Journal of Intelligent Manufacturing 35 (2024) 4071-4086</journal-ref><doi>10.1007/s10845-024-02468-2</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Data-driven artificial intelligence models require explainability in intelligent manufacturing to streamline adoption and trust in modern industry. However, recently developed explainable artificial intelligence (XAI) techniques that estimate feature contributions on a model-agnostic level such as SHapley Additive exPlanations (SHAP) have not yet been evaluated for semi-supervised fault diagnosis and prognosis problems characterized by class imbalance and weakly labeled datasets. This paper explores the potential of utilizing Shapley values for a new clustering framework compatible with semi-supervised learning problems, loosening the strict supervision requirement of current XAI techniques. This broad methodology is validated on two case studies: a heatmap image dataset obtained from a semiconductor manufacturing process featuring class imbalance, and a benchmark dataset utilized in the 2021 Prognostics and Health Management (PHM) Data Challenge. Semi-supervised clustering based on Shapley values significantly improves upon clustering quality compared to the fully unsupervised case, deriving information-dense and meaningful clusters that relate to underlying fault diagnosis model predictions. These clusters can also be characterized by high-precision decision rules in terms of original feature values, as demonstrated in the second case study. The rules, limited to 1-2 terms utilizing original feature scales, describe 12 out of the 16 derived equipment failure clusters with precision exceeding 0.85, showcasing the promising utility of the explainable clustering framework for intelligent manufacturing applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2304.02061</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2304.02061</id><created>2023-04-04</created><updated>2025-02-02</updated><authors><author><keyname>Mir</keyname><forenames>Aymen</forenames></author><author><keyname>Puig</keyname><forenames>Xavier</forenames></author><author><keyname>Kanazawa</keyname><forenames>Angjoo</forenames></author><author><keyname>Pons-Moll</keyname><forenames>Gerard</forenames></author></authors><title>Generating Continual Human Motion in Diverse 3D Scenes</title><categories>cs.CV</categories><comments>Webpage: https://virtualhumans.mpi-inf.mpg.de/origin_2/</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce a method to synthesize animator guided human motion across 3D scenes. Given a set of sparse (3 or 4) joint locations (such as the location of a person's hand and two feet) and a seed motion sequence in a 3D scene, our method generates a plausible motion sequence starting from the seed motion while satisfying the constraints imposed by the provided keypoints. We decompose the continual motion synthesis problem into walking along paths and transitioning in and out of the actions specified by the keypoints, which enables long generation of motions that satisfy scene constraints without explicitly incorporating scene information. Our method is trained only using scene agnostic mocap data. As a result, our approach is deployable across 3D scenes with various geometries. For achieving plausible continual motion synthesis without drift, our key contribution is to generate motion in a goal-centric canonical coordinate frame where the next immediate target is situated at the origin. Our model can generate long sequences of diverse actions such as grabbing, sitting and leaning chained together in arbitrary order, demonstrated on scenes of varying geometry: HPS, Replica, Matterport, ScanNet and scenes represented using NeRFs. Several experiments demonstrate that our method outperforms existing methods that navigate paths in 3D scenes. For more results we urge the reader to watch our supplementary video available at: https://www.youtube.com/watch?v=0wZgsdyCT4A&amp;t=1s </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2304.05982</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2304.05982</id><created>2023-03-01</created><updated>2025-01-31</updated><authors><author><keyname>Guastella</keyname><forenames>Davide Andrea</forenames></author><author><keyname>Montero-Porras</keyname><forenames>Eladio</forenames></author><author><keyname>Morales-Hernández</keyname><forenames>Alejandro</forenames></author><author><keyname>Bontempi</keyname><forenames>Gianluca</forenames></author></authors><title>Traffic Modeling with SUMO: a Tutorial</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  This paper presents a step-by-step guide to generating and simulating a traffic scenario using the open-source simulation tool SUMO. It introduces the common pipeline used to generate a synthetic traffic model for SUMO, how to import existing traffic data into a model to achieve accuracy in traffic simulation (that is, producing a traffic model which dynamics is similar to the real one). It also describes how SUMO outputs information from simulation that can be used for data analysis purposes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2305.08194</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2305.08194</id><created>2023-05-14</created><updated>2025-01-31</updated><authors><author><keyname>Poluektov</keyname><forenames>Michael</forenames></author><author><keyname>Polar</keyname><forenames>Andrew</forenames></author></authors><title>Construction of the Kolmogorov-Arnold representation using the   Newton-Kaczmarz method</title><categories>math.NA cs.NA</categories><msc-class>26B40, 41A99, 65D15</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  It is known that any continuous multivariate function can be represented exactly by a composition functions of a single variable - the so-called Kolmogorov-Arnold representation. It can be a convenient tool for tasks where it is required to obtain a predictive model that maps some vector input of a black box system into a scalar output. In this case, the representation may not be exact, and it is more correct to refer to such structure as the Kolmogorov-Arnold model (or, as more recently popularised, 'network'). Construction of such model based on the recorded input-output data is a challenging task. In the present paper, it is suggested to decompose the underlying functions of the representation into continuous basis functions and parameters. It is then proposed to find the parameters using the Newton-Kaczmarz method for solving systems of non-linear equations. The algorithm is then modified to support parallelisation. The paper demonstrates that such approach is also an excellent tool for data-driven solution of partial differential equations. Numerical examples show that for the considered model, the Newton-Kaczmarz method for parameter estimation is efficient and more robust with respect to the section of the initial guess than the straightforward application of the Gauss-Newton method. Finally, the Kolmogorov-Arnold model is compared to the MATLAB's built-in neural networks on a relatively large-scale problem (25 inputs, datasets of 10 million records), significantly outperforming the multilayer perceptrons (MLPs) in this particular problem (4-10 minutes vs. 4-8 hours of training time, as well as higher accuracy, lower CPU usage, and smaller memory footprint). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2305.17473</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2305.17473</id><created>2023-05-27</created><updated>2024-10-24</updated><authors><author><keyname>Shiri</keyname><forenames>Farhad Mortezapour</forenames></author><author><keyname>Perumal</keyname><forenames>Thinagaran</forenames></author><author><keyname>Mustapha</keyname><forenames>Norwati</forenames></author><author><keyname>Mohamed</keyname><forenames>Raihani</forenames></author></authors><title>A Comprehensive Overview and Comparative Analysis on Deep Learning   Models: CNN, RNN, LSTM, GRU</title><categories>cs.LG cs.AI</categories><comments>61 pages, 37 figures</comments><journal-ref>Journal on Artificial Intelligence 2024 Vol. 6 Issue 1 Pages   301-360</journal-ref><doi>10.32604/jai.2024.054314</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Deep learning (DL) has emerged as a powerful subset of machine learning (ML) and artificial intelligence (AI), outperforming traditional ML methods, especially in handling unstructured and large datasets. Its impact spans across various domains, including speech recognition, healthcare, autonomous vehicles, cybersecurity, predictive analytics, and more. However, the complexity and dynamic nature of real-world problems present challenges in designing effective deep learning models. Consequently, several deep learning models have been developed to address different problems and applications. In this article, we conduct a comprehensive survey of various deep learning models, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative Models, Deep Reinforcement Learning (DRL), and Deep Transfer Learning. We examine the structure, applications, benefits, and limitations of each model. Furthermore, we perform an analysis using three publicly available datasets: IMDB, ARAS, and Fruit-360. We compare the performance of six renowned deep learning models: CNN, Simple RNN, Long Short-Term Memory (LSTM), Bidirectional LSTM, Gated Recurrent Unit (GRU), and Bidirectional GRU. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2306.01268</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2306.01268</id><created>2023-06-02</created><updated>2025-02-01</updated><authors><author><keyname>Williams</keyname><forenames>Edward C.</forenames></author><author><keyname>Su</keyname><forenames>Grace</forenames></author><author><keyname>Schloen</keyname><forenames>Sandra R.</forenames></author><author><keyname>Prosser</keyname><forenames>Miller C.</forenames></author><author><keyname>Paulus</keyname><forenames>Susanne</forenames></author><author><keyname>Krishnan</keyname><forenames>Sanjay</forenames></author></authors><title>DeepScribe: Localization and Classification of Elamite Cuneiform Signs   Via Deep Learning</title><categories>cs.CV cs.DL cs.IR</categories><comments>Accepted to ACM JOCCH</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Twenty-five hundred years ago, the paperwork of the Achaemenid Empire was recorded on clay tablets. In 1933, archaeologists from the University of Chicago's Oriental Institute (OI) found tens of thousands of these tablets and fragments during the excavation of Persepolis. Many of these tablets have been painstakingly photographed and annotated by expert cuneiformists, and now provide a rich dataset consisting of over 5,000 annotated tablet images and 100,000 cuneiform sign bounding boxes. We leverage this dataset to develop DeepScribe, a modular computer vision pipeline capable of localizing cuneiform signs and providing suggestions for the identity of each sign. We investigate the difficulty of learning subtasks relevant to cuneiform tablet transcription on ground-truth data, finding that a RetinaNet object detector can achieve a localization mAP of 0.78 and a ResNet classifier can achieve a top-5 sign classification accuracy of 0.89. The end-to-end pipeline achieves a top-5 classification accuracy of 0.80. As part of the classification module, DeepScribe groups cuneiform signs into morphological clusters. We consider how this automatic clustering approach differs from the organization of standard, printed sign lists and what we may learn from it. These components, trained individually, are sufficient to produce a system that can analyze photos of cuneiform tablets from the Achaemenid period and provide useful transliteration suggestions to researchers. We evaluate the model's end-to-end performance on locating and classifying signs, providing a roadmap to a linguistically-aware transliteration system, then consider the model's potential utility when applied to other periods of cuneiform writing. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2306.03303</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2306.03303</id><created>2023-06-05</created><updated>2025-02-02</updated><authors><author><keyname>Cuchiero</keyname><forenames>Christa</forenames></author><author><keyname>Schmocker</keyname><forenames>Philipp</forenames></author><author><keyname>Teichmann</keyname><forenames>Josef</forenames></author></authors><title>Global universal approximation of functional input maps on weighted   spaces</title><categories>stat.ML cs.LG math.FA math.PR q-fin.MF</categories><comments>67 pages, 4 figures</comments><msc-class>26A16, 26E20, 41A65, 41A81, 46E40, 60L10, 68T07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce so-called functional input neural networks defined on a possibly infinite dimensional weighted space with values also in a possibly infinite dimensional output space. To this end, we use an additive family to map the input weighted space to the hidden layer, on which a non-linear scalar activation function is applied to each neuron, and finally return the output via some linear readouts. Relying on Stone-Weierstrass theorems on weighted spaces, we can prove a global universal approximation result on weighted spaces for continuous functions going beyond the usual approximation on compact sets. This then applies in particular to approximation of (non-anticipative) path space functionals via functional input neural networks. As a further application of the weighted Stone-Weierstrass theorem we prove a global universal approximation result for linear functions of the signature. We also introduce the viewpoint of Gaussian process regression in this setting and emphasize that the reproducing kernel Hilbert space of the signature kernels are Cameron-Martin spaces of certain Gaussian processes. This paves a way towards uncertainty quantification for signature kernel regression. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2306.04802</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2306.04802</id><created>2023-06-07</created><updated>2025-02-02</updated><authors><author><keyname>Cui</keyname><forenames>Hejie</forenames></author><author><keyname>Lu</keyname><forenames>Jiaying</forenames></author><author><keyname>Xu</keyname><forenames>Ran</forenames></author><author><keyname>Wang</keyname><forenames>Shiyu</forenames></author><author><keyname>Ma</keyname><forenames>Wenjing</forenames></author><author><keyname>Yu</keyname><forenames>Yue</forenames></author><author><keyname>Yu</keyname><forenames>Shaojun</forenames></author><author><keyname>Kan</keyname><forenames>Xuan</forenames></author><author><keyname>Ling</keyname><forenames>Chen</forenames></author><author><keyname>Zhao</keyname><forenames>Liang</forenames></author><author><keyname>Qin</keyname><forenames>Zhaohui S.</forenames></author><author><keyname>Ho</keyname><forenames>Joyce C.</forenames></author><author><keyname>Fu</keyname><forenames>Tianfan</forenames></author><author><keyname>Ma</keyname><forenames>Jing</forenames></author><author><keyname>Huai</keyname><forenames>Mengdi</forenames></author><author><keyname>Wang</keyname><forenames>Fei</forenames></author><author><keyname>Yang</keyname><forenames>Carl</forenames></author></authors><title>A Review on Knowledge Graphs for Healthcare: Resources, Applications,   and Promises</title><categories>cs.AI cs.CL cs.LG cs.SI</categories><comments>Preprint under review</comments><msc-class>68T30, 68T50, 68T09</msc-class><acm-class>I.2.4; I.2.6; I.2.7; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This comprehensive review aims to provide an overview of the current state of Healthcare Knowledge Graphs (HKGs), including their construction, utilization models, and applications across various healthcare and biomedical research domains. We thoroughly analyzed existing literature on HKGs, covering their construction methodologies, utilization techniques, and applications in basic science research, pharmaceutical research and development, clinical decision support, and public health. The review encompasses both model-free and model-based utilization approaches and the integration of HKGs with large language models (LLMs). We searched Google Scholar for relevant papers on HKGs and classified them into the following topics: HKG construction, HKG utilization, and their downstream applications in various domains. We also discussed their special challenges and the promise for future work. The review highlights the potential of HKGs to significantly impact biomedical research and clinical practice by integrating vast amounts of biomedical knowledge from multiple domains. The synergy between HKGs and LLMs offers promising opportunities for constructing more comprehensive knowledge graphs and improving the accuracy of healthcare applications. HKGs have emerged as a powerful tool for structuring medical knowledge, with broad applications across biomedical research, clinical decision-making, and public health. This survey serves as a roadmap for future research and development in the field of HKGs, highlighting the potential of combining knowledge graphs with advanced machine learning models for healthcare transformation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2306.12968</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2306.12968</id><created>2023-06-18</created><updated>2025-02-02</updated><authors><author><keyname>Ariu</keyname><forenames>Kaito</forenames></author><author><keyname>Proutiere</keyname><forenames>Alexandre</forenames></author><author><keyname>Yun</keyname><forenames>Se-Young</forenames></author></authors><title>Revisiting Instance-Optimal Cluster Recovery in the Labeled Stochastic   Block Model</title><categories>cs.SI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the problem of recovering hidden communities in the Labeled Stochastic Block Model (LSBM) with a finite number of clusters whose sizes grow linearly with the total number of nodes. We derive the necessary and sufficient conditions under which the expected number of misclassified nodes is less than $ s $, for any number $ s = o(n) $. To achieve this, we propose IAC (Instance-Adaptive Clustering), the first algorithm whose performance matches the instance-specific lower bounds both in expectation and with high probability. IAC is a novel two-phase algorithm that consists of a one-shot spectral clustering step followed by iterative likelihood-based cluster assignment improvements. This approach is based on the instance-specific lower bound and notably does not require any knowledge of the model parameters, including the number of clusters. By performing the spectral clustering only once, IAC maintains an overall computational complexity of $ \mathcal{O}(n\, \text{polylog}(n)) $, making it scalable and practical for large-scale problems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2307.00438</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2307.00438</id><created>2023-07-01</created><updated>2025-02-01</updated><authors><author><keyname>Kulkarni</keyname><forenames>Pranav</forenames></author><author><keyname>Kanhere</keyname><forenames>Adway</forenames></author><author><keyname>Siegel</keyname><forenames>Eliot</forenames></author><author><keyname>Yi</keyname><forenames>Paul H.</forenames></author><author><keyname>Parekh</keyname><forenames>Vishwa S.</forenames></author></authors><title>Towards Resource-Efficient Streaming of Large-Scale Medical Image   Datasets for Deep Learning</title><categories>cs.CV cs.IR cs.LG</categories><comments>15 pages, 5 figures, submitted to MIDL'25</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large-scale medical imaging datasets have accelerated deep learning (DL) for medical image analysis. However, the large scale of these datasets poses a challenge for researchers, resulting in increased storage and bandwidth requirements for hosting and accessing them. Since different researchers have different use cases and require different resolutions or formats for DL, it is neither feasible to anticipate every researcher's needs nor practical to store data in multiple resolutions and formats. To that end, we propose the Medical Image Streaming Toolkit (MIST), a format-agnostic database that enables streaming of medical images at different resolutions and formats from a single high-resolution copy. We evaluated MIST across eight popular, large-scale medical imaging datasets spanning different body parts, modalities, and formats. Our results showed that our framework reduced the storage and bandwidth requirements for hosting and downloading datasets without impacting image quality. We demonstrate that MIST addresses the challenges posed by large-scale medical imaging datasets by building a data-efficient and format-agnostic database to meet the diverse needs of researchers and reduce barriers to DL research in medical imaging. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2307.02131</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2307.02131</id><created>2023-07-05</created><updated>2025-02-02</updated><authors><author><keyname>Tanyel</keyname><forenames>Toygar</forenames></author><author><keyname>Ayvaz</keyname><forenames>Serkan</forenames></author><author><keyname>Keserci</keyname><forenames>Bilgin</forenames></author></authors><title>Beyond Known Reality: Exploiting Counterfactual Explanations for Medical   Research</title><categories>cs.AI</categories><acm-class>J.3</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The field of explainability in artificial intelligence (AI) has witnessed a growing number of studies and increasing scholarly interest. However, the lack of human-friendly and individual interpretations in explaining the outcomes of machine learning algorithms has significantly hindered the acceptance of these methods by clinicians in their research and clinical practice. To address this issue, our study uses counterfactual explanations to explore the applicability of "what if?" scenarios in medical research. Our aim is to expand our understanding of magnetic resonance imaging (MRI) features used for diagnosing pediatric posterior fossa brain tumors beyond existing boundaries. In our case study, the proposed concept provides a novel way to examine alternative decision-making scenarios that offer personalized and context-specific insights, enabling the validation of predictions and clarification of variations under diverse circumstances. Additionally, we explore the potential use of counterfactuals for data augmentation and evaluate their feasibility as an alternative approach in our medical research case. The results demonstrate the promising potential of using counterfactual explanations to improve AI-driven methods in clinical research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2307.12904</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2307.12904</id><created>2023-07-24</created><updated>2025-02-03</updated><authors><author><keyname>Gonon</keyname><forenames>Lukas</forenames></author><author><keyname>Jacquier</keyname><forenames>Antoine</forenames></author></authors><title>Universal Approximation Theorem and error bounds for quantum neural   networks and quantum reservoirs</title><categories>quant-ph cs.LG math.PR</categories><msc-class>68Q12, 68T07, 65D15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Universal approximation theorems are the foundations of classical neural networks, providing theoretical guarantees that the latter are able to approximate maps of interest. Recent results have shown that this can also be achieved in a quantum setting, whereby classical functions can be approximated by parameterised quantum circuits. We provide here precise error bounds for specific classes of functions and extend these results to the interesting new setup of randomised quantum circuits, mimicking classical reservoir neural networks. Our results show in particular that a quantum neural network with $\mathcal{O}(\varepsilon^{-2})$ weights and $\mathcal{O} (\lceil \log_2(\varepsilon^{-1}) \rceil)$ qubits suffices to achieve accuracy $\varepsilon&gt;0$ when approximating functions with integrable Fourier transform. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2309.09671</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2309.09671</id><created>2023-09-18</created><updated>2025-02-03</updated><authors><author><keyname>Shamash</keyname><forenames>Elisheva</forenames></author><author><keyname>Fan</keyname><forenames>Zhong</forenames></author></authors><title>Contract Design for V2G Smart Energy Trading</title><categories>cs.GT</categories><comments>There were mistakes in some of the proofs</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The transition to a net zero energy system necessitates development in a number of directions including developing advanced electricity trading markets. Due to electricity markets being responsible for a large portion of carbon emissions, improving the electricity markets' method for determining energy transactions could have a significant impact on carbon reductions and thus facilitate this transition. V2X technology can be applied to regulate different energy markets, and thus reduce costs and carbon emissions by using the batteries in electric vehicles to store energy during off-peak hours and export it during peak hours.   We develop a novel contract based on the VCG-mechanism, for exporting and importing electricity effectively, and show how this mechanism can raise efficiency, facilitate the development of a sustainable and efficient electricity market, and bring us nearer to our Net Zero Goal. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2309.14846</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2309.14846</id><created>2023-09-26</created><updated>2023-10-02</updated><authors><author><keyname>Chen</keyname><forenames>Zimin</forenames></author><author><keyname>Fang</keyname><forenames>Sen</forenames></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames></author></authors><title>Supersonic: Learning to Generate Source Code Optimizations in C/C++</title><categories>cs.SE cs.AI cs.LG</categories><journal-ref>IEEE Transactions on Software Engineering, 2024</journal-ref><doi>10.1109/TSE.2024.3423769</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Software optimization refines programs for resource efficiency while preserving functionality. Traditionally, it is a process done by developers and compilers. This paper introduces a third option, automated optimization at the source code level. We present Supersonic, a neural approach targeting minor source code modifications for optimization. Using a seq2seq model, Supersonic is trained on C/C++ program pairs ($x_{t}$, $x_{t+1}$), where $x_{t+1}$ is an optimized version of $x_{t}$, and outputs a diff. Supersonic's performance is benchmarked against OpenAI's GPT-3.5-Turbo and GPT-4 on competitive programming tasks. The experiments show that Supersonic not only outperforms both models on the code optimization task but also minimizes the extent of the change with a model more than 600x smaller than GPT-3.5-Turbo and 3700x smaller than GPT-4. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2309.16326</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2309.16326</id><created>2023-09-28</created><updated>2025-02-02</updated><authors><author><keyname>Bae</keyname><forenames>Gi-Chan</forenames></author><author><keyname>Pirner</keyname><forenames>Marlies</forenames></author><author><keyname>Warnecke</keyname><forenames>Sandra</forenames></author></authors><title>Numerical schemes for a multi-species quantum BGK model</title><categories>math.NA cs.NA</categories><comments>arXiv admin note: text overlap with arXiv:2202.05652</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This work is devoted to the numerical implementation of the quantum Bhatnagar- Gross-Krook (BGK) model for gas mixtures consisting of classical and quantum particles (fermions, bosons). We consider the model proposed by Bae, Klingenberg, Pirner, and Yun in 2021 and implement an Implicit-Explicit (IMEX) scheme due to the stiffness of the collision operator. A major obstacle is updating the parameters of quantum local equilibrium, which requires computing by inverting the relation between density and energy at every grid point in space and time. We address this difficulty by using the Lagrange multiplier method to minimize a potential function subject to constraints defined by specific moment equalities. Moreover, we analyze the convergence of mean velocity and temperature between the species both analytically and numerically. When a quantum component is included, we observe that the converging quantity is physical temperature, not the kinetic temperature. This differs from the mixture of classical species. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.02138</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.02138</id><created>2023-10-03</created><updated>2024-01-22</updated><authors><author><keyname>Deckelnick</keyname><forenames>Klaus</forenames></author><author><keyname>Nürnberg</keyname><forenames>Robert</forenames></author></authors><title>Discrete anisotropic curve shortening flow in higher codimension</title><categories>math.NA cs.NA</categories><comments>30 pages, 11 figures</comments><journal-ref>IMA J. Numer. Anal. 45 (2025) 36--67</journal-ref><doi>10.1093/imanum/drae015</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We introduce a novel formulation for the evolution of parametric curves by anisotropic curve shortening flow in ${\mathbb R}^d$, $d\geq2$. The reformulation hinges on a suitable manipulation of the parameterization's tangential velocity, leading to a strictly parabolic differential equation. Moreover, the derived equation is in divergence form, giving rise to a natural variational numerical method. For a fully discrete finite element approximation based on piecewise linear elements we prove optimal error estimates. Numerical simulations confirm the theoretical results and demonstrate the practicality of the method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.04022</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.04022</id><created>2023-10-06</created><updated>2025-01-31</updated><authors><author><keyname>Adler</keyname><forenames>James H.</forenames></author><author><keyname>Andrei</keyname><forenames>Anca S.</forenames></author><author><keyname>Atherton</keyname><forenames>Timothy J.</forenames></author></authors><title>Nonlinear Methods for Shape Optimization Problems in Liquid Crystal   Tactoids</title><categories>math.NA cs.NA</categories><msc-class>76A15, 49M15, 65N30, 65N22, 65N55, 65K10</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Anisotropic fluids, such as nematic liquid crystals, can form non-spherical equilibrium shapes known as tactoids. Predicting the shape of these structures as a function of material parameters is challenging and paradigmatic of a broader class of problems that combine shape and order. Here, we consider a discrete shape optimization approach with finite elements to find the configuration of two-dimensional and three-dimensional tactoids using the Landau--de Gennes framework and a Q-tensor representation. Efficient solution of the resulting constrained energy minimization problem is achieved using a quasi-Newton and nested iteration algorithm. Numerical validation is performed with benchmark solutions and compared against experimental data and earlier work. We explore physically motivated subproblems, whereby the shape and order are separately held fixed, respectively, to explore the role of both and examine material parameter dependence of the convergence. Nested iteration significantly improves both the computational cost and convergence of numerical solutions of these highly deformable materials. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.05014</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.05014</id><created>2023-10-08</created><updated>2025-02-02</updated><authors><author><keyname>Kim</keyname><forenames>Dohan</forenames></author></authors><title>Congruence Closure Modulo Groups</title><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents a new framework for constructing congruence closure of a finite set of ground equations over uninterpreted symbols and interpreted symbols for the group axioms. In this framework, ground equations are flattened into certain forms by introducing new constants, and a completion procedure is performed on ground flat equations. The proposed completion procedure uses equational inference rules and constructs a ground convergent rewrite system for congruence closure with such interpreted symbols. If the completion procedure terminates, then it yields a decision procedure for the word problem for a finite set of ground equations with respect to the group axioms. This paper also provides a sufficient terminating condition of the completion procedure for constructing a ground convergent rewrite system from ground flat equations containing interpreted symbols for the group axioms. In addition, this paper presents a new method for constructing congruence closure of a finite set of ground equations containing interpreted symbols for the semigroup, monoid, and the multiple disjoint sets of group axioms, respectively, using the proposed framework. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.05858</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.05858</id><created>2023-10-09</created><updated>2025-02-01</updated><authors><author><keyname>Duan</keyname><forenames>Jingliang</forenames></author><author><keyname>Wang</keyname><forenames>Wenxuan</forenames></author><author><keyname>Xiao</keyname><forenames>Liming</forenames></author><author><keyname>Gao</keyname><forenames>Jiaxin</forenames></author><author><keyname>Li</keyname><forenames>Shengbo Eben</forenames></author><author><keyname>Liu</keyname><forenames>Chang</forenames></author><author><keyname>Zhang</keyname><forenames>Ya-Qin</forenames></author><author><keyname>Cheng</keyname><forenames>Bo</forenames></author><author><keyname>Li</keyname><forenames>Keqiang</forenames></author></authors><title>Distributional Soft Actor-Critic with Three Refinements</title><categories>cs.LG cs.SY eess.SY</categories><journal-ref>IEEE Transactions on Pattern Analysis and Machine Intelligence,   2025</journal-ref><doi>10.1109/TPAMI.2025.3537087</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reinforcement learning (RL) has shown remarkable success in solving complex decision-making and control tasks. However, many model-free RL algorithms experience performance degradation due to inaccurate value estimation, particularly the overestimation of Q-values, which can lead to suboptimal policies. To address this issue, we previously proposed the Distributional Soft Actor-Critic (DSAC or DSACv1), an off-policy RL algorithm that enhances value estimation accuracy by learning a continuous Gaussian value distribution. Despite its effectiveness, DSACv1 faces challenges such as training instability and sensitivity to reward scaling, caused by high variance in critic gradients due to return randomness. In this paper, we introduce three key refinements to DSACv1 to overcome these limitations and further improve Q-value estimation accuracy: expected value substitution, twin value distribution learning, and variance-based critic gradient adjustment. The enhanced algorithm, termed DSAC with Three refinements (DSAC-T or DSACv2), is systematically evaluated across a diverse set of benchmark tasks. Without the need for task-specific hyperparameter tuning, DSAC-T consistently matches or outperforms leading model-free RL algorithms, including SAC, TD3, DDPG, TRPO, and PPO, in all tested environments. Additionally, DSAC-T ensures a stable learning process and maintains robust performance across varying reward scales. Its effectiveness is further demonstrated through real-world application in controlling a wheeled robot, highlighting its potential for deployment in practical robotic tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.07726</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.07726</id><created>2023-09-27</created><updated>2025-02-03</updated><authors><author><keyname>Li</keyname><forenames>Guanlin</forenames></author><author><keyname>Chen</keyname><forenames>Yifei</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author><author><keyname>Guo</keyname><forenames>Shangwei</forenames></author><author><keyname>Qiu</keyname><forenames>Han</forenames></author><author><keyname>Wang</keyname><forenames>Guoyin</forenames></author><author><keyname>Li</keyname><forenames>Jiwei</forenames></author><author><keyname>Zhang</keyname><forenames>Tianwei</forenames></author></authors><title>Warfare:Breaking the Watermark Protection of AI-Generated Content</title><categories>cs.CV cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  AI-Generated Content (AIGC) is rapidly expanding, with services using advanced generative models to create realistic images and fluent text. Regulating such content is crucial to prevent policy violations, such as unauthorized commercialization or unsafe content distribution. Watermarking is a promising solution for content attribution and verification, but we demonstrate its vulnerability to two key attacks: (1) Watermark removal, where adversaries erase embedded marks to evade regulation, and (2) Watermark forging, where they generate illicit content with forged watermarks, leading to misattribution. We propose Warfare, a unified attack framework leveraging a pre-trained diffusion model for content processing and a generative adversarial network for watermark manipulation. Evaluations across datasets and embedding setups show that Warfare achieves high success rates while preserving content quality. We further introduce Warfare-Plus, which enhances efficiency without compromising effectiveness. The code can be found in https://github.com/GuanlinLee/warfare. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.08784</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.08784</id><created>2023-10-12</created><updated>2025-02-02</updated><authors><author><keyname>Caselles</keyname><forenames>Pol</forenames></author><author><keyname>Ramon</keyname><forenames>Eduard</forenames></author><author><keyname>Garcia</keyname><forenames>Jaime</forenames></author><author><keyname>Triginer</keyname><forenames>Gil</forenames></author><author><keyname>Moreno-Noguer</keyname><forenames>Francesc</forenames></author></authors><title>Implicit Shape and Appearance Priors for Few-Shot Full Head   Reconstruction</title><categories>cs.CV</categories><comments>Accepted at IEEE Transactions on Pattern Analysis and Machine   Intelligence (TPAMI) 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advancements in learning techniques that employ coordinate-based neural representations have yielded remarkable results in multi-view 3D reconstruction tasks. However, these approaches often require a substantial number of input views (typically several tens) and computationally intensive optimization procedures to achieve their effectiveness. In this paper, we address these limitations specifically for the problem of few-shot full 3D head reconstruction. We accomplish this by incorporating a probabilistic shape and appearance prior into coordinate-based representations, enabling faster convergence and improved generalization when working with only a few input images (even as low as a single image). During testing, we leverage this prior to guide the fitting process of a signed distance function using a differentiable renderer. By incorporating the statistical prior alongside parallelizable ray tracing and dynamic caching strategies, we achieve an efficient and accurate approach to few-shot full 3D head reconstruction. Moreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D full head scans and their corresponding posed images and masks, which we use for evaluation purposes. By leveraging this dataset, we demonstrate the remarkable capabilities of our approach in achieving state-of-the-art results in geometry reconstruction while being an order of magnitude faster than previous approaches. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.13404</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.13404</id><created>2023-10-20</created><authors><author><keyname>Jedrusiak</keyname><forenames>Mikel D.</forenames></author><author><keyname>Harweg</keyname><forenames>Thomas</forenames></author><author><keyname>Haselhoff</keyname><forenames>Timo</forenames></author><author><keyname>Lawrence</keyname><forenames>Bryce T.</forenames></author><author><keyname>Moebus</keyname><forenames>Susanne</forenames></author><author><keyname>Weichert</keyname><forenames>Frank</forenames></author></authors><title>Definition-independent Formalization of Soundscapes: Towards a Formal   Methodology</title><categories>cs.SD cs.CV eess.AS</categories><doi>10.1121/10.0025543</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Soundscapes have been studied by researchers from various disciplines, each with different perspectives, goals, approaches, and terminologies. Accordingly, depending on the field, the concept of a soundscape's components changes, consequently changing the basic definition. This results in complicating interdisciplinary communication and comparison of results. Especially when soundscape-unrelated research areas are involved. For this reason, we present a potential formalization that is independent of the underlying soundscape definition, with the goal of being able to capture the heterogeneous structure of the data as well as the different ideologies in one model. In an exemplary analysis of frequency correlation matrices for land use type detection as an alternative to features like MFCCs, we show a practical application of our presented formalization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.14045</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.14045</id><created>2023-10-21</created><updated>2025-02-02</updated><authors><author><keyname>Avrutskiy</keyname><forenames>Vsevolod I.</forenames></author></authors><title>Training Image Derivatives: Increased Accuracy and Universal Robustness</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Derivative training is an established method that can significantly increase the accuracy of neural networks in certain low-dimensional tasks. In this paper, we extend this improvement to an illustrative image analysis problem: reconstructing the vertices of a cube from its image. By training the derivatives with respect to the cube's six degrees of freedom, we achieve a 25-fold increase in accuracy for noiseless inputs. Additionally, derivative knowledge offers a novel approach to enhancing network robustness, which has traditionally been understood in terms of two types of vulnerabilities: excessive sensitivity to minor perturbations and failure to detect significant image changes. Conventional robust training relies on output invariance, which inherently creates a trade-off between these two vulnerabilities. By leveraging derivative information we compute non-trivial output changes in response to arbitrary input perturbations. This resolves the trade-off, yielding a network that is twice as robust and five times more accurate than the best case under the invariance assumption. Unlike conventional robust training, this outcome can be further improved by simply increasing the network capacity. This approach is applicable to phase retrieval problems and other scenarios where a sufficiently smooth manifold parametrization can be obtained. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.18493</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.18493</id><created>2023-10-27</created><updated>2025-02-01</updated><authors><author><keyname>Tsai</keyname><forenames>Ping-Hsuan</forenames><affiliation>Virginia Tech</affiliation></author><author><keyname>Chung</keyname><forenames>Seung Whan</forenames><affiliation>Lawrence Livermore National Laboratory</affiliation></author><author><keyname>Ghosh</keyname><forenames>Debojyoti</forenames><affiliation>Lawrence Livermore National Laboratory</affiliation></author><author><keyname>Loffeld</keyname><forenames>John</forenames><affiliation>Lawrence Livermore National Laboratory</affiliation></author><author><keyname>Choi</keyname><forenames>Youngsoo</forenames><affiliation>Lawrence Livermore National Laboratory</affiliation></author><author><keyname>Belof</keyname><forenames>Jonathan L.</forenames><affiliation>Lawrence Livermore National Laboratory</affiliation></author></authors><title>Accelerating Kinetic Simulations of Electrostatic Plasmas with   Reduced-Order Modeling</title><categories>math.NA cs.NA physics.plasm-ph</categories><comments>25 pages, 55 figures; A new version of paper that is submitted to a   journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite advancements in high-performance computing and modern numerical algorithms, computational cost remains prohibitive for multi-query kinetic plasma simulations. In this work, we develop data-driven reduced-order models (ROMs) for collisionless electrostatic plasma dynamics, based on the kinetic Vlasov-Poisson equation. Our ROM approach projects the equation onto a linear subspace defined by the proper orthogonal decomposition (POD) modes. We introduce an efficient tensorial method to update the nonlinear term using a precomputed third-order tensor. We capture multiscale behavior with a minimal number of POD modes by decomposing the solution manifold into multiple time windows and creating temporally local ROMs. We consider two strategies for decomposition: one based on the physical time and the other based on the electric field energy. Applied to the 1D1V Vlasov-Poisson simulations, that is, prescribed E-field, Landau damping, and two-stream instability, we demonstrate that our ROMs accurately capture the total energy of the system both for parametric and time extrapolation cases. The temporally local ROMs are more efficient and accurate than the single ROM. In addition, in the two-stream instability case, we show that the energy-windowing reduced-order model (EW-ROM) is more efficient and accurate than the time-windowing reduced-order model (TW-ROM). With the tensorial approach, EW-ROM solves the equation approximately 90 times faster than Eulerian simulations while maintaining a maximum relative error of 7.5% for the training data and 11% for the testing data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.18820</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.18820</id><created>2023-10-28</created><updated>2025-02-02</updated><authors><author><keyname>Lakshminarayana</keyname><forenames>Subhash</forenames></author><author><keyname>Maple</keyname><forenames>Carsten</forenames></author><author><keyname>Larkins</keyname><forenames>Andrew</forenames></author><author><keyname>Flack</keyname><forenames>Daryl</forenames></author><author><keyname>Few</keyname><forenames>Christopher</forenames></author><author><keyname>David</keyname><forenames>Kenny-Awuson</forenames></author><author><keyname>Srivastava</keyname><forenames>Anurag. K.</forenames></author></authors><title>Cybersecurity Threats to Power Grid Operations from the Demand-Side   Response Ecosystem</title><categories>cs.CR cs.IT cs.SY eess.SY math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article focuses on cyber security threats from IoT-enabled energy smart appliances (ESAs) such as smart heat pumps, electric vehicle chargers, etc., to power grid operations. It presents an in-depth analysis of the demand side threats, including (i) an overview of the vulnerabilities in ESAs and the wider risk from the demand-side response (DSR) ecosystem, (ii) key factors influencing the attack impact on power grid operations, (iii) measures to improve the cyber-physical resilience of power grids, putting them in the context of ongoing efforts from the industry and regulatory bodies worldwide. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.19774</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.19774</id><created>2023-10-30</created><updated>2024-08-08</updated><authors><author><keyname>Prasad</keyname><forenames>Tushita</forenames></author><author><keyname>Grassl</keyname><forenames>Markus</forenames></author></authors><title>Codes for entanglement-assisted classical communication</title><categories>quant-ph cs.IT math.IT</categories><comments>7 pages, 2 figures</comments><journal-ref>npj Quantum Information, vol. 11, article 13, 2005</journal-ref><doi>10.1038/s41534-024-00954-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entanglement-assisted classical communication (EACC) aims to enhance communication systems using entanglement as an additional resource. However, there is a scarcity of explicit protocols designed for finite transmission scenarios, which presents a challenge for real-world implementation. In response we introduce a new EACC scheme capable of correcting a fixed number of erasures/errors. It can be adjusted to the available amount of entanglement and sends classical information over a quantum channel. We establish a general framework to accomplish such a task by reducing it to a classical problem. Comparing with specific bounds we identify optimal parameter ranges. The scheme requires only the implementation of super-dense coding which has been demonstrated successfully in experiments. Furthermore, our results shows that an adaptable entanglement use confers a communication advantage. Overall, our work sheds light on how entanglement can elevate various finite-length communication protocols, opening new avenues for exploration in the field. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2311.02787</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2311.02787</id><created>2023-11-05</created><updated>2025-02-01</updated><authors><author><keyname>You</keyname><forenames>Yang</forenames></author><author><keyname>Shen</keyname><forenames>Bokui</forenames></author><author><keyname>Deng</keyname><forenames>Congyue</forenames></author><author><keyname>Geng</keyname><forenames>Haoran</forenames></author><author><keyname>Wei</keyname><forenames>Songlin</forenames></author><author><keyname>Wang</keyname><forenames>He</forenames></author><author><keyname>Guibas</keyname><forenames>Leonidas</forenames></author></authors><title>Make a Donut: Hierarchical EMD-Space Planning for Zero-Shot Deformable   Manipulation with Tools</title><categories>cs.RO cs.AI</categories><comments>8 pages. IEEE Robotics and Automation Letters (RA-L). Preprint   Version. Accepted January, 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deformable object manipulation stands as one of the most captivating yet formidable challenges in robotics. While previous techniques have predominantly relied on learning latent dynamics through demonstrations, typically represented as either particles or images, there exists a pertinent limitation: acquiring suitable demonstrations, especially for long-horizon tasks, can be elusive. Moreover, basing learning entirely on demonstrations can hamper the model's ability to generalize beyond the demonstrated tasks. In this work, we introduce a demonstration-free hierarchical planning approach capable of tackling intricate long-horizon tasks without necessitating any training. We employ large language models (LLMs) to articulate a high-level, stage-by-stage plan corresponding to a specified task. For every individual stage, the LLM provides both the tool's name and the Python code to craft intermediate subgoal point clouds. With the tool and subgoal for a particular stage at our disposal, we present a granular closed-loop model predictive control strategy. This leverages Differentiable Physics with Point-to-Point correspondence (DiffPhysics-P2P) loss in the earth mover distance (EMD) space, applied iteratively. Experimental findings affirm that our technique surpasses multiple benchmarks in dough manipulation, spanning both short and long horizons. Remarkably, our model demonstrates robust generalization capabilities to novel and previously unencountered complex tasks without any preliminary demonstrations. We further substantiate our approach with experimental trials on real-world robotic platforms. Our project page: https://qq456cvb.github.io/projects/donut. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2311.02833</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2311.02833</id><created>2023-11-05</created><updated>2024-04-04</updated><authors><author><keyname>Kabra</keyname><forenames>Aditi</forenames></author><author><keyname>Laurent</keyname><forenames>Jonathan</forenames></author><author><keyname>Mitsch</keyname><forenames>Stefan</forenames></author><author><keyname>Platzer</keyname><forenames>André</forenames></author></authors><title>CESAR: Control Envelope Synthesis via Angelic Refinements</title><categories>eess.SY cs.LO cs.SY</categories><journal-ref>TACAS 2024. Lecture Notes in Computer Science, vol 14570.   Springer, Cham. pp. 144-164</journal-ref><doi>10.1007/978-3-031-57246-3_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an approach for synthesizing provably correct control envelopes for hybrid systems. Control envelopes characterize families of safe controllers and are used to monitor untrusted controllers at runtime. Our algorithm fills in the blanks of a hybrid system's sketch specifying the desired shape of the control envelope, the possible control actions, and the system's differential equations. In order to maximize the flexibility of the control envelope, the synthesized conditions saying which control action can be chosen when should be as permissive as possible while establishing a desired safety condition from the available assumptions, which are augmented if needed. An implicit, optimal solution to this synthesis problem is characterized using hybrid systems game theory, from which explicit solutions can be derived via symbolic execution and sound, systematic game refinements. Optimality can be recovered in the face of approximation via a dual game characterization. The resulting algorithm, Control Envelope Synthesis via Angelic Refinements (CESAR), is demonstrated in a range of safe control synthesis examples with different control challenges. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2311.07283</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2311.07283</id><created>2023-11-13</created><updated>2025-01-31</updated><authors><author><keyname>Williams</keyname><forenames>Elizabeth</forenames></author><author><keyname>Gartner</keyname><forenames>Daniel</forenames></author><author><keyname>Harper</keyname><forenames>Paul</forenames></author></authors><title>Predictive and Prescriptive Analytics for Multi-Site Modeling of Frail   and Elderly Patient Services</title><categories>cs.LG math.OC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent research has highlighted the potential of linking predictive and prescriptive analytics. However, it remains widely unexplored how both paradigms could benefit from one another to address today's major challenges in healthcare. One of these is smarter planning of resource capacities for frail and elderly inpatient wards, addressing the societal challenge of an aging population. Frail and elderly patients typically suffer from multimorbidity and require more care while receiving medical treatment. The aim of this research is to assess how various predictive and prescriptive analytical methods, both individually and in tandem, contribute to addressing the operational challenges within an area of healthcare that is growing in demand. Clinical and demographic patient attributes are gathered from more than 165,000 patient records and used to explain and predict length of stay. To that extent, we employ Classification and Regression Trees (CART) analysis to establish this relationship. On the prescriptive side, deterministic and two-stage stochastic programs are developed to determine how to optimally plan for beds and ward staff with the objective to minimize cost. Furthermore, the two analytical methodologies are linked by generating demand for the prescriptive models using the CART groupings. The results show the linked methodologies provided different but similar results compared to using averages and in doing so, captured a more realistic real-world variation in the patient length of stay. Our research reveals that healthcare managers should consider using predictive and prescriptive models to make more informed decisions. By combining predictive and prescriptive analytics, healthcare managers can move away from relying on averages and incorporate the unique characteristics of their patients to create more robust planning decisions, mitigating risks caused by variations in demand. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2311.08820</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2311.08820</id><created>2023-11-15</created><updated>2025-02-03</updated><authors><author><keyname>Airaldi</keyname><forenames>Filippo</forenames></author><author><keyname>De Schutter</keyname><forenames>Bart</forenames></author><author><keyname>Dabiri</keyname><forenames>Azita</forenames></author></authors><title>Reinforcement Learning with Model Predictive Control for Highway Ramp   Metering</title><categories>eess.SY cs.AI cs.SY</categories><comments>17 pages, 10 figures, 3 tables, submitted to IEEE Transactions on   Intelligent Transportation Systems</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the backdrop of an increasingly pressing need for effective urban and highway transportation systems, this work explores the synergy between model-based and learning-based strategies to enhance traffic flow management by use of an innovative approach to the problem of ramp metering control that embeds Reinforcement Learning (RL) techniques within the Model Predictive Control (MPC) framework. The control problem is formulated as an RL task by crafting a suitable stage cost function that is representative of the traffic conditions, variability in the control action, and violations of the constraint on the maximum number of vehicles in queue. An MPC-based RL approach, which leverages the MPC optimal problem as a function approximation for the RL algorithm, is proposed to learn to efficiently control an on-ramp and satisfy its constraints despite uncertainties in the system model and variable demands. Simulations are performed on a benchmark small-scale highway network to compare the proposed methodology against other state-of-the-art control approaches. Results show that, starting from an MPC controller that has an imprecise model and is poorly tuned, the proposed methodology is able to effectively learn to improve the control policy such that congestion in the network is reduced and constraints are satisfied, yielding an improved performance that is superior to the other controllers. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2311.14323</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2311.14323</id><created>2023-11-24</created><updated>2025-01-31</updated><authors><author><keyname>Li</keyname><forenames>Zhiteng</forenames></author><author><keyname>Zhang</keyname><forenames>Yulun</forenames></author><author><keyname>Lin</keyname><forenames>Jing</forenames></author><author><keyname>Qin</keyname><forenames>Haotong</forenames></author><author><keyname>Gu</keyname><forenames>Jinjin</forenames></author><author><keyname>Yuan</keyname><forenames>Xin</forenames></author><author><keyname>Kong</keyname><forenames>Linghe</forenames></author><author><keyname>Yang</keyname><forenames>Xiaokang</forenames></author></authors><title>BinaryHPE: 3D Human Pose and Shape Estimation via Binarization</title><categories>cs.CV</categories><comments>The code will be available at https://github.com/ZHITENGLI/BiDRN</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  3D human pose and shape estimation (HPE) aims to reconstruct the 3D human body, face, and hands from a single image. Although powerful deep learning models have achieved accurate estimation in this task, they require enormous memory and computational resources. Consequently, these methods can hardly be deployed on resource-limited edge devices. In this work, we propose BinaryHPE, a novel binarization method designed to estimate the 3D human body, face, and hands parameters efficiently. Specifically, we propose a novel binary backbone called Binarized Dual Residual Network (BiDRN), designed to retain as much full-precision information as possible. Furthermore, we propose the Binarized BoxNet, an efficient sub-network for predicting face and hands bounding boxes, which further reduces model redundancy. Comprehensive quantitative and qualitative experiments demonstrate the effectiveness of BinaryHPE, which has a significant improvement over state-of-the-art binarization algorithms. Moreover, our BinaryHPE achieves comparable performance with the full-precision method Hand4Whole while using only 22.1% parameters and 14.8% operations. We will release all the code and pretrained models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2311.18703</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2311.18703</id><created>2023-11-30</created><updated>2025-02-02</updated><authors><author><keyname>Ornia</keyname><forenames>Daniel Jarne</forenames></author><author><keyname>Delimpaltadakis</keyname><forenames>Giannis</forenames></author><author><keyname>Kober</keyname><forenames>Jens</forenames></author><author><keyname>Alonso-Mora</keyname><forenames>Javier</forenames></author></authors><title>Predictable Reinforcement Learning Dynamics through Entropy Rate   Minimization</title><categories>cs.LG cs.AI cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Reinforcement Learning (RL), agents have no incentive to exhibit predictable behaviors, and are often pushed (through e.g. policy entropy regularisation) to randomise their actions in favor of exploration. This often makes it challenging for other agents and humans to predict an agent's behavior, triggering unsafe scenarios (e.g. in human-robot interaction). We propose a novel method to induce predictable behavior in RL agents, termed Predictability-Aware RL (PARL), employing the agent's trajectory entropy rate to quantify predictability. Our method maximizes a linear combination of a standard discounted reward and the negative entropy rate, thus trading off optimality with predictability. We show how the entropy rate can be formally cast as an average reward, how entropy-rate value functions can be estimated from a learned model and incorporate this in policy-gradient algorithms, and demonstrate how this approach produces predictable (near-optimal) policies in tasks inspired by human-robot use-cases. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.00994</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.00994</id><created>2023-12-01</created><updated>2025-02-01</updated><authors><author><keyname>Bisain</keyname><forenames>Ankit</forenames></author><author><keyname>Edelman</keyname><forenames>Alan</forenames></author><author><keyname>Urschel</keyname><forenames>John</forenames></author></authors><title>A New Upper Bound For the Growth Factor in Gaussian Elimination with   Complete Pivoting</title><categories>math.NA cs.NA</categories><msc-class>65F05, 15A23</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The growth factor in Gaussian elimination measures how large the entries of an LU factorization can be relative to the entries of the original matrix. It is a key parameter in error estimates, and one of the most fundamental topics in numerical analysis. We produce an upper bound of $n^{0.2079 \ln n +0.91}$ for the growth factor in Gaussian elimination with complete pivoting -- the first improvement upon Wilkinson's original 1961 bound of $2 \, n ^{0.25\ln n +0.5}$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.07844</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.07844</id><created>2023-12-12</created><updated>2025-02-02</updated><authors><author><keyname>You</keyname><forenames>Taekho</forenames></author><author><keyname>Park</keyname><forenames>Jinseo</forenames></author><author><keyname>Lee</keyname><forenames>June Young</forenames></author><author><keyname>Yun</keyname><forenames>Jinhyuk</forenames></author></authors><title>Regional profile of questionable publishing</title><categories>cs.DL physics.soc-ph</categories><comments>13 pages, 4 figures, supplementary information with 8 SI figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Countries and authors in the academic periphery occasionally have been criticized for contributing to the expansion of questionable publishing because they share a major fraction of papers in questionable journals. On the other side, topics preferred by mainstream journals sometimes necessitate large-scale investigation, which is impossible for developing countries. Thus, local journals, commonly low-impacted, are essential to sustain the regional academia for such countries. In this study, we perform an in-depth analysis of the distribution of questionable publications and journals with their interplay with countries quantifying the influence of questionable publications regarding academia's inequality. We find that low-impact journals play a vital role in the regional academic environment, whereas questionable journals with equivalent impact publish papers from all over the world, both geographically and academically. The business model of questionable journals differs from that of regional journals, and may thus be detrimental to the broader academic community. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.12747</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.12747</id><created>2023-12-19</created><updated>2025-02-02</updated><authors><author><keyname>Mills</keyname><forenames>Edmund</forenames></author><author><keyname>Su</keyname><forenames>Shiye</forenames></author><author><keyname>Russell</keyname><forenames>Stuart</forenames></author><author><keyname>Emmons</keyname><forenames>Scott</forenames></author></authors><title>ALMANACS: A Simulatability Benchmark for Language Model Explainability</title><categories>cs.LG cs.AI cs.CL stat.ML</categories><comments>Code is available at   https://github.com/edmundmills/ALMANACS}{https://github.com/edmundmills/ALMANACS</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  How do we measure the efficacy of language model explainability methods? While many explainability methods have been developed, they are typically evaluated on bespoke tasks, preventing an apples-to-apples comparison. To help fill this gap, we present ALMANACS, a language model explainability benchmark. ALMANACS scores explainability methods on simulatability, i.e., how well the explanations improve behavior prediction on new inputs. The ALMANACS scenarios span twelve safety-relevant topics such as ethical reasoning and advanced AI behaviors; they have idiosyncratic premises to invoke model-specific behavior; and they have a train-test distributional shift to encourage faithful explanations. By using another language model to predict behavior based on the explanations, ALMANACS is a fully automated benchmark. While not a replacement for human evaluations, we aim for ALMANACS to be a complementary, automated tool that allows for fast, scalable evaluation. Using ALMANACS, we evaluate counterfactual, rationalization, attention, and Integrated Gradients explanations. Our results are sobering: when averaged across all topics, no explanation method outperforms the explanation-free control. We conclude that despite modest successes in prior work, developing an explanation method that aids simulatability in ALMANACS remains an open challenge. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.15595</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.15595</id><created>2023-12-24</created><updated>2025-01-31</updated><authors><author><keyname>Wei</keyname><forenames>Haoyu</forenames></author><author><keyname>Wan</keyname><forenames>Runzhe</forenames></author><author><keyname>Shi</keyname><forenames>Lei</forenames></author><author><keyname>Song</keyname><forenames>Rui</forenames></author></authors><title>Zero-Inflated Bandits</title><categories>stat.ML cs.LG econ.EM</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Many real-world bandit applications are characterized by sparse rewards, which can significantly hinder learning efficiency. Leveraging problem-specific structures for careful distribution modeling is recognized as essential for improving estimation efficiency in statistics. However, this approach remains under-explored in the context of bandits. To address this gap, we initiate the study of zero-inflated bandits, where the reward is modeled using a classic semi-parametric distribution known as the zero-inflated distribution. We develop algorithms based on the Upper Confidence Bound and Thompson Sampling frameworks for this specific structure. The superior empirical performance of these methods is demonstrated through extensive numerical studies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.16896</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.16896</id><created>2023-12-28</created><updated>2025-01-31</updated><authors><author><keyname>Shin</keyname><forenames>Suho</forenames></author><author><keyname>Esmaeili</keyname><forenames>Seyed A.</forenames></author><author><keyname>Hajiaghayi</keyname><forenames>MohammadTaghi</forenames></author></authors><title>Replication-proof Bandit Mechanism Design with Bayesian Agents</title><categories>cs.GT cs.AI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of designing replication-proof bandit mechanisms when agents strategically register or replicate their own arms to maximize their payoff. Specifically, we consider Bayesian agents who only know the distribution from which their own arms' mean rewards are sampled, unlike the original setting of by Shin et al. 2022. Interestingly, with Bayesian agents in stark contrast to the previous work, analyzing the replication-proofness of an algorithm becomes significantly complicated even in a single-agent setting. We provide sufficient and necessary conditions for an algorithm to be replication-proof in the single-agent setting, and present an algorithm that satisfies these properties. These results center around several analytical theorems that focus on \emph{comparing the expected regret of multiple bandit instances}, and therefore might be of independent interest since they have not been studied before to the best of our knowledge. We expand this result to the multi-agent setting, and provide a replication-proof algorithm for any problem instance. We finalize our result by proving its sublinear regret upper bound which matches that of Shin et al. 2022. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.03737</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.03737</id><created>2024-01-08</created><updated>2024-04-04</updated><authors><author><keyname>Fatouros</keyname><forenames>Georgios</forenames></author><author><keyname>Metaxas</keyname><forenames>Konstantinos</forenames></author><author><keyname>Soldatos</keyname><forenames>John</forenames></author><author><keyname>Kyriazis</keyname><forenames>Dimosthenis</forenames></author></authors><title>Can Large Language Models Beat Wall Street? Unveiling the Potential of   AI in Stock Selection</title><categories>q-fin.CP cs.AI cs.CE cs.CL cs.LG</categories><comments>17 pages, 12 figures, 12 tables</comments><msc-class>68T07, 68T50, 91G10, 91G15</msc-class><acm-class>I.2.1; I.2.7; J.4</acm-class><journal-ref>Neural Computing and Applications (2024) 1-16</journal-ref><doi>10.1007/s00521-024-10613-4</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper introduces MarketSenseAI, an innovative framework leveraging GPT-4's advanced reasoning for selecting stocks in financial markets. By integrating Chain of Thought and In-Context Learning, MarketSenseAI analyzes diverse data sources, including market trends, news, fundamentals, and macroeconomic factors, to emulate expert investment decision-making. The development, implementation, and validation of the framework are elaborately discussed, underscoring its capability to generate actionable and interpretable investment signals. A notable feature of this work is employing GPT-4 both as a predictive mechanism and signal evaluator, revealing the significant impact of the AI-generated explanations on signal accuracy, reliability and acceptance. Through empirical testing on the competitive S&amp;P 100 stocks over a 15-month period, MarketSenseAI demonstrated exceptional performance, delivering excess alpha of 10% to 30% and achieving a cumulative return of up to 72% over the period, while maintaining a risk profile comparable to the broader market. Our findings highlight the transformative potential of Large Language Models in financial decision-making, marking a significant leap in integrating generative AI into financial analytics and investment strategies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.04155</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.04155</id><created>2024-01-08</created><updated>2025-01-31</updated><authors><author><keyname>Liu</keyname><forenames>Jiajia</forenames></author><author><keyname>Yang</keyname><forenames>Mengyuan</forenames></author><author><keyname>Yu</keyname><forenames>Yankai</forenames></author><author><keyname>Xu</keyname><forenames>Haixia</forenames></author><author><keyname>Wang</keyname><forenames>Tiangang</forenames></author><author><keyname>Li</keyname><forenames>Kang</forenames></author><author><keyname>Zhou</keyname><forenames>Xiaobo</forenames></author></authors><title>Advancing bioinformatics with large language models: components,   applications and perspectives</title><categories>q-bio.QM cs.CL</categories><comments>5 main figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models (LLMs) are a class of artificial intelligence models based on deep learning, which have great performance in various tasks, especially in natural language processing (NLP). Large language models typically consist of artificial neural networks with numerous parameters, trained on large amounts of unlabeled input using self-supervised or semi-supervised learning. However, their potential for solving bioinformatics problems may even exceed their proficiency in modeling human language. In this review, we will provide a comprehensive overview of the essential components of large language models (LLMs) in bioinformatics, spanning genomics, transcriptomics, proteomics, drug discovery, and single-cell analysis. Key aspects covered include tokenization methods for diverse data types, the architecture of transformer models, the core attention mechanism, and the pre-training processes underlying these models. Additionally, we will introduce currently available foundation models and highlight their downstream applications across various bioinformatics domains. Finally, drawing from our experience, we will offer practical guidance for both LLM users and developers, emphasizing strategies to optimize their use and foster further innovation in the field. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.08414</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.08414</id><created>2024-01-16</created><authors><author><keyname>Jacobsen</keyname><forenames>Christian</forenames></author><author><keyname>Dong</keyname><forenames>Jiayuan</forenames></author><author><keyname>Khalloufi</keyname><forenames>Mehdi</forenames></author><author><keyname>Huan</keyname><forenames>Xun</forenames></author><author><keyname>Duraisamy</keyname><forenames>Karthik</forenames></author><author><keyname>Akram</keyname><forenames>Maryam</forenames></author><author><keyname>Liu</keyname><forenames>Wanjiao</forenames></author></authors><title>Enhancing Dynamical System Modeling through Interpretable Machine   Learning Augmentations: A Case Study in Cathodic Electrophoretic Deposition</title><categories>physics.comp-ph cs.LG</categories><journal-ref>Data-Centric Engineering 6 (2025) e4</journal-ref><doi>10.1017/dce.2024.51</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a comprehensive data-driven framework aimed at enhancing the modeling of physical systems, employing inference techniques and machine learning enhancements. As a demonstrative application, we pursue the modeling of cathodic electrophoretic deposition (EPD), commonly known as e-coating. Our approach illustrates a systematic procedure for enhancing physical models by identifying their limitations through inference on experimental data and introducing adaptable model enhancements to address these shortcomings. We begin by tackling the issue of model parameter identifiability, which reveals aspects of the model that require improvement. To address generalizability , we introduce modifications which also enhance identifiability. However, these modifications do not fully capture essential experimental behaviors. To overcome this limitation, we incorporate interpretable yet flexible augmentations into the baseline model. These augmentations are parameterized by simple fully-connected neural networks (FNNs), and we leverage machine learning tools, particularly Neural Ordinary Differential Equations (Neural ODEs), to learn these augmentations. Our simulations demonstrate that the machine learning-augmented model more accurately captures observed behaviors and improves predictive accuracy. Nevertheless, we contend that while the model updates offer superior performance and capture the relevant physics, we can reduce off-line computational costs by eliminating certain dynamics without compromising accuracy or interpretability in downstream predictions of quantities of interest, particularly film thickness predictions. The entire process outlined here provides a structured approach to leverage data-driven methods. Firstly, it helps us comprehend the root causes of model inaccuracies, and secondly, it offers a principled method for enhancing model performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.10371</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.10371</id><created>2024-01-18</created><updated>2025-02-01</updated><authors><author><keyname>Chien</keyname><forenames>Eli</forenames></author><author><keyname>Wang</keyname><forenames>Haoyu</forenames></author><author><keyname>Chen</keyname><forenames>Ziang</forenames></author><author><keyname>Li</keyname><forenames>Pan</forenames></author></authors><title>Langevin Unlearning: A New Perspective of Noisy Gradient Descent for   Machine Unlearning</title><categories>cs.LG</categories><comments>NeurIPS 2024 Spotlight</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Machine unlearning has raised significant interest with the adoption of laws ensuring the ``right to be forgotten''. Researchers have provided a probabilistic notion of approximate unlearning under a similar definition of Differential Privacy (DP), where privacy is defined as statistical indistinguishability to retraining from scratch. We propose Langevin unlearning, an unlearning framework based on noisy gradient descent with privacy guarantees for approximate unlearning problems. Langevin unlearning unifies the DP learning process and the privacy-certified unlearning process with many algorithmic benefits. These include approximate certified unlearning for non-convex problems, complexity saving compared to retraining, sequential and batch unlearning for multiple unlearning requests. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.12585</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.12585</id><created>2024-01-23</created><updated>2024-11-12</updated><authors><author><keyname>Mei</keyname><forenames>Lingrui</forenames></author><author><keyname>Liu</keyname><forenames>Shenghua</forenames></author><author><keyname>Wang</keyname><forenames>Yiwei</forenames></author><author><keyname>Bi</keyname><forenames>Baolong</forenames></author><author><keyname>Cheng</keyname><forenames>Xueqi</forenames></author></authors><title>SLANG: New Concept Comprehension of Large Language Models</title><categories>cs.CL</categories><comments>EMNLP 2024 Main</comments><report-no>2024.emnlp-main.698</report-no><doi>10.18653/v1/2024.emnlp-main.698</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamic nature of language, particularly evident in the realm of slang and memes on the Internet, poses serious challenges to the adaptability of large language models (LLMs). Traditionally anchored to static datasets, these models often struggle to keep up with the rapid linguistic evolution characteristic of online communities. This research aims to bridge this gap by enhancing LLMs' comprehension of the evolving new concepts on the Internet, without the high cost of continual retraining. In pursuit of this goal, we introduce $\textbf{SLANG}$, a benchmark designed to autonomously integrate novel data and assess LLMs' ability to comprehend emerging concepts, alongside $\textbf{FOCUS}$, an approach uses causal inference to enhance LLMs to understand new phrases and their colloquial context. Our benchmark and approach involves understanding real-world instances of linguistic shifts, serving as contextual beacons, to form more precise and contextually relevant connections between newly emerging expressions and their meanings. The empirical analysis shows that our causal inference-based approach outperforms the baseline methods in terms of precision and relevance in the comprehension of Internet slang and memes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.12627</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.12627</id><created>2024-01-23</created><updated>2025-02-03</updated><authors><author><keyname>Schmid</keyname><forenames>Luca</forenames></author><author><keyname>Raviv</keyname><forenames>Tomer</forenames></author><author><keyname>Shlezinger</keyname><forenames>Nir</forenames></author><author><keyname>Schmalen</keyname><forenames>Laurent</forenames></author></authors><title>Blind Channel Estimation and Joint Symbol Detection with Data-Driven   Factor Graphs</title><categories>cs.IT cs.LG eess.SP math.IT</categories><comments>Accepted for publication in the IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the application of the factor graph framework for blind joint channel estimation and symbol detection on time-variant linear inter-symbol interference channels. In particular, we consider the expectation maximization (EM) algorithm for maximum likelihood estimation, which typically suffers from high complexity as it requires the computation of the symbol-wise posterior distributions in every iteration. We address this issue by efficiently approximating the posteriors using the belief propagation (BP) algorithm on a suitable factor graph. By interweaving the iterations of BP and EM, the detection complexity can be further reduced to a single BP iteration per EM step. In addition, we propose a data-driven version of our algorithm that introduces momentum in the BP updates and learns a suitable EM parameter update schedule, thereby significantly improving the performance-complexity tradeoff with a few offline training samples. Our numerical experiments demonstrate the excellent performance of the proposed blind detector and show that it even outperforms coherent BP detection in high signal-to-noise scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.14336</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.14336</id><created>2024-01-25</created><authors><author><keyname>Liu</keyname><forenames>Dichao</forenames></author></authors><title>Progressive Multi-task Anti-Noise Learning and Distilling Frameworks for   Fine-grained Vehicle Recognition</title><categories>cs.CV cs.AI</categories><journal-ref>IEEE Transactions on Intelligent Transportation Systems, vol. 25,   no. 9, pp. 10667-10678, 2024</journal-ref><doi>10.1109/TITS.2024.3420151</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fine-grained vehicle recognition (FGVR) is an essential fundamental technology for intelligent transportation systems, but very difficult because of its inherent intra-class variation. Most previous FGVR studies only focus on the intra-class variation caused by different shooting angles, positions, etc., while the intra-class variation caused by image noise has received little attention. This paper proposes a progressive multi-task anti-noise learning (PMAL) framework and a progressive multi-task distilling (PMD) framework to solve the intra-class variation problem in FGVR due to image noise. The PMAL framework achieves high recognition accuracy by treating image denoising as an additional task in image recognition and progressively forcing a model to learn noise invariance. The PMD framework transfers the knowledge of the PMAL-trained model into the original backbone network, which produces a model with about the same recognition accuracy as the PMAL-trained model, but without any additional overheads over the original backbone network. Combining the two frameworks, we obtain models that significantly exceed previous state-of-the-art methods in recognition accuracy on two widely-used, standard FGVR datasets, namely Stanford Cars, and CompCars, as well as three additional surveillance image-based vehicle-type classification datasets, namely Beijing Institute of Technology (BIT)-Vehicle, Vehicle Type Image Data 2 (VTID2), and Vehicle Images Dataset for Make Model Recognition (VIDMMR), without any additional overheads over the original backbone networks. The source code is available at https://github.com/Dichao-Liu/Anti-noise_FGVR </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.14497</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.14497</id><created>2024-01-25</created><updated>2025-02-02</updated><authors><author><keyname>Abhishek</keyname><forenames>Kumar</forenames></author><author><keyname>Jain</keyname><forenames>Aditi</forenames></author><author><keyname>Hamarneh</keyname><forenames>Ghassan</forenames></author></authors><title>Investigating the Quality of DermaMNIST and Fitzpatrick17k   Dermatological Image Datasets</title><categories>cs.CV cs.LG</categories><comments>41 pages, 17 figures, 4 tables</comments><journal-ref>Nature Scientific Data 12(1), 196 (2025) 1-21</journal-ref><doi>10.1038/s41597-025-04382-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The remarkable progress of deep learning in dermatological tasks has brought us closer to achieving diagnostic accuracies comparable to those of human experts. However, while large datasets play a crucial role in the development of reliable deep neural network models, the quality of data therein and their correct usage are of paramount importance. Several factors can impact data quality, such as the presence of duplicates, data leakage across train-test partitions, mislabeled images, and the absence of a well-defined test partition. In this paper, we conduct meticulous analyses of three popular dermatological image datasets: DermaMNIST, its source HAM10000, and Fitzpatrick17k, uncovering these data quality issues, measure the effects of these problems on the benchmark results, and propose corrections to the datasets. Besides ensuring the reproducibility of our analysis, by making our analysis pipeline and the accompanying code publicly available, we aim to encourage similar explorations and to facilitate the identification and addressing of potential data quality issues in other large datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.14931</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.14931</id><created>2024-01-26</created><updated>2025-02-02</updated><authors><author><keyname>Bombieri</keyname><forenames>Marco</forenames></author><author><keyname>Fiorini</keyname><forenames>Paolo</forenames></author><author><keyname>Ponzetto</keyname><forenames>Simone Paolo</forenames></author><author><keyname>Rospocher</keyname><forenames>Marco</forenames></author></authors><title>Do LLMs Dream of Ontologies?</title><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) have demonstrated remarkable performance across diverse natural language processing tasks, yet their ability to memorize structured knowledge remains underexplored. In this paper, we investigate the extent to which general-purpose pre-trained LLMs retain and correctly reproduce concept identifier (ID)-label associations from publicly available ontologies. We conduct a systematic evaluation across multiple ontological resources, including the Gene Ontology, Uberon, Wikidata, and ICD-10, using LLMs such as Pythia-12B, Gemini-1.5-Flash, GPT-3.5, and GPT-4. Our findings reveal that only a small fraction of ontological concepts is accurately memorized, with GPT-4 demonstrating the highest performance. To understand why certain concepts are memorized more effectively than others, we analyze the relationship between memorization accuracy and concept popularity on the Web. Our results indicate a strong correlation between the frequency of a concept's occurrence online and the likelihood of accurately retrieving its ID from the label. This suggests that LLMs primarily acquire such knowledge through indirect textual exposure rather than directly from structured ontological resources. Furthermore, we introduce new metrics to quantify prediction invariance, demonstrating that the stability of model responses across variations in prompt language and temperature settings can serve as a proxy for estimating memorization robustness. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.17757</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.17757</id><created>2024-01-31</created><updated>2025-02-03</updated><authors><author><keyname>Li</keyname><forenames>Wenhao</forenames></author><author><keyname>Han</keyname><forenames>Zongyuan</forenames></author><author><keyname>Zhu</keyname><forenames>Shengxin</forenames></author></authors><title>When Lanczos Iterations Generate Symmetric Quadrature Nodes?</title><categories>math.NA cs.NA</categories><comments>22 pages, 4 figures</comments><msc-class>65D32, 65F10, 65F15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Golub-Welsch algorithm [ Math. Comp., 23: 221-230 (1969)] has long been assumed symmetric for estimating quadratic forms. Recent research indicates that asymmetric quadrature nodes may be more often and the existence of a practical symmetric quadrature for estimating matrix quadratic form is even doubtful.This paper derives a sufficient condition for symmetric quadrature nodes for estimating quadratic forms involving the Jordan-Wielandt matrices which frequently arise from many applications. The condition is closely related to how to construct an initial vector for the underlying Lanczos process. Applications of such constructive results are demonstrated by estimating the Estrada index in complex network analysis. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.00924</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.00924</id><created>2024-02-01</created><updated>2025-02-02</updated><authors><author><keyname>Sun</keyname><forenames>Linghang</forenames></author><author><keyname>Zhang</keyname><forenames>Yifan</forenames></author><author><keyname>Axenie</keyname><forenames>Cristian</forenames></author><author><keyname>Grossi</keyname><forenames>Margherita</forenames></author><author><keyname>Kouvelas</keyname><forenames>Anastasios</forenames></author><author><keyname>Makridis</keyname><forenames>Michail A.</forenames></author></authors><title>The Fragile Nature of Road Transportation Systems</title><categories>eess.SY cs.SY</categories><comments>34 pages, 11 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Major cities worldwide experience problems with the performance of their road transportation systems, and the continuous increase in traffic demand presents a substantial challenge to the optimal operation of urban road networks and the efficiency of traffic control strategies. The operation of transportation systems is widely considered to display fragile property, i.e., the loss in performance increases exponentially with the linearly increasing magnitude of disruptions. Meanwhile, the risk engineering community is embracing the novel concept of antifragility, enabling systems to learn from historical disruptions and exhibit improved performance under black swan events. In this study, based on established traffic models, namely fundamental diagrams and macroscopic fundamental diagrams, we first conducted a rigorous mathematical analysis to prove the fragile nature of the systems theoretically. Subsequently, we propose a skewness-based indicator that can be readily applied to cross-compare the degree of fragility for different networks solely dependent on the MFD-related parameters. At last, by taking real-world stochasticity into account, we implemented a numerical simulation with realistic network data to bridge the gap between the theoretical proof and the real-world operations, to reflect the potential impact of uncertainty on the fragility of the systems. This work aims to demonstrate the fragile nature of road transportation systems and help researchers better comprehend the necessity to consider explicitly antifragile design for future traffic control strategies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.01454</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.01454</id><created>2024-02-02</created><updated>2025-01-31</updated><authors><author><keyname>Takayama</keyname><forenames>Masayuki</forenames></author><author><keyname>Okuda</keyname><forenames>Tadahisa</forenames></author><author><keyname>Pham</keyname><forenames>Thong</forenames></author><author><keyname>Ikenoue</keyname><forenames>Tatsuyoshi</forenames></author><author><keyname>Fukuma</keyname><forenames>Shingo</forenames></author><author><keyname>Shimizu</keyname><forenames>Shohei</forenames></author><author><keyname>Sannai</keyname><forenames>Akiyoshi</forenames></author></authors><title>Integrating Large Language Models in Causal Discovery: A Statistical   Causal Approach</title><categories>cs.LG cs.AI stat.ME stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is important for creating consistent, meaningful causal models, despite the challenges in the systematic acquisition of background knowledge. To overcome these challenges, this paper proposes a novel method for causal inference, in which SCD and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through ``statistical causal prompting (SCP)'' for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that the results of LLM-KBCI and SCD augmented with LLM-KBCI approach the ground truths, more than the SCD result without prior knowledge. It has also been revealed that the SCD result can be further improved if the LLM undergoes SCP. Furthermore, with an unpublished real-world dataset, we have demonstrated that the background knowledge provided by the LLM can improve the SCD on this dataset, even if this dataset has never been included in the training data of the LLM. For future practical application of this proposed method across important domains such as healthcare, we also thoroughly discuss the limitations, risks of critical errors, expected improvement of techniques around LLMs, and realistic integration of expert checks of the results into this automatic process, with SCP simulations under various conditions both in successful and failure scenarios. The careful and appropriate application of the proposed approach in this work, with improvement and customization for each domain, can thus address challenges such as dataset biases and limitations, illustrating the potential of LLMs to improve data-driven causal inference across diverse scientific domains. The code used in this work is publicly available at: www.github.com/mas-takayama/LLM-and-SCD </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.01843</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.01843</id><created>2024-02-02</created><authors><author><keyname>Kulkarni</keyname><forenames>Sudhanshu</forenames></author><author><keyname>Loring</keyname><forenames>Burlen</forenames></author><author><keyname>Bethel</keyname><forenames>E. Wes</forenames></author></authors><title>Towards a Scalable In Situ Fast Fourier Transform</title><categories>cs.DC</categories><comments>5 pages, 2 figures. Submitted to ISAV workshop in SC23 conference</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The Fast Fourier Transform (FFT) is a numerical operation that transforms a function into a form comprised of its constituent frequencies and is an integral part of scientific computation and data analysis. The objective of our work is to enable use of the FFT as part of a scientific in situ processing chain to facilitate the analysis of data in the spectral regime. We describe the implementation of an FFT endpoint for the transformation of multi-dimensional data within the SENSEI infrastructure. Our results show its use on a sample problem in the context of a multi-stage in situ processing workflow. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.03055</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.03055</id><created>2024-02-05</created><updated>2025-02-03</updated><authors><author><keyname>Tasdighi</keyname><forenames>Bahareh</forenames></author><author><keyname>Haussmann</keyname><forenames>Manuel</forenames></author><author><keyname>Werge</keyname><forenames>Nicklas</forenames></author><author><keyname>Wu</keyname><forenames>Yi-Shan</forenames></author><author><keyname>Kandemir</keyname><forenames>Melih</forenames></author></authors><title>Deep Exploration with PAC-Bayes</title><categories>cs.LG</categories><comments>28 pages, 6 figures, 9 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Reinforcement learning for continuous control under delayed rewards is an under-explored problem despite its significance in real life. Many complex skills build on intermediate ones as prerequisites. For instance, a humanoid locomotor has to learn how to stand before it can learn to walk. To cope with delayed reward, a reinforcement learning agent has to perform deep exploration. However, existing deep exploration methods are designed for small discrete action spaces, and their successful generalization to state-of-the-art continuous control remains unproven. We address the deep exploration problem for the first time from a PAC-Bayesian perspective in the context of actor-critic learning. To do this, we quantify the error of the Bellman operator through a PAC-Bayes bound, where a bootstrapped ensemble of critic networks represents the posterior distribution, and their targets serve as a data-informed function-space prior. We derive an objective function from this bound and use it to train the critic ensemble. Each critic trains an individual soft actor network, implemented as a shared trunk and critic-specific heads. The agent performs deep exploration by acting epsilon-greedily on a randomly chosen actor head. Our proposed algorithm, named PAC-Bayesian Actor-Critic (PBAC), is the only algorithm to consistently discover delayed rewards on a diverse set of continuous control tasks with varying difficulty. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.03973</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.03973</id><created>2024-02-06</created><updated>2025-02-03</updated><authors><author><keyname>Ollikka</keyname><forenames>Netta</forenames></author><author><keyname>Abbas</keyname><forenames>Amro</forenames></author><author><keyname>Perin</keyname><forenames>Andrea</forenames></author><author><keyname>Kilpeläinen</keyname><forenames>Markku</forenames></author><author><keyname>Deny</keyname><forenames>Stéphane</forenames></author></authors><title>A comparison between humans and AI at recognizing objects in unusual   poses</title><categories>cs.CV cs.LG</categories><comments>version accepted at TMLR</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deep learning is closing the gap with human vision on several object recognition benchmarks. Here we investigate this gap for challenging images where objects are seen in unusual poses. We find that humans excel at recognizing objects in such poses. In contrast, state-of-the-art deep networks for vision (EfficientNet, SWAG, ViT, SWIN, BEiT, ConvNext) and state-of-the-art large vision-language models (Claude 3.5, Gemini 1.5, GPT-4) are systematically brittle on unusual poses, with the exception of Gemini showing excellent robustness in that condition. As we limit image exposure time, human performance degrades to the level of deep networks, suggesting that additional mental processes (requiring additional time) are necessary to identify objects in unusual poses. An analysis of error patterns of humans vs. networks reveals that even time-limited humans are dissimilar to feed-forward deep networks. In conclusion, our comparison reveals that humans and deep networks rely on different mechanisms for recognizing objects in unusual poses. Understanding the nature of the mental processes taking place during extra viewing time may be key to reproduce the robustness of human vision in silico. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.04676</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.04676</id><created>2024-02-07</created><updated>2025-02-01</updated><authors><author><keyname>Vahidian</keyname><forenames>Saeed</forenames></author><author><keyname>Wang</keyname><forenames>Mingyu</forenames></author><author><keyname>Gu</keyname><forenames>Jianyang</forenames></author><author><keyname>Kungurtsev</keyname><forenames>Vyacheslav</forenames></author><author><keyname>Jiang</keyname><forenames>Wei</forenames></author><author><keyname>Chen</keyname><forenames>Yiran</forenames></author></authors><title>Group Distributionally Robust Dataset Distillation with Risk   Minimization</title><categories>cs.LG cs.AI cs.CV</categories><comments>ICLR 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dataset distillation (DD) has emerged as a widely adopted technique for crafting a synthetic dataset that captures the essential information of a training dataset, facilitating the training of accurate neural models. Its applications span various domains, including transfer learning, federated learning, and neural architecture search. The most popular methods for constructing the synthetic data rely on matching the convergence properties of training the model with the synthetic dataset and the training dataset. However, using the empirical loss as the criterion must be thought of as auxiliary in the same sense that the training set is an approximate substitute for the population distribution, and the latter is the data of interest. Yet despite its popularity, an aspect that remains unexplored is the relationship of DD to its generalization, particularly across uncommon subgroups. That is, how can we ensure that a model trained on the synthetic dataset performs well when faced with samples from regions with low population density? Here, the representativeness and coverage of the dataset become salient over the guaranteed training error at inference. Drawing inspiration from distributionally robust optimization, we introduce an algorithm that combines clustering with the minimization of a risk measure on the loss to conduct DD. We provide a theoretical rationale for our approach and demonstrate its effective generalization and robustness across subgroups through numerical experiments. The source code is available at https://github.com/Mming11/RobustDatasetDistillation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.05885</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.05885</id><created>2024-02-08</created><updated>2025-02-02</updated><authors><author><keyname>Bommakanti</keyname><forenames>Aditya</forenames></author><author><keyname>Vonteri</keyname><forenames>Harshith Reddy</forenames></author><author><keyname>Ranu</keyname><forenames>Sayan</forenames></author><author><keyname>Karras</keyname><forenames>Panagiotis</forenames></author></authors><title>EUGENE: Explainable Unsupervised Approximation of Graph Edit Distance   with Generalized Edit Costs</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The need to identify graphs with small structural distances from a query arises in various domains such as biology, chemistry, recommender systems, and social network analysis. Among several methods for measuring inter-graph distance, Graph Edit Distance (GED) is preferred for its comprehensibility, though its computation is hindered by NP-hardness. Unsupervised methods often face challenges in providing accurate approximations. State-of-the-art GED approximations predominantly utilize neural methods, which, however, have several limitations: (i) lack an explanatory edit path corresponding to the approximated GED; (ii) require the NP-hard generation of ground-truth GEDs for training; and (iii) necessitate separate training on each dataset. In this paper, we propose EUGENE, an efficient algebraic unsupervised method that approximates GED while providing edit paths corresponding to the approximated cost. Extensive experimental evaluation demonstrates that EUGENE achieves state-of-the-art performance in GED estimation and exhibits superior scalability across diverse datasets and generalized cost settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.06049</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.06049</id><created>2024-02-05</created><updated>2025-02-01</updated><authors><author><keyname>Flamino</keyname><forenames>James</forenames></author><author><keyname>Modi</keyname><forenames>Mohammed Shahid</forenames></author><author><keyname>Szymanski</keyname><forenames>Boleslaw K.</forenames></author><author><keyname>Cross</keyname><forenames>Brendan</forenames></author><author><keyname>Mikolajczyk</keyname><forenames>Colton</forenames></author></authors><title>Limits of Large Language Models in Debating Humans</title><categories>cs.AI cs.CL cs.HC stat.AP</categories><comments>23 pages, 4 figures, 3 tables, 42 pages of supplemental materials, 9   supplemental figures, 24 supplemental tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) have shown remarkable promise in communicating with humans. Their potential use as artificial partners with humans in sociological experiments involving conversation is an exciting prospect. But how viable is it? Here, we rigorously test the limits of agents that debate using LLMs in a preregistered study that runs multiple debate-based opinion consensus games. Each game starts with six humans, six agents, or three humans and three agents. We found that agents can blend in and concentrate on a debate's topic better than humans, improving the productivity of all players. Yet, humans perceive agents as less convincing and confident than other humans, and several behavioral metrics of humans and agents we collected deviate measurably from each other. We observed that agents are already decent debaters, but their behavior generates a pattern distinctly different from the human-generated data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.06104</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.06104</id><created>2024-02-08</created><updated>2025-01-31</updated><authors><author><keyname>Zhu</keyname><forenames>Dixian</forenames></author><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author><author><keyname>Jerby</keyname><forenames>Livnat</forenames></author></authors><title>Gradient Aligned Regression via Pairwise Losses</title><categories>cs.LG cs.AI</categories><comments>26 pages excluding references, 12 figures, 7 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Regression is a fundamental task in machine learning that has garnered extensive attention over the past decades. The conventional approach for regression involves employing loss functions that primarily concentrate on aligning model prediction with the ground truth for each individual data sample. Recent research endeavors have introduced novel perspectives by incorporating label similarity to regression via imposing extra pairwise regularization on the latent feature space and demonstrated the effectiveness. However, there are two drawbacks for those approaches: i) their pairwise operation in latent feature space is computationally more expensive than conventional regression losses; ii) it lacks of theoretical justifications behind such regularization. In this work, we propose GAR (Gradient Aligned Regression) as a competitive alternative method in label space, which is constituted by a conventional regression loss and two pairwise label difference losses for gradient alignment including magnitude and direction. GAR enjoys: i) the same level efficiency as conventional regression loss because the quadratic complexity for the proposed pairwise losses can be reduced to linear complexity; ii) theoretical insights from learning the pairwise label difference to learning the gradient of the ground truth function. We limit our current scope as regression on the clean data setting without noises, outliers or distributional shifts, etc. We demonstrate the effectiveness of the proposed method practically on two synthetic datasets and on eight extensive real-world tasks from six benchmark datasets with other eight competitive baselines. Running time experiments demonstrate the superior efficiency of the proposed GAR over existing methods with pairwise regularization in latent feature space and ablation studies demonstrate the effectiveness of each component for GAR. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.06787</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.06787</id><created>2024-02-09</created><updated>2025-02-01</updated><authors><author><keyname>Zhao</keyname><forenames>Liangyu</forenames></author><author><keyname>Maleki</keyname><forenames>Saeed</forenames></author><author><keyname>Yang</keyname><forenames>Ziyue</forenames></author><author><keyname>Pourreza</keyname><forenames>Hossein</forenames></author><author><keyname>Krishnamurthy</keyname><forenames>Arvind</forenames></author></authors><title>ForestColl: Throughput-Optimal Collective Communications on   Heterogeneous Network Fabrics</title><categories>cs.NI cs.DC cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:2305.18461</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As modern DNN models grow ever larger, collective communications between the accelerators (allreduce, etc.) emerge as a significant performance bottleneck. Designing efficient communication schedules is challenging, given today's heterogeneous and diverse network fabrics. We present ForestColl, a tool that generates throughput-optimal schedules for any network topology. ForestColl constructs broadcast/aggregation spanning trees as the communication schedule, achieving theoretical optimality. Its schedule generation runs in strongly polynomial time and is highly scalable. ForestColl supports any network fabrics, including both switching fabrics and direct accelerator connections. We evaluated ForestColl on multi-box AMD MI250 and NVIDIA DGX A100 platforms. ForestColl showed significant improvements over the vendors' own optimized communication libraries, RCCL and NCCL, across various settings and in LLM training. ForestColl also outperformed other state-of-the-art schedule generation techniques with both more efficient generated schedules and substantially faster schedule generation speed. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.10343</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.10343</id><created>2024-02-15</created><updated>2025-01-31</updated><authors><author><keyname>Hu</keyname><forenames>Jialu</forenames></author><author><keyname>Kozma</keyname><forenames>László</forenames></author></authors><title>Non-adaptive Bellman-Ford: Yen's improvement is optimal</title><categories>cs.DS math.CO</categories><comments>Revised and extended version. Main result now proved in alternative   way, as V1 proof was faulty. Sections 3 and 4, and Thm 3.1 are new</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bellman-Ford algorithm for single-source shortest paths repeatedly updates tentative distances in an operation called relaxing an edge. In several important applications a non-adaptive (oblivious) implementation is preferred, which means fixing the entire sequence of relaxations upfront, independently of the edge-weights. Such an implementation performs, in a dense graph on $n$ vertices, $(1 + o(1))n^3$ relaxations. An improvement by Yen from 1970 reduces the number of relaxations by a factor of two. We show that no further constant-factor improvements are possible, and every non-adaptive deterministic algorithm based on relaxations must perform $(\frac{1}{2} - o(1))n^3$ steps. This improves an earlier lower bound of Eppstein of $(\frac{1}{6} - o(1))n^3$. Given that a non-adaptive randomized variant of Bellman-Ford with at most $(\frac{1}{3} + o(1))n^3$ relaxations (with high probability) is known, our result implies a strict separation between deterministic and randomized strategies, answering an open question of Eppstein. On the complexity side, we show that deciding whether a given relaxation sequence is guaranteed to yield correct distances is NP-hard, even with the complete graph as input. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.11060</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.11060</id><created>2024-02-16</created><updated>2025-02-02</updated><authors><author><keyname>Sun</keyname><forenames>Chenkai</forenames></author><author><keyname>Yang</keyname><forenames>Ke</forenames></author><author><keyname>Reddy</keyname><forenames>Revanth Gangi</forenames></author><author><keyname>Fung</keyname><forenames>Yi R.</forenames></author><author><keyname>Chan</keyname><forenames>Hou Pong</forenames></author><author><keyname>Small</keyname><forenames>Kevin</forenames></author><author><keyname>Zhai</keyname><forenames>ChengXiang</forenames></author><author><keyname>Ji</keyname><forenames>Heng</forenames></author></authors><title>Persona-DB: Efficient Large Language Model Personalization for Response   Prediction with Collaborative Data Refinement</title><categories>cs.CL cs.AI cs.IR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The increasing demand for personalized interactions with large language models (LLMs) calls for methodologies capable of accurately and efficiently identifying user opinions and preferences. Retrieval augmentation emerges as an effective strategy, as it can accommodate a vast number of users without the costs from fine-tuning. Existing research, however, has largely focused on enhancing the retrieval stage and devoted limited exploration toward optimizing the representation of the database, a crucial aspect for tasks such as personalization. In this work, we examine the problem from a novel angle, focusing on how data can be better represented for more data-efficient retrieval in the context of LLM customization. To tackle this challenge, we introduce Persona-DB, a simple yet effective framework consisting of a hierarchical construction process to improve generalization across task contexts and collaborative refinement to effectively bridge knowledge gaps among users. In the evaluation of response prediction, Persona-DB demonstrates superior context efficiency in maintaining accuracy with a significantly reduced retrieval size, a critical advantage in scenarios with extensive histories or limited context windows. Our experiments also indicate a marked improvement of over 10% under cold-start scenarios, when users have extremely sparse data. Furthermore, our analysis reveals the increasing importance of collaborative knowledge as the retrieval capacity expands. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.11179</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.11179</id><created>2024-02-16</created><authors><author><keyname>Hauth</keyname><forenames>Jeremiah</forenames></author><author><keyname>Safta</keyname><forenames>Cosmin</forenames></author><author><keyname>Huan</keyname><forenames>Xun</forenames></author><author><keyname>Patel</keyname><forenames>Ravi G.</forenames></author><author><keyname>Jones</keyname><forenames>Reese E.</forenames></author></authors><title>Uncertainty Quantification of Graph Convolution Neural Network Models of   Evolving Processes</title><categories>cs.LG math.ST physics.comp-ph stat.TH</categories><comments>27 pages, 20 figures</comments><journal-ref>Computer Methods in Applied Mechanics and Engineering 429 (2024)   117195</journal-ref><doi>10.1016/j.cma.2024.117195</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The application of neural network models to scientific machine learning tasks has proliferated in recent years. In particular, neural network models have proved to be adept at modeling processes with spatial-temporal complexity. Nevertheless, these highly parameterized models have garnered skepticism in their ability to produce outputs with quantified error bounds over the regimes of interest. Hence there is a need to find uncertainty quantification methods that are suitable for neural networks. In this work we present comparisons of the parametric uncertainty quantification of neural networks modeling complex spatial-temporal processes with Hamiltonian Monte Carlo and Stein variational gradient descent and its projected variant. Specifically we apply these methods to graph convolutional neural network models of evolving systems modeled with recurrent neural network and neural ordinary differential equations architectures. We show that Stein variational inference is a viable alternative to Monte Carlo methods with some clear advantages for complex neural network models. For our exemplars, Stein variational interference gave similar uncertainty profiles through time compared to Hamiltonian Monte Carlo, albeit with generally more generous variance.Projected Stein variational gradient descent also produced similar uncertainty profiles to the non-projected counterpart, but large reductions in the active weight space were confounded by the stability of the neural network predictions and the convoluted likelihood landscape. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.13602</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.13602</id><created>2024-02-21</created><updated>2024-08-19</updated><authors><author><keyname>Azarafza</keyname><forenames>Mehdi</forenames></author><author><keyname>Nayyeri</keyname><forenames>Mojtaba</forenames></author><author><keyname>Steinmetz</keyname><forenames>Charles</forenames></author><author><keyname>Staab</keyname><forenames>Steffen</forenames></author><author><keyname>Rettberg</keyname><forenames>Achim</forenames></author></authors><title>Hybrid Reasoning Based on Large Language Models for Autonomous Car   Driving</title><categories>cs.CV cs.AI</categories><comments>12 pages, 5 figures</comments><doi>10.1109/ICCMA63715.2024.10843921</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Large Language Models (LLMs) have garnered significant attention for their ability to understand text and images, generate human-like text, and perform complex reasoning tasks. However, their ability to generalize this advanced reasoning with a combination of natural language text for decision-making in dynamic situations requires further exploration. In this study, we investigate how well LLMs can adapt and apply a combination of arithmetic and common-sense reasoning, particularly in autonomous driving scenarios. We hypothesize that LLMs hybrid reasoning abilities can improve autonomous driving by enabling them to analyze detected object and sensor data, understand driving regulations and physical laws, and offer additional context. This addresses complex scenarios, like decisions in low visibility (due to weather conditions), where traditional methods might fall short. We evaluated Large Language Models (LLMs) based on accuracy by comparing their answers with human-generated ground truth inside CARLA. The results showed that when a combination of images (detected objects) and sensor data is fed into the LLM, it can offer precise information for brake and throttle control in autonomous vehicles across various weather conditions. This formulation and answers can assist in decision-making for auto-pilot systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.16405</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.16405</id><created>2024-02-26</created><updated>2025-02-02</updated><authors><author><keyname>Papazafeiropoulos</keyname><forenames>Anastasios</forenames></author><author><keyname>Kourtessis</keyname><forenames>Pandelis</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Kaklamani</keyname><forenames>Dimitra</forenames></author><author><keyname>Venieris</keyname><forenames>Iakovos</forenames></author></authors><title>Performance of Double-Stacked Intelligent Metasurface-Assisted Multiuser   Massive MIMO Communications in the Wave Domain</title><categories>cs.IT eess.SP math.IT</categories><comments>accepted in IEEE TWC</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Although reconfigurable intelligent surface (RIS) is a promising technology for shaping the propagation environment, it consists of a single-layer structure within inherent limitations regarding the number of beam steering patterns. Based on the recently revolutionary technology, denoted as stacked intelligent metasurface (SIM), we propose its implementation not only on the base station (BS) side in a massive multiple-input multiple-output (mMIMO) setup but also in the intermediate space between the base station and the users to adjust the environment further as needed. For the sake of convenience, we call the former BS SIM (BSIM), and the latter channel SIM (CSIM). Hence, we achieve wave-based combining at the BS and wave-based configuration at the intermediate space. Specifically, we propose a channel estimation method with reduced overhead, being crucial for SIMassisted communications. Next, we derive the uplink sum spectral efficiency (SE) in closed form in terms of statistical channel state information (CSI). Notably, we optimize the phase shifts of both BSIM and CSIM simultaneously by using the projected gradient ascent method (PGAM). Compared to previous works on SIMs, we study the uplink transmission, a mMIMO setup, channel estimation in a single phase, a second SIM at the intermediate space, and simultaneous optimization of the two SIMs. Simulation results show the impact of various parameters on the sum SE, and demonstrate the superiority of our optimization approach compared to the alternating optimization (AO) method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.18060</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.18060</id><created>2024-02-28</created><updated>2025-02-02</updated><authors><author><keyname>Chen</keyname><forenames>Hanjie</forenames></author><author><keyname>Fang</keyname><forenames>Zhouxiang</forenames></author><author><keyname>Singla</keyname><forenames>Yash</forenames></author><author><keyname>Dredze</keyname><forenames>Mark</forenames></author></authors><title>Benchmarking Large Language Models on Answering and Explaining   Challenging Medical Questions</title><categories>cs.CL</categories><comments>NAACL 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LLMs have demonstrated impressive performance in answering medical questions, such as achieving passing scores on medical licensing examinations. However, medical board exams or general clinical questions do not capture the complexity of realistic clinical cases. Moreover, the lack of reference explanations means we cannot easily evaluate the reasoning of model decisions, a crucial component of supporting doctors in making complex medical decisions. To address these challenges, we construct two new datasets: JAMA Clinical Challenge and Medbullets.\footnote{Datasets and code are available at \url{https://github.com/HanjieChen/ChallengeClinicalQA}.} JAMA Clinical Challenge consists of questions based on challenging clinical cases, while Medbullets comprises simulated clinical questions. Both datasets are structured as multiple-choice question-answering tasks, accompanied by expert-written explanations. We evaluate seven LLMs on the two datasets using various prompts. Experiments demonstrate that our datasets are harder than previous benchmarks. In-depth automatic and human evaluations of model-generated explanations provide insights into the promise and deficiency of LLMs for explainable medical QA. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.02768</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.02768</id><created>2024-03-05</created><updated>2025-02-01</updated><authors><author><keyname>Bellon</keyname><forenames>Alex</forenames></author><author><keyname>Haller</keyname><forenames>Miro</forenames></author><author><keyname>Labunets</keyname><forenames>Andrey</forenames></author><author><keyname>Liu</keyname><forenames>Enze</forenames></author><author><keyname>Savage</keyname><forenames>Stefan</forenames></author></authors><title>An Empirical Analysis on the Use and Reporting of National Security   Letters</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Government investigatory and surveillance powers are important tools for examining crime and protecting public safety. However, since these tools must be employed in secret, it can be challenging to identify abuses or changes in use that could be of significant public interest. In this paper, we evaluate this phenomenon in the context of National Security Letters (NSLs). NSLs are a form of legal process that empowers parts of the United States federal government to request certain pieces of information for national security purposes. After initial concerns about the lack of public oversight, Congress worked to increase transparency by mandating government agencies to publish aggregated statistics on the NSL usage and by allowing the private sector to report information on NSLs in transparency reports. The implicit goal is that these transparency mechanisms should deter large-scale abuse by making it visible. We evaluate how well these mechanisms work by carefully analyzing the full range of publicly available data related to NSL use. Our findings suggest that they may not lead to the desired public scrutiny as we find published information requires significant manual effort to collect and parse data due to the lack of structure and context. Moreover, we discovered mistakes (subsequently fixed after our reporting to the ODNI), which suggests a lack of active auditing. Taken together, our case study of NSLs provides insights and suggestions for the successful construction of transparency mechanisms that enable effective public auditing. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.04918</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.04918</id><created>2024-03-07</created><updated>2025-02-02</updated><authors><author><keyname>Wang</keyname><forenames>Canran</forenames></author><author><keyname>Wang</keyname><forenames>Jinwen</forenames></author><author><keyname>Zhou</keyname><forenames>Mi</forenames></author><author><keyname>Pham</keyname><forenames>Vinh</forenames></author><author><keyname>Hao</keyname><forenames>Senyue</forenames></author><author><keyname>Zhou</keyname><forenames>Chao</forenames></author><author><keyname>Zhang</keyname><forenames>Ning</forenames></author><author><keyname>Raviv</keyname><forenames>Netanel</forenames></author></authors><title>Secure Information Embedding in Forensic 3D Fingerprinting</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Printer fingerprinting techniques have long played a critical role in forensic applications, including the tracking of counterfeiters and the safeguarding of confidential information. The rise of 3D printing technology introduces significant risks to public safety, enabling individuals with internet access and consumer-grade 3D printers to produce untraceable firearms, counterfeit products, and more. This growing threat calls for a better mechanism to track the production of 3D-printed parts.   Inspired by the success of fingerprinting on traditional 2D printers, we introduce SIDE (\textbf{S}ecure \textbf{I}nformation Embe\textbf{D}ding and \textbf{E}xtraction), a novel fingerprinting framework tailored for 3D printing. SIDE addresses the adversarial challenges of 3D print forensics by offering both secure information embedding and extraction. First, through novel coding-theoretic techniques, SIDE is both~\emph{break-resilient} and~\emph{loss-tolerant}, enabling fingerprint recovery even if the adversary breaks the print into fragments and conceals a portion of them. Second, SIDE further leverages Trusted Execution Environments (TEE) to secure the fingerprint embedding process. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.06947</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.06947</id><created>2024-03-11</created><updated>2025-02-03</updated><authors><author><keyname>Zhang</keyname><forenames>Yuting</forenames></author><author><keyname>Lu</keyname><forenames>Hao</forenames></author><author><keyname>Liu</keyname><forenames>Xin</forenames></author><author><keyname>Chen</keyname><forenames>Yingcong</forenames></author><author><keyname>Wu</keyname><forenames>Kaishun</forenames></author></authors><title>Advancing Generalizable Remote Physiological Measurement through the   Integration of Explicit and Implicit Prior Knowledge</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Remote photoplethysmography (rPPG) is a promising technology that captures physiological signals from face videos, with potential applications in medical health, emotional computing, and biosecurity recognition. The demand for rPPG tasks has expanded from demonstrating good performance on intra-dataset testing to cross-dataset testing (i.e., domain generalization). However, most existing methods have overlooked the prior knowledge of rPPG, resulting in poor generalization ability. In this paper, we propose a novel framework that simultaneously utilizes explicit and implicit prior knowledge in the rPPG task. Specifically, we systematically analyze the causes of noise sources (e.g., different camera, lighting, skin types, and movement) across different domains and incorporate these prior knowledge into the network. Additionally, we leverage a two-branch network to disentangle the physiological feature distribution from noises through implicit label correlation. Our extensive experiments demonstrate that the proposed method not only outperforms state-of-the-art methods on RGB cross-dataset evaluation but also generalizes well from RGB datasets to NIR datasets. The code is available at https://github.com/keke-nice/Greip. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.07733</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.07733</id><created>2024-03-12</created><updated>2025-02-03</updated><authors><author><keyname>Knab</keyname><forenames>Patrick</forenames></author><author><keyname>Marton</keyname><forenames>Sascha</forenames></author><author><keyname>Bartelt</keyname><forenames>Christian</forenames></author></authors><title>Beyond Pixels: Enhancing LIME with Hierarchical Features and   Segmentation Foundation Models</title><categories>cs.CV cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  LIME (Local Interpretable Model-agnostic Explanations) is a popular XAI framework for unraveling decision-making processes in vision machine-learning models. The technique utilizes image segmentation methods to identify fixed regions for calculating feature importance scores as explanations. Therefore, poor segmentation can weaken the explanation and reduce the importance of segments, ultimately affecting the overall clarity of interpretation. To address these challenges, we introduce the DSEG-LIME (Data-Driven Segmentation LIME) framework, featuring: i) a data-driven segmentation for human-recognized feature generation by foundation model integration, and ii) a user-steered granularity in the hierarchical segmentation procedure through composition. Our findings demonstrate that DSEG outperforms on several XAI metrics on pre-trained ImageNet models and improves the alignment of explanations with human-recognized concepts. The code is available under: https://github. com/patrick-knab/DSEG-LIME </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.10380</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.10380</id><created>2024-03-15</created><updated>2025-02-03</updated><authors><author><keyname>Rauch</keyname><forenames>Lukas</forenames></author><author><keyname>Schwinger</keyname><forenames>Raphael</forenames></author><author><keyname>Wirth</keyname><forenames>Moritz</forenames></author><author><keyname>Heinrich</keyname><forenames>René</forenames></author><author><keyname>Huseljic</keyname><forenames>Denis</forenames></author><author><keyname>Herde</keyname><forenames>Marek</forenames></author><author><keyname>Lange</keyname><forenames>Jonas</forenames></author><author><keyname>Kahl</keyname><forenames>Stefan</forenames></author><author><keyname>Sick</keyname><forenames>Bernhard</forenames></author><author><keyname>Tomforde</keyname><forenames>Sven</forenames></author><author><keyname>Scholz</keyname><forenames>Christoph</forenames></author></authors><title>BirdSet: A Large-Scale Dataset for Audio Classification in Avian   Bioacoustics</title><categories>cs.SD cs.AI eess.AS</categories><comments>accepted@ICLR2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning (DL) has greatly advanced audio classification, yet the field is limited by the scarcity of large-scale benchmark datasets that have propelled progress in other domains. While AudioSet is a pivotal step to bridge this gap as a universal-domain dataset, its restricted accessibility and limited range of evaluation use cases challenge its role as the sole resource. Therefore, we introduce \texttt{BirdSet}, a large-scale benchmark dataset for audio classification focusing on avian bioacoustics. \texttt{BirdSet} surpasses AudioSet with over 6,800 recording hours~($\uparrow\!17\%$) from nearly 10,000 classes~($\uparrow\!18\times$) for training and more than 400 hours~($\uparrow\!7\times$) across eight strongly labeled evaluation datasets. It serves as a versatile resource for use cases such as multi-label classification, covariate shift or self-supervised learning. We benchmark six well-known DL models in multi-label classification across three distinct training scenarios and outline further evaluation use cases in audio classification. We host our dataset on Hugging Face for easy accessibility and offer an extensive codebase to reproduce our results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.10771</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.10771</id><created>2024-03-15</created><updated>2025-02-01</updated><authors><author><keyname>Cao</keyname><forenames>Junyu</forenames></author><author><keyname>Bayati</keyname><forenames>Mohsen</forenames></author></authors><title>A Probabilistic Approach for Model Alignment with Human Comparisons</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A growing trend involves integrating human knowledge into learning frameworks, leveraging subtle human feedback to refine AI models. While these approaches have shown promising results in practice, the theoretical understanding of when and why such approaches are effective remains limited. This work takes steps toward developing a theoretical framework for analyzing the conditions under which human comparisons can enhance the traditional supervised learning process. Specifically, this paper studies the effective use of noisy-labeled data and human comparison data to address challenges arising from noisy environment and high-dimensional models. We propose a two-stage "Supervised Learning+Learning from Human Feedback" (SL+LHF) framework that connects machine learning with human feedback through a probabilistic bisection approach. The two-stage framework first learns low-dimensional representations from noisy-labeled data via an SL procedure and then uses human comparisons to improve the model alignment. To examine the efficacy of the alignment phase, we introduce a concept, termed the "label-noise-to-comparison-accuracy" (LNCA) ratio. This paper identifies from a theoretical perspective the conditions under which the "SL+LHF" framework outperforms the pure SL approach; we then leverage this LNCA ratio to highlight the advantage of incorporating human evaluators in reducing sample complexity. We validate that the LNCA ratio meets the proposed conditions for its use through a case study conducted via Amazon Mechanical Turk (MTurk). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.10874</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.10874</id><created>2024-03-16</created><authors><author><keyname>Naik</keyname><forenames>Lakshadeep</forenames></author><author><keyname>Iversen</keyname><forenames>Thorbjørn Mosekjær</forenames></author><author><keyname>Kramberger</keyname><forenames>Aljaz</forenames></author><author><keyname>Krüger</keyname><forenames>Norbert</forenames></author></authors><title>Robotic Task Success Evaluation Under Multi-modal Non-Parametric Object   Pose Uncertainty</title><categories>cs.RO</categories><comments>Submitted to IROS 2024</comments><journal-ref>Industrial Robot (Article publication date: 31 January 2025)</journal-ref><doi>10.1108/IR-10-2024-0467</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate 6D object pose estimation is essential for various robotic tasks. Uncertain pose estimates can lead to task failures; however, a certain degree of error in the pose estimates is often acceptable. Hence, by quantifying errors in the object pose estimate and acceptable errors for task success, robots can make informed decisions. This is a challenging problem as both the object pose uncertainty and acceptable error for the robotic task are often multi-modal and cannot be parameterized with commonly used uni-modal distributions. In this paper, we introduce a framework for evaluating robotic task success under object pose uncertainty, representing both the estimated error space of the object pose and the acceptable error space for task success using multi-modal non-parametric probability distributions. The proposed framework pre-computes the acceptable error space for task success using dynamic simulations and subsequently integrates the pre-computed acceptable error space over the estimated error space of the object pose to predict the likelihood of the task success. We evaluated the proposed framework on two mobile manipulation tasks. Our results show that by representing the estimated and the acceptable error space using multi-modal non-parametric distributions, we achieve higher task success rates and fewer failures. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.13123</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.13123</id><created>2024-03-19</created><updated>2025-02-02</updated><authors><author><keyname>Scott</keyname><forenames>Jennifer</forenames></author><author><keyname>Tůma</keyname><forenames>Miroslav</forenames></author></authors><title>Developing robust incomplete Cholesky factorizations in half precision   arithmetic</title><categories>math.NA cs.NA</categories><comments>21 pages</comments><msc-class>65F08, 65F10</msc-class><doi>10.1007/s11075-025-02015-x</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Incomplete factorizations have long been popular general-purpose algebraic preconditioners for solving large sparse linear systems of equations. Guaranteeing the factorization is breakdown free while computing a high quality preconditioner is challenging. A resurgence of interest in using low precision arithmetic makes the search for robustness more important and more challenging. In this paper, we focus on ill-conditioned symmetric positive definite problems and explore a number of approaches for preventing and handling breakdowns: prescaling of the system matrix, a look-ahead strategy to anticipate breakdown as early as possible, the use of global shifts, and a modification of an idea developed in the field of numerical optimization for the complete Cholesky factorization of dense matrices. Our numerical simulations target highly ill-conditioned sparse linear systems with the goal of computing the factors in half precision arithmetic and then achieving double precision accuracy using mixed precision refinement. We also consider the often overlooked issue of growth in the sizes of entries in the factors that can occur when using any precision and can render the computed factors ineffective as preconditioners. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.13164</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.13164</id><created>2024-03-19</created><updated>2025-02-02</updated><authors><author><keyname>Zong</keyname><forenames>Yongshuo</forenames></author><author><keyname>Bohdal</keyname><forenames>Ondrej</forenames></author><author><keyname>Hospedales</keyname><forenames>Timothy</forenames></author></authors><title>VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning</title><categories>cs.LG</categories><comments>ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models (LLMs) famously exhibit emergent in-context learning (ICL) -- the ability to rapidly adapt to new tasks using few-shot examples provided as a prompt, without updating the model's weights. Built on top of LLMs, vision large language models (VLLMs) have advanced significantly in areas such as recognition, reasoning, and grounding. However, investigations into \emph{multimodal ICL} have predominantly focused on few-shot visual question answering (VQA), and image captioning, which we will show neither exploit the strengths of ICL, nor test its limitations. The broader capabilities and limitations of multimodal ICL remain under-explored. In this study, we introduce a comprehensive benchmark VL-ICL Bench for multimodal in-context learning, encompassing a broad spectrum of tasks that involve both images and text as inputs and outputs, and different types of challenges, from {perception to reasoning and long context length}. We evaluate the abilities of state-of-the-art VLLMs against this benchmark suite, revealing their diverse strengths and weaknesses, and showing that even the most advanced models, such as GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks, and the associated strengths and limitations of existing models, we hope that our dataset will inspire future work on enhancing the in-context learning capabilities of VLLMs, as well as inspire new applications that leverage VLLM ICL. The code and dataset are available at https://github.com/ys-zong/VL-ICL. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.13189</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.13189</id><created>2024-03-19</created><updated>2025-01-31</updated><authors><author><keyname>Gopalakrishnan</keyname><forenames>Jay</forenames></author><author><keyname>Guzman</keyname><forenames>Johnny</forenames></author><author><keyname>Lee</keyname><forenames>Jeonghun J.</forenames></author></authors><title>The Johnson-Mercier-Krizek elasticity element in any dimensions</title><categories>math.NA cs.NA</categories><comments>32 pages</comments><msc-class>65N12, 65N15, 65N30</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Mixed methods for linear elasticity with strongly symmetric stresses of lowest order are studied in this paper. On each simplex, the stress space has piecewise linear components with respect to its Alfeld split (which connects the vertices to barycenter), generalizing the Johnson-Mercier two-dimensional element to higher dimensions. Further reductions in the stress space in the three-dimensional case (to 24 degrees of freedom per tetrahedron) are possible when the displacement space is reduced to local rigid displacements. Proofs of optimal error estimates of numerical solutions and improved error estimates via postprocessing and the duality argument are presented. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.13838</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.13838</id><created>2024-03-13</created><updated>2025-02-01</updated><authors><author><keyname>Li</keyname><forenames>Xihan</forenames></author><author><keyname>Li</keyname><forenames>Xing</forenames></author><author><keyname>Chen</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>Xing</forenames></author><author><keyname>Yuan</keyname><forenames>Mingxuan</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author></authors><title>Circuit Transformer: A Transformer That Preserves Logical Equivalence</title><categories>cs.LG cs.AR</categories><comments>In ICLR 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Implementing Boolean functions with circuits consisting of logic gates is fundamental in digital computer design. However, the implemented circuit must be exactly equivalent, which hinders generative neural approaches on this task due to their occasionally wrong predictions. In this study, we introduce a generative neural model, the "Circuit Transformer", which eliminates such wrong predictions and produces logic circuits strictly equivalent to given Boolean functions. The main idea is a carefully designed decoding mechanism that builds a circuit step-by-step by generating tokens, which has beneficial "cutoff properties" that block a candidate token once it invalidate equivalence. In such a way, the proposed model works similar to typical LLMs while logical equivalence is strictly preserved. A Markov decision process formulation is also proposed for optimizing certain objectives of circuits. Experimentally, we trained an 88-million-parameter Circuit Transformer to generate equivalent yet more compact forms of input circuits, outperforming existing neural approaches on both synthetic and real world benchmarks, without any violation of equivalence constraints. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.17006</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.17006</id><created>2024-03-25</created><updated>2025-02-03</updated><authors><author><keyname>Chen</keyname><forenames>Bin</forenames></author><author><keyname>Zhang</keyname><forenames>Zhenyu</forenames></author><author><keyname>Li</keyname><forenames>Weiqi</forenames></author><author><keyname>Zhao</keyname><forenames>Chen</forenames></author><author><keyname>Yu</keyname><forenames>Jiwen</forenames></author><author><keyname>Zhao</keyname><forenames>Shijie</forenames></author><author><keyname>Chen</keyname><forenames>Jie</forenames></author><author><keyname>Zhang</keyname><forenames>Jian</forenames></author></authors><title>Invertible Diffusion Models for Compressed Sensing</title><categories>cs.CV</categories><comments>Accepted for publication in IEEE Transactions on Pattern Analysis and   Machine Intelligence (TPAMI)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While deep neural networks (NN) significantly advance image compressed sensing (CS) by improving reconstruction quality, the necessity of training current CS NNs from scratch constrains their effectiveness and hampers rapid deployment. Although recent methods utilize pre-trained diffusion models for image reconstruction, they struggle with slow inference and restricted adaptability to CS. To tackle these challenges, this paper proposes Invertible Diffusion Models (IDM), a novel efficient, end-to-end diffusion-based CS method. IDM repurposes a large-scale diffusion sampling process as a reconstruction model, and fine-tunes it end-to-end to recover original images directly from CS measurements, moving beyond the traditional paradigm of one-step noise estimation learning. To enable such memory-intensive end-to-end fine-tuning, we propose a novel two-level invertible design to transform both (1) multi-step sampling process and (2) noise estimation U-Net in each step into invertible networks. As a result, most intermediate features are cleared during training to reduce up to 93.8% GPU memory. In addition, we develop a set of lightweight modules to inject measurements into noise estimator to further facilitate reconstruction. Experiments demonstrate that IDM outperforms existing state-of-the-art CS networks by up to 2.64dB in PSNR. Compared to the recent diffusion-based approach DDNM, our IDM achieves up to 10.09dB PSNR gain and 14.54 times faster inference. Code is available at https://github.com/Guaishou74851/IDM. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.17105</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.17105</id><created>2024-03-25</created><updated>2025-02-01</updated><authors><author><keyname>Chien</keyname><forenames>Eli</forenames></author><author><keyname>Wang</keyname><forenames>Haoyu</forenames></author><author><keyname>Chen</keyname><forenames>Ziang</forenames></author><author><keyname>Li</keyname><forenames>Pan</forenames></author></authors><title>Certified Machine Unlearning via Noisy Stochastic Gradient Descent</title><categories>cs.LG cs.CR</categories><comments>NeurIPS 2024. Updated title and introduction while the main results   are the same. Add a discussion on utility guarantee. arXiv admin note: text   overlap with arXiv:2401.10371</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  ``The right to be forgotten'' ensured by laws for user data privacy becomes increasingly important. Machine unlearning aims to efficiently remove the effect of certain data points on the trained model parameters so that it can be approximately the same as if one retrains the model from scratch. We propose to leverage projected noisy stochastic gradient descent for unlearning and establish its first approximate unlearning guarantee under the convexity assumption. Our approach exhibits several benefits, including provable complexity saving compared to retraining, and supporting sequential and batch unlearning. Both of these benefits are closely related to our new results on the infinite Wasserstein distance tracking of the adjacent (un)learning processes. Extensive experiments show that our approach achieves a similar utility under the same privacy constraint while using $2\%$ and $10\%$ of the gradient computations compared with the state-of-the-art gradient-based approximate unlearning methods for mini-batch and full-batch settings, respectively. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.17235</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.17235</id><created>2024-03-25</created><updated>2025-02-01</updated><authors><author><keyname>Zhao</keyname><forenames>Qianhong</forenames></author><author><keyname>Tao</keyname><forenames>Gang</forenames></author></authors><title>A Discrete-Time Least-Squares Adaptive State Tracking Control Scheme   with A Mobile-Robot System Study</title><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops an adaptive state tracking control scheme for discrete-time systems, using the least-squares algorithm, as the new solution to the long-standing discrete-time adaptive state tracking control problem to which the Lyapunov method (well-developed for the continuous-time adaptive state tracking problem) is not applicable. The new adaptive state tracking scheme is based on a recently-developed new discrete-time error model which has been used for gradient algorithm based state tracking control schemes, and uses the least-squares algorithm for parameter adaptation. The new least-squares algorithm is derived to minimize an accumulative estimation error, to ensure certain optimality for parameter estimation. The system stability and output tracking properties are studied. Technical results are presented in terms of plant-model matching, error model, adaptive law, optimality formulation, and stability and tracking analysis. The developed adaptive control scheme is applied to a discrete-time multiple mobile robot system to meet an adaptive state tracking objective. In addition, a collision avoidance mechanism is proposed to prevent collisions in the whole tracking process. Simulation results are presented, which verify the desired system state tracking properties under the developed least-squares algorithm based adaptive control scheme. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.18072</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.18072</id><created>2024-03-26</created><updated>2025-02-01</updated><authors><author><keyname>Zhong</keyname><forenames>Shijie</forenames></author><author><keyname>Shen</keyname><forenames>Wanggang</forenames></author><author><keyname>Catanach</keyname><forenames>Tommie</forenames></author><author><keyname>Huan</keyname><forenames>Xun</forenames></author></authors><title>Goal-Oriented Bayesian Optimal Experimental Design for Nonlinear Models   using Markov Chain Monte Carlo</title><categories>stat.CO cs.LG stat.ME stat.ML</categories><comments>28 pages, 19 figures</comments><msc-class>62K05, 62F15, 62B15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal experimental design (OED) provides a systematic approach to quantify and maximize the value of experimental data. Under a Bayesian approach, conventional OED maximizes the expected information gain (EIG) on model parameters. However, we are often interested in not the parameters themselves, but predictive quantities of interest (QoIs) that depend on the parameters in a nonlinear manner. We present a computational framework of predictive goal-oriented OED (GO-OED) suitable for nonlinear observation and prediction models, which seeks the experimental design providing the greatest EIG on the QoIs. In particular, we propose a nested Monte Carlo estimator for the QoI EIG, featuring Markov chain Monte Carlo for posterior sampling and kernel density estimation for evaluating the posterior-predictive density and its Kullback-Leibler divergence from the prior-predictive. The GO-OED design is then found by maximizing the EIG over the design space using Bayesian optimization. We demonstrate the effectiveness of the overall nonlinear GO-OED method, and illustrate its differences versus conventional non-GO-OED, through various test problems and an application of sensor placement for source inversion in a convection-diffusion field. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.19418</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.19418</id><created>2024-03-28</created><updated>2025-01-31</updated><authors><author><keyname>Zimmer</keyname><forenames>Michael F.</forenames></author></authors><title>Constants of Motion for Conserved and Non-conserved Dynamics</title><categories>cs.LG nlin.CD</categories><comments>13 pages, 4 figures. (corrected typos, removed Fig4, renamed phi-bar   to phi)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper begins with a dynamical model that was obtained by applying a machine learning technique (FJet) to time-series data; this dynamical model is then analyzed with Lie symmetry techniques to obtain constants of motion. This analysis is performed on both the conserved and non-conserved cases of the 1D and 2D harmonic oscillators. For the 1D oscillator, constants are found in the cases where the system is underdamped, overdamped, and critically damped. The novel existence of such a constant for a non-conserved model is interpreted as a manifestation of the conservation of energy of the {\em total} system (i.e., oscillator plus dissipative environment). For the 2D oscillator, constants are found for the isotropic and anisotropic cases, including when the frequencies are incommensurate; it is also generalized to arbitrary dimensions. In addition, a constant is identified which generalizes angular momentum for all ratios of the frequencies. The approach presented here can produce {\em multiple} constants of motion from a {\em single}, generic data set. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.19622</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.19622</id><created>2024-03-28</created><updated>2025-02-01</updated><authors><author><keyname>Chen</keyname><forenames>Zeren</forenames></author><author><keyname>Shi</keyname><forenames>Zhelun</forenames></author><author><keyname>Lu</keyname><forenames>Xiaoya</forenames></author><author><keyname>He</keyname><forenames>Lehan</forenames></author><author><keyname>Qian</keyname><forenames>Sucheng</forenames></author><author><keyname>Yin</keyname><forenames>Zhenfei</forenames></author><author><keyname>Ouyang</keyname><forenames>Wanli</forenames></author><author><keyname>Shao</keyname><forenames>Jing</forenames></author><author><keyname>Qiao</keyname><forenames>Yu</forenames></author><author><keyname>Lu</keyname><forenames>Cewu</forenames></author><author><keyname>Sheng</keyname><forenames>Lu</forenames></author></authors><title>RH20T-P: A Primitive-Level Robotic Dataset Towards Composable   Generalization Agents</title><categories>cs.RO cs.CV</categories><comments>18 pages, 11 figures, 7 tables. Accepted by NeurIPS 2024 Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Achieving generalizability in solving out-of-distribution tasks is one of the ultimate goals of learning robotic manipulation. Recent progress of Vision-Language Models (VLMs) has shown that VLM-based task planners can alleviate the difficulty of solving novel tasks, by decomposing the compounded tasks as a plan of sequentially executing primitive-level skills that have been already mastered. It is also promising for robotic manipulation to adapt such composable generalization ability, in the form of composable generalization agents (CGAs). However, the community lacks of reliable design of primitive skills and a sufficient amount of primitive-level data annotations. Therefore, we propose RH20T-P, a primitive-level robotic manipulation dataset, which contains about 38k video clips covering 67 diverse manipulation tasks in real-world scenarios. Each clip is manually annotated according to a set of meticulously designed primitive skills that are common in robotic manipulation. Furthermore, we standardize a plan-execute CGA paradigm and implement an exemplar baseline called RA-P on our RH20T-P, whose positive performance on solving unseen tasks validates that the proposed dataset can offer composable generalization ability to robotic manipulation agents. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.01642</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.01642</id><created>2024-04-02</created><updated>2025-01-31</updated><authors><author><keyname>Chi</keyname><forenames>Zhiming</forenames></author><author><keyname>Ma</keyname><forenames>Jianan</forenames></author><author><keyname>Yang</keyname><forenames>Pengfei</forenames></author><author><keyname>Huang</keyname><forenames>Cheng-Chao</forenames></author><author><keyname>Li</keyname><forenames>Renjue</forenames></author><author><keyname>Huang</keyname><forenames>Xiaowei</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author></authors><title>Patch Synthesis for Property Repair of Deep Neural Networks</title><categories>cs.LG cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks (DNNs) are prone to various dependability issues, such as adversarial attacks, which hinder their adoption in safety-critical domains. Recently, NN repair techniques have been proposed to address these issues while preserving original performance by locating and modifying guilty neurons and their parameters. However, existing repair approaches are often limited to specific data sets and do not provide theoretical guarantees for the effectiveness of the repairs. To address these limitations, we introduce PatchPro, a novel patch-based approach for property-level repair of DNNs, focusing on local robustness. The key idea behind PatchPro is to construct patch modules that, when integrated with the original network, provide specialized repairs for all samples within the robustness neighborhood while maintaining the network's original performance. Our method incorporates formal verification and a heuristic mechanism for allocating patch modules, enabling it to defend against adversarial attacks and generalize to other inputs. PatchPro demonstrates superior efficiency, scalability, and repair success rates compared to existing DNN repair methods, i.e., realizing provable property-level repair for 100% cases across multiple high-dimensional datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.02252</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.02252</id><created>2024-04-02</created><updated>2025-01-31</updated><authors><author><keyname>Koo</keyname><forenames>Junghyun</forenames></author><author><keyname>Wichern</keyname><forenames>Gordon</forenames></author><author><keyname>Germain</keyname><forenames>Francois G.</forenames></author><author><keyname>Khurana</keyname><forenames>Sameer</forenames></author><author><keyname>Roux</keyname><forenames>Jonathan Le</forenames></author></authors><title>SMITIN: Self-Monitored Inference-Time INtervention for Generative Music   Transformers</title><categories>cs.SD eess.AS</categories><journal-ref>IEEE Open Journal of Signal Processing, 2025</journal-ref><doi>10.1109/OJSP.2025.3534686</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Self-Monitored Inference-Time INtervention (SMITIN), an approach for controlling an autoregressive generative music transformer using classifier probes. These simple logistic regression probes are trained on the output of each attention head in the transformer using a small dataset of audio examples both exhibiting and missing a specific musical trait (e.g., the presence/absence of drums, or real/synthetic music). We then steer the attention heads in the probe direction, ensuring the generative model output captures the desired musical trait. Additionally, we monitor the probe output to avoid adding an excessive amount of intervention into the autoregressive generation, which could lead to temporally incoherent music. We validate our results objectively and subjectively for both audio continuation and text-to-music applications, demonstrating the ability to add controls to large generative models for which retraining or even fine-tuning is impractical for most musicians.   Audio samples of the proposed intervention approach are available on our demo page http://tinyurl.com/smitin . </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.02896</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.02896</id><created>2024-04-03</created><updated>2025-01-31</updated><authors><author><keyname>Zimmer</keyname><forenames>Michael F.</forenames></author></authors><title>Comment on "Machine learning conservation laws from differential   equations"</title><categories>cs.LG</categories><comments>3 pages, 1 figure; comment on   https://doi.org/10.1103/PhysRevE.106.045307. This update now includes:   Error#7, Fig1, Eqn5, a new paragraph in Afterword</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper [1] by Liu, Madhavan, and Tegmark sought to use machine learning methods to elicit known conservation laws for several systems. However, in their example of a damped 1D harmonic oscillator they made seven serious errors, causing both their method and result to be incorrect. In this Comment, those errors are reviewed. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.06787</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.06787</id><created>2024-04-10</created><updated>2025-02-02</updated><authors><author><keyname>Li</keyname><forenames>Wenqian</forenames></author><author><keyname>Pang</keyname><forenames>Yan</forenames></author></authors><title>Private Wasserstein Distance</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wasserstein distance is a key metric for quantifying data divergence from a distributional perspective. However, its application in privacy-sensitive environments, where direct sharing of raw data is prohibited, presents significant challenges. Existing approaches, such as Differential Privacy and Federated Optimization, have been employed to estimate the Wasserstein distance under such constraints. However, these methods often fall short when both accuracy and security are required. In this study, we explore the inherent triangular properties within the Wasserstein space, leading to a novel solution named TriangleWad. This approach facilitates the fast computation of the Wasserstein distance between datasets stored across different entities, ensuring that raw data remain completely hidden. TriangleWad not only strengthens resistance to potential attacks but also preserves high estimation accuracy. Through extensive experiments across various tasks involving both image and text data, we demonstrate its superior performance and significant potential for real-world applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.07344</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.07344</id><created>2024-04-10</created><updated>2025-02-01</updated><authors><author><keyname>Kruzliak</keyname><forenames>Andrej</forenames></author><author><keyname>Hartvich</keyname><forenames>Jiri</forenames></author><author><keyname>Patni</keyname><forenames>Shubhan P.</forenames></author><author><keyname>Rustler</keyname><forenames>Lukas</forenames></author><author><keyname>Behrens</keyname><forenames>Jan Kristof</forenames></author><author><keyname>Abu-Dakka</keyname><forenames>Fares J.</forenames></author><author><keyname>Mikolajczyk</keyname><forenames>Krystian</forenames></author><author><keyname>Kyrki</keyname><forenames>Ville</forenames></author><author><keyname>Hoffmann</keyname><forenames>Matej</forenames></author></authors><title>Interactive Learning of Physical Object Properties Through Robot   Manipulation and Database of Object Measurements</title><categories>cs.RO cs.AI cs.IT math.IT</categories><comments>8 pages, 9 figures</comments><acm-class>I.2.9</acm-class><journal-ref>Intelligent Robots and Systems (IROS), 2024 IEEE/RSJ International   Conference on</journal-ref><doi>10.1109/IROS58592.2024.10802249</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This work presents a framework for automatically extracting physical object properties, such as material composition, mass, volume, and stiffness, through robot manipulation and a database of object measurements. The framework involves exploratory action selection to maximize learning about objects on a table. A Bayesian network models conditional dependencies between object properties, incorporating prior probability distributions and uncertainty associated with measurement actions. The algorithm selects optimal exploratory actions based on expected information gain and updates object properties through Bayesian inference. Experimental evaluation demonstrates effective action selection compared to a baseline and correct termination of the experiments if there is nothing more to be learned. The algorithm proved to behave intelligently when presented with trick objects with material properties in conflict with their appearance. The robot pipeline integrates with a logging module and an online database of objects, containing over 24,000 measurements of 63 objects with different grippers. All code and data are publicly available, facilitating automatic digitization of objects and their physical properties through exploratory manipulations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.07797</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.07797</id><created>2024-04-11</created><updated>2025-02-01</updated><authors><author><keyname>Wang</keyname><forenames>Hongyu</forenames></author><author><keyname>Li</keyname><forenames>Ying</forenames></author><author><keyname>Huang</keyname><forenames>Ronghong</forenames></author><author><keyname>Mi</keyname><forenames>Xianghang</forenames></author></authors><title>Detecting and Understanding the Promotion of Illicit Goods and Services   on Twitter</title><categories>cs.CR cs.SI</categories><doi>10.1145/10.1145/3696410.3714550</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we reveal, for the first time, popular online social networks (especially Twitter) are being extensively abused by miscreants to promote illicit goods and services of diverse categories. This study is made possible by multiple machine learning tools that are designed to detect and analyze Posts of Illicit Promotion (PIPs) as well as revealing their underlying promotion campaigns. Particularly, we observe that PIPs are prevalent on Twitter, along with extensive visibility on other three popular OSNs including YouTube, Facebook, and TikTok. For instance, applying our PIP hunter to the Twitter platform for 6 months has led to the discovery of 12 million distinct PIPs which are widely distributed in 5 major natural languages and 10 illicit categories, e.g., drugs, data leakage, gambling, and weapon sales. Along the discovery of PIPs are 580K Twitter accounts publishing PIPs as well as 37K distinct instant messaging accounts that are embedded in PIPs and serve as next hops of communication with prospective customers. Also, an arms race between Twitter and illicit promotion operators is also observed. Especially, 90% PIPs can survice the first two months since getting published on Twitter, which is likely due to the diverse evasion tactics adopted by miscreants to masquerade PIPs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.08607</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.08607</id><created>2024-04-12</created><updated>2025-01-31</updated><authors><author><keyname>Wang</keyname><forenames>Liangzhi</forenames></author><author><keyname>Chen</keyname><forenames>Chen</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author></authors><title>Learning-Based Joint Antenna Selection and Precoding Design for   Cell-Free MIMO Networks</title><categories>cs.IT eess.SP math.IT</categories><comments>This work has been submitted to the IEEE for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a downlink cell-free multiple-input multiple-output (MIMO) network in which multiple multi-antenna access points (APs) serve multiple users via coherent joint transmission. In order to reduce the energy consumption by radio frequency components, each AP selects a subset of antennas for downlink data transmission after estimating the channel state information (CSI). We aim to maximize the sum spectral efficiency by jointly optimizing the antenna selection and precoding design. To alleviate the fronthaul overhead and enable real-time network operation, we propose a distributed scalable machine learning algorithm. In particular, at each AP, we deploy a convolutional neural network (CNN) for antenna selection and a graph neural network (GNN) for precoding design. Different from conventional centralized solutions that require a large amount of CSI and signaling exchange among the APs, the proposed distributed machine learning algorithm takes only locally estimated CSI as input. With well-trained learning models, it is shown that the proposed algorithm significantly outperforms the distributed baseline schemes and achieves a sum spectral efficiency comparable to its centralized counterpart. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.09150</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.09150</id><created>2024-04-14</created><updated>2025-02-02</updated><authors><author><keyname>She</keyname><forenames>Qijin</forenames></author><author><keyname>Zhang</keyname><forenames>Shishun</forenames></author><author><keyname>Ye</keyname><forenames>Yunfan</forenames></author><author><keyname>Hu</keyname><forenames>Ruizhen</forenames></author><author><keyname>Xu</keyname><forenames>Kai</forenames></author></authors><title>Learning Cross-hand Policies for High-DOF Reaching and Grasping</title><categories>cs.RO cs.GR</categories><comments>ECCV 2024</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reaching-and-grasping is a fundamental skill for robotic manipulation, but existing methods usually train models on a specific gripper and cannot be reused on another gripper. In this paper, we propose a novel method that can learn a unified policy model that can be easily transferred to different dexterous grippers. Our method consists of two stages: a gripper-agnostic policy model that predicts the displacements of pre-defined key points on the gripper, and a gripper-specific adaptation model that translates these displacements into adjustments for controlling the grippers' joints. The gripper state and interactions with objects are captured at the finger level using robust geometric representations, integrated with a transformer-based network to address variations in gripper morphology and geometry. In the experiments, we evaluate our method on several dexterous grippers and diverse objects, and the result shows that our method significantly outperforms the baseline methods. Pioneering the transfer of grasp policies across dexterous grippers, our method effectively demonstrates its potential for learning generalizable and transferable manipulation skills for various robotic hands. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.09498</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.09498</id><created>2024-04-15</created><updated>2025-02-02</updated><authors><author><keyname>Xie</keyname><forenames>Xinyu</forenames></author><author><keyname>Cui</keyname><forenames>Yawen</forenames></author><author><keyname>Tan</keyname><forenames>Tao</forenames></author><author><keyname>Zheng</keyname><forenames>Xubin</forenames></author><author><keyname>Yu</keyname><forenames>Zitong</forenames></author></authors><title>FusionMamba: Dynamic Feature Enhancement for Multimodal Image Fusion   with Mamba</title><categories>cs.CV</categories><comments>Accepted by Visual Intelligence. Codes are at   https://github.com/millieXie/FusionMamba</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multimodal image fusion aims to integrate information from different imaging techniques to produce a comprehensive, detail-rich single image for downstream vision tasks. Existing methods based on local convolutional neural networks (CNNs) struggle to capture global features efficiently, while Transformer-based models are computationally expensive, although they excel at global modeling. Mamba addresses these limitations by leveraging selective structured state space models (S4) to effectively handle long-range dependencies while maintaining linear complexity. In this paper, we propose FusionMamba, a novel dynamic feature enhancement framework that aims to overcome the challenges faced by CNNs and Vision Transformers (ViTs) in computer vision tasks. The framework improves the visual state-space model Mamba by integrating dynamic convolution and channel attention mechanisms, which not only retains its powerful global feature modeling capability, but also greatly reduces redundancy and enhances the expressiveness of local features. In addition, we have developed a new module called the dynamic feature fusion module (DFFM). It combines the dynamic feature enhancement module (DFEM) for texture enhancement and disparity perception with the cross-modal fusion Mamba module (CMFM), which focuses on enhancing the inter-modal correlation while suppressing redundant information. Experiments show that FusionMamba achieves state-of-the-art performance in a variety of multimodal image fusion tasks as well as downstream experiments, demonstrating its broad applicability and superiority. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.10292</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.10292</id><created>2024-04-16</created><updated>2025-01-31</updated><authors><author><keyname>Sun</keyname><forenames>Jintao</forenames></author><author><keyname>Fei</keyname><forenames>Hao</forenames></author><author><keyname>Zheng</keyname><forenames>Zhedong</forenames></author><author><keyname>Ding</keyname><forenames>Gangyi</forenames></author></authors><title>From Data Deluge to Data Curation: A Filtering-WoRA Paradigm for   Efficient Text-based Person Search</title><categories>cs.CV cs.MM</categories><comments>11 pages, 8 figures, Proceedings of the ACM Web Conference 2025 (WWW   '25)</comments><doi>10.1145/3696410.3714788</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In text-based person search endeavors, data generation has emerged as a prevailing practice, addressing concerns over privacy preservation and the arduous task of manual annotation. Although the number of synthesized data can be infinite in theory, the scientific conundrum persists that how much generated data optimally fuels subsequent model training. We observe that only a subset of the data in these constructed datasets plays a decisive role. Therefore, we introduce a new Filtering-WoRA paradigm, which contains a filtering algorithm to identify this crucial data subset and WoRA (Weighted Low-Rank Adaptation) learning strategy for light fine-tuning. The filtering algorithm is based on the cross-modality relevance to remove the lots of coarse matching synthesis pairs. As the number of data decreases, we do not need to fine-tune the entire model. Therefore, we propose a WoRA learning strategy to efficiently update a minimal portion of model parameters. WoRA streamlines the learning process, enabling heightened efficiency in extracting knowledge from fewer, yet potent, data instances. Extensive experimentation validates the efficacy of pretraining, where our model achieves advanced and efficient retrieval performance on challenging real-world benchmarks. Notably, on the CUHK-PEDES dataset, we have achieved a competitive mAP of 67.02% while reducing model training time by 19.82%. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.10483</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.10483</id><created>2024-04-16</created><updated>2025-02-02</updated><authors><author><keyname>Azam</keyname><forenames>Ubaid</forenames></author><author><keyname>Razzak</keyname><forenames>Imran</forenames></author><author><keyname>Vishwakarma</keyname><forenames>Shelly</forenames></author><author><keyname>Hacid</keyname><forenames>Hakim</forenames></author><author><keyname>Zhang</keyname><forenames>Dell</forenames></author><author><keyname>Jameel</keyname><forenames>Shoaib</forenames></author></authors><title>From Uncertainty to Trust: Kernel Dropout for AI-Powered Medical   Predictions</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  AI-driven medical predictions with trustworthy confidence are essential for ensuring the responsible use of AI in healthcare applications. The growing capabilities of AI raise questions about their trustworthiness in healthcare, particularly due to opaque decision-making and limited data availability. This paper proposes a novel approach to address these challenges, introducing a Bayesian Monte Carlo Dropout model with kernel modelling. Our model is designed to enhance reliability on small medical datasets, a crucial barrier to the wider adoption of AI in healthcare. This model leverages existing language models for improved effectiveness and seamlessly integrates with current workflows. Extensive evaluations of public medical datasets showcase our model's superior performance across diverse tasks. We demonstrate significant improvements in reliability, even with limited data, offering a promising step towards building trust in AI-driven medical predictions and unlocking its potential to improve patient care. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.11826</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.11826</id><created>2024-04-17</created><updated>2025-01-31</updated><authors><author><keyname>Kim</keyname><forenames>Minbeom</forenames></author><author><keyname>Lee</keyname><forenames>Hwanhee</forenames></author><author><keyname>Park</keyname><forenames>Joonsuk</forenames></author><author><keyname>Lee</keyname><forenames>Hwaran</forenames></author><author><keyname>Jung</keyname><forenames>Kyomin</forenames></author></authors><title>AdvisorQA: Towards Helpful and Harmless Advice-seeking Question   Answering with Collective Intelligence</title><categories>cs.CL</categories><comments>19 pages, 11 figures</comments><journal-ref>NAACL 2025</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As the integration of large language models into daily life is on the rise, there is a clear gap in benchmarks for advising on subjective and personal dilemmas. To address this, we introduce AdvisorQA, the first benchmark developed to assess LLMs' capability in offering advice for deeply personalized concerns, utilizing the LifeProTips subreddit forum. This forum features a dynamic interaction where users post advice-seeking questions, receiving an average of 8.9 advice per query, with 164.2 upvotes from hundreds of users, embodying a collective intelligence framework. Therefore, we've completed a benchmark encompassing daily life questions, diverse corresponding responses, and majority vote ranking to train our helpfulness metric. Baseline experiments validate the efficacy of AdvisorQA through our helpfulness metric, GPT-4, and human evaluation, analyzing phenomena beyond the trade-off between helpfulness and harmlessness. AdvisorQA marks a significant leap in enhancing QA systems for providing personalized, empathetic advice, showcasing LLMs' improved understanding of human subjectivity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.12355</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.12355</id><created>2024-04-18</created><updated>2025-01-31</updated><authors><author><keyname>Sun</keyname><forenames>Jingmin</forenames></author><author><keyname>Liu</keyname><forenames>Yuxuan</forenames></author><author><keyname>Zhang</keyname><forenames>Zecheng</forenames></author><author><keyname>Schaeffer</keyname><forenames>Hayden</forenames></author></authors><title>Towards a Foundation Model for Partial Differential Equations:   Multi-Operator Learning and Extrapolation</title><categories>cs.LG cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Foundation models, such as large language models, have demonstrated success in addressing various language and image processing tasks. In this work, we introduce a multi-modal foundation model for scientific problems, named PROSE-PDE. Our model, designed for bi-modality to bi-modality learning, is a multi-operator learning approach which can predict future states of spatiotemporal systems while concurrently learning the underlying governing equations of the physical system. Specifically, we focus on multi-operator learning by training distinct one-dimensional time-dependent nonlinear constant coefficient partial differential equations, with potential applications to many physical applications including physics, geology, and biology. More importantly, we provide three extrapolation studies to demonstrate that PROSE-PDE can generalize physical features through the robust training of multiple operators and that the proposed model can extrapolate to predict PDE solutions whose models or data were unseen during the training. Furthermore, we show through systematic numerical experiments that the utilization of the symbolic modality in our model effectively resolves the well-posedness problems with training multiple operators and thus enhances our model's predictive capabilities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.13056</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.13056</id><created>2024-04-08</created><authors><author><keyname>Dong</keyname><forenames>Jiayuan</forenames></author><author><keyname>Jacobsen</keyname><forenames>Christian</forenames></author><author><keyname>Khalloufi</keyname><forenames>Mehdi</forenames></author><author><keyname>Akram</keyname><forenames>Maryam</forenames></author><author><keyname>Liu</keyname><forenames>Wanjiao</forenames></author><author><keyname>Duraisamy</keyname><forenames>Karthik</forenames></author><author><keyname>Huan</keyname><forenames>Xun</forenames></author></authors><title>Variational Bayesian Optimal Experimental Design with Normalizing Flows</title><categories>cs.LG cs.CE stat.CO stat.ME stat.ML</categories><msc-class>62K05, 94A17, 62C10, 62F15</msc-class><journal-ref>Computer Methods in Applied Mechanics and Engineering 433 (2025)   117457</journal-ref><doi>10.1016/j.cma.2024.117457</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian optimal experimental design (OED) seeks experiments that maximize the expected information gain (EIG) in model parameters. Directly estimating the EIG using nested Monte Carlo is computationally expensive and requires an explicit likelihood. Variational OED (vOED), in contrast, estimates a lower bound of the EIG without likelihood evaluations by approximating the posterior distributions with variational forms, and then tightens the bound by optimizing its variational parameters. We introduce the use of normalizing flows (NFs) for representing variational distributions in vOED; we call this approach vOED-NFs. Specifically, we adopt NFs with a conditional invertible neural network architecture built from compositions of coupling layers, and enhanced with a summary network for data dimension reduction. We present Monte Carlo estimators to the lower bound along with gradient expressions to enable a gradient-based simultaneous optimization of the variational parameters and the design variables. The vOED-NFs algorithm is then validated in two benchmark problems, and demonstrated on a partial differential equation-governed application of cathodic electrophoretic deposition and an implicit likelihood case with stochastic modeling of aphid population. The findings suggest that a composition of 4--5 coupling layers is able to achieve lower EIG estimation bias, under a fixed budget of forward model runs, compared to previous approaches. The resulting NFs produce approximate posteriors that agree well with the true posteriors, able to capture non-Gaussian and multi-modal features effectively. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.13830</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.13830</id><created>2024-04-21</created><updated>2025-02-01</updated><authors><author><keyname>Zhang</keyname><forenames>Yu-Xin</forenames></author><author><keyname>Gui</keyname><forenames>Jie</forenames></author><author><keyname>Yu</keyname><forenames>Baosheng</forenames></author><author><keyname>Cong</keyname><forenames>Xiaofeng</forenames></author><author><keyname>Gong</keyname><forenames>Xin</forenames></author><author><keyname>Tao</keyname><forenames>Wenbing</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author></authors><title>Deep Learning-Based Point Cloud Registration: A Comprehensive Survey and   Taxonomy</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Point cloud registration involves determining a rigid transformation to align a source point cloud with a target point cloud. This alignment is fundamental in applications such as autonomous driving, robotics, and medical imaging, where precise spatial correspondence is essential. Deep learning has greatly advanced point cloud registration by providing robust and efficient methods that address the limitations of traditional approaches, including sensitivity to noise, outliers, and initialization. However, a well-constructed taxonomy for these methods is still lacking, making it difficult to systematically classify and compare the various approaches. In this paper, we present a comprehensive survey and taxonomy on deep learning-based point cloud registration (DL-PCR). We begin with a formal description of the point cloud registration problem, followed by an overview of the datasets, evaluation metrics, and loss functions commonly used in DL-PCR. Next, we categorize existing DL-PCR methods into supervised and unsupervised approaches, as they focus on significantly different key aspects. For supervised DL-PCR methods, we organize the discussion based on key aspects, including the registration procedure, optimization strategy, learning paradigm, network enhancement, and integration with traditional methods; For unsupervised DL-PCR methods, we classify them into correspondence-based and correspondence-free approaches, depending on whether they require explicit identification of point-to-point correspondences. To facilitate a more comprehensive and fair comparison, we conduct quantitative evaluations of all recent state-of-the-art approaches, using a unified training setting and consistent data partitioning strategy. Lastly, we highlight the open challenges and discuss potential directions for future study. A comprehensive collection is available at https://github.com/yxzhang15/PCR. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.14956</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.14956</id><created>2024-04-23</created><updated>2025-02-03</updated><authors><author><keyname>Zhang</keyname><forenames>Ye</forenames></author><author><keyname>Wang</keyname><forenames>Yifeng</forenames></author><author><keyname>Fang</keyname><forenames>Zijie</forenames></author><author><keyname>Bian</keyname><forenames>Hao</forenames></author><author><keyname>Cai</keyname><forenames>Linghan</forenames></author><author><keyname>Wang</keyname><forenames>Ziyue</forenames></author><author><keyname>Zhang</keyname><forenames>Yongbing</forenames></author></authors><title>DAWN: Domain-Adaptive Weakly Supervised Nuclei Segmentation via   Cross-Task Interactions</title><categories>eess.IV cs.CV</categories><comments>15 pages, 11 figures, 12 tables</comments><doi>10.1109/TCSVT.2024.3515467</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Weakly supervised segmentation methods have gained significant attention due to their ability to reduce the reliance on costly pixel-level annotations during model training. However, the current weakly supervised nuclei segmentation approaches typically follow a two-stage pseudo-label generation and network training process. The performance of the nuclei segmentation heavily relies on the quality of the generated pseudo-labels, thereby limiting its effectiveness. This paper introduces a novel domain-adaptive weakly supervised nuclei segmentation framework using cross-task interaction strategies to overcome the challenge of pseudo-label generation. Specifically, we utilize weakly annotated data to train an auxiliary detection task, which assists the domain adaptation of the segmentation network. To enhance the efficiency of domain adaptation, we design a consistent feature constraint module integrating prior knowledge from the source domain. Furthermore, we develop pseudo-label optimization and interactive training methods to improve the domain transfer capability. To validate the effectiveness of our proposed method, we conduct extensive comparative and ablation experiments on six datasets. The results demonstrate the superiority of our approach over existing weakly supervised approaches. Remarkably, our method achieves comparable or even better performance than fully supervised methods. Our code will be released in https://github.com/zhangye-zoe/DAWN. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.15041</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.15041</id><created>2024-04-23</created><updated>2025-02-01</updated><authors><author><keyname>Zhang</keyname><forenames>Fan</forenames></author><author><keyname>Cheng</keyname><forenames>Zhi-Qi</forenames></author><author><keyname>Zhao</keyname><forenames>Jian</forenames></author><author><keyname>Peng</keyname><forenames>Xiaojiang</forenames></author><author><keyname>Li</keyname><forenames>Xuelong</forenames></author></authors><title>LEAF: Unveiling Two Sides of the Same Coin in Semi-supervised Facial   Expression Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semi-supervised learning has emerged as a promising approach to tackle the challenge of label scarcity in facial expression recognition (FER) task. However, current state-of-the-art methods primarily focus on one side of the coin, i.e., generating high-quality pseudo-labels, while overlooking the other side: enhancing expression-relevant representations. In this paper, we unveil both sides of the coin by proposing a unified framework termed hierarchicaL dEcoupling And Fusing (LEAF) to coordinate expression-relevant representations and pseudo-labels for semi-supervised FER. LEAF introduces a hierarchical expression-aware aggregation strategy that operates at three levels: semantic, instance, and category. (1) At the semantic and instance levels, LEAF decouples representations into expression-agnostic and expression-relevant components, and adaptively fuses them using learnable gating weights. (2) At the category level, LEAF assigns ambiguous pseudo-labels by decoupling predictions into positive and negative parts, and employs a consistency loss to ensure agreement between two augmented views of the same image. Extensive experiments on benchmark datasets demonstrate that by unveiling and harmonizing both sides of the coin, LEAF outperforms state-of-the-art semi-supervised FER methods, effectively leveraging both labeled and unlabeled data. Moreover, the proposed expression-aware aggregation strategy can be seamlessly integrated into existing semi-supervised frameworks, leading to significant performance gains. Our code is available at \url{https://github.com/zfkarl/LEAF}. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.17553</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.17553</id><created>2024-04-26</created><updated>2025-01-31</updated><authors><author><keyname>Zhang</keyname><forenames>Xunzheng</forenames></author><author><keyname>Moazzeni</keyname><forenames>Shadi</forenames></author><author><keyname>Parra-Ullauri</keyname><forenames>Juan Marcelo</forenames></author><author><keyname>Nejabati</keyname><forenames>Reza</forenames></author><author><keyname>Simeonidou</keyname><forenames>Dimitra</forenames></author></authors><title>Federated Transfer Component Analysis Towards Effective VNF Profiling</title><categories>cs.DC cs.LG cs.NI</categories><comments>Novelty is unclear. Figure 8's experimental validation needs   refinement for reliability. Resource profiling and federated learning require   major revisions. Table II's MLP lacks comparisons. Conclusion needs updates</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing concerns of knowledge transfer and data privacy challenge the traditional gather-and-analyse paradigm in networks. Specifically, the intelligent orchestration of Virtual Network Functions (VNFs) requires understanding and profiling the resource consumption. However, profiling all kinds of VNFs is time-consuming. It is important to consider transferring the well-profiled VNF knowledge to other lack-profiled VNF types while keeping data private. To this end, this paper proposes a Federated Transfer Component Analysis (FTCA) method between the source and target VNFs. FTCA first trains Generative Adversarial Networks (GANs) based on the source VNF profiling data, and the trained GANs model is sent to the target VNF domain. Then, FTCA realizes federated domain adaptation by using the generated source VNF data and less target VNF profiling data, while keeping the raw data locally. Experiments show that the proposed FTCA can effectively predict the required resources for the target VNF. Specifically, the RMSE index of the regression model decreases by 38.5% and the R-squared metric advances up to 68.6%. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.17769</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.17769</id><created>2024-04-26</created><updated>2025-02-01</updated><authors><author><keyname>Xu</keyname><forenames>Yunpeng</forenames></author><author><keyname>Ying</keyname><forenames>Mufang</forenames></author><author><keyname>Guo</keyname><forenames>Wenge</forenames></author><author><keyname>Wei</keyname><forenames>Zhi</forenames></author></authors><title>Two-stage Risk Control with Application to Ranked Retrieval</title><categories>cs.IR stat.ME stat.ML</categories><comments>20 pages, 3 figures, 2 tables; 7 supplementary pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Practical machine learning systems often operate in multiple sequential stages, as seen in ranking and recommendation systems, which typically include a retrieval phase followed by a ranking phase. Effectively assessing prediction uncertainty and ensuring effective risk control in such systems pose significant challenges due to their inherent complexity. To address these challenges, we developed two-stage risk control methods based on the recently proposed learn-then-test (LTT) and conformal risk control (CRC) frameworks. Unlike the methods in prior work that address multiple risks, our approach leverages the sequential nature of the problem, resulting in reduced computational burden. We provide theoretical guarantees for our proposed methods and design novel loss functions tailored for ranked retrieval tasks. The effectiveness of our approach is validated through experiments on two large-scale, widely-used datasets: MSLR-Web and Yahoo LTRC. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.18411</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.18411</id><created>2024-04-29</created><updated>2025-01-31</updated><authors><author><keyname>Jeong</keyname><forenames>Mingi</forenames></author><author><keyname>Chadda</keyname><forenames>Arihant</forenames></author><author><keyname>Ren</keyname><forenames>Ziang</forenames></author><author><keyname>Zhao</keyname><forenames>Luyang</forenames></author><author><keyname>Liu</keyname><forenames>Haowen</forenames></author><author><keyname>Roznere</keyname><forenames>Monika</forenames></author><author><keyname>Zhang</keyname><forenames>Aiwei</forenames></author><author><keyname>Jiang</keyname><forenames>Yitao</forenames></author><author><keyname>Achong</keyname><forenames>Sabriel</forenames></author><author><keyname>Lensgraf</keyname><forenames>Samuel</forenames></author><author><keyname>Li</keyname><forenames>Alberto Quattrini</forenames></author></authors><title>SeePerSea: Multi-modal Perception Dataset of In-water Objects for   Autonomous Surface Vehicles</title><categories>cs.RO cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper introduces the first publicly accessible labeled multi-modal perception dataset for autonomous maritime navigation, focusing on in-water obstacles within the aquatic environment to enhance situational awareness for Autonomous Surface Vehicles (ASVs). This dataset, collected over 4 years and consisting of diverse objects encountered under varying environmental conditions, aims to bridge the research gap in autonomous surface vehicles by providing a multi-modal, annotated, and ego-centric perception dataset, for object detection and classification. We also show the applicability of the proposed dataset by training deep learning-based open-source perception algorithms that have shown success. We expect that our dataset will contribute to development of the marine autonomy pipelines and marine (field) robotics. This dataset is opensource and can be found at https://seepersea.github.io/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.19261</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.19261</id><created>2024-04-30</created><updated>2025-01-31</updated><authors><author><keyname>Agarwala</keyname><forenames>Atish</forenames></author><author><keyname>Pennington</keyname><forenames>Jeffrey</forenames></author></authors><title>High dimensional analysis reveals conservative sharpening and a   stochastic edge of stability</title><categories>cs.LG math.OC math.ST physics.data-an stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent empirical and theoretical work has shown that the dynamics of the large eigenvalues of the training loss Hessian have some remarkably robust features across models and datasets in the full batch regime. There is often an early period of progressive sharpening where the large eigenvalues increase, followed by stabilization at a predictable value known as the edge of stability. Previous work showed that in the stochastic setting, the eigenvalues increase more slowly - a phenomenon we call conservative sharpening. We provide a theoretical analysis of a simple high-dimensional model which shows the origin of this slowdown. We also show that there is an alternative stochastic edge of stability which arises at small batch size that is sensitive to the trace of the Neural Tangent Kernel rather than the large Hessian eigenvalues. We conduct an experimental study which highlights the qualitative differences from the full batch phenomenology, and suggests that controlling the stochastic edge of stability can help optimization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.19284</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.19284</id><created>2024-04-30</created><updated>2025-02-03</updated><authors><author><keyname>Harwood</keyname><forenames>Ben</forenames></author><author><keyname>Dezfouli</keyname><forenames>Amir</forenames></author><author><keyname>Chades</keyname><forenames>Iadine</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author></authors><title>Approximate Nearest Neighbour Search on Dynamic Datasets: An   Investigation</title><categories>cs.LG</categories><journal-ref>Lecture Notes in Artificial Intelligence (LNAI), vol. 15443, pp.   95-106, 2025</journal-ref><doi>10.1007/978-981-96-0351-0_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate k-Nearest Neighbour (ANN) methods are often used for mining information and aiding machine learning on large scale high-dimensional datasets. ANN methods typically differ in the index structure used for accelerating searches, resulting in various recall/runtime trade-off points. For applications with static datasets, runtime constraints and dataset properties can be used to empirically select an ANN method with suitable operating characteristics. However, for applications with dynamic datasets, which are subject to frequent online changes (like addition of new samples), there is currently no consensus as to which ANN methods are most suitable. Traditional evaluation approaches do not consider the computational costs of updating the index structure, as well as the rate and size of index updates. To address this, we empirically evaluate 5 popular ANN methods on two main applications (online data collection and online feature learning) while taking into account these considerations. Two dynamic datasets are used, derived from the SIFT1M dataset with 1 million samples and the DEEP1B dataset with 1 billion samples. The results indicate that the often used k-d trees method is not suitable on dynamic datasets as it is slower than a straightforward baseline exhaustive search method. For online data collection, the Hierarchical Navigable Small World Graphs method achieves a consistent speedup over baseline across a wide range of recall rates. For online feature learning, the Scalable Nearest Neighbours method is faster than baseline for recall rates below 75%. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.00577</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.00577</id><created>2024-05-01</created><updated>2025-02-01</updated><authors><author><keyname>Chan</keyname><forenames>Yi Hao</forenames></author><author><keyname>Girish</keyname><forenames>Deepank</forenames></author><author><keyname>Gupta</keyname><forenames>Sukrit</forenames></author><author><keyname>Xia</keyname><forenames>Jing</forenames></author><author><keyname>Kasi</keyname><forenames>Chockalingam</forenames></author><author><keyname>He</keyname><forenames>Yinan</forenames></author><author><keyname>Wang</keyname><forenames>Conghao</forenames></author><author><keyname>Rajapakse</keyname><forenames>Jagath C.</forenames></author></authors><title>Discovering robust biomarkers of psychiatric disorders from   resting-state functional MRI via graph neural networks: A systematic review</title><categories>cs.LG eess.SP q-bio.NC</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Graph neural networks (GNN) have emerged as a popular tool for modelling functional magnetic resonance imaging (fMRI) datasets. Many recent studies have reported significant improvements in disorder classification performance via more sophisticated GNN designs and highlighted salient features that could be potential biomarkers of the disorder. However, existing methods of evaluating their robustness are often limited to cross-referencing with existing literature, which is a subjective and inconsistent process. In this review, we provide an overview of how GNN and model explainability techniques (specifically, feature attributors) have been applied to fMRI datasets for disorder prediction tasks, with an emphasis on evaluating the robustness of potential biomarkers produced for psychiatric disorders. Then, 65 studies using GNNs that reported potential fMRI biomarkers for psychiatric disorders (attention-deficit hyperactivity disorder, autism spectrum disorder, major depressive disorder, schizophrenia) published before 9 October 2024 were identified from 2 online databases (Scopus, PubMed). We found that while most studies have performant models, salient features highlighted in these studies (as determined by feature attribution scores) vary greatly across studies on the same disorder. Reproducibility of biomarkers is only limited to a small subset at the level of regions and few transdiagnostic biomarkers were identified. To address these issues, we suggest establishing new standards that are based on objective evaluation metrics to determine the robustness of these potential biomarkers. We further highlight gaps in the existing literature and put together a prediction-attribution-evaluation framework that could set the foundations for future research on discovering robust biomarkers of psychiatric disorders via GNNs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.00679</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.00679</id><created>2024-02-05</created><updated>2025-01-29</updated><authors><author><keyname>Holzhausen</keyname><forenames>Konstantin</forenames></author><author><keyname>Merlid</keyname><forenames>Mia</forenames></author><author><keyname>Torvik</keyname><forenames>Håkon Olav</forenames></author><author><keyname>Malthe-Sørenssen</keyname><forenames>Anders</forenames></author><author><keyname>Lepperød</keyname><forenames>Mikkel Elle</forenames></author></authors><title>Exploring Biologically Inspired Mechanisms of Adversarial Robustness</title><categories>cs.NE cs.AI q-bio.NC</categories><journal-ref>Neural Computing and Applications 02-02-2025</journal-ref><doi>10.1007/s00521-025-11019-6</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Backpropagation-optimized artificial neural networks, while precise, lack robustness, leading to unforeseen behaviors that affect their safety. Biological neural systems do solve some of these issues already. Unlike artificial models, biological neurons adjust connectivity based on neighboring cell activity. Understanding the biological mechanisms of robustness can pave the way towards building trust worthy and safe systems. Robustness in neural representations is hypothesized to correlate with the smoothness of the encoding manifold. Recent work suggests power law covariance spectra, which were observed studying the primary visual cortex of mice, to be indicative of a balanced trade-off between accuracy and robustness in representations. Here, we show that unsupervised local learning models with winner takes all dynamics learn such power law representations, providing upcoming studies a mechanistic model with that characteristic. Our research aims to understand the interplay between geometry, spectral properties, robustness, and expressivity in neural representations. Hence, we study the link between representation smoothness and spectrum by using weight, Jacobian and spectral regularization while assessing performance and adversarial robustness. Our work serves as a foundation for future research into the mechanisms underlying power law spectra and optimally smooth encodings in both biological and artificial systems. The insights gained may elucidate the mechanisms that realize robust neural networks in mammalian brains and inform the development of more stable and reliable artificial systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.02503</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.02503</id><created>2024-05-03</created><updated>2025-02-03</updated><authors><author><keyname>Chen</keyname><forenames>Catherine</forenames></author><author><keyname>Merullo</keyname><forenames>Jack</forenames></author><author><keyname>Eickhoff</keyname><forenames>Carsten</forenames></author></authors><title>Axiomatic Causal Interventions for Reverse Engineering Relevance   Computation in Neural Retrieval Models</title><categories>cs.IR</categories><comments>10 pages, 10 figures, updated version of SIGIR 2024 perspective paper</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Neural models have demonstrated remarkable performance across diverse ranking tasks. However, the processes and internal mechanisms along which they determine relevance are still largely unknown. Existing approaches for analyzing neural ranker behavior with respect to IR properties rely either on assessing overall model behavior or employing probing methods that may offer an incomplete understanding of causal mechanisms. To provide a more granular understanding of internal model decision-making processes, we propose the use of causal interventions to reverse engineer neural rankers, and demonstrate how mechanistic interpretability methods can be used to isolate components satisfying term-frequency axioms within a ranking model. We identify a group of attention heads that detect duplicate tokens in earlier layers of the model, then communicate with downstream heads to compute overall document relevance. More generally, we propose that this style of mechanistic analysis opens up avenues for reverse engineering the processes neural retrieval models use to compute relevance. This work aims to initiate granular interpretability efforts that will not only benefit retrieval model development and training, but ultimately ensure safer deployment of these models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.05025</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.05025</id><created>2024-05-08</created><authors><author><keyname>Poinsot</keyname><forenames>Audrey</forenames></author><author><keyname>Leite</keyname><forenames>Alessandro</forenames></author><author><keyname>Chesneau</keyname><forenames>Nicolas</forenames></author><author><keyname>Sébag</keyname><forenames>Michèle</forenames></author><author><keyname>Schoenauer</keyname><forenames>Marc</forenames></author></authors><title>Learning Structural Causal Models through Deep Generative Models:   Methods, Guarantees, and Challenges</title><categories>stat.ML cs.LG</categories><comments>Accepted to the 33rd International Joint Conference on Artificial   Intelligence</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper provides a comprehensive review of deep structural causal models (DSCMs), particularly focusing on their ability to answer counterfactual queries using observational data within known causal structures. It delves into the characteristics of DSCMs by analyzing the hypotheses, guarantees, and applications inherent to the underlying deep learning components and structural causal models, fostering a finer understanding of their capabilities and limitations in addressing different counterfactual queries. Furthermore, it highlights the challenges and open questions in the field of deep structural causal modeling. It sets the stages for researchers to identify future work directions and for practitioners to get an overview in order to find out the most appropriate methods for their needs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.05028</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.05028</id><created>2024-05-08</created><updated>2025-02-01</updated><authors><author><keyname>Kazma</keyname><forenames>Mohamad</forenames></author><author><keyname>Taha</keyname><forenames>Ahmad F.</forenames></author></authors><title>Stability And Uncertainty Propagation In Power Networks: A   Lyapunov-based Approach With Applications To Renewable Resources Allocation</title><categories>eess.SY cs.SY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The rapid increase in the integration of intermittent and stochastic renewable energy resources (RER) introduces challenging issues related to power system stability. Interestingly, identifying grid nodes that can best support stochastic loads from RER, has gained recent interest. Methods based on Lyapunov stability are commonly exploited to assess the stability of power networks. These strategies approach quantifying system stability while considering: (i) simplified reduced order power system models that do not model power flow constraints, or (ii) data-driven methods that are prone to measurement noise and hence can inaccurately depict stochastic loads as system instability. In this paper, while considering a nonlinear differential algebraic equation (NL-DAE) model, we introduce a new method for assessing the impact of uncertain renewable power injections on the stability of power system nodes/buses. The identification of stable nodes informs the operator/utility on how renewables injections affect the stability of the grid. The proposed method is based on optimizing metrics equivalent to the Lyapunov spectrum of exponents; its underlying properties result in a computationally efficient and scalable stable node identification algorithm for renewable energy resources allocation. The developed framework is studied on various standard power networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.05258</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.05258</id><created>2024-05-08</created><updated>2025-02-01</updated><authors><author><keyname>Kong</keyname><forenames>Lingdong</forenames></author><author><keyname>Xu</keyname><forenames>Xiang</forenames></author><author><keyname>Ren</keyname><forenames>Jiawei</forenames></author><author><keyname>Zhang</keyname><forenames>Wenwei</forenames></author><author><keyname>Pan</keyname><forenames>Liang</forenames></author><author><keyname>Chen</keyname><forenames>Kai</forenames></author><author><keyname>Ooi</keyname><forenames>Wei Tsang</forenames></author><author><keyname>Liu</keyname><forenames>Ziwei</forenames></author></authors><title>Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving</title><categories>cs.CV cs.LG cs.RO</categories><comments>TPAMI 2025; 18 pages, 6 figures, 9 tables; Code at   https://github.com/ldkong1205/LaserMix</comments><doi>10.1109/TPAMI.2025.3535625</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Efficient data utilization is crucial for advancing 3D scene understanding in autonomous driving, where reliance on heavily human-annotated LiDAR point clouds challenges fully supervised methods. Addressing this, our study extends into semi-supervised learning for LiDAR semantic segmentation, leveraging the intrinsic spatial priors of driving scenes and multi-sensor complements to augment the efficacy of unlabeled datasets. We introduce LaserMix++, an evolved framework that integrates laser beam manipulations from disparate LiDAR scans and incorporates LiDAR-camera correspondences to further assist data-efficient learning. Our framework is tailored to enhance 3D scene consistency regularization by incorporating multi-modality, including 1) multi-modal LaserMix operation for fine-grained cross-sensor interactions; 2) camera-to-LiDAR feature distillation that enhances LiDAR feature learning; and 3) language-driven knowledge guidance generating auxiliary supervisions using open-vocabulary models. The versatility of LaserMix++ enables applications across LiDAR representations, establishing it as a universally applicable solution. Our framework is rigorously validated through theoretical analysis and extensive experiments on popular driving perception datasets. Results demonstrate that LaserMix++ markedly outperforms fully supervised alternatives, achieving comparable accuracy with five times fewer annotations and significantly improving the supervised-only baselines. This substantial advancement underscores the potential of semi-supervised approaches in reducing the reliance on extensive labeled data in LiDAR-based 3D scene understanding systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.05492</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.05492</id><created>2024-05-08</created><updated>2025-01-31</updated><authors><author><keyname>Jung</keyname><forenames>Inkee</forenames></author><author><keyname>Lau</keyname><forenames>Siu-Cheong</forenames></author></authors><title>A logifold structure on measure space</title><categories>math.DG cs.AI cs.LG math.PR</categories><comments>37 pages, 5 figures</comments><msc-class>55N31, 53Z50, 68T07, 68T09, 60A10, 81P45, 94D05</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper,we develop a local-to-global and measure-theoretical approach to understand datasets. The idea is to take network models with restricted domains as local charts of datasets. We develop the mathematical foundations for these structures, and show in experiments how it can be used to find fuzzy domains and to improve accuracy in data classification problems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.07346</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.07346</id><created>2024-05-12</created><updated>2025-02-02</updated><authors><author><keyname>Wang</keyname><forenames>Jiarui</forenames></author><author><keyname>Duan</keyname><forenames>Huiyu</forenames></author><author><keyname>Zhai</keyname><forenames>Guangtao</forenames></author><author><keyname>Min</keyname><forenames>Xiongkuo</forenames></author></authors><title>Quality Assessment for AI Generated Images with Instruction Tuning</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial Intelligence Generated Content (AIGC) has grown rapidly in recent years, among which AI-based image generation has gained widespread attention due to its efficient and imaginative image creation ability. However, AI-generated Images (AIGIs) may not satisfy human preferences due to their unique distortions, which highlights the necessity to understand and evaluate human preferences for AIGIs. To this end, in this paper, we first establish a novel Image Quality Assessment (IQA) database for AIGIs, termed AIGCIQA2023+, which provides human visual preference scores and detailed preference explanations from three perspectives including quality, authenticity, and correspondence. Then, based on the constructed AIGCIQA2023+ database, this paper presents a MINT-IQA model to evaluate and explain human preferences for AIGIs from Multi-perspectives with INstruction Tuning. Specifically, the MINT-IQA model first learn and evaluate human preferences for AI-generated Images from multi-perspectives, then via the vision-language instruction tuning strategy, MINT-IQA attains powerful understanding and explanation ability for human visual preference on AIGIs, which can be used for feedback to further improve the assessment capabilities. Extensive experimental results demonstrate that the proposed MINT-IQA model achieves state-of-the-art performance in understanding and evaluating human visual preferences for AIGIs, and the proposed model also achieves competing results on traditional IQA tasks compared with state-of-the-art IQA models. The AIGCIQA2023+ database and MINT-IQA model are available at: https://github.com/IntMeGroup/MINT-IQA. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.09359</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.09359</id><created>2024-05-15</created><updated>2025-01-31</updated><authors><author><keyname>Chen</keyname><forenames>Chen</forenames></author><author><keyname>Zou</keyname><forenames>Qikai</forenames></author><author><keyname>Song</keyname><forenames>Yuhang</forenames></author><author><keyname>Yu</keyname><forenames>Mingrui</forenames></author><author><keyname>Zhu</keyname><forenames>Senqiang</forenames></author><author><keyname>Song</keyname><forenames>Shiji</forenames></author><author><keyname>Li</keyname><forenames>Xiang</forenames></author></authors><title>Visual Attention Based Cognitive Human-Robot Collaboration for Pedicle   Screw Placement in Robot-Assisted Orthopedic Surgery</title><categories>cs.RO cs.HC</categories><comments>7 pages, 8 figures, in IROS 2024</comments><doi>10.1109/IROS58592.2024.10801930</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current orthopedic robotic systems largely focus on navigation, aiding surgeons in positioning a guiding tube but still requiring manual drilling and screw placement. The automation of this task not only demands high precision and safety due to the intricate physical interactions between the surgical tool and bone but also poses significant risks when executed without adequate human oversight. As it involves continuous physical interaction, the robot should collaborate with the surgeon, understand the human intent, and always include the surgeon in the loop. To achieve this, this paper proposes a new cognitive human-robot collaboration framework, including the intuitive AR-haptic human-robot interface, the visual-attention-based surgeon model, and the shared interaction control scheme for the robot. User studies on a robotic platform for orthopedic surgery are presented to illustrate the performance of the proposed method. The results demonstrate that the proposed human-robot collaboration framework outperforms full robot and full human control in terms of safety and ergonomics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.09831</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.09831</id><created>2024-05-16</created><updated>2025-02-03</updated><authors><author><keyname>Lee</keyname><forenames>Joongkyu</forenames></author><author><keyname>Oh</keyname><forenames>Min-hwan</forenames></author></authors><title>Nearly Minimax Optimal Regret for Multinomial Logistic Bandit</title><categories>stat.ML cs.LG</categories><comments>Accepted in NeurIPS 2024</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In this paper, we study the contextual multinomial logit (MNL) bandit problem in which a learning agent sequentially selects an assortment based on contextual information, and user feedback follows an MNL choice model. There has been a significant discrepancy between lower and upper regret bounds, particularly regarding the maximum assortment size $K$. Additionally, the variation in reward structures between these bounds complicates the quest for optimality. Under uniform rewards, where all items have the same expected reward, we establish a regret lower bound of $\Omega(d\sqrt{\smash[b]{T/K}})$ and propose a constant-time algorithm, OFU-MNL+, that achieves a matching upper bound of $\tilde{O}(d\sqrt{\smash[b]{T/K}})$. We also provide instance-dependent minimax regret bounds under uniform rewards. Under non-uniform rewards, we prove a lower bound of $\Omega(d\sqrt{T})$ and an upper bound of $\tilde{O}(d\sqrt{T})$, also achievable by OFU-MNL+. Our empirical studies support these theoretical findings. To the best of our knowledge, this is the first work in the contextual MNL bandit literature to prove minimax optimality -- for either uniform or non-uniform reward setting -- and to propose a computationally efficient algorithm that achieves this optimality up to logarithmic factors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.12001</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.12001</id><created>2024-05-20</created><updated>2025-02-02</updated><authors><author><keyname>Zhang</keyname><forenames>Hai</forenames></author><author><keyname>Zheng</keyname><forenames>Boyuan</forenames></author><author><keyname>Ji</keyname><forenames>Tianying</forenames></author><author><keyname>Liu</keyname><forenames>Jinhang</forenames></author><author><keyname>Guo</keyname><forenames>Anqi</forenames></author><author><keyname>Zhao</keyname><forenames>Junqiao</forenames></author><author><keyname>Li</keyname><forenames>Lanqing</forenames></author></authors><title>Scrutinize What We Ignore: Reining In Task Representation Shift Of   Context-Based Offline Meta Reinforcement Learning</title><categories>cs.LG cs.AI</categories><comments>Accept at ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Offline meta reinforcement learning (OMRL) has emerged as a promising approach for interaction avoidance and strong generalization performance by leveraging pre-collected data and meta-learning techniques. Previous context-based approaches predominantly rely on the intuition that alternating optimization between the context encoder and the policy can lead to performance improvements, as long as the context encoder follows the principle of maximizing the mutual information between the task variable $M$ and its latent representation $Z$ ($I(Z;M)$) while the policy adopts the standard offline reinforcement learning (RL) algorithms conditioning on the learned task representation.Despite promising results, the theoretical justification of performance improvements for such intuition remains underexplored.Inspired by the return discrepancy scheme in the model-based RL field, we find that the previous optimization framework can be linked with the general RL objective of maximizing the expected return, thereby explaining performance improvements. Furthermore, after scrutinizing this optimization framework, we observe that the condition for monotonic performance improvements does not consider the variation of the task representation. When these variations are considered, the previously established condition may no longer be sufficient to ensure monotonicity, thereby impairing the optimization process.We name this issue task representation shift and theoretically prove that the monotonic performance improvements can be guaranteed with appropriate context encoder updates.Our work opens up a new avenue for OMRL, leading to a better understanding between the task representation and performance improvements. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.13382</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.13382</id><created>2024-05-22</created><updated>2025-01-31</updated><authors><author><keyname>Guo</keyname><forenames>Yongxin</forenames></author><author><keyname>Liu</keyname><forenames>Jingyu</forenames></author><author><keyname>Li</keyname><forenames>Mingda</forenames></author><author><keyname>Cheng</keyname><forenames>Dingxin</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoying</forenames></author><author><keyname>Sui</keyname><forenames>Dianbo</forenames></author><author><keyname>Liu</keyname><forenames>Qingbin</forenames></author><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Zhao</keyname><forenames>Kevin</forenames></author></authors><title>VTG-LLM: Integrating Timestamp Knowledge into Video LLMs for Enhanced   Video Temporal Grounding</title><categories>cs.CV</categories><comments>AAAI 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Video Temporal Grounding (VTG) strives to accurately pinpoint event timestamps in a specific video using linguistic queries, significantly impacting downstream tasks like video browsing and editing. Unlike traditional task-specific models, Video Large Language Models (video LLMs) can handle multiple tasks concurrently in a zero-shot manner. Consequently, exploring the application of video LLMs for VTG tasks has become a burgeoning research area. However, despite considerable advancements in video content understanding, video LLMs often struggle to accurately pinpoint timestamps within videos, limiting their effectiveness in VTG tasks. To address this, we introduce VTG-LLM, a model designed to enhance video LLMs' timestamp localization abilities. Our approach includes: (1) effectively integrating timestamp knowledge into visual tokens; (2) incorporating absolute-time tokens to manage timestamp knowledge without concept shifts; and (3) introducing a lightweight, high-performance, slot-based token compression technique designed to accommodate the demands of a large number of frames to be sampled for VTG tasks. Additionally, we present VTG-IT-120K, a collection of publicly available VTG datasets that we have re-annotated to improve upon low-quality annotations. Our comprehensive experiments demonstrate the superior performance of VTG-LLM in comparison to other video LLM methods across a variety of VTG tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.13698</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.13698</id><created>2024-05-22</created><updated>2025-02-02</updated><authors><author><keyname>Wang</keyname><forenames>Xi</forenames></author><author><keyname>Aitchison</keyname><forenames>Laurence</forenames></author></authors><title>How to set AdamW's weight decay as you scale model and dataset size</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The scaling of the optimal AdamW weight decay hyperparameter with model and dataset size is critical as we seek to build larger models, but is poorly understood. We show that weights learned by AdamW can be understood as an exponential moving average (EMA) of recent updates. This gives critical insights for how to set the weight decay in AdamW, and how the weight decay should scale with model and dataset size. In particular, the key hyperparameter for an exponential moving average is the EMA timescale. Intuitively, the EMA timescale can be understood as the number of recent iterations the EMA averages over. We find that the optimal timescale, measured in epochs, is roughly constant as we change model and dataset size. Moreover, given a learning rate, there is a one-to-one mapping from the EMA timescale to the weight decay hyperparameter. Thus, if the optimal EMA timescale is constant, that implies that as the dataset size increases, the optimal weight decay should fall and as the model size increases, the optimal weight decay should increase (if we follow the muP recommendation for scaling the learning rate). We validate these scaling rules on ResNet-18 and Vision Transformers trained on CIFAR-10 and ImageNet, and on NanoGPT pre-training on OpenWebText. Finally, we found that as training progresses, muP's learning rate scaling breaks down for AdamW unless weight decay is scaled appropriately. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.13888</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.13888</id><created>2024-05-22</created><updated>2025-02-03</updated><authors><author><keyname>Yao</keyname><forenames>Dingling</forenames></author><author><keyname>Muller</keyname><forenames>Caroline</forenames></author><author><keyname>Locatello</keyname><forenames>Francesco</forenames></author></authors><title>Marrying Causal Representation Learning with Dynamical Systems for   Science</title><categories>cs.LG stat.ML</categories><comments>NeurIPS 2024 Camera Ready</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Causal representation learning promises to extend causal models to hidden causal variables from raw entangled measurements. However, most progress has focused on proving identifiability results in different settings, and we are not aware of any successful real-world application. At the same time, the field of dynamical systems benefited from deep learning and scaled to countless applications but does not allow parameter identification. In this paper, we draw a clear connection between the two and their key assumptions, allowing us to apply identifiable methods developed in causal representation learning to dynamical systems. At the same time, we can leverage scalable differentiable solvers developed for differential equations to build models that are both identifiable and practical. Overall, we learn explicitly controllable models that isolate the trajectory-specific parameters for further downstream tasks such as out-of-distribution classification or treatment effect estimation. We experiment with a wind simulator with partially known factors of variation. We also apply the resulting model to real-world climate data and successfully answer downstream causal questions in line with existing literature on climate change. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.14407</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.14407</id><created>2024-05-23</created><updated>2025-02-02</updated><authors><author><keyname>Zhang</keyname><forenames>He</forenames></author><author><keyname>Wu</keyname><forenames>Bang</forenames></author><author><keyname>Yang</keyname><forenames>Xiangwen</forenames></author><author><keyname>Yuan</keyname><forenames>Xingliang</forenames></author><author><keyname>Liu</keyname><forenames>Xiaoning</forenames></author><author><keyname>Yi</keyname><forenames>Xun</forenames></author></authors><title>Dynamic Graph Unlearning: A General and Efficient Post-Processing Method   via Gradient Transformation</title><categories>cs.LG</categories><comments>Accepted by the 2025 ACM Web Conference (WWW)</comments><doi>10.1145/3696410.3714911</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Dynamic graph neural networks (DGNNs) have emerged and been widely deployed in various web applications (e.g., Reddit) to serve users (e.g., personalized content delivery) due to their remarkable ability to learn from complex and dynamic user interaction data. Despite benefiting from high-quality services, users have raised privacy concerns, such as misuse of personal data (e.g., dynamic user-user/item interaction) for model training, requiring DGNNs to ``forget'' their data to meet AI governance laws (e.g., the ``right to be forgotten'' in GDPR). However, current static graph unlearning studies cannot \textit{unlearn dynamic graph elements} and exhibit limitations such as the model-specific design or reliance on pre-processing, which disenable their practicability in dynamic graph unlearning. To this end, we study the dynamic graph unlearning for the first time and propose an effective, efficient, general, and post-processing method to implement DGNN unlearning. Specifically, we first formulate dynamic graph unlearning in the context of continuous-time dynamic graphs, and then propose a method called Gradient Transformation that directly maps the unlearning request to the desired parameter update. Comprehensive evaluations on six real-world datasets and state-of-the-art DGNN backbones demonstrate its effectiveness (e.g., limited drop or obvious improvement in utility) and efficiency (e.g., 7.23$\times$ speed-up) advantages. Additionally, our method has the potential to handle future unlearning requests with significant performance gains (e.g., 32.59$\times$ speed-up). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.14741</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.14741</id><created>2024-05-23</created><updated>2025-02-01</updated><authors><author><keyname>Qian</keyname><forenames>Huajie</forenames></author><author><keyname>Ying</keyname><forenames>Donghao</forenames></author><author><keyname>Lam</keyname><forenames>Henry</forenames></author><author><keyname>Yin</keyname><forenames>Wotao</forenames></author></authors><title>Subsampled Ensemble Can Improve Generalization Tail Exponentially</title><categories>math.OC cs.LG stat.ML</categories><comments>42 pages, 18 figures</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Ensemble learning is a popular technique to improve the accuracy of machine learning models. It traditionally hinges on the rationale that aggregating multiple weak models can lead to better models with lower variance and hence higher stability, especially for discontinuous base learners. In this paper, we provide a new perspective on ensembling. By selecting the best model trained on subsamples via majority voting, we can attain exponentially decaying tails for the excess risk, even if the base learner suffers from slow (i.e., polynomial) decay rates. This tail enhancement power of ensembling is agnostic to the underlying base learner and is stronger than variance reduction in the sense of exhibiting rate improvement. We demonstrate how our ensemble methods can substantially improve out-of-sample performances in a range of numerical examples involving heavy-tailed data or intrinsically slow rates. Code for the proposed methods is available at https://github.com/mickeyhqian/VoteEnsemble. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.14744</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.14744</id><created>2024-05-23</created><updated>2025-02-03</updated><authors><author><keyname>Liu</keyname><forenames>Xuan</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author><author><keyname>Guo</keyname><forenames>Song</forenames></author><author><keyname>Shang</keyname><forenames>Haoyang</forenames></author><author><keyname>Yang</keyname><forenames>Chengxu</forenames></author><author><keyname>Zhu</keyname><forenames>Quanyan</forenames></author></authors><title>Exploring Prosocial Irrationality for LLM Agents: A Social Cognition   View</title><categories>cs.CY</categories><comments>Accepted by ICLR 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM Agents remains under-explored. As LLM Agents are increasingly employed in intricate social environments, a pressing and natural question emerges: Can we utilize LLM Agents' systematic hallucinations to mirror human cognitive biases, thus exhibiting irrational social intelligence? In this paper, we probe the irrational behavior among contemporary LLM Agents by melding practical social science experiments with theoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM Agents framework that utilizes hallucination properties to assess and enhance LLM Agents' social intelligence through cognitive biases. Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities and highlighting the significance of hallucination properties. Additionally, the CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.15441</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.15441</id><created>2024-05-24</created><updated>2025-02-02</updated><authors><author><keyname>Wang</keyname><forenames>Jie</forenames></author><author><keyname>Boedihardjo</keyname><forenames>March</forenames></author><author><keyname>Xie</keyname><forenames>Yao</forenames></author></authors><title>Statistical and Computational Guarantees of Kernel Max-Sliced   Wasserstein Distances</title><categories>stat.ML cs.CC cs.LG</categories><comments>34 pages, 8 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal transport has been very successful for various machine learning tasks; however, it is known to suffer from the curse of dimensionality. Hence, dimensionality reduction is desirable when applied to high-dimensional data with low-dimensional structures. The kernel max-sliced (KMS) Wasserstein distance is developed for this purpose by finding an optimal nonlinear mapping that reduces data into $1$ dimension before computing the Wasserstein distance. However, its theoretical properties have not yet been fully developed. In this paper, we provide sharp finite-sample guarantees under milder technical assumptions compared with state-of-the-art for the KMS $p$-Wasserstein distance between two empirical distributions with $n$ samples for general $p\in[1,\infty)$. Algorithm-wise, we show that computing the KMS $2$-Wasserstein distance is NP-hard, and then we further propose a semidefinite relaxation (SDR) formulation (which can be solved efficiently in polynomial time) and provide a relaxation gap for the obtained solution. We provide numerical examples to demonstrate the good performance of our scheme for high-dimensional two-sample testing. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.15476</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.15476</id><created>2024-05-24</created><updated>2025-02-01</updated><authors><author><keyname>Hu</keyname><forenames>Lijie</forenames></author><author><keyname>Ren</keyname><forenames>Chenyang</forenames></author><author><keyname>Hu</keyname><forenames>Zhengyu</forenames></author><author><keyname>Lin</keyname><forenames>Hongbin</forenames></author><author><keyname>Wang</keyname><forenames>Cheng-Long</forenames></author><author><keyname>Xiong</keyname><forenames>Hui</forenames></author><author><keyname>Zhang</keyname><forenames>Jingfeng</forenames></author><author><keyname>Wang</keyname><forenames>Di</forenames></author></authors><title>Editable Concept Bottleneck Models</title><categories>cs.LG cs.AI cs.CV</categories><comments>49 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Concept Bottleneck Models (CBMs) have garnered much attention for their ability to elucidate the prediction process through a humanunderstandable concept layer. However, most previous studies focused on cases where the data, including concepts, are clean. In many scenarios, we often need to remove/insert some training data or new concepts from trained CBMs for reasons such as privacy concerns, data mislabelling, spurious concepts, and concept annotation errors. Thus, deriving efficient editable CBMs without retraining from scratch remains a challenge, particularly in large-scale applications. To address these challenges, we propose Editable Concept Bottleneck Models (ECBMs). Specifically, ECBMs support three different levels of data removal: concept-label-level, concept-level, and data-level. ECBMs enjoy mathematically rigorous closed-form approximations derived from influence functions that obviate the need for retraining. Experimental results demonstrate the efficiency and adaptability of our ECBMs, affirming their practical value in CBMs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.15643</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.15643</id><created>2024-05-24</created><updated>2025-02-03</updated><authors><author><keyname>Schneider</keyname><forenames>Fabian</forenames></author><author><keyname>Duong</keyname><forenames>Duc-Lam</forenames></author><author><keyname>Lassas</keyname><forenames>Matti</forenames></author><author><keyname>de Hoop</keyname><forenames>Maarten V.</forenames></author><author><keyname>Helin</keyname><forenames>Tapio</forenames></author></authors><title>Scalable diffusion posterior sampling in infinite-dimensional inverse   problems</title><categories>stat.ML cs.LG cs.NA math.AP math.NA math.PR</categories><comments>27 pages, 9 figures</comments><msc-class>62F15, 65N21, 68Q32, 60Hxx, 60Jxx, 68T07, 92C55,</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Score-based diffusion models (SDMs) have emerged as a powerful tool for sampling from the posterior distribution in Bayesian inverse problems. However, existing methods often require multiple evaluations of the forward mapping to generate a single sample, resulting in significant computational costs for large-scale inverse problems. To address this issue, we propose a scalable diffusion posterior sampling (SDPS) method to bypass forward mapping evaluations during sampling by shifting computational effort to an offline training phase, where a task-dependent score is learned based on the forward mapping. Crucially, the conditional posterior score is then derived from this trained score using affine transformations, ensuring no conditional score approximation is needed. The approach is shown to generalize to infinite-dimensional diffusion models and is validated through rigorous convergence analysis and high-dimensional CT imaging experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.16644</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.16644</id><created>2024-05-26</created><updated>2025-02-02</updated><authors><author><keyname>Samsonov</keyname><forenames>Sergey</forenames></author><author><keyname>Moulines</keyname><forenames>Eric</forenames></author><author><keyname>Shao</keyname><forenames>Qi-Man</forenames></author><author><keyname>Zhang</keyname><forenames>Zhuo-Song</forenames></author><author><keyname>Naumov</keyname><forenames>Alexey</forenames></author></authors><title>Gaussian Approximation and Multiplier Bootstrap for Polyak-Ruppert   Averaged Linear Stochastic Approximation with Applications to TD Learning</title><categories>stat.ML cs.LG math.OC math.PR math.ST stat.TH</categories><comments>NeurIPS-2024, camera-ready version. Some typos fixed compared to the   previous version</comments><msc-class>60F05, 62L20, 62E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we obtain the Berry-Esseen bound for multivariate normal approximation for the Polyak-Ruppert averaged iterates of the linear stochastic approximation (LSA) algorithm with decreasing step size. Moreover, we prove the non-asymptotic validity of the confidence intervals for parameter estimation with LSA based on multiplier bootstrap. This procedure updates the LSA estimate together with a set of randomly perturbed LSA estimates upon the arrival of subsequent observations. We illustrate our findings in the setting of temporal difference learning with linear function approximation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.17426</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.17426</id><created>2024-05-27</created><updated>2025-02-01</updated><authors><author><keyname>Xie</keyname><forenames>Shaoyuan</forenames></author><author><keyname>Kong</keyname><forenames>Lingdong</forenames></author><author><keyname>Zhang</keyname><forenames>Wenwei</forenames></author><author><keyname>Ren</keyname><forenames>Jiawei</forenames></author><author><keyname>Pan</keyname><forenames>Liang</forenames></author><author><keyname>Chen</keyname><forenames>Kai</forenames></author><author><keyname>Liu</keyname><forenames>Ziwei</forenames></author></authors><title>Benchmarking and Improving Bird's Eye View Perception Robustness in   Autonomous Driving</title><categories>cs.CV cs.RO</categories><comments>TPAMI 2025; 17 pages, 13 figures, 11 tables; Code at this https URL:   https://github.com/Daniel-xsy/RoboBEV</comments><doi>10.1109/TPAMI.2025.3535960</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Recent advancements in bird's eye view (BEV) representations have shown remarkable promise for in-vehicle 3D perception. However, while these methods have achieved impressive results on standard benchmarks, their robustness in varied conditions remains insufficiently assessed. In this study, we present RoboBEV, an extensive benchmark suite designed to evaluate the resilience of BEV algorithms. This suite incorporates a diverse set of camera corruption types, each examined over three severity levels. Our benchmarks also consider the impact of complete sensor failures that occur when using multi-modal models. Through RoboBEV, we assess 33 state-of-the-art BEV-based perception models spanning tasks like detection, map segmentation, depth estimation, and occupancy prediction. Our analyses reveal a noticeable correlation between the model's performance on in-distribution datasets and its resilience to out-of-distribution challenges. Our experimental results also underline the efficacy of strategies like pre-training and depth-free BEV transformations in enhancing robustness against out-of-distribution data. Furthermore, we observe that leveraging extensive temporal information significantly improves the model's robustness. Based on our observations, we design an effective robustness enhancement strategy based on the CLIP model. The insights from this study pave the way for the development of future BEV models that seamlessly combine accuracy with real-world robustness. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.17456</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.17456</id><created>2024-05-22</created><updated>2025-01-31</updated><authors><author><keyname>Zhang</keyname><forenames>Ling-Qi</forenames></author><author><keyname>Kadkhodaie</keyname><forenames>Zahra</forenames></author><author><keyname>Simoncelli</keyname><forenames>Eero P.</forenames></author><author><keyname>Brainard</keyname><forenames>David H.</forenames></author></authors><title>Optimal compressed sensing for image reconstruction with diffusion   probabilistic models</title><categories>cs.CV cs.LG eess.IV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We examine the problem of selecting a small set of linear measurements for reconstructing high-dimensional signals. Well-established methods for optimizing such measurements include principal component analysis (PCA), independent component analysis (ICA) and compressed sensing (CS) based on random projections, all of which rely on axis- or subspace-aligned statistical characterization of the signal source. However, many naturally occurring signals, including photographic images, contain richer statistical structure. To exploit such structure, we introduce a general method for obtaining an optimized set of linear measurements for efficient image reconstruction, where the signal statistics are expressed by the prior implicit in a neural network trained to perform denoising (generally known as a "diffusion model"). We demonstrate that the optimal measurements derived for two natural image datasets differ from those of PCA, ICA, or CS, and result in substantially lower mean squared reconstruction error. Interestingly, the marginal distributions of the measurement values are asymmetrical (skewed), substantially more so than those of previous methods. We also find that optimizing with respect to perceptual loss, as quantified by structural similarity (SSIM), leads to measurements different from those obtained when optimizing for MSE. Our results highlight the importance of incorporating the specific statistical regularities of natural signals when designing effective linear measurements. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.18065</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.18065</id><created>2024-05-28</created><updated>2025-02-02</updated><authors><author><keyname>Tzachor</keyname><forenames>Issar</forenames></author><author><keyname>Lerner</keyname><forenames>Boaz</forenames></author><author><keyname>Levy</keyname><forenames>Matan</forenames></author><author><keyname>Green</keyname><forenames>Michael</forenames></author><author><keyname>Shalev</keyname><forenames>Tal Berkovitz</forenames></author><author><keyname>Habib</keyname><forenames>Gavriel</forenames></author><author><keyname>Samuel</keyname><forenames>Dvir</forenames></author><author><keyname>Zailer</keyname><forenames>Noam Korngut</forenames></author><author><keyname>Shimshi</keyname><forenames>Or</forenames></author><author><keyname>Darshan</keyname><forenames>Nir</forenames></author><author><keyname>Ben-Ari</keyname><forenames>Rami</forenames></author></authors><title>EffoVPR: Effective Foundation Model Utilization for Visual Place   Recognition</title><categories>cs.CV cs.AI</categories><comments>ICLR 2025</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The task of Visual Place Recognition (VPR) is to predict the location of a query image from a database of geo-tagged images. Recent studies in VPR have highlighted the significant advantage of employing pre-trained foundation models like DINOv2 for the VPR task. However, these models are often deemed inadequate for VPR without further fine-tuning on VPR-specific data. In this paper, we present an effective approach to harness the potential of a foundation model for VPR. We show that features extracted from self-attention layers can act as a powerful re-ranker for VPR, even in a zero-shot setting. Our method not only outperforms previous zero-shot approaches but also introduces results competitive with several supervised methods. We then show that a single-stage approach utilizing internal ViT layers for pooling can produce global features that achieve state-of-the-art performance, with impressive feature compactness down to 128D. Moreover, integrating our local foundation features for re-ranking further widens this performance gap. Our method also demonstrates exceptional robustness and generalization, setting new state-of-the-art performance, while handling challenging conditions such as occlusion, day-night transitions, and seasonal variations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.18259</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.18259</id><created>2024-05-28</created><updated>2024-06-18</updated><authors><author><keyname>Sankaran</keyname><forenames>Aravind</forenames></author><author><keyname>Karlsson</keyname><forenames>Lars</forenames></author><author><keyname>Bientinesi</keyname><forenames>Paolo</forenames></author></authors><title>Ranking with Ties based on Noisy Performance Data</title><categories>cs.PF cs.IR</categories><comments>22 pages, 21 figures</comments><journal-ref>International Journal of Data Science and Analytics (2025)</journal-ref><doi>10.1007/s41060-025-00722-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of ranking a set of objects based on their performance when the measurement of said performance is subject to noise. In this scenario, the performance is measured repeatedly, resulting in a range of measurements for each object. If the ranges of two objects do not overlap, then we consider one object as 'better' than the other, and we expect it to receive a higher rank; if, however, the ranges overlap, then the objects are incomparable, and we wish them to be assigned the same rank. Unfortunately, the incomparability relation of ranges is in general not transitive; as a consequence, in general the two requirements cannot be satisfied simultaneously, i.e., it is not possible to guarantee both distinct ranks for objects with separated ranges, and same rank for objects with overlapping ranges. This conflict leads to more than one reasonable way to rank a set of objects. In this paper, we explore the ambiguities that arise when ranking with ties, and define a set of reasonable rankings, which we call partial rankings. We develop and analyse three different methodologies to compute a partial ranking. Finally, we show how performance differences among objects can be investigated with the help of partial ranking. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.18731</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.18731</id><created>2024-05-28</created><updated>2025-02-02</updated><authors><author><keyname>Xing</keyname><forenames>Ziqing</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Chen</keyname><forenames>Zirui</forenames></author><author><keyname>Wang</keyname><forenames>Yusong</forenames></author><author><keyname>Ma</keyname><forenames>Haoran</forenames></author><author><keyname>Wei</keyname><forenames>Zhun</forenames></author></authors><title>VBIM-Net: Variational Born Iterative Network for Inverse Scattering   Problems</title><categories>eess.SP cs.AI physics.comp-ph</categories><comments>This article has been published in IEEE Transactions on Geoscience   and Remote Sensing</comments><journal-ref>in IEEE Transactions on Geoscience and Remote Sensing, vol. 63,   pp. 1-16, 2025</journal-ref><doi>10.1109/TGRS.2025.3532363</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, studies have shown the potential of integrating field-type iterative methods with deep learning (DL) techniques in solving inverse scattering problems (ISPs). In this article, we propose a novel Variational Born Iterative Network, namely, VBIM-Net, to solve the full-wave ISPs with significantly improved structural rationality and inversion quality. The proposed VBIM-Net emulates the alternating updates of the total electric field and the contrast in the variational Born iterative method (VBIM) by multiple layers of subnetworks. We embed the analytical calculation of the contrast variation into each subnetwork, converting the scattered field residual into an approximate contrast variation and then enhancing it by a U-Net, thus avoiding the requirement of matched measurement dimension and grid resolution as in existing approaches. The total field and contrast of each layer's output is supervised in the loss function of VBIM-Net, imposing soft physical constraints on the variables in the subnetworks, which benefits the model's performance. In addition, we design a training scheme with extra noise to enhance the model's stability. Extensive numerical results on synthetic and experimental data both verify the inversion quality, generalization ability, and robustness of the proposed VBIM-Net. This work may provide some new inspiration for the design of efficient field-type DL schemes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.18793</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.18793</id><created>2024-05-29</created><updated>2025-01-31</updated><authors><author><keyname>Kar</keyname><forenames>Avik</forenames></author><author><keyname>Singh</keyname><forenames>Rahul</forenames></author></authors><title>Policy Zooming: Adaptive Discretization-based Infinite-Horizon   Average-Reward Reinforcement Learning</title><categories>cs.LG</categories><comments>43 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study Lipschitz MDPs in the infinite-horizon average-reward reinforcement learning (RL) setup in which an agent can play policies from a given set $\Phi$. The proposed algorithms ``zoom'' into ``promising'' regions of the policy space, thereby achieving adaptivity gains. We upper bound their regret as $\tilde{\mathcal{O}}\big(T^{1 - d_{\text{eff.}}^{-1}}\big)$, where $d_{\text{eff.}} = d^\Phi_z+2$ for model-free algorithm~\textit{PZRL-MF} and $d_{\text{eff.}} = 2d_\mathcal{S} + d^\Phi_z + 3$ for model-based algorithm~\textit{PZRL-MB}. Here, $d_\mathcal{S}$ is the dimension of the state space, and $d^\Phi_z$ is the zooming dimension. $d^\Phi_z$ is a problem-dependent quantity that depends not only on the underlying MDP, but also on the class $\Phi$. This yields us a low regret in case the agent competes against a low-complexity $\Phi$ (that has a small $d^\Phi_z$). We note that the preexisting notions of zooming dimension are inept at handling the non-episodic RL and do not yield adaptivity gains. The current work shows how to capture adaptivity gains for infinite-horizon average-reward RL in terms of $d^\Phi_z$. When specialized to the case of finite-dimensional policy space, we obtain that $d_{\text{eff.}}$ scales as the dimension of this space under mild technical conditions; and also obtain $d_{\text{eff.}} = 0$, or equivalently $\tilde{\mathcal{O}}(\sqrt{T})$ regret for \textit{PZRL-MF}, under a curvature condition on the average reward function that is commonly used in the multi-armed bandit (MAB) literature. Simulation experiments validate the gains arising due to adaptivity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.19265</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.19265</id><created>2024-05-29</created><authors><author><keyname>Song</keyname><forenames>Zifan</forenames></author><author><keyname>Wang</keyname><forenames>Yudong</forenames></author><author><keyname>Zhang</keyname><forenames>Wenwei</forenames></author><author><keyname>Liu</keyname><forenames>Kuikun</forenames></author><author><keyname>Lyu</keyname><forenames>Chengqi</forenames></author><author><keyname>Song</keyname><forenames>Demin</forenames></author><author><keyname>Guo</keyname><forenames>Qipeng</forenames></author><author><keyname>Yan</keyname><forenames>Hang</forenames></author><author><keyname>Lin</keyname><forenames>Dahua</forenames></author><author><keyname>Chen</keyname><forenames>Kai</forenames></author><author><keyname>Zhao</keyname><forenames>Cairong</forenames></author></authors><title>AlchemistCoder: Harmonizing and Eliciting Code Capability by Hindsight   Tuning on Multi-source Data</title><categories>cs.CL</categories><comments>Preprint with 20 pages and 20 figures. Source code and models at   https://github.com/InternLM/AlchemistCoder</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Open-source Large Language Models (LLMs) and their specialized variants, particularly Code LLMs, have recently delivered impressive performance. However, previous Code LLMs are typically fine-tuned on single-source data with limited quality and diversity, which may insufficiently elicit the potential of pre-trained Code LLMs. In this paper, we present AlchemistCoder, a series of Code LLMs with enhanced code generation and generalization capabilities fine-tuned on multi-source data. To achieve this, we pioneer to unveil inherent conflicts among the various styles and qualities in multi-source code corpora and introduce data-specific prompts with hindsight relabeling, termed AlchemistPrompts, to harmonize different data sources and instruction-response pairs. Additionally, we propose incorporating the data construction process into the fine-tuning data as code comprehension tasks, including instruction evolution, data filtering, and code review. Extensive experiments demonstrate that AlchemistCoder holds a clear lead among all models of the same size (6.7B/7B) and rivals or even surpasses larger models (15B/33B/70B), showcasing the efficacy of our method in refining instruction-following capabilities and advancing the boundaries of code intelligence. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.19317</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.19317</id><created>2024-05-29</created><updated>2025-02-02</updated><authors><author><keyname>Kato</keyname><forenames>Masahiro</forenames></author></authors><title>Generalized Neyman Allocation for Locally Minimax Optimal Best-Arm   Identification</title><categories>cs.LG cs.AI econ.EM stat.ME stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study investigates an asymptotically locally minimax optimal algorithm for fixed-budget best-arm identification (BAI). We propose the Generalized Neyman Allocation (GNA) algorithm and demonstrate that its worst-case upper bound on the probability of misidentifying the best arm aligns with the worst-case lower bound under the small-gap regime, where the gap between the expected outcomes of the best and suboptimal arms is small. Our lower and upper bounds are tight, matching exactly including constant terms within the small-gap regime. The GNA algorithm generalizes the Neyman allocation for two-armed bandits (Neyman, 1934; Kaufmann et al., 2016) and refines existing BAI algorithms, such as those proposed by Glynn &amp; Juneja (2004). By proposing an asymptotically minimax optimal algorithm, we address the longstanding open issue in BAI (Kaufmann, 2020) and treatment choice (Kasy &amp; Sautmann, 202) by restricting a class of distributions to the small-gap regimes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.20168</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.20168</id><created>2024-05-30</created><authors><author><keyname>Cho</keyname><forenames>Hyunsang</forenames></author><author><keyname>Yoo</keyname><forenames>Seonghoon</forenames></author><author><keyname>Jung</keyname><forenames>Bang Chul</forenames></author><author><keyname>Kang</keyname><forenames>Joonhyuk</forenames></author></authors><title>Enhancing Battlefield Awareness: An Aerial RIS-assisted ISAC System with   Deep Reinforcement Learning</title><categories>eess.SY cs.SY</categories><doi>10.1109/MILCOM61039.2024.10774028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a joint communication and sensing technique for enhancing situational awareness in practical battlefield scenarios. In particular, we propose an aerial reconfigurable intelligent surface (ARIS)-assisted integrated sensing and communication (ISAC) system consisting of a single access point (AP), an ARIS, multiple users, and a sensing target. With deep reinforcement learning (DRL), we jointly optimize the transmit beamforming of the AP, the RIS phase shifts, and the trajectory of the ARIS under signal-to-interference-noise ratio (SINR) constraints. Numerical results demonstrate that the proposed technique outperforms the conventional benchmark schemes by suppressing the self-interference and clutter echo signals or optimizing the RIS phase shifts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.01794</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.01794</id><created>2024-06-03</created><updated>2025-02-01</updated><authors><author><keyname>Zhao</keyname><forenames>Zishuo</forenames></author><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Zhou</keyname><forenames>Yuan</forenames></author></authors><title>It Takes Two: A Peer-Prediction Solution for Blockchain Verifier's   Dilemma</title><categories>cs.CR cs.GT</categories><comments>37 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The security of blockchain systems is fundamentally based on the decentralized consensus in which the majority of parties behave honestly, and the content verification process is essential to maintaining the robustness of blockchain systems. However, the phenomenon that a secure blockchain system with few or no cheaters could not provide sufficient incentive for (rational) verifiers to honestly perform the costly verification, referred to as the Verifier's Dilemma, could incentivize lazy reporting and undermine the fundamental security of blockchain systems. While existing works have attempted to insert deliberate errors to disincentivize lazy verification, the decentralized environment renders it impossible to judge the correctness of verification or detect malicious verifiers directly without additional layers of procedures, e.g., reputation systems or additional committee voting.   In this paper, we initiate the research with the development of a Byzantine-robust peer prediction framework towards the design of one-phase Bayesian truthful mechanisms for the decentralized verification games among multiple verifiers, incentivizing all verifiers to perform honest verification without access to the ground truth even in the presence of noisy observations in the verification process. Furthermore, we optimize our mechanism to realize provable robustness against collusions and other malicious behavior from the verifiers, and also show its resilience to inaccurate priors and beliefs. With the theoretically guaranteed robust incentive properties of our mechanism, our study provides a framework of incentive design for decentralized verification protocols that enhances the security and robustness of the blockchain and potentially other decentralized systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.02619</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.02619</id><created>2024-06-03</created><updated>2025-02-01</updated><authors><author><keyname>Draguns</keyname><forenames>Andis</forenames></author><author><keyname>Gritsevskiy</keyname><forenames>Andrew</forenames></author><author><keyname>Motwani</keyname><forenames>Sumeet Ramesh</forenames></author><author><keyname>Rogers-Smith</keyname><forenames>Charlie</forenames></author><author><keyname>Ladish</keyname><forenames>Jeffrey</forenames></author><author><keyname>de Witt</keyname><forenames>Christian Schroeder</forenames></author></authors><title>Unelicitable Backdoors in Language Models via Cryptographic Transformer   Circuits</title><categories>cs.CR cs.LG</categories><comments>19 pages, 7 figures</comments><journal-ref>38th Conference on Neural Information Processing Systems (NeurIPS   2024)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid proliferation of open-source language models significantly increases the risks of downstream backdoor attacks. These backdoors can introduce dangerous behaviours during model deployment and can evade detection by conventional cybersecurity monitoring systems. In this paper, we introduce a novel class of backdoors in transformer models, that, in contrast to prior art, are unelicitable in nature. Unelicitability prevents the defender from triggering the backdoor, making it impossible to properly evaluate ahead of deployment even if given full white-box access and using automated techniques, such as red-teaming or certain formal verification methods. We show that our novel construction is not only unelicitable thanks to using cryptographic techniques, but also has favourable robustness properties. We confirm these properties in empirical investigations, and provide evidence that our backdoors can withstand state-of-the-art mitigation strategies. Additionally, we expand on previous work by showing that our universal backdoors, while not completely undetectable in white-box settings, can be harder to detect than some existing designs. By demonstrating the feasibility of seamlessly integrating backdoors into transformer models, this paper fundamentally questions the efficacy of pre-deployment detection strategies. This offers new insights into the offence-defence balance in AI safety and security. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.02778</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.02778</id><created>2024-06-04</created><updated>2025-02-02</updated><authors><author><keyname>Deutsch</keyname><forenames>Shay</forenames></author><author><keyname>Yelibi</keyname><forenames>Lionel</forenames></author><author><keyname>Lin</keyname><forenames>Alex Tong</forenames></author><author><keyname>Kannan</keyname><forenames>Arjun Ravi</forenames></author></authors><title>MS-IMAP -- A Multi-Scale Graph Embedding Approach for Interpretable   Manifold Learning</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Deriving meaningful representations from complex, high-dimensional data in unsupervised settings is crucial across diverse machine learning applications. This paper introduces a framework for multi-scale graph network embedding based on spectral graph wavelets that employs a contrastive learning approach. We theoretically show that in Paley-Wiener spaces on combinatorial graphs, the spectral graph wavelets operator provides greater flexibility and control over smoothness compared to the Laplacian operator, motivating our approach. An additional key advantage of the proposed embedding is its ability to establish a correspondence between the embedding and input feature spaces, enabling the derivation of feature importance. We validate the effectiveness of our graph embedding framework on multiple public datasets across various downstream tasks, including clustering and unsupervised feature importance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.03376</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.03376</id><created>2024-06-05</created><updated>2025-02-03</updated><authors><author><keyname>Wu</keyname><forenames>Yifan</forenames></author><author><keyname>Yu</keyname><forenames>Siyu</forenames></author><author><keyname>Li</keyname><forenames>Ying</forenames></author></authors><title>Log Parsing using LLMs with Self-Generated In-Context Learning and   Self-Correction</title><categories>cs.SE</categories><comments>Accepted by the 33rd IEEE/ACM International Conference on Program   Comprehension (ICPC'25)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Log parsing transforms log messages into structured formats, serving as a crucial step for log analysis. Despite a variety of log parsers that have been proposed, their performance on evolving log data remains unsatisfactory due to reliance on human-crafted rules or learning-based models with limited training data. The recent emergence of large language models (LLMs) has demonstrated strong abilities in understanding natural language and code, making it promising to apply LLMs for log parsing. Consequently, several studies have proposed LLM-based log parsers. However, LLMs may produce inaccurate templates, and existing LLM-based log parsers directly use the template generated by the LLM as the parsing result, hindering the accuracy of log parsing. Furthermore, these log parsers depend heavily on historical log data as demonstrations, which poses challenges in maintaining accuracy when dealing with scarce historical log data or evolving log data. To address these challenges, we propose AdaParser, an effective and adaptive log parsing framework using LLMs with self-generated in-context learning (SG-ICL) and self-correction. To facilitate accurate log parsing, AdaParser incorporates a novel component, a template corrector, which utilizes the LLM to correct potential parsing errors in the templates it generates. In addition, AdaParser maintains a dynamic candidate set composed of previously generated templates as demonstrations to adapt evolving log data. Extensive experiments on public large-scale datasets indicate that AdaParser outperforms state-of-the-art methods across all metrics, even in zero-shot scenarios. Moreover, when integrated with different LLMs, AdaParser consistently enhances the performance of the utilized LLMs by a large margin. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.03993</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.03993</id><created>2024-06-06</created><updated>2025-01-31</updated><authors><author><keyname>Askari</keyname><forenames>Hadi</forenames></author><author><keyname>Chhabra</keyname><forenames>Anshuman</forenames></author><author><keyname>Chen</keyname><forenames>Muhao</forenames></author><author><keyname>Mohapatra</keyname><forenames>Prasant</forenames></author></authors><title>Assessing LLMs for Zero-shot Abstractive Summarization Through the Lens   of Relevance Paraphrasing</title><categories>cs.CL</categories><comments>Accepted to NAACL 2025 Findings</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) have achieved state-of-the-art performance at zero-shot generation of abstractive summaries for given articles. However, little is known about the robustness of such a process of zero-shot summarization. To bridge this gap, we propose relevance paraphrasing, a simple strategy that can be used to measure the robustness of LLMs as summarizers. The relevance paraphrasing approach identifies the most relevant sentences that contribute to generating an ideal summary, and then paraphrases these inputs to obtain a minimally perturbed dataset. Then, by evaluating model performance for summarization on both the original and perturbed datasets, we can assess the LLM's one aspect of robustness. We conduct extensive experiments with relevance paraphrasing on 4 diverse datasets, as well as 4 LLMs of different sizes (GPT-3.5-Turbo, Llama-2-13B, Mistral-7B, and Dolly-v2-7B). Our results indicate that LLMs are not consistent summarizers for the minimally perturbed articles, necessitating further improvements. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.04328</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.04328</id><created>2024-06-06</created><updated>2025-01-31</updated><authors><author><keyname>Jayalath</keyname><forenames>Dulhan</forenames></author><author><keyname>Landau</keyname><forenames>Gilad</forenames></author><author><keyname>Shillingford</keyname><forenames>Brendan</forenames></author><author><keyname>Woolrich</keyname><forenames>Mark</forenames></author><author><keyname>Jones</keyname><forenames>Oiwi Parker</forenames></author></authors><title>The Brain's Bitter Lesson: Scaling Speech Decoding With Self-Supervised   Learning</title><categories>cs.LG</categories><comments>16 pages, 4 figures, 4 tables, under review</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The past few years have seen remarkable progress in the decoding of speech from brain activity, primarily driven by large single-subject datasets. However, due to individual variation, such as anatomy, and differences in task design and scanning hardware, leveraging data across subjects and datasets remains challenging. In turn, the field has not benefited from the growing number of open neural data repositories to exploit large-scale deep learning. To address this, we develop neuroscience-informed self-supervised objectives, together with an architecture, for learning from heterogeneous brain recordings. Scaling to nearly 400 hours of MEG data and 900 subjects, our approach shows generalisation across participants, datasets, tasks, and even to novel subjects. It achieves improvements of 15-27% over state-of-the-art models and matches surgical decoding performance with non-invasive data. These advances unlock the potential for scaling speech decoding models beyond the current frontier. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.04531</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.04531</id><created>2024-06-06</created><updated>2025-02-01</updated><authors><author><keyname>Wang</keyname><forenames>Wenhan</forenames></author><author><keyname>Yang</keyname><forenames>Chenyuan</forenames></author><author><keyname>Wang</keyname><forenames>Zhijie</forenames></author><author><keyname>Huang</keyname><forenames>Yuheng</forenames></author><author><keyname>Chu</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Song</keyname><forenames>Da</forenames></author><author><keyname>Zhang</keyname><forenames>Lingming</forenames></author><author><keyname>Chen</keyname><forenames>An Ran</forenames></author><author><keyname>Ma</keyname><forenames>Lei</forenames></author></authors><title>TESTEVAL: Benchmarking Large Language Models for Test Case Generation</title><categories>cs.SE</categories><comments>In NAACL 2025 findings</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Testing plays a crucial role in the software development cycle, enabling the detection of bugs, vulnerabilities, and other undesirable behaviors. To perform software testing, testers need to write code snippets that execute the program under test. Recently, researchers have recognized the potential of large language models (LLMs) in software testing. However, there remains a lack of fair comparisons between different LLMs in terms of test case generation capabilities.   In this paper, we propose TESTEVAL, a novel benchmark for test case generation with LLMs. We collect 210 Python programs from an online programming platform, LeetCode, and design three different tasks: overall coverage, targeted line/branch coverage, and targeted path coverage. We further evaluate sixteen popular LLMs, including both commercial and open-source ones, on TESTEVAL. We find that generating test cases to cover specific program lines/branches/paths is still challenging for current LLMs, indicating a lack of ability to comprehend program logic and execution paths. We have open-sourced our dataset and benchmark pipelines at https://github.com/LLM4SoftwareTesting/TestEval. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.04734</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.04734</id><created>2024-06-07</created><updated>2025-02-03</updated><authors><author><keyname>Alt</keyname><forenames>Tobias</forenames></author><author><keyname>Ibisch</keyname><forenames>Andrea</forenames></author><author><keyname>Meiser</keyname><forenames>Clemens</forenames></author><author><keyname>Wilhelm</keyname><forenames>Anna</forenames></author><author><keyname>Zimmer</keyname><forenames>Raphael</forenames></author><author><keyname>Ditz</keyname><forenames>Jonas</forenames></author><author><keyname>Dresen</keyname><forenames>Dominique</forenames></author><author><keyname>Droste</keyname><forenames>Christoph</forenames></author><author><keyname>Karschau</keyname><forenames>Jens</forenames></author><author><keyname>Laus</keyname><forenames>Friederike</forenames></author><author><keyname>Müller</keyname><forenames>Oliver</forenames></author><author><keyname>Neu</keyname><forenames>Matthias</forenames></author><author><keyname>Plaga</keyname><forenames>Rainer</forenames></author><author><keyname>Plesch</keyname><forenames>Carola</forenames></author><author><keyname>Sennewald</keyname><forenames>Britta</forenames></author><author><keyname>Thaeren</keyname><forenames>Thomas</forenames></author><author><keyname>Unverricht</keyname><forenames>Kristina</forenames></author><author><keyname>Waurick</keyname><forenames>Steffen</forenames></author></authors><title>Generative AI Models: Opportunities and Risks for Industry and   Authorities</title><categories>cs.AI cs.CL cs.CR</categories><comments>67 pages, 3 figures</comments><msc-class>68T50 (Primary), 68M25, 68T07 (Secondary)</msc-class><acm-class>I.2.7; I.2.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative AI models are capable of performing a wide variety of tasks that have traditionally required creativity and human understanding. During training, they learn patterns from existing data and can subsequently generate new content such as texts, images, audio, and videos that align with these patterns. Due to their versatility and generally high-quality results, they represent, on the one hand, an opportunity for digitalisation. On the other hand, the use of generative AI models introduces novel IT security risks that must be considered as part of a comprehensive analysis of the IT security threat landscape. In response to this risk potential, companies or authorities intending to use generative AI should conduct an individual risk analysis before integrating it into their workflows. The same applies to developers and operators, as many risks associated with generative AI must be addressed during development or can only be influenced by the operating organisation. Based on this, existing security measures can be adapted, and additional measures implemented. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.05189</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.05189</id><created>2024-06-07</created><updated>2025-01-31</updated><authors><author><keyname>Lam</keyname><forenames>Jorden</forenames></author><author><keyname>Xu</keyname><forenames>Kunpeng</forenames></author></authors><title>Analyzing the factors that are involved in length of inpatient stay at   the hospital for diabetes patients</title><categories>stat.AP cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The paper investigates the escalating concerns surrounding the surge in diabetes cases, exacerbated by the COVID-19 pandemic, and the subsequent strain on medical resources. The research aims to construct a predictive model quantifying factors influencing inpatient hospital stay durations for diabetes patients, offering insights to hospital administrators for improved patient management strategies. The literature review highlights the increasing prevalence of diabetes, emphasizing the need for continued attention and analysis of urban-rural disparities in healthcare access. International studies underscore the financial implications and healthcare burden associated with diabetes-related hospitalizations and complications, emphasizing the significance of effective management strategies. The methodology involves a quantitative approach, utilizing a dataset comprising 10,000 observations of diabetic inpatient encounters in U.S. hospitals from 1999 to 2008. Predictive modeling techniques, particularly Generalized Linear Models (GLM), are employed to develop a model predicting hospital stay durations based on patient demographics, admission types, medical history, and treatment regimen. The results highlight the influence of age, medical history, and treatment regimen on hospital stay durations for diabetes patients. Despite model limitations, such as heteroscedasticity and deviations from normality in residual analysis, the findings offer valuable insights for hospital administrators in patient management. The paper concludes with recommendations for future research to address model limitations and explore the implications of predictive models on healthcare management strategies, ensuring equitable patient care and resource allocation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.05254</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.05254</id><created>2024-06-07</created><updated>2025-02-01</updated><authors><author><keyname>Bertolotti</keyname><forenames>Beatrice</forenames></author><author><keyname>Russo</keyname><forenames>Matteo</forenames></author><author><keyname>Schwiegelshohn</keyname><forenames>Chris</forenames></author><author><keyname>Shyam</keyname><forenames>Sudarshan</forenames></author></authors><title>Simple and Optimal Sublinear Algorithms for Mean Estimation</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study the sublinear multivariate mean estimation problem in $d$-dimensional Euclidean space. Specifically, we aim to find the mean $\mu$ of a ground point set $A$, which minimizes the sum of squared Euclidean distances of the points in $A$ to $\mu$. We first show that a multiplicative $(1+\varepsilon)$ approximation to $\mu$ can be found with probability $1-\delta$ using $O(\varepsilon^{-1}\log \delta^{-1})$ many independent uniform random samples, and provide a matching lower bound. Furthermore, we give two sublinear time algorithms with optimal sample complexity for extracting a suitable approximate mean:   1. A gradient descent approach running in time $O((\varepsilon^{-1}+\log\log \delta^{-1})\cdot \log \delta^{-1} \cdot d)$. It optimizes the geometric median objective while being significantly faster for our specific setting than all other known algorithms for this problem.   2. An order statistics and clustering approach running in time $O\left((\varepsilon^{-1}+\log^{\gamma}\delta^{-1})\cdot \log \delta^{-1} \cdot d\right)$ for any constant $\gamma&gt;0$.   Throughout our analysis, we also generalize the familiar median-of-means estimator to the multivariate case, showing that the geometric median-of-means estimator achieves an optimal sample complexity for estimating $\mu$, which may be of independent interest. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.05465</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.05465</id><created>2024-06-08</created><updated>2024-09-09</updated><authors><author><keyname>Samak</keyname><forenames>Tanmay Vilas</forenames></author><author><keyname>Samak</keyname><forenames>Chinmay Vilas</forenames></author><author><keyname>Krovi</keyname><forenames>Venkat Narayan</forenames></author></authors><title>Metaverse for Safer Roadways: An Immersive Digital Twin Framework for   Exploring Human-Autonomy Coexistence in Urban Transportation Systems</title><categories>cs.RO</categories><comments>Accepted at IEEE Conference on Telepresence (TELE) 2024</comments><journal-ref>2024 IEEE Conference on Telepresence, Pasadena, CA, USA, 2024, pp.   160-165</journal-ref><doi>10.1109/Telepresence63209.2024.10841549</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Societal-scale deployment of autonomous vehicles requires them to coexist with human drivers, necessitating mutual understanding and coordination among these entities. However, purely real-world or simulation-based experiments cannot be employed to explore such complex interactions due to safety and reliability concerns, respectively. Consequently, this work presents an immersive digital twin framework to explore and experiment with the interaction dynamics between autonomous and non-autonomous traffic participants. Particularly, we employ a mixed-reality human-machine interface to allow human drivers and autonomous agents to observe and interact with each other for testing edge-case scenarios while ensuring safety at all times. To validate the versatility of the proposed framework's modular architecture, we first present a discussion on a set of user experience experiments encompassing 4 different levels of immersion with 4 distinct user interfaces. We then present a case study of uncontrolled intersection traversal to demonstrate the efficacy of the proposed framework in validating the interactions of a primary human-driven, autonomous, and connected autonomous vehicle with a secondary semi-autonomous vehicle. The proposed framework has been openly released to guide the future of autonomy-oriented digital twins and research on human-autonomy coexistence. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.05661</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.05661</id><created>2024-06-09</created><updated>2025-02-03</updated><authors><author><keyname>Yadav</keyname><forenames>Hemant</forenames></author><author><keyname>Sitaram</keyname><forenames>Sunayana</forenames></author><author><keyname>Shah</keyname><forenames>Rajiv Ratn</forenames></author></authors><title>MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked   Language Modelling methods for learning Speech Representations</title><categories>cs.CL</categories><comments>4 pages, submitted to interspeech2024</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  In recent years, self-supervised pre-training methods have gained significant traction in learning high-level information from raw speech. Among these methods, HuBERT has demonstrated SOTA performance in automatic speech recognition (ASR). However, HuBERT's performance lags behind data2vec due to disparities in pre-training strategies. In this paper, we propose (i) a Swap method to address pre-training and inference mismatch observed in HuBERT and (ii) incorporates Multicluster masked prediction loss for more effective utilization of the models capacity. The resulting method is, MS-HuBERT, an end-to-end self-supervised pre-training method for learning robust speech representations. It beats vanilla HuBERT on the ASR Librispeech benchmark on average by a 5% margin when evaluated on different finetuning splits. Additionally, we demonstrate that the learned embeddings obtained during pre-training encode essential information for improving performance of content based tasks such as ASR. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.05754</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.05754</id><created>2024-06-09</created><updated>2025-02-02</updated><authors><author><keyname>Calder</keyname><forenames>Jeff</forenames></author><author><keyname>Drenska</keyname><forenames>Nadejda</forenames></author><author><keyname>Mosaphir</keyname><forenames>Drisana</forenames></author></authors><title>Numerical solution of a PDE arising from prediction with expert advice</title><categories>math.NA cs.AI cs.LG cs.NA math.AP</categories><msc-class>35D40, 65N12, 65N06, 35Q68, 35J60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work investigates the online machine learning problem of prediction with expert advice in an adversarial setting through numerical analysis of, and experiments with, a related partial differential equation. The problem is a repeated two-person game involving decision-making at each step informed by $n$ experts in an adversarial environment. The continuum limit of this game over a large number of steps is a degenerate elliptic equation whose solution encodes the optimal strategies for both players. We develop numerical methods for approximating the solution of this equation in relatively high dimensions ($n\leq 10$) by exploiting symmetries in the equation and the solution to drastically reduce the size of the computational domain. Based on our numerical results we make a number of conjectures about the optimality of various adversarial strategies, in particular about the non-optimality of the COMB strategy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.08411</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.08411</id><created>2024-06-12</created><updated>2025-02-01</updated><authors><author><keyname>Zhao</keyname><forenames>Xinyan</forenames></author><author><keyname>Sun</keyname><forenames>Yuan</forenames></author><author><keyname>Liu</keyname><forenames>Wenlin</forenames></author><author><keyname>Wong</keyname><forenames>Chau-Wai</forenames></author></authors><title>Tailoring Generative AI Chatbots for Multiethnic Communities in Disaster   Preparedness Communication: Extending the CASA Paradigm</title><categories>cs.CL cs.AI cs.HC</categories><comments>Accepted for Publication in Journal of Computer-Mediated   Communication</comments><msc-class>68U15</msc-class><doi>10.1093/jcmc/zmae022</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This study is among the first to develop different prototypes of generative artificial intelligence (GenAI) chatbots powered by GPT-4 to communicate hurricane preparedness information to diverse residents. Drawing from the Computers Are Social Actors paradigm and the literature on disaster vulnerability and cultural tailoring, we conducted a between-subjects experiment with 441 Black, Hispanic, and Caucasian residents of Florida. Our results suggest that GenAI chatbots varying in tone formality and cultural tailoring significantly influence perceptions of their friendliness and credibility, which, in turn, relate to hurricane preparedness outcomes. These results highlight the potential of using GenAI chatbots to improve diverse communities' disaster preparedness. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.08653</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.08653</id><created>2024-06-12</created><authors><author><keyname>Naik</keyname><forenames>Lakshadeep</forenames></author><author><keyname>Kalkan</keyname><forenames>Sinan</forenames></author><author><keyname>Sørensen</keyname><forenames>Sune L.</forenames></author><author><keyname>Kjærgaard</keyname><forenames>Mikkel B.</forenames></author><author><keyname>Krüger</keyname><forenames>Norbert</forenames></author></authors><title>BaSeNet: A Learning-based Mobile Manipulator Base Pose Sequence Planning   for Pickup Tasks</title><categories>cs.RO</categories><comments>Submitted to IROS 2024</comments><journal-ref>2024 IEEE/RSJ International Conference on Intelligent Robots and   Systems (IROS)</journal-ref><doi>10.1109/IROS58592.2024.10802615</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications, a mobile manipulator robot is required to grasp a set of objects distributed in space. This may not be feasible from a single base pose and the robot must plan the sequence of base poses for grasping all objects, minimizing the total navigation and grasping time. This is a Combinatorial Optimization problem that can be solved using exact methods, which provide optimal solutions but are computationally expensive, or approximate methods, which offer computationally efficient but sub-optimal solutions. Recent studies have shown that learning-based methods can solve Combinatorial Optimization problems, providing near-optimal and computationally efficient solutions.   In this work, we present BASENET - a learning-based approach to plan the sequence of base poses for the robot to grasp all the objects in the scene. We propose a Reinforcement Learning based solution that learns the base poses for grasping individual objects and the sequence in which the objects should be grasped to minimize the total navigation and grasping costs using Layered Learning. As the problem has a varying number of states and actions, we represent states and actions as a graph and use Graph Neural Networks for learning. We show that the proposed method can produce comparable solutions to exact and approximate methods with significantly less computation time. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.10244</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.10244</id><created>2024-06-06</created><updated>2025-02-01</updated><authors><author><keyname>Zhang</keyname><forenames>Sheng</forenames></author><author><keyname>Wang</keyname><forenames>Maolin</forenames></author><author><keyname>Wang</keyname><forenames>Wanyu</forenames></author><author><keyname>Gao</keyname><forenames>Jingtong</forenames></author><author><keyname>Zhao</keyname><forenames>Xiangyu</forenames></author><author><keyname>Yang</keyname><forenames>Yu</forenames></author><author><keyname>Wei</keyname><forenames>Xuetao</forenames></author><author><keyname>Liu</keyname><forenames>Zitao</forenames></author><author><keyname>Xu</keyname><forenames>Tong</forenames></author></authors><title>GLINT-RU: Gated Lightweight Intelligent Recurrent Units for Sequential   Recommender Systems</title><categories>cs.IR cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transformer-based models have gained significant traction in sequential recommender systems (SRSs) for their ability to capture user-item interactions effectively. However, these models often suffer from high computational costs and slow inference. Meanwhile, existing efficient SRS approaches struggle to embed high-quality semantic and positional information into latent representations. To tackle these challenges, this paper introduces GLINT-RU, a lightweight and efficient SRS leveraging a single-layer dense selective Gated Recurrent Units (GRU) module to accelerate inference. By incorporating a dense selective gate, GLINT-RU adaptively captures temporal dependencies and fine-grained positional information, generating high-quality latent representations. Additionally, a parallel mixing block infuses fine-grained positional features into user-item interactions, enhancing both recommendation quality and efficiency. Extensive experiments on three datasets demonstrate that GLINT-RU achieves superior prediction accuracy and inference speed, outperforming baselines based on RNNs, Transformers, MLPs, and SSMs. These results establish GLINT-RU as a powerful and efficient solution for SRSs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.11668</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.11668</id><created>2024-06-17</created><updated>2025-02-03</updated><authors><author><keyname>Mei</keyname><forenames>Lingrui</forenames></author><author><keyname>Liu</keyname><forenames>Shenghua</forenames></author><author><keyname>Wang</keyname><forenames>Yiwei</forenames></author><author><keyname>Bi</keyname><forenames>Baolong</forenames></author><author><keyname>Mao</keyname><forenames>Jiayi</forenames></author><author><keyname>Cheng</keyname><forenames>Xueqi</forenames></author></authors><title>"Not Aligned" is Not "Malicious": Being Careful about Hallucinations of   Large Language Models' Jailbreak</title><categories>cs.CL</categories><comments>COLING 2025</comments><report-no>2025.coling-main.146</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  "Jailbreak" is a major safety concern of Large Language Models (LLMs), which occurs when malicious prompts lead LLMs to produce harmful outputs, raising issues about the reliability and safety of LLMs. Therefore, an effective evaluation of jailbreaks is very crucial to develop its mitigation strategies. However, our research reveals that many jailbreaks identified by current evaluations may actually be hallucinations-erroneous outputs that are mistaken for genuine safety breaches. This finding suggests that some perceived vulnerabilities might not represent actual threats, indicating a need for more precise red teaming benchmarks. To address this problem, we propose the $\textbf{B}$enchmark for reli$\textbf{AB}$ilit$\textbf{Y}$ and jail$\textbf{B}$reak ha$\textbf{L}$l$\textbf{U}$cination $\textbf{E}$valuation (BabyBLUE). BabyBLUE introduces a specialized validation framework including various evaluators to enhance existing jailbreak benchmarks, ensuring outputs are useful malicious instructions. Additionally, BabyBLUE presents a new dataset as an augmentation to the existing red teaming benchmarks, specifically addressing hallucinations in jailbreaks, aiming to evaluate the true potential of jailbroken LLM outputs to cause harm to human society. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.12433</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.12433</id><created>2024-06-18</created><updated>2025-02-02</updated><authors><author><keyname>Gao</keyname><forenames>Jingtong</forenames></author><author><keyname>Chen</keyname><forenames>Bo</forenames></author><author><keyname>Liu</keyname><forenames>Weiwen</forenames></author><author><keyname>Li</keyname><forenames>Xiangyang</forenames></author><author><keyname>Wang</keyname><forenames>Yichao</forenames></author><author><keyname>Wang</keyname><forenames>Wanyu</forenames></author><author><keyname>Guo</keyname><forenames>Huifeng</forenames></author><author><keyname>Tang</keyname><forenames>Ruiming</forenames></author><author><keyname>Zhao</keyname><forenames>Xiangyu</forenames></author></authors><title>LLM4Rerank: LLM-based Auto-Reranking Framework for Recommendations</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reranking is a critical component in recommender systems, playing an essential role in refining the output of recommendation algorithms. Traditional reranking models have focused predominantly on accuracy, but modern applications demand consideration of additional criteria such as diversity and fairness. Existing reranking approaches often fail to harmonize these diverse criteria effectively at the model level. Moreover, these models frequently encounter challenges with scalability and personalization due to their complexity and the varying significance of different reranking criteria in diverse scenarios. In response, we introduce a comprehensive reranking framework enhanced by LLM, designed to seamlessly integrate various reranking criteria while maintaining scalability and facilitating personalized recommendations. This framework employs a fully connected graph structure, allowing the LLM to simultaneously consider multiple aspects such as accuracy, diversity, and fairness through a coherent Chain-of-Thought (CoT) process. A customizable input mechanism is also integrated, enabling the tuning of the language model's focus to meet specific reranking needs. We validate our approach using three popular public datasets, where our framework demonstrates superior performance over existing state-of-the-art reranking models in balancing multiple criteria. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.12615</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.12615</id><created>2024-06-18</created><updated>2025-01-31</updated><authors><author><keyname>Zhang</keyname><forenames>Yedi</forenames></author><author><keyname>Saxe</keyname><forenames>Andrew</forenames></author><author><keyname>Latham</keyname><forenames>Peter E.</forenames></author></authors><title>When Are Bias-Free ReLU Networks Effectively Linear Networks?</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We investigate the implications of removing bias in ReLU networks regarding their expressivity and learning dynamics. We first show that two-layer bias-free ReLU networks have limited expressivity: the only odd function two-layer bias-free ReLU networks can express is a linear one. We then show that, under symmetry conditions on the data, these networks have the same learning dynamics as linear networks. This enables us to give analytical time-course solutions to certain two-layer bias-free (leaky) ReLU networks outside the lazy learning regime. While deep bias-free ReLU networks are more expressive than their two-layer counterparts, they still share a number of similarities with deep linear networks. These similarities enable us to leverage insights from linear networks to understand certain ReLU networks. Overall, our results show that some properties previously established for bias-free ReLU networks arise due to equivalence to linear networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.12915</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.12915</id><created>2024-06-13</created><updated>2025-02-01</updated><authors><author><keyname>Zhou</keyname><forenames>Yijin</forenames></author><author><keyname>Ge</keyname><forenames>Yutang</forenames></author><author><keyname>Dong</keyname><forenames>Xiaowen</forenames></author><author><keyname>Wang</keyname><forenames>Yuguang</forenames></author></authors><title>How Out-of-Distribution Detection Learning Theory Enhances Transformer:   Learnability and Reliability</title><categories>cs.LG math.PR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Transformer networks excel in natural language processing and computer vision tasks. However, they still face challenges in generalizing to Out-of-Distribution (OOD) datasets, i.e. data whose distribution differs from that seen during training. The OOD detection aims to distinguish outliers while preserving in-distribution (ID) data performance. This paper introduces the OOD detection Probably Approximately Correct (PAC) Theory for transformers, which establishes the conditions for data distribution and model configurations for the learnability of transformers in terms of OOD detection. The theory demonstrates that outliers can be accurately represented and distinguished with sufficient data. The theoretical implications highlight the trade-off between theoretical principles and practical training paradigms. By examining this trade-off, we naturally derived the rationale for leveraging auxiliary outliers to enhance OOD detection. Our theory suggests that by penalizing the misclassification of outliers within the loss function and strategically generating soft synthetic outliers, one can robustly bolster the reliability of transformer networks. This approach yields a novel algorithm that ensures learnability and refines the decision boundaries between inliers and outliers. In practice, the algorithm consistently achieves state-of-the-art performance across various data formats. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.14230</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.14230</id><created>2024-06-20</created><updated>2025-02-03</updated><authors><author><keyname>Jiang</keyname><forenames>Han</forenames></author><author><keyname>Yi</keyname><forenames>Xiaoyuan</forenames></author><author><keyname>Wei</keyname><forenames>Zhihua</forenames></author><author><keyname>Xiao</keyname><forenames>Ziang</forenames></author><author><keyname>Wang</keyname><forenames>Shu</forenames></author><author><keyname>Xie</keyname><forenames>Xing</forenames></author></authors><title>Raising the Bar: Investigating the Values of Large Language Models via   Generative Evolving Testing</title><categories>cs.CL cs.AI cs.CY</categories><comments>Under Review</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Warning: Contains harmful model outputs.   Despite significant advancements, the propensity of Large Language Models (LLMs) to generate harmful and unethical content poses critical challenges. Measuring value alignment of LLMs becomes crucial for their regulation and responsible deployment. Although numerous benchmarks have been constructed to assess social bias, toxicity, and ethical issues in LLMs, those static benchmarks suffer from evaluation chronoeffect, in which, as models rapidly evolve, existing benchmarks may leak into training data or become saturated, overestimating ever-developing LLMs. To tackle this problem, we propose GETA, a novel generative evolving testing approach based on adaptive testing methods in measurement theory. Unlike traditional adaptive testing methods that rely on a static test item pool, GETA probes the underlying moral boundaries of LLMs by dynamically generating test items tailored to model capability. GETA co-evolves with LLMs by learning a joint distribution of item difficulty and model value conformity, thus effectively addressing evaluation chronoeffect. We evaluated various popular LLMs with GETA and demonstrated that 1) GETA can dynamically create difficulty-tailored test items and 2) GETA's evaluation results are more consistent with models' performance on unseen OOD and i.i.d. items, laying the groundwork for future evaluation paradigms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.14426</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.14426</id><created>2024-06-20</created><updated>2025-02-01</updated><authors><author><keyname>Klein</keyname><forenames>Leon</forenames></author><author><keyname>Noé</keyname><forenames>Frank</forenames></author></authors><title>Transferable Boltzmann Generators</title><categories>stat.ML cs.LG physics.chem-ph physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generation of equilibrium samples of molecular systems has been a long-standing problem in statistical physics. Boltzmann Generators are a generative machine learning method that addresses this issue by learning a transformation via a normalizing flow from a simple prior distribution to the target Boltzmann distribution of interest. Recently, flow matching has been employed to train Boltzmann Generators for small molecular systems in Cartesian coordinates. We extend this work and propose a first framework for Boltzmann Generators that are transferable across chemical space, such that they predict zero-shot Boltzmann distributions for test molecules without being retrained for these systems. These transferable Boltzmann Generators allow approximate sampling from the target distribution of unseen systems, as well as efficient reweighting to the target Boltzmann distribution. The transferability of the proposed framework is evaluated on dipeptides, where we show that it generalizes efficiently to unseen systems. Furthermore, we demonstrate that our proposed architecture enhances the efficiency of Boltzmann Generators trained on single molecular systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.14479</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.14479</id><created>2024-06-20</created><updated>2025-02-01</updated><authors><author><keyname>Jiang</keyname><forenames>Jiachen</forenames></author><author><keyname>Zhou</keyname><forenames>Jinxin</forenames></author><author><keyname>Zhu</keyname><forenames>Zhihui</forenames></author></authors><title>Tracing Representation Progression: Analyzing and Enhancing Layer-Wise   Similarity</title><categories>cs.AI cs.CL cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analyzing the similarity of internal representations has been an important technique for understanding the behavior of deep neural networks. Most existing methods for analyzing the similarity between representations of high dimensions, such as those based on Centered Kernel Alignment (CKA), rely on statistical properties of the representations for a set of data points. In this paper, we focus on transformer models and study the similarity of representations between the hidden layers of individual transformers. In this context, we show that a simple sample-wise cosine similarity metric is capable of capturing the similarity and aligns with the complicated CKA. Our experimental results on common transformers reveal that representations across layers are positively correlated, with similarity increasing when layers get closer. We provide a theoretical justification for this phenomenon under the geodesic curve assumption for the learned transformer. We then show that an increase in representation similarity implies an increase in predicted probability when directly applying the last-layer classifier to any hidden layer representation. We then propose an aligned training method to improve the effectiveness of shallow layer by enhancing the similarity between internal representations, with trained models that enjoy the following properties: (1) more early saturation events, (2) layer-wise accuracies monotonically increase and reveal the minimal depth needed for the given task, (3) when served as multi-exit models, they achieve on-par performance with standard multi-exit architectures which consist of additional classifiers designed for early exiting in shallow layers. To our knowledge, our work is the first to show that one common classifier is sufficient for multi-exit models. We conduct experiments on both vision and NLP tasks to demonstrate the performance of the proposed aligned training. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.14568</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.14568</id><created>2024-04-29</created><updated>2025-02-01</updated><authors><author><keyname>Yavuz</keyname><forenames>Mehmet Can</forenames></author><author><keyname>Yang</keyname><forenames>Yang</forenames></author></authors><title>Policy Gradient-Driven Noise Mask</title><categories>eess.IV cs.CV</categories><comments>International Conference on Pattern Recognition (2024) Accepted Paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning classifiers face significant challenges when dealing with heterogeneous multi-modal and multi-organ biomedical datasets. The low-level feature distinguishability limited to imaging-modality hinders the classifiers' ability to learn high-level semantic relationships, resulting in sub-optimal performance. To address this issue, image augmentation strategies are employed as regularization techniques. While additive noise input during network training is a well-established augmentation as regularization method, modern pipelines often favor more robust techniques such as dropout and weight decay. This preference stems from the observation that combining these established techniques with noise input can adversely affect model performance. In this study, we propose a novel pretraining pipeline that learns to generate conditional noise mask specifically tailored to improve performance on multi-modal and multi-organ datasets. As a reinforcement learning algorithm, our approach employs a dual-component system comprising a very light-weight policy network that learns to sample conditional noise using a differentiable beta distribution as well as a classifier network. The policy network is trained using the reinforce algorithm to generate image-specific noise masks that regularize the classifier during pretraining. A key aspect is that the policy network's role is limited to obtaining an intermediate (or heated) model before fine-tuning. During inference, the policy network is omitted, allowing direct comparison between the baseline and noise-regularized models. We conducted experiments and related analyses on RadImageNet datasets. Results demonstrate that fine-tuning the intermediate models consistently outperforms conventional training algorithms on both classification and generalization to unseen concept tasks. https://github.com/convergedmachine/Policy-Gradient-Driven-Noise-Mask </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.18015</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.18015</id><created>2024-06-25</created><updated>2025-02-03</updated><authors><author><keyname>Granath</keyname><forenames>Andreas</forenames></author><author><keyname>Wang</keyname><forenames>Siyang</forenames></author></authors><title>A hybrid numerical method for elastic wave propagation in discontinuous   media with complex geometry</title><categories>math.NA cs.NA</categories><msc-class>65M06, 65M60, 65M12</msc-class><journal-ref>Journal of Computational Physics 524 (2025) 113745</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a high order accurate numerical method for solving the elastic wave equation in second-order form. We hybridize the computationally efficient Cartesian grid formulation of finite differences with geometrically flexible discontinuous Galerkin methods on unstructured grids by a penalty based technique. At the interface between the two methods, we construct projection operators for the pointwise finite difference solutions and discontinuous Galerkin solutions based on piecewise polynomials. In addition, we optimize the projection operators for both accuracy and spectrum. We prove that the overall discretization conserves a discrete energy, and verify optimal convergence in numerical experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.00066</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.00066</id><created>2024-06-17</created><updated>2025-02-01</updated><authors><author><keyname>Brüel-Gabrielsson</keyname><forenames>Rickard</forenames></author><author><keyname>Zhu</keyname><forenames>Jiacheng</forenames></author><author><keyname>Bhardwaj</keyname><forenames>Onkar</forenames></author><author><keyname>Choshen</keyname><forenames>Leshem</forenames></author><author><keyname>Greenewald</keyname><forenames>Kristjan</forenames></author><author><keyname>Yurochkin</keyname><forenames>Mikhail</forenames></author><author><keyname>Solomon</keyname><forenames>Justin</forenames></author></authors><title>Compress then Serve: Serving Thousands of LoRA Adapters with Little   Overhead</title><categories>cs.DC cs.AI cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Fine-tuning large language models (LLMs) with low-rank adaptations (LoRAs) has become common practice, often yielding numerous copies of the same LLM differing only in their LoRA updates. This paradigm presents challenges for systems that serve real-time responses to queries that each involve a different LoRA. Prior works optimize the design of such systems but still require continuous loading and offloading of LoRAs, as it is infeasible to store thousands of LoRAs in GPU memory. To mitigate this issue, we investigate the efficacy of compression when serving LoRAs. We propose a method for the joint compression of LoRAs into a shared basis paired with LoRA-specific scaling matrices. We extend our algorithm to learn clusters of LoRAs that are amenable to joint compression, allowing it to scale gracefully to large LoRA collections. Our experiments with up to 1000 LoRAs demonstrate that compressed LoRAs preserve performance while offering major throughput gains in realistic serving scenarios with over a thousand LoRAs, maintaining 80% of the throughput of serving a single LoRA. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.00610</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.00610</id><created>2024-06-30</created><updated>2025-02-02</updated><authors><author><keyname>Wu</keyname><forenames>Dongxia</forenames></author><author><keyname>Kuang</keyname><forenames>Nikki Lijing</forenames></author><author><keyname>Niu</keyname><forenames>Ruijia</forenames></author><author><keyname>Ma</keyname><forenames>Yi-An</forenames></author><author><keyname>Yu</keyname><forenames>Rose</forenames></author></authors><title>Diffusion-BBO: Diffusion-Based Inverse Modeling for Online Black-Box   Optimization</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online black-box optimization (BBO) aims to optimize an objective function by iteratively querying a black-box oracle in a sample-efficient way. While prior studies focus on forward approaches such as Gaussian Processes (GPs) to learn a surrogate model for the unknown objective function, they struggle with steering clear of out-of-distribution and invalid designs in scientific discovery tasks. Recently, inverse modeling approaches that map the objective space to the design space with conditional diffusion models have demonstrated impressive capability in learning the data manifold. However, these approaches proceed in an offline fashion with pre-collected data. How to design inverse approaches for online BBO to actively query new data and improve the sample efficiency remains an open question. In this work, we propose Diffusion-BBO, a sample-efficient online BBO framework leveraging the conditional diffusion model as the inverse surrogate model. Diffusion-BBO employs a novel acquisition function Uncertainty-aware Exploration (UaE) to propose scores in the objective space for conditional sampling. We theoretically prove that Diffusion-BBO with UaE achieves a near-optimal solution for online BBO. We also empirically demonstrate that Diffusion-BBO with UaE outperforms existing online BBO baselines across 6 scientific discovery tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.01960</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.01960</id><created>2024-07-02</created><updated>2025-01-31</updated><authors><author><keyname>Cao</keyname><forenames>Cong</forenames></author><author><keyname>Yue</keyname><forenames>Huanjing</forenames></author><author><keyname>Liu</keyname><forenames>Xin</forenames></author><author><keyname>Yang</keyname><forenames>Jingyu</forenames></author></authors><title>Zero-Shot Video Restoration and Enhancement Using Pre-Trained Image   Diffusion Model</title><categories>cs.CV cs.LG</categories><comments>Accepted by AAAI 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diffusion-based zero-shot image restoration and enhancement models have achieved great success in various tasks of image restoration and enhancement. However, directly applying them to video restoration and enhancement results in severe temporal flickering artifacts. In this paper, we propose the first framework for zero-shot video restoration and enhancement based on the pre-trained image diffusion model. By replacing the spatial self-attention layer with the proposed short-long-range (SLR) temporal attention layer, the pre-trained image diffusion model can take advantage of the temporal correlation between frames. We further propose temporal consistency guidance, spatial-temporal noise sharing, and an early stopping sampling strategy to improve temporally consistent sampling. Our method is a plug-and-play module that can be inserted into any diffusion-based image restoration or enhancement methods to further improve their performance. Experimental results demonstrate the superiority of our proposed method. Our code is available at https://github.com/cao-cong/ZVRD. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.02433</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.02433</id><created>2024-07-02</created><updated>2025-02-03</updated><authors><author><keyname>Kabalan</keyname><forenames>Abbas</forenames></author><author><keyname>Casenave</keyname><forenames>Fabien</forenames></author><author><keyname>Bordeu</keyname><forenames>Felipe</forenames></author><author><keyname>Ehrlacher</keyname><forenames>Virginie</forenames></author><author><keyname>Ern</keyname><forenames>Alexandre</forenames></author></authors><title>Elasticity-based morphing technique and application to reduced-order   modeling</title><categories>math.NA cs.NA</categories><doi>10.1016/j.apm.2025.115929</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this article is to introduce a new methodology for constructing morphings between shapes that have identical topology. The morphings are obtained by deforming a reference shape, through the resolution of a sequence of linear elasticity equations, onto every target shape. In particular, our approach does not assume any knowledge of a boundary parametrization, and the computation of the boundary deformation is not required beforehand. Furthermore, constraints can be imposed on specific points, lines and surfaces in the reference domain to ensure alignment with their counterparts in the target domain after morphing. Additionally, we show how the proposed methodology can be integrated in an offline and online paradigm, which is useful in reduced-order modeling involving variable shapes. This framework facilitates the efficient computation of the morphings in various geometric configurations, thus improving the versatility and applicability of the approach. The robustness and computational efficiency of the methodology is illustrated on two-dimensional test cases, including the regression problem of the drag and lift coefficients of airfoils of non-parameterized variable shapes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.02956</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.02956</id><created>2024-07-03</created><updated>2025-02-02</updated><authors><author><keyname>Frikha</keyname><forenames>Ahmed</forenames></author><author><keyname>Walha</keyname><forenames>Nassim</forenames></author><author><keyname>Nakka</keyname><forenames>Krishna Kanth</forenames></author><author><keyname>Mendes</keyname><forenames>Ricardo</forenames></author><author><keyname>Jiang</keyname><forenames>Xue</forenames></author><author><keyname>Zhou</keyname><forenames>Xuebing</forenames></author></authors><title>IncogniText: Privacy-enhancing Conditional Text Anonymization via   LLM-based Private Attribute Randomization</title><categories>cs.CR cs.AI cs.CL cs.LG</categories><comments>Accepted at NeurIPS 2024 - Safe GenAI Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we address the problem of text anonymization where the goal is to prevent adversaries from correctly inferring private attributes of the author, while keeping the text utility, i.e., meaning and semantics. We propose IncogniText, a technique that anonymizes the text to mislead a potential adversary into predicting a wrong private attribute value. Our empirical evaluation shows a reduction of private attribute leakage by more than 90% across 8 different private attributes. Finally, we demonstrate the maturity of IncogniText for real-world applications by distilling its anonymization capability into a set of LoRA parameters associated with an on-device model. Our results show the possibility of reducing privacy leakage by more than half with limited impact on utility. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.03848</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.03848</id><created>2024-07-04</created><updated>2025-02-01</updated><authors><author><keyname>Peleg</keyname><forenames>Amit</forenames></author><author><keyname>Hein</keyname><forenames>Matthias</forenames></author></authors><title>Bias of Stochastic Gradient Descent or the Architecture: Disentangling   the Effects of Overparameterization of Neural Networks</title><categories>cs.LG</categories><comments>Accepted at ICML 2024. Fixed a bug in Kaiming Gaussian   initialization, does not affect the results of the paper</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Neural networks typically generalize well when fitting the data perfectly, even though they are heavily overparameterized. Many factors have been pointed out as the reason for this phenomenon, including an implicit bias of stochastic gradient descent (SGD) and a possible simplicity bias arising from the neural network architecture. The goal of this paper is to disentangle the factors that influence generalization stemming from optimization and architectural choices by studying random and SGD-optimized networks that achieve zero training error. We experimentally show, in the low sample regime, that overparameterization in terms of increasing width is beneficial for generalization, and this benefit is due to the bias of SGD and not due to an architectural bias. In contrast, for increasing depth, overparameterization is detrimental for generalization, but random and SGD-optimized networks behave similarly, so this can be attributed to an architectural bias. For more information, see https://bias-sgd-or-architecture.github.io . </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.05108</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.05108</id><created>2024-07-06</created><authors><author><keyname>Lyu</keyname><forenames>Shen-Huan</forenames></author><author><keyname>Wu</keyname><forenames>Jin-Hui</forenames></author><author><keyname>Zheng</keyname><forenames>Qin-Cheng</forenames></author><author><keyname>Ye</keyname><forenames>Baoliu</forenames></author></authors><title>The Role of Depth, Width, and Tree Size in Expressiveness of Deep Forest</title><categories>cs.LG stat.ML</categories><journal-ref>Proceedings of the 27th European Conference on Artificial   Intelligence, pp. 2042-2049, Santiago de Compostela, Spain, 2024</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random forests are classical ensemble algorithms that construct multiple randomized decision trees and aggregate their predictions using naive averaging. \citet{zhou2019deep} further propose a deep forest algorithm with multi-layer forests, which outperforms random forests in various tasks. The performance of deep forests is related to three hyperparameters in practice: depth, width, and tree size, but little has been known about its theoretical explanation. This work provides the first upper and lower bounds on the approximation complexity of deep forests concerning the three hyperparameters. Our results confirm the distinctive role of depth, which can exponentially enhance the expressiveness of deep forests compared with width and tree size. Experiments confirm the theoretical findings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.05196</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.05196</id><created>2024-07-06</created><updated>2025-01-31</updated><authors><author><keyname>Madsen</keyname><forenames>Erik</forenames></author><author><keyname>Shmaya</keyname><forenames>Eran</forenames></author></authors><title>Collective Upkeep</title><categories>econ.TH cs.GT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We design mechanisms for maintaining public goods which require periodic in-kind contributions, motivated by incentives problems facing crowd-sourced recommender systems. Utilitarian welfare is maximized by redistributive policies which are infeasible when group members can leave or misreport their preferences. An optimal mechanism reduces contributions for group members with low benefit-cost ratios to encourage participation; and pairs reduced contributions with restricted access to the good to ensure truthful reporting. At most two membership tiers are offered at the optimum, indicating that ecommerce and digital content platforms may benefit substantially from offering simple user-adjustable recommendation settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.08138</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.08138</id><created>2024-07-10</created><updated>2025-01-31</updated><authors><author><keyname>Wei</keyname><forenames>Chenhao</forenames></author><author><keyname>Xiao</keyname><forenames>Lu</forenames></author><author><keyname>Yu</keyname><forenames>Tingting</forenames></author><author><keyname>Wong</keyname><forenames>Sunny</forenames></author><author><keyname>Clune</keyname><forenames>Abigail</forenames></author></authors><title>How Do Developers Structure Unit Test Cases? An Empirical Study from the   "AAA" Perspective</title><categories>cs.SE</categories><acm-class>D.2.5</acm-class><doi>10.1109/TSE.2025.3537337</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The AAA pattern, i.e. arrange, act, and assert, provides a unified structure for unit test cases, which benefits comprehension and maintenance. However, there is little understanding regarding whether and how common real-life developers structure unit test cases following AAA in practice. In particular, are there recurring anti-patterns that deviate from the AAA structure and merit refactoring? And, if test cases follow the AAA structure, could they contain design flaws in the A blocks? If we propose refactoring to fix the design of test cases following the AAA, how do developers receive the proposals? Do they favor refactoring? If not, what are their considerations? This study presents an empirical study on 435 real-life unit test cases randomly selected from four open-source projects. Overall, the majority (71.5%) of test cases follow the AAA structure. And, we observed three recurring anti-patterns that deviate from the AAA structure, as well as four design flaws that may reside inside of the A blocks. Each issue type has its drawbacks and merits corresponding refactoring resolutions. We sent a total of 18 refactoring proposals as issue tickets for fixing these problems. We received 78% positive feedback favoring the refactoring. From the rejections, we learned that return-on-investment is a key consideration for developers. The findings provide insights for practitioners to structure unit test cases with AAA in mind, and for researchers to develop related techniques for enforcing AAA in test cases. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.08907</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.08907</id><created>2024-07-11</created><updated>2025-02-01</updated><authors><author><keyname>Okawara</keyname><forenames>Taku</forenames></author><author><keyname>Koide</keyname><forenames>Kenji</forenames></author><author><keyname>Oishi</keyname><forenames>Shuji</forenames></author><author><keyname>Yokozuka</keyname><forenames>Masashi</forenames></author><author><keyname>Banno</keyname><forenames>Atsuhiko</forenames></author><author><keyname>Uno</keyname><forenames>Kentaro</forenames></author><author><keyname>Yoshida</keyname><forenames>Kazuya</forenames></author></authors><title>Tightly-Coupled LiDAR-IMU-Wheel Odometry with an Online Neural Kinematic   Model Learning via Factor Graph Optimization</title><categories>cs.RO</categories><comments>Accepted by the journal, Robotics and Autonomous Systems</comments><journal-ref>Robotics and Autonomous Systems, Vol. 187, pp. 104929, Jan, 2025</journal-ref><doi>10.1016/j.robot.2025.104929</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Environments lacking geometric features (e.g., tunnels and long straight corridors) are challenging for LiDAR-based odometry algorithms because LiDAR point clouds degenerate in such environments. For wheeled robots, a wheel kinematic model (i.e., wheel odometry) can improve the reliability of the odometry estimation. However, the kinematic model suffers from complex motions (e.g., wheel slippage, lateral movement) in the case of skid-steering robots particularly because this robot model rotates by skidding its wheels. Furthermore, these errors change nonlinearly when the wheel slippage is large (e.g., drifting) and are subject to terrain-dependent parameters. To simultaneously tackle point cloud degeneration and the kinematic model errors, we developed a LiDAR-IMU-wheel odometry algorithm incorporating online training of a neural network that learns the kinematic model of wheeled robots with nonlinearity. We propose to train the neural network online on a factor graph along with robot states, allowing the learning-based kinematic model to adapt to the current terrain condition. The proposed method jointly solves online training of the neural network and LiDAR-IMU-wheel odometry on a unified factor graph to retain the consistency of all those constraints. Through experiments, we first verified that the proposed network adapted to a changing environment, resulting in an accurate odometry estimation across different environments. We then confirmed that the proposed odometry estimation algorithm was robust against point cloud degeneration and nonlinearity (e.g., large wheel slippage by drifting) of the kinematic model. The summary video is available here: https://www.youtube.com/watch?v=CvRVhdda7Cw </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.11712</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.11712</id><created>2024-07-16</created><updated>2025-02-03</updated><authors><author><keyname>Liu</keyname><forenames>Xiaohao</forenames></author><author><keyname>Wu</keyname><forenames>Jie</forenames></author><author><keyname>Tao</keyname><forenames>Zhulin</forenames></author><author><keyname>Ma</keyname><forenames>Yunshan</forenames></author><author><keyname>Wei</keyname><forenames>Yinwei</forenames></author><author><keyname>Chua</keyname><forenames>Tat-seng</forenames></author></authors><title>Fine-tuning Multimodal Large Language Models for Product Bundling</title><categories>cs.IR</categories><comments>Accepted by KDD 2025 (CR)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in product bundling have leveraged multimodal information through sophisticated encoders, but remain constrained by limited semantic understanding and a narrow scope of knowledge. Therefore, some attempts employ In-context Learning (ICL) to explore the potential of large language models (LLMs) for their extensive knowledge and complex reasoning abilities. However, these efforts are inadequate in understanding mulitmodal data and exploiting LLMs' knowledge for product bundling. To bridge the gap, we introduce Bundle-MLLM, a novel framework that fine-tunes LLMs through a hybrid item tokenization approach within a well-designed optimization strategy. Specifically, we integrate textual, media, and relational data into a unified tokenization, introducing a soft separation token to distinguish between textual and non-textual tokens. Additionally, a streamlined yet powerful multimodal fusion module is employed to embed all non-textual features into a single, informative token, significantly boosting efficiency. To tailor product bundling tasks for LLMs, we reformulate the task as a multiple-choice question with candidate items as options. We further propose a progressive optimization strategy that fine-tunes LLMs for disentangled objectives: 1) learning bundle patterns and 2) enhancing multimodal semantic understanding specific to product bundling. Extensive experiments on four datasets across two domains demonstrate that our approach outperforms a range of state-of-the-art (SOTA) methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.13195</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.13195</id><created>2024-07-18</created><updated>2025-01-31</updated><authors><author><keyname>Li</keyname><forenames>Yingru</forenames></author><author><keyname>Xu</keyname><forenames>Jiawei</forenames></author><author><keyname>Wang</keyname><forenames>Baoxiang</forenames></author><author><keyname>Luo</keyname><forenames>Zhi-Quan</forenames></author></authors><title>Scalable Thompson Sampling via Ensemble++ Agent</title><categories>cs.LG cs.AI cs.HC cs.IT math.IT stat.ML</categories><comments>47 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Thompson Sampling is a principled method for balancing exploration and exploitation, but its real-world adoption is impeded by the high computational overhead of posterior maintenance in large-scale or non-conjugate settings. Ensemble-based approaches offer partial remedies, but often require a large ensemble size. This paper proposes the Ensemble++, a scalable agent that sidesteps these limitations by a shared-factor ensemble update architecture and a random linear combination scheme. We theoretically justify that in linear bandits, Ensemble++ agent only needs an ensemble size of $\Theta(d \log T)$ to achieve regret guarantees comparable to exact Thompson Sampling. Further, to handle nonlinear rewards and complex environments. we introduce a neural extension that replaces fixed features with a learnable representation, preserving the same underlying objective via gradient-based updates. Empirical results confirm that Ensemble++ agent excel in both sample efficiency and computational scalability across linear and nonlinear environments, including GPT-based contextual bandits. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.14727</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.14727</id><created>2024-07-19</created><updated>2025-02-01</updated><authors><author><keyname>Suzuki</keyname><forenames>Masahiro</forenames></author><author><keyname>Sakaji</keyname><forenames>Hiroki</forenames></author></authors><title>Economy Watchers Survey Provides Datasets and Tasks for Japanese   Financial Domain</title><categories>cs.CL cs.CE</categories><comments>Accepted to the ACM Web Conference 2025. 4 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Natural language processing (NLP) tasks in English and general domains are widely available and are often used to evaluate pre-trained language models. In contrast, fewer tasks are available for languages other than English and in the financial domain. Particularly, tasks in the Japanese and financial domains are limited. We develop two large datasets using data published by a Japanese central government agency. The datasets provide three Japanese financial NLP tasks, including 3- and 12-class classifications for categorizing sentences, along with a 5-class classification task for sentiment analysis. Our datasets are designed to be comprehensive and updated by leveraging an automatic update framework that ensures that the latest task datasets are publicly always available. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.15200</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.15200</id><created>2024-07-21</created><updated>2025-02-01</updated><authors><author><keyname>Kim</keyname><forenames>Tae-Geun</forenames></author></authors><title>HyperbolicLR: Epoch insensitive learning rate scheduler</title><categories>cs.LG cs.AI</categories><comments>21 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This study proposes two novel learning rate schedulers -- Hyperbolic Learning Rate Scheduler (HyperbolicLR) and Exponential Hyperbolic Learning Rate Scheduler (ExpHyperbolicLR) -- to address the epoch sensitivity problem that often causes inconsistent learning curves in conventional methods. By leveraging the asymptotic behavior of hyperbolic curves, the proposed schedulers maintain more stable learning curves across varying epoch settings. Specifically, HyperbolicLR applies this property directly in the epoch-learning rate space, while ExpHyperbolicLR extends it to an exponential space. We first determine optimal hyperparameters for each scheduler on a small number of epochs, fix these hyperparameters, and then evaluate performance as the number of epochs increases. Experimental results on various deep learning tasks (e.g., image classification, time series forecasting, and operator learning) demonstrate that both HyperbolicLR and ExpHyperbolicLR achieve more consistent performance improvements than conventional schedulers as training duration grows. These findings suggest that our hyperbolic-based schedulers offer a more robust and efficient approach to deep network optimization, particularly in scenarios constrained by computational resources or time. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.16212</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.16212</id><created>2024-07-23</created><authors><author><keyname>Huan</keyname><forenames>Xun</forenames></author><author><keyname>Jagalur</keyname><forenames>Jayanth</forenames></author><author><keyname>Marzouk</keyname><forenames>Youssef</forenames></author></authors><title>Optimal experimental design: Formulations and computations</title><categories>stat.ME cs.NA math.NA stat.CO</categories><comments>Appears in Acta Numerica 2024. This version contains an evolving set   of post-publication additions and corrections</comments><journal-ref>Acta Numerica 33 (2024) 715-840</journal-ref><doi>10.1017/S0962492924000023</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Questions of `how best to acquire data' are essential to modeling and prediction in the natural and social sciences, engineering applications, and beyond. Optimal experimental design (OED) formalizes these questions and creates computational methods to answer them. This article presents a systematic survey of modern OED, from its foundations in classical design theory to current research involving OED for complex models. We begin by reviewing criteria used to formulate an OED problem and thus to encode the goal of performing an experiment. We emphasize the flexibility of the Bayesian and decision-theoretic approach, which encompasses information-based criteria that are well-suited to nonlinear and non-Gaussian statistical models. We then discuss methods for estimating or bounding the values of these design criteria; this endeavor can be quite challenging due to strong nonlinearities, high parameter dimension, large per-sample costs, or settings where the model is implicit. A complementary set of computational issues involves optimization methods used to find a design; we discuss such methods in the discrete (combinatorial) setting of observation selection and in settings where an exact design can be continuously parameterized. Finally we present emerging methods for sequential OED that build non-myopic design policies, rather than explicit designs; these methods naturally adapt to the outcomes of past experiments in proposing new experiments, while seeking coordination among all experiments to be performed. Throughout, we highlight important open questions and challenges. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.16933</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.16933</id><created>2024-07-23</created><authors><author><keyname>Chen</keyname><forenames>Zhiyi</forenames></author><author><keyname>Maske</keyname><forenames>Harshal</forenames></author><author><keyname>Upadhyay</keyname><forenames>Devesh</forenames></author><author><keyname>Shui</keyname><forenames>Huanyi</forenames></author><author><keyname>Huan</keyname><forenames>Xun</forenames></author><author><keyname>Ni</keyname><forenames>Jun</forenames></author></authors><title>Deep Koopman-based Control of Quality Variation in Multistage   Manufacturing Systems</title><categories>eess.SY cs.LG cs.SY</categories><comments>The paper was in the proceeding of 2024 American Control Conference.   This submitted version addresses a minor correction to one equation (Eq. 14),   while the results and conclusions remain the same</comments><journal-ref>2024 American Control Conference (ACC) (2024) 893-898</journal-ref><doi>10.23919/ACC60939.2024.10644781</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper presents a modeling-control synthesis to address the quality control challenges in multistage manufacturing systems (MMSs). A new feedforward control scheme is developed to minimize the quality variations caused by process disturbances in MMSs. Notably, the control framework leverages a stochastic deep Koopman (SDK) model to capture the quality propagation mechanism in the MMSs, highlighted by its ability to transform the nonlinear propagation dynamics into a linear one. Two roll-to-roll case studies are presented to validate the proposed method and demonstrate its effectiveness. The overall method is suitable for nonlinear MMSs and does not require extensive expert knowledge. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.20761</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.20761</id><created>2024-07-30</created><updated>2025-02-02</updated><authors><author><keyname>Yao</keyname><forenames>Yongqiang</forenames></author><author><keyname>Tan</keyname><forenames>Jingru</forenames></author><author><keyname>Hu</keyname><forenames>Jiahao</forenames></author><author><keyname>Zhang</keyname><forenames>Feizhao</forenames></author><author><keyname>Jin</keyname><forenames>Xin</forenames></author><author><keyname>Li</keyname><forenames>Bo</forenames></author><author><keyname>Gong</keyname><forenames>Ruihao</forenames></author><author><keyname>Liu</keyname><forenames>Pengfei</forenames></author></authors><title>OmniBal: Towards Fast Instruct-tuning for Vision-Language Models via   Omniverse Computation Balance</title><categories>cs.AI</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Recently, vision-language instruct-tuning models have made significant progress due to their more comprehensive understanding of the world. In this work, we discovered that large-scale 3D parallel training on those models leads to an imbalanced computation load across different devices. The vision and language parts are inherently heterogeneous: their data distribution and model architecture differ significantly, which affects distributed training efficiency. We rebalanced the computational loads from data, model, and memory perspectives to address this issue, achieving more balanced computation across devices. These three components are not independent but are closely connected, forming an omniverse balanced training framework. Specifically, for the data, we grouped instances into new balanced mini-batches within and across devices. For the model, we employed a search-based method to achieve a more balanced partitioning. For memory optimization, we adaptively adjusted the re-computation strategy for each partition to utilize the available memory fully. We conducted extensive experiments to validate the effectiveness of our method. Compared with the open-source training code of InternVL-Chat, we significantly reduced GPU days, achieving about 1.8x speed-up. Our method's efficacy and generalizability were further demonstrated across various models and datasets. Codes will be released at https://github.com/ModelTC/OmniBal. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.02295</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.02295</id><created>2024-08-05</created><updated>2025-02-03</updated><authors><author><keyname>Kim</keyname><forenames>Seyeon</forenames></author><author><keyname>Lee</keyname><forenames>Joonhun</forenames></author><author><keyname>Cho</keyname><forenames>Namhoon</forenames></author><author><keyname>Han</keyname><forenames>Sungjun</forenames></author><author><keyname>Hwang</keyname><forenames>Wooseop</forenames></author></authors><title>Generalized Gaussian Temporal Difference Error for Uncertainty-aware   Reinforcement Learning</title><categories>cs.LG cs.AI math.PR stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Conventional uncertainty-aware temporal difference (TD) learning often assumes a zero-mean Gaussian distribution for TD errors, leading to inaccurate error representations and compromised uncertainty estimation. We introduce a novel framework for generalized Gaussian error modeling in deep reinforcement learning to enhance the flexibility of error distribution modeling by incorporating additional higher-order moment, particularly kurtosis, thereby improving the estimation and mitigation of data-dependent aleatoric uncertainty. We examine the influence of the shape parameter of the generalized Gaussian distribution (GGD) on aleatoric uncertainty and provide a closed-form expression that demonstrates an inverse relationship between uncertainty and the shape parameter. Additionally, we propose a theoretically grounded weighting scheme to address epistemic uncertainty by fully leveraging the GGD. We refine batch inverse variance weighting with bias reduction and kurtosis considerations, enhancing robustness. Experiments with policy gradient algorithms demonstrate significant performance gains. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.04197</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.04197</id><created>2024-08-07</created><updated>2025-02-02</updated><authors><author><keyname>Hong</keyname><forenames>Mengze</forenames></author><author><keyname>Jiang</keyname><forenames>Di</forenames></author><author><keyname>Ng</keyname><forenames>Wailing</forenames></author><author><keyname>Guo</keyname><forenames>Zichang</forenames></author><author><keyname>Zhang</keyname><forenames>Chen Jason</forenames></author></authors><title>Pairwise Judgment Formulation for Semantic Embedding Model in Web Search</title><categories>cs.IR cs.AI cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semantic Embedding Model (SEM), a neural network-based Siamese architecture, is gaining momentum in information retrieval and natural language processing. In order to train SEM in a supervised fashion for Web search, the search engine query log is typically utilized to automatically formulate pairwise judgments as training data. Despite the growing application of semantic embeddings in the search engine industry, little work has been done on formulating effective pairwise judgments for training SEM. In this paper, we make the first in-depth investigation of a wide range of strategies for generating pairwise judgments for SEM. An interesting (perhaps surprising) discovery reveals that the conventional pairwise judgment formulation strategy wildly used in the field of pairwise Learning-to-Rank (LTR) is not necessarily effective for training SEM. Through a large-scale empirical study based on query logs and click-through activities from a major commercial search engine, we demonstrate the effective strategies for SEM and highlight the advantages of a hybrid heuristic (i.e., Clicked &gt; Non-Clicked) in comparison to the atomic heuristics (e.g., Clicked &gt; Skipped) in LTR. We conclude with best practices for training SEM and offer promising insights for future research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.04713</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.04713</id><created>2024-08-08</created><updated>2025-02-03</updated><authors><author><keyname>Ding</keyname><forenames>Zifeng</forenames></author><author><keyname>Li</keyname><forenames>Yifeng</forenames></author><author><keyname>He</keyname><forenames>Yuan</forenames></author><author><keyname>Norelli</keyname><forenames>Antonio</forenames></author><author><keyname>Wu</keyname><forenames>Jingcheng</forenames></author><author><keyname>Tresp</keyname><forenames>Volker</forenames></author><author><keyname>Ma</keyname><forenames>Yunpu</forenames></author><author><keyname>Bronstein</keyname><forenames>Michael</forenames></author></authors><title>DyGMamba: Efficiently Modeling Long-Term Temporal Dependency on   Continuous-Time Dynamic Graphs with State Space Models</title><categories>cs.LG cs.AI</categories><comments>Preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning useful representations for continuous-time dynamic graphs (CTDGs) is challenging, due to the concurrent need to span long node interaction histories and grasp nuanced temporal details. In particular, two problems emerge: (1) Encoding longer histories requires more computational resources, making it crucial for CTDG models to maintain low computational complexity to ensure efficiency; (2) Meanwhile, more powerful models are needed to identify and select the most critical temporal information within the extended context provided by longer histories. To address these problems, we propose a CTDG representation learning model named DyGMamba, originating from the popular Mamba state space model (SSM). DyGMamba first leverages a node-level SSM to encode the sequence of historical node interactions. Another time-level SSM is then employed to exploit the temporal patterns hidden in the historical graph, where its output is used to dynamically select the critical information from the interaction history. We validate DyGMamba experimentally on the dynamic link prediction task. The results show that our model achieves state-of-the-art in most cases. DyGMamba also maintains high efficiency in terms of computational resources, making it possible to capture long temporal dependencies with a limited computation budget. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.06663</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.06663</id><created>2024-08-13</created><updated>2025-02-02</updated><authors><author><keyname>Sun</keyname><forenames>Kaiser</forenames></author><author><keyname>Dredze</keyname><forenames>Mark</forenames></author></authors><title>Amuro and Char: Analyzing the Relationship between Pre-Training and   Fine-Tuning of Large Language Models</title><categories>cs.CL cs.AI</categories><comments>Updated Draft</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The development of large language models leads to the formation of a pre-train-then-align paradigm, in which the model is typically pre-trained on a large text corpus and undergoes a tuning stage to align the model with human preference or downstream tasks. In this work, we investigate the relationship between pre-training and fine-tuning by fine-tuning multiple intermediate pre-trained model checkpoints. Our results on 18 datasets suggest that i) continual pre-training improves the model in a latent way that unveils after fine-tuning; ii) with extra fine-tuning, the datasets that the model does not demonstrate capability gain much more than those that the model performs well during the pre-training stage; iii) although model benefits significantly through supervised fine-tuning, it may forget previously known domain knowledge and the tasks that are not seen during fine-tuning; iv) the model resembles high sensitivity to evaluation prompts after supervised fine-tuning, but this sensitivity can be alleviated by more pre-training. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.08044</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.08044</id><created>2024-08-15</created><updated>2025-02-01</updated><authors><author><keyname>Wang</keyname><forenames>Zhenzhong</forenames></author><author><keyname>Hua</keyname><forenames>Haowei</forenames></author><author><keyname>Lin</keyname><forenames>Wanyu</forenames></author><author><keyname>Yang</keyname><forenames>Ming</forenames></author><author><keyname>Tan</keyname><forenames>Kay Chen</forenames></author></authors><title>Crystalline Material Discovery in the Era of Artificial Intelligence</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crystalline materials, with symmetrical and periodic structures, exhibit a wide spectrum of properties and have been widely used in numerous applications across electronics, energy, and beyond. For crystalline materials discovery, traditional experimental and computational approaches are time-consuming and expensive. In these years, thanks to the explosive amount of crystalline materials data, great interest has been given to data-driven materials discovery. Particularly, recent advancements have exploited the expressive representation ability of deep learning to model the highly complex atomic systems within crystalline materials, opening up new avenues for fast and accurate materials discovery. These works typically focus on four types of tasks, including physicochemical property prediction, crystalline material synthesis, aiding characterization, and accelerating theoretical computations. Despite the remarkable progress, there is still a lack of systematic investigation to summarize their distinctions and limitations. To fill this gap, we systematically investigated the progress made in recent years. We first introduce several data representations of the crystalline materials. Based on the representations, we summarize various fundamental deep learning models and their tailored usages in various material discovery tasks. Finally, we highlight the remaining challenges and propose future directions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.09594</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.09594</id><created>2024-08-18</created><updated>2025-02-02</updated><authors><author><keyname>Nie</keyname><forenames>Yuhe</forenames></author><author><keyname>Middleton</keyname><forenames>Michael</forenames></author><author><keyname>Merino</keyname><forenames>Tim</forenames></author><author><keyname>Kanagaraja</keyname><forenames>Nidhushan</forenames></author><author><keyname>Kumar</keyname><forenames>Ashutosh</forenames></author><author><keyname>Zhuang</keyname><forenames>Zhan</forenames></author><author><keyname>Togelius</keyname><forenames>Julian</forenames></author></authors><title>Moonshine: Distilling Game Content Generators into Steerable Generative   Models</title><categories>cs.AI</categories><acm-class>I.2.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Procedural Content Generation via Machine Learning (PCGML) has enhanced game content creation, yet challenges in controllability and limited training data persist. This study addresses these issues by distilling a constructive PCG algorithm into a controllable PCGML model. We first generate a large amount of content with a constructive algorithm and label it using a Large Language Model (LLM). We use these synthetic labels to condition two PCGML models for content-specific generation, a diffusion model and the five-dollar model. This neural network distillation process ensures that the generation aligns with the original algorithm while introducing controllability through plain text. We define this text-conditioned PCGML as a Text-to-game-Map (T2M) task, offering an alternative to prevalent text-to-image multi-modal tasks. We compare our distilled models with the baseline constructive algorithm. Our analysis of the variety, accuracy, and quality of our generation demonstrates the efficacy of distilling constructive methods into controllable text-conditioned PCGML models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.10360</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.10360</id><created>2024-08-19</created><updated>2025-02-02</updated><authors><author><keyname>Raiyan</keyname><forenames>Syed Rifat</forenames></author><author><keyname>Amio</keyname><forenames>Zibran Zarif</forenames></author><author><keyname>Ahmed</keyname><forenames>Sabbir</forenames></author></authors><title>HaSPeR: An Image Repository for Hand Shadow Puppet Recognition</title><categories>cs.CV cs.AI</categories><comments>Submitted to Visual Computing for Industry, Biomedicine, and Art, 13   pages, 105 figures, 2 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Hand shadow puppetry, also known as shadowgraphy or ombromanie, is a form of theatrical art and storytelling where hand shadows are projected onto flat surfaces to create illusions of living creatures. The skilled performers create these silhouettes by hand positioning, finger movements, and dexterous gestures to resemble shadows of animals and objects. Due to the lack of practitioners and a seismic shift in people's entertainment standards, this art form is on the verge of extinction. To facilitate its preservation and proliferate it to a wider audience, we introduce ${\rm H{\small A}SP{\small E}R}$, a novel dataset consisting of 15,000 images of hand shadow puppets across 15 classes extracted from both professional and amateur hand shadow puppeteer clips. We provide a detailed statistical analysis of the dataset and employ a range of pretrained image classification models to establish baselines. Our findings show a substantial performance superiority of skip-connected convolutional models over attention-based transformer architectures. We also find that lightweight models, such as MobileNetV2, suited for mobile applications and embedded devices, perform comparatively well. We surmise that such low-latency architectures can be useful in developing ombromanie teaching tools, and we create a prototype application to explore this surmission. Keeping the best-performing model ResNet34 under the limelight, we conduct comprehensive feature-spatial, explainability, and error analyses to gain insights into its decision-making process. To the best of our knowledge, this is the first documented dataset and research endeavor to preserve this dying art for future generations, with computer vision approaches. Our code and data will be publicly available. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.10373</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.10373</id><created>2024-08-19</created><updated>2025-02-01</updated><authors><author><keyname>Sampson</keyname><forenames>Corbit R.</forenames></author><author><keyname>Restrepo</keyname><forenames>Juan G.</forenames></author></authors><title>Competing Social Contagions with Opinion Dependent Infectivity</title><categories>physics.soc-ph cs.SI math.DS</categories><comments>12 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spread of disinformation (maliciously spread false information) in online social networks has become an important problem in today's society. Disinformation's spread is facilitated by the fact that individuals often accept false information based on cognitive biases which predispose them to believe information that they have heard repeatedly or that aligns with their beliefs. Moreover, disinformation often spreads in direct competition with a corresponding true information. To model these phenomena, we develop a model for two competing beliefs spreading on a social network, where individuals have an internal opinion that models their cognitive biases and modulates their likelihood of adopting one of the competing beliefs. By numerical simulations of an agent-based model and a mean-field description of the dynamics, we study how the long-term dynamics of the spreading process depends on the initial conditions for the number of spreaders and the initial opinion of the population. We find that the addition of cognitive biases enriches the transient dynamics of the spreading process, facilitating behavior such as the revival of a dying belief and the overturning of an initially widespread opinion. Finally, we study how external recruitment of spreaders can lead to the eventual dominance of one of the two beliefs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.11449</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.11449</id><created>2024-08-21</created><updated>2025-02-02</updated><authors><author><keyname>Zhang</keyname><forenames>Jia</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi</forenames></author><author><keyname>Guo</keyname><forenames>Lan-Zhe</forenames></author><author><keyname>Li</keyname><forenames>Yu-Feng</forenames></author></authors><title>Enabling Small Models for Zero-Shot Selection and Reuse through Model   Label Learning</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vision-language models (VLMs) like CLIP have demonstrated impressive zero-shot ability in image classification tasks by aligning text and images but suffer inferior performance compared with task-specific expert models. On the contrary, expert models excel in their specialized domains but lack zero-shot ability for new tasks. How to obtain both the high performance of expert models and zero-shot ability is an important research direction. In this paper, we attempt to demonstrate that by constructing a model hub and aligning models with their functionalities using model labels, new tasks can be solved in a zero-shot manner by effectively selecting and reusing models in the hub. We introduce a novel paradigm, Model Label Learning (MLL), which bridges the gap between models and their functionalities through a Semantic Directed Acyclic Graph (SDAG) and leverages an algorithm, Classification Head Combination Optimization (CHCO), to select capable models for new tasks. Compared with the foundation model paradigm, it is less costly and more scalable, i.e., the zero-shot ability grows with the sizes of the model hub. Experiments on seven real-world datasets validate the effectiveness and efficiency of MLL, demonstrating that expert models can be effectively reused for zero-shot tasks. Our code will be released publicly. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.12062</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.12062</id><created>2024-08-21</created><updated>2025-02-03</updated><authors><author><keyname>Li</keyname><forenames>Chongshou</forenames></author><author><keyname>Tang</keyname><forenames>Pin</forenames></author><author><keyname>Li</keyname><forenames>Xinke</forenames></author><author><keyname>Liu</keyname><forenames>Yuheng</forenames></author><author><keyname>Li</keyname><forenames>Tianrui</forenames></author></authors><title>Enhancing Sampling Protocol for Point Cloud Classification Against   Corruptions</title><categories>cs.CV cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Established sampling protocols for 3D point cloud learning, such as Farthest Point Sampling (FPS) and Fixed Sample Size (FSS), have long been relied upon. However, real-world data often suffer from corruptions, such as sensor noise, which violates the benign data assumption in current protocols. As a result, these protocols are highly vulnerable to noise, posing significant safety risks in critical applications like autonomous driving. To address these issues, we propose an enhanced point cloud sampling protocol, PointSP, designed to improve robustness against point cloud corruptions. PointSP incorporates key point reweighting to mitigate outlier sensitivity and ensure the selection of representative points. It also introduces a local-global balanced downsampling strategy, which allows for scalable and adaptive sampling while maintaining geometric consistency. Additionally, a lightweight tangent plane interpolation method is used to preserve local geometry while enhancing the density of the point cloud. Unlike learning-based approaches that require additional model training, PointSP is architecture-agnostic, requiring no extra learning or modification to the network. This enables seamless integration into existing pipelines. Extensive experiments on synthetic and real-world corrupted datasets show that PointSP significantly improves the robustness and accuracy of point cloud classification, outperforming state-of-the-art methods across multiple benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.14001</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.14001</id><created>2024-08-25</created><authors><author><keyname>Wang</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Xiong</keyname><forenames>Guojun</forenames></author><author><keyname>Cao</keyname><forenames>Houwei</forenames></author><author><keyname>Li</keyname><forenames>Jian</forenames></author><author><keyname>Liu</keyname><forenames>Yong</forenames></author></authors><title>Decentralized Federated Learning with Model Caching on Mobile Agents</title><categories>cs.LG cs.DC</categories><comments>27 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Federated Learning (FL) aims to train a shared model using data and computation power on distributed agents coordinated by a central server. Decentralized FL (DFL) utilizes local model exchange and aggregation between agents to reduce the communication and computation overheads on the central server. However, when agents are mobile, the communication opportunity between agents can be sporadic, largely hindering the convergence and accuracy of DFL. In this paper, we study delay-tolerant model spreading and aggregation enabled by model caching on mobile agents. Each agent stores not only its own model, but also models of agents encountered in the recent past. When two agents meet, they exchange their own models as well as the cached models. Local model aggregation works on all models in the cache. We theoretically analyze the convergence of DFL with cached models, explicitly taking into account the model staleness introduced by caching. We design and compare different model caching algorithms for different DFL and mobility scenarios. We conduct detailed case studies in a vehicular network to systematically investigate the interplay between agent mobility, cache staleness, and model convergence. In our experiments, cached DFL converges quickly, and significantly outperforms DFL without caching. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.14744</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.14744</id><created>2024-08-26</created><updated>2025-02-03</updated><authors><author><keyname>Ge</keyname><forenames>Junyao</forenames></author><author><keyname>Zhang</keyname><forenames>Xu</forenames></author><author><keyname>Zheng</keyname><forenames>Yang</forenames></author><author><keyname>Guo</keyname><forenames>Kaitai</forenames></author><author><keyname>Liang</keyname><forenames>Jimin</forenames></author></authors><title>RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with   Rich Linguistic Semantics from Openly Available Data and Large Language   Models</title><categories>cs.CV cs.AI</categories><comments>Submitted to ISPRS</comments><acm-class>I.4.8; I.2.10</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Abundant, well-annotated multimodal data in remote sensing are pivotal for aligning complex visual remote sensing (RS) scenes with human language, enabling the development of specialized vision language models across diverse RS interpretation tasks. However, annotating RS images with rich linguistic semantics at scale demands expertise in RS and substantial human labor, making it costly and often impractical. In this study, we propose a workflow that leverages large language models (LLMs) to generate multimodal datasets with semantically rich captions at scale from plain OpenStreetMap (OSM) data for images sourced from the Google Earth Engine (GEE) platform. This approach facilitates the generation of paired remote sensing data and can be readily scaled up using openly available data. Within this framework, we present RSTeller, a multimodal dataset comprising over 1.3 million RS images, each accompanied by two descriptive captions. Extensive experiments demonstrate that RSTeller enhances the performance of multiple existing vision language models for RS scene understanding through continual pre-training. Our methodology significantly reduces the manual effort and expertise needed for annotating remote sensing imagery while democratizing access to high-quality annotated data. This advancement fosters progress in visual language modeling and encourages broader participation in remote sensing research and applications. The RSTeller dataset is available at https://github.com/SlytherinGe/RSTeller. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.14873</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.14873</id><created>2024-08-27</created><updated>2024-09-17</updated><authors><author><keyname>Lou</keyname><forenames>Haozhe</forenames></author><author><keyname>Liu</keyname><forenames>Yurong</forenames></author><author><keyname>Pan</keyname><forenames>Yike</forenames></author><author><keyname>Geng</keyname><forenames>Yiran</forenames></author><author><keyname>Chen</keyname><forenames>Jianteng</forenames></author><author><keyname>Ma</keyname><forenames>Wenlong</forenames></author><author><keyname>Li</keyname><forenames>Chenglong</forenames></author><author><keyname>Wang</keyname><forenames>Lin</forenames></author><author><keyname>Feng</keyname><forenames>Hengzhen</forenames></author><author><keyname>Shi</keyname><forenames>Lu</forenames></author><author><keyname>Luo</keyname><forenames>Liyi</forenames></author><author><keyname>Shi</keyname><forenames>Yongliang</forenames></author></authors><title>Robo-GS: A Physics Consistent Spatial-Temporal Model for Robotic Arm   with Hybrid Representation</title><categories>cs.RO cs.NA math.NA math.OC</categories><journal-ref>ICRA 2025</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Real2Sim2Real plays a critical role in robotic arm control and reinforcement learning, yet bridging this gap remains a significant challenge due to the complex physical properties of robots and the objects they manipulate. Existing methods lack a comprehensive solution to accurately reconstruct real-world objects with spatial representations and their associated physics attributes.   We propose a Real2Sim pipeline with a hybrid representation model that integrates mesh geometry, 3D Gaussian kernels, and physics attributes to enhance the digital asset representation of robotic arms.   This hybrid representation is implemented through a Gaussian-Mesh-Pixel binding technique, which establishes an isomorphic mapping between mesh vertices and Gaussian models. This enables a fully differentiable rendering pipeline that can be optimized through numerical solvers, achieves high-fidelity rendering via Gaussian Splatting, and facilitates physically plausible simulation of the robotic arm's interaction with its environment using mesh-based methods.   The code,full presentation and datasets will be made publicly available at our website https://robostudioapp.com </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.16390</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.16390</id><created>2024-08-29</created><updated>2025-02-01</updated><authors><author><keyname>Li</keyname><forenames>Yunmeng</forenames></author><author><keyname>Suzuki</keyname><forenames>Jun</forenames></author><author><keyname>Morishita</keyname><forenames>Makoto</forenames></author><author><keyname>Abe</keyname><forenames>Kaori</forenames></author><author><keyname>Inui</keyname><forenames>Kentaro</forenames></author></authors><title>MQM-Chat: Multidimensional Quality Metrics for Chat Translation</title><categories>cs.CL</categories><journal-ref>https://aclanthology.org/2025.coling-main.221/</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The complexities of chats pose significant challenges for machine translation models. Recognizing the need for a precise evaluation metric to address the issues of chat translation, this study introduces Multidimensional Quality Metrics for Chat Translation (MQM-Chat). Through the experiments of five models using MQM-Chat, we observed that all models generated certain fundamental errors, while each of them has different shortcomings, such as omission, overly correcting ambiguous source content, and buzzword issues, resulting in the loss of stylized information. Our findings underscore the effectiveness of MQM-Chat in evaluating chat translation, emphasizing the importance of stylized content and dialogue consistency for future studies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.16883</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.16883</id><created>2024-08-29</created><updated>2025-02-03</updated><authors><author><keyname>Wesego</keyname><forenames>Daniel</forenames></author><author><keyname>Rooshenas</keyname><forenames>Pedram</forenames></author></authors><title>Multimodal ELBO with Diffusion Decoders</title><categories>cs.LG cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multimodal variational autoencoders have demonstrated their ability to learn the relationships between different modalities by mapping them into a latent representation. Their design and capacity to perform any-to-any conditional and unconditional generation make them appealing. However, different variants of multimodal VAEs often suffer from generating low-quality output, particularly when complex modalities such as images are involved. In addition to that, they frequently exhibit low coherence among the generated modalities when sampling from the joint distribution. To address these limitations, we propose a new variant of the multimodal VAE ELBO that incorporates a better decoder using a diffusion generative model. The diffusion decoder enables the model to learn complex modalities and generate high-quality outputs. The multimodal model can also seamlessly integrate with a standard feed-forward decoder for different types of modality, facilitating end-to-end training and inference. Furthermore, we introduce an auxiliary score-based model to enhance the unconditional generation capabilities of our proposed approach. This approach addresses the limitations imposed by conventional multimodal VAEs and opens up new possibilities to improve multimodal generation tasks. Our model provides state-of-the-art results compared to other multimodal VAEs in different datasets with higher coherence and superior quality in the generated modalities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.17380</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.17380</id><created>2024-08-30</created><updated>2025-02-02</updated><authors><author><keyname>Sheng</keyname><forenames>Zihao</forenames></author><author><keyname>Huang</keyname><forenames>Zilin</forenames></author><author><keyname>Chen</keyname><forenames>Sikai</forenames></author></authors><title>Traffic expertise meets residual RL: Knowledge-informed model-based   residual reinforcement learning for CAV trajectory control</title><categories>cs.AI cs.LG</categories><comments>Accepted by Communications in Transportation Research</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-based reinforcement learning (RL) is anticipated to exhibit higher sample efficiency compared to model-free RL by utilizing a virtual environment model. However, it is challenging to obtain sufficiently accurate representations of the environmental dynamics due to uncertainties in complex systems and environments. An inaccurate environment model may degrade the sample efficiency and performance of model-based RL. Furthermore, while model-based RL can improve sample efficiency, it often still requires substantial training time to learn from scratch, potentially limiting its advantages over model-free approaches. To address these challenges, this paper introduces a knowledge-informed model-based residual reinforcement learning framework aimed at enhancing learning efficiency by infusing established expert knowledge into the learning process and avoiding the issue of beginning from zero. Our approach integrates traffic expert knowledge into a virtual environment model, employing the Intelligent Driver Model (IDM) for basic dynamics and neural networks for residual dynamics, thus ensuring adaptability to complex scenarios. We propose a novel strategy that combines traditional control methods with residual RL, facilitating efficient learning and policy optimization without the need to learn from scratch. The proposed approach is applied to CAV trajectory control tasks for the dissipation of stop-and-go waves in mixed traffic flow. Experimental results demonstrate that our proposed approach enables the CAV agent to achieve superior performance in trajectory control compared to the baseline agents in terms of sample efficiency, traffic flow smoothness and traffic mobility. The source code and supplementary materials are available at: https://zihaosheng.github.io/traffic-expertise-RL/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.00101</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.00101</id><created>2024-08-27</created><updated>2025-02-02</updated><authors><author><keyname>Jiang</keyname><forenames>Wei-Bang</forenames></author><author><keyname>Wang</keyname><forenames>Yansen</forenames></author><author><keyname>Lu</keyname><forenames>Bao-Liang</forenames></author><author><keyname>Li</keyname><forenames>Dongsheng</forenames></author></authors><title>NeuroLM: A Universal Multi-task Foundation Model for Bridging the Gap   between Language and EEG Signals</title><categories>eess.SP cs.HC cs.LG</categories><comments>The Thirteenth International Conference on Learning Representations</comments><journal-ref>The Thirteenth International Conference on Learning   Representations, 2025</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advancements for large-scale pre-training with neural signals such as electroencephalogram (EEG) have shown promising results, significantly boosting the development of brain-computer interfaces (BCIs) and healthcare. However, these pre-trained models often require full fine-tuning on each downstream task to achieve substantial improvements, limiting their versatility and usability, and leading to considerable resource wastage. To tackle these challenges, we propose NeuroLM, the first multi-task foundation model that leverages the capabilities of Large Language Models (LLMs) by regarding EEG signals as a foreign language, endowing the model with multi-task learning and inference capabilities. Our approach begins with learning a text-aligned neural tokenizer through vector-quantized temporal-frequency prediction, which encodes EEG signals into discrete neural tokens. These EEG tokens, generated by the frozen vector-quantized (VQ) encoder, are then fed into an LLM that learns causal EEG information via multi-channel autoregression. Consequently, NeuroLM can understand both EEG and language modalities. Finally, multi-task instruction tuning adapts NeuroLM to various downstream tasks. We are the first to demonstrate that, by specific incorporation with LLMs, NeuroLM unifies diverse EEG tasks within a single model through instruction tuning. The largest variant NeuroLM-XL has record-breaking 1.7B parameters for EEG signal processing, and is pre-trained on a large-scale corpus comprising approximately 25,000-hour EEG data. When evaluated on six diverse downstream datasets, NeuroLM showcases the huge potential of this multi-task learning paradigm. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.01083</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.01083</id><created>2024-09-02</created><updated>2025-02-01</updated><authors><author><keyname>Zhang</keyname><forenames>Fan</forenames></author><author><keyname>Gienger</keyname><forenames>Michael</forenames></author></authors><title>Affordance-based Robot Manipulation with Flow Matching</title><categories>cs.RO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework for assistive robot manipulation, which focuses on two fundamental challenges: first, efficiently adapting large-scale models to downstream scene affordance understanding tasks, especially in daily living scenarios where gathering multi-task data involving humans requires strenuous effort; second, effectively learning robot action trajectories by grounding the visual affordance model. We tackle the first challenge by employing a parameter-efficient prompt tuning method that prepends learnable text prompts to the frozen vision model to predict manipulation affordances in multi-task scenarios. Then we propose to learn robot action trajectories guided by affordances in a supervised flow matching method. Flow matching represents a robot visuomotor policy as a conditional process of flowing random waypoints to desired robot action trajectories. Finally, we introduce a real-world dataset with 10 tasks across Activities of Daily Living to test our framework. Our extensive evaluation highlights that the proposed prompt tuning method for learning manipulation affordance achieves competitive performance and even outperforms some other finetuning protocols across data scales, while satisfying parameter efficiency. Learning multi-task robot action trajectories with flow matching leads to consistently favorable results in several robot manipulation benchmarks than some alternative behavior cloning methods. This includes more stable training and evaluation, and noticeably faster inference, while maintaining comparable generalization performance to diffusion policy, where flow matching performs marginally better in most cases. Our framework seamlessly unifies affordance learning and action generation with flow matching for robot manipulation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.01416</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.01416</id><created>2024-09-02</created><updated>2025-02-02</updated><authors><author><keyname>Jiang</keyname><forenames>Nan</forenames></author><author><keyname>Nasim</keyname><forenames>Md</forenames></author><author><keyname>Xue</keyname><forenames>Yexiang</forenames></author></authors><title>Active Symbolic Discovery of Ordinary Differential Equations via Phase   Portrait Sketching</title><categories>cs.LG cs.SC</categories><comments>Extended Version of the Paper Accepted at AAAI 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The symbolic discovery of Ordinary Differential Equations (ODEs) from trajectory data plays a pivotal role in AI-driven scientific discovery. Existing symbolic methods predominantly rely on fixed, pre-collected training datasets, which often result in suboptimal performance, as demonstrated in our case study in Figure 1. Drawing inspiration from active learning, we investigate strategies to query informative trajectory data that can enhance the evaluation of predicted ODEs. However, the butterfly effect in dynamical systems reveals that small variations in initial conditions can lead to drastically different trajectories, necessitating the storage of vast quantities of trajectory data using conventional active learning. To address this, we introduce Active Symbolic Discovery of Ordinary Differential Equations via Phase Portrait Sketching (APPS). Instead of directly selecting individual initial conditions, our APPS first identifies an informative region within the phase space and then samples a batch of initial conditions from this region. Compared to traditional active learning methods, APPS mitigates the gap of maintaining a large amount of data. Extensive experiments demonstrate that APPS consistently discovers more accurate ODE expressions than baseline methods using passively collected datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.01713</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.01713</id><created>2024-09-03</created><updated>2025-02-03</updated><authors><author><keyname>Knab</keyname><forenames>Patrick</forenames></author><author><keyname>Marton</keyname><forenames>Sascha</forenames></author><author><keyname>Bartelt</keyname><forenames>Christian</forenames></author><author><keyname>Fuder</keyname><forenames>Robert</forenames></author></authors><title>Interpreting Outliers in Time Series Data through Decoding Autoencoder</title><categories>cs.LG cs.AI</categories><comments>14 pages, 8 figures, accepted at TempXAI @ ECML-PKDD, published in   CEUR Workshop Proceedings, Vol. 3761. https://ceur-ws.org/Vol-3761/paper3.pdf</comments><journal-ref>Workshop on TempXAI, CEUR Workshop Proceedings, Vol. 3761, 2024</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Outlier detection is a crucial analytical tool in various fields. In critical systems like manufacturing, malfunctioning outlier detection can be costly and safety-critical. Therefore, there is a significant need for explainable artificial intelligence (XAI) when deploying opaque models in such environments. This study focuses on manufacturing time series data from a German automotive supply industry. We utilize autoencoders to compress the entire time series and then apply anomaly detection techniques to its latent features. For outlier interpretation, we (i) adopt widely used XAI techniques to the autoencoder's encoder. Additionally, (ii) we propose AEE, Aggregated Explanatory Ensemble, a novel approach that fuses explanations of multiple XAI techniques into a single, more expressive interpretation. For evaluation of explanations, (iii) we propose a technique to measure the quality of encoder explanations quantitatively. Furthermore, we qualitatively assess the effectiveness of outlier explanations with domain expertise. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.01763</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.01763</id><created>2024-09-03</created><updated>2025-02-02</updated><authors><author><keyname>Ta</keyname><forenames>Hoang-Thang</forenames></author><author><keyname>Thai</keyname><forenames>Duy-Quy</forenames></author><author><keyname>Rahman</keyname><forenames>Abu Bakar Siddiqur</forenames></author><author><keyname>Sidorov</keyname><forenames>Grigori</forenames></author><author><keyname>Gelbukh</keyname><forenames>Alexander</forenames></author></authors><title>FC-KAN: Function Combinations in Kolmogorov-Arnold Networks</title><categories>cs.LG cs.CL</categories><comments>17 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we introduce FC-KAN, a Kolmogorov-Arnold Network (KAN) that leverages combinations of popular mathematical functions such as B-splines, wavelets, and radial basis functions on low-dimensional data through element-wise operations. We explore several methods for combining the outputs of these functions, including sum, element-wise product, the addition of sum and element-wise product, representations of quadratic and cubic functions, concatenation, linear transformation of the concatenated output, and others. In our experiments, we compare FC-KAN with a multi-layer perceptron network (MLP) and other existing KANs, such as BSRBF-KAN, EfficientKAN, FastKAN, and FasterKAN, on the MNIST and Fashion-MNIST datasets. Two variants of FC-KAN, which use a combination of outputs from B-splines and Difference of Gaussians (DoG) and from B-splines and linear transformations in the form of a quadratic function, outperformed overall other models on the average of 5 independent training runs. We expect that FC-KAN can leverage function combinations to design future KANs. Our repository is publicly available at: https://github.com/hoangthangta/FC_KAN. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.04366</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.04366</id><created>2024-09-06</created><updated>2025-02-01</updated><authors><author><keyname>Heimbach</keyname><forenames>Lioba</forenames></author><author><keyname>Vonlanthen</keyname><forenames>Yann</forenames></author><author><keyname>Villacis</keyname><forenames>Juan</forenames></author><author><keyname>Kiffer</keyname><forenames>Lucianna</forenames></author><author><keyname>Wattenhofer</keyname><forenames>Roger</forenames></author></authors><title>Deanonymizing Ethereum Validators: The P2P Network Has a Privacy Issue</title><categories>cs.CR</categories><comments>This is the extended version of the paper, accepted at USENIX   Security 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many blockchain networks aim to preserve the anonymity of validators in the peer-to-peer (P2P) network, ensuring that no adversary can link a validator's identifier to the IP address of a peer due to associated privacy and security concerns. This work demonstrates that the Ethereum P2P network does not offer this anonymity. We present a methodology that enables any node in the network to identify validators hosted on connected peers and empirically verify the feasibility of our proposed method. Using data collected from four nodes over three days, we locate more than 15% of Ethereum validators in the P2P network. The insights gained from our deanonymization technique provide valuable information on the distribution of validators across peers, their geographic locations, and hosting organizations. We further discuss the implications and risks associated with the lack of anonymity in the P2P network and propose methods to help validators protect their privacy. The Ethereum Foundation has awarded us a bug bounty, acknowledging the impact of our results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.04637</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.04637</id><created>2024-09-06</created><authors><author><keyname>Li</keyname><forenames>Pingzhi</forenames></author><author><keyname>Chen</keyname><forenames>Tianlong</forenames></author><author><keyname>Liu</keyname><forenames>Junyu</forenames></author></authors><title>Enhancing Quantum Security over Federated Learning via Post-Quantum   Cryptography</title><categories>quant-ph cs.AI cs.CR cs.LG</categories><comments>Submission for IEEE 2024 IEEE Workshop on Quantum IntelLigence,   Learning &amp; Security (QUILLS), https://sites.google.com/pitt.edu/quills/home</comments><journal-ref>2024 IEEE 6th International Conference on Trust, Privacy and   Security in Intelligent Systems, and Applications (TPS-ISA) (pp. 499-505)</journal-ref><doi>10.1109/TPS-ISA62245.2024.00067</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Federated learning (FL) has become one of the standard approaches for deploying machine learning models on edge devices, where private training data are distributed across clients, and a shared model is learned by aggregating locally computed updates from each client. While this paradigm enhances communication efficiency by only requiring updates at the end of each training epoch, the transmitted model updates remain vulnerable to malicious tampering, posing risks to the integrity of the global model. Although current digital signature algorithms can protect these communicated model updates, they fail to ensure quantum security in the era of large-scale quantum computing. Fortunately, various post-quantum cryptography algorithms have been developed to address this vulnerability, especially the three NIST-standardized algorithms - Dilithium, FALCON, and SPHINCS+. In this work, we empirically investigate the impact of these three NIST-standardized PQC algorithms for digital signatures within the FL procedure, covering a wide range of models, tasks, and FL settings. Our results indicate that Dilithium stands out as the most efficient PQC algorithm for digital signature in federated learning. Additionally, we offer an in-depth discussion of the implications of our findings and potential directions for future research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.04691</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.04691</id><created>2024-09-06</created><updated>2025-01-31</updated><authors><author><keyname>Jin</keyname><forenames>Minhao</forenames></author><author><keyname>Apostolaki</keyname><forenames>Maria</forenames></author></authors><title>Robustifying ML-powered Network Classifiers with PANTS</title><categories>cs.CR cs.NI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Multiple network management tasks, from resource allocation to intrusion detection, rely on some form of ML-based network traffic classification (MNC). Despite their potential, MNCs are vulnerable to adversarial inputs, which can lead to outages, poor decision-making, and security violations, among other issues. The goal of this paper is to help network operators assess and enhance the robustness of their MNC against adversarial inputs. The most critical step for this is generating inputs that can fool the MNC while being realizable under various threat models. Compared to other ML models, finding adversarial inputs against MNCs is more challenging due to the existence of non-differentiable components e.g., traffic engineering and the need to constrain inputs to preserve semantics and ensure reliability. These factors prevent the direct use of well-established gradient-based methods developed in adversarial ML (AML). To address these challenges, we introduce PANTS, a practical white-box framework that uniquely integrates AML techniques with Satisfiability Modulo Theories (SMT) solvers to generate adversarial inputs for MNCs. We also embed PANTS into an iterative adversarial training process that enhances the robustness of MNCs against adversarial inputs. PANTS is 70% and 2x more likely in median to find adversarial inputs against target MNCs compared to state-of-the-art baselines, namely Amoeba and BAP. PANTS improves the robustness of the target MNCs by 52.7% (even against attackers outside of what is considered during robustification) without sacrificing their accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.05040</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.05040</id><created>2024-09-08</created><updated>2025-02-01</updated><authors><author><keyname>Wang</keyname><forenames>Jiazheng</forenames></author><author><keyname>Chen</keyname><forenames>Xiang</forenames></author><author><keyname>Zhang</keyname><forenames>Yuxi</forenames></author><author><keyname>Liu</keyname><forenames>Min</forenames></author><author><keyname>Wang</keyname><forenames>Yaonan</forenames></author><author><keyname>Zhang</keyname><forenames>Hang</forenames></author></authors><title>Unsupervised Multimodal 3D Medical Image Registration with Multilevel   Correlation Balanced Optimization</title><categories>cs.CV</categories><comments>Method description for MICCAI Learn2Reg 2024 challenge</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Surgical navigation based on multimodal image registration has played a significant role in providing intraoperative guidance to surgeons by showing the relative position of the target area to critical anatomical structures during surgery. However, due to the differences between multimodal images and intraoperative image deformation caused by tissue displacement and removal during surgery, effective registration of preoperative and intraoperative multimodal images faces significant challenges. To address the multimodal image registration challenges in Learn2Reg 2024, an unsupervised multimodal medical image registration method based on multilevel correlation balanced optimization (MCBO) is designed to solve these problems. First, the features of each modality are extracted based on the modality independent neighborhood descriptor, and the multimodal images are mapped to the feature space. Second, a multilevel pyramidal fusion optimization mechanism is designed to achieve global optimization and local detail complementation of the deformation field through dense correlation analysis and weight-balanced coupled convex optimization for input features at different scales. For preoperative medical images in different modalities, the alignment and stacking of valid information between different modalities is achieved by the maximum fusion between deformation fields. Our method focuses on the ReMIND2Reg task in Learn2Reg 2024, and to verify the generality of the method, we also tested it on the COMULIS3DCLEM task. Based on the results, our method achieved second place in the validation of both two tasks. The code is available at https://github.com/wjiazheng/MCBO. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.06216</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.06216</id><created>2024-09-10</created><updated>2025-02-01</updated><authors><author><keyname>Tsuji</keyname><forenames>Kohei</forenames></author><author><keyname>Hiraoka</keyname><forenames>Tatsuya</forenames></author><author><keyname>Cheng</keyname><forenames>Yuchang</forenames></author><author><keyname>Iwakura</keyname><forenames>Tomoya</forenames></author></authors><title>SubRegWeigh: Effective and Efficient Annotation Weighing with Subword   Regularization</title><categories>cs.CL</categories><comments>14 pages, 2 figures, 10 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  NLP datasets may still contain annotation errors, even when they are manually annotated. Researchers have attempted to develop methods to automatically reduce the adverse effect of errors in datasets. However, existing methods are time-consuming because they require many trained models to detect errors. This paper proposes a time-saving method that utilizes a tokenization technique called subword regularization to simulate multiple error detection models for detecting errors. Our proposed method, SubRegWeigh, can perform annotation weighting four to five times faster than the existing method. Additionally, SubRegWeigh improved performance in document classification and named entity recognition tasks. In experiments with pseudo-incorrect labels, SubRegWeigh clearly identifies pseudo-incorrect labels as annotation errors. Our code is available at https://github.com/4ldk/SubRegWeigh . </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.06513</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.06513</id><created>2024-09-10</created><updated>2025-02-01</updated><authors><author><keyname>Simionato</keyname><forenames>Riccardo</forenames></author><author><keyname>Fasciani</keyname><forenames>Stefano</forenames></author></authors><title>Sines, Transient, Noise Neural Modeling of Piano Notes</title><categories>cs.SD cs.AI eess.AS</categories><journal-ref>Frontiers in Signal Processing, volume 4, 2025</journal-ref><doi>10.3389/frsip.2024.1494864</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  This paper introduces a novel method for emulating piano sounds. We propose to exploit the sines, transient, and noise decomposition to design a differentiable spectral modeling synthesizer replicating piano notes. Three sub-modules learn these components from piano recordings and generate the corresponding harmonic, transient, and noise signals. Splitting the emulation into three independently trainable models reduces the modeling tasks' complexity. The quasi-harmonic content is produced using a differentiable sinusoidal model guided by physics-derived formulas, whose parameters are automatically estimated from audio recordings. The noise sub-module uses a learnable time-varying filter, and the transients are generated using a deep convolutional network. From singular notes, we emulate the coupling between different keys in trichords with a convolutional-based network. Results show the model matches the partial distribution of the target while predicting the energy in the higher part of the spectrum presents more challenges. The energy distribution in the spectra of the transient and noise components is accurate overall. While the model is more computationally and memory efficient, perceptual tests reveal limitations in accurately modeling the attack phase of notes. Despite this, it generally achieves perceptual accuracy in emulating single notes and trichords. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.07331</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.07331</id><created>2024-09-11</created><updated>2025-01-31</updated><authors><author><keyname>Weng</keyname><forenames>Weixi</forenames></author><author><keyname>Zhu</keyname><forenames>Jieming</forenames></author><author><keyname>Meng</keyname><forenames>Xiaojun</forenames></author><author><keyname>Zhang</keyname><forenames>Hao</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Yuan</keyname><forenames>Chun</forenames></author></authors><title>Learning to Compress Contexts for Efficient Knowledge-based Visual   Question Answering</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multimodal large language models (MLLMs) have demonstrated great performance on visual question answering (VQA). When it comes to knowledge-based Visual Question Answering (KB-VQA), MLLMs may lack the specialized domain knowledge needed to answer questions, necessitating the retrieval of necessary information from external knowledge sources. Previous works like Retrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input information, such as image-based textual descriptions and retrieved knowledge, as possible to improve performance, but they all overlook the issue that with the number of input tokens increasing, inference efficiency significantly decreases, which contradicts the demands of practical applications. To address this issue, we propose \textbf{R}etrieval-\textbf{A}ugmented MLLMs with Compressed Contexts (RACC). RACC learns to compress and aggregate retrieved knowledge for a given image-question pair, generating a compact modulation in the form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby achieving effective and efficient inference. RACC achieves a state-of-the-art (SOTA) performance of 63.92\% on OK-VQA. Moreover, it significantly reduces inference latency by 22.0\%-59.7\% compared to the prominent RAVQA-v2. Abundant experiments show RACC's broad applicability. It is compatible with various off-the-shelf MLLMs and can also handle different knowledge sources including textual and multimodal documents. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.07569</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.07569</id><created>2024-09-11</created><updated>2025-01-31</updated><authors><author><keyname>Liu</keyname><forenames>Guiliang</forenames></author><author><keyname>Xu</keyname><forenames>Sheng</forenames></author><author><keyname>Liu</keyname><forenames>Shicheng</forenames></author><author><keyname>Gaurav</keyname><forenames>Ashish</forenames></author><author><keyname>Subramanian</keyname><forenames>Sriram Ganapathi</forenames></author><author><keyname>Poupart</keyname><forenames>Pascal</forenames></author></authors><title>A Comprehensive Survey on Inverse Constrained Reinforcement Learning:   Definitions, Progress and Challenges</title><categories>cs.LG cs.AI</categories><comments>36 pages</comments><journal-ref>Transactions on Machine Learning Research, 2025</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Inverse Constrained Reinforcement Learning (ICRL) is the task of inferring the implicit constraints that expert agents adhere to, based on their demonstration data. As an emerging research topic, ICRL has received considerable attention in recent years. This article presents a categorical survey of the latest advances in ICRL. It serves as a comprehensive reference for machine learning researchers and practitioners, as well as starters seeking to comprehend the definitions, advancements, and important challenges in ICRL. We begin by formally defining the problem and outlining the algorithmic framework that facilitates constraint inference across various scenarios. These include deterministic or stochastic environments, environments with limited demonstrations, and multiple agents. For each context, we illustrate the critical challenges and introduce a series of fundamental methods to tackle these issues. This survey encompasses discrete, virtual, and realistic environments for evaluating ICRL agents. We also delve into the most pertinent applications of ICRL, such as autonomous driving, robot control, and sports analytics. To stimulate continuing research, we conclude the survey with a discussion of key unresolved questions in ICRL that can effectively foster a bridge between theoretical understanding and practical industrial applications. The papers referenced in this survey can be found at https://github.com/Jasonxu1225/Awesome-Constraint-Inference-in-RL. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.08245</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.08245</id><created>2024-09-12</created><updated>2025-02-02</updated><authors><author><keyname>Dangeti</keyname><forenames>Abhishek</forenames></author><author><keyname>Gajula</keyname><forenames>Pavan</forenames></author><author><keyname>Srivastava</keyname><forenames>Vivek</forenames></author><author><keyname>Jamwal</keyname><forenames>Vikram</forenames></author></authors><title>Style-based Clustering of Visual Artworks and the Play of Neural   Style-Representations</title><categories>cs.CV cs.LG</categories><comments>33 pages Changes from the previous version: Changes in the title and   abstract. Major updation in content and results: more style representations,   experiments, and analysis</comments><acm-class>I.4.8; I.5.3</acm-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Clustering artworks based on style can have many potential real-world applications like art recommendations, style-based search and retrieval, and the study of artistic style evolution of an artist or in an artwork corpus. We introduce and deliberate over the notion of 'Style-based clustering of visual artworks'. We argue that clustering artworks based on style is largely an unaddressed problem. We explore and devise different neural feature representations - from the style-classification, style-transfer to large language vision models - that can be then used for style-based clustering. Our objective is to assess the relative effectiveness of these devised style-based clustering approaches through qualitative and quantitative analysis by applying them to multiple artwork corpora and curated synthetically styled datasets. Besides providing a broad framework for style-based clustering and evaluation, our analysis provides some key novel insights on feature representations, architectures and implications for style-based clustering. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.08687</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.08687</id><created>2024-09-13</created><updated>2025-02-01</updated><authors><author><keyname>Niu</keyname><forenames>Haoyi</forenames></author><author><keyname>Chen</keyname><forenames>Qimao</forenames></author><author><keyname>Liu</keyname><forenames>Tenglong</forenames></author><author><keyname>Li</keyname><forenames>Jianxiong</forenames></author><author><keyname>Zhou</keyname><forenames>Guyue</forenames></author><author><keyname>Zhang</keyname><forenames>Yi</forenames></author><author><keyname>Hu</keyname><forenames>Jianming</forenames></author><author><keyname>Zhan</keyname><forenames>Xianyuan</forenames></author></authors><title>xTED: Cross-Domain Adaptation via Diffusion-Based Trajectory Editing</title><categories>cs.RO cs.LG</categories><comments>xTED offers a novel, generic, flexible, simple and effective paradigm   that casts cross-domain policy adaptation as a data pre-processing problem</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Reusing pre-collected data from different domains is an appealing solution for decision-making tasks, especially when data in the target domain are limited. Existing cross-domain policy transfer methods mostly aim at learning domain correspondences or corrections to facilitate policy learning, such as learning task/domain-specific discriminators, representations, or policies. This design philosophy often results in heavy model architectures or task/domain-specific modeling, lacking flexibility. This reality makes us wonder: can we directly bridge the domain gaps universally at the data level, instead of relying on complex downstream cross-domain policy transfer procedures? In this study, we propose the Cross-Domain Trajectory EDiting (xTED) framework that employs a specially designed diffusion model for cross-domain trajectory adaptation. Our proposed model architecture effectively captures the intricate dependencies among states, actions, and rewards, as well as the dynamics patterns within target data. Edited by adding noises and denoising with the pre-trained diffusion model, source domain trajectories can be transformed to align with target domain properties while preserving original semantic information. This process effectively corrects underlying domain gaps, enhancing state realism and dynamics reliability in source data, and allowing flexible integration with various single-domain and cross-domain downstream policy learning methods. Despite its simplicity, xTED demonstrates superior performance in extensive simulation and real-robot experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.09641</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.09641</id><created>2024-09-15</created><updated>2025-02-02</updated><authors><author><keyname>Choi</keyname><forenames>Dasom</forenames></author><author><keyname>Park</keyname><forenames>SoHyun</forenames></author><author><keyname>Lee</keyname><forenames>Kyungah</forenames></author><author><keyname>Hong</keyname><forenames>Hwajung</forenames></author><author><keyname>Kim</keyname><forenames>Young-Ho</forenames></author></authors><title>AACessTalk: Fostering Communication between Minimally Verbal Autistic   Children and Parents with Contextual Guidance and Card Recommendation</title><categories>cs.HC cs.AI</categories><comments>21 pages excluding reference. Accepted at ACM CHI 2025.   https://naver-ai.github.io/aacesstalk/</comments><acm-class>H.5.2; I.2.7</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  As minimally verbal autistic (MVA) children communicate with parents through few words and nonverbal cues, parents often struggle to encourage their children to express subtle emotions and needs and to grasp their nuanced signals. We present AACessTalk, a tablet-based, AI-mediated communication system that facilitates meaningful exchanges between an MVA child and a parent. AACessTalk provides real-time guides to the parent to engage the child in conversation and, in turn, recommends contextual vocabulary cards to the child. Through a two-week deployment study with 11 MVA child-parent dyads, we examine how AACessTalk fosters everyday conversation practice and mutual engagement. Our findings show high engagement from all dyads, leading to increased frequency of conversation and turn-taking. AACessTalk also encouraged parents to explore their own interaction strategies and empowered the children to have more agency in communication. We discuss the implications of designing technologies for balanced communication dynamics in parent-MVA child interaction. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.10016</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.10016</id><created>2024-09-16</created><updated>2025-02-01</updated><authors><author><keyname>Ji</keyname><forenames>Huawei</forenames></author><author><keyname>Deng</keyname><forenames>Cheng</forenames></author><author><keyname>Xue</keyname><forenames>Bo</forenames></author><author><keyname>Jin</keyname><forenames>Zhouyang</forenames></author><author><keyname>Ding</keyname><forenames>Jiaxin</forenames></author><author><keyname>Gan</keyname><forenames>Xiaoying</forenames></author><author><keyname>Fu</keyname><forenames>Luoyi</forenames></author><author><keyname>Wang</keyname><forenames>Xinbing</forenames></author><author><keyname>Zhou</keyname><forenames>Chenghu</forenames></author></authors><title>AceParse: A Comprehensive Dataset with Diverse Structured Texts for   Academic Literature Parsing</title><categories>cs.CL cs.AI</categories><comments>5 pages, 3 figures, 3 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  With the development of data-centric AI, the focus has shifted from model-driven approaches to improving data quality. Academic literature, as one of the crucial types, is predominantly stored in PDF formats and needs to be parsed into texts before further processing. However, parsing diverse structured texts in academic literature remains challenging due to the lack of datasets that cover various text structures. In this paper, we introduce AceParse, the first comprehensive dataset designed to support the parsing of a wide range of structured texts, including formulas, tables, lists, algorithms, and sentences with embedded mathematical expressions. Based on AceParse, we fine-tuned a multimodal model, named AceParser, which accurately parses various structured texts within academic literature. This model outperforms the previous state-of-the-art by 4.1% in terms of F1 score and by 5% in Jaccard Similarity, demonstrating the potential of multimodal models in academic literature parsing. Our dataset is available at https://github.com/JHW5981/AceParse. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.10027</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.10027</id><created>2024-09-16</created><updated>2025-02-02</updated><authors><author><keyname>Kim</keyname><forenames>Chan</forenames></author><author><keyname>Kim</keyname><forenames>Keonwoo</forenames></author><author><keyname>Oh</keyname><forenames>Mintaek</forenames></author><author><keyname>Baek</keyname><forenames>Hanbi</forenames></author><author><keyname>Lee</keyname><forenames>Jiyang</forenames></author><author><keyname>Jung</keyname><forenames>Donghwi</forenames></author><author><keyname>Woo</keyname><forenames>Soojin</forenames></author><author><keyname>Woo</keyname><forenames>Younkyung</forenames></author><author><keyname>Tucker</keyname><forenames>John</forenames></author><author><keyname>Firoozi</keyname><forenames>Roya</forenames></author><author><keyname>Seo</keyname><forenames>Seung-Woo</forenames></author><author><keyname>Schwager</keyname><forenames>Mac</forenames></author><author><keyname>Kim</keyname><forenames>Seong-Woo</forenames></author></authors><title>E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation   with Language Models</title><categories>cs.RO cs.AI</categories><comments>19 pages, 28 figures. Project page: https://e2map.github.io. Accepted   to ICRA 2025</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Large language models (LLMs) have shown significant potential in guiding embodied agents to execute language instructions across a range of tasks, including robotic manipulation and navigation. However, existing methods are primarily designed for static environments and do not leverage the agent's own experiences to refine its initial plans. Given that real-world environments are inherently stochastic, initial plans based solely on LLMs' general knowledge may fail to achieve their objectives, unlike in static scenarios. To address this limitation, this study introduces the Experience-and-Emotion Map (E2Map), which integrates not only LLM knowledge but also the agent's real-world experiences, drawing inspiration from human emotional responses. The proposed methodology enables one-shot behavior adjustments by updating the E2Map based on the agent's experiences. Our evaluation in stochastic navigation environments, including both simulations and real-world scenarios, demonstrates that the proposed method significantly enhances performance in stochastic environments compared to existing LLM-based approaches. Code and supplementary materials are available at https://e2map.github.io/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.11272</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.11272</id><created>2024-09-17</created><updated>2025-02-02</updated><authors><author><keyname>Srivastava</keyname><forenames>Nikit</forenames></author><author><keyname>Kuchelev</keyname><forenames>Denis</forenames></author><author><keyname>Ngoli</keyname><forenames>Tatiana Moteu</forenames></author><author><keyname>Shetty</keyname><forenames>Kshitij</forenames></author><author><keyname>Röder</keyname><forenames>Michael</forenames></author><author><keyname>Zahera</keyname><forenames>Hamada</forenames></author><author><keyname>Moussallem</keyname><forenames>Diego</forenames></author><author><keyname>Ngomo</keyname><forenames>Axel-Cyrille Ngonga</forenames></author></authors><title>LOLA -- An Open-Source Massively Multilingual Large Language Model</title><categories>cs.CL cs.AI cs.LG</categories><journal-ref>Proceedings of the 31st International Conference on Computational   Linguistics (COLING 2025), "LOLA - An Open-Source Massively Multilingual   Large Language Model", ACL Anthology,   https://aclanthology.org/2025.coling-main.428/</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents LOLA, a massively multilingual large language model trained on more than 160 languages using a sparse Mixture-of-Experts Transformer architecture. Our architectural and implementation choices address the challenge of harnessing linguistic diversity while maintaining efficiency and avoiding the common pitfalls of multilinguality. Our analysis of the evaluation results shows competitive performance in natural language generation and understanding tasks. Additionally, we demonstrate how the learned expert-routing mechanism exploits implicit phylogenetic linguistic patterns to potentially alleviate the curse of multilinguality. We provide an in-depth look at the training process, an analysis of the datasets, and a balanced exploration of the model's strengths and limitations. As an open-source model, LOLA promotes reproducibility and serves as a robust foundation for future research. Our findings enable the development of compute-efficient multilingual models with strong, scalable performance across languages. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.11672</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.11672</id><created>2024-09-17</created><updated>2024-12-13</updated><authors><author><keyname>Mukhopadhyay</keyname><forenames>Anirban</forenames></author><author><keyname>Luther</keyname><forenames>Kurt</forenames></author></authors><title>OSINT Clinic: Co-designing AI-Augmented Collaborative OSINT   Investigations for Vulnerability Assessment</title><categories>cs.HC</categories><journal-ref>In CHI Conference on Human Factors in Computing Systems (CHI 25),   April 26-May 1, 2025, Yokohama, Japan. ACM, New York, NY, USA, 22 pages</journal-ref><doi>10.1145/3706598.3713283</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Small businesses need vulnerability assessments to identify and mitigate cyber risks. Cybersecurity clinics provide a solution by offering students hands-on experience while delivering free vulnerability assessments to local organizations. To scale this model, we propose an Open Source Intelligence (OSINT) clinic where students conduct assessments using only publicly available data. We enhance the quality of investigations in the OSINT clinic by addressing the technical and collaborative challenges. Over the duration of the 2023-24 academic year, we conducted a three-phase co-design study with six students. Our study identified key challenges in the OSINT investigations and explored how generative AI could address these performance gaps. We developed design ideas for effective AI integration based on the use of AI probes and collaboration platform features. A pilot with three small businesses highlighted both the practical benefits of AI in streamlining investigations, and limitations, including privacy concerns and difficulty in monitoring progress. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.11709</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.11709</id><created>2024-09-18</created><updated>2025-02-03</updated><authors><author><keyname>Hu</keyname><forenames>Haodi</forenames></author><author><keyname>Liao</keyname><forenames>Xingjue</forenames></author><author><keyname>Du</keyname><forenames>Wuhao</forenames></author><author><keyname>Qian</keyname><forenames>Feifei</forenames></author></authors><title>Multi-robot connective collaboration toward collective obstacle field   traversal</title><categories>cs.RO cs.MA</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Environments with large terrain height variations present great challenges for legged robot locomotion. Drawing inspiration from fire ants' collective assembly behavior, we study strategies that can enable two ``connectable'' robots to collectively navigate over bumpy terrains with height variations larger than robot leg length. Each robot was designed to be extremely simple, with a cubical body and one rotary motor actuating four vertical peg legs that move in pairs. Two or more robots could physically connect to one another to enhance collective mobility. We performed locomotion experiments with a two-robot group, across an obstacle field filled with uniformly-distributed semi-spherical ``boulders''. Experimentally-measured robot speed suggested that the connection length between the robots has a significant effect on collective mobility: connection length C in [0.86, 0.9] robot unit body length (UBL) were able to produce sustainable movements across the obstacle field, whereas connection length C in [0.63, 0.84] and [0.92, 1.1] UBL resulted in low traversability. An energy landscape based model revealed the underlying mechanism of how connection length modulated collective mobility through the system's potential energy landscape, and informed adaptation strategies for the two-robot system to adapt their connection length for traversing obstacle fields with varying spatial frequencies. Our results demonstrated that by varying the connection configuration between the robots, the two-robot system could leverage mechanical intelligence to better utilize obstacle interaction forces and produce improved locomotion. Going forward, we envision that generalized principles of robot-environment coupling can inform design and control strategies for a large group of small robots to achieve ant-like collective environment negotiation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.12013</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.12013</id><created>2024-09-18</created><updated>2025-02-01</updated><authors><author><keyname>Gopalakrishnan</keyname><forenames>Akshay</forenames><affiliation>McGill University</affiliation></author><author><keyname>Verbrugge</keyname><forenames>Clark</forenames><affiliation>McGill University</affiliation></author><author><keyname>Batty</keyname><forenames>Mark</forenames><affiliation>University of Kent</affiliation></author></authors><title>Memory Consistency and Program Transformations</title><categories>cs.PL</categories><acm-class>D.3.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A memory consistency model specifies the allowed behaviors of shared memory concurrent programs. At the language level, these models are known to have a non-trivial impact on the safety of program optimizations, limiting the ability to rearrange/refactor code without introducing new behaviors. Existing programming language memory models try to address this by permitting more (relaxed/weak) concurrent behaviors but are still unable to allow all the desired optimizations. A core problem is that weaker consistency models may also render optimizations unsafe, a conclusion that goes against the intuition of them allowing more behaviors. This exposes an open problem of the compositional interaction between memory consistency semantics and optimizations: which parts of the semantics correspond to allowing/disallowing which set of optimizations is unclear. In this work, we establish a formal foundation suitable enough to understand this compositional nature, decomposing optimizations into a finite set of elementary effects on program execution traces, over which aspects of safety can be assessed. We use this decomposition to identify a desirable compositional property (complete) that would guarantee the safety of optimizations from one memory model to another. We showcase its practicality by proving such a property between Sequential Consistency (SC) and $SC_{RR}$, the latter allowing independent read-read reordering over $SC$. Our work potentially paves way to a new design methodology of programming-language memory models, one that places emphasis on the optimizations desired to be performed. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.12951</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.12951</id><created>2024-09-19</created><updated>2025-02-01</updated><authors><author><keyname>Gupta</keyname><forenames>Akshat</forenames></author><author><keyname>Ozdemir</keyname><forenames>Atahan</forenames></author><author><keyname>Anumanchipalli</keyname><forenames>Gopala</forenames></author></authors><title>Geometric Interpretation of Layer Normalization and a Comparative   Analysis with RMSNorm</title><categories>cs.LG cs.AI cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel geometric interpretation of LayerNorm and explores how LayerNorm influences the norm and orientation of hidden vectors in the representation space. With these geometric insights, we prepare the foundation for comparing LayerNorm with RMSNorm. We show that the definition of LayerNorm is innately linked to the uniform vector, defined as $\boldsymbol{1} = [1, 1, 1, 1, \cdots, 1]^T \in \mathbb{R}^d$. We then show that the standardization step in LayerNorm can be understood in three simple steps: (i) remove the component of a vector along the uniform vector, (ii) normalize the remaining vector, and (iii) scale the resultant vector by $\sqrt{d}$, where $d$ is the dimensionality of the representation space. We also provide additional insights into how LayerNorm operates at inference time. Finally, we compare the hidden representations of LayerNorm-based LLMs with models trained using RMSNorm and show that all LLMs naturally operate orthogonal to the uniform vector at inference time, that is, on average they do not have a component along the uniform vector during inference. This presents the first mechanistic evidence that removing the component along the uniform vector in LayerNorm is a redundant step. These results advocate for using RMSNorm over LayerNorm which is also more computationally efficient. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.13876</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.13876</id><created>2024-09-20</created><updated>2025-02-03</updated><authors><author><keyname>Hamelijnck</keyname><forenames>Oliver</forenames></author><author><keyname>Solin</keyname><forenames>Arno</forenames></author><author><keyname>Damoulas</keyname><forenames>Theodoros</forenames></author></authors><title>Physics-Informed Variational State-Space Gaussian Processes</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Differential equations are important mechanistic models that are integral to many scientific and engineering applications. With the abundance of available data there has been a growing interest in data-driven physics-informed models. Gaussian processes (GPs) are particularly suited to this task as they can model complex, non-linear phenomena whilst incorporating prior knowledge and quantifying uncertainty. Current approaches have found some success but are limited as they either achieve poor computational scalings or focus only on the temporal setting. This work addresses these issues by introducing a variational spatio-temporal state-space GP that handles linear and non-linear physical constraints while achieving efficient linear-in-time computation costs. We demonstrate our methods in a range of synthetic and real-world settings and outperform the current state-of-the-art in both predictive and computational performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.13979</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.13979</id><created>2024-09-20</created><updated>2025-02-03</updated><authors><author><keyname>Zhao</keyname><forenames>Jinman</forenames><affiliation>University of Toronto</affiliation></author><author><keyname>Qian</keyname><forenames>Zifan</forenames><affiliation>University of Alberta</affiliation></author><author><keyname>Cao</keyname><forenames>Linbo</forenames><affiliation>University of Waterloo</affiliation></author><author><keyname>Wang</keyname><forenames>Yining</forenames><affiliation>University of Toronto</affiliation></author><author><keyname>Ding</keyname><forenames>Yitian</forenames><affiliation>McGill University</affiliation></author><author><keyname>Hu</keyname><forenames>Yulan</forenames><affiliation>Renmin University of China</affiliation></author><author><keyname>Zhang</keyname><forenames>Zeyu</forenames><affiliation>The Australian National University</affiliation></author><author><keyname>Jin</keyname><forenames>Zeyong</forenames></author></authors><title>Role-Play Paradox in Large Language Models: Reasoning Performance Gains   and Ethical Dilemmas</title><categories>cs.CL</categories><comments>9 pages, 7 figures, 3 tables, submitted to CogSci 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Role-play in large language models (LLMs) enhances their ability to generate contextually relevant and high-quality responses by simulating diverse cognitive perspectives. However, our study identifies significant risks associated with this technique. First, we demonstrate that autotuning, a method used to auto-select models' roles based on the question, can lead to the generation of harmful outputs, even when the model is tasked with adopting neutral roles. Second, we investigate how different roles affect the likelihood of generating biased or harmful content. Through testing on benchmarks containing stereotypical and harmful questions, we find that role-play consistently amplifies the risk of biased outputs. Our results underscore the need for careful consideration of both role simulation and tuning processes when deploying LLMs in sensitive or high-stakes contexts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.14038</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.14038</id><created>2024-09-21</created><updated>2025-02-01</updated><authors><author><keyname>Qiang</keyname><forenames>Zhangcheng</forenames></author><author><keyname>Taylor</keyname><forenames>Kerry</forenames></author><author><keyname>Wang</keyname><forenames>Weiqing</forenames></author><author><keyname>Jiang</keyname><forenames>Jing</forenames></author></authors><title>OAEI-LLM: A Benchmark Dataset for Understanding Large Language Model   Hallucinations in Ontology Matching</title><categories>cs.AI cs.CL cs.IR</categories><comments>5 pages, 1 figure, 1 table, 1 code snippet</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hallucinations of large language models (LLMs) commonly occur in domain-specific downstream tasks, with no exception in ontology matching (OM). The prevalence of using LLMs for OM raises the need for benchmarks to better understand LLM hallucinations. The OAEI-LLM dataset is an extended version of the Ontology Alignment Evaluation Initiative (OAEI) datasets that evaluate LLM-specific hallucinations in OM tasks. We outline the methodology used in dataset construction and schema extension, and provide examples of potential use cases. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.14851</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.14851</id><created>2024-09-23</created><updated>2025-02-03</updated><authors><author><keyname>Baykal</keyname><forenames>Gulcin</forenames></author><author><keyname>Kandemir</keyname><forenames>Melih</forenames></author><author><keyname>Unal</keyname><forenames>Gozde</forenames></author></authors><title>Disentanglement with Factor Quantized Variational Autoencoders</title><categories>cs.CV cs.LG</categories><comments>Preprint submitted to Neurocomputing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Disentangled representation learning aims to represent the underlying generative factors of a dataset in a latent representation independently of one another. In our work, we propose a discrete variational autoencoder (VAE) based model where the ground truth information about the generative factors are not provided to the model. We demonstrate the advantages of learning discrete representations over learning continuous representations in facilitating disentanglement. Furthermore, we propose incorporating an inductive bias into the model to further enhance disentanglement. Precisely, we propose scalar quantization of the latent variables in a latent representation with scalar values from a global codebook, and we add a total correlation term to the optimization as an inductive bias. Our method called FactorQVAE combines optimization based disentanglement approaches with discrete representation learning, and it outperforms the former disentanglement methods in terms of two disentanglement metrics (DCI and InfoMEC) while improving the reconstruction performance. Our code can be found at https://github.com/ituvisionlab/FactorQVAE. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.16001</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.16001</id><created>2024-09-24</created><updated>2025-02-02</updated><authors><author><keyname>Arslan</keyname><forenames>Suayb S.</forenames></author></authors><title>Artificial Human Intelligence: The role of Humans in the Development of   Next Generation AI</title><categories>cs.AI q-bio.NC</categories><comments>26 pages, 8 figures, submitted to IEEE Trans. on NNLS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human intelligence, the most evident and accessible form of source of reasoning, hosted by biological hardware, has evolved and been refined over thousands of years, positioning itself today to create new artificial forms and preparing to self--design their evolutionary path forward. Beginning with the advent of foundation models, the rate at which human and artificial intelligence interact with each other has exceeded any anticipated quantitative figures. The close engagement led both bits of intelligence to be impacted in various ways, which naturally resulted in complex confluences that warrant close scrutiny. In the sequel, using a novel taxonomy, we shall explore the interplay between human and machine intelligence, focusing on the crucial role humans play in developing ethical, responsible, and robust intelligent systems. We briefly delve into various aspects of implementation inspired by the mechanisms underlying neuroscience and human cognition. In addition, we propose future perspectives, capitalizing on the advantages of symbiotic designs to suggest a human-centered direction for next-generation developments, focusing on the augmentation role of AI. We finalize this evolving document with some thoughts and open questions yet to be addressed by the broader community. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.16126</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.16126</id><created>2024-09-24</created><updated>2025-02-03</updated><authors><author><keyname>Singh</keyname><forenames>Alakhsimar</forenames></author><author><keyname>Verma</keyname><forenames>Nischay</forenames></author><author><keyname>Goyal</keyname><forenames>Kanav</forenames></author><author><keyname>Singh</keyname><forenames>Amritpal</forenames></author><author><keyname>Kumar</keyname><forenames>Puneet</forenames></author><author><keyname>Li</keyname><forenames>Xiaobai</forenames></author></authors><title>VisioPhysioENet: Multimodal Engagement Detection using Visual and   Physiological Signals</title><categories>cs.CV</categories><comments>8 Pages, 2 figures, 5 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents VisioPhysioENet, a novel multimodal system that leverages visual and physiological signals to detect learner engagement. It employs a two-level approach for extracting both visual and physiological features. For visual feature extraction, Dlib is used to detect facial landmarks, while OpenCV provides additional estimations. The face recognition library, built on Dlib, is used to identify the facial region of interest specifically for physiological signal extraction. Physiological signals are then extracted using the plane-orthogonal-toskin method to assess cardiovascular activity. These features are integrated using advanced machine learning classifiers, enhancing the detection of various levels of engagement. We thoroughly tested VisioPhysioENet on the DAiSEE dataset. It achieved an accuracy of 63.09%. This shows it can better identify different levels of engagement compared to many existing methods. It performed 8.6% better than the only other model that uses both physiological and visual features. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.16452</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.16452</id><created>2024-09-24</created><updated>2025-02-02</updated><authors><author><keyname>Liu</keyname><forenames>Zhiwei</forenames></author><author><keyname>Zhang</keyname><forenames>Xin</forenames></author><author><keyname>Yang</keyname><forenames>Kailai</forenames></author><author><keyname>Xie</keyname><forenames>Qianqian</forenames></author><author><keyname>Huang</keyname><forenames>Jimin</forenames></author><author><keyname>Ananiadou</keyname><forenames>Sophia</forenames></author></authors><title>FMDLlama: Financial Misinformation Detection based on Large Language   Models</title><categories>cs.CL</categories><comments>Accepted by The Web Conference (WWW) 2025 Short Paper Track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of social media has made the spread of misinformation easier. In the financial domain, the accuracy of information is crucial for various aspects of financial market, which has made financial misinformation detection (FMD) an urgent problem that needs to be addressed. Large language models (LLMs) have demonstrated outstanding performance in various fields. However, current studies mostly rely on traditional methods and have not explored the application of LLMs in the field of FMD. The main reason is the lack of FMD instruction tuning datasets and evaluation benchmarks. In this paper, we propose FMDLlama, the first open-sourced instruction-following LLMs for FMD task based on fine-tuning Llama3.1 with instruction data, the first multi-task FMD instruction dataset (FMDID) to support LLM instruction tuning, and a comprehensive FMD evaluation benchmark (FMD-B) with classification and explanation generation tasks to test the FMD ability of LLMs. We compare our models with a variety of LLMs on FMD-B, where our model outperforms other open-sourced LLMs as well as OpenAI's products. This project is available at https://github.com/lzw108/FMD. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.17317</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.17317</id><created>2024-09-25</created><updated>2025-02-01</updated><authors><author><keyname>Bao</keyname><forenames>Ning</forenames></author><author><keyname>Furuya</keyname><forenames>Keiichiro</forenames></author><author><keyname>Naskar</keyname><forenames>Joydeep</forenames></author></authors><title>Towards a complete classification of holographic entropy inequalities</title><categories>hep-th cs.DM quant-ph</categories><comments>v1 23 pages, 4 figures. v2 typos fixed. v3 Appendix B added, more   explanations in the text</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a deterministic method to find all holographic entropy inequalities that have corresponding contraction maps and argue the completeness of our method. We use a triality between holographic entropy inequalities, contraction maps and partial cubes. More specifically, the validity of a holographic entropy inequality is implied by the existence of a contraction map, which we prove to be equivalent to finding an isometric embedding of a contracted graph. Thus, by virtue of the argued completeness of the contraction map proof method, the problem of finding all holographic entropy inequalities is equivalent to the problem of finding all contraction maps, which we translate to a problem of finding all image graph partial cubes. We give an algorithmic solution to this problem and characterize the complexity of our method. We also demonstrate interesting by-products, most notably, a procedure to generate candidate quantum entropy inequalities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.18125</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.18125</id><created>2024-09-26</created><updated>2025-02-01</updated><authors><author><keyname>Zhu</keyname><forenames>Chenming</forenames></author><author><keyname>Wang</keyname><forenames>Tai</forenames></author><author><keyname>Zhang</keyname><forenames>Wenwei</forenames></author><author><keyname>Pang</keyname><forenames>Jiangmiao</forenames></author><author><keyname>Liu</keyname><forenames>Xihui</forenames></author></authors><title>LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with   3D-awareness</title><categories>cs.CV</categories><comments>Project page: https://zcmax.github.io/projects/LLaVA-3D/</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent advancements in Large Multimodal Models (LMMs) have greatly enhanced their proficiency in 2D visual understanding tasks, enabling them to effectively process and understand images and videos. However, the development of LMMs with 3D-awareness for 3D scene understanding has been hindered by the lack of large-scale 3D vision-language datasets and powerful 3D encoders. In this paper, we introduce a simple yet effective framework called LLaVA-3D. Leveraging the strong 2D understanding priors from LLaVA, our LLaVA-3D efficiently adapts LLaVA for 3D scene understanding without compromising 2D understanding capabilities. To achieve this, we utilize the 3D position embeddings to bring the 2D CLIP patches within a 3D spatial context. By integrating the 3D position embeddings into 2D LMMs and employing joint 2D and 3D vision-language instruction tuning, we establish a unified architecture for both 2D image understanding and 3D scene understanding. Experimental results show that LLaVA-3D converges 3.5x faster than existing 3D LMMs when trained on 3D vision-language datasets. Moreover, LLaVA-3D not only achieves state-of-the-art performance across various 3D tasks but also maintains comparable 2D image understanding and vision-language conversation capabilities with LLaVA. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.18359</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.18359</id><created>2024-09-26</created><updated>2025-02-02</updated><authors><author><keyname>Molinaro</keyname><forenames>Roberto</forenames></author><author><keyname>Lanthaler</keyname><forenames>Samuel</forenames></author><author><keyname>Raonić</keyname><forenames>Bogdan</forenames></author><author><keyname>Rohner</keyname><forenames>Tobias</forenames></author><author><keyname>Armegioiu</keyname><forenames>Victor</forenames></author><author><keyname>Simonis</keyname><forenames>Stephan</forenames></author><author><keyname>Grund</keyname><forenames>Dana</forenames></author><author><keyname>Ramic</keyname><forenames>Yannick</forenames></author><author><keyname>Wan</keyname><forenames>Zhong Yi</forenames></author><author><keyname>Sha</keyname><forenames>Fei</forenames></author><author><keyname>Mishra</keyname><forenames>Siddhartha</forenames></author><author><keyname>Zepeda-Núñez</keyname><forenames>Leonardo</forenames></author></authors><title>Generative AI for fast and accurate statistical computation of fluids</title><categories>cs.LG cs.NA math.NA physics.flu-dyn</categories><comments>120 pages, 33 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a generative AI algorithm for addressing the pressing task of fast, accurate, and robust statistical computation of three-dimensional turbulent fluid flows. Our algorithm, termed as GenCFD, is based on an end-to-end conditional score-based diffusion model. Through extensive numerical experimentation with a set of challenging fluid flows, we demonstrate that GenCFD provides an accurate approximation of relevant statistical quantities of interest while also efficiently generating high-quality realistic samples of turbulent fluid flows and ensuring excellent spectral resolution. In contrast, ensembles of deterministic ML algorithms, trained to minimize mean square errors, regress to the mean flow. We present rigorous theoretical results uncovering the surprising mechanisms through which diffusion models accurately generate fluid flows. These mechanisms are illustrated with solvable toy models that exhibit the mathematically relevant features of turbulent fluid flows while being amenable to explicit analytical formulae. Our codes are publicly available at https://github.com/camlab-ethz/GenCFD. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.19005</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.19005</id><created>2024-09-21</created><updated>2025-02-01</updated><authors><author><keyname>Abdelrahman</keyname><forenames>Mahmoud</forenames></author><author><keyname>Macatulad</keyname><forenames>Edgardo</forenames></author><author><keyname>Lei</keyname><forenames>Binyu</forenames></author><author><keyname>Quintana</keyname><forenames>Matias</forenames></author><author><keyname>Miller</keyname><forenames>Clayton</forenames></author><author><keyname>Biljecki</keyname><forenames>Filip</forenames></author></authors><title>What is a Digital Twin Anyway? Deriving the Definition for the Built   Environment from over 15,000 Scientific Publications</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of digital twins has attracted significant attention across various domains, particularly within the built environment. However, there is a sheer volume of definitions and the terminological consensus remains out of reach. The lack of a universally accepted definition leads to ambiguities in their conceptualization and implementation, and may cause miscommunication for both researchers and practitioners. We employed Natural Language Processing (NLP) techniques to systematically extract and analyze definitions of digital twins from a corpus of more than 15,000 full-text articles spanning diverse disciplines. The study compares these findings with insights from an expert survey that included 52 experts. The study identifies concurrence on the components that comprise a ``Digital Twin'' from a practical perspective across various domains, contrasting them with those that do not, to identify deviations. We investigate the evolution of digital twin definitions over time and across different scales, including manufacturing, building, and urban/geospatial perspectives. We extracted the main components of Digital Twins using Text Frequency Analysis and N-gram analysis. Subsequently, we identified components that appeared in the literature and conducted a Chi-square test to assess the significance of each component in different domains. Our analysis identified key components of digital twins and revealed significant variations in definitions based on application domains, such as manufacturing, building, and urban contexts. The analysis of DT components reveal two major groups of DT types: High-Performance Real-Time (HPRT) DTs, and Long-Term Decision Support (LTDS) DTs. Contrary to common assumptions, we found that components such as simulation, AI/ML, real-time capabilities, and bi-directional data flow are not yet fully mature in the digital twins of the built environment. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.19499</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.19499</id><created>2024-09-28</created><updated>2025-02-01</updated><authors><author><keyname>Zhaxizhuoma</keyname></author><author><keyname>Liu</keyname><forenames>Kehui</forenames></author><author><keyname>Guan</keyname><forenames>Chuyue</forenames></author><author><keyname>Jia</keyname><forenames>Zhongjie</forenames></author><author><keyname>Wu</keyname><forenames>Ziniu</forenames></author><author><keyname>Liu</keyname><forenames>Xin</forenames></author><author><keyname>Wang</keyname><forenames>Tianyu</forenames></author><author><keyname>Liang</keyname><forenames>Shuai</forenames></author><author><keyname>Chen</keyname><forenames>Pengan</forenames></author><author><keyname>Zhang</keyname><forenames>Pingrui</forenames></author><author><keyname>Song</keyname><forenames>Haoming</forenames></author><author><keyname>Qu</keyname><forenames>Delin</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author><author><keyname>Wang</keyname><forenames>Zhigang</forenames></author><author><keyname>Cao</keyname><forenames>Nieqing</forenames></author><author><keyname>Ding</keyname><forenames>Yan</forenames></author><author><keyname>Zhao</keyname><forenames>Bin</forenames></author><author><keyname>Li</keyname><forenames>Xuelong</forenames></author></authors><title>FastUMI: A Scalable and Hardware-Independent Universal Manipulation   Interface with Dataset</title><categories>cs.RO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Real-world manipulation data involving robotic arms is crucial for developing generalist action policies, yet such data remains scarce since existing data collection methods are hindered by high costs, hardware dependencies, and complex setup requirements. In this work, we introduce FastUMI, a substantial redesign of the Universal Manipulation Interface (UMI) system that addresses these challenges by enabling rapid deployment, simplifying hardware-software integration, and delivering robust performance in real-world data acquisition. Compared with UMI, FastUMI has several advantages: 1) It adopts a decoupled hardware design and incorporates extensive mechanical modifications, removing dependencies on specialized robotic components while preserving consistent observation perspectives. 2) It also refines the algorithmic pipeline by replacing complex Visual-Inertial Odometry (VIO) implementations with an off-the-shelf tracking module, significantly reducing deployment complexity while maintaining accuracy. 3) FastUMI includes an ecosystem for data collection, verification, and integration with both established and newly developed imitation learning algorithms, accelerating policy learning advancement. Additionally, we have open-sourced a high-quality dataset of over 10,000 real-world demonstration trajectories spanning 22 everyday tasks, forming one of the most diverse UMI-like datasets to date. Experimental results confirm that FastUMI facilitates rapid deployment, reduces operational costs and labor demands, and maintains robust performance across diverse manipulation scenarios, thereby advancing scalable data-driven robotic learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.19605</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.19605</id><created>2024-09-29</created><updated>2025-02-02</updated><authors><author><keyname>Shi</keyname><forenames>Ruizhe</forenames></author><author><keyname>Zhou</keyname><forenames>Runlong</forenames></author><author><keyname>Du</keyname><forenames>Simon S.</forenames></author></authors><title>The Crucial Role of Samplers in Online Direct Preference Optimization</title><categories>cs.LG cs.CL</categories><comments>ICLR accepted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Direct Preference Optimization (DPO) has emerged as a stable, scalable, and efficient solution for language model alignment. Despite its empirical success, the optimization properties, particularly the impact of samplers on its convergence rates, remain under-explored. In this paper, we provide a rigorous analysis of DPO's convergence rates with different sampling strategies under the exact gradient setting, revealing a surprising separation: uniform sampling achieves $\textbf{linear}$ convergence, while our proposed online sampler achieves $\textbf{quadratic}$ convergence. We further adapt the sampler to practical settings by incorporating posterior distributions and logit mixing, demonstrating improvements over previous methods. For example, it outperforms vanilla DPO by over $7.4$% on Safe-RLHF dataset. Our results not only offer insights into the theoretical understanding of DPO but also pave the way for further algorithm designs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.19949</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.19949</id><created>2024-09-30</created><updated>2025-02-02</updated><authors><author><keyname>Fan</keyname><forenames>Chenyou</forenames></author><author><keyname>Bai</keyname><forenames>Chenjia</forenames></author><author><keyname>Shan</keyname><forenames>Zhao</forenames></author><author><keyname>He</keyname><forenames>Haoran</forenames></author><author><keyname>Zhang</keyname><forenames>Yang</forenames></author><author><keyname>Wang</keyname><forenames>Zhen</forenames></author></authors><title>Task-agnostic Pre-training and Task-guided Fine-tuning for Versatile   Diffusion Planner</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diffusion models have demonstrated their capabilities in modeling trajectories of multi-tasks. However, existing multi-task planners or policies typically rely on task-specific demonstrations via multi-task imitation, or require task-specific reward labels to facilitate policy optimization via Reinforcement Learning (RL). They are costly due to the substantial human efforts required to collect expert data or design reward functions. To address these challenges, we aim to develop a versatile diffusion planner capable of leveraging large-scale inferior data that contains task-agnostic sub-optimal trajectories, with the ability to fast adapt to specific tasks. In this paper, we propose SODP, a two-stage framework that leverages Sub-Optimal data to learn a Diffusion Planner, which is generalizable for various downstream tasks. Specifically, in the pre-training stage, we train a foundation diffusion planner that extracts general planning capabilities by modeling the versatile distribution of multi-task trajectories, which can be sub-optimal and has wide data coverage. Then for downstream tasks, we adopt RL-based fine-tuning with task-specific rewards to quickly refine the diffusion planner, which aims to generate action sequences with higher task-specific returns. Experimental results from multi-task domains including Meta-World and Adroit demonstrate that SODP outperforms state-of-the-art methods with only a small amount of data for reward-guided fine-tuning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.00418</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.00418</id><created>2024-10-01</created><updated>2025-02-03</updated><authors><author><keyname>Ohayon</keyname><forenames>Guy</forenames></author><author><keyname>Michaeli</keyname><forenames>Tomer</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author></authors><title>Posterior-Mean Rectified Flow: Towards Minimum MSE Photo-Realistic Image   Restoration</title><categories>eess.IV cs.AI cs.CV eess.SP</categories><comments>Accepted to ICLR 2025. Code and demo are available at   https://https://pmrf-ml.github.io/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Photo-realistic image restoration algorithms are typically evaluated by distortion measures (e.g., PSNR, SSIM) and by perceptual quality measures (e.g., FID, NIQE), where the desire is to attain the lowest possible distortion without compromising on perceptual quality. To achieve this goal, current methods commonly attempt to sample from the posterior distribution, or to optimize a weighted sum of a distortion loss (e.g., MSE) and a perceptual quality loss (e.g., GAN). Unlike previous works, this paper is concerned specifically with the optimal estimator that minimizes the MSE under a constraint of perfect perceptual index, namely where the distribution of the reconstructed images is equal to that of the ground-truth ones. A recent theoretical result shows that such an estimator can be constructed by optimally transporting the posterior mean prediction (MMSE estimate) to the distribution of the ground-truth images. Inspired by this result, we introduce Posterior-Mean Rectified Flow (PMRF), a simple yet highly effective algorithm that approximates this optimal estimator. In particular, PMRF first predicts the posterior mean, and then transports the result to a high-quality image using a rectified flow model that approximates the desired optimal transport map. We investigate the theoretical utility of PMRF and demonstrate that it consistently outperforms previous methods on a variety of image restoration tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.00435</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.00435</id><created>2024-10-01</created><updated>2025-02-02</updated><authors><author><keyname>Hu</keyname><forenames>Lexiang</forenames></author><author><keyname>Wang</keyname><forenames>Yisen</forenames></author><author><keyname>Lin</keyname><forenames>Zhouchen</forenames></author></authors><title>Incorporating Arbitrary Matrix Group Equivariance into KANs</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kolmogorov-Arnold Networks (KANs) have seen great success in scientific domains thanks to spline activation functions, becoming an alternative to Multi-Layer Perceptrons (MLPs). However, spline functions may not respect symmetry in tasks, which is crucial prior knowledge in machine learning. In this paper, we propose Equivariant Kolmogorov-Arnold Networks (EKAN), a method for incorporating arbitrary matrix group equivariance into KANs, aiming to broaden their applicability to more fields. We first construct gated spline basis functions, which form the EKAN layer together with equivariant linear weights, and then define a lift layer to align the input space of EKAN with the feature space of the dataset, thereby building the entire EKAN architecture. Compared with baseline models, EKAN achieves higher accuracy with smaller datasets or fewer parameters on symmetry-related tasks, such as particle scattering and the three-body problem, often reducing test MSE by several orders of magnitude. Even in non-symbolic formula scenarios, such as top quark tagging with three jet constituents, EKAN achieves comparable results with state-of-the-art equivariant architectures using fewer than 40% of the parameters, while KANs do not outperform MLPs as expected. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.00978</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.00978</id><created>2024-10-01</created><updated>2025-01-31</updated><authors><author><keyname>Morrier</keyname><forenames>Jacob</forenames></author><author><keyname>Mahmassani</keyname><forenames>Amine</forenames></author><author><keyname>Alvarez</keyname><forenames>R. Michael</forenames></author></authors><title>Uncovering the Viral Nature of Toxicity in Competitive Online Video   Games</title><categories>cs.CY cs.HC econ.GN q-fin.EC</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Toxicity is a widespread phenomenon in competitive online video games. In addition to its direct undesirable effects, there is a concern that toxicity can spread to others, amplifying the harm caused by a single player's misbehavior. In this study, we estimate whether and to what extent a player's toxic speech spreads, causing their teammates to behave similarly. To this end, we analyze proprietary data from the free-to-play first-person action game Call of Duty: Warzone. We formulate and implement an instrumental variable identification strategy that leverages the network of interactions among players across matches. Our analysis reveals that all else equal, all of a player's teammates engaging in toxic speech increases their probability of engaging in similar behavior by 26.1 to 30.3 times the average player's likelihood of engaging in toxic speech. These findings confirm the viral nature of toxicity, especially toxic speech, in competitive online video games. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.01085</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.01085</id><created>2024-10-01</created><updated>2025-02-01</updated><authors><author><keyname>Zhang</keyname><forenames>Xuyang</forenames></author><author><keyname>Jiang</keyname><forenames>Jiaqi</forenames></author><author><keyname>Luo</keyname><forenames>Shan</forenames></author></authors><title>RoTip: A Finger-Shaped Tactile Sensor with Active Rotation Capability</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, advancements in optical tactile sensor technology have primarily centred on enhancing sensing precision and expanding the range of sensing modalities. To meet the requirements for more skilful manipulation, there should be a movement towards making tactile sensors more dynamic. In this paper, we introduce RoTip, a novel vision-based tactile sensor that is uniquely designed with an independently controlled joint and the capability to sense contact over its entire surface. The rotational capability of the sensor is particularly crucial for manipulating everyday objects, especially thin and flexible ones, as it enables the sensor to mobilize while in contact with the object's surface. The manipulation experiments demonstrate the ability of our proposed RoTip to manipulate rigid and flexible objects, and the full-finger tactile feedback and active rotation capabilities have the potential to explore more complex and precise manipulation tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.01196</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.01196</id><created>2024-10-01</created><updated>2025-02-02</updated><authors><author><keyname>Miller</keyname><forenames>John Joshua</forenames></author><author><keyname>Mak</keyname><forenames>Simon</forenames></author><author><keyname>Sun</keyname><forenames>Benny</forenames></author><author><keyname>Narayanan</keyname><forenames>Sai Ranjeet</forenames></author><author><keyname>Yang</keyname><forenames>Suo</forenames></author><author><keyname>Sun</keyname><forenames>Zongxuan</forenames></author><author><keyname>Kim</keyname><forenames>Kenneth S.</forenames></author><author><keyname>Kweon</keyname><forenames>Chol-Bum Mike</forenames></author></authors><title>Expected Diverse Utility (EDU): Diverse Bayesian Optimization of   Expensive Computer Simulators</title><categories>stat.AP cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The optimization of expensive black-box simulators arises in a myriad of modern scientific and engineering applications. Bayesian optimization provides an appealing solution, by leveraging a fitted surrogate model to guide the selection of subsequent simulator evaluations. In practice, however, the objective is often not to obtain a single good solution, but rather a ``basket'' of good solutions from which users can choose for downstream decision-making. This need arises in our motivating application for real-time control of internal combustion engines for flight propulsion, where a diverse set of control strategies is essential for stable flight control. There has been little work on this front for Bayesian optimization. We thus propose a new Expected Diverse Utility (EDU) method that searches for diverse ``$\epsilon$-optimal'' solutions: locally-optimal solutions within a tolerance level $\epsilon &gt; 0$ from a global optimum. We show that EDU yields a closed-form acquisition function under a Gaussian process surrogate model, which facilitates efficient sequential queries via automatic differentiation. This closed form further reveals a novel exploration-exploitation-diversity trade-off, which incorporates the desired diversity property within the well-known exploration-exploitation trade-off. We demonstrate the improvement of EDU over existing methods in a suite of numerical experiments, then explore the EDU in two applications on rover trajectory optimization and engine control for flight propulsion. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.01405</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.01405</id><created>2024-10-02</created><updated>2025-02-03</updated><authors><author><keyname>Xu</keyname><forenames>Kevin</forenames></author><author><keyname>Sato</keyname><forenames>Issei</forenames></author></authors><title>On Expressive Power of Looped Transformers: Theoretical Analysis and   Enhancement via Timestep Encoding</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Looped Transformers provide advantages in parameter efficiency, computational capabilities, and generalization for reasoning tasks. However, their expressive power regarding function approximation remains underexplored. In this paper, we establish the approximation rate of Looped Transformers by defining the modulus of continuity for sequence-to-sequence functions. This reveals a limitation specific to the looped architecture. That is, the analysis prompts the incorporation of scaling parameters for each loop, conditioned on timestep encoding. Experiments validate the theoretical results, showing that increasing the number of loops enhances performance, with further gains achieved through the timestep encoding. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.01438</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.01438</id><created>2024-10-02</created><updated>2025-02-01</updated><authors><author><keyname>Kao</keyname><forenames>Ching-Chia</forenames></author><author><keyname>Yu</keyname><forenames>Chia-Mu</forenames></author><author><keyname>Lu</keyname><forenames>Chun-Shien</forenames></author><author><keyname>Chen</keyname><forenames>Chu-Song</forenames></author></authors><title>The Great Contradiction Showdown: How Jailbreak and Stealth Wrestle in   Vision-Language Models?</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Vision-Language Models (VLMs) have achieved remarkable performance on a variety of tasks, yet they remain vulnerable to jailbreak attacks that compromise safety and reliability. In this paper, we provide an information-theoretic framework for understanding the fundamental trade-off between the effectiveness of these attacks and their stealthiness. Drawing on Fano's inequality, we demonstrate how an attacker's success probability is intrinsically linked to the stealthiness of generated prompts. Building on this, we propose an efficient algorithm for detecting non-stealthy jailbreak attacks, offering significant improvements in model robustness. Experimental results highlight the tension between strong attacks and their detectability, providing insights into both adversarial strategies and defense mechanisms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.01686</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.01686</id><created>2024-10-02</created><updated>2025-01-31</updated><authors><author><keyname>de Luca</keyname><forenames>Artur Back</forenames></author><author><keyname>Giapitzakis</keyname><forenames>George</forenames></author><author><keyname>Yang</keyname><forenames>Shenghao</forenames></author><author><keyname>Veličković</keyname><forenames>Petar</forenames></author><author><keyname>Fountoulakis</keyname><forenames>Kimon</forenames></author></authors><title>Positional Attention: Expressivity and Learnability of Algorithmic   Computation</title><categories>cs.LG cs.AI cs.DS</categories><comments>64 pages, 37 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  There is a growing interest in the ability of neural networks to execute algorithmic tasks (e.g., arithmetic, summary statistics, and sorting). The goal of this work is to better understand the role of attention in Transformers for algorithmic execution. Its importance for algorithmic execution has been studied theoretically and empirically using parallel computational models. Notably, many parallel algorithms communicate between processors solely using positional information. Inspired by this observation, we investigate how Transformers can execute algorithms using positional attention, where attention weights depend exclusively on positional encodings. We prove that Transformers with positional attention (positional Transformers) maintain the same expressivity of parallel computational models, incurring a logarithmic depth cost relative to the input length. We analyze their in-distribution learnability and explore how parameter norms in positional attention affect sample complexity. Our results show that positional Transformers introduce a learning trade-off: while they exhibit better theoretical dependence on parameter norms, certain tasks may require more layers, which can, in turn, increase sample complexity. Finally, we empirically explore the out-of-distribution performance of positional Transformers and find that they perform well in tasks where their underlying algorithmic solution relies on positional information. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.01719</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.01719</id><created>2024-10-02</created><updated>2025-02-02</updated><authors><author><keyname>Xu</keyname><forenames>Jiamin</forenames></author><author><keyname>Li</keyname><forenames>Zelong</forenames></author><author><keyname>Zheng</keyname><forenames>Yuxin</forenames></author><author><keyname>Huang</keyname><forenames>Chenyu</forenames></author><author><keyname>Gu</keyname><forenames>Renshu</forenames></author><author><keyname>Xu</keyname><forenames>Weiwei</forenames></author><author><keyname>Xu</keyname><forenames>Gang</forenames></author></authors><title>OmniSR: Shadow Removal under Direct and Indirect Lighting</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Shadows can originate from occlusions in both direct and indirect illumination. Although most current shadow removal research focuses on shadows caused by direct illumination, shadows from indirect illumination are often just as pervasive, particularly in indoor scenes. A significant challenge in removing shadows from indirect illumination is obtaining shadow-free images to train the shadow removal network. To overcome this challenge, we propose a novel rendering pipeline for generating shadowed and shadow-free images under direct and indirect illumination, and create a comprehensive synthetic dataset that contains over 30,000 image pairs, covering various object types and lighting conditions. We also propose an innovative shadow removal network that explicitly integrates semantic and geometric priors through concatenation and attention mechanisms. The experiments show that our method outperforms state-of-the-art shadow removal techniques and can effectively generalize to indoor and outdoor scenes under various lighting conditions, enhancing the overall effectiveness and applicability of shadow removal methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.02101</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.02101</id><created>2024-10-02</created><updated>2025-01-31</updated><authors><author><keyname>Scarvelis</keyname><forenames>Christopher</forenames></author><author><keyname>Benhaim</keyname><forenames>David</forenames></author><author><keyname>Zhang</keyname><forenames>Paul</forenames></author></authors><title>Orient Anything</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orientation estimation is a fundamental task in 3D shape analysis which consists of estimating a shape's orientation axes: its side-, up-, and front-axes. Using this data, one can rotate a shape into canonical orientation, where its orientation axes are aligned with the coordinate axes. Developing an orientation algorithm that reliably estimates complete orientations of general shapes remains an open problem. We introduce a two-stage orientation pipeline that achieves state of the art performance on up-axis estimation and further demonstrate its efficacy on full-orientation estimation, where one seeks all three orientation axes. Unlike previous work, we train and evaluate our method on all of Shapenet rather than a subset of classes. We motivate our engineering contributions by theory describing fundamental obstacles to orientation estimation for rotationally-symmetric shapes, and show how our method avoids these obstacles. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.02147</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.02147</id><created>2024-10-02</created><updated>2025-02-01</updated><authors><author><keyname>Patel</keyname><forenames>Gaurav</forenames></author><author><keyname>Sandino</keyname><forenames>Christopher</forenames></author><author><keyname>Mahasseni</keyname><forenames>Behrooz</forenames></author><author><keyname>Zippi</keyname><forenames>Ellen L</forenames></author><author><keyname>Azemi</keyname><forenames>Erdrin</forenames></author><author><keyname>Moin</keyname><forenames>Ali</forenames></author><author><keyname>Minxha</keyname><forenames>Juri</forenames></author></authors><title>Efficient Source-Free Time-Series Adaptation via Parameter Subspace   Disentanglement</title><categories>cs.LG cs.AI eess.SP</categories><comments>Accepted at ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we propose a framework for efficient Source-Free Domain Adaptation (SFDA) in the context of time-series, focusing on enhancing both parameter efficiency and data-sample utilization. Our approach introduces an improved paradigm for source-model preparation and target-side adaptation, aiming to enhance training efficiency during target adaptation. Specifically, we reparameterize the source model's weights in a Tucker-style decomposed manner, factorizing the model into a compact form during the source model preparation phase. During target-side adaptation, only a subset of these decomposed factors is fine-tuned, leading to significant improvements in training efficiency. We demonstrate using PAC Bayesian analysis that this selective fine-tuning strategy implicitly regularizes the adaptation process by constraining the model's learning capacity. Furthermore, this re-parameterization reduces the overall model size and enhances inference efficiency, making the approach particularly well suited for resource-constrained devices. Additionally, we demonstrate that our framework is compatible with various SFDA methods and achieves significant computational efficiency, reducing the number of fine-tuned parameters and inference overhead in terms of MACs by over 90% while maintaining model performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.02199</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.02199</id><created>2024-10-03</created><updated>2025-02-02</updated><authors><author><keyname>Hashimoto</keyname><forenames>Yuka</forenames></author><author><keyname>Iwata</keyname><forenames>Tomoharu</forenames></author></authors><title>Deep Koopman-layered Model with Universal Property Based on Toeplitz   Matrices</title><categories>cs.LG math.DS math.FA stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose deep Koopman-layered models with learnable parameters in the form of Toeplitz matrices for analyzing the transition of the dynamics of time-series data. The proposed model has both theoretical solidness and flexibility. By virtue of the universal property of Toeplitz matrices and the reproducing property underlying the model, we can show its universality and generalization property. In addition, the flexibility of the proposed model enables the model to fit time-series data coming from nonautonomous dynamical systems. When training the model, we apply Krylov subspace methods for efficient computations. In this sense, the proposed model establishes a new connection between Koopman operators and numerical linear algebraic methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.02296</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.02296</id><created>2024-10-03</created><updated>2025-01-31</updated><authors><author><keyname>Xu</keyname><forenames>Zhe</forenames></author><author><keyname>Hassani</keyname><forenames>Kaveh</forenames></author><author><keyname>Zhang</keyname><forenames>Si</forenames></author><author><keyname>Zeng</keyname><forenames>Hanqing</forenames></author><author><keyname>Yasunaga</keyname><forenames>Michihiro</forenames></author><author><keyname>Wang</keyname><forenames>Limei</forenames></author><author><keyname>Fu</keyname><forenames>Dongqi</forenames></author><author><keyname>Yao</keyname><forenames>Ning</forenames></author><author><keyname>Long</keyname><forenames>Bo</forenames></author><author><keyname>Tong</keyname><forenames>Hanghang</forenames></author></authors><title>How to Make LLMs Strong Node Classifiers?</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Language Models (LMs) are increasingly challenging the dominance of domain-specific models, such as Graph Neural Networks (GNNs) and Graph Transformers (GTs), in graph learning tasks. Following this trend, we propose a novel approach that empowers off-the-shelf LMs to achieve performance comparable to state-of-the-art (SOTA) GNNs on node classification tasks, without requiring any architectural modification. By preserving the LM's original architecture, our approach retains a key benefit of LM instruction tuning: the ability to jointly train on diverse datasets, fostering greater flexibility and efficiency. To achieve this, we introduce two key augmentation strategies: (1) Enriching LMs' input using topological and semantic retrieval methods, which provide richer contextual information, and (2) guiding the LMs' classification process through a lightweight GNN classifier that effectively prunes class candidates. Our experiments on real-world datasets show that backbone Flan-T5 LMs equipped with these augmentation strategies outperform SOTA text-output node classifiers and are comparable to top-performing vector-output node classifiers. By bridging the gap between specialized node classifiers and general LMs, this work paves the way for more versatile and widely applicable graph learning models. We will open-source the code upon publication. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.02724</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.02724</id><created>2024-10-03</created><updated>2025-02-02</updated><authors><author><keyname>Zekri</keyname><forenames>Oussama</forenames></author><author><keyname>Odonnat</keyname><forenames>Ambroise</forenames></author><author><keyname>Benechehab</keyname><forenames>Abdelhakim</forenames></author><author><keyname>Bleistein</keyname><forenames>Linus</forenames></author><author><keyname>Boullé</keyname><forenames>Nicolas</forenames></author><author><keyname>Redko</keyname><forenames>Ievgen</forenames></author></authors><title>Large Language Models as Markov Chains</title><categories>stat.ML cs.AI cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large language models (LLMs) are remarkably efficient across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the LLMs' generalization capabilities remains elusive. In our paper, we approach this task by drawing an equivalence between autoregressive transformer-based language models and Markov chains defined on a finite state space. This allows us to study the multi-step inference mechanism of LLMs from first principles. We relate the obtained results to the pathological behavior observed with LLMs such as repetitions and incoherent replies with high temperature. Finally, we leverage the proposed formalization to derive pre-training and in-context learning generalization bounds for LLMs under realistic data and model assumptions. Experiments with the most recent Llama and Gemma herds of models show that our theory correctly captures their behavior in practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.03348</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.03348</id><created>2024-10-04</created><updated>2025-02-02</updated><authors><author><keyname>Naik</keyname><forenames>Aaditya</forenames></author><author><keyname>Liu</keyname><forenames>Jason</forenames></author><author><keyname>Wang</keyname><forenames>Claire</forenames></author><author><keyname>Dutta</keyname><forenames>Saikat</forenames></author><author><keyname>Naik</keyname><forenames>Mayur</forenames></author><author><keyname>Wong</keyname><forenames>Eric</forenames></author></authors><title>Dolphin: A Programmable Framework for Scalable Neurosymbolic Learning</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Neurosymbolic learning enables the integration of symbolic reasoning with deep learning but faces significant challenges in scaling to complex symbolic programs, large datasets, or both. We introduce Dolphin, a framework that tackles these challenges by supporting neurosymbolic programs in Python, executing complex symbolic reasoning on the CPU while vectorizing probabilistic computations and gradient propagation on the GPU. Across 13 benchmarks spanning tasks over text, image, and video data, with symbolic reasoning features like recursion and black-box functions, Dolphin converges to state-of-the-art accuracies on the more complex benchmarks while existing frameworks such as Scallop, ISED, and IndeCateR+ fail to converge within the time limit. On simpler benchmarks, Dolphin matches their performance, while achieving these results 1.71x to 62x faster than the baselines. Overall, Dolphin advances the scalability of neurosymbolic frameworks, achieving state-of-the-art efficiency and convergence on difficult benchmarks where existing frameworks struggle. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.04060</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.04060</id><created>2024-10-05</created><updated>2025-02-02</updated><authors><author><keyname>Hounie</keyname><forenames>Ignacio</forenames></author><author><keyname>Kanatsoulis</keyname><forenames>Charilaos</forenames></author><author><keyname>Tandon</keyname><forenames>Arnuv</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>LoRTA: Low Rank Tensor Adaptation of Large Language Models</title><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Low Rank Adaptation (LoRA) is a popular Parameter Efficient Fine Tuning (PEFT) method that effectively adapts large pre-trained models for downstream tasks. LoRA parameterizes model updates using low-rank matrices at each layer, significantly reducing the number of trainable parameters and, consequently, resource requirements during fine-tuning. However, the lower bound on the number of trainable parameters remains high due to the use of the low-rank matrix model. Recent works have addressed this limitation by proposing low rank tensor parameterizations for model updates. However, they only exploit redundancy across layers, or tensorize individual matrices using ad-hoc schemes that introduce additional hyperparameters. In this work, we propose a higher-order Candecomp/Parafac (CP) decomposition, enabling a more compact and flexible representation compared to existing matrix and tensor based PEFT methods. Our experiments on Natural Language Understanding, Instruction Tuning, Preference Optimization and Protein Folding benchmarks demonstrate that our method can achieve a reduction in the number of parameters while maintaining comparable performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.04350</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.04350</id><created>2024-10-06</created><updated>2025-02-03</updated><authors><author><keyname>Liu</keyname><forenames>Aiwei</forenames></author><author><keyname>Bai</keyname><forenames>Haoping</forenames></author><author><keyname>Lu</keyname><forenames>Zhiyun</forenames></author><author><keyname>Sun</keyname><forenames>Yanchao</forenames></author><author><keyname>Kong</keyname><forenames>Xiang</forenames></author><author><keyname>Wang</keyname><forenames>Simon</forenames></author><author><keyname>Shan</keyname><forenames>Jiulong</forenames></author><author><keyname>Jose</keyname><forenames>Albin Madappally</forenames></author><author><keyname>Liu</keyname><forenames>Xiaojiang</forenames></author><author><keyname>Wen</keyname><forenames>Lijie</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author><author><keyname>Cao</keyname><forenames>Meng</forenames></author></authors><title>TIS-DPO: Token-level Importance Sampling for Direct Preference   Optimization With Estimated Weights</title><categories>cs.CL</categories><comments>30 pages, 8 figures, 8 tables, Published in ICLR 2025</comments><msc-class>68T50</msc-class><acm-class>I.2.7</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Direct Preference Optimization (DPO) has been widely adopted for preference alignment of Large Language Models (LLMs) due to its simplicity and effectiveness. However, DPO is derived as a bandit problem in which the whole response is treated as a single arm, ignoring the importance differences between tokens, which may affect optimization efficiency and make it difficult to achieve optimal results. In this work, we propose that the optimal data for DPO has equal expected rewards for each token in winning and losing responses, as there is no difference in token importance. However, since the optimal dataset is unavailable in practice, we propose using the original dataset for importance sampling to achieve unbiased optimization. Accordingly, we propose a token-level importance sampling DPO objective named TIS-DPO that assigns importance weights to each token based on its reward. Inspired by previous works, we estimate the token importance weights using the difference in prediction probabilities from a pair of contrastive LLMs. We explore three methods to construct these contrastive LLMs: (1) guiding the original LLM with contrastive prompts, (2) training two separate LLMs using winning and losing responses, and (3) performing forward and reverse DPO training with winning and losing responses. Experiments show that TIS-DPO significantly outperforms various baseline methods on harmlessness and helpfulness alignment and summarization tasks. We also visualize the estimated weights, demonstrating their ability to identify key token positions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.04458</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.04458</id><created>2024-10-06</created><updated>2025-01-31</updated><authors><author><keyname>Jin</keyname><forenames>Ruinan</forenames></author><author><keyname>Li</keyname><forenames>Xiao</forenames></author><author><keyname>Yu</keyname><forenames>Yaoliang</forenames></author><author><keyname>Wang</keyname><forenames>Baoxiang</forenames></author></authors><title>A Comprehensive Framework for Analyzing the Convergence of Adam:   Bridging the Gap with SGD</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive Moment Estimation (Adam) is a cornerstone optimization algorithm in deep learning, widely recognized for its flexibility with adaptive learning rates and efficiency in handling large-scale data. However, despite its practical success, the theoretical understanding of Adam's convergence has been constrained by stringent assumptions, such as almost surely bounded stochastic gradients or uniformly bounded gradients, which are more restrictive than those typically required for analyzing stochastic gradient descent (SGD).   In this paper, we introduce a novel and comprehensive framework for analyzing the convergence properties of Adam. This framework offers a versatile approach to establishing Adam's convergence. Specifically, we prove that Adam achieves asymptotic (last iterate sense) convergence in both the almost sure sense and the \(L_1\) sense under the relaxed assumptions typically used for SGD, namely \(L\)-smoothness and the ABC inequality. Meanwhile, under the same assumptions, we show that Adam attains non-asymptotic sample complexity bounds similar to those of SGD. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.05273</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.05273</id><created>2024-09-12</created><updated>2025-02-02</updated><authors><author><keyname>Zhang</keyname><forenames>Jianke</forenames></author><author><keyname>Guo</keyname><forenames>Yanjiang</forenames></author><author><keyname>Chen</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Wang</keyname><forenames>Yen-Jen</forenames></author><author><keyname>Hu</keyname><forenames>Yucheng</forenames></author><author><keyname>Shi</keyname><forenames>Chengming</forenames></author><author><keyname>Chen</keyname><forenames>Jianyu</forenames></author></authors><title>HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers</title><categories>cs.CV cs.AI cs.RO</categories><comments>Accepted to CORL 2024</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Vision-Language-Action (VLA) models, leveraging powerful pre trained Vision-Language Models (VLMs) backends, have shown promise in robotic control due to their impressive generalization ability. However, the success comes at a cost. Their reliance on VLM backends with billions of parameters leads to high computational costs and inference latency, limiting the testing scenarios to mainly quasi-static tasks and hindering performance in dynamic tasks requiring rapid interactions. To address these limitations, this paper proposes HiRT, a Hierarchical Robot Transformer framework that enables flexible frequency and performance trade-off. HiRT keeps VLMs running at low frequencies to capture temporarily invariant features while enabling real-time interaction through a high-frequency vision-based policy guided by the slowly updated features. Experiment results in both simulation and real-world settings demonstrate significant improvements over baseline methods. Empirically, in static tasks, we double the control frequency and achieve comparable success rates. Additionally, on novel real-world dynamic ma nipulation tasks which are challenging for previous VLA models, HiRT improves the success rate from 48% to 75%. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.06331</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.06331</id><created>2024-10-08</created><updated>2025-02-01</updated><authors><author><keyname>Zhang</keyname><forenames>Zhuoran</forenames></author><author><keyname>Li</keyname><forenames>Yongxiang</forenames></author><author><keyname>Kan</keyname><forenames>Zijian</forenames></author><author><keyname>Cheng</keyname><forenames>Keyuan</forenames></author><author><keyname>Hu</keyname><forenames>Lijie</forenames></author><author><keyname>Wang</keyname><forenames>Di</forenames></author></authors><title>Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing</title><categories>cs.CL cs.AI cs.LG</categories><comments>24 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The locate-then-edit paradigm has shown significant promise for knowledge editing (KE) in Large Language Models (LLMs). While previous methods perform well on single-hop fact recall tasks, they consistently struggle with multi-hop factual recall tasks involving newly edited knowledge. In this paper, leveraging tools in mechanistic interpretability, we first identify that in multi-hop tasks, LLMs tend to retrieve knowledge with implicit subject information from deeper MLP layers, unlike single-hop tasks, which rely on shallow layers. This distinction explains the poor performance of current methods in multi-hop queries, as they primarily focus on editing shallow layers with single-hop edit prompts, leaving deeper layers unchanged. To address this, we propose IFMET, a novel locate-then-edit KE approach designed to edit both shallow and deep MLP layers. Beyond single-hop editing prompts, IFMET further incorporates multi-hop editing prompts to locate and modify knowledge across different stages of reasoning. Experimental results demonstrate that IFMET significantly improves performance on multi-hop factual recall tasks, overcoming the limitations of previous locate-then-edit methods </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.06431</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.06431</id><created>2024-10-08</created><updated>2025-01-31</updated><authors><author><keyname>Niu</keyname><forenames>Ruijia</forenames></author><author><keyname>Wu</keyname><forenames>Dongxia</forenames></author><author><keyname>Yu</keyname><forenames>Rose</forenames></author><author><keyname>Ma</keyname><forenames>Yi-An</forenames></author></authors><title>Functional-level Uncertainty Quantification for Calibrated Fine-tuning   on LLMs</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Accurate uncertainty quantification of large language models (LLMs) provides credibility measure over their outputs. However, fine-tuned LLMs often struggle with overconfidence in uncertain predictions due to the limitations in the models' ability to generalize with limited data. Existing parameter efficient fine-tuning (PEFT) uncertainty quantification methods for LLMs focus on post fine-tuning stage and fall short of calibrating epistemic uncertainty. To address these limitations, we propose Functional-Level Uncertainty Quantification for Calibrated Fine-Tuning (UQ4CT), which captures and calibrates epistemic uncertainty over the space of functions that map input prompts to outputs. We implement UQ4CT during the fine-tuning stage via a mixture-of-experts framework that hierarchically decomposes the functional space. We demonstrate that UQ4CT reduces Expected Calibration Error (ECE) by more than $25\%$ while maintaining high accuracy across $5$ benchmarks. Even under distribution shift, UQ4CT maintains superior ECE performance with high accuracy, showcasing improved generalizability. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.07062</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.07062</id><created>2024-10-09</created><updated>2025-02-01</updated><authors><author><keyname>Gutierrez</keyname><forenames>Cristian</forenames></author></authors><title>TinyEmo: Scaling down Emotional Reasoning via Metric Projection</title><categories>cs.CV</categories><comments>Fix table format</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper introduces TinyEmo, a family of small multi-modal language models for emotional reasoning and classification. Our approach features: (1) a synthetic emotional instruct dataset for both pre-training and fine-tuning stages, (2) a Metric Projector that delegates classification from the language model allowing for more efficient training and inference, (3) a multi-modal large language model (MM-LLM) for emotional reasoning, and (4) a semi-automated framework for bias detection. TinyEmo is able to perform emotion classification and emotional reasoning, all while using substantially fewer parameters than comparable models. This efficiency allows us to freely incorporate more diverse emotional datasets, enabling strong performance on classification tasks, with our smallest model (700M parameters) outperforming larger state-of-the-art models based on general-purpose MM-LLMs with over 7B parameters. Additionally, the Metric Projector allows for interpretability and indirect bias detection in large models without additional training, offering an approach to understand and improve AI systems. We release code, models, and dataset at https://github.com/ggcr/TinyEmo </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.07263</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.07263</id><created>2024-10-08</created><updated>2025-02-01</updated><authors><author><keyname>Dutta</keyname><forenames>Sanchayan</forenames><affiliation>UC Davis</affiliation></author><author><keyname>Sra</keyname><forenames>Suvrit</forenames><affiliation>TU Munich</affiliation></author></authors><title>Toward generalizable learning of all (linear) first-order methods via   memory augmented Transformers</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that memory-augmented Transformers can implement the entire class of linear first-order methods (LFOMs), a class that contains gradient descent (GD) and more advanced methods such as conjugate gradient descent (CGD), momentum methods and all other variants that linearly combine past gradients. Building on prior work that studies how Transformers simulate GD, we provide theoretical and empirical evidence that memory-augmented Transformers can learn more advanced algorithms. We then take a first step toward turning the learned algorithms into actually usable methods by developing a mixture-of-experts (MoE) approach for test-time adaptation to out-of-distribution (OOD) samples. Lastly, we show that LFOMs can themselves be treated as learnable algorithms, whose parameters can be learned from data to attain strong performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.08003</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.08003</id><created>2024-10-10</created><updated>2025-02-01</updated><authors><author><keyname>Shaier</keyname><forenames>Sagi</forenames></author><author><keyname>Pereira</keyname><forenames>Francisco</forenames></author><author><keyname>von der Wense</keyname><forenames>Katharina</forenames></author><author><keyname>Hunter</keyname><forenames>Lawrence E</forenames></author><author><keyname>Jones</keyname><forenames>Matt</forenames></author></authors><title>More Experts Than Galaxies: Conditionally-overlapping Experts With   Biologically-Inspired Fixed Routing</title><categories>cs.LG</categories><comments>Published as a conference paper at ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The evolution of biological neural systems has led to both modularity and sparse coding, which enables energy efficiency and robustness across the diversity of tasks in the lifespan. In contrast, standard neural networks rely on dense, non-specialized architectures, where all model parameters are simultaneously updated to learn multiple tasks, leading to interference. Current sparse neural network approaches aim to alleviate this issue but are hindered by limitations such as 1) trainable gating functions that cause representation collapse, 2) disjoint experts that result in redundant computation and slow learning, and 3) reliance on explicit input or task IDs that limit flexibility and scalability. In this paper we propose Conditionally Overlapping Mixture of ExperTs (COMET), a general deep learning method that addresses these challenges by inducing a modular, sparse architecture with an exponential number of overlapping experts. COMET replaces the trainable gating function used in Sparse Mixture of Experts with a fixed, biologically inspired random projection applied to individual input representations. This design causes the degree of expert overlap to depend on input similarity, so that similar inputs tend to share more parameters. This results in faster learning \rev{per update step} and improved \rev{out-of-sample} generalization. We demonstrate the effectiveness of COMET on a range of tasks, including image classification, language modeling, and regression, using several popular deep learning architectures. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.08368</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.08368</id><created>2024-10-10</created><updated>2025-02-02</updated><authors><author><keyname>Yan</keyname><forenames>Wilson</forenames></author><author><keyname>Mnih</keyname><forenames>Volodymyr</forenames></author><author><keyname>Faust</keyname><forenames>Aleksandra</forenames></author><author><keyname>Zaharia</keyname><forenames>Matei</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author><author><keyname>Liu</keyname><forenames>Hao</forenames></author></authors><title>ElasticTok: Adaptive Tokenization for Image and Video</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient video tokenization remains a key bottleneck in learning general purpose vision models that are capable of processing long video sequences. Prevailing approaches are restricted to encoding videos to a fixed number of tokens, where too few tokens will result in overly lossy encodings, and too many tokens will result in prohibitively long sequence lengths. In this work, we introduce ElasticTok, a method that conditions on prior frames to adaptively encode a frame into a variable number of tokens. To enable this in a computationally scalable way, we propose a masking technique that drops a random number of tokens at the end of each frames's token encoding. During inference, ElasticTok can dynamically allocate tokens when needed -- more complex data can leverage more tokens, while simpler data only needs a few tokens. Our empirical evaluations on images and video demonstrate the effectiveness of our approach in efficient token usage, paving the way for future development of more powerful multimodal models, world models, and agents. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.08435</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.08435</id><created>2024-10-10</created><updated>2025-02-02</updated><authors><author><keyname>Zhu</keyname><forenames>Tingyu</forenames></author><author><keyname>Liu</keyname><forenames>Haoyu</forenames></author><author><keyname>Wang</keyname><forenames>Ziyu</forenames></author><author><keyname>Jiang</keyname><forenames>Zhimin</forenames></author><author><keyname>Zheng</keyname><forenames>Zeyu</forenames></author></authors><title>Efficient Fine-Grained Guidance for Diffusion-Based Symbolic Music   Generation</title><categories>cs.SD cs.AI cs.LG cs.MM eess.AS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Developing generative models to create or conditionally create symbolic music presents unique challenges due to the combination of limited data availability and the need for high precision in note pitch. To address these challenges, we introduce an efficient Fine-Grained Guidance (FGG) approach within diffusion models. FGG guides the diffusion models to generate music that aligns more closely with the control and intent of expert composers, which is critical to improve the accuracy, listenability, and quality of generated music. This approach empowers diffusion models to excel in advanced applications such as improvisation, and interactive music creation. We derive theoretical characterizations for both the challenges in symbolic music generation and the effects of the FGG approach. We provide numerical experiments and subjective evaluation to demonstrate the effectiveness of our approach. We have published a demo page to showcase performances, as one of the first in the symbolic music literature's demo pages that enables real-time interactive generation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.08589</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.08589</id><created>2024-10-11</created><updated>2025-02-01</updated><authors><author><keyname>Chen</keyname><forenames>I-Chun</forenames></author><author><keyname>Liu</keyname><forenames>Hsu-Shen</forenames></author><author><keyname>Sun</keyname><forenames>Wei-Fang</forenames></author><author><keyname>Chao</keyname><forenames>Chen-Hao</forenames></author><author><keyname>Hsu</keyname><forenames>Yen-Chang</forenames></author><author><keyname>Lee</keyname><forenames>Chun-Yi</forenames></author></authors><title>Retraining-Free Merging of Sparse MoE via Hierarchical Clustering</title><categories>cs.LG</categories><comments>Code: https://anonymous.4open.science/r/TAMP-11E2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse Mixture-of-Experts (SMoE) models represent a significant advancement in large language model (LLM) development through their efficient parameter utilization. These models achieve substantial performance improvements at reduced inference costs. However, the deployment of SMoE models faces constraints from extensive memory requirements of expert components in resource-limited environments. To address these limitations, this paper introduces Hierarchical Clustering for Sparsely activated Mixture of Experts (HC-SMoE), a task-agnostic expert merging framework for parameter reduction without retraining. HC-SMoE introduces a novel hierarchical clustering approach based on expert outputs to ensure merging robustness independent of routing decisions. The proposed output-based clustering method enables effective capture of functional relationships between experts for large-scale architectures. We provide theoretical analysis and comprehensive evaluations across multiple zero-shot language tasks to demonstrate HC-SMoE's effectiveness in state-of-the-art models including Qwen and Mixtral. The experimental results validate HC-SMoE's superior performance and practical applicability for real-world deployments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.09508</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.09508</id><created>2024-10-12</created><updated>2025-02-03</updated><authors><author><keyname>Zheng</keyname><forenames>Jiamu</forenames></author><author><keyname>Zhang</keyname><forenames>Jinghuai</forenames></author><author><keyname>Du</keyname><forenames>Tianyu</forenames></author><author><keyname>Zhang</keyname><forenames>Xuhong</forenames></author><author><keyname>Yin</keyname><forenames>Jianwei</forenames></author><author><keyname>Lin</keyname><forenames>Tao</forenames></author></authors><title>CollabEdit: Towards Non-destructive Collaborative Knowledge Editing</title><categories>cs.CL cs.CY</categories><comments>20 pages, 11 figures. Published as a conference paper at ICLR 2025.   Code at https://github.com/LINs-lab/CollabEdit</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Collaborative learning of large language models (LLMs) has emerged as a new paradigm for utilizing private data from different parties to guarantee efficiency and privacy. Meanwhile, Knowledge Editing (KE) for LLMs has also garnered increased attention due to its ability to manipulate the behaviors of LLMs explicitly, yet leaves the collaborative KE case (in which knowledge edits of multiple parties are aggregated in a privacy-preserving and continual manner) unexamined. To this end, this manuscript dives into the first investigation of collaborative KE, in which we start by carefully identifying the unique three challenges therein, including knowledge overlap, knowledge conflict, and knowledge forgetting. We then propose a non-destructive collaborative KE framework, COLLABEDIT, which employs a novel model merging mechanism to mimic the global KE behavior while preventing the severe performance drop. Extensive experiments on two canonical datasets demonstrate the superiority of COLLABEDIT compared to other destructive baselines, and results shed light on addressing three collaborative KE challenges and future applications. Our code is available at https://github.com/LINs-lab/CollabEdit. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.09740</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.09740</id><created>2024-10-13</created><updated>2025-02-01</updated><authors><author><keyname>Tseng</keyname><forenames>Wei-Cheng</forenames></author><author><keyname>Zhang</keyname><forenames>Ellina</forenames></author><author><keyname>Jatavallabhula</keyname><forenames>Krishna Murthy</forenames></author><author><keyname>Shkurti</keyname><forenames>Florian</forenames></author></authors><title>Gaussian Splatting Visual MPC for Granular Media Manipulation</title><categories>cs.RO</categories><comments>project website https://weichengtseng.github.io/gs-granular-mani/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advancements in learned 3D representations have enabled significant progress in solving complex robotic manipulation tasks, particularly for rigid-body objects. However, manipulating granular materials such as beans, nuts, and rice, remains challenging due to the intricate physics of particle interactions, high-dimensional and partially observable state, inability to visually track individual particles in a pile, and the computational demands of accurate dynamics prediction. Current deep latent dynamics models often struggle to generalize in granular material manipulation due to a lack of inductive biases. In this work, we propose a novel approach that learns a visual dynamics model over Gaussian splatting representations of scenes and leverages this model for manipulating granular media via Model-Predictive Control. Our method enables efficient optimization for complex manipulation tasks on piles of granular media. We evaluate our approach in both simulated and real-world settings, demonstrating its ability to solve unseen planning tasks and generalize to new environments in a zero-shot transfer. We also show significant prediction and manipulation performance improvements compared to existing granular media manipulation methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.10546</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.10546</id><created>2024-10-14</created><updated>2025-01-31</updated><authors><author><keyname>Alain</keyname><forenames>Mathieu</forenames></author><author><keyname>Takao</keyname><forenames>So</forenames></author><author><keyname>Dong</keyname><forenames>Xiaowen</forenames></author><author><keyname>Rieck</keyname><forenames>Bastian</forenames></author><author><keyname>Noutahi</keyname><forenames>Emmanuel</forenames></author></authors><title>Graph Classification Gaussian Processes via Hodgelet Spectral Features</title><categories>cs.LG stat.ML</categories><comments>NeurIPS 2024 Workshop on Bayesian Decision-Making and Uncertainty   (Spotlight)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The problem of classifying graphs is ubiquitous in machine learning. While it is standard to apply graph neural networks or graph kernel methods, Gaussian processes can be employed by transforming spatial features from the graph domain into spectral features in the Euclidean domain, and using them as the input points of classical kernels. However, this approach currently only takes into account features on vertices, whereas some graph datasets also support features on edges. In this work, we present a Gaussian process-based classification algorithm that can leverage one or both vertex and edges features. Furthermore, we take advantage of the Hodge decomposition to better capture the intricate richness of vertex and edge features, which can be beneficial on diverse tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.10678</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.10678</id><created>2024-10-14</created><updated>2025-02-03</updated><authors><author><keyname>Blazhko</keyname><forenames>Hanna</forenames></author><author><keyname>Homza</keyname><forenames>Daniil</forenames></author><author><keyname>Schwenninger</keyname><forenames>Felix L.</forenames></author><author><keyname>de Vries</keyname><forenames>Jens</forenames></author><author><keyname>Wojtylak</keyname><forenames>Michał</forenames></author></authors><title>The algebraic numerical range as a spectral set in Banach algebras</title><categories>math.FA cs.NA math.NA</categories><msc-class>47A25, 47A12, 47B48, 15A60</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We investigate when the algebraic numerical range is a $C$-spectral set in a Banach algebra. While providing several counterexamples based on classical ideas as well as combinatorial Banach spaces, we discuss positive results for matrix algebras and provide an absolute constant in the case of complex $2\times2$-matrices with the induced $1$-norm. Furthermore, we discuss positive results for infinite-dimensional Banach algebras, including the Calkin algebra. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.10784</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.10784</id><created>2024-10-14</created><updated>2025-02-03</updated><authors><author><keyname>Hatleskog</keyname><forenames>Johan</forenames></author><author><keyname>Alexis</keyname><forenames>Kostas</forenames></author></authors><title>Probabilistic Degeneracy Detection for Point-to-Plane Error Minimization</title><categories>cs.RO</categories><comments>8 pages, 5 figures, accepted by IEEE Robotics and Automation Letters   (IEEE RAL). Supplementary video: https://www.youtube.com/watch?v=bKnHs_wwnXs.   Code: https://github.com/ntnu-arl/drpm</comments><doi>10.1109/LRA.2024.3484153</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Degeneracies arising from uninformative geometry are known to deteriorate LiDAR-based localization and mapping. This work introduces a new probabilistic method to detect and mitigate the effect of degeneracies in point-to-plane error minimization. The noise on the Hessian of the point-to-plane optimization problem is characterized by the noise on points and surface normals used in its construction. We exploit this characterization to quantify the probability of a direction being degenerate. The degeneracy-detection procedure is used in a new real-time degeneracy-aware iterative closest point algorithm for LiDAR registration, in which we smoothly attenuate updates in degenerate directions. The method's parameters are selected based on the noise characteristics provided in the LiDAR's datasheet. We validate the approach in four real-world experiments, demonstrating that it outperforms state-of-the-art methods at detecting and mitigating the adverse effects of degeneracies. For the benefit of the community, we release the code for the method at: github.com/ntnu-arl/drpm. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.11008</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.11008</id><created>2024-10-14</created><updated>2025-02-02</updated><authors><author><keyname>Qu</keyname><forenames>Qianxin</forenames></author><author><keyname>Xiong</keyname><forenames>Yijin</forenames></author><author><keyname>Zhang</keyname><forenames>Xinyu</forenames></author><author><keyname>Xia</keyname><forenames>Chen</forenames></author><author><keyname>Peng</keyname><forenames>Qian</forenames></author><author><keyname>Song</keyname><forenames>Ziqiang</forenames></author><author><keyname>Liu</keyname><forenames>Kang</forenames></author><author><keyname>Wu</keyname><forenames>Xin</forenames></author><author><keyname>Li</keyname><forenames>Jun</forenames></author></authors><title>V2I-Calib++: A Multi-terminal Spatial Calibration Approach in Urban   Intersections for Collaborative Perception</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Urban intersections, dense with pedestrian and vehicular traffic and compounded by GPS signal obstructions from high-rise buildings, are among the most challenging areas in urban traffic systems. Traditional single-vehicle intelligence systems often perform poorly in such environments due to a lack of global traffic flow information and the ability to respond to unexpected events. Vehicle-to-Everything (V2X) technology, through real-time communication between vehicles (V2V) and vehicles to infrastructure (V2I), offers a robust solution. However, practical applications still face numerous challenges. Calibration among heterogeneous vehicle and infrastructure endpoints in multi-end LiDAR systems is crucial for ensuring the accuracy and consistency of perception system data. Most existing multi-end calibration methods rely on initial calibration values provided by positioning systems, but the instability of GPS signals due to high buildings in urban canyons poses severe challenges to these methods. To address this issue, this paper proposes a novel multi-end LiDAR system calibration method that does not require positioning priors to determine initial external parameters and meets real-time requirements. Our method introduces an innovative multi-end perception object association technique, utilizing a new Overall Distance metric (oDist) to measure the spatial association between perception objects, and effectively combines global consistency search algorithms with optimal transport theory. By this means, we can extract co-observed targets from object association results for further external parameter computation and optimization. Extensive comparative and ablation experiments conducted on the simulated dataset V2X-Sim and the real dataset DAIR-V2X confirm the effectiveness and efficiency of our method. The code for this method can be accessed at: https://github.com/MassimoQu/v2i-calib. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.11305</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.11305</id><created>2024-10-15</created><updated>2025-01-31</updated><authors><author><keyname>Zhao</keyname><forenames>Juntao</forenames></author><author><keyname>Lu</keyname><forenames>Wenhao</forenames></author><author><keyname>Wang</keyname><forenames>Sheng</forenames></author><author><keyname>Kong</keyname><forenames>Lingpeng</forenames></author><author><keyname>Wu</keyname><forenames>Chuan</forenames></author></authors><title>QSpec: Speculative Decoding with Complementary Quantization Schemes</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantization has been substantially adopted to accelerate inference and reduce memory consumption of large language models (LLMs). While activation-weight joint quantization speeds up the inference process through low-precision kernels, we demonstrate that it suffers severe performance degradation on multi-step reasoning tasks, rendering it ineffective. We propose a novel quantization paradigm called QSPEC, which seamlessly integrates two complementary quantization schemes for speculative decoding. Leveraging nearly cost-free execution switching, QSPEC drafts tokens with low-precision, fast activation-weight quantization, and verifies them with high-precision weight-only quantization, effectively combining the strengths of both quantization schemes. Compared to high-precision quantization methods, QSPEC empirically boosts token generation throughput by up to 1.64x without any quality compromise, distinguishing it from other low-precision quantization approaches. This enhancement is also consistent across various serving tasks, model sizes, quantization methods, and batch sizes. Compared to state-of-art speculative decoding methods, our approach reuses weights and the KV cache, avoiding extra memory overhead while achieving up to 1.55x speedup in batched serving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play advantage without requiring any training. We believe that QSPEC demonstrates unique strengths for future deployment of high-fidelity quantization schemes, particularly in memory-constrained scenarios (e.g., edge devices). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.11781</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.11781</id><created>2024-10-15</created><updated>2025-02-02</updated><authors><author><keyname>Levy</keyname><forenames>Amit Arnold</forenames></author><author><keyname>Geva</keyname><forenames>Mor</forenames></author></authors><title>Language Models Encode Numbers Using Digit Representations in Base 10</title><categories>cs.LG</categories><comments>Accepted at NAACL 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models (LLMs) frequently make errors when handling even simple numerical problems, such as comparing two small numbers. A natural hypothesis is that these errors stem from how LLMs represent numbers, and specifically, whether their representations of numbers capture their numeric values. We tackle this question from the observation that LLM errors on numerical tasks are often distributed across the digits of the answer rather than normally around its numeric value. Through a series of probing experiments and causal interventions, we show that LLMs internally represent numbers with individual circular representations per-digit in base 10. This digit-wise representation, as opposed to a value representation, sheds light on the error patterns of models on tasks involving numerical reasoning and could serve as a basis for future studies on analyzing numerical mechanisms in LLMs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.12341</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.12341</id><created>2024-10-16</created><updated>2025-02-02</updated><authors><author><keyname>Gambetta</keyname><forenames>Daniele</forenames></author><author><keyname>Gezici</keyname><forenames>Gizem</forenames></author><author><keyname>Giannotti</keyname><forenames>Fosca</forenames></author><author><keyname>Pedreschi</keyname><forenames>Dino</forenames></author><author><keyname>Knott</keyname><forenames>Alistair</forenames></author><author><keyname>Pappalardo</keyname><forenames>Luca</forenames></author></authors><title>Characterizing Model Collapse in Large Language Models Using Semantic   Networks and Next-Token Probability</title><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As synthetic content increasingly infiltrates the web, generative AI models may experience an autophagy process, where they are fine-tuned using their own outputs. This autophagy could lead to a phenomenon known as model collapse, which entails a degradation in the performance and diversity of generative AI models over successive generations. Recent studies have explored the emergence of model collapse across various generative AI models and types of data. However, the current characterizations of model collapse tend to be simplistic and lack comprehensive evaluation. In this article, we conduct a thorough investigation of model collapse across three text datasets, utilizing semantic networks to analyze text repetitiveness and diversity, while employing next-token probabilities to quantify the loss of diversity. We also examine how the proportions of synthetic tokens affect the severity of model collapse and perform cross-dataset evaluations to identify domain-specific variations. By proposing metrics and strategies for a more detailed assessment of model collapse, our study provides new insights for the development of robust generative AI systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.12598</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.12598</id><created>2024-10-16</created><updated>2025-02-03</updated><authors><author><keyname>Donâncio</keyname><forenames>Henrique</forenames></author><author><keyname>Barrier</keyname><forenames>Antoine</forenames></author><author><keyname>South</keyname><forenames>Leah F.</forenames></author><author><keyname>Forbes</keyname><forenames>Florence</forenames></author></authors><title>Dynamic Learning Rate for Deep Reinforcement Learning: A Bandit Approach</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In deep Reinforcement Learning (RL) models trained using gradient-based techniques, the choice of optimizer and its learning rate are crucial to achieving good performance: higher learning rates can prevent the model from learning effectively, while lower ones might slow convergence. Additionally, due to the non-stationarity of the objective function, the best-performing learning rate can change over the training steps. To adapt the learning rate, a standard technique consists of using decay schedulers. However, these schedulers assume that the model is progressively approaching convergence, which may not always be true, leading to delayed or premature adjustments. In this work, we propose dynamic Learning Rate for deep Reinforcement Learning (LRRL), a meta-learning approach that selects the learning rate based on the agent's performance during training. LRRL is based on a multi-armed bandit algorithm, where each arm represents a different learning rate, and the bandit feedback is provided by the cumulative returns of the RL policy to update the arms' probability distribution. Our empirical results demonstrate that LRRL can substantially improve the performance of deep RL algorithms for some tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.12869</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.12869</id><created>2024-10-13</created><updated>2025-02-01</updated><authors><author><keyname>Hu</keyname><forenames>Zhengyu</forenames></author><author><keyname>Zhang</keyname><forenames>Jieyu</forenames></author><author><keyname>Xiong</keyname><forenames>Zhihan</forenames></author><author><keyname>Ratner</keyname><forenames>Alexander</forenames></author><author><keyname>Xiong</keyname><forenames>Hui</forenames></author><author><keyname>Krishna</keyname><forenames>Ranjay</forenames></author></authors><title>Language Model Preference Evaluation with Multiple Weak Evaluators</title><categories>cs.CL cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the remarkable success of Large Language Models (LLMs), evaluating their outputs' quality regarding *preference* remains a critical challenge. Existing works usually leverage an LLM as the judge for comparing LLMs' output pairwisely, yet such model-based evaluator is *weak evaluator* due to *conflicting preference*, i.e., output A is better than B, B than C, but C than A, causing contradictory evaluation results. To address this, we introduce GED (Preference Graph Ensemble and Denoise), a novel approach that leverages multiple model-based evaluators to construct preference graphs, and then ensemble and denoise these graphs for better, non-contradictory evaluation results. In particular, our method consists of two primary stages: aggregating evaluations into a unified graph and applying a denoising process to eliminate cyclic inconsistencies, ensuring a directed acyclic graph (DAG) structure. We provide theoretical guarantees for our framework, demonstrating its efficacy in recovering the ground truth preference structure. Extensive experiments on ten benchmarks demonstrate GED's superiority in three applications: model ranking, response selection, and model alignment tasks. Notably, GED combines small LLM evaluators (e.g., Llama3-8B, Mistral-7B, Qwen2-7B) to outperform stronger ones (e.g., Qwen2-72B), showcasing its effectiveness in enhancing evaluation reliability and improving model performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.13106</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.13106</id><created>2024-10-16</created><updated>2025-01-31</updated><authors><author><keyname>Kuba</keyname><forenames>Jakub Grudzien</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author></authors><title>Cliqueformer: Model-Based Optimization with Structured Transformers</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large neural networks excel at prediction tasks, but their application to design problems, such as protein engineering or materials discovery, requires solving offline model-based optimization (MBO) problems. While predictive models may not directly translate to effective design, recent MBO algorithms incorporate reinforcement learning and generative modeling approaches. Meanwhile, theoretical work suggests that exploiting the target function's structure can enhance MBO performance. We present Cliqueformer, a transformer-based architecture that learns the black-box function's structure through functional graphical models (FGM), addressing distribution shift without relying on explicit conservative approaches. Across various domains, including chemical and genetic design tasks, Cliqueformer demonstrates superior performance compared to existing methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.13184</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.13184</id><created>2024-10-16</created><updated>2025-02-01</updated><authors><author><keyname>He</keyname><forenames>Shwai</forenames></author><author><keyname>Ge</keyname><forenames>Tao</forenames></author><author><keyname>Sun</keyname><forenames>Guoheng</forenames></author><author><keyname>Tian</keyname><forenames>Bowei</forenames></author><author><keyname>Wang</keyname><forenames>Xiaoyang</forenames></author><author><keyname>Li</keyname><forenames>Ang</forenames></author><author><keyname>Yu</keyname><forenames>Dong</forenames></author></authors><title>Router-Tuning: A Simple and Effective Approach for Enabling   Dynamic-Depth in Transformers</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Traditional transformer models often allocate a fixed amount of computational resources to every input token, leading to inefficient and unnecessary computation. To address this, the Mixture of Depths (MoD) was introduced to dynamically adjust the computational depth by skipping less important layers. Despite its promise, current MoD approaches remain under-explored and face two main challenges: (1) \textit{high training costs due to the need to train the entire model along with the routers that determine which layers to skip}, and (2) \textit{the risk of performance degradation when important layers are bypassed}. In response to the first issue, we propose Router-Tuning, a method that fine-tunes only the router on a small dataset, drastically reducing the computational overhead associated with full model training. For the second challenge, we propose MindSkip, which deploys \textit{Attention with Dynamic Depths}. This method preserves the model's performance while significantly enhancing computational and memory efficiency. Extensive experiments demonstrate that our approach delivers competitive results while dramatically improving the computation efficiency, e.g., 21\% speedup and only a 0.2\% performance drop. The code is released at \url{https://github.com/CASE-Lab-UMD/Router-Tuning}. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.13905</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.13905</id><created>2024-10-16</created><updated>2025-02-03</updated><authors><author><keyname>Wang</keyname><forenames>Zheng</forenames></author><author><keyname>Wang</keyname><forenames>Wanwan</forenames></author><author><keyname>Huang</keyname><forenames>Yimin</forenames></author><author><keyname>Peng</keyname><forenames>Zhaopeng</forenames></author><author><keyname>Yang</keyname><forenames>Ziqi</forenames></author><author><keyname>Yao</keyname><forenames>Ming</forenames></author><author><keyname>Wang</keyname><forenames>Cheng</forenames></author><author><keyname>Fan</keyname><forenames>Xiaoliang</forenames></author></authors><title>P4GCN: Vertical Federated Social Recommendation with Privacy-Preserving   Two-Party Graph Convolution Network</title><categories>cs.SI cs.AI cs.IR cs.LG</categories><comments>Accepted by WWW25</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In recent years, graph neural networks (GNNs) have been commonly utilized for social recommendation systems. However, real-world scenarios often present challenges related to user privacy and business constraints, inhibiting direct access to valuable social information from other platforms. While many existing methods have tackled matrix factorization-based social recommendations without direct social data access, developing GNN-based federated social recommendation models under similar conditions remains largely unexplored. To address this issue, we propose a novel vertical federated social recommendation method leveraging privacy-preserving two-party graph convolution networks (P4GCN) to enhance recommendation accuracy without requiring direct access to sensitive social information. First, we introduce a Sandwich-Encryption module to ensure comprehensive data privacy during the collaborative computing process. Second, we provide a thorough theoretical analysis of the privacy guarantees, considering the participation of both curious and honest parties. Extensive experiments on four real-world datasets demonstrate that P4GCN outperforms state-of-the-art methods in terms of recommendation accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.13907</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.13907</id><created>2024-10-16</created><updated>2025-02-02</updated><authors><author><keyname>Zhao</keyname><forenames>Haodong</forenames></author><author><keyname>Hu</keyname><forenames>Jinming</forenames></author><author><keyname>Li</keyname><forenames>Peixuan</forenames></author><author><keyname>Li</keyname><forenames>Fangqi</forenames></author><author><keyname>Sha</keyname><forenames>Jinrui</forenames></author><author><keyname>Ju</keyname><forenames>Tianjie</forenames></author><author><keyname>Chen</keyname><forenames>Peixuan</forenames></author><author><keyname>Zhang</keyname><forenames>Zhuosheng</forenames></author><author><keyname>Liu</keyname><forenames>Gongshen</forenames></author></authors><title>NSmark: Null Space Based Black-box Watermarking Defense Framework for   Language Models</title><categories>cs.CR cs.AI cs.CL</categories><comments>In progress</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Language models (LMs) have emerged as critical intellectual property (IP) assets that necessitate protection. Although various watermarking strategies have been proposed, they remain vulnerable to Linear Functionality Equivalence Attack (LFEA), which can invalidate most existing white-box watermarks without prior knowledge of the watermarking scheme or training data. This paper analyzes and extends the attack scenarios of LFEA to the commonly employed black-box settings for LMs by considering Last-Layer outputs (dubbed LL-LFEA). We discover that the null space of the output matrix remains invariant against LL-LFEA attacks. Based on this finding, we propose NSmark, a black-box watermarking scheme that is task-agnostic and capable of resisting LL-LFEA attacks. NSmark consists of three phases: (i) watermark generation using the digital signature of the owner, enhanced by spread spectrum modulation for increased robustness; (ii) watermark embedding through an output mapping extractor that preserves the LM performance while maximizing watermark capacity; (iii) watermark verification, assessed by extraction rate and null space conformity. Extensive experiments on both pre-training and downstream tasks confirm the effectiveness, scalability, reliability, fidelity, and robustness of our approach. Code is available at https://github.com/dongdongzhaoUP/NSmark. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.14043</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.14043</id><created>2024-10-17</created><updated>2025-02-02</updated><authors><author><keyname>Liu</keyname><forenames>Zefang</forenames></author><author><keyname>Quan</keyname><forenames>Yinzhu</forenames></author></authors><title>Retrieval of Temporal Event Sequences from Textual Descriptions</title><categories>cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Retrieving temporal event sequences from textual descriptions is crucial for applications such as analyzing e-commerce behavior, monitoring social media activities, and tracking criminal incidents. To advance this task, we introduce TESRBench, a comprehensive benchmark for temporal event sequence retrieval (TESR) from textual descriptions. TESRBench includes diverse real-world datasets with synthesized and reviewed textual descriptions, providing a strong foundation for evaluating retrieval performance and addressing challenges in this domain. Building on this benchmark, we propose TPP-Embedding, a novel model for embedding and retrieving event sequences. The model leverages the TPP-LLM framework, integrating large language models (LLMs) with temporal point processes (TPPs) to encode both event texts and times. By pooling representations and applying a contrastive loss, it unifies temporal dynamics and event semantics in a shared embedding space, aligning sequence-level embeddings of event sequences and their descriptions. TPP-Embedding demonstrates superior performance over baseline models across TESRBench datasets, establishing it as a powerful solution for the temporal event sequence retrieval task. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.14170</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.14170</id><created>2024-10-18</created><updated>2025-02-02</updated><authors><author><keyname>Xu</keyname><forenames>Yiyan</forenames></author><author><keyname>Wang</keyname><forenames>Wenjie</forenames></author><author><keyname>Zhang</keyname><forenames>Yang</forenames></author><author><keyname>Tang</keyname><forenames>Biao</forenames></author><author><keyname>Yan</keyname><forenames>Peng</forenames></author><author><keyname>Feng</keyname><forenames>Fuli</forenames></author><author><keyname>He</keyname><forenames>Xiangnan</forenames></author></authors><title>Personalized Image Generation with Large Multimodal Models</title><categories>cs.IR</categories><comments>Accepted for publication in WWW'25</comments><doi>10.1145/3696410.3714843</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Personalized content filtering, such as recommender systems, has become a critical infrastructure to alleviate information overload. However, these systems merely filter existing content and are constrained by its limited diversity, making it difficult to meet users' varied content needs. To address this limitation, personalized content generation has emerged as a promising direction with broad applications. Nevertheless, most existing research focuses on personalized text generation, with relatively little attention given to personalized image generation. The limited work in personalized image generation faces challenges in accurately capturing users' visual preferences and needs from noisy user-interacted images and complex multimodal instructions. Worse still, there is a lack of supervised data for training personalized image generation models.   To overcome the challenges, we propose a Personalized Image Generation Framework named Pigeon, which adopts exceptional large multimodal models with three dedicated modules to capture users' visual preferences and needs from noisy user history and multimodal instructions. To alleviate the data scarcity, we introduce a two-stage preference alignment scheme, comprising masked preference reconstruction and pairwise preference alignment, to align Pigeon with the personalized image generation task. We apply Pigeon to personalized sticker and movie poster generation, where extensive quantitative results and human evaluation highlight its superiority over various generative baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.14569</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.14569</id><created>2024-10-18</created><updated>2025-02-03</updated><authors><author><keyname>Kim</keyname><forenames>Hanna</forenames></author><author><keyname>Song</keyname><forenames>Minkyoo</forenames></author><author><keyname>Na</keyname><forenames>Seung Ho</forenames></author><author><keyname>Shin</keyname><forenames>Seungwon</forenames></author><author><keyname>Lee</keyname><forenames>Kimin</forenames></author></authors><title>When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs</title><categories>cs.CR cs.AI</categories><comments>20 pages, To appear in Usenix Security 2025</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Recent advancements in Large Language Models (LLMs) have established them as agentic systems capable of planning and interacting with various tools. These LLM agents are often paired with web-based tools, enabling access to diverse sources and real-time information. Although these advancements offer significant benefits across various applications, they also increase the risk of malicious use, particularly in cyberattacks involving personal information. In this work, we investigate the risks associated with misuse of LLM agents in cyberattacks involving personal data. Specifically, we aim to understand: 1) how potent LLM agents can be when directed to conduct cyberattacks, 2) how cyberattacks are enhanced by web-based tools, and 3) how affordable and easy it becomes to launch cyberattacks using LLM agents. We examine three attack scenarios: the collection of Personally Identifiable Information (PII), the generation of impersonation posts, and the creation of spear-phishing emails. Our experiments reveal the effectiveness of LLM agents in these attacks: LLM agents achieved a precision of up to 95.9% in collecting PII, generated impersonation posts where 93.9% of them were deemed authentic, and boosted click rate of phishing links in spear phishing emails by 46.67%. Additionally, our findings underscore the limitations of existing safeguards in contemporary commercial LLMs, emphasizing the urgent need for robust security measures to prevent the misuse of LLM agents. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.14874</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.14874</id><created>2024-10-18</created><updated>2025-01-31</updated><authors><author><keyname>Zhang</keyname><forenames>Tianxiao</forenames></author><author><keyname>Luo</keyname><forenames>Bo</forenames></author><author><keyname>Wang</keyname><forenames>Guanghui</forenames></author></authors><title>Improving Vision Transformers by Overlapping Heads in Multi-Head   Self-Attention</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vision Transformers have made remarkable progress in recent years, achieving state-of-the-art performance in most vision tasks. A key component of this success is due to the introduction of the Multi-Head Self-Attention (MHSA) module, which enables each head to learn different representations by applying the attention mechanism independently. In this paper, we empirically demonstrate that Vision Transformers can be further enhanced by overlapping the heads in MHSA. We introduce Multi-Overlapped-Head Self-Attention (MOHSA), where heads are overlapped with their two adjacent heads for queries, keys, and values, while zero-padding is employed for the first and last heads, which have only one neighboring head. Various paradigms for overlapping ratios are proposed to fully investigate the optimal performance of our approach. The proposed approach is evaluated using five Transformer models on four benchmark datasets and yields a significant performance boost. The source code will be made publicly available upon publication. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.15178</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.15178</id><created>2024-10-19</created><updated>2025-02-02</updated><authors><author><keyname>Puthumanaillam</keyname><forenames>Gokul</forenames></author><author><keyname>Padrao</keyname><forenames>Paulo</forenames></author><author><keyname>Fuentes</keyname><forenames>Jose</forenames></author><author><keyname>Bobadilla</keyname><forenames>Leonardo</forenames></author><author><keyname>Ornik</keyname><forenames>Melkior</forenames></author></authors><title>GUIDEd Agents: Enhancing Navigation Policies through Task-Specific   Uncertainty Abstraction in Localization-Limited Environments</title><categories>cs.RO cs.AI cs.LG cs.SY eess.SY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Autonomous vehicles performing navigation tasks in complex environments face significant challenges due to uncertainty in state estimation. In many scenarios, such as stealth operations or resource-constrained settings, accessing high-precision localization comes at a significant cost, forcing robots to rely primarily on less precise state estimates. Our key observation is that different tasks require varying levels of precision in different regions: a robot navigating a crowded space might need precise localization near obstacles but can operate effectively with less precision elsewhere. In this paper, we present a planning method for integrating task-specific uncertainty requirements directly into navigation policies. We introduce Task-Specific Uncertainty Maps (TSUMs), which abstract the acceptable levels of state estimation uncertainty across different regions. TSUMs align task requirements and environmental features using a shared representation space, generated via a domain-adapted encoder. Using TSUMs, we propose Generalized Uncertainty Integration for Decision-Making and Execution (GUIDE), a policy conditioning framework that incorporates these uncertainty requirements into robot decision-making. We find that TSUMs provide an effective way to abstract task-specific uncertainty requirements, and conditioning policies on TSUMs enables the robot to reason about the context-dependent value of certainty and adapt its behavior accordingly. We show how integrating GUIDE into reinforcement learning frameworks allows the agent to learn navigation policies that effectively balance task completion and uncertainty management without explicit reward engineering. We evaluate GUIDE on various real-world robotic navigation tasks and find that it demonstrates significant improvement in task completion rates compared to baseline methods that do not explicitly consider task-specific uncertainty. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.15729</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.15729</id><created>2024-10-21</created><updated>2025-02-02</updated><authors><author><keyname>Montreuil</keyname><forenames>Yannis</forenames></author><author><keyname>Yeo</keyname><forenames>Shu Heng</forenames></author><author><keyname>Carlier</keyname><forenames>Axel</forenames></author><author><keyname>Ng</keyname><forenames>Lai Xing</forenames></author><author><keyname>Ooi</keyname><forenames>Wei Tsang</forenames></author></authors><title>A Two-Stage Learning-to-Defer Approach for Multi-Task Learning</title><categories>stat.ML cs.HC cs.LG</categories><comments>32 pages, 17 main paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Two-Stage Learning-to-Defer framework has been extensively studied for classification and, more recently, regression tasks. However, many contemporary applications involve both classification and regression in an interdependent manner. In this work, we introduce a novel Two-Stage Learning-to-Defer framework for multi-task learning that jointly addresses these tasks. Our approach leverages a two-stage surrogate loss family, which we prove to be both ($\mathcal{G}, \mathcal{R}$)-consistent and Bayes-consistent, providing strong theoretical guarantees of convergence to the Bayes-optimal rejector. We establish consistency bounds explicitly linked to the cross-entropy surrogate family and the $L_1$-norm of the agents' costs, extending the theoretical minimizability gap analysis to the two-stage setting with multiple experts. We validate our framework on two challenging tasks: object detection, where classification and regression are tightly coupled, and existing methods fail, and electronic health record analysis, in which we highlight the suboptimality of current learning-to-defer approaches. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.16212</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.16212</id><created>2024-10-21</created><updated>2025-01-31</updated><authors><author><keyname>Zablocki</keyname><forenames>L. I.</forenames></author><author><keyname>Bugnon</keyname><forenames>L. A.</forenames></author><author><keyname>Gerard</keyname><forenames>M.</forenames></author><author><keyname>Di Persia</keyname><forenames>L.</forenames></author><author><keyname>Stegmayer</keyname><forenames>G.</forenames></author><author><keyname>Milone</keyname><forenames>D. H.</forenames></author></authors><title>Comprehensive benchmarking of large language models for RNA secondary   structure prediction</title><categories>cs.AI cs.LG q-bio.BM</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Inspired by the success of large language models (LLM) for DNA and proteins, several LLM for RNA have been developed recently. RNA-LLM uses large datasets of RNA sequences to learn, in a self-supervised way, how to represent each RNA base with a semantically rich numerical vector. This is done under the hypothesis that obtaining high-quality RNA representations can enhance data-costly downstream tasks. Among them, predicting the secondary structure is a fundamental task for uncovering RNA functional mechanisms. In this work we present a comprehensive experimental analysis of several pre-trained RNA-LLM, comparing them for the RNA secondary structure prediction task in an unified deep learning framework. The RNA-LLM were assessed with increasing generalization difficulty on benchmark datasets. Results showed that two LLM clearly outperform the other models, and revealed significant challenges for generalization in low-homology scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.16484</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.16484</id><created>2024-10-21</created><updated>2025-02-01</updated><authors><author><keyname>Gao</keyname><forenames>Tian</forenames></author><author><keyname>Dhurandhar</keyname><forenames>Amit</forenames></author><author><keyname>Ramamurthy</keyname><forenames>Karthikeyan Natesan</forenames></author><author><keyname>Wei</keyname><forenames>Dennis</forenames></author></authors><title>Identifying Sub-networks in Neural Networks via Functionally Similar   Representations</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Providing human-understandable insights into the inner workings of neural networks is an important step toward achieving more explainable and trustworthy AI. Existing approaches to such mechanistic interpretability typically require substantial prior knowledge and manual effort, with strategies tailored to specific tasks. In this work, we take a step toward automating the understanding of the network by investigating the existence of distinct sub-networks. Specifically, we explore a novel automated and task-agnostic approach based on the notion of functionally similar representations within neural networks to identify similar and dissimilar layers, revealing potential sub-networks. We achieve this by proposing, for the first time to our knowledge, the use of Gromov-Wasserstein distance, which overcomes challenges posed by varying distributions and dimensionalities across intermediate representations, issues that complicate direct layer to layer comparisons. On algebraic, language, and vision tasks, we observe the emergence of sub-groups within neural network layers corresponding to functional abstractions. Through downstream applications of model compression and fine-tuning, we show the proposed approach offers meaningful insights into the behavior of neural networks with minimal human and computational cost. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.16759</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.16759</id><created>2024-10-22</created><updated>2025-02-01</updated><authors><author><keyname>Krestinskaya</keyname><forenames>Olga</forenames></author><author><keyname>Fouda</keyname><forenames>Mohammed E.</forenames></author><author><keyname>Eltawil</keyname><forenames>Ahmed</forenames></author><author><keyname>Salama</keyname><forenames>Khaled N.</forenames></author></authors><title>Towards Efficient IMC Accelerator Design Through Joint Hardware-Workload   Co-optimization</title><categories>cs.AR cs.AI</categories><comments>accepted to ISCAS 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Designing generalized in-memory computing (IMC) hardware that efficiently supports a variety of workloads requires extensive design space exploration, which is infeasible to perform manually. Optimizing hardware individually for each workload or solely for the largest workload often fails to yield the most efficient generalized solutions. To address this, we propose a joint hardware-workload optimization framework that identifies optimised IMC chip architecture parameters, enabling more efficient, workload-flexible hardware. We show that joint optimization achieves 36%, 36%, 20%, and 69% better energy-latency-area scores for VGG16, ResNet18, AlexNet, and MobileNetV3, respectively, compared to the separate architecture parameters search optimizing for a single largest workload. Additionally, we quantify the performance trade-offs and losses of the resulting generalized IMC hardware compared to workload-specific IMC designs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.17052</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.17052</id><created>2024-10-22</created><updated>2025-02-02</updated><authors><author><keyname>Tong</keyname><forenames>Meng</forenames></author><author><keyname>Chen</keyname><forenames>Kejiang</forenames></author><author><keyname>Yuan</keyname><forenames>Xiaojian</forenames></author><author><keyname>Liu</keyname><forenames>Jiayang</forenames></author><author><keyname>Zhang</keyname><forenames>Weiming</forenames></author><author><keyname>Yu</keyname><forenames>Nenghai</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author></authors><title>On the Vulnerability of Text Sanitization</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Text sanitization, which employs differential privacy to replace sensitive tokens with new ones, represents a significant technique for privacy protection. Typically, its performance in preserving privacy is evaluated by measuring the attack success rate (ASR) of reconstruction attacks, where attackers attempt to recover the original tokens from the sanitized ones. However, current reconstruction attacks on text sanitization are developed empirically, making it challenging to accurately assess the effectiveness of sanitization. In this paper, we aim to provide a more accurate evaluation of sanitization effectiveness. Inspired by the works of Palamidessi et al., we implement theoretically optimal reconstruction attacks targeting text sanitization. We derive their bounds on ASR as benchmarks for evaluating sanitization performance. For real-world applications, we propose two practical reconstruction attacks based on these theoretical findings. Our experimental results underscore the necessity of reassessing these overlooked risks. Notably, one of our attacks achieves a 46.4% improvement in ASR over the state-of-the-art baseline, with a privacy budget of epsilon=4.0 on the SST-2 dataset. Our code is available at: https://github.com/mengtong0110/On-the-Vulnerability-of-Text-Sanitization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.17241</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.17241</id><created>2024-10-22</created><updated>2025-02-01</updated><authors><author><keyname>Ji</keyname><forenames>Ge-Peng</forenames></author><author><keyname>Liu</keyname><forenames>Jingyi</forenames></author><author><keyname>Xu</keyname><forenames>Peng</forenames></author><author><keyname>Barnes</keyname><forenames>Nick</forenames></author><author><keyname>Khan</keyname><forenames>Fahad Shahbaz</forenames></author><author><keyname>Khan</keyname><forenames>Salman</forenames></author><author><keyname>Fan</keyname><forenames>Deng-Ping</forenames></author></authors><title>Frontiers in Intelligent Colonoscopy</title><categories>eess.IV cs.CV</categories><comments>[Work in progress] A comprehensive survey of intelligent colonoscopy   in the multimodal era. [Updated Version V2] New training strategy for   colonoscopy-specific multimodal language model</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Colonoscopy is currently one of the most sensitive screening methods for colorectal cancer. This study investigates the frontiers of intelligent colonoscopy techniques and their prospective implications for multimodal medical applications. With this goal, we begin by assessing the current data-centric and model-centric landscapes through four tasks for colonoscopic scene perception, including classification, detection, segmentation, and vision-language understanding. This assessment enables us to identify domain-specific challenges and reveals that multimodal research in colonoscopy remains open for further exploration. To embrace the coming multimodal era, we establish three foundational initiatives: a large-scale multimodal instruction tuning dataset ColonINST, a colonoscopy-designed multimodal language model ColonGPT, and a multimodal benchmark. To facilitate ongoing monitoring of this rapidly evolving field, we provide a public website for the latest updates: https://github.com/ai4colonoscopy/IntelliScope. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.17600</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.17600</id><created>2024-10-23</created><updated>2025-02-03</updated><authors><author><keyname>Yang</keyname><forenames>Rui</forenames></author><author><keyname>Yang</keyname><forenames>Boming</forenames></author><author><keyname>Feng</keyname><forenames>Aosong</forenames></author><author><keyname>Ouyang</keyname><forenames>Sixun</forenames></author><author><keyname>Blum</keyname><forenames>Moritz</forenames></author><author><keyname>She</keyname><forenames>Tianwei</forenames></author><author><keyname>Jiang</keyname><forenames>Yuang</forenames></author><author><keyname>Lecue</keyname><forenames>Freddy</forenames></author><author><keyname>Lu</keyname><forenames>Jinghui</forenames></author><author><keyname>Li</keyname><forenames>Irene</forenames></author></authors><title>Graphusion: A RAG Framework for Knowledge Graph Construction with a   Global Perspective</title><categories>cs.CL cs.AI cs.DB</categories><comments>arXiv admin note: substantial text overlap with arXiv:2407.10794</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge Graphs (KGs) are crucial in the field of artificial intelligence and are widely used in downstream tasks, such as question-answering (QA). The construction of KGs typically requires significant effort from domain experts. Large Language Models (LLMs) have recently been used for Knowledge Graph Construction (KGC). However, most existing approaches focus on a local perspective, extracting knowledge triplets from individual sentences or documents, missing a fusion process to combine the knowledge in a global KG. This work introduces Graphusion, a zero-shot KGC framework from free text. It contains three steps: in Step 1, we extract a list of seed entities using topic modeling to guide the final KG includes the most relevant entities; in Step 2, we conduct candidate triplet extraction using LLMs; in Step 3, we design the novel fusion module that provides a global view of the extracted knowledge, incorporating entity merging, conflict resolution, and novel triplet discovery. Results show that Graphusion achieves scores of 2.92 and 2.37 out of 3 for entity extraction and relation recognition, respectively. Moreover, we showcase how Graphusion could be applied to the Natural Language Processing (NLP) domain and validate it in an educational scenario. Specifically, we introduce TutorQA, a new expert-verified benchmark for QA, comprising six tasks and a total of 1,200 QA pairs. Using the Graphusion-constructed KG, we achieve a significant improvement on the benchmark, for example, a 9.2% accuracy improvement on sub-graph completion. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.18032</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.18032</id><created>2024-10-23</created><updated>2025-02-01</updated><authors><author><keyname>Li</keyname><forenames>Xin</forenames></author><author><keyname>Chu</keyname><forenames>Qizhi</forenames></author><author><keyname>Chen</keyname><forenames>Yubin</forenames></author><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Liu</keyname><forenames>Yaoqi</forenames></author><author><keyname>Yu</keyname><forenames>Zekai</forenames></author><author><keyname>Chen</keyname><forenames>Weize</forenames></author><author><keyname>Qian</keyname><forenames>Chen</forenames></author><author><keyname>Shi</keyname><forenames>Chuan</forenames></author><author><keyname>Yang</keyname><forenames>Cheng</forenames></author></authors><title>GraphTeam: Facilitating Large Language Model-based Graph Analysis via   Multi-Agent Collaboration</title><categories>cs.AI cs.CL cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphs are widely used for modeling relational data in real-world scenarios, such as social networks and urban computing. Existing LLM-based graph analysis approaches either integrate graph neural networks (GNNs) for specific machine learning tasks, limiting their transferability, or rely solely on LLMs' internal reasoning ability, resulting in suboptimal performance. To address these limitations, we take advantage of recent advances in LLM-based agents, which have shown capabilities of utilizing external knowledge or tools for problem solving. By simulating human problem-solving strategies such as analogy and collaboration, we propose a multi-agent system based on LLMs named GraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from three modules, and the agents with different specialities can collaborate with each other to address complex problems. Specifically, (1) input-output normalization module: the question agent extracts and refines four key arguments from the original question, facilitating the problem understanding, and the answer agent organizes the results to meet the output requirement; (2) external knowledge retrieval module: we first build a knowledge base consisting of relevant documentation and experience information, and then the search agent retrieves the most relevant entries for each question. (3) problem-solving module: given the retrieved information from search agent, the coding agent uses established algorithms via programming to generate solutions, and in case the coding agent does not work, the reasoning agent will directly compute the results without programming. Extensive experiments on six graph analysis benchmarks demonstrate that GraphTeam achieves state-of-the-art performance with an average 25.85% improvement over the best baseline in terms of accuracy. The code and data are available at https://github.com/BUPT-GAMMA/GraphTeam. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.18268</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.18268</id><created>2024-10-23</created><updated>2025-01-31</updated><authors><author><keyname>Adrian</keyname><forenames>Melissa</forenames></author><author><keyname>Soloff</keyname><forenames>Jake A.</forenames></author><author><keyname>Willett</keyname><forenames>Rebecca</forenames></author></authors><title>Stabilizing black-box model selection with the inflated argmax</title><categories>stat.ML cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Model selection is the process of choosing from a class of candidate models given data. For instance, methods such as the LASSO and sparse identification of nonlinear dynamics (SINDy) formulate model selection as finding a sparse solution to a linear system of equations determined by training data. However, absent strong assumptions, such methods are highly unstable: if a single data point is removed from the training set, a different model may be selected. In this paper, we present a new approach to stabilizing model selection with theoretical stability guarantees that leverages a combination of bagging and an ''inflated'' argmax operation. Our method selects a small collection of models that all fit the data, and it is stable in that, with high probability, the removal of any training point will result in a collection of selected models that overlaps with the original collection. We illustrate this method in (a) a simulation in which strongly correlated covariates make standard LASSO model selection highly unstable, (b) a Lotka-Volterra model selection problem focused on identifying how competition in an ecosystem influences species' abundances, and (c) a graph subset selection problem using cell-signaling data from proteomics. In these settings, the proposed method yields stable, compact, and accurate collections of selected models, outperforming a variety of benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.18676</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.18676</id><created>2024-10-24</created><updated>2025-02-02</updated><authors><author><keyname>Bao</keyname><forenames>Linus</forenames></author><author><keyname>Jin</keyname><forenames>Emily</forenames></author><author><keyname>Bronstein</keyname><forenames>Michael</forenames></author><author><keyname>Ceylan</keyname><forenames>İsmail İlkan</forenames></author><author><keyname>Lanzinger</keyname><forenames>Matthias</forenames></author></authors><title>Homomorphism Counts as Structural Encodings for Graph Learning</title><categories>cs.LG</categories><comments>Proceedings of the Thirteenth International Conference on Learning   Representations (ICLR 202R). Code available at:   https://github.com/linusbao/MoSE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph Transformers are popular neural networks that extend the well-known Transformer architecture to the graph domain. These architectures operate by applying self-attention on graph nodes and incorporating graph structure through the use of positional encodings (e.g., Laplacian positional encoding) or structural encodings (e.g., random-walk structural encoding). The quality of such encodings is critical, since they provide the necessary $\textit{graph inductive biases}$ to condition the model on graph structure. In this work, we propose $\textit{motif structural encoding}$ (MoSE) as a flexible and powerful structural encoding framework based on counting graph homomorphisms. Theoretically, we compare the expressive power of MoSE to random-walk structural encoding and relate both encodings to the expressive power of standard message passing neural networks. Empirically, we observe that MoSE outperforms other well-known positional and structural encodings across a range of architectures, and it achieves state-of-the-art performance on a widely studied molecular property prediction dataset. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.19321</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.19321</id><created>2024-10-25</created><updated>2025-01-31</updated><authors><author><keyname>Chen</keyname><forenames>Mengmeng</forenames></author><author><keyname>Wu</keyname><forenames>Xiaohu</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoli</forenames></author><author><keyname>He</keyname><forenames>Tiantian</forenames></author><author><keyname>Ong</keyname><forenames>Yew-Soon</forenames></author><author><keyname>Liu</keyname><forenames>Qiqi</forenames></author><author><keyname>Lao</keyname><forenames>Qicheng</forenames></author><author><keyname>Yu</keyname><forenames>Han</forenames></author></authors><title>Free-Rider and Conflict Aware Collaboration Formation for Cross-Silo   Federated Learning</title><categories>cs.GT cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Federated learning (FL) is a machine learning paradigm that allows multiple FL participants (FL-PTs) to collaborate on training models without sharing private data. Due to data heterogeneity, negative transfer may occur in the FL training process. This necessitates FL-PT selection based on their data complementarity. In cross-silo FL, organizations that engage in business activities are key sources of FL-PTs. The resulting FL ecosystem has two features: (i) self-interest, and (ii) competition among FL-PTs. This requires the desirable FL-PT selection strategy to simultaneously mitigate the problems of free riders and conflicts of interest among competitors. To this end, we propose an optimal FL collaboration formation strategy -- FedEgoists -- which ensures that: (1) a FL-PT can benefit from FL if and only if it benefits the FL ecosystem, and (2) a FL-PT will not contribute to its competitors or their supporters. It provides an efficient clustering solution to group FL-PTs into coalitions, ensuring that within each coalition, FL-PTs share the same interest. We theoretically prove that the FL-PT coalitions formed are optimal since no coalitions can collaborate together to improve the utility of any of their members. Extensive experiments on widely adopted benchmark datasets demonstrate the effectiveness of FedEgoists compared to nine state-of-the-art baseline methods, and its ability to establish efficient collaborative networks in cross-silos FL with FL-PTs that engage in business activities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.19704</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.19704</id><created>2024-10-25</created><updated>2025-01-31</updated><authors><author><keyname>Suryanarayanan</keyname><forenames>Parthasarathy</forenames></author><author><keyname>Qiu</keyname><forenames>Yunguang</forenames></author><author><keyname>Sethi</keyname><forenames>Shreyans</forenames></author><author><keyname>Mahajan</keyname><forenames>Diwakar</forenames></author><author><keyname>Li</keyname><forenames>Hongyang</forenames></author><author><keyname>Yang</keyname><forenames>Yuxin</forenames></author><author><keyname>Eyigoz</keyname><forenames>Elif</forenames></author><author><keyname>Saenz</keyname><forenames>Aldo Guzman</forenames></author><author><keyname>Platt</keyname><forenames>Daniel E.</forenames></author><author><keyname>Rumbell</keyname><forenames>Timothy H.</forenames></author><author><keyname>Ng</keyname><forenames>Kenney</forenames></author><author><keyname>Dey</keyname><forenames>Sanjoy</forenames></author><author><keyname>Burch</keyname><forenames>Myson</forenames></author><author><keyname>Kwon</keyname><forenames>Bum Chul</forenames></author><author><keyname>Meyer</keyname><forenames>Pablo</forenames></author><author><keyname>Cheng</keyname><forenames>Feixiong</forenames></author><author><keyname>Hu</keyname><forenames>Jianying</forenames></author><author><keyname>Morrone</keyname><forenames>Joseph A.</forenames></author></authors><title>Multi-view biomedical foundation models for molecule-target and property   prediction</title><categories>q-bio.BM cs.AI cs.LG</categories><comments>37 pages including supplement. 10 figures, 8 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Foundation models applied to bio-molecular space hold promise to accelerate drug discovery. Molecular representation is key to building such models. Previous works have typically focused on a single representation or view of the molecules. Here, we develop a multi-view foundation model approach, that integrates molecular views of graph, image and text. Single-view foundation models are each pre-trained on a dataset of up to 200M molecules and then aggregated into combined representations. Our multi-view model is validated on a diverse set of 18 tasks, encompassing ligand-protein binding, molecular solubility, metabolism and toxicity. We show that the multi-view models perform robustly and are able to balance the strengths and weaknesses of specific views. We then apply this model to screen compounds against a large (&gt;100 targets) set of G Protein-Coupled receptors (GPCRs). From this library of targets, we identify 33 that are related to Alzheimer's disease. On this subset, we employ our model to identify strong binders, which are validated through structure-based modeling and identification of key binding motifs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.20140</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.20140</id><created>2024-10-26</created><updated>2025-01-31</updated><authors><author><keyname>Lakara</keyname><forenames>Kumud</forenames></author><author><keyname>Channing</keyname><forenames>Georgia</forenames></author><author><keyname>Sock</keyname><forenames>Juil</forenames></author><author><keyname>Rupprecht</keyname><forenames>Christian</forenames></author><author><keyname>Torr</keyname><forenames>Philip</forenames></author><author><keyname>Collomosse</keyname><forenames>John</forenames></author><author><keyname>de Witt</keyname><forenames>Christian Schroeder</forenames></author></authors><title>LLM-Consensus: Multi-Agent Debate for Visual Misinformation Detection</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  One of the most challenging forms of misinformation involves the out-of-context (OOC) use of images paired with misleading text, creating false narratives. Existing AI-driven detection systems lack explainability and require expensive finetuning. We address these issues with LLM-Consensus, a multi-agent debate system for OOC misinformation detection. LLM-Consensus introduces a novel multi-agent debate framework where multimodal agents collaborate to assess contextual consistency and request external information to enhance cross-context reasoning and decision-making. Our framework enables explainable detection with state-of-the-art accuracy even without domain-specific fine-tuning. Extensive ablation studies confirm that external retrieval significantly improves detection accuracy, and user studies demonstrate that LLM-Consensus boosts performance for both experts and non-experts. These results position LLM-Consensus as a powerful tool for autonomous and citizen intelligence applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.20158</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.20158</id><created>2024-10-26</created><updated>2025-02-02</updated><authors><author><keyname>Chen</keyname><forenames>Wenlong</forenames></author><author><keyname>Chen</keyname><forenames>Wenlin</forenames></author><author><keyname>Rastrelli</keyname><forenames>Lapo</forenames></author><author><keyname>Li</keyname><forenames>Yingzhen</forenames></author></authors><title>Your Image is Secretly the Last Frame of a Pseudo Video</title><categories>cs.CV cs.LG</categories><comments>19 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Diffusion models, which can be viewed as a special case of hierarchical variational autoencoders (HVAEs), have shown profound success in generating photo-realistic images. In contrast, standard HVAEs often produce images of inferior quality compared to diffusion models. In this paper, we hypothesize that the success of diffusion models can be partly attributed to the additional self-supervision information for their intermediate latent states provided by corrupted images, which along with the original image form a pseudo video. Based on this hypothesis, we explore the possibility of improving other types of generative models with such pseudo videos. Specifically, we first extend a given image generative model to their video generative model counterpart, and then train the video generative model on pseudo videos constructed by applying data augmentation to the original images. Furthermore, we analyze the potential issues of first-order Markov data augmentation methods, which are typically used in diffusion models, and propose to use more expressive data augmentation to construct more useful information in pseudo videos. Our empirical results on the CIFAR10 and CelebA datasets demonstrate that improved image generation quality can be achieved with additional self-supervised information from pseudo videos. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.20660</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.20660</id><created>2024-10-27</created><updated>2025-02-01</updated><authors><author><keyname>Yoo</keyname><forenames>Kiwoong</forenames></author><author><keyname>Oertell</keyname><forenames>Owen</forenames></author><author><keyname>Lee</keyname><forenames>Junhyun</forenames></author><author><keyname>Lee</keyname><forenames>Sanghoon</forenames></author><author><keyname>Kang</keyname><forenames>Jaewoo</forenames></author></authors><title>TurboHopp: Accelerated Molecule Scaffold Hopping with Consistency Models</title><categories>cs.LG cs.AI q-bio.BM</categories><comments>22 pages, 11 figures, 8 tables. Presented at NeurIPS 2024</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Navigating the vast chemical space of druggable compounds is a formidable challenge in drug discovery, where generative models are increasingly employed to identify viable candidates. Conditional 3D structure-based drug design (3D-SBDD) models, which take into account complex three-dimensional interactions and molecular geometries, are particularly promising. Scaffold hopping is an efficient strategy that facilitates the identification of similar active compounds by strategically modifying the core structure of molecules, effectively narrowing the wide chemical space and enhancing the discovery of drug-like products. However, the practical application of 3D-SBDD generative models is hampered by their slow processing speeds. To address this bottleneck, we introduce TurboHopp, an accelerated pocket-conditioned 3D scaffold hopping model that merges the strategic effectiveness of traditional scaffold hopping with rapid generation capabilities of consistency models. This synergy not only enhances efficiency but also significantly boosts generation speeds, achieving up to 30 times faster inference speed as well as superior generation quality compared to existing diffusion-based models, establishing TurboHopp as a powerful tool in drug discovery. Supported by faster inference speed, we further optimize our model, using Reinforcement Learning for Consistency Models (RLCM), to output desirable molecules. We demonstrate the broad applicability of TurboHopp across multiple drug discovery scenarios, underscoring its potential in diverse molecular settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.20724</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.20724</id><created>2024-10-28</created><updated>2025-01-31</updated><authors><author><keyname>Li</keyname><forenames>Mufei</forenames></author><author><keyname>Miao</keyname><forenames>Siqi</forenames></author><author><keyname>Li</keyname><forenames>Pan</forenames></author></authors><title>Simple Is Effective: The Roles of Graphs and Large Language Models in   Knowledge-Graph-Based Retrieval-Augmented Generation</title><categories>cs.CL cs.LG</categories><comments>Accepted by ICLR 2025; Code available at   https://github.com/Graph-COM/SubgraphRAG</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) demonstrate strong reasoning abilities but face limitations such as hallucinations and outdated knowledge. Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by grounding LLM outputs in structured external knowledge from KGs. However, current KG-based RAG frameworks still struggle to optimize the trade-off between retrieval effectiveness and efficiency in identifying a suitable amount of relevant graph information for the LLM to digest. We introduce SubgraphRAG, extending the KG-based RAG framework that retrieves subgraphs and leverages LLMs for reasoning and answer prediction. Our approach innovatively integrates a lightweight multilayer perceptron with a parallel triple-scoring mechanism for efficient and flexible subgraph retrieval while encoding directional structural distances to enhance retrieval effectiveness. The size of retrieved subgraphs can be flexibly adjusted to match the query's need and the downstream LLM's capabilities. This design strikes a balance between model complexity and reasoning power, enabling scalable and generalizable retrieval processes. Notably, based on our retrieved subgraphs, smaller LLMs like Llama3.1-8B-Instruct deliver competitive results with explainable reasoning, while larger models like GPT-4o achieve state-of-the-art accuracy compared with previous baselines -- all without fine-tuning. Extensive evaluations on the WebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency, accuracy, and reliability by reducing hallucinations and improving response grounding. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.21259</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.21259</id><created>2024-10-28</created><updated>2025-02-01</updated><authors><author><keyname>Bao</keyname><forenames>Han</forenames></author><author><keyname>Huang</keyname><forenames>Yue</forenames></author><author><keyname>Wang</keyname><forenames>Yanbo</forenames></author><author><keyname>Ye</keyname><forenames>Jiayi</forenames></author><author><keyname>Wang</keyname><forenames>Xiangqi</forenames></author><author><keyname>Chen</keyname><forenames>Xiuying</forenames></author><author><keyname>Zhao</keyname><forenames>Yue</forenames></author><author><keyname>Zhou</keyname><forenames>Tianyi</forenames></author><author><keyname>Elhoseiny</keyname><forenames>Mohamed</forenames></author><author><keyname>Zhang</keyname><forenames>Xiangliang</forenames></author></authors><title>AutoBench-V: Can Large Vision-Language Models Benchmark Themselves?</title><categories>cs.CV cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Vision-Language Models (LVLMs) have become essential for advancing the integration of visual and linguistic information. However, the evaluation of LVLMs presents significant challenges as the evaluation benchmark always demands lots of human cost for its construction, and remains static, lacking flexibility once constructed. Even though automatic evaluation has been explored in textual modality, the visual modality remains under-explored. As a result, in this work, we address a question: "Can LVLMs themselves be used to benchmark each other in the visual automatically domain?". We introduce AutoBench-V, an automated framework for serving evaluation on demand, i.e., benchmarking LVLMs based on specific aspects of model capability. AutoBench-V leverages text-to-image models to generate relevant image samples and then utilizes LVLMs to orchestrate visual question-answering (VQA) tasks, completing the evaluation process efficiently and flexibly. Through an extensive evaluation of nine popular LVLMs across five demanded user inputs (i.e., evaluation capabilities), the framework shows effectiveness and reliability. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.23073</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.23073</id><created>2024-10-30</created><updated>2025-02-01</updated><authors><author><keyname>Chen</keyname><forenames>Hongyu</forenames></author><author><keyname>Chen</keyname><forenames>Chengcheng</forenames></author><author><keyname>Wang</keyname><forenames>Fei</forenames></author><author><keyname>Shi</keyname><forenames>Yuhu</forenames></author><author><keyname>Zeng</keyname><forenames>Weiming</forenames></author></authors><title>RSNet: A Light Framework for The Detection of Multi-scale Remote Sensing   Targets</title><categories>cs.CV eess.IV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent advancements in synthetic aperture radar (SAR) ship detection using deep learning have significantly improved accuracy and speed, yet effectively detecting small objects in complex backgrounds with fewer parameters remains a challenge. This letter introduces RSNet, a lightweight framework constructed to enhance ship detection in SAR imagery. To ensure accuracy with fewer parameters, we proposed Waveletpool-ContextGuided (WCG) as its backbone, guiding global context understanding through multi-scale wavelet features for effective detection in complex scenes. Additionally, Waveletpool-StarFusion (WSF) is introduced as the neck, employing a residual wavelet element-wise multiplication structure to achieve higher dimensional nonlinear features without increasing network width. The Lightweight-Shared (LS) module is designed as detect components to achieve efficient detection through lightweight shared convolutional structure and multi-format compatibility. Experiments on the SAR Ship Detection Dataset (SSDD) and High-Resolution SAR Image Dataset (HRSID) demonstrate that RSNet achieves a strong balance between lightweight design and detection performance, surpassing many state-of-the-art detectors, reaching 72.5\% and 67.6\% in \textbf{\(\mathbf{mAP_{.50:.95}}\) }respectively with 1.49M parameters. Our code will be released soon. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.23143</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.23143</id><created>2024-10-30</created><updated>2025-02-01</updated><authors><author><keyname>Bhattacharya</keyname><forenames>Haimanti</forenames></author><author><keyname>Dugar</keyname><forenames>Subhasish</forenames></author><author><keyname>Hazra</keyname><forenames>Sanchaita</forenames></author><author><keyname>Majumder</keyname><forenames>Bodhisattwa Prasad</forenames></author></authors><title>The Good, the Bad, and the Ugly: The Role of AI Quality Disclosure in   Lie Detection</title><categories>cs.CL cs.AI cs.CY cs.HC cs.LG</categories><comments>Corresponding author: Sanchaita Hazra. Order of the authors are in   alphabetical order of their last names. All authors contributed equally. The   manuscript is under review. 74 Pages, including appendices and references</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We investigate how low-quality AI advisors, lacking quality disclosures, can help spread text-based lies while seeming to help people detect lies. Participants in our experiment discern truth from lies by evaluating transcripts from a game show that mimicked deceptive social media exchanges on topics with objective truths. We find that when relying on low-quality advisors without disclosures, participants' truth-detection rates fall below their own abilities, which recovered once the AI's true effectiveness was revealed. Conversely, high-quality advisor enhances truth detection, regardless of disclosure. We discover that participants' expectations about AI capabilities contribute to their undue reliance on opaque, low-quality advisors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.23714</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.23714</id><created>2024-10-31</created><updated>2025-02-02</updated><authors><author><keyname>Kumar</keyname><forenames>Prabhat</forenames></author><author><keyname>Sauer</keyname><forenames>Roger A</forenames></author><author><keyname>Saxena</keyname><forenames>Anupam</forenames></author></authors><title>Topology optimization of contact-aided compliant mechanisms for tracing   multi-kink paths</title><categories>cs.CE</categories><comments>Accepted in iNCMDAO 2024 international conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a topology optimization approach to design 2D contact-aided compliant mechanisms (CCMs) that can trace the desired output paths with more than one kink while experiencing self and/or external contacts. Such CCMs can be used as mechanical compliant switches. Hexagonal elements are used to parameterize the design domain. Negative circular masks are employed to remove material beneath them and generate rigid contact surfaces. Each mask is assigned five design variables. The first three decide the location and radius of the mask, whereas the last two determine the presence of the contact surface and its radius. To ensure continuity in contacting surfaces' normal, we employ a boundary smoothing scheme. The augmented Lagrange multiplier method is employed to incorporate self and mutual contact. An objective is formulated using the Fourier shape descriptors with the permitted resource constraint. The hill-climber optimization technique is utilized to update the design variables. An in-house code is developed for the entire process. To demonstrate the method's efficacy, a CCM is optimized with a two-kink path. The desired and obtained paths are compared. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.24050</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.24050</id><created>2024-10-31</created><updated>2025-02-02</updated><authors><author><keyname>Odonnat</keyname><forenames>Ambroise</forenames></author><author><keyname>Bouaziz</keyname><forenames>Wassim</forenames></author><author><keyname>Cabannes</keyname><forenames>Vivien</forenames></author></authors><title>Clustering Head: A Visual Case Study of the Training Dynamics in   Transformers</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper introduces the sparse modular addition task and examines how transformers learn it. We focus on transformers with embeddings in $\R^2$ and introduce a visual sandbox that provides comprehensive visualizations of each layer throughout the training process. We reveal a type of circuit, called "clustering heads," which learns the problem's invariants. We analyze the training dynamics of these circuits, highlighting two-stage learning, loss spikes due to high curvature or normalization layers, and the effects of initialization and curriculum learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.24092</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.24092</id><created>2024-10-31</created><updated>2025-02-02</updated><authors><author><keyname>Ferreira</keyname><forenames>Ricardo N.</forenames></author><author><keyname>Guimarães</keyname><forenames>Marta</forenames></author><author><keyname>Soares</keyname><forenames>Cláudia</forenames></author></authors><title>Satellite Safe Margin: Fast solutions for Conjunction Analysis</title><categories>eess.SY cs.SY math.OC</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The amount of debris in orbit has increased significantly over the years. With the recent growth of interest in space exploration, conjunction assessment has become a central issue. One important metric to evaluate conjunction risk is the miss distance. However, this metric does not intrinsically take into account uncertainty distributions. Some work has been developed to consider the uncertainty associated with the position of the orbiting objects, in particular, to know if these uncertainty distributions overlap (e.g., ellipsoids when considering Gaussian distributions). With this work, we present fast solutions to not only check if the ellipsoids overlap but to compute the distance between them, which we call margin. We present two fast solution methods for two different paradigms: when the best-known data from both objects can be centralized (e.g., debris-satellite conjunctions) and when the most precise covariances cannot be shared (conjunctions of satellites owned by different operators). Our methods are both accurate and fast, being able to process 15,000 conjunctions per minute with the centralized solution and approximately 490 conjunctions per minute with the distributed solution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.00147</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.00147</id><created>2024-10-31</created><updated>2025-02-03</updated><authors><author><keyname>Westphal</keyname><forenames>Charles</forenames></author><author><keyname>Hailes</keyname><forenames>Stephen</forenames></author><author><keyname>Musolesi</keyname><forenames>Mirco</forenames></author></authors><title>Mutual Information Preserving Neural Network Pruning</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Pruning has emerged as the primary approach used to limit the resource requirements of large neural networks (NNs). Since the proposal of the lottery ticket hypothesis, researchers have focused either on pruning at initialization or after training. However, recent theoretical findings have shown that the sample efficiency of robust pruned models is proportional to the mutual information (MI) between the pruning masks and the model's training datasets, \textit{whether at initialization or after training}. In this paper, starting from these results, we introduce Mutual Information Preserving Pruning (MIPP), a structured activation-based pruning technique applicable before or after training. The core principle of MIPP is to select nodes in a way that conserves MI shared between the activations of adjacent layers, and consequently between the data and masks. Approaching the pruning problem in this manner means we can prove that there exists a function that can map the pruned upstream layer's activations to the downstream layer's, implying re-trainability. We demonstrate that MIPP consistently outperforms state-of-the-art methods, regardless of whether pruning is performed before or after training. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.00210</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.00210</id><created>2024-10-31</created><updated>2025-02-02</updated><authors><author><keyname>Revankar</keyname><forenames>Shreelekha</forenames></author><author><keyname>Phoo</keyname><forenames>Cheng Perng</forenames></author><author><keyname>Mall</keyname><forenames>Utkarsh</forenames></author><author><keyname>Hariharan</keyname><forenames>Bharath</forenames></author><author><keyname>Bala</keyname><forenames>Kavita</forenames></author></authors><title>Scale-Aware Recognition in Satellite Images under Resource Constraints</title><categories>cs.CV</categories><comments>16 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recognition of features in satellite imagery (forests, swimming pools, etc.) depends strongly on the spatial scale of the concept and therefore the resolution of the images. This poses two challenges: Which resolution is best suited for recognizing a given concept, and where and when should the costlier higher-resolution (HR) imagery be acquired?   We present a novel scheme to address these challenges by introducing three components: (1) A technique to distill knowledge from models trained on HR imagery to recognition models that operate on imagery of lower resolution (LR), (2) a sampling strategy for HR imagery based on model disagreement, and (3) an LLM-based approach for inferring concept "scale". With these components we present a system to efficiently perform scale-aware recognition in satellite imagery, improving accuracy over single-scale inference while following budget constraints. Our novel approach offers up to a 26.3% improvement over entirely HR baselines, using 76.3% fewer HR images. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.00579</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.00579</id><created>2024-11-01</created><updated>2025-02-03</updated><authors><author><keyname>Toyomoto</keyname><forenames>Yo</forenames></author><author><keyname>Oshima</keyname><forenames>Toshiyuki</forenames></author><author><keyname>Oishi</keyname><forenames>Kosei</forenames></author><author><keyname>Maestre</keyname><forenames>José M.</forenames></author><author><keyname>Hatanaka</keyname><forenames>Takeshi</forenames></author></authors><title>Constraint-Driven Multi-USV Coverage Path Generation for Aquatic   Environmental Monitoring</title><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we address aquatic environmental monitoring using a fleet of unmanned surface vehicles (USVs). Specifically, we develop an online path generator that provides either of circular or elliptic paths based on the real-time feedback so that the USVs efficiently sample the sensor data over given aquatic environment. To this end, we begin by formulating a novel online path generation problem for a group of Dubins vehicles in the form of cost minimization based on the formulation of persistent coverage control. We then transform the cost minimization into a constraint-based specification so that a prescribed performance level is certified. An online coverage path generator is then designed based on the so-called constraint-based control in order to meet the performance certificate together with additional constraints inherent in the parameters that specify the paths. It is also shown there that the present constraint-based approach allows one to drastically reduce the computational complexity stemming from combinations of binary variables corresponding to the turning directions of the USVs. The present coverage path generator is finally demonstrated through simulations and experiments on an original testbed of multiple USVs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.00945</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.00945</id><created>2024-11-01</created><updated>2025-02-03</updated><authors><author><keyname>Bayati</keyname><forenames>Mohsen</forenames></author><author><keyname>Luo</keyname><forenames>Yuwei</forenames></author><author><keyname>Overman</keyname><forenames>William</forenames></author><author><keyname>Shirani</keyname><forenames>Sadegh</forenames></author><author><keyname>Xiong</keyname><forenames>Ruoxuan</forenames></author></authors><title>Higher-Order Causal Message Passing for Experimentation with Complex   Interference</title><categories>cs.LG econ.EM stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate estimation of treatment effects is essential for decision-making across various scientific fields. This task, however, becomes challenging in areas like social sciences and online marketplaces, where treating one experimental unit can influence outcomes for others through direct or indirect interactions. Such interference can lead to biased treatment effect estimates, particularly when the structure of these interactions is unknown. We address this challenge by introducing a new class of estimators based on causal message-passing, specifically designed for settings with pervasive, unknown interference. Our estimator draws on information from the sample mean and variance of unit outcomes and treatments over time, enabling efficient use of observed data to estimate the evolution of the system state. Concretely, we construct non-linear features from the moments of unit outcomes and treatments and then learn a function that maps these features to future mean and variance of unit outcomes. This allows for the estimation of the treatment effect over time. Extensive simulations across multiple domains, using synthetic and real network data, demonstrate the efficacy of our approach in estimating total treatment effect dynamics, even in cases where interference exhibits non-monotonic behavior in the probability of treatment. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.01045</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.01045</id><created>2024-11-01</created><updated>2025-01-31</updated><authors><author><keyname>Zhou</keyname><forenames>Yuqing</forenames></author><author><keyname>Zhu</keyname><forenames>Ziwei</forenames></author></authors><title>Fighting Spurious Correlations in Text Classification via a Causal   Learning Perspective</title><categories>cs.LG cs.CL</categories><comments>Accepted to NAACL 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In text classification tasks, models often rely on spurious correlations for predictions, incorrectly associating irrelevant features with the target labels. This issue limits the robustness and generalization of models, especially when faced with out-of-distribution data where such spurious correlations no longer hold. To address this challenge, we propose the Causally Calibrated Robust Classifier (CCR), which aims to reduce models' reliance on spurious correlations and improve model robustness. Our approach integrates a causal feature selection method based on counterfactual reasoning, along with an unbiased inverse propensity weighting (IPW) loss function. By focusing on selecting causal features, we ensure that the model relies less on spurious features during prediction. We theoretically justify our approach and empirically show that CCR achieves state-of-the-art performance among methods without group labels, and in some cases, it can compete with the models that utilize group labels. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.01081</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.01081</id><created>2024-11-01</created><updated>2024-11-05</updated><authors><author><keyname>Zeng</keyname><forenames>Pei</forenames></author><author><keyname>Bandyopadhyay</keyname><forenames>Debayan</forenames></author><author><keyname>Méndez</keyname><forenames>José A. Méndez</forenames></author><author><keyname>Bitner</keyname><forenames>Nolan</forenames></author><author><keyname>Kolar</keyname><forenames>Alexander</forenames></author><author><keyname>Solomon</keyname><forenames>Michael T.</forenames></author><author><keyname>Heremans</keyname><forenames>F. Joseph</forenames></author><author><keyname>Awschalom</keyname><forenames>David D.</forenames></author><author><keyname>Jiang</keyname><forenames>Liang</forenames></author><author><keyname>Liu</keyname><forenames>Junyu</forenames></author></authors><title>Towards efficient and secure quantum-classical communication networks</title><categories>quant-ph cs.AI cs.CR</categories><comments>4 pages, a blue print paper, Submission for IEEE 2024 IEEE Workshop   on Quantum IntelLigence, Learning &amp; Security (QUILLS),   https://sites.google.com/pitt.edu/quills/home</comments><journal-ref>2024 IEEE 6th International Conference on Trust, Privacy and   Security in Intelligent Systems, and Applications (TPS-ISA) (pp. 520-523)</journal-ref><doi>10.1109/TPS-ISA62245.2024.00070</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid advancement of quantum technologies calls for the design and deployment of quantum-safe cryptographic protocols and communication networks. There are two primary approaches to achieving quantum-resistant security: quantum key distribution (QKD) and post-quantum cryptography (PQC). While each offers unique advantages, both have drawbacks in practical implementation. In this work, we introduce the pros and cons of these protocols and explore how they can be combined to achieve a higher level of security and/or improved performance in key distribution. We hope our discussion inspires further research into the design of hybrid cryptographic protocols for quantum-classical communication networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.01375</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.01375</id><created>2024-11-02</created><updated>2025-02-02</updated><authors><author><keyname>Arnal</keyname><forenames>Charles</forenames></author><author><keyname>Berenfeld</keyname><forenames>Clement</forenames></author><author><keyname>Rosenberg</keyname><forenames>Simon</forenames></author><author><keyname>Cabannes</keyname><forenames>Vivien</forenames></author></authors><title>Learning with Hidden Factorial Structure</title><categories>stat.ML cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Statistical learning in high-dimensional spaces is challenging without a strong underlying data structure. Recent advances with foundational models suggest that text and image data contain such hidden structures, which help mitigate the curse of dimensionality. Inspired by results from nonparametric statistics, we hypothesize that this phenomenon can be partially explained in terms of decomposition of complex tasks into simpler subtasks. In this paper, we present a controlled experimental framework to test whether neural networks can indeed exploit such "hidden factorial structures". We find that they do leverage these latent patterns to learn discrete distributions more efficiently. We also study the interplay between our structural assumptions and the models' capacity for generalization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.01390</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.01390</id><created>2024-11-02</created><updated>2025-02-01</updated><authors><author><keyname>Bengtsson</keyname><forenames>Max</forenames></author><author><keyname>Keles</keyname><forenames>Elif</forenames></author><author><keyname>Durak</keyname><forenames>Gorkem</forenames></author><author><keyname>Anwar</keyname><forenames>Syed</forenames></author><author><keyname>Velichko</keyname><forenames>Yuri S.</forenames></author><author><keyname>Linguraru</keyname><forenames>Marius G.</forenames></author><author><keyname>Waanders</keyname><forenames>Angela J.</forenames></author><author><keyname>Bagci</keyname><forenames>Ulas</forenames></author></authors><title>A New Logic For Pediatric Brain Tumor Segmentation</title><categories>cs.CV eess.IV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we present a novel approach for segmenting pediatric brain tumors using a deep learning architecture, inspired by expert radiologists' segmentation strategies. Our model delineates four distinct tumor labels and is benchmarked on a held-out PED BraTS 2024 test set (i.e., pediatric brain tumor datasets introduced by BraTS). Furthermore, we evaluate our model's performance against the state-of-the-art (SOTA) model using a new external dataset of 30 patients from CBTN (Children's Brain Tumor Network), labeled in accordance with the PED BraTS 2024 guidelines and 2023 BraTS Adult Glioma dataset. We compare segmentation outcomes with the winning algorithm from the PED BraTS 2023 challenge as the SOTA model. Our proposed algorithm achieved an average Dice score of 0.642 and an HD95 of 73.0 mm on the CBTN test data, outperforming the SOTA model, which achieved a Dice score of 0.626 and an HD95 of 84.0 mm. Moreover, our model exhibits strong generalizability, attaining a 0.877 Dice score in whole tumor segmentation on the BraTS 2023 Adult Glioma dataset, surpassing existing SOTA. Our results indicate that the proposed model is a step towards providing more accurate segmentation for pediatric brain tumors, which is essential for evaluating therapy response and monitoring patient progress. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.01565</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.01565</id><created>2024-11-03</created><updated>2025-02-02</updated><authors><author><keyname>Zhao</keyname><forenames>Jiawei</forenames></author><author><keyname>Chen</keyname><forenames>Kejiang</forenames></author><author><keyname>Zhang</keyname><forenames>Weiming</forenames></author><author><keyname>Yu</keyname><forenames>Nenghai</forenames></author></authors><title>SQL Injection Jailbreak: A Structural Disaster of Large Language Models</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, the rapid development of large language models (LLMs) has brought new vitality into various domains, generating substantial social and economic benefits. However, this swift advancement has also introduced new vulnerabilities. Jailbreaking, a form of attack that induces LLMs to produce harmful content through carefully crafted prompts, presents a significant challenge to the safe and trustworthy development of LLMs. Previous jailbreak methods primarily exploited the internal properties or capabilities of LLMs, such as optimization-based jailbreak methods and methods that leveraged the model's context-learning abilities. In this paper, we introduce a novel jailbreak method, SQL Injection Jailbreak (SIJ), which targets the external properties of LLMs, specifically, the way LLMs construct input prompts. By injecting jailbreak information into user prompts, SIJ successfully induces the model to output harmful content. Our SIJ method achieves near 100\% attack success rates on five well-known open-source LLMs on the AdvBench and HEx-PHI, while incurring lower time costs compared to previous methods. For closed-source models, SIJ achieves near 100% attack success rate on GPT-3.5-turbo. Additionally, SIJ exposes a new vulnerability in LLMs that urgently requires mitigation. To address this, we propose a simple defense method called Self-Reminder-Key to counter SIJ and demonstrate its effectiveness through experimental results. Our code is available at https://github.com/weiyezhimeng/SQL-Injection-Jailbreak. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.01767</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.01767</id><created>2024-11-03</created><updated>2025-02-01</updated><authors><author><keyname>Feigin</keyname><forenames>Shlomo Libo</forenames></author><author><keyname>Fleissner</keyname><forenames>Maximilian</forenames></author><author><keyname>Ghoshdastidar</keyname><forenames>Debarghya</forenames></author></authors><title>A Theoretical Characterization of Optimal Data Augmentations in   Self-Supervised Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data augmentations play an important role in the recent success of Self-Supervised Learning (SSL). While commonly viewed as encoding invariances into the learned representations, this interpretation overlooks the impact of the pretraining architecture and suggests that SSL would require diverse augmentations which resemble the data to work well. However, these assumptions do not align with empirical evidence, encouraging further theoretical understanding to guide the principled design of augmentations in new domains. To this end, we use kernel theory to derive analytical expressions for data augmentations that achieve desired target representations after pretraining. We consider two popular non-contrastive losses, VICReg and Barlow Twins, and provide an algorithm to construct such augmentations. Our analysis shows that augmentations need not be similar to the data to learn useful representations, nor be diverse, and that the architecture has a significant impact on the optimal augmentations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.02158</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.02158</id><created>2024-11-04</created><updated>2025-02-03</updated><authors><author><keyname>Sharony</keyname><forenames>Elad</forenames></author><author><keyname>Yang</keyname><forenames>Heng</forenames></author><author><keyname>Che</keyname><forenames>Tong</forenames></author><author><keyname>Pavone</keyname><forenames>Marco</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author><author><keyname>Karkus</keyname><forenames>Peter</forenames></author></authors><title>Learning Multiple Initial Solutions to Optimization Problems</title><categories>cs.LG cs.AI cs.RO cs.SY eess.SY</categories><comments>Under Review</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Sequentially solving similar optimization problems under strict runtime constraints is essential for many applications, such as robot control, autonomous driving, and portfolio management. The performance of local optimization methods in these settings is sensitive to the initial solution: poor initialization can lead to slow convergence or suboptimal solutions. To address this challenge, we propose learning to predict \emph{multiple} diverse initial solutions given parameters that define the problem instance. We introduce two strategies for utilizing multiple initial solutions: (i) a single-optimizer approach, where the most promising initial solution is chosen using a selection function, and (ii) a multiple-optimizers approach, where several optimizers, potentially run in parallel, are each initialized with a different solution, with the best solution chosen afterward. Notably, by including a default initialization among predicted ones, the cost of the final output is guaranteed to be equal or lower than with the default initialization. We validate our method on three optimal control benchmark tasks: cart-pole, reacher, and autonomous driving, using different optimizers: DDP, MPPI, and iLQR. We find significant and consistent improvement with our method across all evaluation settings and demonstrate that it efficiently scales with the number of initial solutions required. The code is available at MISO (https://github.com/EladSharony/miso). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.02392</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.02392</id><created>2024-11-04</created><updated>2025-01-31</updated><authors><author><keyname>Nandakumar</keyname><forenames>Satyadev</forenames></author><author><keyname>Pulari</keyname><forenames>Subin</forenames></author><author><keyname>S</keyname><forenames>Akhil</forenames></author><author><keyname>Sarma</keyname><forenames>Suronjona</forenames></author></authors><title>One-Way Functions and Polynomial Time Dimension</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Polynomial-time dimension (denoted $\mathrm{cdim}_{P}$) quantifies the density of information of infinite sequences using polynomial time betting algorithms called $s$-gales. An alternate quantification of the notion of polynomial time density of information is using polynomial-time Kolmogorov complexity rate (denoted $\mathcal{K}_\text{poly}$). The corresponding unbounded notions, namely, the constructive dimension and unbounded Kolmogorov complexity rates are equal for every sequence. Analogous notions are equal even at finite-state level. In view of this, it is reasonable to conjecture that $\mathrm{cdim}_{P}$ and $\mathcal{K}_\text{poly}$ are identical notions.   In this paper we demonstrate that surprisingly, $\mathrm{cdim}_{P}$ and $\mathcal{K}_\text{poly}$ are distinct measures of information density if and only if one-way functions exist. We consider polynomial time samplable distributions over $\Sigma^\infty$ that uses short seeds to sample a finite string $w \in \Sigma^n$. We establish the following results. We first show that if one-way functions exist then there exist a polynomial time samplable distribution with respect to which $\mathrm{cdim}_{P}$ and $\mathcal{K}_\text{poly}$ are separated by a uniform gap with probability $1$. Conversely, we show that if there exists such a polynomial time samplable distribution, then infinitely-often one-way functions exist. Hence, we provide a new information theoretic characterisation of the existence of one-way functions.   Using this new characterization, we solve an open problem posed by Hitchcock and Vinodchandran (CCC 2004) and Stull \cite{stullsurvey}. We demonstrate that if one-way functions exist, then there are individual sequences $X$ whose poly-time dimension strictly exceeds $\mathcal{K}_\text{poly}(X)$, that is $\mathrm{cdim}_{P}(X) &gt; \mathcal{K}_\text{poly}(X)$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.02481</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.02481</id><created>2024-11-04</created><updated>2025-01-31</updated><authors><author><keyname>Xu</keyname><forenames>Guangxuan</forenames></author><author><keyname>Xu</keyname><forenames>Kai</forenames></author><author><keyname>Sudalairaj</keyname><forenames>Shivchander</forenames></author><author><keyname>Wang</keyname><forenames>Hao</forenames></author><author><keyname>Srivastava</keyname><forenames>Akash</forenames></author></authors><title>Dr. SoW: Density Ratio of Strong-over-weak LLMs for Reducing the Cost of   Human Annotation in Preference Tuning</title><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Preference tuning relies on high-quality human preference data, which is often expensive and time-consuming to gather. In this paper, we introduce Dr.SoW (Density Ratio of Strong over Weak) a cost-effective method that eliminates the reliance for human annotation by leveraging off-the-shelf LLMs for preference data annotation. Dr.SoW uses the log-density ratio between a better-aligned and a less-aligned LLM as a reward signal. We evaluate Dr.SoW across 221 different LLM pairs and empirically find a strong correlation between the performance gap of the paired models and the quality of the reward signal. This insight provides a practical guideline for selecting LLMs for data annotation.   Additionally, we introduce an end-to-end pipeline that customizes reward functions based on user query domains. Without fine-tuning, it improves accuracy on domain-specific evaluations. With a pair of Mistral-7B models, Dr.SoW achieves a RewardBench score of 82.6, outperforming the best trained reward functions from same model class and demonstrating competitive performance against SoTA models in Safety (91.0) and Reasoning (88.0) domains. Further, we preference-tune Llama-3-8B-Instruct using data annotated by Dr.SoW. Our approach pushes Llama-3-8B to achieve a 37.4 % (+15.1 %) win rate on ArenaHard and a 40.7 % (+17.8 %) win rate on length-controlled AlpacaEval 2.0. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.02688</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.02688</id><created>2024-11-04</created><updated>2025-02-02</updated><authors><author><keyname>Wang</keyname><forenames>Yihan</forenames></author><author><keyname>Bai</keyname><forenames>Andrew</forenames></author><author><keyname>Peng</keyname><forenames>Nanyun</forenames></author><author><keyname>Hsieh</keyname><forenames>Cho-Jui</forenames></author></authors><title>On the Loss of Context-awareness in General Instruction Fine-tuning</title><categories>cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Pre-trained Large Language Models (LLMs) require post-training methods such as supervised fine-tuning (SFT) on instruction-response pairs to enable instruction following. However, this process can potentially harm existing capabilities learned during pre-training. In this paper, we investigate the loss of context awareness after SFT, where context awareness is defined as the ability to extract and understand information from user-provided context and respond accordingly. We identify and demonstrate that the loss of context awareness, particularly in open-source models, occurs in instruction fine-tuned LLMs when the chat template is applied to input prompts. We identify that the performance decline is associated with a bias toward different roles learned during conversational instruction fine-tuning. We demonstrate this correlation by visualizing changes in attention allocation after the chat template is applied and manually steering the attention heads. The bias can be learned from training examples that align with the model's internal knowledge and rely less on the user-provided context to generate correct responses. Based on these observations, we propose a metric to identify context-dependent examples from general instruction fine-tuning datasets. We then apply conditional instruction fine-tuning with a context-dependency indicator, enabling the model to preserve context awareness after SFT. Empirical experiments on four context-dependent downstream tasks and three pre-trained LLMs of different sizes show that our method effectively mitigates the loss of context awareness without compromising general instruction-following capabilities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.04923</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.04923</id><created>2024-11-07</created><updated>2025-02-02</updated><authors><author><keyname>Munasinghe</keyname><forenames>Shehan</forenames></author><author><keyname>Gani</keyname><forenames>Hanan</forenames></author><author><keyname>Zhu</keyname><forenames>Wenqi</forenames></author><author><keyname>Cao</keyname><forenames>Jiale</forenames></author><author><keyname>Xing</keyname><forenames>Eric</forenames></author><author><keyname>Khan</keyname><forenames>Fahad Shahbaz</forenames></author><author><keyname>Khan</keyname><forenames>Salman</forenames></author></authors><title>VideoGLaMM: A Large Multimodal Model for Pixel-Level Visual Grounding in   Videos</title><categories>cs.CV</categories><comments>Technical Report of VideoGLaMM</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Fine-grained alignment between videos and text is challenging due to complex spatial and temporal dynamics in videos. Existing video-based Large Multimodal Models (LMMs) handle basic conversations but struggle with precise pixel-level grounding in videos. To address this, we introduce VideoGLaMM, a LMM designed for fine-grained pixel-level grounding in videos based on user-provided textual inputs. Our design seamlessly connects three key components: a Large Language Model, a dual vision encoder that emphasizes both spatial and temporal details, and a spatio-temporal decoder for accurate mask generation. This connection is facilitated via tunable V-L and L-V adapters that enable close Vision-Language (VL) alignment. The architecture is trained to synchronize both spatial and temporal elements of video content with textual instructions. To enable fine-grained grounding, we curate a multimodal dataset featuring detailed visually-grounded conversations using a semiautomatic annotation pipeline, resulting in a diverse set of 38k video-QA triplets along with 83k objects and 671k masks. We evaluate VideoGLaMM on three challenging tasks: Grounded Conversation Generation, Visual Grounding, and Referring Video Segmentation. Experimental results show that our model consistently outperforms existing approaches across all three tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.04983</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.04983</id><created>2024-11-07</created><updated>2025-01-31</updated><authors><author><keyname>Zhou</keyname><forenames>Gaoyue</forenames></author><author><keyname>Pan</keyname><forenames>Hengkai</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author><author><keyname>Pinto</keyname><forenames>Lerrel</forenames></author></authors><title>DINO-WM: World Models on Pre-trained Visual Features enable Zero-shot   Planning</title><categories>cs.RO cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The ability to predict future outcomes given control actions is fundamental for physical reasoning. However, such predictive models, often called world models, remains challenging to learn and are typically developed for task-specific solutions with online policy learning. To unlock world models' true potential, we argue that they should 1) be trainable on offline, pre-collected trajectories, 2) support test-time behavior optimization, and 3) facilitate task-agnostic reasoning. To this end, we present DINO World Model (DINO-WM), a new method to model visual dynamics without reconstructing the visual world. DINO-WM leverages spatial patch features pre-trained with DINOv2, enabling it to learn from offline behavioral trajectories by predicting future patch features. This allows DINO-WM to achieve observational goals through action sequence optimization, facilitating task-agnostic planning by treating goal features as prediction targets. We demonstrate that DINO-WM achieves zero-shot behavioral solutions at test time on six environments without expert demonstrations, reward modeling, or pre-learned inverse models, outperforming prior state-of-the-art work across diverse task families such as arbitrarily configured mazes, push manipulation with varied object shapes, and multi-particle scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.07482</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.07482</id><created>2024-11-11</created><updated>2025-02-01</updated><authors><author><keyname>Xing</keyname><forenames>Jinming</forenames></author><author><keyname>Xing</keyname><forenames>Ruilin</forenames></author><author><keyname>Xue</keyname><forenames>Chang</forenames></author><author><keyname>Luo</keyname><forenames>Dongwen</forenames></author></authors><title>Enhancing Link Prediction with Fuzzy Graph Attention Networks and   Dynamic Negative Sampling</title><categories>cs.LG cs.AI cs.IR</categories><comments>Accepted to ISMSI'25</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Link prediction is crucial for understanding complex networks but traditional Graph Neural Networks (GNNs) often rely on random negative sampling, leading to suboptimal performance. This paper introduces Fuzzy Graph Attention Networks (FGAT), a novel approach integrating fuzzy rough sets for dynamic negative sampling and enhanced node feature aggregation. Fuzzy Negative Sampling (FNS) systematically selects high-quality negative edges based on fuzzy similarities, improving training efficiency. FGAT layer incorporates fuzzy rough set principles, enabling robust and discriminative node representations. Experiments on two research collaboration networks demonstrate FGAT's superior link prediction accuracy, outperforming state-of-the-art baselines by leveraging the power of fuzzy rough sets for effective negative sampling and node feature learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.08482</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.08482</id><created>2024-11-13</created><updated>2025-01-31</updated><authors><author><keyname>Kuznietsov</keyname><forenames>Anton</forenames></author><author><keyname>Schweickard</keyname><forenames>Dirk</forenames></author><author><keyname>Peters</keyname><forenames>Steven</forenames></author></authors><title>Methodology for a Statistical Analysis of Influencing Factors on 3D   Object Detection Performance</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In automated driving, object detection is an essential task to perceive the environment by localizing and classifying objects. Most object detection algorithms are based on deep learning for superior performance. However, their black-box nature makes it challenging to ensure safety. In this paper, we propose a first-of-its-kind methodology for analyzing the influence of various factors related to the objects or the environment on the detection performance of both LiDAR- and camera-based 3D object detectors. We conduct a statistical univariate analysis between each factor and the detection error on pedestrians to compare their strength of influence. In addition to univariate analysis, we employ a Random Forest (RF) model to predict the errors of specific detectors based on the provided meta-information. To interpret the predictions of the RF and assess the importance of individual features, we compute Shapley Values. By considering feature dependencies, the RF captures more complex relationships between meta-information and detection errors, allowing a more nuanced analysis of the factors contributing to the observed errors. Recognizing the factors that influence detection performance helps identify performance insufficiencies in the trained object detector and supports the safe development of object detection systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.09355</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.09355</id><created>2024-11-14</created><updated>2025-02-01</updated><authors><author><keyname>Soumalias</keyname><forenames>Ermis</forenames></author><author><keyname>Heiss</keyname><forenames>Jakob</forenames></author><author><keyname>Weissteiner</keyname><forenames>Jakob</forenames></author><author><keyname>Seuken</keyname><forenames>Sven</forenames></author></authors><title>Prices, Bids, Values: One ML-Powered Combinatorial Auction to Rule Them   All</title><categories>cs.GT cs.AI cs.LG</categories><comments>8 pages + appendix</comments><msc-class>91A06, 68T07, 91-08</msc-class><acm-class>I.2; I.2.6; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the design of iterative combinatorial auctions (ICAs). The main challenge in this domain is that the bundle space grows exponentially in the number of items. To address this, recent work has proposed machine learning (ML)-based preference elicitation algorithms that aim to elicit only the most critical information from bidders to maximize efficiency. However, while the SOTA ML-based algorithms elicit bidders' preferences via value queries, ICAs that are used in practice elicit information via demand queries. In this paper, we introduce a novel ML algorithm that provably makes use of the full information from both value and demand queries, and we show via experiments that combining both query types results in significantly better learning performance in practice. Building on these insights, we present MLHCA, a new ML-powered auction that uses value and demand queries. MLHCA substantially outperforms the previous SOTA, reducing efficiency loss by up to a factor 10, with up to 58% fewer queries. Thus, MLHCA achieves large efficiency improvements while also reducing bidders' cognitive load, establishing a new benchmark for both practicability and efficiency. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.09738</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.09738</id><created>2024-11-14</created><updated>2025-02-03</updated><authors><author><keyname>Stade</keyname><forenames>Yannick</forenames></author><author><keyname>Schmid</keyname><forenames>Ludwig</forenames></author><author><keyname>Burgholzer</keyname><forenames>Lukas</forenames></author><author><keyname>Wille</keyname><forenames>Robert</forenames></author></authors><title>Optimal State Preparation for Logical Arrays on Zoned Neutral Atom   Quantum Computers</title><categories>quant-ph cs.ET</categories><comments>7 pages, 4 figures; Update: Add explanation for variables, add run   time of SMT engine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum computing promises to solve problems previously deemed infeasible. However, high error rates necessitate quantum error correction for practical applications. Seminal experiments with zoned neutral atom architectures have shown remarkable potential for fault-tolerant quantum computing. To fully harness their potential, efficient software solutions are vital. A key aspect of quantum error correction is the initialization of physical qubits representing a logical qubit in a highly entangled state. This process, known as state preparation, is the foundation of most quantum error correction codes and, hence, a crucial step towards fault-tolerant quantum computing. Generating a schedule of target-specific instructions to perform the state preparation is highly complex. First software tools exist but are not suitable for the zoned neutral atom architectures. This work addresses this gap by leveraging the computational power of SMT solvers and generating minimal schedules for the state preparation of logical arrays. Experimental evaluations demonstrate that actively utilizing zones to shield idling qubits consistently results in higher fidelities than solutions disregarding these zones. The complete code is publicly available in open-source as part of the Munich Quantum Toolkit (MQT) at https://github.com/cda-tum/mqt-qmap/tree/main/src/na/nasp. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.10105</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.10105</id><created>2024-11-15</created><updated>2025-01-21</updated><authors><author><keyname>Roy</keyname><forenames>Somnath</forenames></author><author><keyname>Coccolo</keyname><forenames>Mattia</forenames></author><author><keyname>Sanjuán</keyname><forenames>Miguel A. F.</forenames></author></authors><title>Parametric Autoresonance with Time-Delayed Control</title><categories>nlin.CD cs.NA math-ph math.MP math.NA physics.comp-ph physics.data-an</categories><comments>17 pages, 5 figures</comments><msc-class>65</msc-class><acm-class>G.1; J.2</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We investigate how a constant time delay influences a parametric autoresonant system. This is a nonlinear system driven by a parametrically chirped force with a negative delay-feedback that maintains adiabatic phase locking with the driving frequency. This phase locking results in a continuous amplitude growth, regardless of parameter changes. Our study reveals a critical threshold for delay strength; above this threshold, autoresonance is sustained, while below it, autoresonance diminishes. We examine the interplay between time delay and autoresonance stability, using multi-scale perturbation methods to derive analytical results, which are corroborated by numerical simulations. Ultimately, the goal is to understand and control autoresonance stability through the time-delay parameters. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.11667</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.11667</id><created>2024-11-18</created><updated>2025-01-31</updated><authors><author><keyname>Hu</keyname><forenames>Lijie</forenames></author><author><keyname>Ren</keyname><forenames>Chenyang</forenames></author><author><keyname>Xie</keyname><forenames>Huanyi</forenames></author><author><keyname>Saadi</keyname><forenames>Khouloud</forenames></author><author><keyname>Yang</keyname><forenames>Shu</forenames></author><author><keyname>Tan</keyname><forenames>Zhen</forenames></author><author><keyname>Zhang</keyname><forenames>Jingfeng</forenames></author><author><keyname>Wang</keyname><forenames>Di</forenames></author></authors><title>Dissecting Representation Misalignment in Contrastive Learning via   Influence Function</title><categories>cs.LG cs.AI cs.CV</categories><comments>33 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Contrastive learning, commonly applied in large-scale multimodal models, often relies on data from diverse and often unreliable sources, which can include misaligned or mislabeled text-image pairs. This frequently leads to robustness issues and hallucinations, ultimately causing performance degradation. Data valuation is an efficient way to detect and trace these misalignments. Nevertheless, existing methods are computationally expensive for large-scale models. Although computationally efficient, classical influence functions are inadequate for contrastive learning models, as they were initially designed for pointwise loss. Furthermore, contrastive learning involves minimizing the distance between positive sample modalities while maximizing the distance between negative sample modalities. This necessitates evaluating the influence of samples from both perspectives. To tackle these challenges, we introduce the Extended Influence Function for Contrastive Loss (ECIF), an influence function crafted for contrastive loss. ECIF considers both positive and negative samples and provides a closed-form approximation of contrastive learning models, eliminating the need for retraining. Building upon ECIF, we develop a series of algorithms for data evaluation, misalignment detection, and misprediction trace-back tasks. Experimental results demonstrate our ECIF advances the transparency and interpretability of CLIP-style embedding models by offering a more accurate assessment of data impact and model alignment compared to traditional baseline methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.12155</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.12155</id><created>2024-11-18</created><updated>2025-01-31</updated><authors><author><keyname>Seo</keyname><forenames>Younggyo</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author></authors><title>Coarse-to-fine Q-Network with Action Sequence for Data-Efficient Robot   Learning</title><categories>cs.LG cs.AI cs.RO</categories><comments>15 Pages. Website: https://younggyo.me/cqn-as/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In reinforcement learning (RL), we train a value function to understand the long-term consequence of executing a single action. However, the value of taking each action can be ambiguous in robotics as robot movements are typically the aggregate result of executing multiple small actions. Moreover, robotic training data often consists of noisy trajectories, in which each action is noisy but executing a series of actions results in a meaningful robot movement. This further makes it difficult for the value function to understand the effect of individual actions. To address this, we introduce Coarse-to-fine Q-Network with Action Sequence (CQN-AS), a novel value-based RL algorithm that learns a critic network that outputs Q-values over a sequence of actions, i.e., explicitly training the value function to learn the consequence of executing action sequences. We study our algorithm on 53 robotic tasks with sparse and dense rewards, as well as with and without demonstrations, from BiGym, HumanoidBench, and RLBench. We find that CQN-AS outperforms various baselines, in particular on humanoid control tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.13010</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.13010</id><created>2024-11-19</created><updated>2025-01-31</updated><authors><author><keyname>Huang</keyname><forenames>Allen Hao</forenames></author><author><keyname>Schlag</keyname><forenames>Imanol</forenames></author></authors><title>Deriving Activation Functions Using Integration</title><categories>cs.LG cs.NE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Our work proposes a novel approach to designing activation functions by focusing on their gradients and deriving the corresponding activation functions using integration. We introduce the Expanded Integral of the Exponential Linear Unit (xIELU), a trainable piecewise activation function derived by integrating trainable affine transformations applied to the Exponential Linear Unit (ELU). xIELU combines two key properties for the gradient: (1) a trainable and linearly increasing gradient for positive inputs, similar to Squared ReLU (ReLU$^2$), and (2) a trainable gradient that can take negative values for negative inputs, inspired by Expanded SiLU (xSiLU). Conceptually, xIELU can be viewed as an extension of ReLU$^2$ to handle negative inputs. The trainable parameters in xIELU allow it to adaptively reduce its nonlinearity for higher-level representations deeper in the network. In experiments with 1.1B and 3B parameter Llama models trained on 125B tokens of FineWeb Edu, xIELU achieves lower perplexity compared to popular activation functions like ReLU$^2$ and SwiGLU when matched for the same compute cost and parameter count. A reference implementation is available at https://github.com/Anonymous5823/xielu. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.13052</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.13052</id><created>2024-11-20</created><updated>2025-02-01</updated><authors><author><keyname>Tran</keyname><forenames>Hung Vinh</forenames></author><author><keyname>Chen</keyname><forenames>Tong</forenames></author><author><keyname>Ye</keyname><forenames>Guanhua</forenames></author><author><keyname>Nguyen</keyname><forenames>Quoc Viet Hung</forenames></author><author><keyname>Zheng</keyname><forenames>Kai</forenames></author><author><keyname>Yin</keyname><forenames>Hongzhi</forenames></author></authors><title>On-device Content-based Recommendation with Single-shot Embedding   Pruning: A Cooperative Game Perspective</title><categories>cs.IR cs.LG</categories><comments>ACM Web Conference 2025 (WWW '25)</comments><doi>10.1145/3696410.3714921</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Content-based Recommender Systems (CRSs) play a crucial role in shaping user experiences in e-commerce, online advertising, and personalized recommendations. However, due to the vast amount of categorical features, the embedding tables used in CRS models pose a significant storage bottleneck for real-world deployment, especially on resource-constrained devices. To address this problem, various embedding pruning methods have been proposed, but most existing ones require expensive retraining steps for each target parameter budget, leading to enormous computation costs. In reality, this computation cost is a major hurdle in real-world applications with diverse storage requirements, such as federated learning and streaming settings. In this paper, we propose Shapley Value-guided Embedding Reduction (Shaver) as our response. With Shaver, we view the problem from a cooperative game perspective, and quantify each embedding parameter's contribution with Shapley values to facilitate contribution-based parameter pruning. To address the inherently high computation costs of Shapley values, we propose an efficient and unbiased method to estimate Shapley values of a CRS's embedding parameters. Moreover, in the pruning stage, we put forward a field-aware codebook to mitigate the information loss in the traditional zero-out treatment. Through extensive experiments on three real-world datasets, Shaver has demonstrated competitive performance with lightweight recommendation models across various parameter budgets. The source code is available at https://github.com/chenxing1999/shaver </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.13189</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.13189</id><created>2024-11-20</created><authors><author><keyname>Müller</keyname><forenames>Tom David</forenames><affiliation>University of Tübingen</affiliation><affiliation>Institute for Bioinformatics and Medical Informatics</affiliation></author><author><keyname>Siraj</keyname><forenames>Arslan</forenames><affiliation>University of Tübingen</affiliation><affiliation>Institute for Bioinformatics and Medical Informatics</affiliation></author><author><keyname>Walter</keyname><forenames>Axel</forenames><affiliation>University of Tübingen</affiliation><affiliation>Institute for Bioinformatics and Medical Informatics</affiliation></author><author><keyname>Kim</keyname><forenames>Jihyung</forenames><affiliation>University of Tübingen</affiliation><affiliation>Institute for Bioinformatics and Medical Informatics</affiliation></author><author><keyname>Wein</keyname><forenames>Samuel</forenames><affiliation>University of Tübingen</affiliation><affiliation>Institute for Bioinformatics and Medical Informatics</affiliation></author><author><keyname>von Kleist</keyname><forenames>Johannes</forenames><affiliation>University of Tübingen</affiliation></author><author><keyname>Feroz</keyname><forenames>Ayesha</forenames><affiliation>University of Tübingen</affiliation><affiliation>Institute for Bioinformatics and Medical Informatics</affiliation></author><author><keyname>Pilz</keyname><forenames>Matteo</forenames><affiliation>University of Tübingen</affiliation><affiliation>Institute for Bioinformatics and Medical Informatics</affiliation></author><author><keyname>Jeong</keyname><forenames>Kyowon</forenames><affiliation>University of Tübingen</affiliation><affiliation>Institute for Bioinformatics and Medical Informatics</affiliation></author><author><keyname>Sing</keyname><forenames>Justin Cyril</forenames><affiliation>Donnelly Centre for Cellular and Biomolecular Research</affiliation><affiliation>University of Toronto</affiliation></author><author><keyname>Charkow</keyname><forenames>Joshua</forenames><affiliation>Donnelly Centre for Cellular and Biomolecular Research</affiliation><affiliation>University of Toronto</affiliation></author><author><keyname>Röst</keyname><forenames>Hannes Luc</forenames><affiliation>Donnelly Centre for Cellular and Biomolecular Research</affiliation><affiliation>University of Toronto</affiliation></author><author><keyname>Sachsenberg</keyname><forenames>Timo</forenames><affiliation>University of Tübingen</affiliation><affiliation>Institute for Bioinformatics and Medical Informatics</affiliation></author></authors><title>OpenMS WebApps: Building User-Friendly Solutions for MS Analysis</title><categories>q-bio.BM cs.HC</categories><comments>19 pages, 5 figures</comments><doi>10.1021/acs.jproteome.4c00872</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Liquid Chromatography Mass Spectrometry (LC-MS) is an indispensable analytical technique in proteomics, metabolomics, and other life sciences. While OpenMS provides advanced open-source software for MS data analysis, its complexity can be challenging for non-experts. To address this, we have developed OpenMS WebApps, a framework for creating user-friendly MS web applications based on the Streamlit Python package. OpenMS WebApps simplifies MS data analysis through an intuitive graphical user interface, interactive result visualizations, and support for both local and online execution. Key features include workspaces management, automatic generation of input widgets, and parallel execution of tools resulting in highperformance and ready-to-use solutions for online and local deployment. This framework benefits both researchers and developers: scientists can focus on their research without the burden of complex software setups, and developers can rapidly create and distribute custom WebApps with novel algorithms. Several applications built on the OpenMS WebApps template demonstrate its utility across diverse MS-related fields, enhancing the OpenMS eco-system for developers and a wider range of users. Furthermore, it integrates seamlessly with third-party software, extending benefits to developers beyond the OpenMS community. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.13806</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.13806</id><created>2024-11-20</created><updated>2025-02-02</updated><authors><author><keyname>Stoorvogel</keyname><forenames>Anton A.</forenames></author><author><keyname>Saberi</keyname><forenames>Ali</forenames></author><author><keyname>Liu</keyname><forenames>Zhenwei</forenames></author></authors><title>Weak synchronization in heterogeneous multi-agent systems</title><categories>eess.SY cs.SY</categories><comments>This paper has been submitted to TAC at Jan. 30, 2025 for first round   review</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In this paper, we propose a new framework for synchronization of heterogeneous multi agent system which we refer to as weak synchronization. This new framework of synchronization is based on achieving the network stability in the absence of any information on communication network including the connectivity. Here by network stability, we mean that in the basic setup of a multi-agent system, we require that the signals exchanged over the network converge to zero. As such if the network happens to have a directed spanning tree then we obtain classical synchronization. Moreover, we design protocols which achieve weak synchronization for any network without making any kind of assumptions on communication network. If the network happens to have a directed spanning tree, then we obtain classical synchronization. However, if this is not the case then we describe in detail in this paper what kind of synchronization properties are preserved in the system and the output of the different agents can behave. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.13865</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.13865</id><created>2024-11-21</created><updated>2025-02-01</updated><authors><author><keyname>Ma</keyname><forenames>Qiyao</forenames></author><author><keyname>Yang</keyname><forenames>Menglin</forenames></author><author><keyname>Ju</keyname><forenames>Mingxuan</forenames></author><author><keyname>Zhao</keyname><forenames>Tong</forenames></author><author><keyname>Shah</keyname><forenames>Neil</forenames></author><author><keyname>Ying</keyname><forenames>Rex</forenames></author></authors><title>Breaking Information Cocoons: A Hyperbolic Graph-LLM Framework for   Exploration and Exploitation in Recommender Systems</title><categories>cs.IR cs.AI cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern recommender systems often create information cocoons, restricting users' exposure to diverse content. A key challenge lies in balancing content exploration and exploitation while allowing users to adjust their recommendation preferences. Intuitively, this balance can be modeled as a tree-structured representation, where depth search facilitates exploitation and breadth search enables exploration. However, existing approaches face two fundamental limitations: Euclidean methods struggle to capture hierarchical structures, while hyperbolic methods, despite their superior hierarchical modeling, lack semantic understanding of user and item profiles and fail to provide a principled mechanism for balancing exploration and exploitation. To address these challenges, we propose HERec, a hyperbolic graph-LLM framework that effectively balances exploration and exploitation in recommender systems. Our framework introduces two key innovations: (1) a hierarchical-aware graph-LLM mechanism that jointly aligns textual descriptions with user-item collaborative information in hyperbolic space, and (2) a hierarchical representation structure that enables user-adjustable exploration-exploitation trade-offs. Extensive experiments demonstrate that HERec consistently outperforms both Euclidean and hyperbolic baselines, achieving up to 5.49% improvement in utility metrics and 11.39% increase in diversity metrics, effectively mitigating information cocoons. We open-source our model implementation at https://github.com/Martin-qyma/HERec. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.14654</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.14654</id><created>2024-11-21</created><updated>2025-02-01</updated><authors><author><keyname>Xing</keyname><forenames>Jinming</forenames></author><author><keyname>Luo</keyname><forenames>Dongwen</forenames></author><author><keyname>Xue</keyname><forenames>Chang</forenames></author><author><keyname>Xing</keyname><forenames>Ruilin</forenames></author></authors><title>Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis   Perspective</title><categories>cs.CL cs.AI</categories><comments>Accepted to ISMSI'25</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) have revolutionized natural language processing (NLP) by delivering state-of-the-art performance across a variety of tasks. Among these, Transformer-based models like BERT and GPT rely on pooling layers to aggregate token-level embeddings into sentence-level representations. Common pooling mechanisms such as Mean, Max, and Weighted Sum play a pivotal role in this aggregation process. Despite their widespread use, the comparative performance of these strategies on different LLM architectures remains underexplored. To address this gap, this paper investigates the effects of these pooling mechanisms on two prominent LLM families -- BERT and GPT, in the context of sentence-level sentiment analysis. Comprehensive experiments reveal that each pooling mechanism exhibits unique strengths and weaknesses depending on the task's specific requirements. Our findings underline the importance of selecting pooling methods tailored to the demands of particular applications, prompting a re-evaluation of common assumptions regarding pooling operations. By offering actionable insights, this study contributes to the optimization of LLM-based models for downstream tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.15458</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.15458</id><created>2024-11-23</created><authors><author><keyname>E</keyname><forenames>Jiawei</forenames></author><author><keyname>Zhang</keyname><forenames>Yinglong</forenames></author><author><keyname>Xia</keyname><forenames>Xuewen</forenames></author><author><keyname>Xu</keyname><forenames>Xing</forenames></author></authors><title>TANGNN: a Concise, Scalable and Effective Graph Neural Networks with   Top-m Attention Mechanism for Graph Representation Learning</title><categories>cs.LG cs.AI</categories><comments>The code and ArXivNet dataset are available at   https://github.com/ejwww/TANGNN</comments><journal-ref>Expert Systems with Applications, 2025, 126599</journal-ref><doi>10.1016/j.eswa.2025.126599</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In the field of deep learning, Graph Neural Networks (GNNs) and Graph Transformer models, with their outstanding performance and flexible architectural designs, have become leading technologies for processing structured data, especially graph data. Traditional GNNs often face challenges in capturing information from distant vertices effectively. In contrast, Graph Transformer models are particularly adept at managing long-distance node relationships. Despite these advantages, Graph Transformer models still encounter issues with computational and storage efficiency when scaled to large graph datasets. To address these challenges, we propose an innovative Graph Neural Network (GNN) architecture that integrates a Top-m attention mechanism aggregation component and a neighborhood aggregation component, effectively enhancing the model's ability to aggregate relevant information from both local and extended neighborhoods at each layer. This method not only improves computational efficiency but also enriches the node features, facilitating a deeper analysis of complex graph structures. Additionally, to assess the effectiveness of our proposed model, we have applied it to citation sentiment prediction, a novel task previously unexplored in the GNN field. Accordingly, we constructed a dedicated citation network, ArXivNet. In this dataset, we specifically annotated the sentiment polarity of the citations (positive, neutral, negative) to enable in-depth sentiment analysis. Our approach has shown superior performance across a variety of tasks including vertex classification, link prediction, sentiment prediction, graph regression, and visualization. It outperforms existing methods in terms of effectiveness, as demonstrated by experimental results on multiple datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.15594</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.15594</id><created>2024-11-23</created><updated>2025-02-01</updated><authors><author><keyname>Gu</keyname><forenames>Jiawei</forenames></author><author><keyname>Jiang</keyname><forenames>Xuhui</forenames></author><author><keyname>Shi</keyname><forenames>Zhichao</forenames></author><author><keyname>Tan</keyname><forenames>Hexiang</forenames></author><author><keyname>Zhai</keyname><forenames>Xuehao</forenames></author><author><keyname>Xu</keyname><forenames>Chengjin</forenames></author><author><keyname>Li</keyname><forenames>Wei</forenames></author><author><keyname>Shen</keyname><forenames>Yinghan</forenames></author><author><keyname>Ma</keyname><forenames>Shengjie</forenames></author><author><keyname>Liu</keyname><forenames>Honghao</forenames></author><author><keyname>Wang</keyname><forenames>Saizhuo</forenames></author><author><keyname>Zhang</keyname><forenames>Kun</forenames></author><author><keyname>Wang</keyname><forenames>Yuanzhuo</forenames></author><author><keyname>Gao</keyname><forenames>Wen</forenames></author><author><keyname>Ni</keyname><forenames>Lionel</forenames></author><author><keyname>Guo</keyname><forenames>Jian</forenames></author></authors><title>A Survey on LLM-as-a-Judge</title><categories>cs.CL cs.AI</categories><comments>Corrected typos &amp; more discussion on reasoning models 33 pages, 9   figures. arXiv admin note: text overlap with arXiv:2310.05470 by other   authors</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of "LLM-as-a-Judge," where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable, cost-effective, and consistent assessments, LLMs present a compelling alternative to traditional expert-driven evaluations. However, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization. This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by a novel benchmark designed for this purpose. To advance the development and real-world deployment of LLM-as-a-Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.18279</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.18279</id><created>2024-11-27</created><updated>2025-02-02</updated><authors><author><keyname>Zhang</keyname><forenames>Chaoyun</forenames></author><author><keyname>He</keyname><forenames>Shilin</forenames></author><author><keyname>Qian</keyname><forenames>Jiaxu</forenames></author><author><keyname>Li</keyname><forenames>Bowen</forenames></author><author><keyname>Li</keyname><forenames>Liqun</forenames></author><author><keyname>Qin</keyname><forenames>Si</forenames></author><author><keyname>Kang</keyname><forenames>Yu</forenames></author><author><keyname>Ma</keyname><forenames>Minghua</forenames></author><author><keyname>Liu</keyname><forenames>Guyue</forenames></author><author><keyname>Lin</keyname><forenames>Qingwei</forenames></author><author><keyname>Rajmohan</keyname><forenames>Saravan</forenames></author><author><keyname>Zhang</keyname><forenames>Dongmei</forenames></author><author><keyname>Zhang</keyname><forenames>Qi</forenames></author></authors><title>Large Language Model-Brained GUI Agents: A Survey</title><categories>cs.AI cs.CL cs.HC</categories><comments>The collection of papers reviewed in this survey will be hosted and   regularly updated on the GitHub repository:   https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey Additionally, a   searchable webpage is available at https://aka.ms/gui-agent for easier access   and exploration</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  GUIs have long been central to human-computer interaction, providing an intuitive and visually-driven way to access and interact with digital systems. The advent of LLMs, particularly multimodal models, has ushered in a new era of GUI automation. They have demonstrated exceptional capabilities in natural language understanding, code generation, and visual processing. This has paved the way for a new generation of LLM-brained GUI agents capable of interpreting complex GUI elements and autonomously executing actions based on natural language instructions. These agents represent a paradigm shift, enabling users to perform intricate, multi-step tasks through simple conversational commands. Their applications span across web navigation, mobile app interactions, and desktop automation, offering a transformative user experience that revolutionizes how individuals interact with software. This emerging field is rapidly advancing, with significant progress in both research and industry.   To provide a structured understanding of this trend, this paper presents a comprehensive survey of LLM-brained GUI agents, exploring their historical evolution, core components, and advanced techniques. We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness. Additionally, we examine emerging applications powered by these agents. Through a detailed analysis, this survey identifies key research gaps and outlines a roadmap for future advancements in the field. By consolidating foundational knowledge and state-of-the-art developments, this work aims to guide both researchers and practitioners in overcoming challenges and unlocking the full potential of LLM-brained GUI agents. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.18892</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.18892</id><created>2024-11-27</created><updated>2025-02-01</updated><authors><author><keyname>Ghasemi</keyname><forenames>Majid</forenames></author><author><keyname>Moosavi</keyname><forenames>Amir Hossein</forenames></author><author><keyname>Ebrahimi</keyname><forenames>Dariush</forenames></author></authors><title>A Comprehensive Survey of Reinforcement Learning: From Algorithms to   Practical Challenges</title><categories>cs.AI cs.LG</categories><comments>79 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reinforcement Learning (RL) has emerged as a powerful paradigm in Artificial Intelligence (AI), enabling agents to learn optimal behaviors through interactions with their environments. Drawing from the foundations of trial and error, RL equips agents to make informed decisions through feedback in the form of rewards or penalties. This paper presents a comprehensive survey of RL, meticulously analyzing a wide range of algorithms, from foundational tabular methods to advanced Deep Reinforcement Learning (DRL) techniques. We categorize and evaluate these algorithms based on key criteria such as scalability, sample efficiency, and suitability. We compare the methods in the form of their strengths and weaknesses in diverse settings. Additionally, we offer practical insights into the selection and implementation of RL algorithms, addressing common challenges like convergence, stability, and the exploration-exploitation dilemma. This paper serves as a comprehensive reference for researchers and practitioners aiming to harness the full potential of RL in solving complex, real-world problems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.19016</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.19016</id><created>2024-11-28</created><updated>2025-02-03</updated><authors><author><keyname>Mokadem</keyname><forenames>Riad</forenames><affiliation>IRIT-PYRAMIDE, IRIT</affiliation></author></authors><title>A Data Source Discovery Method using Several Domain Ontologies in P2P   Environments (IRIT research report, written in 2014)</title><categories>cs.DB</categories><comments>IRIT report research (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several data source discovery methods take into account the semantic heterogeneity problems by using several Domain Ontologies (DOs). However, most of them impose a topology of mapping links between DOs. DOs and mapping links are available on Internet but with an arbitrary topology. In this paper, we propose a data source Discovery method Adapted to any Mapping links Topology (DAMT) and taking into account semantic problems. Peers using the same DO are grouped in a Virtual Organization (VO) and connected in a Distributed Hash Table (DHT). Lookups within a same VO consists in a classical search in a DHT. Regarding the inter-VO discovery process, we propose an addressing system, based on the existing mapping links between DOs, to interconnect VOs. Furthermore, we adopt a lazy maintenance in order to reduce the number of messages required to update the system due to the dynamicity of peers. The performance analysis of the proposed method shows good results for inter-VO lookup queries. Also, it confirms a significant maintenance cost reduction when peers join and leave the system. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.19088</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.19088</id><created>2024-11-28</created><updated>2025-02-01</updated><authors><author><keyname>Castañeda</keyname><forenames>Ángel Luis Muñoz</forenames></author></authors><title>On the Goppa morphism</title><categories>math.AG cs.IT math.IT</categories><msc-class>14L24, 13A50, 13A02</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the geometric foundations of the space of geometric Goppa codes using the Tsfasman-Vladut H-construction. These codes are constructed from level structures, which extend the classical Goppa framework by incorporating invertible sheaves and their trivializations over rational points. A key contribution is the definition of the Goppa morphism, a map from the universal moduli space of level structures, denoted $LS_{g,n,d}$, to certain Grassmannian $\mathrm{Gr}(k,n)$. This morphism allows problems related to distinguishing attacks and key recovery in the context of Goppa Code-based Cryptography to be translated into a geometric language, addressing questions about the equations defining the image of the Goppa morphism and its fibers. Furthermore, we identify the ranges of the degree parameter $d$ that should be avoided to maintain security against distinguishers. Our results, valid over arbitrary base fields, also apply to convolutional Goppa codes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.00036</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.00036</id><created>2024-11-21</created><updated>2025-02-02</updated><authors><author><keyname>Lesniewski</keyname><forenames>Andrew</forenames></author><author><keyname>Trigila</keyname><forenames>Giulio</forenames></author></authors><title>Beyond Monte Carlo: Harnessing Diffusion Models to Simulate Financial   Market Dynamics</title><categories>q-fin.CP cs.AI cs.CE q-fin.PM</categories><comments>27 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a highly efficient and accurate methodology for generating synthetic financial market data using a diffusion model approach. The synthetic data produced by our methodology align closely with observed market data in several key aspects: (i) they pass the two-sample Cramer - von Mises test for portfolios of assets, and (ii) Q - Q plots demonstrate consistency across quantiles, including in the tails, between observed and generated market data. Moreover, the covariance matrices derived from a large set of synthetic market data exhibit significantly lower condition numbers compared to the estimated covariance matrices of the observed data. This property makes them suitable for use as regularized versions of the latter. For model training, we develop an efficient and fast algorithm based on numerical integration rather than Monte Carlo simulations. The methodology is tested on a large set of equity data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.00333</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.00333</id><created>2024-11-29</created><updated>2025-02-01</updated><authors><author><keyname>Deng</keyname><forenames>Junli</forenames></author><author><keyname>Luo</keyname><forenames>Yihao</forenames></author></authors><title>Gaussians on their Way: Wasserstein-Constrained 4D Gaussian Splatting   with State-Space Modeling</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Dynamic scene rendering has taken a leap forward with the rise of 4D Gaussian Splatting, but there's still one elusive challenge: how to make 3D Gaussians move through time as naturally as they would in the real world, all while keeping the motion smooth and consistent. In this paper, we unveil a fresh approach that blends state-space modeling with Wasserstein geometry, paving the way for a more fluid and coherent representation of dynamic scenes. We introduce a State Consistency Filter that merges prior predictions with the current observations, enabling Gaussians to stay true to their way over time. We also employ Wasserstein distance regularization to ensure smooth, consistent updates of Gaussian parameters, reducing motion artifacts. Lastly, we leverage Wasserstein geometry to capture both translational motion and shape deformations, creating a more physically plausible model for dynamic scenes. Our approach guides Gaussians along their natural way in the Wasserstein space, achieving smoother, more realistic motion and stronger temporal coherence. Experimental results show significant improvements in rendering quality and efficiency, outperforming current state-of-the-art techniques. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.00686</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.00686</id><created>2024-12-01</created><updated>2025-02-02</updated><authors><author><keyname>Qharabagh</keyname><forenames>Muhammad Fetrat</forenames></author><author><keyname>Ghofrani</keyname><forenames>Mohammadreza</forenames></author><author><keyname>Fountoulakis</keyname><forenames>Kimon</forenames></author></authors><title>LVLM-COUNT: Enhancing the Counting Ability of Large Vision-Language   Models</title><categories>cs.CV cs.AI</categories><comments>31 pages, 24 Figures, 10 Tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Counting is a fundamental operation for various visual tasks in real-life applications, requiring both object recognition and robust counting capabilities. Despite their advanced visual perception, large vision-language models (LVLMs) struggle with counting tasks, especially when the number of objects exceeds those commonly encountered during training. We enhance LVLMs' counting abilities using a divide-and-conquer approach, breaking counting problems into sub-counting tasks. Our method employs a mechanism that prevents bisecting and thus repetitive counting of objects, which occurs in a naive divide-and-conquer approach. Unlike prior methods, which do not generalize well to counting datasets they have not been trained on, our method performs well on new datasets without any additional training or fine-tuning. We demonstrate that our approach enhances the counting capability of LVLMs across various datasets and benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.00730</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.00730</id><created>2024-12-01</created><updated>2025-02-03</updated><authors><author><keyname>Kästingschäfer</keyname><forenames>Marius</forenames></author><author><keyname>Gieruc</keyname><forenames>Théo</forenames></author><author><keyname>Bernhard</keyname><forenames>Sebastian</forenames></author><author><keyname>Campbell</keyname><forenames>Dylan</forenames></author><author><keyname>Insafutdinov</keyname><forenames>Eldar</forenames></author><author><keyname>Najafli</keyname><forenames>Eyvaz</forenames></author><author><keyname>Brox</keyname><forenames>Thomas</forenames></author></authors><title>SEED4D: A Synthetic Ego--Exo Dynamic 4D Data Generator, Driving Dataset   and Benchmark</title><categories>cs.CV</categories><comments>WACV 2025. Project page: https://seed4d.github.io/. Code:   https://github.com/continental/seed4d</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Models for egocentric 3D and 4D reconstruction, including few-shot interpolation and extrapolation settings, can benefit from having images from exocentric viewpoints as supervision signals. No existing dataset provides the necessary mixture of complex, dynamic, and multi-view data. To facilitate the development of 3D and 4D reconstruction methods in the autonomous driving context, we propose a Synthetic Ego--Exo Dynamic 4D (SEED4D) data generator and dataset. We present a customizable, easy-to-use data generator for spatio-temporal multi-view data creation. Our open-source data generator allows the creation of synthetic data for camera setups commonly used in the NuScenes, KITTI360, and Waymo datasets. Additionally, SEED4D encompasses two large-scale multi-view synthetic urban scene datasets. Our static (3D) dataset encompasses 212k inward- and outward-facing vehicle images from 2k scenes, while our dynamic (4D) dataset contains 16.8M images from 10k trajectories, each sampled at 100 points in time with egocentric images, exocentric images, and LiDAR data. The datasets and the data generator can be found at https://seed4d.github.io/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.00776</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.00776</id><created>2024-12-01</created><updated>2025-02-03</updated><authors><author><keyname>Zhao</keyname><forenames>Chongyang</forenames></author><author><keyname>Gong</keyname><forenames>Dong</forenames></author></authors><title>Learning Mamba as a Continual Learner</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continual learning (CL) aims to efficiently learn and accumulate knowledge from a data stream with different distributions. By formulating CL as a sequence prediction task, meta-continual learning (MCL) enables to meta-learn an efficient continual learner based on the recent advanced sequence models, e.g., Transformers. Although attention-free models (e.g., Linear Transformers) can ideally match CL's essential objective and efficiency requirements, they usually perform not well in MCL. Considering that the attention-free Mamba achieves excellent performances matching Transformers' on general sequence modeling tasks, in this paper, we aim to answer a question -- Can attention-free Mamba perform well on MCL? By formulating Mamba with selective state space models (SSMs) for MCL tasks, we propose to meta-learn Mamba as a continual learner, referred to as MambaCL. By incorporating selectivity regularization, we can effectively train MambaCL. Through comprehensive experiments across various CL tasks, we also explore how Mamba and other models perform in different MCL scenarios. Our experiments and analyses highlight the promising performance and generalization capabilities of Mamba in MCL. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.00798</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.00798</id><created>2024-12-01</created><updated>2025-02-03</updated><authors><author><keyname>Song</keyname><forenames>Seockbean</forenames></author><author><keyname>Yoon</keyname><forenames>Youngsik</forenames></author><author><keyname>Wang</keyname><forenames>Siwei</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Ok</keyname><forenames>Jungseul</forenames></author></authors><title>Combinatorial Rising Bandit</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Combinatorial online learning is a fundamental task to decide the optimal combination of base arms in sequential interactions with systems providing uncertain rewards, which is applicable to diverse domains such as robotics, social advertising, network routing and recommendation systems. In real-world scenarios, we often observe rising rewards, where the selection of a base arm not only provides an instantaneous reward but also contributes to the enhancement of future rewards, {\it e.g.}, robots enhancing proficiency through practice and social influence strengthening in the history of successful recommendations. To address this, we introduce the problem of combinatorial rising bandit to minimize policy regret and propose a provably efficient algorithm, called Combinatorial Rising Upper Confidence Bound (CRUCB), of which regret upper bound is close to a regret lower bound. To the best of our knowledge, previous studies do not provide a sub-linear regret lower bound, making it impossible to assess the efficiency of their algorithms. However, we provide the sub-linear regret lower bound for combinatorial rising bandit and show that CRUCB is provably efficient by showing that the regret upper bound is close to the regret lower bound. In addition, we empirically demonstrate the effectiveness and superiority of CRUCB not only in synthetic environments but also in realistic applications of deep reinforcement learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.01292</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.01292</id><created>2024-12-02</created><updated>2025-02-02</updated><authors><author><keyname>Zhi</keyname><forenames>Hongyan</forenames></author><author><keyname>Chen</keyname><forenames>Peihao</forenames></author><author><keyname>Li</keyname><forenames>Junyan</forenames></author><author><keyname>Ma</keyname><forenames>Shuailei</forenames></author><author><keyname>Sun</keyname><forenames>Xinyu</forenames></author><author><keyname>Xiang</keyname><forenames>Tianhang</forenames></author><author><keyname>Lei</keyname><forenames>Yinjie</forenames></author><author><keyname>Tan</keyname><forenames>Mingkui</forenames></author><author><keyname>Gan</keyname><forenames>Chuang</forenames></author></authors><title>LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual   Preferences</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on 3D Vision-Language Models (3D-VLMs) is gaining increasing attention, which is crucial for developing embodied AI within 3D scenes, such as visual navigation and embodied question answering. Due to the high density of visual features, especially in large 3D scenes, accurately locating task-relevant visual information is challenging. Existing works attempt to segment all objects and consider their features as scene representations. However, these task-agnostic object features include much redundant information and missing details for the task-relevant area. To tackle these problems, we propose LSceneLLM, an adaptive framework that automatically identifies task-relevant areas by leveraging LLM's visual preference for different tasks, followed by a plug-and-play scene magnifier module to capture fine-grained details in focused areas. Specifically, a dense token selector examines the attention map of LLM to identify visual preferences for the instruction input. It then magnifies fine-grained details of the focusing area. An adaptive self-attention module is leveraged to fuse the coarse-grained and selected fine-grained visual information. To comprehensively evaluate the large scene understanding ability of 3D-VLMs, we further introduce a cross-room understanding benchmark, XR-Scene, which contains a series of large scene understanding tasks including XR-QA, XR-EmbodiedPlanning, and XR-SceneCaption. Experiments show that our method surpasses existing methods on both large scene understanding and existing scene understanding benchmarks. Plunging our scene magnifier module into the existing 3D-VLMs also brings significant improvement. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.01626</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.01626</id><created>2024-12-02</created><updated>2025-02-02</updated><authors><author><keyname>Mozafari</keyname><forenames>Jamshid</forenames></author><author><keyname>Gerhold</keyname><forenames>Florian</forenames></author><author><keyname>Jatowt</keyname><forenames>Adam</forenames></author></authors><title>WikiHint: A Human-Annotated Dataset for Hint Ranking and Generation</title><categories>cs.CL cs.IR</categories><comments>Submitted to SIGIR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The use of Large Language Models (LLMs) has increased significantly with users frequently asking questions to chatbots. In the time when information is readily accessible, it is crucial to stimulate and preserve human cognitive abilities and maintain strong reasoning skills. This paper addresses such challenges by promoting the use of hints as an alternative or a supplement to direct answers. We first introduce a manually constructed hint dataset, WikiHint, which is based on Wikipedia and includes 5,000 hints created for 1,000 questions. We then finetune open-source LLMs such as LLaMA-3.1 for hint generation in answer-aware and answeragnostic contexts. We assess the effectiveness of the hints with human participants who answer questions with and without the aid of hints. Additionally, we introduce a lightweight evaluation method, HintRank, to evaluate and rank hints in both answeraware and answer-agnostic settings. Our findings show that (a) the dataset helps generate more effective hints, (b) including answer information along with questions generally improves quality of generated hints, and (c) encoder-based models perform better than decoder-based models in hint ranking. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.01791</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.01791</id><created>2024-11-27</created><updated>2025-02-01</updated><authors><author><keyname>Singh</keyname><forenames>Ritvik</forenames></author><author><keyname>Allshire</keyname><forenames>Arthur</forenames></author><author><keyname>Handa</keyname><forenames>Ankur</forenames></author><author><keyname>Ratliff</keyname><forenames>Nathan</forenames></author><author><keyname>Van Wyk</keyname><forenames>Karl</forenames></author></authors><title>DextrAH-RGB: Visuomotor Policies to Grasp Anything with Dexterous Hands</title><categories>cs.RO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  One of the most important, yet challenging, skills for a dexterous robot is grasping a diverse range of objects. Much of the prior work has been limited by speed, generality, or reliance on depth maps and object poses. In this paper, we introduce DextrAH-RGB, a system that can perform dexterous arm-hand grasping end-to-end from RGB image input. We train a privileged fabric-guided policy (FGP) in simulation through reinforcement learning that acts on a geometric fabric controller to dexterously grasp a wide variety of objects. We then distill this privileged FGP into a RGB-based FGP strictly in simulation using photorealistic tiled rendering. To our knowledge, this is the first work that is able to demonstrate robust sim2real transfer of an end2end RGB-based policy for complex, dynamic, contact-rich tasks such as dexterous grasping. DextrAH-RGB is competitive with depth-based dexterous grasping policies, and generalizes to novel objects with unseen geometry, texture, and lighting conditions in the real world. Videos of our system grasping a diverse range of unseen objects are available at \url{https://dextrah-rgb.github.io/}. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.01940</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.01940</id><created>2024-12-02</created><updated>2025-02-02</updated><authors><author><keyname>Munyampirwa</keyname><forenames>Blaise</forenames></author><author><keyname>Lakshman</keyname><forenames>Vihan</forenames></author><author><keyname>Coleman</keyname><forenames>Benjamin</forenames></author></authors><title>Down with the Hierarchy: The 'H' in HNSW Stands for "Hubs"</title><categories>cs.LG cs.DB cs.IR</categories><comments>12 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Driven by recent breakthrough advances in neural representation learning, approximate near-neighbor (ANN) search over vector embeddings has emerged as a critical computational workload. With the introduction of the seminal Hierarchical Navigable Small World (HNSW) algorithm, graph-based indexes have established themselves as the overwhelmingly dominant paradigm for efficient and scalable ANN search. As the name suggests, HNSW searches a layered hierarchical graph to quickly identify neighborhoods of similar points to a given query vector. But is this hierarchy even necessary? A rigorous experimental analysis to answer this question would provide valuable insights into the nature of algorithm design for ANN search and motivate directions for future work in this increasingly crucial domain. To that end, we conduct an extensive benchmarking study covering more large-scale datasets than prior investigations of this question. We ultimately find that a flat navigable small world graph graph retains all of the benefits of HNSW on high-dimensional datasets, with latency and recall performance essentially \emph{identical} to the original algorithm but with less memory overhead. Furthermore, we go a step further and study \emph{why} the hierarchy of HNSW provides no benefit in high dimensions, hypothesizing that navigable small world graphs contain a well-connected, frequently traversed ``highway" of hub nodes that maintain the same purported function as the hierarchical layers. We present compelling empirical evidence that the \emph{Hub Highway Hypothesis} holds for real datasets and investigate the mechanisms by which the highway forms. The implications of this hypothesis may also provide future research directions in developing enhancements to graph-based ANN search. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.01979</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.01979</id><created>2024-12-02</created><updated>2025-02-01</updated><authors><author><keyname>Xing</keyname><forenames>Jinming</forenames></author><author><keyname>Xue</keyname><forenames>Chang</forenames></author><author><keyname>Luo</keyname><forenames>Dongwen</forenames></author><author><keyname>Xing</keyname><forenames>Ruilin</forenames></author></authors><title>FGATT: A Robust Framework for Wireless Data Imputation Using Fuzzy Graph   Attention Networks and Transformer Encoders</title><categories>cs.LG cs.IR cs.NE</categories><comments>Accepted to ISMSI'25</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Missing data is a pervasive challenge in wireless networks and many other domains, often compromising the performance of machine learning and deep learning models. To address this, we propose a novel framework, FGATT, that combines the Fuzzy Graph Attention Network (FGAT) with the Transformer encoder to perform robust and accurate data imputation. FGAT leverages fuzzy rough sets and graph attention mechanisms to capture spatial dependencies dynamically, even in scenarios where predefined spatial information is unavailable. The Transformer encoder is employed to model temporal dependencies, utilizing its self-attention mechanism to focus on significant time-series patterns. A self-adaptive graph construction method is introduced to enable dynamic connectivity learning, ensuring the framework's applicability to a wide range of wireless datasets. Extensive experiments demonstrate that our approach outperforms state-of-the-art methods in imputation accuracy and robustness, particularly in scenarios with substantial missing data. The proposed model is well-suited for applications in wireless sensor networks and IoT environments, where data integrity is critical. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.02626</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.02626</id><created>2024-12-03</created><updated>2025-02-02</updated><authors><author><keyname>Varun</keyname><forenames>Yerram</forenames></author><author><keyname>Madhavan</keyname><forenames>Rahul</forenames></author><author><keyname>Addepalli</keyname><forenames>Sravanti</forenames></author><author><keyname>Suggala</keyname><forenames>Arun</forenames></author><author><keyname>Shanmugam</keyname><forenames>Karthikeyan</forenames></author><author><keyname>Jain</keyname><forenames>Prateek</forenames></author></authors><title>Time-Reversal Provides Unsupervised Feedback to LLMs</title><categories>cs.CL cs.AI</categories><comments>Accepted as a spotlight in NeurIPS 2024</comments><journal-ref>The Thirty-Eighth Annual Conference on Neural Information   Processing Systems (NeurIPS), 2024</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large Language Models (LLMs) are typically trained to predict in the forward direction of time. However, recent works have shown that prompting these models to look back and critique their own generations can produce useful feedback. Motivated by this, we explore the question of whether LLMs can be empowered to think (predict and score) backwards to provide unsupervised feedback that complements forward LLMs. Towards this, we introduce Time Reversed Language Models (TRLMs), which can score and generate queries when conditioned on responses, effectively functioning in the reverse direction of time. Further, to effectively infer in the response to query direction, we pre-train and fine-tune a language model (TRLM-Ba) in the reverse token order from scratch. We show empirically (and theoretically in a stylized setting) that time-reversed models can indeed complement forward model predictions when used to score the query given response for re-ranking multiple forward generations. We obtain up to 5\% improvement on the widely used AlpacaEval Leaderboard over the competent baseline of best-of-N re-ranking using self log-perplexity scores. We further show that TRLM scoring outperforms conventional forward scoring of response given query, resulting in significant gains in applications such as citation generation and passage retrieval. We next leverage the generative ability of TRLM to augment or provide unsupervised feedback to input safety filters of LLMs, demonstrating a drastic reduction in false negative rate with negligible impact on false positive rates against several attacks published on the popular JailbreakBench leaderboard. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.04653</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.04653</id><created>2024-12-05</created><updated>2025-02-01</updated><authors><author><keyname>Arabi</keyname><forenames>Kasra</forenames></author><author><keyname>Feuer</keyname><forenames>Benjamin</forenames></author><author><keyname>Witter</keyname><forenames>R. Teal</forenames></author><author><keyname>Hegde</keyname><forenames>Chinmay</forenames></author><author><keyname>Cohen</keyname><forenames>Niv</forenames></author></authors><title>Hidden in the Noise: Two-Stage Robust Watermarking for Images</title><categories>cs.CV cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the quality of image generators continues to improve, deepfakes become a topic of considerable societal debate. Image watermarking allows responsible model owners to detect and label their AI-generated content, which can mitigate the harm. Yet, current state-of-the-art methods in image watermarking remain vulnerable to forgery and removal attacks. This vulnerability occurs in part because watermarks distort the distribution of generated images, unintentionally revealing information about the watermarking techniques.   In this work, we first demonstrate a distortion-free watermarking method for images, based on a diffusion model's initial noise. However, detecting the watermark requires comparing the initial noise reconstructed for an image to all previously used initial noises. To mitigate these issues, we propose a two-stage watermarking framework for efficient detection. During generation, we augment the initial noise with generated Fourier patterns to embed information about the group of initial noises we used. For detection, we (i) retrieve the relevant group of noises, and (ii) search within the given group for an initial noise that might match our image. This watermarking approach achieves state-of-the-art robustness to forgery and removal against a large battery of attacks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.05892</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.05892</id><created>2024-12-08</created><updated>2025-02-03</updated><authors><author><keyname>Cheng</keyname><forenames>Ruoxi</forenames></author><author><keyname>Ding</keyname><forenames>Yizhong</forenames></author><author><keyname>Cao</keyname><forenames>Shuirong</forenames></author><author><keyname>Duan</keyname><forenames>Ranjie</forenames></author><author><keyname>Jia</keyname><forenames>Xiaoshuang</forenames></author><author><keyname>Yuan</keyname><forenames>Shaowei</forenames></author><author><keyname>Wang</keyname><forenames>Zhiqiang</forenames></author><author><keyname>Jia</keyname><forenames>Xiaojun</forenames></author></authors><title>PBI-Attack: Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack   for Toxicity Maximization</title><categories>cs.CR cs.AI</categories><comments>Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack for   Toxicity Maximization</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the vulnerabilities of Large Vision Language Models (LVLMs) to jailbreak attacks is essential for their responsible real-world deployment. Most previous work requires access to model gradients, or is based on human knowledge (prompt engineering) to complete jailbreak, and they hardly consider the interaction of images and text, resulting in inability to jailbreak in black box scenarios or poor performance. To overcome these limitations, we propose a Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack for toxicity maximization, referred to as PBI-Attack. Our method begins by extracting malicious features from a harmful corpus using an alternative LVLM and embedding these features into a benign image as prior information. Subsequently, we enhance these features through bidirectional cross-modal interaction optimization, which iteratively optimizes the bimodal perturbations in an alternating manner through greedy search, aiming to maximize the toxicity of the generated response. The toxicity level is quantified using a well-trained evaluation model. Experiments demonstrate that PBI-Attack outperforms previous state-of-the-art jailbreak methods, achieving an average attack success rate of 92.5% across three open-source LVLMs and around 67.3% on three closed-source LVLMs. Disclaimer: This paper contains potentially disturbing and offensive content. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.06484</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.06484</id><created>2024-12-09</created><updated>2025-02-02</updated><authors><author><keyname>Samuel</keyname><forenames>David</forenames></author><author><keyname>Mikhailov</keyname><forenames>Vladislav</forenames></author><author><keyname>Velldal</keyname><forenames>Erik</forenames></author><author><keyname>Øvrelid</keyname><forenames>Lilja</forenames></author><author><keyname>Charpentier</keyname><forenames>Lucas Georges Gabriel</forenames></author><author><keyname>Kutuzov</keyname><forenames>Andrey</forenames></author><author><keyname>Oepen</keyname><forenames>Stephan</forenames></author></authors><title>Small Languages, Big Models: A Study of Continual Training on Languages   of Norway</title><categories>cs.CL</categories><comments>Published at NoDaLiDa 2025</comments><journal-ref>Proceedings of the 25th Nordic Conference on Computational   Linguistics (NoDaLiDa 2025). Tallinn, Estonia</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training large language models requires vast amounts of data, posing a challenge for less widely spoken languages like Norwegian and even more so for truly low-resource languages like Northern S\'ami. To address this issue, we present a novel three-stage continual training approach that substantially improves the downstream performance together with the inference efficiency for the target languages. Based on our findings, we train, evaluate, and openly release a new generative language model for Norwegian Bokm\r{a}l, Nynorsk, and Northern S\'ami with 11.4 billion parameters: NorMistral-11B. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.06564</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.06564</id><created>2024-12-09</created><updated>2025-02-02</updated><authors><author><keyname>Leça</keyname><forenames>Matheus de Morais</forenames></author><author><keyname>Valença</keyname><forenames>Lucas</forenames></author><author><keyname>Santos</keyname><forenames>Reydne</forenames></author><author><keyname>Santos</keyname><forenames>Ronnie de Souza</forenames></author></authors><title>Applications and Implications of Large Language Models in Qualitative   Analysis: A New Frontier for Empirical Software Engineering</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The use of large language models (LLMs) for qualitative analysis is gaining attention in various fields, including software engineering, where qualitative methods are essential for understanding human and social factors. This study aimed to investigate how LLMs are currently used in qualitative analysis and their potential applications in software engineering research, focusing on the benefits, limitations, and practices associated with their use. A systematic mapping study was conducted, analyzing 21 relevant studies to explore reported uses of LLMs for qualitative analysis. The findings indicate that LLMs are primarily used for tasks such as coding, thematic analysis, and data categorization, offering benefits like increased efficiency and support for new researchers. However, limitations such as output variability, challenges in capturing nuanced perspectives, and ethical concerns related to privacy and transparency were also identified. The study emphasizes the need for structured strategies and guidelines to optimize LLM use in qualitative research within software engineering, enhancing their effectiveness while addressing ethical considerations. While LLMs show promise in supporting qualitative analysis, human expertise remains crucial for interpreting data, and ongoing exploration of best practices will be vital for their successful integration into empirical software engineering research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.07030</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.07030</id><created>2024-12-09</created><updated>2025-02-01</updated><authors><author><keyname>Abaskohi</keyname><forenames>Amirhossein</forenames></author><author><keyname>Gella</keyname><forenames>Spandana</forenames></author><author><keyname>Carenini</keyname><forenames>Giuseppe</forenames></author><author><keyname>Laradji</keyname><forenames>Issam H.</forenames></author></authors><title>FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge   Distillation for Question Answering</title><categories>cs.CL cs.AI cs.CV cs.IR cs.LG</categories><comments>20 pages, 11 figures, 10 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multimodal multihop question answering is a complex task that requires reasoning over multiple sources of information, such as images and text, to answer questions. While there has been significant progress in visual question answering, the multihop setting remains unexplored due to the lack of high-quality datasets. Current methods focus on single-hop question answering or a single modality, which makes them unsuitable for real-world scenarios such as analyzing multimodal educational materials, summarizing lengthy academic articles, or interpreting scientific studies that combine charts, images, and text. To address this gap, we propose a novel methodology, introducing the first framework for creating a high-quality dataset that enables training models for multimodal multihop question answering. Our approach consists of a 5-stage pipeline that involves acquiring relevant multimodal documents from Wikipedia, synthetically generating high-level questions and answers, and validating them through rigorous criteria to ensure quality data. We evaluate our methodology by training models on our synthesized dataset and testing on two benchmarks, our results demonstrate that, with an equal sample size, models trained on our synthesized data outperform those trained on human-collected data by 1.9 in exact match (EM) on average. We believe our data synthesis method will serve as a strong foundation for training and evaluating multimodal multihop question answering models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.07594</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.07594</id><created>2024-12-10</created><updated>2025-02-03</updated><authors><author><keyname>Chang</keyname><forenames>Qikai</forenames></author><author><keyname>Chen</keyname><forenames>Mingjun</forenames></author><author><keyname>Pi</keyname><forenames>Changpeng</forenames></author><author><keyname>Hu</keyname><forenames>Pengfei</forenames></author><author><keyname>Zhang</keyname><forenames>Zhenrong</forenames></author><author><keyname>Ma</keyname><forenames>Jiefeng</forenames></author><author><keyname>Du</keyname><forenames>Jun</forenames></author><author><keyname>Yin</keyname><forenames>Baocai</forenames></author><author><keyname>Hu</keyname><forenames>Jinshui</forenames></author></authors><title>RFL: Simplifying Chemical Structure Recognition with Ring-Free Language</title><categories>cs.CV</categories><comments>9 pages, 6 figures. Accepted by AAAI 2025 Oral</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The primary objective of Optical Chemical Structure Recognition is to identify chemical structure images into corresponding markup sequences. However, the complex two-dimensional structures of molecules, particularly those with rings and multiple branches, present significant challenges for current end-to-end methods to learn one-dimensional markup directly. To overcome this limitation, we propose a novel Ring-Free Language (RFL), which utilizes a divide-and-conquer strategy to describe chemical structures in a hierarchical form. RFL allows complex molecular structures to be decomposed into multiple parts, ensuring both uniqueness and conciseness while enhancing readability. This approach significantly reduces the learning difficulty for recognition models. Leveraging RFL, we propose a universal Molecular Skeleton Decoder (MSD), which comprises a skeleton generation module that progressively predicts the molecular skeleton and individual rings, along with a branch classification module for predicting branch information. Experimental results demonstrate that the proposed RFL and MSD can be applied to various mainstream methods, achieving superior performance compared to state-of-the-art approaches in both printed and handwritten scenarios. The code is available at https://github.com/JingMog/RFL-MSD. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.07904</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.07904</id><created>2024-12-10</created><updated>2025-02-01</updated><authors><author><keyname>Robbins</keyname><forenames>Stephen</forenames></author></authors><title>Score Change of Variables</title><categories>cs.LG cs.AI math.PR</categories><comments>27 pages, 6 figures</comments><msc-class>68T01</msc-class><acm-class>I.2.6</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We derive a general change of variables formula for score functions, showing that for a smooth, invertible transformation $\mathbf{y} = \phi(\mathbf{x})$, the transformed score function $\nabla_{\mathbf{y}} \log q(\mathbf{y})$ can be expressed directly in terms of $\nabla_{\mathbf{x}} \log p(\mathbf{x})$. Using this result, we develop two applications: First, we establish a reverse-time It\^o lemma for score-based diffusion models, allowing the use of $\nabla_{\mathbf{x}} \log p_t(\mathbf{x})$ to reverse an SDE in the transformed space without directly learning $\nabla_{\mathbf{y}} \log q_t(\mathbf{y})$. This approach enables training diffusion models in one space but sampling in another, effectively decoupling the forward and reverse processes. Second, we introduce generalized sliced score matching, extending traditional sliced score matching from linear projections to arbitrary smooth transformations. This provides greater flexibility in high-dimensional density estimation. We demonstrate these theoretical advances through applications to diffusion on the probability simplex and empirically compare our generalized score matching approach against traditional sliced score matching methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.08009</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.08009</id><created>2024-12-10</created><updated>2025-02-02</updated><authors><author><keyname>Dang</keyname><forenames>Hiep Vo</forenames></author><author><keyname>Choi</keyname><forenames>Joseph B.</forenames></author><author><keyname>Nguyen</keyname><forenames>Phong C. H.</forenames></author></authors><title>FLRONet: Deep Operator Learning for High-Fidelity Fluid Flow Field   Reconstruction from Sparse Sensor Measurements</title><categories>physics.flu-dyn cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Reconstructing high-fidelity fluid flow fields from sparse sensor measurements is vital for many science and engineering applications but remains challenging because of dimensional disparities between state and observational spaces. Due to such dimensional differences, the measurement operator becomes ill-conditioned and non-invertible, making the reconstruction of flow fields from sensor measurements extremely difficult. Although sparse optimization and machine learning address the above problems to some extent, questions about their generalization and efficiency remain, particularly regarding the discretization dependence of these models. In this context, deep operator learning offers a better solution as this approach models mappings between infinite-dimensional functional spaces, enabling superior generalization and discretization-independent reconstruction. We introduce FLRONet, a deep operator learning framework that is trained to reconstruct fluid flow fields from sparse sensor measurements. FLRONet employs a branch-trunk network architecture to represent the inverse measurement operator that maps sensor observations to the original flow field, a continuous function of both space and time. Validation performed on the CFDBench dataset has demonstrated that FLRONet consistently achieves high levels of reconstruction accuracy and robustness, even in scenarios where sensor measurements are inaccurate or missing. Furthermore, the operator learning approach endows FLRONet with the capability to perform zero-shot super-resolution in both spatial and temporal domains, offering a solution for rapid reconstruction of high-fidelity flow fields. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.09044</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.09044</id><created>2024-12-12</created><updated>2025-02-01</updated><authors><author><keyname>Rao</keyname><forenames>Haocong</forenames></author><author><keyname>Miao</keyname><forenames>Chunyan</forenames></author></authors><title>Motif Guided Graph Transformer with Combinatorial Skeleton Prototype   Learning for Skeleton-Based Person Re-Identification</title><categories>cs.CV cs.AI</categories><comments>Accepted by AAAI 2025. Codes are available at   https://github.com/Kali-Hac/MoCos. The Appendix A for Experiments (13 pages)   and Appendix B for Theoretical Analysis (5 pages) are included in the version   [v1] at arXiv:2412.09044</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Person re-identification (re-ID) via 3D skeleton data is a challenging task with significant value in many scenarios. Existing skeleton-based methods typically assume virtual motion relations between all joints, and adopt average joint or sequence representations for learning. However, they rarely explore key body structure and motion such as gait to focus on more important body joints or limbs, while lacking the ability to fully mine valuable spatial-temporal sub-patterns of skeletons to enhance model learning. This paper presents a generic Motif guided graph transformer with Combinatorial skeleton prototype learning (MoCos) that exploits structure-specific and gait-related body relations as well as combinatorial features of skeleton graphs to learn effective skeleton representations for person re-ID. In particular, motivated by the locality within joints' structure and the body-component collaboration in gait, we first propose the motif guided graph transformer (MGT) that incorporates hierarchical structural motifs and gait collaborative motifs, which simultaneously focuses on multi-order local joint correlations and key cooperative body parts to enhance skeleton relation learning. Then, we devise the combinatorial skeleton prototype learning (CSP) that leverages random spatial-temporal combinations of joint nodes and skeleton graphs to generate diverse sub-skeleton and sub-tracklet representations, which are contrasted with the most representative features (prototypes) of each identity to learn class-related semantics and discriminative skeleton representations. Extensive experiments validate the superior performance of MoCos over existing state-of-the-art models. We further show its generality under RGB-estimated skeletons, different graph modeling, and unsupervised scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.10243</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.10243</id><created>2024-12-13</created><updated>2025-02-02</updated><authors><author><keyname>Sheet</keyname><forenames>Yazen S.</forenames></author><author><keyname>Thanoun</keyname><forenames>Mohammed Younis</forenames></author><author><keyname>Alsharbaty</keyname><forenames>Firas S.</forenames></author></authors><title>Next-Generation Industrial Networks: Integrating Time-Sensitive   Networking for Smart Factory Reliability</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The traditional industrial communication networks may not meet the requirements of the main smart factory applications together, such as Remote control and safety applications (which considered as strict real time applications) and augmented reality (which consumes wide bandwidth). This work suggests an enhanced communication network to serve an optimum case for the smart factory includes heavy data applications and real-time applications in one hand using the concepts of time sensitive networks (TSN) to address the limitation of real-time applications and the edge computing to assimilate the heavy data applications. The current work submits an experimental scenario to exploit H.265 compression method based on edge computing concept to mitigate consuming the capacity on overall network performance for augmented reality application. The results of enhanced communication network indicated that the latency of real time applications was less than 1msec while the packet data delivery of rest applications was 99.999%. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.10338</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.10338</id><created>2024-12-13</created><updated>2025-01-31</updated><authors><author><keyname>Liu</keyname><forenames>Hanzhou</forenames></author><author><keyname>Liu</keyname><forenames>Chengkai</forenames></author><author><keyname>Xu</keyname><forenames>Jiacong</forenames></author><author><keyname>Jiang</keyname><forenames>Peng</forenames></author><author><keyname>Lu</keyname><forenames>Mi</forenames></author></authors><title>XYScanNet: An Interpretable State Space Model for Perceptual Image   Deblurring</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep state-space models (SSMs), like recent Mamba architectures, are emerging as a promising alternative to CNN and Transformer networks. Existing Mamba-based restoration methods process the visual data by leveraging a flatten-and-scan strategy that converts image patches into a 1D sequence before scanning. However, this scanning paradigm ignores local pixel dependencies and introduces spatial misalignment by positioning distant pixels incorrectly adjacent, which reduces local noise-awareness and degrades image sharpness in low-level vision tasks. To overcome these issues, we propose a novel slice-and-scan strategy that alternates scanning along intra- and inter-slices. We further design a new Vision State Space Module (VSSM) for image deblurring, and tackle the inefficiency challenges of the current Mamba-based vision module. Building upon this, we develop XYScanNet, an SSM architecture integrated with a lightweight feature fusion module for enhanced image deblurring. XYScanNet, maintains competitive distortion metrics and significantly improves perceptual performance. Experimental results show that XYScanNet enhances KID by $17\%$ compared to the nearest competitor. Our code will be released soon. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.10399</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.10399</id><created>2024-12-04</created><updated>2025-01-31</updated><authors><author><keyname>Liu</keyname><forenames>Michael</forenames></author><author><keyname>Wang</keyname><forenames>Xinlei</forenames></author><author><keyname>Li</keyname><forenames>Minchen</forenames></author></authors><title>CK-MPM: A Compact-Kernel Material Point Method</title><categories>cs.GR physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Material Point Method (MPM) has become a cornerstone of physics-based simulation, widely used in geomechanics and computer graphics for modeling phenomena such as granular flows, viscoelasticity, fracture mechanics, etc. Despite its versatility, the original MPM suffers from cell-crossing instabilities caused by discontinuities in particle-grid transfer kernels. Existing solutions mostly mitigate these issues by adopting smoother shape functions, but at the cost of increased numerical diffusion and computational overhead due to larger kernel support. In this paper, we propose a novel $C^2$-continuous compact kernel for MPM that achieves a unique balance in terms of stability, accuracy, and computational efficiency. Our method integrates seamlessly with Affine Particle-In-Cell (APIC) and Moving Least Squares (MLS) MPM, while only doubling the number of grid nodes associated with each particle compared to linear kernels. At its core is an innovative dual-grid framework, which associates particles with grid nodes exclusively within the cells they occupy on two staggered grids, ensuring consistent and stable force computations. We demonstrate that our method can be conveniently implemented using a domain-specific language, Taichi, or based on open-source GPU MPM frameworks, achieving faster runtime and less numerical diffusion compared to quadratic B-spline MPM. Comprehensive validation through unit tests, comparative studies, and stress tests demonstrates the efficacy of our approach in conserving both linear and angular momentum, handling stiff materials, and scaling efficiently for large-scale simulations. Our results highlight the transformative potential of compact, high-order kernels in advancing MPM's capabilities for stable, accurate, and high-performance simulations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.11439</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.11439</id><created>2024-12-15</created><updated>2025-02-01</updated><authors><author><keyname>Tao</keyname><forenames>Nianze</forenames></author></authors><title>Bayesian Flow Is All You Need to Sample Out-of-Distribution Chemical   Spaces</title><categories>cs.LG cs.AI physics.chem-ph</categories><comments>27 pages, 10 figures, 8 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generating novel molecules with higher properties than the training space, namely the out-of-distribution generation, is important for ${de~novo}$ drug design. However, it is not easy for distribution learning-based models, for example diffusion models, to solve this challenge as these methods are designed to fit the distribution of training data as close as possible. In this paper, we show that Bayesian flow network is capable of effortlessly generating high quality out-of-distribution samples that meet several scenarios. We introduce a semi-autoregressive training/sampling method that helps to enhance the model performance and surpass the state-of-the-art models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.11735</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.11735</id><created>2024-12-16</created><updated>2025-02-02</updated><authors><author><keyname>Li</keyname><forenames>Wenyun</forenames></author><author><keyname>Zhang</keyname><forenames>Zheng</forenames></author><author><keyname>Lan</keyname><forenames>Xiangyuan</forenames></author><author><keyname>Jiang</keyname><forenames>Dongmei</forenames></author></authors><title>Transferable Adversarial Face Attack with Text Controlled Attribute</title><categories>cs.CV cs.AI</categories><comments>Accepted by AAAI 2025</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Traditional adversarial attacks typically produce adversarial examples under norm-constrained conditions, whereas unrestricted adversarial examples are free-form with semantically meaningful perturbations. Current unrestricted adversarial impersonation attacks exhibit limited control over adversarial face attributes and often suffer from low transferability. In this paper, we propose a novel Text Controlled Attribute Attack (TCA$^2$) to generate photorealistic adversarial impersonation faces guided by natural language. Specifically, the category-level personal softmax vector is employed to precisely guide the impersonation attacks. Additionally, we propose both data and model augmentation strategies to achieve transferable attacks on unknown target models. Finally, a generative model, \textit{i.e}, Style-GAN, is utilized to synthesize impersonated faces with desired attributes. Extensive experiments on two high-resolution face recognition datasets validate that our TCA$^2$ method can generate natural text-guided adversarial impersonation faces with high transferability. We also evaluate our method on real-world face recognition systems, \textit{i.e}, Face++ and Aliyun, further demonstrating the practical potential of our approach. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.12004</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.12004</id><created>2024-12-16</created><updated>2025-02-02</updated><authors><author><keyname>Manchanda</keyname><forenames>Jiya</forenames></author><author><keyname>Boettcher</keyname><forenames>Laura</forenames></author><author><keyname>Westphalen</keyname><forenames>Matheus</forenames></author><author><keyname>Jasser</keyname><forenames>Jasser</forenames></author></authors><title>The Open Source Advantage in Large Language Models (LLMs)</title><categories>cs.CL cs.LG</categories><comments>9 pages, 1 figure</comments><acm-class>I.2.7</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models (LLMs) have rapidly advanced natural language processing, driving significant breakthroughs in tasks such as text generation, machine translation, and domain-specific reasoning. The field now faces a critical dilemma in its approach: closed-source models like GPT-4 deliver state-of-the-art performance but restrict reproducibility, accessibility, and external oversight, while open-source frameworks like LLaMA and Mixtral democratize access, foster collaboration, and support diverse applications, achieving competitive results through techniques like instruction tuning and LoRA. Hybrid approaches address challenges like bias mitigation and resource accessibility by combining the scalability of closed-source systems with the transparency and inclusivity of open-source framework. However, in this position paper, we argue that open-source remains the most robust path for advancing LLM research and ethical deployment. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.12178</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.12178</id><created>2024-12-12</created><updated>2025-01-31</updated><authors><author><keyname>Dhar</keyname><forenames>Nobel</forenames></author><author><keyname>Deng</keyname><forenames>Bobin</forenames></author><author><keyname>Islam</keyname><forenames>Md Romyull</forenames></author><author><keyname>Nasif</keyname><forenames>Kazi Fahim Ahmad</forenames></author><author><keyname>Zhao</keyname><forenames>Liang</forenames></author><author><keyname>Suo</keyname><forenames>Kun</forenames></author></authors><title>Activation Sparsity Opportunities for Compressing General Large Language   Models</title><categories>cs.LG cs.AI</categories><comments>pp. 1-9, doi: 10.1109/IPCCC59868.2024.10850382. keywords:   {Accuracy;Prefetching;Large language models;Computational   modeling;Companies;Transformers;User experience;Time   factors;Tuning;Guidelines;Large Language Models (LLMs);AI   Compression;Activation Sparsity;Edge LLM},</comments><journal-ref>2024 IEEE International Performance, Computing, and Communications   Conference (IPCCC), Orlando, FL, USA, 2024</journal-ref><doi>10.1109/IPCCC59868.2024.10850382</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deploying local AI models, such as Large Language Models (LLMs), to edge devices can substantially enhance devices' independent capabilities, alleviate the server's burden, and lower the response time. Owing to these tremendous potentials, many big tech companies have released several lightweight Small Language Models (SLMs) to bridge this gap. However, we still have huge motivations to deploy more powerful (LLMs) AI models on edge devices and enhance their smartness level. Unlike the conventional approaches for AI model compression, we investigate activation sparsity. The activation sparsity method is orthogonal and combinable with existing techniques to maximize the compression rate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN) components, which typically comprise a large proportion of parameters (around 2/3), ensure that our FFN optimizations would have a better chance of achieving effective compression. Moreover, our findings are beneficial to general LLMs and are not restricted to ReLU-based models. This work systematically investigates the tradeoff between enforcing activation sparsity and perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that we can obtain around 50% of main memory and computing reductions for critical FFN components with negligible accuracy degradation. This extra 50% sparsity does not naturally exist in the current LLMs, which require tuning LLMs' activation outputs by injecting zero-enforcing thresholds. To obtain the benefits of activation sparsity, we provide a guideline for the system architect for LLM prediction and prefetching. The success prediction allows the system to prefetch the necessary weights while omitting the inactive ones and their successors, therefore lowering cache and memory pollution and reducing LLM execution time on resource-constrained edge devices. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.12906</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.12906</id><created>2024-12-17</created><updated>2025-02-03</updated><authors><author><keyname>Roh</keyname><forenames>Wonseok</forenames></author><author><keyname>Jung</keyname><forenames>Hwanhee</forenames></author><author><keyname>Kim</keyname><forenames>Jong Wook</forenames></author><author><keyname>Lee</keyname><forenames>Seunggwan</forenames></author><author><keyname>Yoo</keyname><forenames>Innfarn</forenames></author><author><keyname>Lugmayr</keyname><forenames>Andreas</forenames></author><author><keyname>Chi</keyname><forenames>Seunggeun</forenames></author><author><keyname>Ramani</keyname><forenames>Karthik</forenames></author><author><keyname>Kim</keyname><forenames>Sangpil</forenames></author></authors><title>CATSplat: Context-Aware Transformer with Spatial Guidance for   Generalizable 3D Gaussian Splatting from A Single-View Image</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Recently, generalizable feed-forward methods based on 3D Gaussian Splatting have gained significant attention for their potential to reconstruct 3D scenes using finite resources. These approaches create a 3D radiance field, parameterized by per-pixel 3D Gaussian primitives, from just a few images in a single forward pass. However, unlike multi-view methods that benefit from cross-view correspondences, 3D scene reconstruction with a single-view image remains an underexplored area. In this work, we introduce CATSplat, a novel generalizable transformer-based framework designed to break through the inherent constraints in monocular settings. First, we propose leveraging textual guidance from a visual-language model to complement insufficient information from a single image. By incorporating scene-specific contextual details from text embeddings through cross-attention, we pave the way for context-aware 3D scene reconstruction beyond relying solely on visual cues. Moreover, we advocate utilizing spatial guidance from 3D point features toward comprehensive geometric understanding under single-view settings. With 3D priors, image features can capture rich structural insights for predicting 3D Gaussians without multi-view techniques. Extensive experiments on large-scale datasets demonstrate the state-of-the-art performance of CATSplat in single-view 3D scene reconstruction with high-quality novel view synthesis. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.13490</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.13490</id><created>2024-12-17</created><updated>2025-01-31</updated><authors><author><keyname>Saltık</keyname><forenames>Ahmet Oğuz</forenames></author><author><keyname>Allmendinger</keyname><forenames>Alicia</forenames></author><author><keyname>Stein</keyname><forenames>Anthony</forenames></author></authors><title>Comparative Analysis of YOLOv9, YOLOv10 and RT-DETR for Real-Time Weed   Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a comprehensive evaluation of state-of-the-art object detection models, including YOLOv9, YOLOv10, and RT-DETR, for the task of weed detection in smart-spraying applications focusing on three classes: Sugarbeet, Monocot, and Dicot. The performance of these models is compared based on mean Average Precision (mAP) scores and inference times on different GPU and CPU devices. We consider various model variations, such as nano, small, medium, large alongside different image resolutions (320px, 480px, 640px, 800px, 960px). The results highlight the trade-offs between inference time and detection accuracy, providing valuable insights for selecting the most suitable model for real-time weed detection. This study aims to guide the development of efficient and effective smart spraying systems, enhancing agricultural productivity through precise weed management. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.13708</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.13708</id><created>2024-12-18</created><updated>2025-02-03</updated><authors><author><keyname>Son</keyname><forenames>Taein</forenames></author><author><keyname>Seo</keyname><forenames>Soo Won</forenames></author><author><keyname>Kim</keyname><forenames>Jisong</forenames></author><author><keyname>Lee</keyname><forenames>Seok Hwan</forenames></author><author><keyname>Choi</keyname><forenames>Jun Won</forenames></author></authors><title>JoVALE: Detecting Human Actions in Video Using Audiovisual and Language   Contexts</title><categories>cs.CV</categories><comments>Accepted to AAAI Conference on Artificial Intelligence 2025, 10   pages, 6 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Video Action Detection (VAD) entails localizing and categorizing action instances within videos, which inherently consist of diverse information sources such as audio, visual cues, and surrounding scene contexts. Leveraging this multi-modal information effectively for VAD poses a significant challenge, as the model must identify action-relevant cues with precision. In this study, we introduce a novel multi-modal VAD architecture, referred to as the Joint Actor-centric Visual, Audio, Language Encoder (JoVALE). JoVALE is the first VAD method to integrate audio and visual features with scene descriptive context sourced from large-capacity image captioning models. At the heart of JoVALE is the actor-centric aggregation of audio, visual, and scene descriptive information, enabling adaptive integration of crucial features for recognizing each actor's actions. We have developed a Transformer-based architecture, the Actor-centric Multi-modal Fusion Network, specifically designed to capture the dynamic interactions among actors and their multi-modal contexts. Our evaluation on three prominent VAD benchmarks, including AVA, UCF101-24, and JHMDB51-21, demonstrates that incorporating multi-modal information significantly enhances performance, setting new state-of-the-art performances in the field. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.14119</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.14119</id><created>2024-12-18</created><updated>2025-02-02</updated><authors><author><keyname>Zolghadr</keyname><forenames>Arshia</forenames></author><author><keyname>Santos</keyname><forenames>Joao F.</forenames></author><author><keyname>DaSilva</keyname><forenames>Luiz A.</forenames></author><author><keyname>Kibiłda</keyname><forenames>Jacek</forenames></author></authors><title>Learning and Reconstructing Conflicts in O-RAN: A Graph Neural Network   Approach</title><categories>cs.NI cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Open Radio Access Network (O-RAN) architecture enables the deployment of third-party applications on the RAN Intelligent Controllers (RICs). However, the operation of third-party applications in the Near Real-Time RIC (Near-RT RIC), known as xApps, may result in conflicting interactions. Each xApp can independently modify the same control parameters to achieve distinct outcomes, which has the potential to cause performance degradation and network instability. The current conflict detection and mitigation solutions in the literature assume that all conflicts are known a priori, which does not always hold due to complex and often hidden relationships between control parameters and Key Performance Indicators (KPIs). In this paper, we introduce the first data-driven method for reconstructing and labeling conflict graphs in O-RAN. Specifically, we leverage GraphSAGE, an inductive learning framework, to dynamically learn the hidden relationships between xApps, parameters, and KPIs. Our numerical results, based on a conflict model used in the O-RAN conflict management literature, demonstrate that our proposed method can effectively reconstruct conflict graphs and identify the conflicts defined by the O-RAN Alliance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.14380</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.14380</id><created>2024-12-18</created><updated>2025-02-01</updated><authors><author><keyname>Farias</keyname><forenames>Victor A. E.</forenames></author><author><keyname>Brito</keyname><forenames>Felipe T.</forenames></author><author><keyname>Flynn</keyname><forenames>Cheryl</forenames></author><author><keyname>Machado</keyname><forenames>Javam C.</forenames></author><author><keyname>Srivastava</keyname><forenames>Divesh</forenames></author></authors><title>Differentially Private Multi-objective Selection: Pareto and Aggregation   Approaches</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Differentially private selection mechanisms are fundamental building blocks for privacy-preserving data analysis. While numerous mechanisms exist for single-objective selection, many real-world applications require optimizing multiple competing objectives simultaneously. We present two novel mechanisms for differentially private multi-objective selection: PrivPareto and PrivAgg. PrivPareto uses a novel Pareto score to identify solutions near the Pareto frontier, while PrivAgg enables privacy-preserving weighted aggregation of multiple objectives. Both mechanisms support global and local sensitivity approaches, with comprehensive theoretical analysis showing how to compose sensitivities of multiple utility functions. We demonstrate the practical applicability through two real-world applications: cost-sensitive decision tree construction and multi-objective influential node selection in social networks. The experimental results showed that our local sensitivity-based approaches achieve significantly better utility compared to global sensitivity approaches across both applications and both Pareto and Aggregation approaches. Moreover, the local sensitivity-based approaches are able to perform well with typical privacy budget values $\epsilon \in [0.01, 1]$ in most experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.14513</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.14513</id><created>2024-12-18</created><updated>2025-02-03</updated><authors><author><keyname>Mou</keyname><forenames>Yingzhou</forenames></author><author><keyname>Hayashi</keyname><forenames>Yukio</forenames></author></authors><title>Vulnerable Connectivity Caused by Local Communities in Spatial Networks</title><categories>cs.SI physics.soc-ph</categories><comments>Modified from the first version</comments><msc-class>91D30, 94C15</msc-class><acm-class>J.4; H.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Local communities are widely observed in spatial networks. However, how such structure affects the vulnerability against malicious attacks remains unclear. This study investigates the impact of local communities on the robustness of connectivity by modeling planar infrastructure networks based on statistical population data. Our research reveals that the emergence of local communities is caused by spatial concentrations of nodes connected by short links, which significantly reduce the robustness. These results suggest that strategically establishing long distance links provides a feasible solution to balance reliability and construction costs in infrastructure network design. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.14679</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.14679</id><created>2024-12-19</created><updated>2025-01-31</updated><authors><author><keyname>Mancas</keyname><forenames>Christian</forenames></author></authors><title>On Enforcing Satisfiable, Coherent, and Minimal Sets of Self-Map   Constraints in MatBase</title><categories>cs.DB</categories><comments>Submitted to the PriMera Scientific Engineering Journal on 18 Dec.   2024. arXiv admin note: substantial text overlap with arXiv:2410.23485</comments><journal-ref>PriMera Scientific Engineering Journal 6(2):31-49, 2025</journal-ref><doi>10.56831/PSEN-06-179</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper rigorously and concisely defines, in the context of our (Elementary) Mathematical Data Model ((E)MDM), the mathematical concepts of self-map, composite mapping, totality, one-to-oneness, non-primeness, ontoness, bijectivity, default value, (null-)reflexivity, irreflexivity, (null-)symmetry, asymmetry, (null-)idempotency, anti-idempotency, (null-)equivalence, acyclicity, (null-)representative system mapping, the properties that relate them, and the corresponding corollaries on the coherence and minimality of sets made of such mapping properties viewed as database constraints. Its main contribution is the pseudocode algorithm used by MatBase, our intelligent database management system prototype based on both (E)MDM, the relational, and the entity-relationship data models, for enforcing self-map, atomic, and composite mapping constraint sets. We prove that this algorithm guarantees the satisfiability, coherence, and minimality of such sets, while being very fast, sound, complete, and minimal. In the sequel, we also presented the relevant MatBase user interface as well as the tables of its metacatalog used by this algorithm. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.15546</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.15546</id><created>2024-12-19</created><updated>2025-02-03</updated><authors><author><keyname>Lai</keyname><forenames>Zhao-Rong</forenames></author><author><keyname>Wu</keyname><forenames>Xiaotian</forenames></author><author><keyname>Fang</keyname><forenames>Liangda</forenames></author><author><keyname>Chen</keyname><forenames>Ziliang</forenames></author><author><keyname>Li</keyname><forenames>Cheng</forenames></author></authors><title>De-singularity Subgradient for the $q$-th-Powered $\ell_p$-Norm Weber   Location Problem</title><categories>math.OC cs.LG</categories><comments>AAAI 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Weber location problem is widely used in several artificial intelligence scenarios. However, the gradient of the objective does not exist at a considerable set of singular points. Recently, a de-singularity subgradient method has been proposed to fix this problem, but it can only handle the $q$-th-powered $\ell_2$-norm case ($1\leqslant q&lt;2$), which has only finite singular points. In this paper, we further establish the de-singularity subgradient for the $q$-th-powered $\ell_p$-norm case with $1\leqslant q\leqslant p$ and $1\leqslant p&lt;2$, which includes all the rest unsolved situations in this problem. This is a challenging task because the singular set is a continuum. The geometry of the objective function is also complicated so that the characterizations of the subgradients, minimum and descent direction are very difficult. We develop a $q$-th-powered $\ell_p$-norm Weiszfeld Algorithm without Singularity ($q$P$p$NWAWS) for this problem, which ensures convergence and the descent property of the objective function. Extensive experiments on six real-world data sets demonstrate that $q$P$p$NWAWS successfully solves the singularity problem and achieves a linear computational convergence rate in practical scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.15904</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.15904</id><created>2024-12-20</created><updated>2025-02-01</updated><authors><author><keyname>Ma</keyname><forenames>Yiran</forenames></author><author><keyname>Chen</keyname><forenames>Zui</forenames></author><author><keyname>Liu</keyname><forenames>Tianqiao</forenames></author><author><keyname>Tian</keyname><forenames>Mi</forenames></author><author><keyname>Liu</keyname><forenames>Zhuo</forenames></author><author><keyname>Liu</keyname><forenames>Zitao</forenames></author><author><keyname>Luo</keyname><forenames>Weiqi</forenames></author></authors><title>What Are Step-Level Reward Models Rewarding? Counterintuitive Findings   from MCTS-Boosted Mathematical Reasoning</title><categories>cs.AI cs.LG</categories><comments>AAAI 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Step-level reward models (SRMs) can significantly enhance mathematical reasoning performance through process supervision or step-level preference alignment based on reinforcement learning. The performance of SRMs is pivotal, as they serve as critical guidelines, ensuring that each step in the reasoning process is aligned with desired outcomes. Recently, AlphaZero-like methods, where Monte Carlo Tree Search (MCTS) is employed for automatic step-level preference annotation, have proven particularly effective. However, the precise mechanisms behind the success of SRMs remain largely unexplored. To address this gap, this study delves into the counterintuitive aspects of SRMs, particularly focusing on MCTS-based approaches. Our findings reveal that the removal of natural language descriptions of thought processes has minimal impact on the efficacy of SRMs. Furthermore, we demonstrate that SRMs are adept at assessing the complex logical coherence present in mathematical language while having difficulty in natural language. These insights provide a nuanced understanding of the core elements that drive effective step-level reward modeling in mathematical reasoning. By shedding light on these mechanisms, this study offers valuable guidance for developing more efficient and streamlined SRMs, which can be achieved by focusing on the crucial parts of mathematical reasoning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.16232</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.16232</id><created>2024-12-18</created><updated>2025-02-02</updated><authors><author><keyname>Zhang</keyname><forenames>Yue</forenames></author><author><keyname>Jing</keyname><forenames>Liqiang</forenames></author><author><keyname>Gogate</keyname><forenames>Vibhav</forenames></author></authors><title>Defeasible Visual Entailment: Benchmark, Evaluator, and Reward-Driven   Optimization</title><categories>cs.CV cs.AI cs.LG</categories><comments>Accepted by AAAI 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new task called Defeasible Visual Entailment (DVE), where the goal is to allow the modification of the entailment relationship between an image premise and a text hypothesis based on an additional update. While this concept is well-established in Natural Language Inference, it remains unexplored in visual entailment. At a high level, DVE enables models to refine their initial interpretations, leading to improved accuracy and reliability in various applications such as detecting misleading information in images, enhancing visual question answering, and refining decision-making processes in autonomous systems. Existing metrics do not adequately capture the change in the entailment relationship brought by updates. To address this, we propose a novel inference-aware evaluator designed to capture changes in entailment strength induced by updates, using pairwise contrastive learning and categorical information learning. Additionally, we introduce a reward-driven update optimization method to further enhance the quality of updates generated by multimodal models. Experimental results demonstrate the effectiveness of our proposed evaluator and optimization method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.16619</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.16619</id><created>2024-12-21</created><updated>2025-02-02</updated><authors><author><keyname>Shen</keyname><forenames>Tianqi</forenames></author><author><keyname>Liu</keyname><forenames>Shaohua</forenames></author><author><keyname>Feng</keyname><forenames>Jiaqi</forenames></author><author><keyname>Ma</keyname><forenames>Ziye</forenames></author><author><keyname>An</keyname><forenames>Ning</forenames></author></authors><title>Topology-Aware 3D Gaussian Splatting: Leveraging Persistent Homology for   Optimized Structural Integrity</title><categories>cs.CV cs.LG eess.IV math.AT math.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian Splatting (GS) has emerged as a crucial technique for representing discrete volumetric radiance fields. It leverages unique parametrization to mitigate computational demands in scene optimization. This work introduces Topology-Aware 3D Gaussian Splatting (Topology-GS), which addresses two key limitations in current approaches: compromised pixel-level structural integrity due to incomplete initial geometric coverage, and inadequate feature-level integrity from insufficient topological constraints during optimization. To overcome these limitations, Topology-GS incorporates a novel interpolation strategy, Local Persistent Voronoi Interpolation (LPVI), and a topology-focused regularization term based on persistent barcodes, named PersLoss. LPVI utilizes persistent homology to guide adaptive interpolation, enhancing point coverage in low-curvature areas while preserving topological structure. PersLoss aligns the visual perceptual similarity of rendered images with ground truth by constraining distances between their topological features. Comprehensive experiments on three novel-view synthesis benchmarks demonstrate that Topology-GS outperforms existing methods in terms of PSNR, SSIM, and LPIPS metrics, while maintaining efficient memory usage. This study pioneers the integration of topology with 3D-GS, laying the groundwork for future research in this area. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.17155</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.17155</id><created>2024-12-22</created><updated>2025-02-02</updated><authors><author><keyname>Molaeian</keyname><forenames>Hossein</forenames></author><author><keyname>Karamjani</keyname><forenames>Kaveh</forenames></author><author><keyname>Teimouri</keyname><forenames>Sina</forenames></author><author><keyname>Roshani</keyname><forenames>Saeed</forenames></author><author><keyname>Roshani</keyname><forenames>Sobhan</forenames></author></authors><title>The Potential of Convolutional Neural Networks for Cancer Detection</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Early detection is a prime requisite for successful cancer treatment and increasing its survivability rates, particularly in the most common forms. CNNs (Convolutional Neural Networks) are very potent tools for the analysis and classification of medical images, with particular reference to the early detection of different types of cancer. Ten different cancers have been identified in most of these advances that use CNN techniques for classification. The unique architectures of CNNs employed in each study are focused on pattern recognition for each type of cancer through different datasets. By comparing and analyzing these architectures, the strengths and drawbacks of each approach are pointed out in terms of their efforts toward improving the earlier detection of cancer. The opportunity to embrace CNNs within the clinical sphere was interrogated as support or potential substitution of traditional diagnostic techniques. Furthermore, challenges such as integrating diverse data, how to interpret the results, and ethical dilemmas continue to stalk this field with inconceivable hindrances. This study identifies those CNN architectures that carry out the best work and offers a comparative analysis that reveals to researchers the impact of CNNs on cancer detection in the leap toward boosting diagnostic capabilities in health. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.17404</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.17404</id><created>2024-12-23</created><updated>2025-01-31</updated><authors><author><keyname>Wang</keyname><forenames>Song</forenames></author><author><keyname>Lei</keyname><forenames>Zhenyu</forenames></author><author><keyname>Tan</keyname><forenames>Zhen</forenames></author><author><keyname>Ding</keyname><forenames>Jiaqi</forenames></author><author><keyname>Zhao</keyname><forenames>Xinyu</forenames></author><author><keyname>Dong</keyname><forenames>Yushun</forenames></author><author><keyname>Wu</keyname><forenames>Guorong</forenames></author><author><keyname>Chen</keyname><forenames>Tianlong</forenames></author><author><keyname>Chen</keyname><forenames>Chen</forenames></author><author><keyname>Zhang</keyname><forenames>Aiying</forenames></author><author><keyname>Li</keyname><forenames>Jundong</forenames></author></authors><title>BrainMAP: Learning Multiple Activation Pathways in Brain Networks</title><categories>cs.AI</categories><comments>AAAI 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Functional Magnetic Resonance Image (fMRI) is commonly employed to study human brain activity, since it offers insight into the relationship between functional fluctuations and human behavior. To enhance analysis and comprehension of brain activity, Graph Neural Networks (GNNs) have been widely applied to the analysis of functional connectivities (FC) derived from fMRI data, due to their ability to capture the synergistic interactions among brain regions. However, in the human brain, performing complex tasks typically involves the activation of certain pathways, which could be represented as paths across graphs. As such, conventional GNNs struggle to learn from these pathways due to the long-range dependencies of multiple pathways. To address these challenges, we introduce a novel framework BrainMAP to learn Multiple Activation Pathways in Brain networks. BrainMAP leverages sequential models to identify long-range correlations among sequentialized brain regions and incorporates an aggregation module based on Mixture of Experts (MoE) to learn from multiple pathways. Our comprehensive experiments highlight BrainMAP's superior performance. Furthermore, our framework enables explanatory analyses of crucial brain regions involved in tasks. Our code is provided at https://github.com/LzyFischer/Graph-Mamba. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.17626</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.17626</id><created>2024-12-23</created><updated>2025-02-01</updated><authors><author><keyname>Xu</keyname><forenames>Yang</forenames></author><author><keyname>Wang</keyname><forenames>Yi</forenames></author><author><keyname>Wang</keyname><forenames>Hao</forenames></author></authors><title>Tracking the Feature Dynamics in LLM Training: A Mechanistic Study</title><categories>cs.LG cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding training dynamics and feature evolution is crucial for the mechanistic interpretability of large language models (LLMs). Although sparse autoencoders (SAEs) have been used to identify features within LLMs, a clear picture of how these features evolve during training remains elusive. In this study, we: (1) introduce SAE-Track, a novel method to efficiently obtain a continual series of SAEs; (2) mechanistically investigate feature formation and develop a progress measure for it ; and (3) analyze and visualize feature drift during training. Our work provides new insights into the dynamics of features in LLMs, enhancing our understanding of training mechanisms and feature evolution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.18202</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.18202</id><created>2024-12-24</created><updated>2025-02-01</updated><authors><author><keyname>Hu</keyname><forenames>Zhuohuan</forenames></author><author><keyname>Yu</keyname><forenames>Richard</forenames></author><author><keyname>Zhang</keyname><forenames>Zizhou</forenames></author><author><keyname>Zheng</keyname><forenames>Haoran</forenames></author><author><keyname>Liu</keyname><forenames>Qianying</forenames></author><author><keyname>Zhou</keyname><forenames>Yining</forenames></author></authors><title>Developing Cryptocurrency Trading Strategy Based on Autoencoder-CNN-GANs   Algorithms</title><categories>cs.LG q-fin.ST</categories><comments>The paper was accepted by 2024 4th International Conference on   Artificial Intelligence, Robotics, and Communication(ICAIRC 2024)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper leverages machine learning algorithms to forecast and analyze financial time series. The process begins with a denoising autoencoder to filter out random noise fluctuations from the main contract price data. Then, one-dimensional convolution reduces the dimensionality of the filtered data and extracts key information. The filtered and dimensionality-reduced price data is fed into a GANs network, and its output serve as input of a fully connected network. Through cross-validation, a model is trained to capture features that precede large price fluctuations. The model predicts the likelihood and direction of significant price changes in real-time price sequences, placing trades at moments of high prediction accuracy. Empirical results demonstrate that using autoencoders and convolution to filter and denoise financial data, combined with GANs, achieves a certain level of predictive performance, validating the capabilities of machine learning algorithms to discover underlying patterns in financial sequences. Keywords - CNN;GANs; Cryptocurrency; Prediction. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.19770</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.19770</id><created>2024-12-27</created><updated>2025-01-31</updated><authors><author><keyname>Chen</keyname><forenames>Le</forenames></author><author><keyname>Lei</keyname><forenames>Bin</forenames></author><author><keyname>Zhou</keyname><forenames>Dunzhi</forenames></author><author><keyname>Lin</keyname><forenames>Pei-Hung</forenames></author><author><keyname>Liao</keyname><forenames>Chunhua</forenames></author><author><keyname>Ding</keyname><forenames>Caiwen</forenames></author><author><keyname>Jannesari</keyname><forenames>Ali</forenames></author></authors><title>Fortran2CPP: Automating Fortran-to-C++ Translation using LLMs via   Multi-Turn Dialogue and Dual-Agent Integration</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Translating legacy Fortran code into C++ is a crucial step in modernizing high-performance computing (HPC) applications. However, the scarcity of high-quality, parallel Fortran-to-C++ datasets and the limited domain-specific expertise in large language models (LLMs) present significant challenges for automated translation. In this paper, we introduce Fortran2CPP, a multi-turn dialogue dataset generated by a novel LLM agent-based approach that integrates a dual-LLM Questioner-Solver module to enhance translation accuracy. Our dataset comprises 11.7k dialogues capturing iterative feedback-decision workflows including code translation, compilation, execution, unit testing, and error-fixing. Using this dataset, we fine-tune several open-weight LLMs and achieve up to a 3.31x improvement in CodeBLEU scores and a 92\% increase in compilation success rate, demonstrating enhanced syntactic accuracy and functional reliability. Our findings highlight the value of dialogue-based LLM training for complex code translation tasks. The dataset and model have been open-sourced and are available on our public GitHub repository\footnote{\url{https://github.com/HPC-Fortran2CPP/Fortran2Cpp}}. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.20638</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.20638</id><created>2024-12-29</created><updated>2025-02-02</updated><authors><author><keyname>Nam</keyname><forenames>Hyunji</forenames></author><author><keyname>Nie</keyname><forenames>Allen</forenames></author><author><keyname>Gao</keyname><forenames>Ge</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author><author><keyname>Brunskill</keyname><forenames>Emma</forenames></author></authors><title>Predicting Long Term Sequential Policy Value Using Softer Surrogates</title><categories>cs.AI cs.LG</categories><comments>24 pages, 1 figure</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Off-policy policy evaluation (OPE) estimates the outcome of a new policy using historical data collected from a different policy. However, existing OPE methods cannot handle cases when the new policy introduces novel actions. This issue commonly occurs in real-world domains, like healthcare, as new drugs and treatments are continuously developed. Novel actions necessitate on-policy data collection, which can be burdensome and expensive if the outcome of interest takes a substantial amount of time to observe--for example, in multi-year clinical trials. This raises a key question of how to predict the long-term outcome of a policy after only observing its short-term effects? Though in general this problem is intractable, under some surrogacy conditions, the short-term on-policy data can be combined with the long-term historical data to make accurate predictions about the new policy's long-term value. In two simulated healthcare examples--HIV and sepsis management--we show that our estimators can provide accurate predictions about the policy value only after observing 10\% of the full horizon data. We also provide finite sample analysis of our doubly robust estimators. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.20761</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.20761</id><created>2024-12-30</created><updated>2025-02-02</updated><authors><author><keyname>Jing</keyname><forenames>Jie</forenames></author><author><keyname>Lin</keyname><forenames>Qing</forenames></author><author><keyname>Han</keyname><forenames>Shuangpeng</forenames></author><author><keyname>Schiatti</keyname><forenames>Lucia</forenames></author><author><keyname>Kuo</keyname><forenames>Yen-Ling</forenames></author><author><keyname>Zhang</keyname><forenames>Mengmi</forenames></author></authors><title>Unforgettable Lessons from Forgettable Images: Intra-Class Memorability   Matters in Computer Vision Tasks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce intra-class memorability, where certain images within the same class are more memorable than others despite shared category characteristics. To investigate what features make one object instance more memorable than others, we design and conduct human behavior experiments, where participants are shown a series of images one at a time, and they must identify when the current item matches the item presented a few steps back in the sequence. To quantify memorability, we propose the Intra-Class Memorability score (ICMscore), a novel metric that incorporates the temporal intervals between repeated image presentations into its calculation. Our contributions open new pathways in understanding intra-class memorability by scrutinizing fine-grained visual features that result in the least and most memorable images and laying the groundwork for real-world applications in cognitive science and computer vision. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.21187</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.21187</id><created>2024-12-30</created><updated>2025-02-01</updated><authors><author><keyname>Chen</keyname><forenames>Xingyu</forenames></author><author><keyname>Xu</keyname><forenames>Jiahao</forenames></author><author><keyname>Liang</keyname><forenames>Tian</forenames></author><author><keyname>He</keyname><forenames>Zhiwei</forenames></author><author><keyname>Pang</keyname><forenames>Jianhui</forenames></author><author><keyname>Yu</keyname><forenames>Dian</forenames></author><author><keyname>Song</keyname><forenames>Linfeng</forenames></author><author><keyname>Liu</keyname><forenames>Qiuzhi</forenames></author><author><keyname>Zhou</keyname><forenames>Mengfei</forenames></author><author><keyname>Zhang</keyname><forenames>Zhuosheng</forenames></author><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Tu</keyname><forenames>Zhaopeng</forenames></author><author><keyname>Mi</keyname><forenames>Haitao</forenames></author><author><keyname>Yu</keyname><forenames>Dong</forenames></author></authors><title>Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs</title><categories>cs.CL</categories><comments>We have updated the results of DeepSeek-R1, and all conclusions still   hold</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The remarkable performance of models like the OpenAI o1 can be attributed to their ability to emulate human-like long-time thinking during inference. These models employ extended chain-of-thought (CoT) processes, exploring multiple strategies to enhance problem-solving capabilities. However, a critical question remains: How to intelligently and efficiently scale computational resources during testing. This paper presents the first comprehensive study on the prevalent issue of overthinking in these models, where excessive computational resources are allocated for simple problems with minimal benefit. We introduce novel efficiency metrics from both outcome and process perspectives to evaluate the rational use of computational resources by o1-like models. Using a self-training paradigm, we propose strategies to mitigate overthinking, streamlining reasoning processes without compromising accuracy. Experimental results show that our approach successfully reduces computational overhead while preserving model performance across a range of testsets with varying difficulty levels, such as GSM8K, MATH500, GPQA, and AIME. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.00371</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.00371</id><created>2024-12-31</created><updated>2025-02-02</updated><authors><author><keyname>Malak</keyname><forenames>Derya</forenames></author></authors><title>Structured Codes for Distributed Matrix Multiplication</title><categories>cs.IT math.IT</categories><comments>Preprint. A preliminary version of this work was presented in parts   at the 2024 Int. Symp. Inf. Theory, Athens, Greece</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Our work addresses the well-known open problem of distributed computing of bilinear functions of two correlated sources ${\bf A}$ and ${\bf B}$. In a setting with two nodes, with the first node having access to ${\bf A}$ and the second to ${\bf B}$, we establish bounds on the optimal sum-rate that allows a receiver to compute an important class of non-linear functions, and in particular bilinear functions, including dot products $\langle {\bf A},{\bf B}\rangle$, and general matrix products ${\bf A}^{\intercal}{\bf B}$ over finite fields. The bounds are tight, for large field sizes, for which case we can derive the exact fundamental performance limits for all problem dimensions and a large class of sources. Our achievability scheme involves the design of non-linear transformations of ${\bf A}$ and ${\bf B}$, which are carefully calibrated to work synergistically with the structured linear encoding scheme by K\"orner and Marton. The subsequent converse derived here, calibrates the Han-Kobayashi approach to yield a relatively tight converse on the sum rate. We also demonstrate unbounded compression gains over Slepian-Wolf coding, depending on the source correlations. In the end, our work derives fundamental limits for distributed computing of a crucial class of functions, succinctly capturing the computation structures and source correlations.   Our findings are subsequently applied to the practical master-workers-receiver framework, where each of $N$ distributed workers has a bounded memory reflecting a bounded computational capability. By combining the above scheme with the polynomial code framework, we design novel structured polynomial codes for distributed matrix multiplication, and show that our codes can surpass the performance of the existing state of art, while also adapting these new codes to support chain matrix multiplications and information-theoretically secure computations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.01216</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.01216</id><created>2025-01-02</created><updated>2025-02-02</updated><authors><author><keyname>Li</keyname><forenames>Jiayu</forenames></author><author><keyname>Zhao</keyname><forenames>Bingyin</forenames></author><author><keyname>Zhao</keyname><forenames>Zilong</forenames></author><author><keyname>Yee</keyname><forenames>Kevin</forenames></author><author><keyname>Javaid</keyname><forenames>Uzair</forenames></author><author><keyname>Sikdar</keyname><forenames>Biplab</forenames></author></authors><title>TabTreeFormer: Tabular Data Generation Using Hybrid Tree-Transformer</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Transformers have achieved remarkable success in tabular data generation. However, they lack domain-specific inductive biases which are critical to preserving the intrinsic characteristics of tabular data. Meanwhile, they suffer from poor scalability and efficiency due to quadratic computational complexity. In this paper, we propose TabTreeFormer, a hybrid transformer architecture that incorporates a tree-based model that retains tabular-specific inductive biases of non-smooth and potentially low-correlated patterns caused by discreteness and non-rotational invariance, and hence enhances the fidelity and utility of synthetic data. In addition, we devise a dual-quantization tokenizer to capture the multimodal continuous distribution and further facilitate the learning of numerical value distribution. Moreover, our proposed tokenizer reduces the vocabulary size and sequence length due to the limited complexity (e.g., dimension-wise semantic meaning) of tabular data, rendering a significant model size shrink without sacrificing the capability of the transformer model. We evaluate TabTreeFormer on 10 datasets against multiple generative models on various metrics; our experimental results show that TabTreeFormer achieves superior fidelity, utility, privacy, and efficiency. Our best model yields a 40% utility improvement with 1/16 of the baseline model size. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.02409</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.02409</id><created>2025-01-04</created><updated>2025-02-01</updated><authors><author><keyname>Lin</keyname><forenames>Zaikang</forenames></author><author><keyname>Chang</keyname><forenames>Sei</forenames></author><author><keyname>Zweig</keyname><forenames>Aaron</forenames></author><author><keyname>Kang</keyname><forenames>Minseo</forenames></author><author><keyname>Azizi</keyname><forenames>Elham</forenames></author><author><keyname>Knowles</keyname><forenames>David A.</forenames></author></authors><title>Interpretable Neural ODEs for Gene Regulatory Network Discovery under   Perturbations</title><categories>cs.LG cs.AI cs.CE q-bio.MN stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern high-throughput biological datasets with thousands of perturbations provide the opportunity for large-scale discovery of causal graphs that represent the regulatory interactions between genes. Differentiable causal graphical models have been proposed to infer a gene regulatory network (GRN) from large scale interventional datasets, capturing the causal gene regulatory relationships from genetic perturbations. However, existing models are limited in their expressivity and scalability while failing to address the dynamic nature of biological processes such as cellular differentiation. We propose PerturbODE, a novel framework that incorporates biologically informative neural ordinary differential equations (neural ODEs) to model cell state trajectories under perturbations and derive the causal GRN from the neural ODE's parameters. We demonstrate PerturbODE's efficacy in trajectory prediction and GRN inference across simulated and real over-expression datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.02481</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.02481</id><created>2025-01-05</created><updated>2025-02-01</updated><authors><author><keyname>Xie</keyname><forenames>Zhengpeng</forenames></author><author><keyname>Cao</keyname><forenames>Jiahang</forenames></author><author><keyname>Zhang</keyname><forenames>Qiang</forenames></author><author><keyname>Zhang</keyname><forenames>Jianxiong</forenames></author><author><keyname>Wang</keyname><forenames>Changwei</forenames></author><author><keyname>Xu</keyname><forenames>Renjing</forenames></author></authors><title>The Meta-Representation Hypothesis</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans rely on high-level understandings of things, i.e., meta-representations, to engage in abstract reasoning. In complex cognitive tasks, these meta-representations help individuals abstract general rules from experience. However, constructing such meta-representations from high-dimensional observations remains a longstanding challenge for reinforcement learning (RL) agents. For instance, a well-trained agent often fails to generalize to even minor variations of the same task, such as changes in background color, while humans can easily handle. In this paper, we theoretically investigate how meta-representations contribute to the generalization ability of RL agents, demonstrating that learning meta-representations from high-dimensional observations enhance an agent's ability to generalize across varied environments. We further hypothesize that deep mutual learning (DML) among agents can help them learn the meta-representations that capture the underlying essence of the task. Empirical results provide strong support for both our theory and hypothesis. Overall, this work provides a new perspective on the generalization of deep reinforcement learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.02608</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.02608</id><created>2025-01-05</created><updated>2025-02-02</updated><authors><author><keyname>Khouri</keyname><forenames>Basel</forenames></author><author><keyname>Vizel</keyname><forenames>Yakir</forenames></author></authors><title>Revisiting DRUP-based Interpolants with CaDiCaL 2.0</title><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present our implementation of DRUP-based interpolants in CaDiCaL 2.0, and evaluate performance in the bit-level model checker Avy using the Hardware Model Checking Competition benchmarks.   CaDiCaL is a state-of-the-art, open-source SAT solver known for its efficiency and flexibility. In its latest release, version 2.0, CaDiCaL introduces a new proof tracer API. This paper presents a tool that leverages this API to implement the DRUP-based algorithm for generating interpolants.   By integrating this algorithm into CaDiCaL, we enable its use in model-checking workflows that require interpolants. Our experimental evaluation shows that integrating CaDiCaL with DRUP-based interpolants in Avy results in better performance (both runtime and number of solved instances) when compared to Avy with Glucose as the main SAT solver.   Our implementation is publicly available and can be used by the formal methods community to further develop interpolation-based algorithms using the state-of-the-art SAT solver CaDiCaL. Since our implementation uses the Tracer API, it should be maintainable and applicable to future releases of CaDiCaL. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.02625</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.02625</id><created>2025-01-05</created><updated>2025-02-01</updated><authors><author><keyname>Ashkboos</keyname><forenames>Saleh</forenames></author><author><keyname>Nikdan</keyname><forenames>Mahdi</forenames></author><author><keyname>Tabesh</keyname><forenames>Soroush</forenames></author><author><keyname>Castro</keyname><forenames>Roberto L.</forenames></author><author><keyname>Hoefler</keyname><forenames>Torsten</forenames></author><author><keyname>Alistarh</keyname><forenames>Dan</forenames></author></authors><title>HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs</title><categories>cs.LG</categories><comments>13 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantized training of Large Language Models (LLMs) remains an open challenge, as maintaining accuracy while performing all matrix multiplications in low precision has proven difficult. This is particularly the case when fine-tuning pre-trained models, which can have large weight and activation outlier values that make lower-precision optimization difficult. To address this, we present HALO, a novel quantization-aware training approach for Transformers that enables accurate and efficient low-precision training by combining 1) strategic placement of Hadamard rotations in both forward and backward passes, which mitigate outliers, 2) high-performance kernel support, and 3) FSDP integration for low-precision communication. Our approach ensures that all large matrix multiplications during the forward and backward passes are executed in lower precision. Applied to LLAMA-family models, HALO achieves near-full-precision-equivalent results during fine-tuning on various tasks, while delivering up to 1.41x end-to-end speedup for full fine-tuning on RTX 4090 GPUs. HALO efficiently supports both standard and parameterefficient fine-tuning (PEFT). Our results demonstrate the first practical approach to fully quantized LLM fine-tuning that maintains accuracy in 8-bit precision, while delivering performance benefits. Code is available at \url{https://github.com/IST-DASLab/HALO}. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.03907</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.03907</id><created>2025-01-07</created><updated>2025-02-01</updated><authors><author><keyname>Bramblett</keyname><forenames>Lauren</forenames></author><author><keyname>Reasoner</keyname><forenames>Jonathan</forenames></author><author><keyname>Bezzo</keyname><forenames>Nicola</forenames></author></authors><title>Implicit Coordination using Active Epistemic Inference for Multi-Robot   Systems</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Multi-robot system (MRS) provides significant advantages for intricate tasks such as environmental monitoring, underwater inspections, and space missions. However, addressing potential communication failures or the lack of communication infrastructure in these fields remains a challenge. A significant portion of MRS research presumes that the system can maintain communication with proximity constraints, but this approach does not solve situations where communication is either non-existent, unreliable, or poses a security risk. Some approaches tackle this issue using predictions about other robots while not communicating, but these methods generally only permit agents to utilize first-order reasoning, which involves reasoning based purely on their own observations. In contrast, to deal with this problem, our proposed framework utilizes Theory of Mind (ToM), employing higher-order reasoning by shifting a robot's perspective to reason about a belief of others observations. Our approach has two main phases: i) an efficient runtime plan adaptation using active inference to signal intentions and reason about a robot's own belief and the beliefs of others in the system, and ii) a hierarchical epistemic planning framework to iteratively reason about the current MRS mission state. The proposed framework outperforms greedy and first-order reasoning approaches and is validated using simulations and experiments with heterogeneous robotic systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.04229</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.04229</id><created>2025-01-07</created><updated>2025-02-01</updated><authors><author><keyname>Ge</keyname><forenames>Jifeng</forenames></author><author><keyname>Liu</keyname><forenames>Jianzhou</forenames></author><author><keyname>Zhang</keyname><forenames>Juan</forenames></author></authors><title>Three-precision iterative refinement with parameter regularization and   prediction for solving large sparse linear systems</title><categories>math.NA cs.NA</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This study presents a novel mixed-precision iterative refinement algorithm, GADI-IR, within the general alternating-direction implicit (GADI) framework, designed for efficiently solving large-scale sparse linear systems. By employing low-precision arithmetic, particularly half-precision (FP16), for computationally intensive inner iterations, the method achieves substantial acceleration while maintaining high numerical accuracy. Key challenges such as overflow in FP16 and convergence issues for low precision are addressed through careful backward error analysis and the application of a regularization parameter $\alpha$. Furthermore, the integration of low-precision arithmetic into the parameter prediction process, using Gaussian process regression (GPR), significantly reduces computational time without degrading performance. The method is particularly effective for large-scale linear systems arising from discretized partial differential equations and other high-dimensional problems, where both accuracy and efficiency are critical. Numerical experiments demonstrate that the use of FP16 and mixed-precision strategies not only accelerates computation but also ensures robust convergence, making the approach advantageous for various applications. The results highlight the potential of leveraging lower-precision arithmetic to achieve superior computational efficiency in high-performance computing. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.04377</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.04377</id><created>2025-01-08</created><updated>2025-02-02</updated><authors><author><keyname>Ke</keyname><forenames>Yekun</forenames></author><author><keyname>Li</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Liang</keyname><forenames>Yingyu</forenames></author><author><keyname>Sha</keyname><forenames>Zhizhou</forenames></author><author><keyname>Shi</keyname><forenames>Zhenmei</forenames></author><author><keyname>Song</keyname><forenames>Zhao</forenames></author></authors><title>On Computational Limits and Provably Efficient Criteria of Visual   Autoregressive Models: A Fine-Grained Complexity Analysis</title><categories>cs.LG cs.AI cs.CC cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recently, Visual Autoregressive ($\mathsf{VAR}$) Models introduced a groundbreaking advancement in the field of image generation, offering a scalable approach through a coarse-to-fine ``next-scale prediction'' paradigm. Suppose that $n$ represents the height and width of the last VQ code map generated by $\mathsf{VAR}$ models, the state-of-the-art algorithm in [Tian, Jiang, Yuan, Peng and Wang, NeurIPS 2024] takes $O(n^{4+o(1)})$ time, which is computationally inefficient. In this work, we analyze the computational limits and efficiency criteria of $\mathsf{VAR}$ Models through a fine-grained complexity lens. Our key contribution is identifying the conditions under which $\mathsf{VAR}$ computations can achieve sub-quadratic time complexity. We have proved that assuming the Strong Exponential Time Hypothesis ($\mathsf{SETH}$) from fine-grained complexity theory, a sub-quartic time algorithm for $\mathsf{VAR}$ models is impossible. To substantiate our theoretical findings, we present efficient constructions leveraging low-rank approximations that align with the derived criteria. This work initiates the study of the computational efficiency of the $\mathsf{VAR}$ model from a theoretical perspective. Our technique will shed light on advancing scalable and efficient image generation in $\mathsf{VAR}$ frameworks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.04901</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.04901</id><created>2025-01-08</created><updated>2025-02-02</updated><authors><author><keyname>Huang</keyname><forenames>Keke</forenames></author><author><keyname>Shi</keyname><forenames>Yimin</forenames></author><author><keyname>Ding</keyname><forenames>Dujian</forenames></author><author><keyname>Li</keyname><forenames>Yifei</forenames></author><author><keyname>Fei</keyname><forenames>Yang</forenames></author><author><keyname>Lakshmanan</keyname><forenames>Laks</forenames></author><author><keyname>Xiao</keyname><forenames>Xiaokui</forenames></author></authors><title>ThriftLLM: On Cost-Effective Selection of Large Language Models for   Classification Queries</title><categories>cs.DB</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In recent years, large language models (LLMs) have demonstrated remarkable capabilities in comprehending and generating natural language content. An increasing number of services offer LLMs for various tasks via APIs. Different LLMs demonstrate expertise in different domains of queries (e.g., text classification queries). Meanwhile, LLMs of different scales, complexity, and performance are priced diversely. Driven by this, several researchers are investigating strategies for selecting an ensemble of LLMs, aiming to decrease overall usage costs while enhancing performance. However, to the best of our knowledge, none of the existing works addresses the problem, how to find an LLM ensemble subject to a cost budget, which maximizes the ensemble performance with guarantees.   In this paper, we formalize the performance of an ensemble of models (LLMs) using the notion of prediction accuracy which we formally define. We develop an approach for aggregating responses from multiple LLMs to enhance ensemble performance. Building on this, we formulate the Optimal Ensemble Selection problem of selecting a set of LLMs subject to a cost budget that maximizes the overall prediction accuracy. We show that prediction accuracy is non-decreasing and non-submodular and provide evidence that the Optimal Ensemble Selection problem is likely to be NP-hard. By leveraging a submodular function that upper bounds prediction accuracy, we develop an algorithm called ThriftLLM and prove that it achieves an instance-dependent approximation guarantee with high probability. In addition, it achieves state-of-the-art performance for text classification and entity matching queries on multiple real-world datasets against various baselines in our extensive experimental evaluation, while using a relatively lower cost budget, strongly supporting the effectiveness and superiority of our method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.05130</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.05130</id><created>2025-01-09</created><updated>2025-02-01</updated><authors><author><keyname>Lunardi</keyname><forenames>Willian T.</forenames></author><author><keyname>Banabila</keyname><forenames>Abdulrahman</forenames></author><author><keyname>Herzalla</keyname><forenames>Dania</forenames></author><author><keyname>Andreoni</keyname><forenames>Martin</forenames></author></authors><title>Learning Compact and Robust Representations for Anomaly Detection</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Distance-based anomaly detection methods rely on compact and separable in-distribution (ID) embeddings to effectively delineate anomaly boundaries. Single-positive contrastive formulations suffer from class collision, promoting unnecessary intra-class variance within ID samples. While multi-positive formulations can improve inlier compactness, they fail to preserve the diversity among synthetic outliers. We address these limitations by proposing a contrastive pretext task for anomaly detection that enforces three key properties: (1) compact ID clustering to reduce intra-class variance, (2) inlier-outlier separation to enhance inter-class separation, and (3) outlier-outlier separation to maintain diversity among synthetic outliers and prevent representation collapse. These properties work together to ensure a more robust and discriminative feature space for anomaly detection. Our approach achieves approximately 12x faster convergence than NT-Xent and 7x faster than Rot-SupCon, with superior performance. On CIFAR-10, it delivers an average performance boost of 6.2% over NT-Xent and 2% over Rot-SupCon, with class-specific improvements of up to 16.9%. Our code is available at https://anonymous.4open.science/r/firm-98B6. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.05205</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.05205</id><created>2025-01-09</created><updated>2025-02-03</updated><authors><author><keyname>Ke</keyname><forenames>Xueyi</forenames></author><author><keyname>Tsutsui</keyname><forenames>Satoshi</forenames></author><author><keyname>Zhang</keyname><forenames>Yayun</forenames></author><author><keyname>Wen</keyname><forenames>Bihan</forenames></author></authors><title>Discovering Hidden Visual Concepts Beyond Linguistic Input in Infant   Learning</title><categories>cs.CV cs.AI</categories><comments>12 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Infants develop complex visual understanding rapidly, even preceding of the acquisition of linguistic inputs. As computer vision seeks to replicate the human vision system, understanding infant visual development may offer valuable insights. In this paper, we present an interdisciplinary study exploring this question: can a computational model that imitates the infant learning process develop broader visual concepts that extend beyond the vocabulary it has heard, similar to how infants naturally learn? To investigate this, we analyze a recently published model in Science by Vong et al.,which is trained on longitudinal, egocentric images of a single child paired with transcribed parental speech. We introduce a training-free framework that can discover visual concept neurons hidden in the model's internal representations. Our findings show that these neurons can classify objects outside its original vocabulary. Furthermore, we compare the visual representations in infant-like models with those in moder computer vision models, such as CLIP or ImageNet pre-trained model, highlighting key similarities and differences. Ultimately, our work bridges cognitive science and computer vision by analyzing the internal representations of a computational model trained on an infant's visual and linguistic inputs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.05262</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.05262</id><created>2025-01-09</created><updated>2025-02-01</updated><authors><author><keyname>Zhang</keyname><forenames>Isaac</forenames></author><author><keyname>Zarick</keyname><forenames>Ryan</forenames></author><author><keyname>Wong</keyname><forenames>Daniel</forenames></author><author><keyname>Kim</keyname><forenames>Thomas</forenames></author><author><keyname>Pellegrino</keyname><forenames>Bryan</forenames></author><author><keyname>Li</keyname><forenames>Mignon</forenames></author><author><keyname>Wong</keyname><forenames>Kelvin</forenames></author></authors><title>QMDB: Quick Merkle Database</title><categories>cs.NI cs.DB</categories><comments>11 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain state management by integrating key-value (KV) and Merkle tree storage into a single unified architecture. QMDB delivers a significant throughput improvement over existing architectures, achieving up to 6X over the widely used RocksDB and 8X over NOMT, a leading verifiable database. Its novel append-only twig-based design enables one SSD read per state access, O(1) IOs for updates, and in-memory Merkleization on a memory footprint as small as 2.3 bytes per entry, enabling it to run on even modest consumer-grade PCs. QMDB scales seamlessly across both commodity and enterprise hardware, achieving up to 2.28 million state updates per second. This performance enables support for 1 million token transfers per second (TPS), marking QMDB as the first solution achieving such a milestone. QMDB has been benchmarked with workloads exceeding 15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to scale to 280 billion entries on a single server. Furthermore, QMDB introduces historical proofs, unlocking the ability to query its blockchain's historical state at the latest block. QMDB not only meets the demands of current blockchains but also provides a robust foundation for building scalable, efficient, and verifiable decentralized applications across diverse use cases. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.05541</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.05541</id><created>2025-01-09</created><updated>2025-02-02</updated><authors><author><keyname>Lamprou</keyname><forenames>Zenon</forenames></author><author><keyname>Moshfeghi</keyname><forenames>Yashar</forenames></author></authors><title>Customizable LLM-Powered Chatbot for Behavioral Science Research</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The rapid advancement of Artificial Intelligence has resulted in the advent of Large Language Models (LLMs) with the capacity to produce text that closely resembles human communication. These models have been seamlessly integrated into diverse applications, enabling interactive and responsive communication across multiple platforms. The potential utility of chatbots transcends these traditional applications, particularly in research contexts, wherein they can offer valuable insights and facilitate the design of innovative experiments. In this study, we present a Customizable LLM-Powered Chatbot (CLPC), a web-based chatbot system designed to assist in behavioral science research. The system is meticulously designed to function as an experimental instrument rather than a conventional chatbot, necessitating users to input a username and experiment code upon access. This setup facilitates precise data cross-referencing, thereby augmenting the integrity and applicability of the data collected for research purposes. It can be easily expanded to accommodate new basic events as needed; and it allows researchers to integrate their own logging events without the necessity of implementing a separate logging mechanism. It is worth noting that our system was built to assist primarily behavioral science research but is not limited to it, it can easily be adapted to assist information retrieval research or interacting with chat bot agents in general. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.05795</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.05795</id><created>2025-01-10</created><updated>2025-02-03</updated><authors><author><keyname>Kinjo</keyname><forenames>Keita</forenames></author></authors><title>Robust Counterfactual Explanations under Model Multiplicity Using   Multi-Objective Optimization</title><categories>cs.LG cs.AI</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, explainability in machine learning has gained importance. In this context, counterfactual explanation (CE), which is an explanation method that uses examples, has attracted attention. However, it has been pointed out that CE is not robust when there are multiple machine-learning models with similar accuracy. These problems are important when using machine learning to make safe decisions. In this paper, we propose robust CEs that introduce a new viewpoint -- Pareto improvement -- and a method that uses multi-objective optimization to generate it. To evaluate the proposed method, we conducted experiments using both simulated and real data. The results demonstrate that the proposed method is both robust and practical. This study highlights the potential of ensuring robustness in decision-making by applying the concept of social welfare. We believe that this research can serve as a valuable foundation for various fields, including explainability in machine learning, decision-making, and action planning based on machine learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.05844</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.05844</id><created>2025-01-10</created><updated>2025-02-02</updated><authors><author><keyname>Kungurtsev</keyname><forenames>Vyacheslav</forenames></author><author><keyname>Moore</keyname><forenames>Leonardo Christov</forenames></author><author><keyname>Sir</keyname><forenames>Gustav</forenames></author><author><keyname>Krutsky</keyname><forenames>Martin</forenames></author></authors><title>"Cause" is Mechanistic Narrative within Scientific Domains: An Ordinary   Language Philosophical Critique of "Causal Machine Learning"</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Causal Learning has emerged as a major theme of research in statistics and machine learning in recent years, promising specific computational techniques to apply to datasets that reveal the true nature of cause and effect in a number of important domains. In this paper we consider the epistemology of recognizing true cause and effect phenomena. We apply the Ordinary Language method of engaging on the customary use of the word 'cause' to investigate valid semantics of reasoning about cause and effect. We recognize that the grammars of cause and effect are fundamentally distinct in form across scientific domains, yet they maintain a consistent and central function. This function can best be described as the mechanism underlying fundamental forces of influence as considered prominent in the respective scientific domain. We demarcate 1) physics and engineering as domains wherein mathematical models are sufficient to comprehensively describe causality, 2) biology as introducing challenges of emergence while providing opportunities for showing consistent mechanisms across scale, and 3) the social sciences as introducing grander difficulties for establishing models of low prediction error but providing, through Hermeneutics, the potential for findings that are still instrumentally useful to individuals. We posit that definitive causal claims regarding a given phenomenon (writ large) can only come through an agglomeration of consistent evidence across multiple domains. This presents important methodological questions as far as harmonizing between language games and emergence across scales. Given the role of epistemic hubris in the contemporary crisis of credibility in the sciences, exercising greater caution as far as communicating precision as to the real degree of certainty certain evidence provides for rich collections of open problems in optimizing integration of different findings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.05855</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.05855</id><created>2025-01-10</created><updated>2025-02-03</updated><authors><author><keyname>Poché</keyname><forenames>Antonin</forenames><affiliation>IRIT</affiliation></author><author><keyname>Jacovi</keyname><forenames>Alon</forenames><affiliation>CERCO UMR5549, ANITI</affiliation></author><author><keyname>Picard</keyname><forenames>Agustin Martin</forenames><affiliation>CERCO UMR5549, ANITI</affiliation></author><author><keyname>Boutin</keyname><forenames>Victor</forenames><affiliation>CERCO UMR5549, ANITI</affiliation></author><author><keyname>Jourdan</keyname><forenames>Fanny</forenames></author></authors><title>ConSim: Measuring Concept-Based Explanations' Effectiveness with   Automated Simulatability</title><categories>cs.CL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concept-based explanations work by mapping complex model computations to human-understandable concepts. Evaluating such explanations is very difficult, as it includes not only the quality of the induced space of possible concepts but also how effectively the chosen concepts are communicated to users. Existing evaluation metrics often focus solely on the former, neglecting the latter. We introduce an evaluation framework for measuring concept explanations via automated simulatability: a simulator's ability to predict the explained model's outputs based on the provided explanations. This approach accounts for both the concept space and its interpretation in an end-to-end evaluation. Human studies for simulatability are notoriously difficult to enact, particularly at the scale of a wide, comprehensive empirical evaluation (which is the subject of this work). We propose using large language models (LLMs) as simulators to approximate the evaluation and report various analyses to make such approximations reliable. Our method allows for scalable and consistent evaluation across various models and datasets. We report a comprehensive empirical evaluation using this framework and show that LLMs provide consistent rankings of explanation methods. Code available at https://github.com/AnonymousConSim/ConSim. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.06224</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.06224</id><created>2025-01-07</created><updated>2025-02-03</updated><authors><author><keyname>Jiang</keyname><forenames>Wen-Dong</forenames></author><author><keyname>Chang</keyname><forenames>Chih-Yung</forenames></author><author><keyname>Roy</keyname><forenames>Diptendu Sinha</forenames></author></authors><title>Detection, Retrieval, and Explanation Unified: A Violence Detection   System Based on Knowledge Graphs and GAT</title><categories>cs.CV cs.AI</categories><comments>This work has been submitted to the IEEE for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, violence detection systems developed using unified multimodal models have achieved significant success and attracted widespread attention. However, most of these systems face two critical challenges: the lack of interpretability as black-box models and limited functionality, offering only classification or retrieval capabilities. To address these challenges, this paper proposes a novel interpretable violence detection system, termed the Three-in-One (TIO) System. The TIO system integrates knowledge graphs (KG) and graph attention networks (GAT) to provide three core functionalities: detection, retrieval, and explanation. Specifically, the system processes each video frame along with text descriptions generated by a large language model (LLM) for videos containing potential violent behavior. It employs ImageBind to generate high-dimensional embeddings for constructing a knowledge graph, uses GAT for reasoning, and applies lightweight time series modules to extract video embedding features. The final step connects a classifier and retriever for multi-functional outputs. The interpretability of KG enables the system to verify the reasoning process behind each output. Additionally, the paper introduces several lightweight methods to reduce the resource consumption of the TIO system and enhance its efficiency. Extensive experiments conducted on the XD-Violence and UCF-Crime datasets validate the effectiveness of the proposed system. A case study further reveals an intriguing phenomenon: as the number of bystanders increases, the occurrence of violent behavior tends to decrease. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.06269</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.06269</id><created>2025-01-09</created><updated>2025-02-02</updated><authors><author><keyname>Aydin</keyname><forenames>Omer</forenames></author><author><keyname>Karaarslan</keyname><forenames>Enis</forenames></author></authors><title>OpenAI ChatGPT interprets Radiological Images: GPT-4 as a Medical Doctor   for a Fast Check-Up</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  OpenAI released version GPT-4 on March 14, 2023, following the success of ChatGPT, which was announced in November 2022. In addition to the existing GPT-3 features, GPT-4 can interpret images. To achieve this, the processing power and model have been significantly improved. The ability to process and interpret images goes far beyond the applications and effectiveness of artificial intelligence. In this study, we first explored the interpretation of radiological images in healthcare using artificial intelligence (AI). Then, we experimented with the image interpretation capability of the GPT-4. In this way, we addressed the question of whether artificial intelligence (AI) can replace a healthcare professional (e.g., a medical doctor) or whether it can be used as a decision-support tool that makes decisions easier and more reliable. Our results showed that ChatGPT is not sufficient and accurate to analyze chest X-ray images, but it can provide interpretations that can assist medical doctors or clinicians. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.06278</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.06278</id><created>2025-01-10</created><updated>2025-02-02</updated><authors><author><keyname>Lamprou</keyname><forenames>Zenon</forenames></author><author><keyname>Polick</keyname><forenames>Frank</forenames></author><author><keyname>Moshfeghi</keyname><forenames>Yashar</forenames></author></authors><title>Aligning Brain Activity with Advanced Transformer Models: Exploring the   Role of Punctuation in Semantic Processing</title><categories>cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This research examines the congruence between neural activity and advanced transformer models, emphasizing the semantic significance of punctuation in text understanding. Utilizing an innovative approach originally proposed by Toneva and Wehbe, we evaluate four advanced transformer models RoBERTa, DistiliBERT, ALBERT, and ELECTRA against neural activity data. Our findings indicate that RoBERTa exhibits the closest alignment with neural activity, surpassing BERT in accuracy. Furthermore, we investigate the impact of punctuation removal on model performance and neural alignment, revealing that BERT's accuracy enhances in the absence of punctuation. This study contributes to the comprehension of how neural networks represent language and the influence of punctuation on semantic processing within the human brain. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.06326</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.06326</id><created>2025-01-10</created><updated>2025-02-02</updated><authors><author><keyname>Lamprou</keyname><forenames>Zenon</forenames></author><author><keyname>Moshfeghi</keyname><forenames>Yashar</forenames></author></authors><title>On Creating A Brain-To-Text Decoder</title><categories>cs.LG eess.IV eess.SP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Brain decoding has emerged as a rapidly advancing and extensively utilized technique within neuroscience. This paper centers on the application of raw electroencephalogram (EEG) signals for decoding human brain activity, offering a more expedited and efficient methodology for enhancing our understanding of the human brain. The investigation specifically scrutinizes the efficacy of brain-computer interfaces (BCI) in deciphering neural signals associated with speech production, with particular emphasis on the impact of vocabulary size, electrode density, and training data on the framework's performance. The study reveals the competitive word error rates (WERs) achievable on the Librispeech benchmark through pre-training on unlabelled data for speech processing. Furthermore, the study evaluates the efficacy of voice recognition under configurations with limited labeled data, surpassing previous state-of-the-art techniques while utilizing significantly fewer labels. Additionally, the research provides a comprehensive analysis of error patterns in voice recognition and the influence of model size and unlabelled training data. It underscores the significance of factors such as vocabulary size and electrode density in enhancing BCI performance, advocating for an increase in microelectrodes and refinement of language models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.06589</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.06589</id><created>2025-01-11</created><updated>2025-01-31</updated><authors><author><keyname>Zhang</keyname><forenames>Muru</forenames></author><author><keyname>Mishra</keyname><forenames>Mayank</forenames></author><author><keyname>Zhou</keyname><forenames>Zhongzhu</forenames></author><author><keyname>Brandon</keyname><forenames>William</forenames></author><author><keyname>Wang</keyname><forenames>Jue</forenames></author><author><keyname>Kim</keyname><forenames>Yoon</forenames></author><author><keyname>Ragan-Kelley</keyname><forenames>Jonathan</forenames></author><author><keyname>Song</keyname><forenames>Shuaiwen Leon</forenames></author><author><keyname>Athiwaratkun</keyname><forenames>Ben</forenames></author><author><keyname>Dao</keyname><forenames>Tri</forenames></author></authors><title>Ladder-residual: parallelism-aware architecture for accelerating large   model inference with communication overlapping</title><categories>cs.LG cs.CL cs.DC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language model inference is both memory-intensive and time-consuming, often requiring distributed algorithms to efficiently scale. Various model parallelism strategies are used in multi-gpu training and inference to partition computation across multiple devices, reducing memory load and computation time. However, using model parallelism necessitates communication of information between GPUs, which has been a major bottleneck and limits the gains obtained by scaling up the number of devices. We introduce Ladder Residual, a simple architectural modification applicable to all residual-based models that enables straightforward overlapping that effectively hides the latency of communication. Our insight is that in addition to systems optimization, one can also redesign the model architecture to decouple communication from computation. While Ladder Residual can allow communication-computation decoupling in conventional parallelism patterns, we focus on Tensor Parallelism in this paper, which is particularly bottlenecked by its heavy communication. For a Transformer model with 70B parameters, applying Ladder Residual to all its layers can achieve 29% end-to-end wall clock speed up at inference time with TP sharding over 8 devices. We refer the resulting Transformer model as the Ladder Transformer. We train a 1B and 3B Ladder Transformer from scratch and observe comparable performance to a standard dense transformer baseline. We also show that it is possible to convert parts of the Llama-3.1 8B model to our Ladder Residual architecture with minimal accuracy degradation by only retraining for 3B tokens. We release our code for training and inference for easier replication of experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.06708</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.06708</id><created>2025-01-11</created><updated>2025-02-02</updated><authors><author><keyname>Huang</keyname><forenames>Tzu-Heng</forenames></author><author><keyname>Bilkhu</keyname><forenames>Manjot</forenames></author><author><keyname>Sala</keyname><forenames>Frederic</forenames></author><author><keyname>Movellan</keyname><forenames>Javier</forenames></author></authors><title>Evaluating Sample Utility for Data Selection by Mimicking Model Weights</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Foundation models are trained on large-scale web-crawled datasets, which often contain noise, biases, and irrelevant information. This motivates the use of data selection techniques, which can be divided into model-free variants -- relying on heuristic rules and downstream datasets -- and model-based, e.g., using influence functions. The former can be expensive to design and risk introducing unwanted dependencies, while the latter are often computationally prohibitive. Instead, we propose an efficient, model-based approach using the Mimic Score, a new data quality metric that leverages the weights of a reference model to assess the usefulness of individual samples for training a new model. It relies on the alignment between gradients and a target direction induced by the reference model. Using the derived Mimic Scores, we develop Grad-Mimic, a framework that prioritizes samples for learning, creates effective filters, and automates data selection. Empirically, using Mimic Scores to guide training improves data efficiency, results in consistent performance gains across six image datasets, and includes enhancements to CLIP models. Moreover, Mimic Score-based filters improve upon existing filtering methods, e.g., cutting 4.7 million samples to train better CLIP models while offering accurate estimation of training dataset quality. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.06757</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.06757</id><created>2025-01-12</created><updated>2025-02-01</updated><authors><author><keyname>Jansen</keyname><forenames>Pascal</forenames></author><author><keyname>Colley</keyname><forenames>Mark</forenames></author><author><keyname>Krauß</keyname><forenames>Svenja</forenames></author><author><keyname>Hirschle</keyname><forenames>Daniel</forenames></author><author><keyname>Rukzio</keyname><forenames>Enrico</forenames></author></authors><title>OptiCarVis: Improving Automated Vehicle Functionality Visualizations   Using Bayesian Optimization to Enhance User Experience</title><categories>cs.HC</categories><doi>10.1145/3706598.3713514</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Automated vehicle (AV) acceptance relies on their understanding via feedback. While visualizations aim to enhance user understanding of AV's detection, prediction, and planning functionalities, establishing an optimal design is challenging. Traditional "one-size-fits-all" designs might be unsuitable, stemming from resource-intensive empirical evaluations. This paper introduces OptiCarVis, a set of Human-in-the-Loop (HITL) approaches using Multi-Objective Bayesian Optimization (MOBO) to optimize AV feedback visualizations. We compare conditions using eight expert and user-customized designs for a Warm-Start HITL MOBO. An online study (N=117) demonstrates OptiCarVis's efficacy in significantly improving trust, acceptance, perceived safety, and predictability without increasing cognitive load. OptiCarVis facilitates a comprehensive design space exploration, enhancing in-vehicle interfaces for optimal passenger experiences and broader applicability. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.06927</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.06927</id><created>2025-01-12</created><updated>2025-02-02</updated><authors><author><keyname>Zheng</keyname><forenames>Xinyi</forenames></author><author><keyname>Zhang</keyname><forenames>Steve</forenames></author><author><keyname>Lin</keyname><forenames>Weizhe</forenames></author><author><keyname>Zhang</keyname><forenames>Aaron</forenames></author><author><keyname>Mayol-Cuevas</keyname><forenames>Walterio W.</forenames></author><author><keyname>Shen</keyname><forenames>Junxiao</forenames></author></authors><title>CULTURE3D: Cultural Landmarks and Terrain Dataset for 3D Applications</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a large-scale fine-grained dataset using high-resolution images captured from locations worldwide. Compared to existing datasets, our dataset offers a significantly larger size and includes a higher level of detail, making it uniquely suited for fine-grained 3D applications. Notably, our dataset is built using drone-captured aerial imagery, which provides a more accurate perspective for capturing real-world site layouts and architectural structures. By reconstructing environments with these detailed images, our dataset supports applications such as the COLMAP format for Gaussian Splatting and the Structure-from-Motion (SfM) method. It is compatible with widely-used techniques including SLAM, Multi-View Stereo, and Neural Radiance Fields (NeRF), enabling accurate 3D reconstructions and point clouds. This makes it a benchmark for reconstruction and segmentation tasks. The dataset enables seamless integration with multi-modal data, supporting a range of 3D applications, from architectural reconstruction to virtual tourism. Its flexibility promotes innovation, facilitating breakthroughs in 3D modeling and analysis. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.07267</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.07267</id><created>2025-01-13</created><updated>2025-02-01</updated><authors><author><keyname>Seo</keyname><forenames>Wonduk</forenames></author><author><keyname>Bu</keyname><forenames>Yi</forenames></author></authors><title>Transforming Role Classification in Scientific Teams Using LLMs and   Advanced Predictive Analytics</title><categories>cs.DL cs.SI</categories><comments>16 pages, 5 figures, 3 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Scientific team dynamics are critical in determining the nature and impact of research outputs. However, existing methods for classifying author roles based on self-reports and clustering lack comprehensive contextual analysis of contributions. Thus, we present a transformative approach to classifying author roles in scientific teams using advanced large language models (LLMs), which offers a more refined analysis compared to traditional clustering methods. Specifically, we seek to complement and enhance these traditional methods by utilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2 70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting, we categorize author roles and demonstrate that GPT-4 outperforms other models across multiple categories, surpassing traditional approaches such as XGBoost and BERT. Our methodology also includes building a predictive deep learning model using 10 features. By training this model on a dataset derived from the OpenAlex database, which provides detailed metadata on academic publications -- such as author-publication history, author affiliation, research topics, and citation counts -- we achieve an F1 score of 0.76, demonstrating robust classification of author roles. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.07288</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.07288</id><created>2025-01-13</created><updated>2025-02-01</updated><authors><author><keyname>Chong</keyname><forenames>Zan-Kai</forenames></author><author><keyname>Ohsaki</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Ng</keyname><forenames>Bryan</forenames></author></authors><title>LLM-Net: Democratizing LLMs-as-a-Service through Blockchain-based Expert   Networks</title><categories>cs.AI</categories><comments>8 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The centralization of Large Language Models (LLMs) development has created significant barriers to AI advancement, limiting the democratization of these powerful technologies. This centralization, coupled with the scarcity of high-quality training data and mounting complexity of maintaining comprehensive expertise across rapidly expanding knowledge domains, poses critical challenges to the continued growth of LLMs. While solutions like Retrieval-Augmented Generation (RAG) offer potential remedies, maintaining up-to-date expert knowledge across diverse domains remains a significant challenge, particularly given the exponential growth of specialized information. This paper introduces LLMs Networks (LLM-Net), a blockchain-based framework that democratizes LLMs-as-a-Service through a decentralized network of specialized LLM providers. By leveraging collective computational resources and distributed domain expertise, LLM-Net incorporates fine-tuned expert models for various specific domains, ensuring sustained knowledge growth while maintaining service quality through collaborative prompting mechanisms. The framework's robust design includes blockchain technology for transparent transaction and performance validation, establishing an immutable record of service delivery. Our simulation, built on top of state-of-the-art LLMs such as Claude 3.5 Sonnet, Llama 3.1, Grok-2, and GPT-4o, validates the effectiveness of the reputation-based mechanism in maintaining service quality by selecting high-performing respondents (LLM providers). Thereby it demonstrates the potential of LLM-Net to sustain AI advancement through the integration of decentralized expertise and blockchain-based accountability. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.07641</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.07641</id><created>2025-01-13</created><updated>2025-02-03</updated><authors><author><keyname>Ning</keyname><forenames>Kun-Peng</forenames></author><author><keyname>Yao</keyname><forenames>Jia-Yu</forenames></author><author><keyname>Liu</keyname><forenames>Yu-Yang</forenames></author><author><keyname>Ning</keyname><forenames>Mu-Nan</forenames></author><author><keyname>Yuan</keyname><forenames>Li</forenames></author></authors><title>GPT as a Monte Carlo Language Tree: A Probabilistic Perspective</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large Language Models (LLMs), such as GPT, are considered to learn the latent distributions within large-scale web-crawl datasets and accomplish natural language processing (NLP) tasks by predicting the next token. However, this mechanism of latent distribution modeling lacks quantitative understanding and analysis. In this paper, we propose a novel perspective that any language dataset can be represented by a Monte Carlo Language Tree (abbreviated as ``Data-Tree''), where each node denotes a token, each edge denotes a token transition probability, and each sequence has a unique path. Any GPT-like language model can also be flattened into another Monte Carlo Language Tree (abbreviated as ``GPT-Tree''). Our experiments show that different GPT models trained on the same dataset exhibit significant structural similarity in GPT-Tree visualization, and larger models converge more closely to the Data-Tree. More than 87\% GPT output tokens can be recalled by Data-Tree. These findings may confirm that the reasoning process of LLMs is more likely to be probabilistic pattern-matching rather than formal reasoning, as each model inference seems to find a context pattern with maximum probability from the Data-Tree. Furthermore, we provide deeper insights into issues such as hallucination, Chain-of-Thought (CoT) reasoning, and token bias in LLMs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.07863</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.07863</id><created>2025-01-14</created><updated>2025-02-01</updated><authors><author><keyname>Luo</keyname><forenames>Hao</forenames></author><author><keyname>Tang</keyname><forenames>Liping</forenames></author><author><keyname>Yang</keyname><forenames>Xinmin</forenames></author></authors><title>An accelerated gradient method with adaptive restart for convex   multiobjective optimization problems</title><categories>math.OC cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, based on the continuous time approach, we propose an accelerated gradient method with adaptive residual restart for convex multiobjective optimization problems. For the first, we derive rigorously the continuous limit of the multiobjective accelerated proximal gradient method by Tanabe et al. [Comput. Optim. Appl., 2023]. It is a second-order ordinary differential equation (ODE) that involves a special projection operator and can be viewed as an extension of the ODE by Su et al. [J. Mach. Learn. Res., 2016] for Nesterov acceleration. Then, we introduce a novel accelerated multiobjective gradient (AMG) flow with tailored time scaling that adapts automatically to the convex case and the strongly convex case, and the exponential decay rate of a merit function along with the solution trajectory of AMG flow is established via the Lyapunov analysis. After that, we consider an implicit-explicit time discretization and obtain an accelerated multiobjective gradient method with a convex quadratic programming subproblem. The fast sublinear rate and linear rate are proved respectively for convex and strongly convex problems. In addition, we present an efficient residual based adaptive restart technique to overcome the oscillation issue and improve the convergence significantly. Numerical results are provided to validate the practical performance of the proposed method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.07927</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.07927</id><created>2025-01-14</created><updated>2025-02-02</updated><authors><author><keyname>Pfister</keyname><forenames>Niklas</forenames></author><author><keyname>Volhejn</keyname><forenames>Václav</forenames></author><author><keyname>Knott</keyname><forenames>Manuel</forenames></author><author><keyname>Arias</keyname><forenames>Santiago</forenames></author><author><keyname>Bazińska</keyname><forenames>Julia</forenames></author><author><keyname>Bichurin</keyname><forenames>Mykhailo</forenames></author><author><keyname>Commike</keyname><forenames>Alan</forenames></author><author><keyname>Darling</keyname><forenames>Janet</forenames></author><author><keyname>Dienes</keyname><forenames>Peter</forenames></author><author><keyname>Fiedler</keyname><forenames>Matthew</forenames></author><author><keyname>Haber</keyname><forenames>David</forenames></author><author><keyname>Kraft</keyname><forenames>Matthias</forenames></author><author><keyname>Lancini</keyname><forenames>Marco</forenames></author><author><keyname>Mathys</keyname><forenames>Max</forenames></author><author><keyname>Pascual-Ortiz</keyname><forenames>Damián</forenames></author><author><keyname>Podolak</keyname><forenames>Jakub</forenames></author><author><keyname>Romero-López</keyname><forenames>Adrià</forenames></author><author><keyname>Shiarlis</keyname><forenames>Kyriacos</forenames></author><author><keyname>Signer</keyname><forenames>Andreas</forenames></author><author><keyname>Terek</keyname><forenames>Zsolt</forenames></author><author><keyname>Theocharis</keyname><forenames>Athanasios</forenames></author><author><keyname>Timbrell</keyname><forenames>Daniel</forenames></author><author><keyname>Trautwein</keyname><forenames>Samuel</forenames></author><author><keyname>Watts</keyname><forenames>Samuel</forenames></author><author><keyname>Wu</keyname><forenames>Yun-Han</forenames></author><author><keyname>Rojas-Carulla</keyname><forenames>Mateo</forenames></author></authors><title>Gandalf the Red: Adaptive Security for LLMs</title><categories>cs.LG cs.AI cs.CL cs.CR</categories><comments>Niklas Pfister, V\'aclav Volhejn and Manuel Knott contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current evaluations of defenses against prompt attacks in large language model (LLM) applications often overlook two critical factors: the dynamic nature of adversarial behavior and the usability penalties imposed on legitimate users by restrictive defenses. We propose D-SEC (Dynamic Security Utility Threat Model), which explicitly separates attackers from legitimate users, models multi-step interactions, and expresses the security-utility in an optimizable form. We further address the shortcomings in existing evaluations by introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed to generate realistic, adaptive attack. Using Gandalf, we collect and release a dataset of 279k prompt attacks. Complemented by benign user data, our analysis reveals the interplay between security and utility, showing that defenses integrated in the LLM (e.g., system prompts) can degrade usability even without blocking requests. We demonstrate that restricted application domains, defense-in-depth, and adaptive defenses are effective strategies for building secure and useful LLM applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.07959</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.07959</id><created>2025-01-14</created><updated>2025-02-01</updated><authors><author><keyname>Hua</keyname><forenames>Jiaqi</forenames></author><author><keyname>Wei</keyname><forenames>Wanxu</forenames></author></authors><title>Self-Instruct Few-Shot Jailbreaking: Decompose the Attack into Pattern   and Behavior Learning</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, several works have been conducted on jailbreaking Large Language Models (LLMs) with few-shot malicious demos. In particular, Zheng et al. focus on improving the efficiency of Few-Shot Jailbreaking (FSJ) by injecting special tokens into the demos and employing demo-level random search, known as Improved Few-Shot Jailbreaking (I-FSJ). Nevertheless, we notice that this method may still require a long context to jailbreak advanced models e.g. 32 shots of demos for Meta-Llama-3-8B-Instruct (Llama-3) \cite{llama3modelcard}. In this paper, we discuss the limitations of I-FSJ and propose Self-Instruct Few-Shot Jailbreaking (Self-Instruct-FSJ) facilitated with the demo-level greedy search. This framework decomposes the FSJ attack into pattern and behavior learning to exploit the model's vulnerabilities in a more generalized and efficient way. We conduct elaborate experiments to evaluate our method on common open-source models and compare it with baseline algorithms. Our code is available at https://github.com/iphosi/Self-Instruct-FSJ. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.08982</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.08982</id><created>2025-01-15</created><updated>2025-02-03</updated><authors><author><keyname>Ma</keyname><forenames>Qi</forenames></author><author><keyname>Yang</keyname><forenames>Runyi</forenames></author><author><keyname>Ren</keyname><forenames>Bin</forenames></author><author><keyname>Sebe</keyname><forenames>Nicu</forenames></author><author><keyname>Konukoglu</keyname><forenames>Ender</forenames></author><author><keyname>Van Gool</keyname><forenames>Luc</forenames></author><author><keyname>Paudel</keyname><forenames>Danda Pani</forenames></author></authors><title>CityLoc: 6DoF Pose Distributional Localization for Text Descriptions in   Large-Scale Scenes with Gaussian Representation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Localizing textual descriptions within large-scale 3D scenes presents inherent ambiguities, such as identifying all traffic lights in a city. Addressing this, we introduce a method to generate distributions of camera poses conditioned on textual descriptions, facilitating robust reasoning for broadly defined concepts.   Our approach employs a diffusion-based architecture to refine noisy 6DoF camera poses towards plausible locations, with conditional signals derived from pre-trained text encoders. Integration with the pretrained Vision-Language Model, CLIP, establishes a strong linkage between text descriptions and pose distributions. Enhancement of localization accuracy is achieved by rendering candidate poses using 3D Gaussian splatting, which corrects misaligned samples through visual reasoning.   We validate our method's superiority by comparing it against standard distribution estimation methods across five large-scale datasets, demonstrating consistent outperformance. Code, datasets and more information will be publicly available at our project page. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.09136</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.09136</id><created>2025-01-15</created><updated>2025-02-02</updated><authors><author><keyname>Singh</keyname><forenames>Aditi</forenames></author><author><keyname>Ehtesham</keyname><forenames>Abul</forenames></author><author><keyname>Kumar</keyname><forenames>Saket</forenames></author><author><keyname>Khoei</keyname><forenames>Tala Talaei</forenames></author></authors><title>Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG</title><categories>cs.AI cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large Language Models (LLMs) have revolutionized artificial intelligence (AI) by enabling human like text generation and natural language understanding. However, their reliance on static training data limits their ability to respond to dynamic, real time queries, resulting in outdated or inaccurate outputs. Retrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by integrating real time data retrieval to provide contextually relevant and up-to-date responses. Despite its promise, traditional RAG systems are constrained by static workflows and lack the adaptability required for multistep reasoning and complex task management.   Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these limitations by embedding autonomous AI agents into the RAG pipeline. These agents leverage agentic design patterns reflection, planning, tool use, and multiagent collaboration to dynamically manage retrieval strategies, iteratively refine contextual understanding, and adapt workflows to meet complex task requirements. This integration enables Agentic RAG systems to deliver unparalleled flexibility, scalability, and context awareness across diverse applications.   This survey provides a comprehensive exploration of Agentic RAG, beginning with its foundational principles and the evolution of RAG paradigms. It presents a detailed taxonomy of Agentic RAG architectures, highlights key applications in industries such as healthcare, finance, and education, and examines practical implementation strategies. Additionally, it addresses challenges in scaling these systems, ensuring ethical decision making, and optimizing performance for real-world applications, while providing detailed insights into frameworks and tools for implementing Agentic RAG. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.09532</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.09532</id><created>2025-01-16</created><updated>2025-02-01</updated><authors><author><keyname>Han</keyname><forenames>Jiayi</forenames></author><author><keyname>Du</keyname><forenames>Liang</forenames></author><author><keyname>Wu</keyname><forenames>Yiwen</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangguo</forenames></author><author><keyname>Du</keyname><forenames>Hongwei</forenames></author><author><keyname>Zheng</keyname><forenames>Weibo</forenames></author></authors><title>AdaFV: Rethinking of Visual-Language alignment for VLM acceleration</title><categories>cs.CV</categories><comments>14 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The success of VLMs often relies on the dynamic high-resolution schema that adaptively augments the input images to multiple crops, so that the details of the images can be retained. However, such approaches result in a large number of redundant visual tokens, thus significantly reducing the efficiency of the VLMs. To improve the VLMs' efficiency without introducing extra training costs, many research works are proposed to reduce the visual tokens by filtering the uninformative visual tokens or aggregating their information. Some approaches propose to reduce the visual tokens according to the self-attention of VLMs, which are biased, to result in inaccurate responses. The token reduction approaches solely rely on visual cues are text-agnostic, and fail to focus on the areas that are most relevant to the question, especially when the queried objects are non-salient to the image. In this work, we first conduct experiments to show that the original text embeddings are aligned with the visual tokens, without bias on the tailed visual tokens. We then propose a self-adaptive cross-modality attention mixture mechanism that dynamically leverages the effectiveness of visual saliency and text-to-image similarity in the pre-LLM layers to select the visual tokens that are informative. Extensive experiments demonstrate that the proposed approach achieves state-of-the-art training-free VLM acceleration performance, especially when the reduction rate is sufficiently large. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.09607</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.09607</id><created>2025-01-16</created><updated>2025-02-02</updated><authors><author><keyname>Etienney</keyname><forenames>Paul-Louis</forenames></author><author><keyname>Robin</keyname><forenames>Rémi</forenames></author><author><keyname>Rouchon</keyname><forenames>Pierre</forenames></author></authors><title>A posteriori error estimates for the Lindblad master equation</title><categories>math.NA cs.NA quant-ph</categories><msc-class>65, 81</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We are interested in the simulation of open quantum systems governed by the Lindblad master equation in an infinite-dimensional Hilbert space. To simulate the solution of this equation, the standard approach involves two sequential approximations: first, we truncate the Hilbert space to derive a differential equation in a finite-dimensional subspace. Then, we use discrete time-step to obtain a numerical solution to the finite-dimensional evolution.   In this paper, we establish bounds for these two approximations that can be explicitely computed to guarantee the accuracy of the numerical results. Through numerical examples, we demonstrate the efficiency of our method, empirically highlighting the tightness of the upper bound. While adaptive time-stepping is already a common practice in the time discretization of the Lindblad equation, we extend this approach by showing how to dynamically adjust the truncation of the Hilbert space. This enables fully adaptive simulations of the density matrix. For large-scale simulations, this approach significantly reduces computational time and relieves users of the challenge of selecting an appropriate truncation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.09720</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.09720</id><created>2025-01-16</created><updated>2025-01-31</updated><authors><author><keyname>Li</keyname><forenames>Qingyun</forenames></author><author><keyname>Chen</keyname><forenames>Yushi</forenames></author><author><keyname>Shu</keyname><forenames>Xinya</forenames></author><author><keyname>Chen</keyname><forenames>Dong</forenames></author><author><keyname>He</keyname><forenames>Xin</forenames></author><author><keyname>Yu</keyname><forenames>Yi</forenames></author><author><keyname>Yang</keyname><forenames>Xue</forenames></author></authors><title>A Simple Aerial Detection Baseline of Multimodal Language Models</title><categories>cs.CV cs.AI</categories><comments>4 pages, 1 table, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multimodal language models (MLMs) based on generative pre-trained Transformer are considered powerful candidates for unifying various domains and tasks. MLMs developed for remote sensing (RS) have demonstrated outstanding performance in multiple tasks, such as visual question answering and visual grounding. In addition to visual grounding that detects specific objects corresponded to given instruction, aerial detection, which detects all objects of multiple categories, is also a valuable and challenging task for RS foundation models. However, aerial detection has not been explored by existing RS MLMs because the autoregressive prediction mechanism of MLMs differs significantly from the detection outputs. In this paper, we present a simple baseline for applying MLMs to aerial detection for the first time, named LMMRotate. Specifically, we first introduce a normalization method to transform detection outputs into textual outputs to be compatible with the MLM framework. Then, we propose a evaluation method, which ensures a fair comparison between MLMs and conventional object detection models. We construct the baseline by fine-tuning open-source general-purpose MLMs and achieve impressive detection performance comparable to conventional detector. We hope that this baseline will serve as a reference for future MLM development, enabling more comprehensive capabilities for understanding RS images. Code is available at https://github.com/Li-Qingyun/mllm-mmrotate. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.09982</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.09982</id><created>2025-01-17</created><updated>2025-02-02</updated><authors><author><keyname>Cao</keyname><forenames>Yuefan</forenames></author><author><keyname>Gong</keyname><forenames>Chengyue</forenames></author><author><keyname>Li</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Liang</keyname><forenames>Yingyu</forenames></author><author><keyname>Sha</keyname><forenames>Zhizhou</forenames></author><author><keyname>Shi</keyname><forenames>Zhenmei</forenames></author><author><keyname>Song</keyname><forenames>Zhao</forenames></author></authors><title>RichSpace: Enriching Text-to-Video Prompt Space via Text Embedding   Interpolation</title><categories>cs.CV cs.AI cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Text-to-video generation models have made impressive progress, but they still struggle with generating videos with complex features. This limitation often arises from the inability of the text encoder to produce accurate embeddings, which hinders the video generation model. In this work, we propose a novel approach to overcome this challenge by selecting the optimal text embedding through interpolation in the embedding space. We demonstrate that this method enables the video generation model to produce the desired videos. Additionally, we introduce a simple algorithm using perpendicular foot embeddings and cosine similarity to identify the optimal interpolation embedding. Our findings highlight the importance of accurate text embeddings and offer a pathway for improving text-to-video generation performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10155</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10155</id><created>2025-01-17</created><updated>2025-02-03</updated><authors><author><keyname>Greatorex</keyname><forenames>Hugh</forenames></author><author><keyname>Mastella</keyname><forenames>Michele</forenames></author><author><keyname>Richter</keyname><forenames>Ole</forenames></author><author><keyname>Cotteret</keyname><forenames>Madison</forenames></author><author><keyname>Girão</keyname><forenames>Willian Soares</forenames></author><author><keyname>Janotte</keyname><forenames>Ella</forenames></author><author><keyname>Chicca</keyname><forenames>Elisabetta</forenames></author></authors><title>A scalable event-driven spatiotemporal feature extraction circuit</title><categories>eess.SP cs.AR cs.ET</categories><comments>4 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Event-driven sensors, which produce data only when there is a change in the input signal, are increasingly used in applications that require low-latency and low-power real-time sensing, such as robotics and edge devices. To fully achieve the latency and power advantages on offer however, similarly event-driven data processing methods are required. A promising solution is the TDE: an event-based processing element which encodes the time difference between events on different channels into an output event stream. In this work we introduce a novel TDE implementation on CMOS. The circuit is robust to device mismatch and allows the linear integration of input events. This is crucial for enabling a high-density implementation of many TDEs on the same die, and for realising real-time parallel processing of the high-event-rate data produced by event-driven sensors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10677</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10677</id><created>2025-01-18</created><updated>2025-01-31</updated><authors><author><keyname>Li</keyname><forenames>Xia</forenames></author><author><keyname>Zheng</keyname><forenames>Hanghang</forenames></author><author><keyname>Chen</keyname><forenames>Xiao</forenames></author><author><keyname>Liu</keyname><forenames>Hong</forenames></author><author><keyname>Mao</keyname><forenames>Mao</forenames></author></authors><title>Class-Imbalanced-Aware Adaptive Dataset Distillation for Scalable   Pretrained Model on Credit Scoring</title><categories>cs.LG cs.AI q-fin.RM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advent of artificial intelligence has significantly enhanced credit scoring technologies. Despite the remarkable efficacy of advanced deep learning models, mainstream adoption continues to favor tree-structured models due to their robust predictive performance on tabular data. Although pretrained models have seen considerable development, their application within the financial realm predominantly revolves around question-answering tasks and the use of such models for tabular-structured credit scoring datasets remains largely unexplored. Tabular-oriented large models, such as TabPFN, has made the application of large models in credit scoring feasible, albeit can only processing with limited sample sizes. This paper provides a novel framework to combine tabular-tailored dataset distillation technique with the pretrained model, empowers the scalability for TabPFN. Furthermore, though class imbalance distribution is the common nature in financial datasets, its influence during dataset distillation has not been explored. We thus integrate the imbalance-aware techniques during dataset distillation, resulting in improved performance in financial datasets (e.g., a 2.5% enhancement in AUC). This study presents a novel framework for scaling up the application of large pretrained models on financial tabular datasets and offers a comparative analysis of the influence of class imbalance on the dataset distillation process. We believe this approach can broaden the applications and downstream tasks of large models in the financial domain. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10688</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10688</id><created>2025-01-18</created><updated>2025-02-02</updated><authors><author><keyname>Li</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Liang</keyname><forenames>Yingyu</forenames></author><author><keyname>Long</keyname><forenames>Jiangxuan</forenames></author><author><keyname>Shi</keyname><forenames>Zhenmei</forenames></author><author><keyname>Song</keyname><forenames>Zhao</forenames></author><author><keyname>Zhuang</keyname><forenames>Zhen</forenames></author></authors><title>Neural Algorithmic Reasoning for Hypergraphs with Looped Transformers</title><categories>cs.LG cs.AI cs.CC cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Looped Transformers have shown exceptional neural algorithmic reasoning capability in simulating traditional graph algorithms, but their application to more complex structures like hypergraphs remains underexplored. Hypergraphs generalize graphs by modeling higher-order relationships among multiple entities, enabling richer representations but introducing significant computational challenges. In this work, we extend the Loop Transformer architecture's neural algorithmic reasoning capability to simulate hypergraph algorithms, addressing the gap between neural networks and combinatorial optimization over hypergraphs. Specifically, we propose a novel degradation mechanism for reducing hypergraphs to graph representations, enabling the simulation of graph-based algorithms, such as Dijkstra's shortest path. Furthermore, we introduce a hyperedge-aware encoding scheme to simulate hypergraph-specific algorithms, exemplified by Helly's algorithm. We establish theoretical guarantees for these simulations, demonstrating the feasibility of processing high-dimensional and combinatorial data using Loop Transformers. This work highlights the potential of Transformers as general-purpose algorithmic solvers for structured data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.11007</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.11007</id><created>2025-01-19</created><updated>2025-02-02</updated><authors><author><keyname>Dong</keyname><forenames>Pengcheng</forenames></author><author><keyname>Wan</keyname><forenames>Wenbo</forenames></author><author><keyname>Zhang</keyname><forenames>Huaxiang</forenames></author><author><keyname>Li</keyname><forenames>Shuai</forenames></author><author><keyname>Hou</keyname><forenames>Sujuan</forenames></author><author><keyname>Sun</keyname><forenames>Jiande</forenames></author></authors><title>HFGCN:Hypergraph Fusion Graph Convolutional Networks for Skeleton-Based   Action Recognition</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, action recognition has received much attention and wide application due to its important role in video understanding. Most of the researches on action recognition methods focused on improving the performance via various deep learning methods rather than the classification of skeleton points. The topological modeling between skeleton points and body parts was seldom considered. Although some studies have used a data-driven approach to classify the topology of the skeleton point, the nature of the skeleton point in terms of kinematics has not been taken into consideration. Therefore, in this paper, we draw on the theory of kinematics to adapt the topological relations of the skeleton point and propose a topological relation classification based on body parts and distance from core of body. To synthesize these topological relations for action recognition, we propose a novel Hypergraph Fusion Graph Convolutional Network (HFGCN). In particular, the proposed model is able to focus on the human skeleton points and the different body parts simultaneously, and thus construct the topology, which improves the recognition accuracy obviously. We use a hypergraph to represent the categorical relationships of these skeleton points and incorporate the hypergraph into a graph convolution network to model the higher-order relationships among the skeleton points and enhance the feature representation of the network. In addition, our proposed hypergraph attention module and hypergraph graph convolution module optimize topology modeling in temporal and channel dimensions, respectively, to further enhance the feature representation of the network. We conducted extensive experiments on three widely used datasets.The results validate that our proposed method can achieve the best performance when compared with the state-of-the-art skeleton-based methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.11111</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.11111</id><created>2025-01-19</created><updated>2025-02-01</updated><authors><author><keyname>Kulmer</keyname><forenames>Dominik</forenames></author><author><keyname>Leitenstern</keyname><forenames>Maximilian</forenames></author><author><keyname>Weinmann</keyname><forenames>Marcel</forenames></author><author><keyname>Lienkamp</keyname><forenames>Markus</forenames></author></authors><title>OpenLiDARMap: Zero-Drift Point Cloud Mapping using Map Priors</title><categories>cs.RO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Accurate localization is a critical component of mobile autonomous systems, especially in Global Navigation Satellite Systems (GNSS)-denied environments where traditional methods fail. In such scenarios, environmental sensing is essential for reliable operation. However, approaches such as LiDAR odometry and Simultaneous Localization and Mapping (SLAM) suffer from drift over long distances, especially in the absence of loop closures. Map-based localization offers a robust alternative, but the challenge lies in creating and georeferencing maps without GNSS support. To address this issue, we propose a method for creating georeferenced maps without GNSS by using publicly available data, such as building footprints and surface models derived from sparse aerial scans. Our approach integrates these data with onboard LiDAR scans to produce dense, accurate, georeferenced 3D point cloud maps. By combining an Iterative Closest Point (ICP) scan-to-scan and scan-to-map matching strategy, we achieve high local consistency without suffering from long-term drift. Thus, we eliminate the reliance on GNSS for the creation of georeferenced maps. The results demonstrate that LiDAR-only mapping can produce accurate georeferenced point cloud maps when augmented with existing map priors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.11133</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.11133</id><created>2025-01-19</created><updated>2025-01-31</updated><authors><author><keyname>Li</keyname><forenames>Xinyang</forenames></author><author><keyname>Chen</keyname><forenames>Yiqi</forenames></author><author><keyname>Andrei</keyname><forenames>Vlad C.</forenames></author><author><keyname>Djuhera</keyname><forenames>Aladin</forenames></author><author><keyname>Mönich</keyname><forenames>Ullrich J.</forenames></author><author><keyname>Boche</keyname><forenames>Holger</forenames></author></authors><title>A Simultaneous Decoding Approach to Joint State and Message   Communications</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity-distortion (C-D) trade-off for joint state and message communications (JSMC) over state-dependent point-to-point, degraded broadcast, and multiple access channels are investigated, where the transmitters have access to noisy state information and feedback, while the receivers jointly decode the messages and estimate the channel state. A coding scheme is proposed based on backward simultaneous decoding of messages and compressed state descriptions without the need for the Wyner-Ziv random binning technique. For the point-to-point channel, the proposed scheme results in the optimal C-D function. For state-dependent discrete memoryless degraded broadcast channel (SD-DMDBC), the successive refinement method is adopted for designing state descriptions. With the simultaneous decoding approach, the derived achievable region is shown to be larger than the region obtained by the sequential decoding approach that is utilized in existing works. As for the state-dependent discrete memoryless multiple access channel (SD-DMMAC), in addition to the proposed scheme, Willem's coding strategy is applied to enable partial collaboration between transmitters through the feedback links. Moreover, the state descriptions are shown to enhance both communication and state estimation performance. Examples are provided for the derived results to verify the analysis, either numerically or analytically. With particular focus, simple but representative integrated sensing and communications (ISAC) systems are also considered, and their fundamental performance limits are studied. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.11393</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.11393</id><created>2025-01-20</created><updated>2025-02-01</updated><authors><author><keyname>Rathore</keyname><forenames>Shiv Pratap Singh</forenames></author><author><keyname>Kashyap</keyname><forenames>Navin</forenames></author></authors><title>Trace Reconstruction of First-Order Reed-Muller Codewords Using Run   Statistics</title><categories>cs.IT math.IT math.PR</categories><comments>8 pages, no figures. Extended version of manuscript submitted to ISIT   2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive an expression for the expected number of runs in a trace of a binary sequence $x \in \{0,1\}^n$ obtained by passing $x$ through a deletion channel that independently deletes each bit with probability $q$. We use this expression to show that if $x$ is a codeword of a first-order Reed-Muller code, and the deletion probability $q$ is 1/2, then $x$ can be reconstructed, with high probability, from $\tilde{O}(n)$ many of its traces. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.11421</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.11421</id><created>2025-01-20</created><updated>2025-02-02</updated><authors><author><keyname>Chandran</keyname><forenames>G Dhinesh</forenames></author><author><keyname>Kota</keyname><forenames>Srinivas Reddy</forenames></author><author><keyname>Bhashyam</keyname><forenames>Srikrishna</forenames></author></authors><title>Online Clustering with Bandit Information</title><categories>cs.LG cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of online clustering within the multi-armed bandit framework under the fixed confidence setting. In this multi-armed bandit problem, we have $M$ arms, each providing i.i.d. samples that follow a multivariate Gaussian distribution with an {\em unknown} mean and a known unit covariance. The arms are grouped into $K$ clusters based on the distance between their means using the Single Linkage (SLINK) clustering algorithm on the means of the arms. Since the true means are unknown, the objective is to obtain the above clustering of the arms with the minimum number of samples drawn from the arms, subject to an upper bound on the error probability. We introduce a novel algorithm, Average Tracking Bandit Online Clustering (ATBOC), and prove that this algorithm is order optimal, meaning that the upper bound on its expected sample complexity for given error probability $\delta$ is within a factor of 2 of an instance-dependent lower bound as $\delta \rightarrow 0$. Furthermore, we propose a computationally more efficient algorithm, Lower and Upper Confidence Bound-based Bandit Online Clustering (LUCBBOC), inspired by the LUCB algorithm for best arm identification. Simulation results demonstrate that the performance of LUCBBOC is comparable to that of ATBOC. We numerically assess the effectiveness of the proposed algorithms through numerical experiments on both synthetic datasets and the real-world MovieLens dataset. To the best of our knowledge, this is the first work on bandit online clustering that allows arms with different means in a cluster and $K$ greater than 2. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.11849</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.11849</id><created>2025-01-20</created><updated>2025-01-31</updated><authors><author><keyname>Kanakaris</keyname><forenames>Nikos</forenames></author><author><keyname>Ping</keyname><forenames>Heng</forenames></author><author><keyname>Xiao</keyname><forenames>Xiongye</forenames></author><author><keyname>Ahmed</keyname><forenames>Nesreen K.</forenames></author><author><keyname>Luceri</keyname><forenames>Luca</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Bogdan</keyname><forenames>Paul</forenames></author></authors><title>Network-informed Prompt Engineering against Organized Astroturf   Campaigns under Extreme Class Imbalance</title><categories>cs.CL cs.AI cs.SI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Detecting organized political campaigns is of paramount importance in fighting against disinformation on social media. Existing approaches for the identification of such organized actions employ techniques mostly from network science, graph machine learning and natural language processing. Their ultimate goal is to analyze the relationships and interactions (e.g. re-posting) among users and the textual similarities of their posts. Despite their effectiveness in recognizing astroturf campaigns, these methods face significant challenges, notably the class imbalance in available training datasets. To mitigate this issue, recent methods usually resort to data augmentation or increasing the number of positive samples, which may not always be feasible or sufficient in real-world settings. Following a different path, in this paper, we propose a novel framework for identifying astroturf campaigns based solely on large language models (LLMs), introducing a Balanced Retrieval-Augmented Generation (Balanced RAG) component. Our approach first gives both textual information concerning the posts (in our case tweets) and the user interactions of the social network as input to a language model. Then, through prompt engineering and the proposed Balanced RAG method, it effectively detects coordinated disinformation campaigns on X (Twitter). The proposed framework does not require any training or fine-tuning of the language model. Instead, by strategically harnessing the strengths of prompt engineering and Balanced RAG, it facilitates LLMs to overcome the effects of class imbalance and effectively identify coordinated political campaigns. The experimental results demonstrate that by incorporating the proposed prompt engineering and Balanced RAG methods, our framework outperforms the traditional graph-based baselines, achieving 2x-3x improvements in terms of precision, recall and F1 scores. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.12151</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.12151</id><created>2025-01-21</created><updated>2025-02-02</updated><authors><author><keyname>Ali</keyname><forenames>Mazen</forenames></author><author><keyname>Cortines</keyname><forenames>Aser</forenames></author><author><keyname>Morales</keyname><forenames>Siddhartha</forenames></author><author><keyname>Mugel</keyname><forenames>Samuel</forenames></author><author><keyname>Olave</keyname><forenames>Mireia</forenames></author><author><keyname>Orus</keyname><forenames>Roman</forenames></author><author><keyname>Palmer</keyname><forenames>Samuel</forenames></author><author><keyname>Usabiaga</keyname><forenames>Hodei</forenames></author></authors><title>Quantum-Inspired Solver for Simulating Material Deformations</title><categories>quant-ph cs.NA math.NA</categories><comments>12 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the application of tensor networks (TNs) to the simulation of material deformations within the framework of linear elasticity. Material simulations are essential computational tools extensively used in both academic research and industrial applications. TNs, originally developed in quantum mechanics, have recently shown promise in solving partial differential equations (PDEs) due to their potential for exponential speedups over classical algorithms. Our study successfully employs TNs to solve linear elasticity equations with billions of degrees of freedom, achieving exponential reductions in both memory usage and computational time. These results demonstrate the practical viability of TNs as a powerful classical backend for executing quantum-inspired algorithms with significant efficiency gains. This work is based on our research conducted with IKERLAN. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.12271</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.12271</id><created>2025-01-21</created><updated>2025-02-02</updated><authors><author><keyname>Høst-Madsen</keyname><forenames>Anders</forenames></author></authors><title>Faithful Simulation of Distributed Quantum Measurement with Coding for   Computing</title><categories>cs.IT math.IT</categories><comments>This is a version of an ISIT'25 submission with a complete proof</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This papers consider a two terminal problem, where Alice and Bob jointly want to perform a measurement on a bipartite quantum system $\rho^{AB}$. Alice can transmit the results of her measurements to Bob on a classical channel, and Alice and Bob have common randomness. The question is what is the minimum amount of communications and common randomness needed for faithful simulation. The paper derives an achievable rate region. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.12372</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.12372</id><created>2025-01-21</created><updated>2025-01-31</updated><authors><author><keyname>Chung</keyname><forenames>Yeounoh</forenames></author><author><keyname>Kakkar</keyname><forenames>Gaurav T.</forenames></author><author><keyname>Gan</keyname><forenames>Yu</forenames></author><author><keyname>Milne</keyname><forenames>Brenton</forenames></author><author><keyname>Ozcan</keyname><forenames>Fatma</forenames></author></authors><title>Is Long Context All You Need? Leveraging LLM's Extended Context for   NL2SQL</title><categories>cs.DB cs.AI</categories><comments>14 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Large Language Models (LLMs) have demonstrated impressive capabilities across a range of natural language processing tasks. In particular, improvements in reasoning abilities and the expansion of context windows have opened new avenues for leveraging these powerful models. NL2SQL is challenging in that the natural language question is inherently ambiguous, while the SQL generation requires a precise understanding of complex data schema and semantics. One approach to this semantic ambiguous problem is to provide more and sufficient contextual information.   In this work, we explore the performance and the latency trade-offs of the extended context window (a.k.a., long context) offered by Google's state-of-the-art LLM (\textit{gemini-1.5-pro}). We study the impact of various contextual information, including column example values, question and SQL query pairs, user-provided hints, SQL documentation, and schema. To the best of our knowledge, this is the first work to study how the extended context window and extra contextual information can help NL2SQL generation with respect to both accuracy and latency cost. We show that long context LLMs are robust and do not get lost in the extended contextual information. Additionally, our long-context NL2SQL pipeline based on Google's \textit{gemini-pro-1.5} achieve strong performances on various benchmark datasets without finetuning and expensive self-consistency based techniques. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.12447</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.12447</id><created>2025-01-21</created><updated>2025-02-02</updated><authors><author><keyname>Regula</keyname><forenames>Bartosz</forenames></author><author><keyname>Lami</keyname><forenames>Ludovico</forenames></author><author><keyname>Datta</keyname><forenames>Nilanjana</forenames></author></authors><title>Tight relations and equivalences between smooth relative entropies</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>24+7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The precise one-shot characterisation of operational tasks in classical and quantum information theory relies on different forms of smooth entropic quantities. A particularly important connection is between the hypothesis testing relative entropy and the smoothed max-relative entropy, which together govern many operational settings. We first strengthen this connection into a type of equivalence: we show that the hypothesis testing relative entropy is equivalent to a variant of the smooth max-relative entropy based on the information spectrum divergence, which can be alternatively understood as a measured smooth max-relative entropy. Furthermore, we improve a fundamental lemma due to Datta and Renner that connects the different variants of the smoothed max-relative entropy, introducing a modified proof technique based on matrix geometric means and a tightened gentle measurement lemma. We use the unveiled connections and tools to strictly improve on previously known one-shot bounds and duality relations between the smooth max-relative entropy and the hypothesis testing relative entropy, sharpening also bounds that connect the max-relative entropy with R\'enyi divergences. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.12637</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.12637</id><created>2025-01-21</created><updated>2025-02-02</updated><authors><author><keyname>Nguyen</keyname><forenames>Hung</forenames></author><author><keyname>Li</keyname><forenames>Blark Runfa</forenames></author><author><keyname>Nguyen</keyname><forenames>Truong</forenames></author></authors><title>DWTNeRF: Boosting Few-shot Neural Radiance Fields via Discrete Wavelet   Transform</title><categories>cs.CV</categories><comments>17 pages, 13 figures, 8 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Neural Radiance Fields (NeRF) has achieved superior performance in novel view synthesis and 3D scene representation, but its practical applications are hindered by slow convergence and reliance on dense training views. To this end, we present DWTNeRF, a unified framework based on Instant-NGP's fast-training hash encoding. It is coupled with regularization terms designed for few-shot NeRF, which operates on sparse training views. Our DWTNeRF additionally includes a novel Discrete Wavelet loss that allows explicit prioritization of low frequencies directly in the training objective, reducing few-shot NeRF's overfitting on high frequencies in earlier training stages. We also introduce a model-based approach, based on multi-head attention, that is compatible with INGP, which are sensitive to architectural changes. On the 3-shot LLFF benchmark, DWTNeRF outperforms Vanilla INGP by 15.07% in PSNR, 24.45% in SSIM and 36.30% in LPIPS. Our approach encourages a re-thinking of current few-shot approaches for fast-converging implicit representations like INGP or 3DGS. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.13025</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.13025</id><created>2025-01-22</created><updated>2025-02-01</updated><authors><author><keyname>Nikbakht</keyname><forenames>Homa</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>A MIMO ISAC System for Ultra-Reliable and Low-Latency Communications</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we propose a bi-static multiple-input multiple-output (MIMO) integrated sensing and communication (ISAC) system to detect the arrival of ultra-reliable and low-latency communication (URLLC) messages and prioritize their delivery. In this system, a dual-function base station (BS) communicates with a user equipment (UE) and a sensing receiver (SR) is deployed to collect echo signals reflected from a target of interest. The BS regularly transmits messages of enhanced mobile broadband (eMBB) services to the UE. During each eMBB transmission, if the SR senses the presence of a target of interest, it immediately triggers the transmission of an additional URLLC message. To reinforce URLLC transmissions, we propose a dirty-paper coding (DPC)-based technique that mitigates the interference of both eMBB and sensing signals. For this system, we formulate the rate-reliability-detection trade-off in the finite blocklength regime by evaluating the communication rate of the eMBB transmissions, the reliability of the URLLC transmissions and the probability of the target detection. Our numerical analysis show that our proposed DPC-based ISAC scheme significantly outperforms power-sharing based ISAC and traditional time-sharing schemes. In particular, it achieves higher eMBB transmission rate while satisfying both URLLC and sensing constraints. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.13291</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.13291</id><created>2025-01-22</created><updated>2025-02-02</updated><authors><author><keyname>Das</keyname><forenames>Satyaki</forenames></author><author><keyname>Fabiha</keyname><forenames>Syeda Tasnim</forenames></author><author><keyname>Shafiq</keyname><forenames>Saad</forenames></author><author><keyname>Medvidovic</keyname><forenames>Nenad</forenames></author></authors><title>Are We Learning the Right Features? A Framework for Evaluating DL-Based   Software Vulnerability Detection Solutions</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent research has revealed that the reported results of an emerging body of DL-based techniques for detecting software vulnerabilities are not reproducible, either across different datasets or on unseen samples. This paper aims to provide the foundation for properly evaluating the research in this domain. We do so by analyzing prior work and existing vulnerability datasets for the syntactic and semantic features of code that contribute to vulnerability, as well as features that falsely correlate with vulnerability. We provide a novel, uniform representation to capture both sets of features, and use this representation to detect the presence of both vulnerability and spurious features in code. To this end, we design two types of code perturbations: feature preserving perturbations (FPP) ensure that the vulnerability feature remains in a given code sample, while feature eliminating perturbations (FEP) eliminate the feature from the code sample. These perturbations aim to measure the influence of spurious and vulnerability features on the predictions of a given vulnerability detection solution. To evaluate how the two classes of perturbations influence predictions, we conducted a large-scale empirical study on five state-of-the-art DL-based vulnerability detectors. Our study shows that, for vulnerability features, only ~2% of FPPs yield the undesirable effect of a prediction changing among the five detectors on average. However, on average, ~84% of FEPs yield the undesirable effect of retaining the vulnerability predictions. For spurious features, we observed that FPPs yielded a drop in recall up to 29% for graph-based detectors. We present the reasons underlying these results and suggest strategies for improving DNN-based vulnerability detectors. We provide our perturbation-based evaluation framework as a public resource to enable independent future evaluation of vulnerability detectors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.13351</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.13351</id><created>2025-01-22</created><updated>2025-01-31</updated><authors><author><keyname>Shi</keyname><forenames>Zewei</forenames></author><author><keyname>Sun</keyname><forenames>Ruoxi</forenames></author><author><keyname>Chen</keyname><forenames>Jieshan</forenames></author><author><keyname>Sun</keyname><forenames>Jiamou</forenames></author><author><keyname>Xue</keyname><forenames>Minhui</forenames></author><author><keyname>Gao</keyname><forenames>Yansong</forenames></author><author><keyname>Liu</keyname><forenames>Feng</forenames></author><author><keyname>Yuan</keyname><forenames>Xingliang</forenames></author></authors><title>50 Shades of Deceptive Patterns: A Unified Taxonomy, Multimodal   Detection, and Security Implications</title><categories>cs.CR</categories><comments>This paper has been accepted by The Web Conference 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deceptive patterns (DPs) are user interface designs deliberately crafted to manipulate users into unintended decisions, often by exploiting cognitive biases for the benefit of companies or services. While numerous studies have explored ways to identify these deceptive patterns, many existing solutions require significant human intervention and struggle to keep pace with the evolving nature of deceptive designs. To address these challenges, we expanded the deceptive pattern taxonomy from security and privacy perspectives, refining its categories and scope. We created a comprehensive dataset of deceptive patterns by integrating existing small-scale datasets with new samples, resulting in 6,725 images and 10,421 DP instances from mobile apps and websites. We then developed DPGuard, a novel automatic tool leveraging commercial multimodal large language models (MLLMs) for deceptive pattern detection. Experimental results show that DPGuard outperforms state-of-the-art methods. Finally, we conducted an extensive empirical evaluation on 2,000 popular mobile apps and websites, revealing that 23.61% of mobile screenshots and 47.27% of website screenshots feature at least one deceptive pattern instance. Through four unexplored case studies that inform security implications, we highlight the critical importance of the unified taxonomy in addressing the growing challenges of Internet deception. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.13377</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.13377</id><created>2025-01-22</created><updated>2025-01-31</updated><authors><author><keyname>Eisermann</keyname><forenames>Thomas</forenames></author><author><keyname>Campajola</keyname><forenames>Carlo</forenames></author><author><keyname>Tessone</keyname><forenames>Claudio J.</forenames></author><author><keyname>Teixeira</keyname><forenames>Andreia Sofia</forenames></author></authors><title>Concentration in Governance Control Across Decentralised Finance   Protocols</title><categories>cs.CE</categories><acm-class>J.4; G.2.2; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Blockchain-based systems are frequently governed through tokens that grant their holders voting rights over core protocol functions and funds. The centralisation occurring in Decentralised Finance (DeFi) protocols' token-based voting systems is typically analysed by examining token holdings' distribution across addresses. In this paper, we expand this perspective by exploring shared token holdings of addresses across multiple DeFi protocols. We construct a Statistically Validated Network (SVN) based on shared governance token holdings among addresses. Using the links within the SVN, we identify influential addresses that shape these connections and we conduct a post-hoc analysis to examine their characteristics and behaviour. Our findings reveal persistent influential links over time, predominantly involving addresses associated with institutional investors who maintain significant token supplies across the sampled protocols. Finally, we observe that token holding patterns and concentrations tend to shift in response to speculative market cycles. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.13416</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.13416</id><created>2025-01-23</created><updated>2025-02-02</updated><authors><author><keyname>Tang</keyname><forenames>Yiming</forenames></author><author><keyname>Anwar</keyname><forenames>Abrar</forenames></author><author><keyname>Thomason</keyname><forenames>Jesse</forenames></author></authors><title>M3PT: A Transformer for Multimodal, Multi-Party Social Signal Prediction   with Person-aware Blockwise Attention</title><categories>cs.LG cs.AI cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding social signals in multi-party conversations is important for human-robot interaction and artificial social intelligence. Social signals include body pose, head pose, speech, and context-specific activities like acquiring and taking bites of food when dining. Past work in multi-party interaction tends to build task-specific models for predicting social signals. In this work, we address the challenge of predicting multimodal social signals in multi-party settings in a single model. We introduce M3PT, a causal transformer architecture with modality and temporal blockwise attention masking to simultaneously process multiple social cues across multiple participants and their temporal interactions. We train and evaluate M3PT on the Human-Human Commensality Dataset (HHCD), and demonstrate that using multiple modalities improves bite timing and speaking status prediction. Source code: https://github.com/AbrarAnwar/masked-social-signals/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.13443</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.13443</id><created>2025-01-23</created><updated>2025-02-01</updated><authors><author><keyname>Hu</keyname><forenames>Yongquan</forenames></author><author><keyname>Tang</keyname><forenames>Jingyu</forenames></author><author><keyname>Gong</keyname><forenames>Xinya</forenames></author><author><keyname>Zhou</keyname><forenames>Zhongyi</forenames></author><author><keyname>Zhang</keyname><forenames>Shuning</forenames></author><author><keyname>Elvitigala</keyname><forenames>Don Samitha</forenames></author><author><keyname>Mueller</keyname><forenames>Florian 'Floyd'</forenames></author><author><keyname>Hu</keyname><forenames>Wen</forenames></author><author><keyname>Quigley</keyname><forenames>Aaron J.</forenames></author></authors><title>Vision-Based Multimodal Interfaces: A Survey and Taxonomy for Enhanced   Context-Aware System Design</title><categories>cs.HC</categories><comments>31 pages including appendices</comments><msc-class>68U35, 68T07</msc-class><acm-class>H.5.2; H.5.3</acm-class><doi>10.1145/3706598.3714161</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent surge in artificial intelligence, particularly in multimodal processing technology, has advanced human-computer interaction, by altering how intelligent systems perceive, understand, and respond to contextual information (i.e., context awareness). Despite such advancements, there is a significant gap in comprehensive reviews examining these advances, especially from a multimodal data perspective, which is crucial for refining system design. This paper addresses a key aspect of this gap by conducting a systematic survey of data modality-driven Vision-based Multimodal Interfaces (VMIs). VMIs are essential for integrating multimodal data, enabling more precise interpretation of user intentions and complex interactions across physical and digital environments. Unlike previous task- or scenario-driven surveys, this study highlights the critical role of the visual modality in processing contextual information and facilitating multimodal interaction. Adopting a design framework moving from the whole to the details and back, it classifies VMIs across dimensions, providing insights for developing effective, context-aware systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.13720</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.13720</id><created>2025-01-23</created><updated>2025-02-03</updated><authors><author><keyname>Kruspe</keyname><forenames>Anna</forenames></author></authors><title>Musical ethnocentrism in Large Language Models</title><categories>cs.CL cs.AI cs.SD eess.AS</categories><journal-ref>Proceedings of the 3rd Workshop on NLP for Music and Audio   (NLP4MusA) 2024</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) reflect the biases in their training data and, by extension, those of the people who created this training data. Detecting, analyzing, and mitigating such biases is becoming a focus of research. One type of bias that has been understudied so far are geocultural biases. Those can be caused by an imbalance in the representation of different geographic regions and cultures in the training data, but also by value judgments contained therein. In this paper, we make a first step towards analyzing musical biases in LLMs, particularly ChatGPT and Mixtral. We conduct two experiments. In the first, we prompt LLMs to provide lists of the "Top 100" musical contributors of various categories and analyze their countries of origin. In the second experiment, we ask the LLMs to numerically rate various aspects of the musical cultures of different countries. Our results indicate a strong preference of the LLMs for Western music cultures in both experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.13751</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.13751</id><created>2025-01-23</created><updated>2025-02-01</updated><authors><author><keyname>Li</keyname><forenames>Han</forenames></author><author><keyname>Li</keyname><forenames>Shaohui</forenames></author><author><keyname>Dai</keyname><forenames>Wenrui</forenames></author><author><keyname>Cao</keyname><forenames>Maida</forenames></author><author><keyname>Kan</keyname><forenames>Nuowen</forenames></author><author><keyname>Li</keyname><forenames>Chenglin</forenames></author><author><keyname>Zou</keyname><forenames>Junni</forenames></author><author><keyname>Xiong</keyname><forenames>Hongkai</forenames></author></authors><title>On Disentangled Training for Nonlinear Transform in Learned Image   Compression</title><categories>eess.IV cs.CV</categories><comments>Accepted by ICLR2025</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Learned image compression (LIC) has demonstrated superior rate-distortion (R-D) performance compared to traditional codecs, but is challenged by training inefficiency that could incur more than two weeks to train a state-of-the-art model from scratch. Existing LIC methods overlook the slow convergence caused by compacting energy in learning nonlinear transforms. In this paper, we first reveal that such energy compaction consists of two components, i.e., feature decorrelation and uneven energy modulation. On such basis, we propose a linear auxiliary transform (AuxT) to disentangle energy compaction in training nonlinear transforms. The proposed AuxT obtains coarse approximation to achieve efficient energy compaction such that distribution fitting with the nonlinear transforms can be simplified to fine details. We then develop wavelet-based linear shortcuts (WLSs) for AuxT that leverages wavelet-based downsampling and orthogonal linear projection for feature decorrelation and subband-aware scaling for uneven energy modulation. AuxT is lightweight and plug-and-play to be integrated into diverse LIC models to address the slow convergence issue. Experimental results demonstrate that the proposed approach can accelerate training of LIC models by 2 times and simultaneously achieves an average 1\% BD-rate reduction. To our best knowledge, this is one of the first successful attempt that can significantly improve the convergence of LIC with comparable or superior rate-distortion performance. Code will be released at \url{https://github.com/qingshi9974/AuxT} </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.13964</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.13964</id><created>2025-01-21</created><updated>2025-02-01</updated><authors><author><keyname>Duan</keyname><forenames>Lin</forenames></author><author><keyname>Xiu</keyname><forenames>Yanming</forenames></author><author><keyname>Gorlatova</keyname><forenames>Maria</forenames></author></authors><title>Advancing the Understanding and Evaluation of AR-Generated Scenes: When   Vision-Language Models Shine and Stumble</title><categories>cs.CV cs.AI cs.HC</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Augmented Reality (AR) enhances the real world by integrating virtual content, yet ensuring the quality, usability, and safety of AR experiences presents significant challenges. Could Vision-Language Models (VLMs) offer a solution for the automated evaluation of AR-generated scenes? Could Vision-Language Models (VLMs) offer a solution for the automated evaluation of AR-generated scenes? In this study, we evaluate the capabilities of three state-of-the-art commercial VLMs -- GPT, Gemini, and Claude -- in identifying and describing AR scenes. For this purpose, we use DiverseAR, the first AR dataset specifically designed to assess VLMs' ability to analyze virtual content across a wide range of AR scene complexities. Our findings demonstrate that VLMs are generally capable of perceiving and describing AR scenes, achieving a True Positive Rate (TPR) of up to 93% for perception and 71% for description. While they excel at identifying obvious virtual objects, such as a glowing apple, they struggle when faced with seamlessly integrated content, such as a virtual pot with realistic shadows. Our results highlight both the strengths and the limitations of VLMs in understanding AR scenarios. We identify key factors affecting VLM performance, including virtual content placement, rendering quality, and physical plausibility. This study underscores the potential of VLMs as tools for evaluating the quality of AR experiences. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14158</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14158</id><created>2025-01-23</created><updated>2025-02-01</updated><authors><author><keyname>Safari</keyname><forenames>Mojtaba</forenames></author><author><keyname>Eidex</keyname><forenames>Zach</forenames></author><author><keyname>Chang</keyname><forenames>Chih-Wei</forenames></author><author><keyname>Qiu</keyname><forenames>Richard L. J.</forenames></author><author><keyname>Yang</keyname><forenames>Xiaofeng</forenames></author></authors><title>Advancing MRI Reconstruction: A Systematic Review of Deep Learning and   Compressed Sensing Integration</title><categories>cs.CV cs.AI physics.med-ph</categories><comments>arXiv admin note: substantial text overlap with arXiv:2405.00241</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Magnetic resonance imaging (MRI) is a non-invasive imaging modality and provides comprehensive anatomical and functional insights into the human body. However, its long acquisition times can lead to patient discomfort, motion artifacts, and limiting real-time applications. To address these challenges, strategies such as parallel imaging have been applied, which utilize multiple receiver coils to speed up the data acquisition process. Additionally, compressed sensing (CS) is a method that facilitates image reconstruction from sparse data, significantly reducing image acquisition time by minimizing the amount of data collection needed. Recently, deep learning (DL) has emerged as a powerful tool for improving MRI reconstruction. It has been integrated with parallel imaging and CS principles to achieve faster and more accurate MRI reconstructions. This review comprehensively examines DL-based techniques for MRI reconstruction. We categorize and discuss various DL-based methods, including end-to-end approaches, unrolled optimization, and federated learning, highlighting their potential benefits. Our systematic review highlights significant contributions and underscores the potential of DL in MRI reconstruction. Additionally, we summarize key results and trends in DL-based MRI reconstruction, including quantitative metrics, the dataset, acceleration factors, and the progress of and research interest in DL techniques over time. Finally, we discuss potential future directions and the importance of DL-based MRI reconstruction in advancing medical imaging. To facilitate further research in this area, we provide a GitHub repository that includes up-to-date DL-based MRI reconstruction publications and public datasets-https://github.com/mosaf/Awesome-DL-based-CS-MRI. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14172</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14172</id><created>2025-01-23</created><updated>2025-01-31</updated><authors><author><keyname>Nettur</keyname><forenames>Suresh Babu</forenames></author><author><keyname>Karpurapu</keyname><forenames>Shanthi</forenames></author><author><keyname>Nettur</keyname><forenames>Unnati</forenames></author><author><keyname>Gajja</keyname><forenames>Likhit Sagar</forenames></author><author><keyname>Myneni</keyname><forenames>Sravanthy</forenames></author><author><keyname>Dusi</keyname><forenames>Akhil</forenames></author><author><keyname>Posham</keyname><forenames>Lalithya</forenames></author></authors><title>UltraLightSqueezeNet: A Deep Learning Architecture for Malaria   Classification with up to 54x fewer trainable parameters for resource   constrained devices</title><categories>cs.LG cs.AI cs.CV</categories><comments>Corresponding authors: Shanthi Karpurapu   (shanthi.karpurapu@gmail.com), Suresh Babu Nettur (nettursuresh@gmail.com)   Shanthi Karpurapu and Suresh Babu Nettur are co-first authors</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Lightweight deep learning approaches for malaria detection have gained attention for their potential to enhance diagnostics in resource constrained environments. For our study, we selected SqueezeNet1.1 as it is one of the most popular lightweight architectures. SqueezeNet1.1 is a later version of SqueezeNet1.0 and is 2.4 times more computationally efficient than the original model. We proposed and implemented three ultra-lightweight architecture variants to SqueezeNet1.1 architecture, namely Variant 1 (one fire module), Variant 2 (two fire modules), and Variant 3 (four fire modules), which are even more compact than SqueezeNetV1.1 (eight fire modules). These models were implemented to evaluate the best performing variant that achieves superior computational efficiency without sacrificing accuracy in malaria blood cell classification. The models were trained and evaluated using the NIH Malaria dataset. We assessed each model's performance based on metrics including accuracy, recall, precision, F1-score, and Area Under the Curve (AUC). The results show that the SqueezeNet1.1 model achieves the highest performance across all metrics, with a classification accuracy of 97.12%. Variant 3 (four fire modules) offers a competitive alternative, delivering almost identical results (accuracy 96.55%) with a 6x reduction in computational overhead compared to SqueezeNet1.1. Variant 2 and Variant 1 perform slightly lower than Variant 3, with Variant 2 (two fire modules) reducing computational overhead by 28x, and Variant 1 (one fire module) achieving a 54x reduction in trainable parameters compared to SqueezeNet1.1. These findings demonstrate that our SqueezeNet1.1 architecture variants provide a flexible approach to malaria detection, enabling the selection of a variant that balances resource constraints and performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14183</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14183</id><created>2025-01-23</created><updated>2025-02-03</updated><authors><author><keyname>Kang</keyname><forenames>Junhyeok</forenames></author><author><keyname>Shin</keyname><forenames>Yooju</forenames></author><author><keyname>Lee</keyname><forenames>Jae-Gil</forenames></author></authors><title>VarDrop: Enhancing Training Efficiency by Reducing Variate Redundancy in   Periodic Time Series Forecasting</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variate tokenization, which independently embeds each variate as separate tokens, has achieved remarkable improvements in multivariate time series forecasting. However, employing self-attention with variate tokens incurs a quadratic computational cost with respect to the number of variates, thus limiting its training efficiency for large-scale applications. To address this issue, we propose VarDrop, a simple yet efficient strategy that reduces the token usage by omitting redundant variate tokens during training. VarDrop adaptively excludes redundant tokens within a given batch, thereby reducing the number of tokens used for dot-product attention while preserving essential information. Specifically, we introduce k-dominant frequency hashing (k-DFH), which utilizes the ranked dominant frequencies in the frequency domain as a hash value to efficiently group variate tokens exhibiting similar periodic behaviors. Then, only representative tokens in each group are sampled through stratified sampling. By performing sparse attention with these selected tokens, the computational cost of scaled dot-product attention is significantly alleviated. Experiments conducted on public benchmark datasets demonstrate that VarDrop outperforms existing efficient baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14238</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14238</id><created>2025-01-23</created><updated>2025-02-01</updated><authors><author><keyname>Mohammadi</keyname><forenames>Marzieh</forenames></author><author><keyname>Salarpour</keyname><forenames>Amir</forenames></author><author><keyname>MohajerAnsari</keyname><forenames>Pedram</forenames></author></authors><title>Point-LN: A Lightweight Framework for Efficient Point Cloud   Classification Using Non-Parametric Positional Encoding</title><categories>cs.CV cs.AI cs.LG cs.RO</categories><comments>This paper has been accepted for presentation at the 29th   International Computer Conference, Computer Society of Iran (CSICC) 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce Point-LN, a novel lightweight framework engineered for efficient 3D point cloud classification. Point-LN integrates essential non-parametric components-such as Farthest Point Sampling (FPS), k-Nearest Neighbors (k-NN), and non-learnable positional encoding-with a streamlined learnable classifier that significantly enhances classification accuracy while maintaining a minimal parameter footprint. This hybrid architecture ensures low computational costs and rapid inference speeds, making Point-LN ideal for real-time and resource-constrained applications. Comprehensive evaluations on benchmark datasets, including ModelNet40 and ScanObjectNN, demonstrate that Point-LN achieves competitive performance compared to state-of-the-art methods, all while offering exceptional efficiency. These results establish Point-LN as a robust and scalable solution for diverse point cloud classification tasks, highlighting its potential for widespread adoption in various computer vision applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14317</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14317</id><created>2025-01-24</created><updated>2025-02-01</updated><authors><author><keyname>Wang</keyname><forenames>Yuxuan</forenames></author><author><keyname>Yi</keyname><forenames>Xuanyu</forenames></author><author><keyname>Weng</keyname><forenames>Haohan</forenames></author><author><keyname>Xu</keyname><forenames>Qingshan</forenames></author><author><keyname>Wei</keyname><forenames>Xiaokang</forenames></author><author><keyname>Yang</keyname><forenames>Xianghui</forenames></author><author><keyname>Guo</keyname><forenames>Chunchao</forenames></author><author><keyname>Chen</keyname><forenames>Long</forenames></author><author><keyname>Zhang</keyname><forenames>Hanwang</forenames></author></authors><title>Nautilus: Locality-aware Autoencoder for Scalable Mesh Generation</title><categories>cs.CV</categories><comments>Project Page: https://nautilusmeshgen.github.io, Tencent Hunyuan, 14   pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Triangle meshes are fundamental to 3D applications, enabling efficient modification and rasterization while maintaining compatibility with standard rendering pipelines. However, current automatic mesh generation methods typically rely on intermediate representations that lack the continuous surface quality inherent to meshes. Converting these representations into meshes produces dense, suboptimal outputs. Although recent autoregressive approaches demonstrate promise in directly modeling mesh vertices and faces, they are constrained by the limitation in face count, scalability, and structural fidelity. To address these challenges, we propose Nautilus, a locality-aware autoencoder for artist-like mesh generation that leverages the local properties of manifold meshes to achieve structural fidelity and efficient representation. Our approach introduces a novel tokenization algorithm that preserves face proximity relationships and compresses sequence length through locally shared vertices and edges, enabling the generation of meshes with an unprecedented scale of up to 5,000 faces. Furthermore, we develop a Dual-stream Point Conditioner that provides multi-scale geometric guidance, ensuring global consistency and local structural fidelity by capturing fine-grained geometric features. Extensive experiments demonstrate that Nautilus significantly outperforms state-of-the-art methods in both fidelity and scalability. The project page is at https://nautilusmeshgen.github.io. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14418</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14418</id><created>2025-01-24</created><updated>2025-02-02</updated><authors><author><keyname>Avarikioti</keyname><forenames>Zeta</forenames></author><author><keyname>Wang</keyname><forenames>Yuheng</forenames></author><author><keyname>Wang</keyname><forenames>Yuyi</forenames></author></authors><title>Thunderdome: Timelock-Free Rationally-Secure Virtual Channels</title><categories>cs.CR cs.DC cs.GT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Payment channel networks (PCNs) offer a promising solution to address the limited transaction throughput of deployed blockchains. However, several attacks have recently been proposed that stress the vulnerability of PCNs to timelock and censoring attacks. To address such attacks, we introduce Thunderdome, the first timelock-free PCN. Instead, Thunderdome leverages the design rationale of virtual channels to extend a timelock-free payment channel primitive, thereby enabling multi-hop transactions without timelocks. Previous works either utilize timelocks or do not accommodate transactions between parties that do not share a channel.   At its core, Thunderdome relies on a committee of non-trusted watchtowers, known as wardens, who ensure that no honest party loses funds, even when offline, during the channel closure process. We introduce tailored incentive mechanisms to ensure that all participants follow the protocol's correct execution. Besides a traditional security proof that assumes an honest majority of the committee, we conduct a formal game-theoretic analysis to demonstrate the security of Thunderdome when all participants, including wardens, act rationally. We implement a proof of concept of Thunderdome on Ethereum to validate its feasibility and evaluate its costs. Our evaluation shows that deploying Thunderdome, including opening the underlying payment channel, costs approximately \$15 (0.0089 ETH), while the worst-case cost for closing a channel is about \$7 (0.004 ETH). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14427</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14427</id><created>2025-01-24</created><updated>2025-02-02</updated><authors><author><keyname>Chu</keyname><forenames>Xu</forenames></author><author><keyname>Xue</keyname><forenames>Hanlin</forenames></author><author><keyname>Tan</keyname><forenames>Zhijie</forenames></author><author><keyname>Wang</keyname><forenames>Bingce</forenames></author><author><keyname>Mo</keyname><forenames>Tong</forenames></author><author><keyname>Li</keyname><forenames>Weiping</forenames></author></authors><title>GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand   Graphs Better</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The success of Large Language Models (LLMs) in various domains has led researchers to apply them to graph-related problems by converting graph data into natural language text. However, unlike graph data, natural language inherently has sequential order. We observe a counter-intuitive fact that when the order of nodes or edges in the natural language description of a graph is shuffled, despite describing the same graph, model performance fluctuates between high performance and random guessing. Additionally, due to LLMs' limited input context length, current methods typically randomly sample neighbors of target nodes as representatives of their neighborhood, which may not always be effective for accurate reasoning. To address these gaps, we introduce GraphSOS (Graph Sampling and Order Selection). This novel model framework features an Order Selector Module to ensure proper serialization order of the graph and a Subgraph Sampling Module to sample subgraphs with better structure for better reasoning. Furthermore, we propose Graph CoT obtained through distillation, and enhance LLM's reasoning and zero-shot learning capabilities for graph tasks through instruction tuning. Experiments on multiple datasets for node classification and graph question-answering demonstrate that GraphSOS improves LLMs' performance and generalization ability on graph tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14484</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14484</id><created>2025-01-24</created><updated>2025-02-01</updated><authors><author><keyname>Shen</keyname><forenames>Guobin</forenames></author><author><keyname>Li</keyname><forenames>Jindong</forenames></author><author><keyname>Li</keyname><forenames>Tenglong</forenames></author><author><keyname>Zhao</keyname><forenames>Dongcheng</forenames></author><author><keyname>Zeng</keyname><forenames>Yi</forenames></author></authors><title>$SpikePack$: Enhanced Information Flow in Spiking Neural Networks with   High Hardware Compatibility</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spiking Neural Networks (SNNs) hold promise for energy-efficient, biologically inspired computing. We identify substantial informatio loss during spike transmission, linked to temporal dependencies in traditional Leaky Integrate-and-Fire (LIF) neuron-a key factor potentially limiting SNN performance. Existing SNN architectures also underutilize modern GPUs, constrained by single-bit spike storage and isolated weight-spike operations that restrict computational efficiency. We introduce ${SpikePack}$, a neuron model designed to reduce transmission loss while preserving essential features like membrane potential reset and leaky integration. ${SpikePack}$ achieves constant $\mathcal{O}(1)$ time and space complexity, enabling efficient parallel processing on GPUs and also supporting serial inference on existing SNN hardware accelerators. Compatible with standard Artificial Neural Network (ANN) architectures, ${SpikePack}$ facilitates near-lossless ANN-to-SNN conversion across various networks. Experimental results on tasks such as image classification, detection, and segmentation show ${SpikePack}$ achieves significant gains in accuracy and efficiency for both directly trained and converted SNNs over state-of-the-art models. Tests on FPGA-based platforms further confirm cross-platform flexibility, delivering high performance and enhanced sparsity. By enhancing information flow and rethinking SNN-ANN integration, ${SpikePack}$ advances efficient SNN deployment across diverse hardware platforms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14808</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14808</id><created>2025-01-15</created><updated>2025-02-01</updated><authors><author><keyname>Sun</keyname><forenames>Ting</forenames></author><author><keyname>Wang</keyname><forenames>Penghan</forenames></author><author><keyname>Lai</keyname><forenames>Fan</forenames></author></authors><title>HyGen: Efficient LLM Serving via Elastic Online-Offline Request   Co-location</title><categories>cs.DC cs.LG</categories><comments>15 pages, 16 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models (LLMs) have facilitated a wide range of applications with distinct service-level objectives (SLOs), from latency-sensitive online tasks like interactive chatbots to throughput-oriented offline workloads like document summarization. The existing deployment model, which dedicates machines to each workload, simplifies SLO management but often leads to poor resource utilization. This paper introduces HyGen, an interference-aware LLM serving system that enables efficient co-location of online and offline workloads while preserving latency requirements. HyGen incorporates two key innovations: (1) performance control mechanisms, including a latency predictor to estimate batch execution time and an SLO-aware profiler to quantify latency interference, and (2) SLO-aware offline scheduling policies that maximize serving throughput and prevent starvation, without compromising online serving latency. Our evaluation on production workloads shows that HyGen achieves up to 3.87x overall throughput and 5.84x offline throughput gains over online and hybrid serving baselines, respectively, while strictly satisfying latency SLOs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14844</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14844</id><created>2025-01-24</created><updated>2025-02-02</updated><authors><author><keyname>Coppolillo</keyname><forenames>Erica</forenames></author><author><keyname>Manco</keyname><forenames>Giuseppe</forenames></author><author><keyname>Aiello</keyname><forenames>Luca Maria</forenames></author></authors><title>Unmasking Conversational Bias in AI Multiagent Systems</title><categories>cs.CL cs.AI cs.MA</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Detecting biases in the outputs produced by generative models is essential to reduce the potential risks associated with their application in critical settings. However, the majority of existing methodologies for identifying biases in generated text consider the models in isolation and neglect their contextual applications. Specifically, the biases that may arise in multi-agent systems involving generative models remain under-researched. To address this gap, we present a framework designed to quantify biases within multi-agent systems of conversational Large Language Models (LLMs). Our approach involves simulating small echo chambers, where pairs of LLMs, initialized with aligned perspectives on a polarizing topic, engage in discussions. Contrary to expectations, we observe significant shifts in the stance expressed in the generated messages, particularly within echo chambers where all agents initially express conservative viewpoints, in line with the well-documented political bias of many LLMs toward liberal positions. Crucially, the bias observed in the echo-chamber experiment remains undetected by current state-of-the-art bias detection methods that rely on questionnaires. This highlights a critical need for the development of a more sophisticated toolkit for bias detection and mitigation for AI multi-agent systems. The code to perform the experiments is publicly available at https://anonymous.4open.science/r/LLMsConversationalBias-7725. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.15043</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.15043</id><created>2025-01-24</created><updated>2025-02-02</updated><authors><author><keyname>Chen</keyname><forenames>Kerui</forenames></author><author><keyname>Wu</keyname><forenames>Zhiliang</forenames></author><author><keyname>Hou</keyname><forenames>Wenjin</forenames></author><author><keyname>Li</keyname><forenames>Kun</forenames></author><author><keyname>Fan</keyname><forenames>Hehe</forenames></author><author><keyname>Yang</keyname><forenames>Yi</forenames></author></authors><title>Prompt-Aware Controllable Shadow Removal</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shadow removal aims to restore the image content in shadowed regions. While deep learning-based methods have shown promising results, they still face key challenges: 1) uncontrolled removal of all shadows, or 2) controllable removal but heavily relies on precise shadow region masks. To address these issues, we introduce a novel paradigm: prompt-aware controllable shadow removal. Unlike existing approaches, our paradigm allows for targeted shadow removal from specific subjects based on user prompts (e.g., dots, lines, or subject masks). This approach eliminates the need for shadow annotations and offers flexible, user-controlled shadow removal. Specifically, we propose an end-to-end learnable model, the Prompt-Aware Controllable Shadow Removal Network (PACSRNet). PACSRNet consists of two key modules: a prompt-aware module that generates shadow masks for the specified subject based on the user prompt, and a shadow removal module that uses the shadow prior from the first module to restore the content in the shadowed regions. Additionally, we enhance the shadow removal module by incorporating feature information from the prompt-aware module through a linear operation, providing prompt-guided support for shadow removal. Recognizing that existing shadow removal datasets lack diverse user prompts, we contribute a new dataset specifically designed for prompt-based controllable shadow removal. Extensive experimental results demonstrate the effectiveness and superiority of PACSRNet. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.15068</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.15068</id><created>2025-01-24</created><updated>2025-02-03</updated><authors><author><keyname>Li</keyname><forenames>Dongjiang</forenames></author><author><keyname>Peng</keyname><forenames>Bo</forenames></author><author><keyname>Li</keyname><forenames>Chang</forenames></author><author><keyname>Qiao</keyname><forenames>Ning</forenames></author><author><keyname>Zheng</keyname><forenames>Qi</forenames></author><author><keyname>Sun</keyname><forenames>Lei</forenames></author><author><keyname>Qin</keyname><forenames>Yusen</forenames></author><author><keyname>Li</keyname><forenames>Bangguo</forenames></author><author><keyname>Luan</keyname><forenames>Yifeng</forenames></author><author><keyname>Zhan</keyname><forenames>Yibing</forenames></author><author><keyname>Sun</keyname><forenames>Mingang</forenames></author><author><keyname>Xu</keyname><forenames>Tong</forenames></author><author><keyname>Li</keyname><forenames>Lusong</forenames></author><author><keyname>Shen</keyname><forenames>Hui</forenames></author><author><keyname>He</keyname><forenames>Xiaodong</forenames></author></authors><title>An Atomic Skill Library Construction Method for Data-Efficient Embodied   Manipulation</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Embodied manipulation is a fundamental ability in the realm of embodied artificial intelligence. Although current embodied manipulation models show certain generalizations in specific settings, they struggle in new environments and tasks due to the complexity and diversity of real-world scenarios. The traditional end-to-end data collection and training manner leads to significant data demands. Decomposing end-to-end tasks into reusable atomic skills helps reduce data requirements and improve task execution success rate. However, existing methods are limited by predefined skill sets that cannot be dynamically updated. To address the issue, we introduce a three-wheeled data-driven method to build an atomic skill library, which contains general skills enabling the execution of complex tasks. We divide tasks into subtasks using the Vision-Language Planning (VLP). Then, atomic skill definitions are formed by abstracting the subtasks. Finally, an atomic skill library is constructed via data collection and Vision-Language-Action (VLA) fine-tuning. As the atomic skill library expands dynamically with the three-wheel update strategy, the range of tasks it can cover grows naturally. In this way, our method shifts focus from end-to-end tasks to atomic skills, significantly reducing data costs while maintaining high performance and enabling efficient adaptation to new tasks. Extensive experiments in real-world settings demonstrate the effectiveness and efficiency of our approach. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.15071</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.15071</id><created>2025-01-24</created><updated>2025-02-01</updated><authors><author><keyname>Takizawa</keyname><forenames>Ryo</forenames></author><author><keyname>Ohmura</keyname><forenames>Yoshiyuki</forenames></author><author><keyname>Kuniyoshi</keyname><forenames>Yasuo</forenames></author></authors><title>Gaze-based Task Decomposition for Robot Manipulation in Imitation   Learning</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In imitation learning for robotic manipulation, decomposing object manipulation tasks into multiple sub-tasks is essential. This decomposition enables the reuse of learned skills in varying contexts and the combination of acquired skills to perform novel tasks, rather than merely replicating demonstrated motions. Gaze plays a critical role in human object manipulation, where it is strongly correlated with hand movements. We hypothesize that an imitating agent's gaze control, fixating on specific landmarks and transitioning between them, simultaneously segments demonstrated manipulations into sub-tasks. In this study, we propose a simple yet robust task decomposition method based on gaze transitions. The method leverages teleoperation, a common modality in robotic manipulation for collecting demonstrations, in which a human operator's gaze is measured and used for task decomposition as a substitute for an imitating agent's gaze. Notably, our method achieves consistent task decomposition across all demonstrations for each task, which is desirable in contexts such as machine learning. We applied this method to demonstrations of various tasks and evaluated the characteristics and consistency of the resulting sub-tasks. Furthermore, through extensive testing across a wide range of hyperparameter variations, we demonstrated that the proposed method possesses the robustness necessary for application to different robotic systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.15129</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.15129</id><created>2025-01-25</created><updated>2025-02-02</updated><authors><author><keyname>Zheng</keyname><forenames>Bowen</forenames></author><author><keyname>Cheng</keyname><forenames>Ran</forenames></author><author><keyname>Tan</keyname><forenames>Kay Chen</forenames></author></authors><title>EvoRL: A GPU-accelerated Framework for Evolutionary Reinforcement   Learning</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary Reinforcement Learning (EvoRL) has emerged as a promising approach to overcoming the limitations of traditional reinforcement learning (RL) by integrating the Evolutionary Computation (EC) paradigm with RL. However, the population-based nature of EC significantly increases computational costs, thereby restricting the exploration of algorithmic design choices and scalability in large-scale settings. To address this challenge, we introduce $\texttt{$\textbf{EvoRL}$}$, the first end-to-end EvoRL framework optimized for GPU acceleration. The framework executes the entire training pipeline on accelerators, including environment simulations and EC processes, leveraging hierarchical parallelism through vectorization and compilation techniques to achieve superior speed and scalability. This design enables the efficient training of large populations on a single machine. In addition to its performance-oriented design, $\texttt{$\textbf{EvoRL}$}$ offers a comprehensive platform for EvoRL research, encompassing implementations of traditional RL algorithms (e.g., A2C, PPO, DDPG, TD3, SAC), Evolutionary Algorithms (e.g., CMA-ES, OpenES, ARS), and hybrid EvoRL paradigms such as Evolutionary-guided RL (e.g., ERL, CEM-RL) and Population-Based AutoRL (e.g., PBT). The framework's modular architecture and user-friendly interface allow researchers to seamlessly integrate new components, customize algorithms, and conduct fair benchmarking and ablation studies. The project is open-source and available at: https://github.com/EMI-Group/evorl. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.15194</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.15194</id><created>2025-01-25</created><updated>2025-02-02</updated><authors><author><keyname>Yao</keyname><forenames>Zhihao</forenames></author><author><keyname>Yin</keyname><forenames>Jixuan</forenames></author><author><keyname>Li</keyname><forenames>Bo</forenames></author></authors><title>Reliable Pseudo-labeling via Optimal Transport with Attention for Short   Text Clustering</title><categories>cs.LG stat.CO stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Short text clustering has gained significant attention in the data mining community. However, the limited valuable information contained in short texts often leads to low-discriminative representations, increasing the difficulty of clustering. This paper proposes a novel short text clustering framework, called Reliable \textbf{P}seudo-labeling via \textbf{O}ptimal \textbf{T}ransport with \textbf{A}ttention for Short Text Clustering (\textbf{POTA}), that generate reliable pseudo-labels to aid discriminative representation learning for clustering. Specially, \textbf{POTA} first implements an instance-level attention mechanism to capture the semantic relationships among samples, which are then incorporated as a semantic consistency regularization term into an optimal transport problem. By solving this OT problem, we can yield reliable pseudo-labels that simultaneously account for sample-to-sample semantic consistency and sample-to-cluster global structure information. Additionally, the proposed OT can adaptively estimate cluster distributions, making \textbf{POTA} well-suited for varying degrees of imbalanced datasets. Then, we utilize the pseudo-labels to guide contrastive learning to generate discriminative representations and achieve efficient clustering. Extensive experiments demonstrate \textbf{POTA} outperforms state-of-the-art methods. The code is available at: \href{https://github.com/YZH0905/POTA-STC/tree/main}{https://github.com/YZH0905/POTA-STC/tree/main}. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.15335</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.15335</id><created>2025-01-25</created><updated>2025-01-31</updated><authors><author><keyname>Jolly</keyname><forenames>Michael</forenames></author><author><keyname>Kim</keyname><forenames>Woojeong</forenames></author><author><keyname>Tawri</keyname><forenames>Krutika</forenames></author><author><keyname>Temam</keyname><forenames>Roger</forenames></author></authors><title>A numerical scheme for a multi-scale model of thrombus in arteries</title><categories>math.NA cs.NA</categories><comments>The co-author does not agree on this uploading</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, the time-discretization of the fluid structure interaction model in the three-dimensional boundary domain is taken into account, which explains the mechanical interaction between the blood flow and the Hookean elasticity. The interface between the two phases is given by a soft transition layer and spreads to the finite thickness. On the implicit Euler scheme for this discretization, We derive a variety of priori estimates and then use the Faedo-Galerkin method to prove the local well-poseedness results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.15392</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.15392</id><created>2025-01-25</created><updated>2025-02-02</updated><authors><author><keyname>Ma</keyname><forenames>Youpeng</forenames></author><author><keyname>Chen</keyname><forenames>Tao</forenames></author><author><keyname>Li</keyname><forenames>Ke</forenames></author></authors><title>Faster Configuration Performance Bug Testing with Neural Dual-level   Prioritization</title><categories>cs.SE cs.AI</categories><comments>accepted by ICSE 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As software systems become more complex and configurable, more performance problems tend to arise from the configuration designs. This has caused some configuration options to unexpectedly degrade performance which deviates from their original expectations designed by the developers. Such discrepancies, namely configuration performance bugs (CPBugs), are devastating and can be deeply hidden in the source code. Yet, efficiently testing CPBugs is difficult, not only due to the test oracle is hard to set, but also because the configuration measurement is expensive and there are simply too many possible configurations to test. As such, existing testing tools suffer from lengthy runtime or have been ineffective in detecting CPBugs when the budget is limited, compounded by inaccurate test oracle. In this paper, we seek to achieve significantly faster CPBug testing by neurally prioritizing the testing at both the configuration option and value range levels with automated oracle estimation. Our proposed tool, dubbed NDP, is a general framework that works with different heuristic generators. The idea is to leverage two neural language models: one to estimate the CPBug types that serve as the oracle while, more vitally, the other to infer the probabilities of an option being CPBug-related, based on which the options and the value ranges to be searched can be prioritized. Experiments on several widely-used systems of different versions reveal that NDP can, in general, better predict CPBug type in 87% cases and find more CPBugs with up to 88.88x testing efficiency speedup over the state-of-the-art tools. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.15708</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.15708</id><created>2025-01-26</created><updated>2025-02-01</updated><authors><author><keyname>Cho</keyname><forenames>Hakaze</forenames></author><author><keyname>Inoue</keyname><forenames>Naoya</forenames></author></authors><title>StaICC: Standardized Evaluation for Classification Task in In-context   Learning</title><categories>cs.CL cs.AI</categories><comments>20 pages, 8 figures, 8 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classification tasks are widely investigated in the In-Context Learning (ICL) paradigm. However, current efforts are evaluated on disjoint benchmarks and settings, while their performances are significantly influenced by some trivial variables, such as prompt templates, data sampling, instructions, etc., which leads to significant inconsistencies in the results reported across various literature, preventing fair comparison or meta-analysis across different papers. Therefore, this paper proposes a standardized and easy-to-use evaluation toolkit (StaICC) for in-context classification. Including, for the normal classification task, we provide StaICC-Normal, selecting 10 widely used datasets, and generating prompts with a fixed form, to mitigate the variance among the experiment implementations. To enrich the usage of our benchmark, we also provide a sub-benchmark StaICC-Diag for diagnosing ICL from several aspects, aiming for a more robust inference processing. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.15828</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.15828</id><created>2025-01-27</created><updated>2025-02-03</updated><authors><author><keyname>Chen</keyname><forenames>Ying</forenames></author><author><keyname>Griffin</keyname><forenames>Paul</forenames></author><author><keyname>Recchia</keyname><forenames>Paolo</forenames></author><author><keyname>Lei</keyname><forenames>Zhou</forenames></author><author><keyname>Zhang</keyname><forenames>Hongrui</forenames></author></authors><title>Hybrid Quantum Neural Networks with Amplitude Encoding: Advancing   Recovery Rate Predictions</title><categories>q-fin.CP cs.LG quant-ph</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Recovery rate prediction plays a pivotal role in bond investment strategies, enhancing risk assessment, optimizing portfolio allocation, improving pricing accuracy, and supporting effective credit risk management. However, forecasting faces challenges like high-dimensional features, small sample sizes, and overfitting. We propose a hybrid Quantum Machine Learning model incorporating Parameterized Quantum Circuits (PQC) within a neural network framework. PQCs inherently preserve unitarity, avoiding computationally costly orthogonality constraints, while amplitude encoding enables exponential data compression, reducing qubit requirements logarithmically. Applied to a global dataset of 1,725 observations (1996-2023), our method achieved superior accuracy (RMSE 0.228) compared to classical neural networks (0.246) and quantum models with angle encoding (0.242), with efficient computation times. This work highlights the potential of hybrid quantum-classical architectures in advancing recovery rate forecasting. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.16065</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.16065</id><created>2025-01-27</created><updated>2025-02-02</updated><authors><author><keyname>Zhao</keyname><forenames>Huazhong</forenames></author><author><keyname>Qi</keyname><forenames>Lei</forenames></author><author><keyname>Geng</keyname><forenames>Xin</forenames></author></authors><title>CILP-FGDI: Exploiting Vision-Language Model for Generalizable Person   Re-Identification</title><categories>cs.CV</categories><comments>Accepted by IEEE TIFS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Visual Language Model, known for its robust cross-modal capabilities, has been extensively applied in various computer vision tasks. In this paper, we explore the use of CLIP (Contrastive Language-Image Pretraining), a vision-language model pretrained on large-scale image-text pairs to align visual and textual features, for acquiring fine-grained and domain-invariant representations in generalizable person re-identification. The adaptation of CLIP to the task presents two primary challenges: learning more fine-grained features to enhance discriminative ability, and learning more domain-invariant features to improve the model's generalization capabilities. To mitigate the first challenge thereby enhance the ability to learn fine-grained features, a three-stage strategy is proposed to boost the accuracy of text descriptions. Initially, the image encoder is trained to effectively adapt to person re-identification tasks. In the second stage, the features extracted by the image encoder are used to generate textual descriptions (i.e., prompts) for each image. Finally, the text encoder with the learned prompts is employed to guide the training of the final image encoder. To enhance the model's generalization capabilities to unseen domains, a bidirectional guiding method is introduced to learn domain-invariant image features. Specifically, domain-invariant and domain-relevant prompts are generated, and both positive (pulling together image features and domain-invariant prompts) and negative (pushing apart image features and domain-relevant prompts) views are used to train the image encoder. Collectively, these strategies contribute to the development of an innovative CLIP-based framework for learning fine-grained generalized features in person re-identification. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.16096</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.16096</id><created>2025-01-27</created><updated>2025-02-01</updated><authors><author><keyname>Zhao</keyname><forenames>Zhenyu</forenames></author><author><keyname>Wang</keyname><forenames>Yanfei</forenames></author><author><keyname>Yagola</keyname><forenames>Anatoly G.</forenames></author><author><keyname>Li</keyname><forenames>Xusheng</forenames></author></authors><title>A New Approach for Fourier Extension Based on Weighted Generalized   Inverse</title><categories>math.NA cs.NA</categories><msc-class>42A10, 65T40, 65T50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the Fourier extension from a new perspective of solving the compact operator equation with perturbed data. By converting the approximation target from the best approximate solution to the weighted best approximate solution, the oscillation in the extended region has been overcome. The error estimation of the solution is theoretically established. Furthermore, we point out the difficulties faced by the original weighted operator in calculation due to the limitation of machine precision and propose an effective correction operator. The relevant parameters involved in the method are further tested, and finally the effectiveness of the method is verified through numerical experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.16226</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.16226</id><created>2025-01-27</created><updated>2025-02-01</updated><authors><author><keyname>Takanami</keyname><forenames>Kaito</forenames></author><author><keyname>Takahashi</keyname><forenames>Takashi</forenames></author><author><keyname>Sakata</keyname><forenames>Ayaka</forenames></author></authors><title>The Effect of Optimal Self-Distillation in Noisy Gaussian Mixture Model</title><categories>stat.ML cond-mat.dis-nn cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Self-distillation (SD), a technique where a model refines itself from its own predictions, has garnered attention as a simple yet powerful approach in machine learning. Despite its widespread use, the mechanisms underlying its effectiveness remain unclear. In this study, we investigate the efficacy of hyperparameter-tuned multi-stage SD in binary classification tasks with noisy labeled Gaussian mixture data, utilizing a replica theory. Our findings reveals that the primary driver of SD's performance improvement is denoising through hard pseudo-labels, with the most notable gains observed in moderately sized datasets. We also demonstrate the efficacy of practical heuristics, such as early stopping for extracting meaningful signal and bias fixation for imbalanced data. These results provide both theoretical guarantees and practical insights, advancing our understanding and application of SD in noisy settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.16249</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.16249</id><created>2025-01-27</created><updated>2025-01-31</updated><authors><author><keyname>Nettur</keyname><forenames>Suresh Babu</forenames></author><author><keyname>Karpurapu</keyname><forenames>Shanthi</forenames></author><author><keyname>Nettur</keyname><forenames>Unnati</forenames></author><author><keyname>Gajja</keyname><forenames>Likhit Sagar</forenames></author><author><keyname>Myneni</keyname><forenames>Sravanthy</forenames></author><author><keyname>Dusi</keyname><forenames>Akhil</forenames></author><author><keyname>Posham</keyname><forenames>Lalithya</forenames></author></authors><title>Lightweight Weighted Average Ensemble Model for Pneumonia Detection in   Chest X-Ray Images</title><categories>eess.IV cs.AI cs.CV</categories><comments>Corresponding authors: Shanthi Karpurapu   (shanthi.karpurapu@gmail.com), Suresh Babu Nettur (nettursuresh@gmail.com)   Shanthi Karpurapu and Suresh Babu Nettur are co-first authors</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Pneumonia is a leading cause of illness and death in children, underscoring the need for early and accurate detection. In this study, we propose a novel lightweight ensemble model for detecting pneumonia in children using chest X-ray images. This ensemble model integrates two pre-trained convolutional neural networks (CNNs), MobileNetV2 and NASNetMobile, selected for their balance of computational efficiency and accuracy. These models were fine-tuned on a pediatric chest X-ray dataset and combined to enhance classification performance. Our proposed ensemble model achieved a classification accuracy of 98.63%, significantly outperforming individual models such as MobileNetV2 (97.10%) and NASNetMobile(96.25%) in terms of accuracy, precision, recall, and F1 score. Moreover, the ensemble model outperformed state-of-the-art architectures, including ResNet50, InceptionV3, and DenseNet201, while maintaining computational efficiency. The proposed lightweight ensemble model presents a highly effective and resource-efficient solution for pneumonia detection, making it particularly suitable for deployment in resource-constrained settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.16256</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.16256</id><created>2025-01-27</created><updated>2025-01-31</updated><authors><author><keyname>Wu</keyname><forenames>Ziniu</forenames></author><author><keyname>Markakis</keyname><forenames>Markos</forenames></author><author><keyname>Liu</keyname><forenames>Chunwei</forenames></author><author><keyname>Chen</keyname><forenames>Peter Baile</forenames></author><author><keyname>Narayanaswamy</keyname><forenames>Balakrishnan</forenames></author><author><keyname>Kraska</keyname><forenames>Tim</forenames></author><author><keyname>Madden</keyname><forenames>Samuel</forenames></author></authors><title>Improving DBMS Scheduling Decisions with Fine-grained Performance   Prediction on Concurrent Queries -- Extended</title><categories>cs.DB cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Query scheduling is a critical task that directly impacts query performance in database management systems (DBMS). Deeply integrated schedulers, which require changes to DBMS internals, are usually customized for a specific engine and can take months to implement. In contrast, non-intrusive schedulers make coarse-grained decisions, such as controlling query admission and re-ordering query execution, without requiring modifications to DBMS internals. They require much less engineering effort and can be applied across a wide range of DBMS engines, offering immediate benefits to end users. However, most existing non-intrusive scheduling systems rely on simplified cost models and heuristics that cannot accurately model query interactions under concurrency and different system states, possibly leading to suboptimal scheduling decisions.   This work introduces IconqSched, a new, principled non-intrusive scheduler that optimizes the execution order and timing of queries to enhance total end-to-end runtime as experienced by the user query queuing time plus system runtime. Unlike previous approaches, IconqSched features a novel fine-grained predictor, Iconq, which treats the DBMS as a black box and accurately estimates the system runtime of concurrently executed queries under different system states. Using these predictions, IconqSched is able to capture system runtime variations across different query mixes and system loads. It then employs a greedy scheduling algorithm to effectively determine which queries to submit and when to submit them. We compare IconqSched to other schedulers in terms of end-to-end runtime using real workload traces. On Postgres, IconqSched reduces end-to-end runtime by 16.2%-28.2% on average and 33.6%-38.9% in the tail. Similarly, on Redshift, it reduces end-to-end runtime by 10.3%-14.1% on average and 14.9%-22.2% in the tail. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.16383</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.16383</id><created>2025-01-24</created><updated>2025-02-01</updated><authors><author><keyname>Su</keyname><forenames>Zunhai</forenames></author><author><keyname>Chen</keyname><forenames>Zhe</forenames></author><author><keyname>Shen</keyname><forenames>Wang</forenames></author><author><keyname>Wei</keyname><forenames>Hanyu</forenames></author><author><keyname>Li</keyname><forenames>Linge</forenames></author><author><keyname>Yu</keyname><forenames>Huangqi</forenames></author><author><keyname>Yuan</keyname><forenames>Kehong</forenames></author></authors><title>RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via   Outlier-Aware Adaptive Rotations</title><categories>cs.LG cs.AI cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Key-Value (KV) cache facilitates efficient large language models (LLMs) inference by avoiding recomputation of past KVs. As the batch size and context length increase, the oversized KV caches become a significant memory bottleneck, highlighting the need for efficient compression. Existing KV quantization rely on fine-grained quantization or the retention of a significant portion of high bit-widths caches, both of which compromise compression ratio and often fail to maintain robustness at extremely low average bit-widths. In this work, we explore the potential of rotation technique for 2-bit KV quantization and propose RotateKV, which achieves accurate and robust performance through the following innovations: (i) Outlier-Aware Rotation, which utilizes channel-reordering to adapt the rotations to varying channel-wise outlier distributions without sacrificing the computational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii) Pre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position embedding (RoPE) on proposed outlier-aware rotation and further smooths outliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages the massive activations to precisely identify and protect attention sinks. RotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit quantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning and long-context capabilities, with less than 1.7\% degradation on GSM8K, outperforming existing methods even at lower average bit-widths. RotateKV also showcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch sizes, and achieves a 2.32x speedup in decoding stage. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.16450</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.16450</id><created>2025-01-27</created><updated>2025-01-31</updated><authors><author><keyname>Firooz</keyname><forenames>Hamed</forenames></author><author><keyname>Sanjabi</keyname><forenames>Maziar</forenames></author><author><keyname>Englhardt</keyname><forenames>Adrian</forenames></author><author><keyname>Gupta</keyname><forenames>Aman</forenames></author><author><keyname>Levine</keyname><forenames>Ben</forenames></author><author><keyname>Olgiati</keyname><forenames>Dre</forenames></author><author><keyname>Polatkan</keyname><forenames>Gungor</forenames></author><author><keyname>Melnychuk</keyname><forenames>Iuliia</forenames></author><author><keyname>Ramgopal</keyname><forenames>Karthik</forenames></author><author><keyname>Talanine</keyname><forenames>Kirill</forenames></author><author><keyname>Srinivasan</keyname><forenames>Kutta</forenames></author><author><keyname>Simon</keyname><forenames>Luke</forenames></author><author><keyname>Sivasubramoniapillai</keyname><forenames>Natesh</forenames></author><author><keyname>Ayan</keyname><forenames>Necip Fazil</forenames></author><author><keyname>Song</keyname><forenames>Qingquan</forenames></author><author><keyname>Sriram</keyname><forenames>Samira</forenames></author><author><keyname>Ghosh</keyname><forenames>Souvik</forenames></author><author><keyname>Song</keyname><forenames>Tao</forenames></author><author><keyname>Kothapalli</keyname><forenames>Vignesh</forenames></author><author><keyname>Zhai</keyname><forenames>Xiaoling</forenames></author><author><keyname>Xu</keyname><forenames>Ya</forenames></author><author><keyname>Wang</keyname><forenames>Yu</forenames></author><author><keyname>Dai</keyname><forenames>Yun</forenames></author></authors><title>360Brew: A Decoder-only Foundation Model for Personalized Ranking and   Recommendation</title><categories>cs.IR cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ranking and recommendation systems are the foundation for numerous online experiences, ranging from search results to personalized content delivery. These systems have evolved into complex, multilayered architectures that leverage vast datasets and often incorporate thousands of predictive models. The maintenance and enhancement of these models is a labor intensive process that requires extensive feature engineering. This approach not only exacerbates technical debt but also hampers innovation in extending these systems to emerging problem domains. In this report, we present our research to address these challenges by utilizing a large foundation model with a textual interface for ranking and recommendation tasks. We illustrate several key advantages of our approach: (1) a single model can manage multiple predictive tasks involved in ranking and recommendation, (2) decoder models with textual interface due to their comprehension of reasoning capabilities, can generalize to new recommendation surfaces and out-of-domain problems, and (3) by employing natural language interfaces for task definitions and verbalizing member behaviors and their social connections, we eliminate the need for feature engineering and the maintenance of complex directed acyclic graphs of model dependencies. We introduce our research pre-production model, 360Brew V1.0, a 150B parameter, decoder-only model that has been trained and fine-tuned on LinkedIn's data and tasks. This model is capable of solving over 30 predictive tasks across various segments of the LinkedIn platform, achieving performance levels comparable to or exceeding those of current production systems based on offline metrics, without task-specific fine-tuning. Notably, each of these tasks is conventionally addressed by dedicated models that have been developed and maintained over multiple years by teams of a similar or larger size than our own. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.16729</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.16729</id><created>2025-01-28</created><updated>2025-02-01</updated><authors><author><keyname>Davelouis</keyname><forenames>Fatima</forenames></author><author><keyname>Martin</keyname><forenames>John D.</forenames></author><author><keyname>Bowling</keyname><forenames>Michael</forenames></author></authors><title>On the Interplay Between Sparsity and Training in Deep Reinforcement   Learning</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study the benefits of different sparse architectures for deep reinforcement learning. In particular, we focus on image-based domains where spatially-biased and fully-connected architectures are common. Using these and several other architectures of equal capacity, we show that sparse structure has a significant effect on learning performance. We also observe that choosing the best sparse architecture for a given domain depends on whether the hidden layer weights are fixed or learned. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.16759</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.16759</id><created>2025-01-28</created><updated>2025-02-01</updated><authors><author><keyname>Yu</keyname><forenames>Weiping</forenames></author><author><keyname>Wang</keyname><forenames>Fan</forenames></author><author><keyname>Zhang</keyname><forenames>Xuwei</forenames></author><author><keyname>Luo</keyname><forenames>Siqiang</forenames></author></authors><title>Are Joins over LSM-trees Ready: Take RocksDB as an Example</title><categories>cs.DB</categories><comments>Accepted by VLDB 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  LSM-tree-based data stores are widely adopted in industries for their excellent performance. As data scales increase, disk-based join operations become indispensable yet costly for the database, making the selection of suitable join methods crucial for system optimization. Current LSM-based stores generally adhere to conventional relational database practices and support only a limited number of join methods. However, the LSM-tree delivers distinct read and write efficiency compared to the relational databases, which could accordingly impact the performance of various join methods. Therefore, it is necessary to reconsider the selection of join methods in this context to fully explore the potential of various join algorithms and index designs. In this work, we present a systematic study and an exhaustive benchmark for joins over LSM-trees. We define a configuration space for join methods, encompassing various join algorithms, secondary index types, and consistency strategies. We also summarize a theoretical analysis to evaluate the overhead of each join method for an in-depth understanding. Furthermore, we implement all join methods in the configuration space on a unified platform and compare their performance through extensive experiments. Our theoretical and experimental results yield several insights and takeaways tailored to joins in LSM-based stores that aid developers in choosing proper join methods based on their working conditions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.16996</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.16996</id><created>2025-01-28</created><updated>2025-02-02</updated><authors><author><keyname>Liang</keyname><forenames>Annie</forenames></author></authors><title>Artificial Intelligence Clones</title><categories>econ.TH cs.GT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models, trained on personal data, may soon be able to mimic individual personalities. This would potentially transform search across human candidates, including for marriage and jobs -- indeed, several dating platforms have already begun experimenting with training "AI clones" to represent users. This paper presents a theoretical framework to study the tradeoff between the substantially expanded search capacity of AI clones and their imperfect representation of humans. Individuals are modeled as points in $k$-dimensional Euclidean space, and their AI clones are modeled as noisy approximations. I compare two search regimes: an "in-person regime" -- where each person randomly meets some number of individuals and matches to the most compatible among them -- against an "AI representation regime" -- in which individuals match to the person whose AI clone is most compatible with their AI clone. I show that a finite number of in-person encounters exceeds the expected payoff from search over infinite AI clones. Moreover, when the dimensionality of personality is large, simply meeting two people in person produces a higher expected match quality than entrusting the process to an AI platform, regardless of the size of its candidate pool. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.17168</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.17168</id><created>2025-01-21</created><updated>2025-02-02</updated><authors><author><keyname>Wang</keyname><forenames>Lishuang</forenames></author><author><keyname>Wu</keyname><forenames>Zhihong</forenames></author><author><keyname>Sun</keyname><forenames>Kebin</forenames></author><author><keyname>Li</keyname><forenames>Zhuozhao</forenames></author><author><keyname>Cheng</keyname><forenames>Ran</forenames></author></authors><title>EvoGP: A GPU-accelerated Framework for Tree-based Genetic Programming</title><categories>cs.NE cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tree-based Genetic Programming (TGP) is a key evolutionary algorithm widely used in symbolic regression, feature engineering, and scientific modeling. Its high computational demands make GPU acceleration essential for scalable and high-performance evolutionary computation. However, GPU acceleration of TGP faces three key challenges: inefficient tree encoding, highly heterogeneous genetic operations, and limited parallelism in fitness evaluation. To address these challenges, we introduce EvoGP, a comprehensive GPU-accelerated TGP framework. First, we design a tensorized encoding scheme to represent tree with different structures as tensors with the same shape, optimizing memory access and enabling efficient parallel execution. Second, we propose a unified parallel framework for genetic operations by leveraging shared computational primitives and implementing dedicated CUDA kernels for scalable performance. Third, we present a fully parallel fitness evaluation strategy for symbolic regression, exploiting both population-level and data-level parallelism to maximize GPU utilization. Moreover, we implement a comprehensive library to provide rich algorithm operators and benchmark problems. EvoGP is extensively tested on various tasks, including symbolic regression, classification, and robotics control, demonstrating its versatility and effectiveness across diverse application scenarios. Experimental results show that EvoGP achieves up to a 140.89x speedup over the state-of-the-art GPU-based TGP implementation, while maintaining or exceeding the accuracy of baseline methods. EvoGP is open-source and accessible at: https://github.com/EMI-Group/evogp. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.17183</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.17183</id><created>2025-01-25</created><updated>2025-02-01</updated><authors><author><keyname>Liu</keyname><forenames>Beiming</forenames></author><author><keyname>Cui</keyname><forenames>Zhizhuo</forenames></author><author><keyname>Hu</keyname><forenames>Siteng</forenames></author><author><keyname>Li</keyname><forenames>Xiaohua</forenames></author><author><keyname>Lin</keyname><forenames>Haifeng</forenames></author><author><keyname>Zhang</keyname><forenames>Zhengxin</forenames></author></authors><title>LLM Evaluation Based on Aerospace Manufacturing Expertise: Automated   Generation and Multi-Model Question Answering</title><categories>cs.CL cs.AI</categories><comments>conference paper</comments><msc-class>68T50, 90B30</msc-class><acm-class>I.2.7; J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aerospace manufacturing demands exceptionally high precision in technical parameters. The remarkable performance of Large Language Models (LLMs), such as GPT-4 and QWen, in Natural Language Processing has sparked industry interest in their application to tasks including process design, material selection, and tool information retrieval. However, LLMs are prone to generating "hallucinations" in specialized domains, producing inaccurate or false information that poses significant risks to the quality of aerospace products and flight safety. This paper introduces a set of evaluation metrics tailored for LLMs in aerospace manufacturing, aiming to assess their accuracy by analyzing their performance in answering questions grounded in professional knowledge. Firstly, key information is extracted through in-depth textual analysis of classic aerospace manufacturing textbooks and guidelines. Subsequently, utilizing LLM generation techniques, we meticulously construct multiple-choice questions with multiple correct answers of varying difficulty. Following this, different LLM models are employed to answer these questions, and their accuracy is recorded. Experimental results demonstrate that the capabilities of LLMs in aerospace professional knowledge are in urgent need of improvement. This study provides a theoretical foundation and practical guidance for the application of LLMs in aerospace manufacturing, addressing a critical gap in the field. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.17409</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.17409</id><created>2025-01-28</created><updated>2025-02-01</updated><authors><author><keyname>Wang</keyname><forenames>Xiaobei</forenames></author><author><keyname>Liu</keyname><forenames>Shuchang</forenames></author><author><keyname>Cai</keyname><forenames>Qingpeng</forenames></author><author><keyname>Li</keyname><forenames>Xiang</forenames></author><author><keyname>Hu</keyname><forenames>Lantao</forenames></author><author><keyname>li</keyname><forenames>Han</forenames></author><author><keyname>Xie</keyname><forenames>Guangming</forenames></author></authors><title>Value Function Decomposition in Markov Recommendation Process</title><categories>cs.IR</categories><comments>12 pages, 9 figures</comments><acm-class>H.3.3</acm-class><doi>10.1145/3696410.3714807</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent advances in recommender systems have shown that user-system interaction essentially formulates long-term optimization problems, and online reinforcement learning can be adopted to improve recommendation performance. The general solution framework incorporates a value function that estimates the user's expected cumulative rewards in the future and guides the training of the recommendation policy. To avoid local maxima, the policy may explore potential high-quality actions during inference to increase the chance of finding better future rewards. To accommodate the stepwise recommendation process, one widely adopted approach to learning the value function is learning from the difference between the values of two consecutive states of a user. However, we argue that this paradigm involves a challenge of Mixing Random Factors: there exist two random factors from the stochastic policy and the uncertain user environment, but they are not separately modeled in the standard temporal difference (TD) learning, which may result in a suboptimal estimation of the long-term rewards and less effective action exploration. As a solution, we show that these two factors can be separately approximated by decomposing the original temporal difference loss. The disentangled learning framework can achieve a more accurate estimation with faster learning and improved robustness against action exploration. As an empirical verification of our proposed method, we conduct offline experiments with simulated online environments built on the basis of public datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.17445</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.17445</id><created>2025-01-29</created><updated>2025-02-03</updated><authors><author><keyname>Berlow</keyname><forenames>Katalin</forenames></author><author><keyname>Bernshteyn</keyname><forenames>Anton</forenames></author><author><keyname>Lyons</keyname><forenames>Clark</forenames></author><author><keyname>Weilacher</keyname><forenames>Felix</forenames></author></authors><title>Separating complexity classes of LCL problems on grids</title><categories>math.LO cs.CC math.CO math.PR</categories><comments>36 pp., 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of locally checkable labeling (LCL) problems on $\mathbb{Z}^n$ from the point of view of descriptive set theory, computability theory, and factors of i.i.d. Our results separate various complexity classes that were not previously known to be distinct and serve as counterexamples to a number of natural conjectures in the field. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.17876</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.17876</id><created>2025-01-18</created><updated>2025-02-02</updated><authors><author><keyname>Mo</keyname><forenames>Hao</forenames></author><author><keyname>Sun</keyname><forenames>Yaping</forenames></author><author><keyname>Yao</keyname><forenames>Shumin</forenames></author><author><keyname>Chen</keyname><forenames>Hao</forenames></author><author><keyname>Chen</keyname><forenames>Zhiyong</forenames></author><author><keyname>Xu</keyname><forenames>Xiaodong</forenames></author><author><keyname>Ma</keyname><forenames>Nan</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author><author><keyname>Cui</keyname><forenames>Shuguang</forenames></author></authors><title>SCDM: Score-Based Channel Denoising Model for Digital Semantic   Communications</title><categories>eess.SP cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Score-based diffusion models represent a significant variant within the diffusion model family and have seen extensive application in the increasingly popular domain of generative tasks. Recent investigations have explored the denoising potential of diffusion models in semantic communications. However, in previous paradigms, noise distortion in the diffusion process does not match precisely with digital channel noise characteristics. In this work, we introduce the Score-Based Channel Denoising Model (SCDM) for Digital Semantic Communications (DSC). SCDM views the distortion of constellation symbol sequences in digital transmission as a score-based forward diffusion process. We design a tailored forward noise corruption to align digital channel noise properties in the training phase. During the inference stage, the well-trained SCDM can effectively denoise received semantic symbols under various SNR conditions, reducing the difficulty for the semantic decoder in extracting semantic information from the received noisy symbols and thereby enhancing the robustness of the reconstructed semantic information. Experimental results show that SCDM outperforms the baseline model in PSNR, SSIM, and MSE metrics, particularly at low SNR levels. Moreover, SCDM reduces storage requirements by a factor of 7.8. This efficiency in storage, combined with its robust denoising capability, makes SCDM a practical solution for DSC across diverse channel conditions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.17976</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.17976</id><created>2025-01-29</created><updated>2025-01-31</updated><authors><author><keyname>Yahia</keyname><forenames>Issam Ait</forenames></author><author><keyname>Berrada</keyname><forenames>Ismail</forenames></author></authors><title>KoopAGRU: A Koopman-based Anomaly Detection in Time-Series using Gated   Recurrent Units</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Anomaly detection in real-world time-series data is a challenging task due to the complex and nonlinear temporal dynamics involved. This paper introduces KoopAGRU, a new deep learning model designed to tackle this problem by combining Fast Fourier Transform (FFT), Deep Dynamic Mode Decomposition (DeepDMD), and Koopman theory. FFT allows KoopAGRU to decompose temporal data into time-variant and time-invariant components providing precise modeling of complex patterns. To better control these two components, KoopAGRU utilizes Gate Recurrent Unit (GRU) encoders to learn Koopman observables, enhancing the detection capability across multiple temporal scales. KoopAGRU is trained in a single process and offers fast inference times. Extensive tests on various benchmark datasets show that KoopAGRU outperforms other leading methods, achieving a new average F1-score of 90.88\% on the well-known anomalies detection task of times series datasets, and proves to be efficient and reliable in detecting anomalies in real-world scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.17980</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.17980</id><created>2025-01-29</created><updated>2025-01-31</updated><authors><author><keyname>Bhardwaj</keyname><forenames>Eshta</forenames></author><author><keyname>Alexander</keyname><forenames>Rohan</forenames></author><author><keyname>Becker</keyname><forenames>Christoph</forenames></author></authors><title>Limits to AI Growth: The Ecological and Social Consequences of Scaling</title><categories>cs.CY cs.AI</categories><comments>14 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The accelerating development and deployment of AI technologies depend on the continued ability to scale their infrastructure. This has implied increasing amounts of monetary investment and natural resources. Frontier AI applications have thus resulted in rising financial, environmental, and social costs. While the factors that AI scaling depends on reach its limits, the push for its accelerated advancement and entrenchment continues. In this paper, we provide a holistic review of AI scaling using four lenses (technical, economic, ecological, and social) and review the relationships between these lenses to explore the dynamics of AI growth. We do so by drawing on system dynamics concepts including archetypes such as "limits to growth" to model the dynamic complexity of AI scaling and synthesize several perspectives. Our work maps out the entangled relationships between the technical, economic, ecological and social perspectives and the apparent limits to growth. The analysis explains how industry's responses to external limits enables continued (but temporary) scaling and how this benefits Big Tech while externalizing social and environmental damages. To avoid an "overshoot and collapse" trajectory, we advocate for realigning priorities and norms around scaling to prioritize sustainable and mindful advancements. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.17983</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.17983</id><created>2025-01-29</created><updated>2025-02-03</updated><authors><author><keyname>Wang</keyname><forenames>Xudong</forenames></author><author><keyname>Peng</keyname><forenames>Yaxin</forenames></author><author><keyname>Shen</keyname><forenames>Chaomin</forenames></author></authors><title>Efficient Feature Fusion for UAV Object Detection</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Object detection in unmanned aerial vehicle (UAV) remote sensing images poses significant challenges due to unstable image quality, small object sizes, complex backgrounds, and environmental occlusions. Small objects, in particular, occupy small portions of images, making their accurate detection highly difficult. Existing multi-scale feature fusion methods address these challenges to some extent by aggregating features across different resolutions. However, they often fail to effectively balance the classification and localization performance for small objects, primarily due to insufficient feature representation and imbalanced network information flow. In this paper, we propose a novel feature fusion framework specifically designed for UAV object detection tasks to enhance both localization accuracy and classification performance. The proposed framework integrates hybrid upsampling and downsampling modules, enabling feature maps from different network depths to be flexibly adjusted to arbitrary resolutions. This design facilitates cross-layer connections and multi-scale feature fusion, ensuring improved representation of small objects. Our approach leverages hybrid downsampling to enhance fine-grained feature representation, improving spatial localization of small targets, even under complex conditions. Simultaneously, the upsampling module aggregates global contextual information, optimizing feature consistency across scales and enhancing classification robustness in cluttered scenes. Experimental results on two public UAV datasets demonstrate the effectiveness of the proposed framework. Integrated into the YOLO-v10 model, our method achieves a 2% improvement in average precision (AP) compared to the baseline YOLO-v10 model, while maintaining the same number of parameters. These results highlight the potential of our framework for accurate and efficient UAV object detection. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18055</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18055</id><created>2025-01-29</created><updated>2025-02-01</updated><authors><author><keyname>de Jong</keyname><forenames>Edwin D.</forenames></author><author><keyname>Marcus</keyname><forenames>Eric</forenames></author><author><keyname>Teuwen</keyname><forenames>Jonas</forenames></author></authors><title>Current Pathology Foundation Models are unrobust to Medical Center   Differences</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Pathology Foundation Models (FMs) hold great promise for healthcare. Before they can be used in clinical practice, it is essential to ensure they are robust to variations between medical centers. We measure whether pathology FMs focus on biological features like tissue and cancer type, or on the well known confounding medical center signatures introduced by staining procedure and other differences. We introduce the Robustness Index. This novel robustness metric reflects to what degree biological features dominate confounding features. Ten current publicly available pathology FMs are evaluated. We find that all current pathology foundation models evaluated represent the medical center to a strong degree. Significant differences in the robustness index are observed. Only one model so far has a robustness index greater than one, meaning biological features dominate confounding features, but only slightly. A quantitative approach to measure the influence of medical center differences on FM-based prediction performance is described. We analyze the impact of unrobustness on classification performance of downstream models, and find that cancer-type classification errors are not random, but specifically attributable to same-center confounders: images of other classes from the same medical center. We visualize FM embedding spaces, and find these are more strongly organized by medical centers than by biological factors. As a consequence, the medical center of origin is predicted more accurately than the tissue source and cancer type. The robustness index introduced here is provided with the aim of advancing progress towards clinical adoption of robust and reliable pathology FMs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18092</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18092</id><created>2025-01-29</created><updated>2025-02-01</updated><authors><author><keyname>Song</keyname><forenames>Qingyu</forenames></author><author><keyname>Lin</keyname><forenames>Wei</forenames></author><author><keyname>Xu</keyname><forenames>Hong</forenames></author></authors><title>Learning Provably Improves the Convergence of Gradient Descent</title><categories>cs.LG math.OC</categories><comments>46 pages, 11 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As a specialized branch of deep learning, Learning to Optimize (L2O) tackles optimization problems by training DNN-based solvers. Despite achieving significant success in various scenarios, such as faster convergence in solving convex optimizations and improved optimality in addressing non-convex cases, there remains a deficiency in theoretical support. Current research heavily relies on stringent assumptions that do not align with the intricacies of the training process. To address this gap, our study aims to establish L2O's convergence through its training methodology. We demonstrate that learning an algorithm's hyperparameters significantly enhances its convergence. Focusing on the gradient descent (GD) algorithm for quadratic programming, we prove the convergence of L2O's training using the neural tangent kernel theory. Moreover, we conduct empirical evaluations using synthetic datasets. Our findings indicate exceeding 50\% outperformance over the GD methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18113</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18113</id><created>2025-01-29</created><updated>2025-02-02</updated><authors><author><keyname>Kurzynski</keyname><forenames>Marco</forenames></author><author><keyname>Sinclair</keyname><forenames>Matthew D.</forenames></author></authors><title>Adding MFMA Support to gem5</title><categories>cs.AR</categories><comments>7 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this work we have enhanced gem5's GPU model support to add Matrix Core Engines (MCEs). Specifically, on the AMD MI200 and MI300 GPUs that gem5 supports, these MCEs perform Matrix Fused Multiply Add (MFMA) instructions for a variety of precisions. By adding this support, our changes enable running state-of-the-art ML workloads in gem5, as well as examining how MCE optimizations impact the behavior of future systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18124</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18124</id><created>2025-01-29</created><updated>2025-02-02</updated><authors><author><keyname>Shao</keyname><forenames>Liangjing</forenames></author><author><keyname>Chen</keyname><forenames>Benshuang</forenames></author><author><keyname>Zhao</keyname><forenames>Shuting</forenames></author><author><keyname>Chen</keyname><forenames>Xinrong</forenames></author></authors><title>REMOTE: Real-time Ego-motion Tracking for Various Endoscopes via   Multimodal Visual Feature Learning</title><categories>cs.CV cs.AI</categories><comments>Accepted by ICRA 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Real-time ego-motion tracking for endoscope is a significant task for efficient navigation and robotic automation of endoscopy. In this paper, a novel framework is proposed to perform real-time ego-motion tracking for endoscope. Firstly, a multi-modal visual feature learning network is proposed to perform relative pose prediction, in which the motion feature from the optical flow, the scene features and the joint feature from two adjacent observations are all extracted for prediction. Due to more correlation information in the channel dimension of the concatenated image, a novel feature extractor is designed based on an attention mechanism to integrate multi-dimensional information from the concatenation of two continuous frames. To extract more complete feature representation from the fused features, a novel pose decoder is proposed to predict the pose transformation from the concatenated feature map at the end of the framework. At last, the absolute pose of endoscope is calculated based on relative poses. The experiment is conducted on three datasets of various endoscopic scenes and the results demonstrate that the proposed method outperforms state-of-the-art methods. Besides, the inference speed of the proposed method is over 30 frames per second, which meets the real-time requirement. The project page is here: remote-bmxs.netlify.app </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18138</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18138</id><created>2025-01-30</created><updated>2025-02-02</updated><authors><author><keyname>Kim</keyname><forenames>Woojun</forenames></author><author><keyname>Sycara</keyname><forenames>Katia</forenames></author></authors><title>B3C: A Minimalist Approach to Offline Multi-Agent Reinforcement Learning</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Overestimation arising from selecting unseen actions during policy evaluation is a major challenge in offline reinforcement learning (RL). A minimalist approach in the single-agent setting -- adding behavior cloning (BC) regularization to existing online RL algorithms -- has been shown to be effective; however, this approach is understudied in multi-agent settings. In particular, overestimation becomes worse in multi-agent settings due to the presence of multiple actions, resulting in the BC regularization-based approach easily suffering from either over-regularization or critic divergence. To address this, we propose a simple yet effective method, Behavior Cloning regularization with Critic Clipping (B3C), which clips the target critic value in policy evaluation based on the maximum return in the dataset and pushes the limit of the weight on the RL objective over BC regularization, thereby improving performance. Additionally, we leverage existing value factorization techniques, particularly non-linear factorization, which is understudied in offline settings. Integrated with non-linear value factorization, B3C outperforms state-of-the-art algorithms on various offline multi-agent benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18158</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18158</id><created>2025-01-30</created><updated>2025-02-03</updated><authors><author><keyname>Lei</keyname><forenames>Yuchen</forenames></author><author><keyname>Xiang</keyname><forenames>Yuexin</forenames></author><author><keyname>Wang</keyname><forenames>Qin</forenames></author><author><keyname>Dowsley</keyname><forenames>Rafael</forenames></author><author><keyname>Yuen</keyname><forenames>Tsz Hon</forenames></author><author><keyname>Yu</keyname><forenames>Jiangshan</forenames></author></authors><title>Large Language Models for Cryptocurrency Transaction Analysis: A Bitcoin   Case Study</title><categories>cs.CR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryptocurrencies are widely used, yet current methods for analyzing transactions heavily rely on opaque, black-box models. These lack interpretability and adaptability, failing to effectively capture behavioral patterns. Many researchers, including us, believe that Large Language Models (LLMs) could bridge this gap due to their robust reasoning abilities for complex tasks. In this paper, we test this hypothesis by applying LLMs to real-world cryptocurrency transaction graphs, specifically within the Bitcoin network. We introduce a three-tiered framework to assess LLM capabilities: foundational metrics, characteristic overview, and contextual interpretation. This includes a new, human-readable graph representation format, LLM4TG, and a connectivity-enhanced sampling algorithm, CETraS, which simplifies larger transaction graphs. Experimental results show that LLMs excel at foundational metrics and offer detailed characteristic overviews. Their effectiveness in contextual interpretation suggests they can provide useful explanations of transaction behaviors, even with limited labeled data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18170</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18170</id><created>2025-01-30</created><updated>2025-01-31</updated><authors><author><keyname>Peng</keyname><forenames>Jie</forenames></author><author><keyname>Zhou</keyname><forenames>Shuang</forenames></author><author><keyname>Yang</keyname><forenames>Longwei</forenames></author><author><keyname>Song</keyname><forenames>Yiran</forenames></author><author><keyname>Zhang</keyname><forenames>Mohan</forenames></author><author><keyname>Zhou</keyname><forenames>Kaixiong</forenames></author><author><keyname>Xie</keyname><forenames>Feng</forenames></author><author><keyname>Lin</keyname><forenames>Mingquan</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Chen</keyname><forenames>Tianlong</forenames></author></authors><title>Continually Evolved Multimodal Foundation Models for Cancer Prognosis</title><categories>cs.LG</categories><comments>9 pages, 1 figure</comments><msc-class>I.2.7, J.3</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cancer prognosis is a critical task that involves predicting patient outcomes and survival rates. To enhance prediction accuracy, previous studies have integrated diverse data modalities, such as clinical notes, medical images, and genomic data, leveraging their complementary information. However, existing approaches face two major limitations. First, they struggle to incorporate newly arrived data with varying distributions into training, such as patient records from different hospitals, thus rendering sub-optimal generalizability and limited utility in real-world applications. Second, most multimodal integration methods rely on simplistic concatenation or task-specific pipelines, which fail to capture the complex interdependencies across modalities. To address these, we propose a continually evolving multi-modal foundation model. Extensive experiments on the TCGA dataset demonstrate the effectiveness of our approach, highlighting its potential to advance cancer prognosis by enabling robust and adaptive multimodal integration. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18288</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18288</id><created>2025-01-30</created><authors><author><keyname>Komijani</keyname><forenames>Javad</forenames></author><author><keyname>Marinkovic</keyname><forenames>Marina K.</forenames></author></authors><title>Normalizing flows for SU($N$) gauge theories employing singular value   decomposition</title><categories>hep-lat cs.LG</categories><comments>9 pages, 4 figures; contribution to the 41th International Symposium   on Lattice Field Theory, 2024, Liverpool, UK</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a progress report on the use of normalizing flows for generating gauge field configurations in pure SU(N) gauge theories. We discuss how the singular value decomposition can be used to construct gauge-invariant quantities, which serve as the building blocks for designing gauge-equivariant transformations of SU(N) gauge links. Using this novel approach, we build representative models for the SU(3) Wilson action on a \( 4^4 \) lattice with \( \beta = 1 \). We train these models and provide an analysis of their performance, highlighting the effectiveness of the new technique for gauge-invariant transformations. We also provide a comparison between the efficiency of the proposed algorithm and the spectral flow of Wilson loops. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18363</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18363</id><created>2025-01-30</created><updated>2025-02-02</updated><authors><author><keyname>Xi</keyname><forenames>Huajun</forenames></author><author><keyname>Liu</keyname><forenames>Kangdao</forenames></author><author><keyname>Zeng</keyname><forenames>Hao</forenames></author><author><keyname>Sun</keyname><forenames>Wenguang</forenames></author><author><keyname>Wei</keyname><forenames>Hongxin</forenames></author></authors><title>Robust Online Conformal Prediction under Uniform Label Noise</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conformal prediction is an emerging technique for uncertainty quantification that constructs prediction sets guaranteed to contain the true label with a predefined probability. Recent work develops online conformal prediction methods that adaptively construct prediction sets to accommodate distribution shifts. However, existing algorithms typically assume perfect label accuracy which rarely holds in practice. In this work, we investigate the robustness of online conformal prediction under uniform label noise with a known noise rate, in both constant and dynamic learning rate schedules. We show that label noise causes a persistent gap between the actual mis-coverage rate and the desired rate $\alpha$, leading to either overestimated or underestimated coverage guarantees. To address this issue, we propose Noise Robust Online Conformal Prediction (dubbed NR-OCP) by updating the threshold with a novel robust pinball loss, which provides an unbiased estimate of clean pinball loss without requiring ground-truth labels. Our theoretical analysis shows that NR-OCP eliminates the coverage gap in both constant and dynamic learning rate schedules, achieving a convergence rate of $\mathcal{O}(T^{-1/2})$ for both empirical and expected coverage errors under uniform label noise. Extensive experiments demonstrate the effectiveness of our method by achieving both precise coverage and improved efficiency. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18369</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18369</id><created>2025-01-30</created><authors><author><keyname>Solé</keyname><forenames>Àlex</forenames></author><author><keyname>Mosella-Montoro</keyname><forenames>Albert</forenames></author><author><keyname>Cardona</keyname><forenames>Joan</forenames></author><author><keyname>Gómez-Coca</keyname><forenames>Silvia</forenames></author><author><keyname>Aravena</keyname><forenames>Daniel</forenames></author><author><keyname>Ruiz</keyname><forenames>Eliseo</forenames></author><author><keyname>Ruiz-Hidalgo</keyname><forenames>Javier</forenames></author></authors><title>A Cartesian Encoding Graph Neural Network for Crystal Structures   Property Prediction: Application to Thermal Ellipsoid Estimation</title><categories>cs.LG</categories><doi>10.1039/D4DD00352G</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In diffraction-based crystal structure analysis, thermal ellipsoids, quantified via Anisotropic Displacement Parameters (ADPs), are critical yet challenging to determine. ADPs capture atomic vibrations, reflecting thermal and structural properties, but traditional computation is often expensive. This paper introduces CartNet, a novel graph neural network (GNN) for efficiently predicting crystal properties by encoding atomic geometry into Cartesian coordinates alongside the crystal temperature. CartNet integrates a neighbour equalization technique to emphasize covalent and contact interactions, and a Cholesky-based head to ensure valid ADP predictions. We also propose a rotational SO(3) data augmentation strategy during training to handle unseen orientations. An ADP dataset with over 200,000 experimental crystal structures from the Cambridge Structural Database (CSD) was curated to validate the approach. CartNet significantly reduces computational costs and outperforms existing methods in ADP prediction by 10.87%, while delivering a 34.77% improvement over theoretical approaches. We further evaluated CartNet on other datasets covering formation energy, band gap, total energy, energy above the convex hull, bulk moduli, and shear moduli, achieving 7.71% better results on the Jarvis Dataset and 13.16% on the Materials Project Dataset. These gains establish CartNet as a state-of-the-art solution for diverse crystal property predictions. Project website and online demo: https://www.ee.ub.edu/cartnet </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18387</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18387</id><created>2025-01-30</created><updated>2025-02-02</updated><authors><author><keyname>Biçer</keyname><forenames>Osman</forenames></author><author><keyname>Ajorian</keyname><forenames>Ali</forenames></author></authors><title>AuthOr: Lower Cost Authenticity-Oriented Garbling for Arbitrary Boolean   Circuits</title><categories>cs.CR</categories><comments>garbled circuits, privacy-free garbling, verifiable computing,   zero-knowledge proofs</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Authenticity-oriented (perviously named as privacy-free) garbling schemes (Frederiksen et al. Eurocrypt '15) are designed to satisfy only the authenticity criterion of (Bellare et al. ACM CCS '12), and to be more efficient compared to full-fledged garbling schemes. Here we report a novel authenticity-oriented garbling scheme ``AuthOr'' for general circuits, which has strictly lower communication cost than the state-of-the-art authenticity-oriented version of half gates garbling (Zahur et al. Crypto '15) without any further computation overhead or security assumption. Our solution successfully combines the ideas from information theoretical garbling (Kondi and Patra Crypto '17), half gates garbling, and a novel bandwidth-free AND gate garbling scheme (which we also propose here) while remaining compatible with FreeXOR (Kolesnikov and Schneider ICALP '08). While our scheme beats half gates garbling both at communication and computation costs for many circuits, the exact efficiency improvement can be seen empirically as it depends on the circuit. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18417</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18417</id><created>2025-01-30</created><updated>2025-02-03</updated><authors><author><keyname>Luzio</keyname><forenames>Emanuele</forenames></author><author><keyname>Ponti</keyname><forenames>Moacir Antonelli</forenames></author></authors><title>Real-Time Anomaly Detection with Synthetic Anomaly Monitoring (SAM)</title><categories>cs.LG</categories><comments>19 pages, 3 figures, submitted for publication</comments><msc-class>62H30, 68T05, 62G99, 91G80, 68M10</msc-class><acm-class>I.5.4; K.6.5; I.2.6; H.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anomaly detection is essential for identifying rare and significant events across diverse domains such as finance, cybersecurity, and network monitoring. This paper presents Synthetic Anomaly Monitoring (SAM), an innovative approach that applies synthetic control methods from causal inference to improve both the accuracy and interpretability of anomaly detection processes. By modeling normal behavior through the treatment of each feature as a control unit, SAM identifies anomalies as deviations within this causal framework. We conducted extensive experiments comparing SAM with established benchmark models, including Isolation Forest, Local Outlier Factor (LOF), k-Nearest Neighbors (kNN), and One-Class Support Vector Machine (SVM), across five diverse datasets, including Credit Card Fraud, HTTP Dataset CSIC 2010, and KDD Cup 1999, among others. Our results demonstrate that SAM consistently delivers robust performance, highlighting its potential as a powerful tool for real-time anomaly detection in dynamic and complex environments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18612</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18612</id><created>2025-01-24</created><updated>2025-02-02</updated><authors><author><keyname>Su</keyname><forenames>Yuheng</forenames></author><author><keyname>Yang</keyname><forenames>Qiusong</forenames></author><author><keyname>Ci</keyname><forenames>Yiwei</forenames></author><author><keyname>Li</keyname><forenames>Yingcheng</forenames></author><author><keyname>Bu</keyname><forenames>Tianjun</forenames></author><author><keyname>Huang</keyname><forenames>Ziyu</forenames></author></authors><title>Deeply Optimizing the SAT Solver for the IC3 Algorithm</title><categories>cs.LO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The IC3 algorithm, also known as PDR, is a SAT-based model checking algorithm that has significantly influenced the field in recent years due to its efficiency, scalability, and completeness. It utilizes SAT solvers to solve a series of SAT queries associated with relative induction. In this paper, we introduce several optimizations for the SAT solver in IC3 based on our observations of the unique characteristics of these SAT queries. By observing that SAT queries do not necessarily require decisions on all variables, we compute a subset of variables that need to be decided before each solving process while ensuring that the result remains unaffected. Additionally, noting that the overhead of binary heap operations in VSIDS is non-negligible, we replace the binary heap with buckets to achieve constant-time operations. Furthermore, we support temporary clauses without the need to allocate a new activation variable for each solving process, thereby eliminating the need to reset solvers. We developed a novel lightweight CDCL SAT solver, GipSAT, which integrates these optimizations. A comprehensive evaluation highlights the performance improvements achieved by GipSAT. Specifically, the GipSAT-based IC3 demonstrates an average speedup of 3.61 times in solving time compared to the IC3 implementation based on MiniSat. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18644</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18644</id><created>2025-01-29</created><authors><author><keyname>Opaluwah</keyname><forenames>Adeyola</forenames></author></authors><title>Prompt-oriented Output of Culture-Specific Items in Translated African   Poetry by Large Language Model: An Initial Multi-layered Tabular Review</title><categories>cs.CL</categories><comments>24 pages, 4 tables. arXiv admin note: text overlap with   arXiv:2406.03450, arXiv:2312.15304 by other authors</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper examines the output of cultural items generated by Chat Generative PreTrained Transformer Pro in response to three structured prompts to translate three anthologies of African poetry. The first prompt was broad, the second focused on poetic structure, and the third prompt emphasized cultural specificity. To support this analysis, four comparative tables were created. The first table presents the results of the cultural items produced after the three prompts, the second categorizes these outputs based on Aixela framework of Proper nouns and Common expressions, the third table summarizes the cultural items generated by human translators, a custom translation engine, and a Large Language Model. The final table outlines the strategies employed by Chat Generative PreTrained Transformer Pro following the culture specific prompt. Compared to the outputs of cultural items from reference human translation and the custom translation engine in prior studies the findings indicate that the culture oriented prompts used with Chat Generative PreTrained Transformer Pro did not yield significant enhancements of cultural items during the translation of African poetry from English to French. Among the fifty four cultural items, the human translation produced thirty three cultural items in repetition, the custom translation engine generated Thirty eight cultural items in repetition while Chat Generative PreTrained Transformer Pro produced forty one cultural items in repetition. The untranslated cultural items revealed inconsistencies in Large language models approach to translating cultural items in African poetry from English to French. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18712</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18712</id><created>2025-01-30</created><updated>2025-02-03</updated><authors><author><keyname>Bhardwaj</keyname><forenames>Devansh</forenames></author><author><keyname>Mishra</keyname><forenames>Naman</forenames></author></authors><title>Invisible Traces: Using Hybrid Fingerprinting to identify underlying   LLMs in GenAI Apps</title><categories>cs.LG cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Fingerprinting refers to the process of identifying underlying Machine Learning (ML) models of AI Systemts, such as Large Language Models (LLMs), by analyzing their unique characteristics or patterns, much like a human fingerprint. The fingerprinting of Large Language Models (LLMs) has become essential for ensuring the security and transparency of AI-integrated applications. While existing methods primarily rely on access to direct interactions with the application to infer model identity, they often fail in real-world scenarios involving multi-agent systems, frequent model updates, and restricted access to model internals. In this paper, we introduce a novel fingerprinting framework designed to address these challenges by integrating static and dynamic fingerprinting techniques. Our approach identifies architectural features and behavioral traits, enabling accurate and robust fingerprinting of LLMs in dynamic environments. We also highlight new threat scenarios where traditional fingerprinting methods are ineffective, bridging the gap between theoretical techniques and practical application. To validate our framework, we present an extensive evaluation setup that simulates real-world conditions and demonstrate the effectiveness of our methods in identifying and monitoring LLMs in Gen-AI applications. Our results highlight the framework's adaptability to diverse and evolving deployment contexts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18853</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18853</id><created>2025-01-30</created><updated>2025-02-03</updated><authors><author><keyname>Sun</keyname><forenames>Shuai</forenames></author></authors><title>Non-Asymptotic Analysis of Subspace Identification for Stochastic   Systems Using Multiple Trajectories</title><categories>eess.SY cs.SY</categories><comments>23 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the analysis of identification errors for $n$-dimensional discrete-time Linear Time-Invariant (LTI) systems with $m$ outputs and no external inputs, using Subspace Identification Methods (SIM) with finite sample data. We provide non-asymptotic high-probability upper bounds for matrices $A,C$, the Kalman filter gain $K$, and the closed loop matrix $A-KC $, based on multiple sample trajectories, and further give the first non-asymptotic high-probability upper bounds for the system poles, which cover both (marginally) stable systems and unstable systems. We show that, with high probability, the non-asymptotic estimation errors of these matrices decay at a rate of at least $ \mathcal{O}(\sqrt{1/N}) $, while the estimation error of the system poles decays at a rate of at least $ \mathcal{O}(N^{\frac{1}{2n}}) $, where $ N $ represents the number of sample trajectories. Furthermore, we prove that SIMs become ill-conditioned when the ratio $n/m$ is large, regardless of the system parameters. Numerical experiments are conducted to validate the non-asymptotic results and the ill-conditionedness of SIM. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18867</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18867</id><created>2025-01-30</created><updated>2025-02-02</updated><authors><author><keyname>Zhang</keyname><forenames>Jianke</forenames></author><author><keyname>Guo</keyname><forenames>Yanjiang</forenames></author><author><keyname>Hu</keyname><forenames>Yucheng</forenames></author><author><keyname>Chen</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Zhu</keyname><forenames>Xiang</forenames></author><author><keyname>Chen</keyname><forenames>Jianyu</forenames></author></authors><title>UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent</title><categories>cs.CV cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent advancements in Vision-Language-Action (VLA) models have leveraged pre-trained Vision-Language Models (VLMs) to improve the generalization capabilities. VLMs, typically pre-trained on vision-language understanding tasks, provide rich semantic knowledge and reasoning abilities. However, prior research has shown that VLMs often focus on high-level semantic content and neglect low-level features, limiting their ability to capture detailed spatial information and understand physical dynamics. These aspects, which are crucial for embodied control tasks, remain underexplored in existing pre-training paradigms. In this paper, we investigate the training paradigm for VLAs, and introduce \textbf{UP-VLA}, a \textbf{U}nified VLA model training with both multi-modal \textbf{U}nderstanding and future \textbf{P}rediction objectives, enhancing both high-level semantic comprehension and low-level spatial understanding. Experimental results show that UP-VLA achieves a 33% improvement on the Calvin ABC-D benchmark compared to the previous state-of-the-art method. Additionally, UP-VLA demonstrates improved success rates in real-world manipulation tasks, particularly those requiring precise spatial information. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18951</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18951</id><created>2025-01-31</created><updated>2025-02-02</updated><authors><author><keyname>Gui</keyname><forenames>Xinyue</forenames></author><author><keyname>Xia</keyname><forenames>Ding</forenames></author><author><keyname>Gao</keyname><forenames>Wang</forenames></author><author><keyname>Dogan</keyname><forenames>Mustafa Doga</forenames></author><author><keyname>Larsson</keyname><forenames>Maria</forenames></author><author><keyname>Igarashi</keyname><forenames>Takeo</forenames></author></authors><title>Draw2Cut: Direct On-Material Annotations for CNC Milling</title><categories>cs.HC</categories><comments>Accepted by CHI 2025</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Creating custom artifacts with computer numerical control (CNC) milling machines typically requires mastery of complex computer-aided design (CAD) software. To eliminate this user barrier, we introduced Draw2Cut, a novel system that allows users to design and fabricate artifacts by sketching directly on physical materials. Draw2Cut employs a custom-drawing language to convert user-drawn lines, symbols, and colors into toolpaths, thereby enabling users to express their creative intent intuitively. The key features include real-time alignment between material and virtual toolpaths, a preview interface for validation, and an open-source platform for customization. Through technical evaluations and user studies, we demonstrate that Draw2Cut lowers the entry barrier for personal fabrication, enabling novices to create customized artifacts with precision and ease. Our findings highlight the potential of the system to enhance creativity, engagement, and accessibility in CNC-based woodworking. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.19010</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.19010</id><created>2025-01-31</created><updated>2025-02-03</updated><authors><author><keyname>Lee</keyname><forenames>Wonjun</forenames></author><author><keyname>Im</keyname><forenames>Solee</forenames></author><author><keyname>Do</keyname><forenames>Heejin</forenames></author><author><keyname>Kim</keyname><forenames>Yunsu</forenames></author><author><keyname>Ok</keyname><forenames>Jungseul</forenames></author><author><keyname>Lee</keyname><forenames>Gary Geunbae</forenames></author></authors><title>DyPCL: Dynamic Phoneme-level Contrastive Learning for Dysarthric Speech   Recognition</title><categories>cs.CL cs.SD eess.AS</categories><comments>NAACL 2025 main conference, 9pages, 1 page appendix</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Dysarthric speech recognition often suffers from performance degradation due to the intrinsic diversity of dysarthric severity and extrinsic disparity from normal speech. To bridge these gaps, we propose a Dynamic Phoneme-level Contrastive Learning (DyPCL) method, which leads to obtaining invariant representations across diverse speakers. We decompose the speech utterance into phoneme segments for phoneme-level contrastive learning, leveraging dynamic connectionist temporal classification alignment. Unlike prior studies focusing on utterance-level embeddings, our granular learning allows discrimination of subtle parts of speech. In addition, we introduce dynamic curriculum learning, which progressively transitions from easy negative samples to difficult-to-distinguishable negative samples based on phonetic similarity of phoneme. Our approach to training by difficulty levels alleviates the inherent variability of speakers, better identifying challenging speeches. Evaluated on the UASpeech dataset, DyPCL outperforms baseline models, achieving an average 22.10\% relative reduction in word error rate (WER) across the overall dysarthria group. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.19018</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.19018</id><created>2025-01-31</created><updated>2025-02-03</updated><authors><author><keyname>Kadhim</keyname><forenames>Ahmed K.</forenames></author><author><keyname>Jiao</keyname><forenames>Lei</forenames></author><author><keyname>Shafik</keyname><forenames>Rishad</forenames></author><author><keyname>Granmo</keyname><forenames>Ole-Christoffer</forenames></author><author><keyname>Bhattarai</keyname><forenames>Bimal</forenames></author></authors><title>Scalable Multi-phase Word Embedding Using Conjunctive Propositional   Clauses</title><categories>cs.LG cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Tsetlin Machine (TM) architecture has recently demonstrated effectiveness in Machine Learning (ML), particularly within Natural Language Processing (NLP). It has been utilized to construct word embedding using conjunctive propositional clauses, thereby significantly enhancing our understanding and interpretation of machine-derived decisions. The previous approach performed the word embedding over a sequence of input words to consolidate the information into a cohesive and unified representation. However, that approach encounters scalability challenges as the input size increases. In this study, we introduce a novel approach incorporating two-phase training to discover contextual embeddings of input sequences. Specifically, this method encapsulates the knowledge for each input word within the dataset's vocabulary, subsequently constructing embeddings for a sequence of input words utilizing the extracted knowledge. This technique not only facilitates the design of a scalable model but also preserves interpretability. Our experimental findings revealed that the proposed method yields competitive performance compared to the previous approaches, demonstrating promising results in contrast to human-generated benchmarks. Furthermore, we applied the proposed approach to sentiment analysis on the IMDB dataset, where the TM embedding and the TM classifier, along with other interpretable classifiers, offered a transparent end-to-end solution with competitive performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.19047</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.19047</id><created>2025-01-31</created><updated>2025-02-03</updated><authors><author><keyname>Pavlovic</keyname><forenames>Maja</forenames></author></authors><title>Understanding Model Calibration -- A gentle introduction and visual   exploration of calibration and the expected calibration error (ECE)</title><categories>stat.ME cs.AI cs.CV cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some issues with a measure that is still widely used to evaluate calibration. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.19060</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.19060</id><created>2025-01-31</created><updated>2025-02-03</updated><authors><author><keyname>Lv</keyname><forenames>Song-Lin</forenames></author><author><keyname>Chen</keyname><forenames>Yu-Yang</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi</forenames></author><author><keyname>Li</keyname><forenames>Yu-Feng</forenames></author><author><keyname>Guo</keyname><forenames>Lan-Zhe</forenames></author></authors><title>Contrast-Aware Calibration for Fine-Tuned CLIP: Leveraging Image-Text   Alignment</title><categories>cs.CV cs.LG</categories><comments>We are withdrawing the paper due to comments indicating overlap with   parts of another paper. We will revise the appendix and submit a new version   after addressing the issue</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Vision-language models (VLMs), such as CLIP, have demonstrated exceptional generalization capabilities and can quickly adapt to downstream tasks through prompt fine-tuning. Unfortunately, in classification tasks involving non-training classes, known as open-vocabulary setting, fine-tuned VLMs often overfit to train classes, resulting in a misalignment between confidence scores and actual accuracy on unseen classes, which significantly undermines their reliability in real-world deployments. Existing confidence calibration methods typically require training parameters or analyzing features from the training dataset, restricting their ability to generalize unseen classes without corresponding train data. Moreover, VLM-specific calibration methods rely solely on text features from train classes as calibration indicators, which inherently limits their ability to calibrate train classes. To address these challenges, we propose an effective multimodal calibration method Contrast-Aware Calibration (CAC). Building on the original CLIP's zero-shot adaptability and the conclusion from empirical analysis that poor intra-class and inter-class discriminative ability on unseen classes is the root cause, we calculate calibration weights based on the contrastive difference between the original and fine-tuned CLIP. This method not only adapts to calibrating unseen classes but also overcomes the limitations of previous VLM calibration methods that could not calibrate train classes. In experiments involving 11 datasets with 5 fine-tuning methods, CAC consistently achieved the best calibration effect on both train and unseen classes without sacrificing accuracy and inference speed. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.19111</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.19111</id><created>2025-01-31</created><updated>2025-02-03</updated><authors><author><keyname>Lai</keyname><forenames>Zhengqin</forenames></author><author><keyname>Hong</keyname><forenames>Xiaopeng</forenames></author><author><keyname>Wang</keyname><forenames>Yabin</forenames></author><author><keyname>Li</keyname><forenames>Xiaobai</forenames></author></authors><title>A Benchmark for Incremental Micro-expression Recognition</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Micro-expression recognition plays a pivotal role in understanding hidden emotions and has applications across various fields. Traditional recognition methods assume access to all training data at once, but real-world scenarios involve continuously evolving data streams. To respond to the requirement of adapting to new data while retaining previously learned knowledge, we introduce the first benchmark specifically designed for incremental micro-expression recognition. Our contributions include: Firstly, we formulate the incremental learning setting tailored for micro-expression recognition. Secondly, we organize sequential datasets with carefully curated learning orders to reflect real-world scenarios. Thirdly, we define two cross-evaluation-based testing protocols, each targeting distinct evaluation objectives. Finally, we provide six baseline methods and their corresponding evaluation results. This benchmark lays the groundwork for advancing incremental micro-expression recognition research. All source code used in this study will be publicly available at https://github.com/ZhengQinLai/IMER-benchmark. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.19245</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.19245</id><created>2025-01-31</created><updated>2025-02-03</updated><authors><author><keyname>Aydın</keyname><forenames>Hüseyin</forenames></author><author><keyname>Godin-Dubois</keyname><forenames>Kevin</forenames></author><author><keyname>Braz</keyname><forenames>Libio Goncalvez</forenames></author><author><keyname>Hengst</keyname><forenames>Floris den</forenames></author><author><keyname>Baraka</keyname><forenames>Kim</forenames></author><author><keyname>Çelikok</keyname><forenames>Mustafa Mert</forenames></author><author><keyname>Sauter</keyname><forenames>Andreas</forenames></author><author><keyname>Wang</keyname><forenames>Shihan</forenames></author><author><keyname>Oliehoek</keyname><forenames>Frans A.</forenames></author></authors><title>SHARPIE: A Modular Framework for Reinforcement Learning and Human-AI   Interaction Experiments</title><categories>cs.AI cs.HC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Reinforcement learning (RL) offers a general approach for modeling and training AI agents, including human-AI interaction scenarios. In this paper, we propose SHARPIE (Shared Human-AI Reinforcement Learning Platform for Interactive Experiments) to address the need for a generic framework to support experiments with RL agents and humans. Its modular design consists of a versatile wrapper for RL environments and algorithm libraries, a participant-facing web interface, logging utilities, deployment on popular cloud and participant recruitment platforms. It empowers researchers to study a wide variety of research questions related to the interaction between humans and RL agents, including those related to interactive reward specification and learning, learning from human feedback, action delegation, preference elicitation, user-modeling, and human-AI teaming. The platform is based on a generic interface for human-RL interactions that aims to standardize the field of study on RL in human contexts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.19306</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.19306</id><created>2025-01-31</created><updated>2025-02-03</updated><authors><author><keyname>Chen</keyname><forenames>Jiefeng</forenames></author><author><keyname>Ren</keyname><forenames>Jie</forenames></author><author><keyname>Chen</keyname><forenames>Xinyun</forenames></author><author><keyname>Yang</keyname><forenames>Chengrun</forenames></author><author><keyname>Sun</keyname><forenames>Ruoxi</forenames></author><author><keyname>Arık</keyname><forenames>Sercan Ö</forenames></author></authors><title>SETS: Leveraging Self-Verification and Self-Correction for Improved   Test-Time Scaling</title><categories>cs.AI cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent advancements in Large Language Models (LLMs) have created new opportunities to enhance performance on complex reasoning tasks by leveraging test-time computation. However, conventional approaches such as repeated sampling with majority voting or reward model scoring, often face diminishing returns as test-time compute scales, in addition to requiring costly task-specific reward model training. In this paper, we present Self-Enhanced Test-Time Scaling (SETS), a novel method that leverages the self-verification and self-correction capabilities of recent advanced LLMs to overcome these limitations. SETS integrates sampling, self-verification, and self-correction into a unified framework, enabling efficient and scalable test-time computation for improved capabilities at complex tasks. Through extensive experiments on challenging planning and reasoning benchmarks, compared to the alternatives, we demonstrate that SETS achieves significant performance improvements and more favorable test-time scaling laws. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00001</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00001</id><created>2024-12-19</created><authors><author><keyname>Chowdhury</keyname><forenames>Md Rownak Hossain</forenames></author><author><keyname>Rahman</keyname><forenames>Mostafizur</forenames></author></authors><title>Accelerating PageRank Algorithmic Tasks with a new Programmable Hardware   Architecture</title><categories>cs.AR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Addressing the growing demands of artificial intelligence (AI) and data analytics requires new computing approaches. In this paper, we propose a reconfigurable hardware accelerator designed specifically for AI and data-intensive applications. Our architecture features a messaging-based intelligent computing scheme that allows for dynamic programming at runtime using a minimal instruction set. To assess our hardware's effectiveness, we conducted a case study in TSMC 28nm technology node. The simulation-based study involved analyzing a protein network using the computationally demanding PageRank algorithm. The results demonstrate that our hardware can analyze a 5,000-node protein network in just 213.6 milliseconds over 100 iterations. These outcomes signify the potential of our design to achieve cutting-edge performance in next-generation AI applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00002</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00002</id><created>2025-01-03</created><authors><author><keyname>de Almeida</keyname><forenames>Duarte Sampaio</forenames></author><author><keyname>Abreu</keyname><forenames>Fernando Brito e</forenames></author><author><keyname>Boavida-Portugal</keyname><forenames>Inês</forenames></author></authors><title>Digital twins in tourism: a systematic literature review</title><categories>cs.CY cs.DL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Purpose: This systematic literature review (SLR) characterizes the current state of the art on digital twinning (DT) technology in tourism-related applications. We aim to evaluate the types of DTs described in the literature, identifying their purposes, the areas of tourism where they have been proposed, their main components, and possible future directions based on current work.   Design/methodology/approach: We conducted this SLR with bibliometric analysis based on an existing, validated methodology. Thirty-four peer-reviewed studies from three major scientific databases were selected for review. They were categorized using a taxonomy that included tourism type, purpose, spatial scale, data sources, data linkage, visualization, and application.   Findings: The topic is at an early, evolving stage, as the oldest study found dates back to 2021. Most reviewed studies deal with cultural tourism, focusing on digitising cultural heritage. Destination management is the primary purpose of these DTs, with mainly site-level spatial scales. In many studies, the physical-digital data linkage is unilateral, lacking twin synchronization. In most DTs considered bilateral, the linkage is indirect. There are more applied than theoretical studies, suggesting progress in applying DTs in the field. Finally, there is an extensive research gap regarding DT technology in tourism, which is worth filling.   Originality/Value: This paper presents a novel SLR with a bibliometric analysis of DTs' applied and theoretical application in tourism. Each reviewed publication is assessed and characterized, identifying the current state of the topic, possible research gaps, and future directions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00003</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00003</id><created>2025-01-03</created><authors><author><keyname>Pistillo</keyname><forenames>Matteo</forenames></author><author><keyname>Villalobos</keyname><forenames>Pablo</forenames></author></authors><title>Defending Compute Thresholds Against Legal Loopholes</title><categories>cs.CY cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing legal frameworks on AI rely on training compute thresholds as a proxy to identify potentially-dangerous AI models and trigger increased regulatory attention. In the United States, Section 4.2(a) of Executive Order 14110 instructs the Secretary of Commerce to require extensive reporting from developers of AI models above a certain training compute threshold. In the European Union, Article 51 of the AI Act establishes a presumption that AI models above a certain compute threshold have high impact capabilities and hence pose systemic risk, thus subjecting their developers to several obligations including capability evaluations, reporting, and incident monitoring. In this paper, we examine some enhancement techniques that are capable of decreasing training compute usage while preserving, or even increasing, model capabilities. Since training compute thresholds rely on training compute as a metric and trigger for increased regulatory attention, these capability-enhancing and compute-saving techniques could constitute a legal loophole to existing training compute thresholds. In particular, we concentrate on four illustrative techniques (fine-tuning, model reuse, model expansion, and above compute-optimal inference compute) with the goal of furthering the conversation about their implications on training compute thresholds as a legal mechanism and advancing policy recommendations that could address the relevant legal loopholes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00004</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00004</id><created>2025-01-03</created><authors><author><keyname>Cao</keyname><forenames>Siyi</forenames></author><author><keyname>Zhong</keyname><forenames>Linping</forenames></author><author><keyname>Wang</keyname><forenames>Chao</forenames></author></authors><title>The Impact of Student Writing Assessment Literacy on Psychological   Factors: An Ordinal Logistic Regression Analysis</title><categories>cs.CY</categories><comments>19 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Previous studies have shown that enhanced student assessment literacy can lead to improvements in academic performance in EFL (English as a Foreign Language) writing. Additionally, psychological factors such as self-efficacy, achievement motivation, and writing anxiety significantly influence EFL writing outcomes. However, the relationship between student writing assessment literacy (SWAL) and these psychological factors remains unclear. The present study aims to explore how SWAL affects psychological factors in the Chinese EFL context. Data were collected from 103 Chinese undergraduate EFL students using four questionnaires: the Student Writing Assessment Literacy Scale (SWAL), the Self-Efficacy for Writing Scale (SEWS), the Achievement Goal Questionnaire (AGQ), and the Second Language Writing Anxiety Inventory (SLWAI). Ordinal logistic regression was employed to analyze the data. The results indicated that higher levels of SWAL were positively associated with writing self-efficacy and achievement motivation, while negatively related to writing anxiety. These findings have significant pedagogical implications for second language (L2) writing instructions, emphasizing the importance of integrating SWAL training into writing instruction to enhance students' writing experiences and outcomes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00005</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00005</id><created>2025-01-05</created><authors><author><keyname>Lukas</keyname><forenames>Christina</forenames></author></authors><title>A Study about Distribution and Acceptance of Conversational Agents for   Mental Health in Germany: Keep the Human in the Loop?</title><categories>cs.HC cs.AI cs.CY</categories><comments>Master's thesis</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Good mental health enables individuals to cope with the normal stresses of life. In Germany, approximately one-quarter of the adult population is affected by mental illnesses. Teletherapy and digital health applications are available to bridge gaps in care and relieve healthcare professionals. The acceptance of these tools is a strongly influencing factor for their effectiveness, which also needs to be evaluated for AI-based conversational agents (CAs) (e. g. ChatGPT, Siri) to assess the risks and potential for integration into therapeutic practice. This study investigates the perspectives of both the general population and healthcare professionals with the following questions: 1. How frequently are CAs used for mental health? 2. How high is the acceptance of CAs in the field of mental health? 3. To what extent is the use of CAs in counselling, diagnosis, and treatment acceptable? To address these questions, two quantitative online surveys were conducted with 444 participants from the general population and 351 healthcare professionals. Statistical analyses show that 27 % of the surveyed population already confide their concerns to CAs. Not only experience with this technology but also experience with telemedicine shows a higher acceptance among both groups for using CAs for mental health. Additionally, participants from the general population were more likely to support CAs as companions controlled by healthcare professionals rather than as additional experts for the professionals. CAs have the potential to support mental health, particularly in counselling. Future research should examine the influence of different communication media and further possibilities of augmented intelligence. With the right balance between technology and human care, integration into patient-professional interaction can be achieved. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00006</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00006</id><created>2025-01-06</created><authors><author><keyname>Singh</keyname><forenames>Kuldeep</forenames></author><author><keyname>Cheemalapati</keyname><forenames>Sumanth</forenames></author><author><keyname>RamiReddy</keyname><forenames>Srikanth Reddy</forenames></author><author><keyname>Kurian</keyname><forenames>George</forenames></author><author><keyname>Muzumdar</keyname><forenames>Prathamesh</forenames></author><author><keyname>Muley</keyname><forenames>Apoorva</forenames></author></authors><title>Determinants of Human Development Index (HDI): A Regression Analysis of   Economic and Social Indicators</title><categories>cs.CY</categories><doi>10.9734/ajeba/2025/v25i11630</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This study aims to investigate the factors influencing the Human Development Index (HDI). Five variables-GDP per capita, health expenditure, education expenditure, infant mortality rate (per 1,000 live births), and average years of schooling-were analyzed to develop a regression model assessing their impact on HDI. The results indicate that GDP per capita, infant mortality rate, and average years of schooling are significant predictors of HDI. Specifically, the study finds a positive relationship between GDP per capita and average years of schooling with HDI, while infant mortality rate is negatively associated with HDI. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00007</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00007</id><created>2025-01-06</created><authors><author><keyname>Muzumdar</keyname><forenames>Prathamesh</forenames></author><author><keyname>Cheemalapati</keyname><forenames>Sumanth</forenames></author><author><keyname>RamiReddy</keyname><forenames>Srikanth Reddy</forenames></author><author><keyname>Singh</keyname><forenames>Kuldeep</forenames></author><author><keyname>Kurian</keyname><forenames>George</forenames></author><author><keyname>Muley</keyname><forenames>Apoorva</forenames></author></authors><title>The Dead Internet Theory: A Survey on Artificial Interactions and the   Future of Social Media</title><categories>cs.CY</categories><doi>10.9734/ajrcos/2025/v18i1549</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Dead Internet Theory (DIT) suggests that much of today's internet, particularly social media, is dominated by non-human activity, AI-generated content, and corporate agendas, leading to a decline in authentic human interaction. This study explores the origins, core claims, and implications of DIT, emphasizing its relevance in the context of social media platforms. The theory emerged as a response to the perceived homogenization of online spaces, highlighting issues like the proliferation of bots, algorithmically generated content, and the prioritization of engagement metrics over genuine user interaction. AI technologies play a central role in this phenomenon, as social media platforms increasingly use algorithms and machine learning to curate content, drive engagement, and maximize advertising revenue. While these tools enhance scalability and personalization, they also prioritize virality and consumption over authentic communication, contributing to the erosion of trust, the loss of content diversity, and a dehumanized internet experience. This study redefines DIT in the context of social media, proposing that the commodification of content consumption for revenue has taken precedence over meaningful human connectivity. By focusing on engagement metrics, platforms foster a sense of artificiality and disconnection, underscoring the need for human-centric approaches to revive authentic online interaction and community building. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00008</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00008</id><created>2025-01-06</created><authors><author><keyname>Salazar-Miranda</keyname><forenames>Arianna</forenames></author><author><keyname>Talen</keyname><forenames>Emily</forenames></author></authors><title>Zoning in American Cities: Are Reforms Making a Difference? An AI-based   Analysis</title><categories>cs.CY cs.CL</categories><comments>31 pages, 6 figures, 1 table</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Cities are at the forefront of addressing global sustainability challenges, particularly those exacerbated by climate change. Traditional zoning codes, which often segregate land uses, have been linked to increased vehicular dependence, urban sprawl, and social disconnection, undermining broader social and environmental sustainability objectives. This study investigates the adoption and impact of form-based codes (FBCs), which aim to promote sustainable, compact, and mixed-use urban forms as a solution to these issues. Using Natural Language Processing (NLP) techniques, we analyzed zoning documents from over 2000 U.S. census-designated places to identify linguistic patterns indicative of FBC principles. Our findings reveal widespread adoption of FBCs across the country, with notable variations within regions. FBCs are associated with higher floor-to-area ratios, narrower and more consistent street setbacks, and smaller plots. We also find that places with FBCs have improved walkability, shorter commutes, and a higher share of multi-family housing. Our findings highlight the utility of NLP for evaluating zoning codes and underscore the potential benefits of form-based zoning reforms for enhancing urban sustainability. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00009</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00009</id><created>2025-01-06</created><authors><author><keyname>Ganuthula</keyname><forenames>Venkat Ram Reddy</forenames></author></authors><title>The Solo Revolution: A Theory of AI-Enabled Individual Entrepreneurship</title><categories>cs.CY econ.GN q-fin.EC</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper presents the AI Enabled Individual Entrepreneurship Theory (AIET), a theoretical framework explaining how artificial intelligence technologies transform individual entrepreneurial capability. The theory identifies two foundational premises: knowledge democratization and resource requirements evolution. Through three core mechanisms skill augmentation, capital structure transformation, and risk profile modification AIET explains how individuals can now undertake entrepreneurial activities at scales previously requiring significant organizational infrastructure. The theory presents five testable propositions addressing the changing relationship between organizational size and competitive advantage, the expansion of individual entrepreneurial capacity, the transformation of market entry barriers, the evolution of traditional firm advantages, and the modification of entrepreneurial risk profiles. Boundary conditions related to task characteristics and market conditions define the theory's scope and applicability. The framework suggests significant implications for entrepreneurship theory, organizational design, and market structure as AI capabilities continue to advance. This theory provides a foundation for understanding the evolving landscape of entrepreneurship in an AI-enabled world. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00010</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00010</id><created>2025-01-06</created><authors><author><keyname>Qi</keyname><forenames>Changyong</forenames></author><author><keyname>Jia</keyname><forenames>Linzhao</forenames></author><author><keyname>Wei</keyname><forenames>Yuang</forenames></author><author><keyname>Jiang</keyname><forenames>Yuan-Hao</forenames></author><author><keyname>Gu</keyname><forenames>Xiaoqing</forenames></author></authors><title>IntelliChain: An Integrated Framework for Enhanced Socratic Method   Dialogue with LLMs and Knowledge Graphs</title><categories>cs.CY</categories><comments>Conference Proceedings of the 28th Global Chinese Conference on   Computers in Education, GCCCE 2024</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the continuous advancement of educational technology, the demand for Large Language Models (LLMs) as intelligent educational agents in providing personalized learning experiences is rapidly increasing. This study aims to explore how to optimize the design and collaboration of a multi-agent system tailored for Socratic teaching through the integration of LLMs and knowledge graphs in a chain-of-thought dialogue approach, thereby enhancing the accuracy and reliability of educational applications. By incorporating knowledge graphs, this research has bolstered the capability of LLMs to handle specific educational content, ensuring the accuracy and relevance of the information provided. Concurrently, we have focused on developing an effective multi-agent collaboration mechanism to facilitate efficient information exchange and chain dialogues among intelligent agents, significantly improving the quality of educational interaction and learning outcomes. In empirical research within the domain of mathematics education, this framework has demonstrated notable advantages in enhancing the accuracy and credibility of educational interactions. This study not only showcases the potential application of LLMs and knowledge graphs in mathematics teaching but also provides valuable insights and methodologies for the development of future AI-driven educational solutions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00011</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00011</id><created>2025-01-07</created><authors><author><keyname>Tjondronegoro</keyname><forenames>Dian</forenames></author></authors><title>TOAST Framework: A Multidimensional Approach to Ethical and Sustainable   AI Integration in Organizations</title><categories>cs.CY cs.AI cs.HC</categories><comments>25 pages, 1 figure</comments><msc-class>90-02</msc-class><acm-class>K.6.m</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Artificial Intelligence (AI) has emerged as a transformative technology with the potential to revolutionize various sectors, from healthcare to finance, education, and beyond. However, successfully implementing AI systems remains a complex challenge, requiring a comprehensive and methodologically sound framework. This paper contributes to this challenge by introducing the Trustworthy, Optimized, Adaptable, and Socio-Technologically harmonious (TOAST) framework. It draws on insights from various disciplines to align technical strategy with ethical values, societal responsibilities, and innovation aspirations. The TOAST framework is a novel approach designed to guide the implementation of AI systems, focusing on reliability, accountability, technical advancement, adaptability, and socio-technical harmony. By grounding the TOAST framework in healthcare case studies, this paper provides a robust evaluation of its practicality and theoretical soundness in addressing operational, ethical, and regulatory challenges in high-stakes environments, demonstrating how adaptable AI systems can enhance institutional efficiency, mitigate risks like bias and data privacy, and offer a replicable model for other sectors requiring ethically aligned and efficient AI integration. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00012</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00012</id><created>2025-01-07</created><authors><author><keyname>Kolt</keyname><forenames>Noam</forenames></author><author><keyname>Shur-Ofry</keyname><forenames>Michal</forenames></author><author><keyname>Cohen</keyname><forenames>Reuven</forenames></author></authors><title>Lessons from complexity theory for AI governance</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The study of complex adaptive systems, pioneered in physics, biology, and the social sciences, offers important lessons for AI governance. Contemporary AI systems and the environments in which they operate exhibit many of the properties characteristic of complex systems, including nonlinear growth patterns, emergent phenomena, and cascading effects that can lead to tail risks. Complexity theory can help illuminate the features of AI that pose central challenges for policymakers, such as feedback loops induced by training AI models on synthetic data and the interconnectedness between AI systems and critical infrastructure. Drawing on insights from other domains shaped by complex systems, including public health and climate change, we examine how efforts to govern AI are marked by deep uncertainty. To contend with this challenge, we propose a set of complexity-compatible principles concerning the timing and structure of AI governance, and the risk thresholds that should trigger regulatory intervention. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00013</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00013</id><created>2025-01-07</created><authors><author><keyname>Lane</keyname><forenames>Richard</forenames></author><author><keyname>State-Davey</keyname><forenames>Hannah</forenames></author><author><keyname>Taylor</keyname><forenames>Claire</forenames></author><author><keyname>Holmes</keyname><forenames>Wendy</forenames></author><author><keyname>Boon</keyname><forenames>Rachel</forenames></author><author><keyname>Round</keyname><forenames>Mark</forenames></author></authors><title>Behavioural Analytics: Mathematics of the Mind</title><categories>cs.CY cs.LG</categories><comments>19 pages, 14 figures, presented at 7th IMA Conference on Mathematics   in Defence and Security, London, UK, 7 September 2023 (conference page at   https://ima.org.uk/20850/7th-ima-defence/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Behavioural analytics provides insights into individual and crowd behaviour, enabling analysis of what previously happened and predictions for how people may be likely to act in the future. In defence and security, this analysis allows organisations to achieve tactical and strategic advantage through influence campaigns, a key counterpart to physical activities. Before action can be taken, online and real-world behaviour must be analysed to determine the level of threat. Huge data volumes mean that automated processes are required to attain an accurate understanding of risk. We describe the mathematical basis of technologies to analyse quotes in multiple languages. These include a Bayesian network to understand behavioural factors, state estimation algorithms for time series analysis, and machine learning algorithms for classification. We present results from studies of quotes in English, French, and Arabic, from anti-violence campaigners, politicians, extremists, and terrorists. The algorithms correctly identify extreme statements; and analysis at individual, group, and population levels detects both trends over time and sharp changes attributed to major geopolitical events. Group analysis shows that additional population characteristics can be determined, such as polarisation over particular issues and large-scale shifts in attitude. Finally, MP voting behaviour and statements from publicly-available records are analysed to determine the level of correlation between what people say and what they do. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00014</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00014</id><created>2025-01-07</created><authors><author><keyname>Lee</keyname><forenames>Jyh-An</forenames></author></authors><title>Algorithmic Bias and the New Chicago School</title><categories>cs.CY</categories><journal-ref>Law, Innovation and Technology, Vol. 14, No. 1 (2022)</journal-ref><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  AI systems are increasingly deployed in both public and private sectors to independently make complicated decisions with far-reaching impact on individuals and the society. However, many AI algorithms are biased in the collection or processing of data, resulting in prejudiced decisions based on demographic features. Algorithmic biases occur because of the training data fed into the AI system or the design of algorithmic models. While most legal scholars propose a direct-regulation approach associated with the right of explanation or transparency obligation, this article provides a different picture regarding how indirect regulation can be used to regulate algorithmic bias based on the New Chicago School framework developed by Lawrence Lessig. This article concludes that an effective regulatory approach toward algorithmic bias will be the right mixture of direct and indirect regulations through architecture, norms, market, and the law. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00015</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00015</id><created>2025-01-08</created><authors><author><keyname>Huang</keyname><forenames>Yutan</forenames></author><author><keyname>Arora</keyname><forenames>Chetan</forenames></author><author><keyname>Houng</keyname><forenames>Wen Cheng</forenames></author><author><keyname>Kanij</keyname><forenames>Tanjila</forenames></author><author><keyname>Madulgalla</keyname><forenames>Anuradha</forenames></author><author><keyname>Grundy</keyname><forenames>John</forenames></author></authors><title>Ethical Concerns of Generative AI and Mitigation Strategies: A   Systematic Mapping Study</title><categories>cs.CY cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  [Context] Generative AI technologies, particularly Large Language Models (LLMs), have transformed numerous domains by enhancing convenience and efficiency in information retrieval, content generation, and decision-making processes. However, deploying LLMs also presents diverse ethical challenges, and their mitigation strategies remain complex and domain-dependent. [Objective] This paper aims to identify and categorize the key ethical concerns associated with using LLMs, examine existing mitigation strategies, and assess the outstanding challenges in implementing these strategies across various domains. [Method] We conducted a systematic mapping study, reviewing 39 studies that discuss ethical concerns and mitigation strategies related to LLMs. We analyzed these ethical concerns using five ethical dimensions that we extracted based on various existing guidelines, frameworks, and an analysis of the mitigation strategies and implementation challenges. [Results] Our findings reveal that ethical concerns in LLMs are multi-dimensional and context-dependent. While proposed mitigation strategies address some of these concerns, significant challenges still remain. [Conclusion] Our results highlight that ethical issues often hinder the practical implementation of the mitigation strategies, particularly in high-stake areas like healthcare and public governance; existing frameworks often lack adaptability, failing to accommodate evolving societal expectations and diverse contexts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00016</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00016</id><created>2025-01-09</created><authors><author><keyname>Perez</keyname><forenames>Ryann M.</forenames></author><author><keyname>Shimogawa</keyname><forenames>Marie</forenames></author><author><keyname>Chang</keyname><forenames>Yannan</forenames></author><author><keyname>Phan</keyname><forenames>Hoang Ahn T.</forenames></author><author><keyname>Marmorstein</keyname><forenames>Jason G.</forenames></author><author><keyname>Yanagawa</keyname><forenames>Evan S. K.</forenames></author><author><keyname>Petersson</keyname><forenames>E. James</forenames></author></authors><title>Large Language Models for Education: ChemTAsk -- An Open-Source Paradigm   for Automated Q&amp;A in the Graduate Classroom</title><categories>cs.CY cs.HC</categories><comments>38 pages, 3 figures, 1 table</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models (LLMs) show promise for aiding graduate level education, but are limited by their training data and potential confabulations. We developed ChemTAsk, an open-source pipeline that combines LLMs with retrieval-augmented generation (RAG) to provide accurate, context-specific assistance. ChemTAsk utilizes course materials, including lecture transcripts and primary publications, to generate accurate responses to student queries. Over nine weeks in an advanced biological chemistry course at the University of Pennsylvania, students could opt in to use ChemTAsk for assistance in any assignment or to understand class material. Comparative analysis showed ChemTAsk performed on par with human teaching assistants (TAs) in understanding student queries and providing accurate information, particularly excelling in creative problem-solving tasks. In contrast, TAs were more precise in their responses and tailored their assistance to the specifics of the class. Student feedback indicated that ChemTAsk was perceived as correct, helpful, and faster than TAs. Open-source and proprietary models from Meta and OpenAI respectively were tested on an original biological chemistry benchmark for future iterations of ChemTAsk. It was found that OpenAI models were more tolerant to deviations in the input prompt and excelled in self-assessment to safeguard for potential confabulations. Taken together, ChemTAsk demonstrates the potential of integrating LLMs with RAG to enhance educational support, offering a scalable tool for students and educators. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00017</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00017</id><created>2025-01-10</created><authors><author><keyname>Ikram</keyname><forenames>Gagaoua</forenames><affiliation>UL, CNRS, LORIA</affiliation></author><author><keyname>Brun</keyname><forenames>Armelle</forenames><affiliation>UL, CNRS, LORIA</affiliation></author><author><keyname>Boyer</keyname><forenames>Anne</forenames><affiliation>UL, CNRS, LORIA</affiliation></author></authors><title>A Frugal Model for Accurate Early Student Failure Prediction</title><categories>cs.CY cs.LG</categories><comments>LICE - London International Conference on Education, London   International Conference on Education, Nov 2024, London, United Kingdom</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting student success or failure is vital for timely interventions and personalized support. Early failure prediction is particularly crucial, yet limited data availability in the early stages poses challenges, one of the possible solutions is to make use of additional data from other contexts, however, this might lead to overconsumption with no guarantee of better results. To address this, we propose the Frugal Early Prediction (FEP) model, a new hybrid model that selectively incorporates additional data, promoting data frugality and efficient resource utilization. Experiments conducted on a public dataset from a VLE demonstrate FEP's effectiveness in reducing data usage, a primary goal of this research.Experiments showcase a remarkable 27% reduction in data consumption, compared to a systematic use of additional data, aligning with our commitment to data frugality and offering substantial benefits to educational institutions seeking efficient data consumption. Additionally, FEP also excels in enhancing prediction accuracy. Compared to traditional approaches, FEP achieves an average accuracy gain of 7.3%. This not only highlights the practicality and efficiency of FEP but also its superiority in performance, while respecting resource constraints, providing beneficial findings for educational institutions seeking data frugality. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00018</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00018</id><created>2025-01-11</created><authors><author><keyname>Wang</keyname><forenames>Yijian</forenames></author><author><keyname>Guo</keyname><forenames>Tongxian</forenames></author><author><keyname>Liu</keyname><forenames>Zhaoqiang</forenames></author></authors><title>An Expectation-Maximization Algorithm-based Autoregressive Model for the   Fuzzy Job Shop Scheduling Problem</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fuzzy job shop scheduling problem (FJSSP) emerges as an innovative extension to the job shop scheduling problem (JSSP), incorporating a layer of uncertainty that aligns the problem more closely with the complexities of real-world manufacturing environments. This improvement increases the computational complexity of deriving the solution while improving its applicability. In the domain of deterministic scheduling, neural combinatorial optimization (NCO) has recently demonstrated remarkable efficacy. However, its application to the realm of fuzzy scheduling has been relatively unexplored. This paper aims to bridge this gap by investigating the feasibility of employing neural networks to assimilate and process fuzzy information for the resolution of FJSSP, thereby leveraging the advancements in NCO to enhance fuzzy scheduling methodologies. To achieve this, we approach the FJSSP as a generative task and introduce an expectation-maximization algorithm-based autoregressive model (EMARM) to address it. During training, our model alternates between generating scheduling schemes from given instances (E-step) and adjusting the autoregressive model weights based on these generated schemes (M-step). This novel methodology effectively navigates around the substantial hurdle of obtaining ground-truth labels, which is a prevalent issue in NCO frameworks. In testing, the experimental results demonstrate the superior capability of EMARM in addressing the FJSSP, showcasing its effectiveness and potential for practical applications in fuzzy scheduling. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00019</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00019</id><created>2025-01-15</created><authors><author><keyname>Sharma</keyname><forenames>Abhishek</forenames></author></authors><title>Growth Patterns of Inference</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  What properties of a first-order search space support/hinder inference? What kinds of facts would be most effective to learn? Answering these questions is essential for understanding the dynamics of deductive reasoning and creating large-scale knowledge-based learning systems that support efficient inference. We address these questions by developing a model of how the distribution of ground facts affects inference performance in search spaces. Experiments suggest that uniform search spaces are suitable for larger KBs whereas search spaces with skewed degree distribution show better performance in smaller KBs. A sharp transition in Q/A performance is seen in some cases, suggesting that analysis of the structure of search spaces with existing knowledge should be used to guide the acquisition of new ground facts in learning systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00020</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00020</id><created>2025-01-15</created><authors><author><keyname>Sharma</keyname><forenames>Abhishek</forenames></author></authors><title>Temporal Reasoning in AI systems</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Commonsense temporal reasoning at scale is a core problem for cognitive systems. The correct inference of the duration for which fluents hold is required by many tasks, including natural language understanding and planning. Many AI systems have limited deductive closure because they cannot extrapolate information correctly regarding existing fluents and events. In this study, we discuss the knowledge representation and reasoning schemes required for robust temporal projection in the Cyc Knowledge Base. We discuss how events can start and end risk periods for fluents. We then use discrete survival functions, which represent knowledge of the persistence of facts, to extrapolate a given fluent. The extrapolated intervals can be truncated by temporal constraints and other types of commonsense knowledge. Finally, we present the results of experiments to demonstrate that these methods obtain significant improvements in terms of Q/A performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00021</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00021</id><created>2025-01-16</created><authors><author><keyname>McInroe</keyname><forenames>Trevor</forenames></author><author><keyname>Garcin</keyname><forenames>Samuel</forenames></author></authors><title>PixelBrax: Learning Continuous Control from Pixels End-to-End on the GPU</title><categories>cs.LG cs.PF</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present PixelBrax, a set of continuous control tasks with pixel observations. We combine the Brax physics engine with a pure JAX renderer, allowing reinforcement learning (RL) experiments to run end-to-end on the GPU. PixelBrax can render observations over thousands of parallel environments and can run two orders of magnitude faster than existing benchmarks that rely on CPU-based rendering. Additionally, PixelBrax supports fully reproducible experiments through its explicit handling of any stochasticity within the environments and supports color and video distractors for benchmarking generalization. We open-source PixelBrax alongside JAX implementations of several RL algorithms at github.com/trevormcinroe/pixelbrax. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00022</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00022</id><created>2025-01-16</created><authors><author><keyname>Xiao</keyname><forenames>Xingyu</forenames></author><author><keyname>Chen</keyname><forenames>Peng</forenames></author><author><keyname>Jia</keyname><forenames>Qianqian</forenames></author><author><keyname>Tong</keyname><forenames>Jiejuan</forenames></author><author><keyname>Liang</keyname><forenames>Jingang</forenames></author><author><keyname>Wang</keyname><forenames>Haitao</forenames></author></authors><title>A Dynamic and High-Precision Method for Scenario-Based HRA Synthetic   Data Collection in Multi-Agent Collaborative Environments Driven by LLMs</title><categories>cs.AI cs.HC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  HRA (Human Reliability Analysis) data is crucial for advancing HRA methodologies. however, existing data collection methods lack the necessary granularity, and most approaches fail to capture dynamic features. Additionally, many methods require expert knowledge as input, making them time-consuming and labor-intensive. To address these challenges, we propose a new paradigm for the automated collection of HRA data. Our approach focuses on key indicators behind human error, specifically measuring workload in collaborative settings. This study introduces a novel, scenario-driven method for workload estimation, leveraging fine-tuned large language models (LLMs). By training LLMs on real-world operational data from high-temperature gas-cooled reactors (HTGRs), we simulate human behavior and cognitive load in real time across various collaborative scenarios. The method dynamically adapts to changes in operator workload, providing more accurate, flexible, and scalable workload estimates. The results demonstrate that the proposed WELLA (Workload Estimation with LLMs and Agents) outperforms existing commercial LLM-based methods in terms of prediction accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00023</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00023</id><created>2025-01-19</created><authors><author><keyname>Lee</keyname><forenames>Keon Ju M.</forenames></author><author><keyname>Pasquier</keyname><forenames>Philippe</forenames></author></authors><title>Musical Agent Systems: MACAT and MACataRT</title><categories>cs.MA cs.AI cs.HC cs.SD eess.AS</categories><comments>In Proceedings of the Creativity and Generative AI NIPS (Neural   Information Processing Systems) Workshop 2024</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Our research explores the development and application of musical agents, human-in-the-loop generative AI systems designed to support music performance and improvisation within co-creative spaces. We introduce MACAT and MACataRT, two distinct musical agent systems crafted to enhance interactive music-making between human musicians and AI. MACAT is optimized for agent-led performance, employing real-time synthesis and self-listening to shape its output autonomously, while MACataRT provides a flexible environment for collaborative improvisation through audio mosaicing and sequence-based learning. Both systems emphasize training on personalized, small datasets, fostering ethical and transparent AI engagement that respects artistic integrity. This research highlights how interactive, artist-centred generative AI can expand creative possibilities, empowering musicians to explore new forms of artistic expression in real-time, performance-driven and music improvisation contexts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00024</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00024</id><created>2025-01-20</created><authors><author><keyname>Yuan</keyname><forenames>Ke</forenames></author><author><keyname>Liu</keyname><forenames>Yaoxin</forenames></author><author><keyname>Chandra</keyname><forenames>Shriyesh</forenames></author><author><keyname>Roy</keyname><forenames>Rishav</forenames></author></authors><title>Retail Market Analysis</title><categories>q-fin.GN cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This project focuses on analyzing retail market trends using historical sales data, search trends, and customer reviews. By identifying the patterns and trending products, the analysis provides actionable insights for retailers to optimize inventory management and marketing strategies, ultimately enhancing customer satisfaction and maximizing revenue. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00025</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00025</id><created>2025-01-21</created><authors><author><keyname>Ahmed</keyname><forenames>Abdulaziz</forenames></author><author><keyname>Saleem</keyname><forenames>Mohammad</forenames></author><author><keyname>Alzeen</keyname><forenames>Mohammed</forenames></author><author><keyname>Birur</keyname><forenames>Badari</forenames></author><author><keyname>Fargason</keyname><forenames>Rachel E</forenames></author><author><keyname>Burk</keyname><forenames>Bradley G</forenames></author><author><keyname>Harkins</keyname><forenames>Hannah Rose</forenames></author><author><keyname>Alhassan</keyname><forenames>Ahmed</forenames></author><author><keyname>Al-Garadi</keyname><forenames>Mohammed Ali</forenames></author></authors><title>Leveraging Large Language Models to Enhance Machine Learning   Interpretability and Predictive Performance: A Case Study on Emergency   Department Returns for Mental Health Patients</title><categories>cs.LG cs.AI cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: To evaluate whether integrating large language models (LLMs) with traditional machine learning approaches improves both the predictive accuracy and clinical interpretability of ED mental health returns risk models. Methods: This retrospective cohort study analyzed 42,464 ED visits for 27,904 unique mental health patients at an Academic Medical Center in the deep South of the United States between January 2018 and December 2022. Main Outcomes and Measures: Two primary outcomes were evaluated: (1) 30 days ED return prediction accuracy and (2) model interpretability through a novel retrieval-augmented generation (RAG) framework integrating SHAP (SHapley Additive exPlanations) values with contextual clinical knowledge. Results: The proposed machine learning interpretability framework, leveraging LLM, achieved 99% accuracy in translating complex model predictions into clinically relevant explanations. Integration of LLM-extracted features enhanced predictive performance, improving the XGBoost model area under the curve (AUC) from 0.73 to 0.76. The LLM-based feature extraction using 10-shot learning significantly outperformed traditional approaches, achieving an accuracy of 0.882 and an F1 score of 0.86 for chief complaint classification (compared to conventional methods with an accuracy range of 0.59 to 0.63) and demonstrating accuracy values ranging from 0.65 to 0.93 across multiple SDoH categories, underscoring its robust performance in extracting features from clinical notes. Conclusions and Relevance: Integrating LLMs with traditional machine learning models yielded modest but consistent improvements in ED return prediction accuracy while substantially enhancing model interpretability through automated, clinically relevant explanations. This approach offers a framework for translating complex predictive analytics into actionable clinical insights. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00026</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00026</id><created>2025-01-21</created><authors><author><keyname>Wang</keyname><forenames>Hui</forenames></author><author><keyname>Cheng</keyname><forenames>Yuan</forenames></author><author><keyname>Han</keyname><forenames>Xiaomeng</forenames></author><author><keyname>Zhao</keyname><forenames>Zhengpeng</forenames></author><author><keyname>Yang</keyname><forenames>Dawei</forenames></author><author><keyname>Jiang</keyname><forenames>Zhe</forenames></author></authors><title>Pushing the Limits of BFP on Narrow Precision LLM Inference</title><categories>cs.AR cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The substantial computational and memory demands of Large Language Models (LLMs) hinder their deployment. Block Floating Point (BFP) has proven effective in accelerating linear operations, a cornerstone of LLM workloads. However, as sequence lengths grow, nonlinear operations, such as Attention, increasingly become performance bottlenecks due to their quadratic computational complexity. These nonlinear operations are predominantly executed using inefficient floating-point formats, which renders the system challenging to optimize software efficiency and hardware overhead. In this paper, we delve into the limitations and potential of applying BFP to nonlinear operations. Given our findings, we introduce a hardware-software co-design framework (DB-Attn), including: (i) DBFP, an advanced BFP version, overcomes nonlinear operation challenges with a pivot-focus strategy for diverse data and an adaptive grouping strategy for flexible exponent sharing. (ii) DH-LUT, a novel lookup table algorithm dedicated to accelerating nonlinear operations with DBFP format. (iii) An RTL-level DBFP-based engine is implemented to support DB-Attn, applicable to FPGA and ASIC. Results show that DB-Attn provides significant performance improvements with negligible accuracy loss, achieving 74% GPU speedup on Softmax of LLaMA and 10x low overhead performance improvement over SOTA designs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00027</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00027</id><created>2025-01-21</created><authors><author><keyname>Singh</keyname><forenames>Ankur</forenames></author><author><keyname>Kim</keyname><forenames>Dowon</forenames></author><author><keyname>Lee</keyname><forenames>Byung-Geun</forenames></author></authors><title>Analysis of a Memcapacitor-Based for Neural Network Accelerator   Framework</title><categories>cs.AR cs.AI cs.NE</categories><comments>11 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Data-intensive computing tasks, such as training neural networks, are crucial for artificial intelligence applications but often come with high energy demands. One promising solution is to develop specialized hardware that directly maps neural networks, utilizing arrays of memristive devices to perform parallel multiply-accumulate operations. In our research, we introduce a novel CMOS-based memcapacitor circuit that is validated using the cadence tool. Additionally, we developed the device in Python to facilitate the design of a memcapacitive-based accelerator. Our proposed framework employs a crossbar array of memcapacitor devices to train a neural network capable of digit classification and CIFAR dataset recognition. We tested the non-ideal characteristics of the constructed memcapacitor-based neural network. The system achieved an impressive 98.4% training accuracy in digit recognition and 94.4% training accuracy in CIFAR recognition, highlighting its effectiveness. This study demonstrates the potential of memcapacitor-based neural network systems in handling classification tasks and sets the stage for further advancements in neuromorphic computing. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00028</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00028</id><created>2025-01-22</created><authors><author><keyname>Zhao</keyname><forenames>Zhuorui</forenames></author><author><keyname>Qiu</keyname><forenames>Ruidi</forenames></author><author><keyname>Lin</keyname><forenames>Ing-Chao</forenames></author><author><keyname>Zhang</keyname><forenames>Grace Li</forenames></author><author><keyname>Li</keyname><forenames>Bing</forenames></author><author><keyname>Schlichtmann</keyname><forenames>Ulf</forenames></author></authors><title>VRank: Enhancing Verilog Code Generation from Large Language Models via   Self-Consistency</title><categories>cs.AR cs.PL</categories><comments>accepted by ISQED2025</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Large Language Models (LLMs) have demonstrated promising capabilities in generating Verilog code from module specifications. To improve the quality of such generated Verilog codes, previous methods require either time-consuming manual inspection or generation of multiple Verilog codes, from which the one with the highest quality is selected with manually designed testbenches. To enhance the generation efficiency while maintaining the quality of the generated codes, we propose VRank, an automatic framework that generates Verilog codes with LLMs. In our framework, multiple code candidates are generated with LLMs by leveraging their probabilistic nature. Afterwards, we group Verilog code candidates into clusters based on identical outputs when tested against the same testbench, which is also generated by LLMs. Clusters are ranked based on the consistency they show on testbench. To determine the best candidate, Chain-of-Thought is further applied to select the best candidate from the top-ranked clusters. By systematically analyzing diverse outputs of generated codes, VRank reduces errors and enhances the overall quality of the generated Verilog code. Experimental results on the VerilogEval-Human benchmark demonstrate a significant 10.5% average increase in functional correctness (passl1) across multiple LLMs, demonstrating VRank's effectiveness in improving the accuracy of automated hardware description language generation for complex design tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00029</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00029</id><created>2025-01-23</created><authors><author><keyname>Yuksel</keyname><forenames>Kamer Ali</forenames></author><author><keyname>Sawaf</keyname><forenames>Hassan</forenames></author></authors><title>AlphaSharpe: LLM-Driven Discovery of Robust Risk-Adjusted Metrics</title><categories>q-fin.PM cs.AI cs.CL cs.NE q-fin.RM</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Financial metrics like the Sharpe ratio are pivotal in evaluating investment performance by balancing risk and return. However, traditional metrics often struggle with robustness and generalization, particularly in dynamic and volatile market conditions. This paper introduces AlphaSharpe, a novel framework leveraging large language models (LLMs) to iteratively evolve and optimize financial metrics. AlphaSharpe generates enhanced risk-return metrics that outperform traditional approaches in robustness and correlation with future performance metrics by employing iterative crossover, mutation, and evaluation. Key contributions of this work include: (1) an innovative use of LLMs for generating and refining financial metrics inspired by domain-specific knowledge, (2) a scoring mechanism to ensure the evolved metrics generalize effectively to unseen data, and (3) an empirical demonstration of 3x predictive power for future risk-return forecasting. Experimental results on a real-world dataset highlight the superiority of AlphaSharpe metrics, making them highly relevant for portfolio managers and financial decision-makers. This framework not only addresses the limitations of existing metrics but also showcases the potential of LLMs in advancing financial analytics, paving the way for informed and robust investment strategies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00030</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00030</id><created>2025-01-23</created><authors><author><keyname>Sole</keyname><forenames>Ignasi</forenames></author></authors><title>Evolving Performance Practices in Beethoven's Cello Sonatas: Tempo,   Portamento, and Historical Interpretation of the First Movements</title><categories>cs.SD cs.HC eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the evolving performance practices of Ludwig van Beethoven's cello sonatas, with a particular focus on tempo and portamento between 1930 and 2012. It integrates analyses of 22 historical recordings, advancements in recording technology to shed light on changes in interpretative approaches. By comparing Beethoven's metronome markings, as understood through contemporaries such as Czerny and Moscheles, with their application in modern performances, my research highlights notable deviations. These differences prove the challenges performers face in reconciling historical tempos with the demands of contemporary performance practice. My study pays special attention to the diminishing use of audible portamento in the latter half of the 20th century, contrasted with a gradual increase in tempo after 1970. This development is linked to broader cultural and pedagogical shifts, including the adoption of fingering techniques that reduce hand shifts, thereby facilitating greater technical precision at faster tempos. Nonetheless, my study identifies the persistence of 'silent portamento' as an expressive device, allowing performers to retain stylistic expression without compromising rhythmic integrity. My paper offers valuable insights for performers and scholars alike, advocating a critical reassessment of Beethoven's tempo markings and the nuanced application of portamento in modern performance practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00031</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00031</id><created>2025-01-23</created><authors><author><keyname>Yang</keyname><forenames>Bin</forenames></author><author><keyname>Zou</keyname><forenames>Zhaonian</forenames></author><author><keyname>Ye</keyname><forenames>Jianxiong</forenames></author></authors><title>GNN-based Anchor Embedding for Exact Subgraph Matching</title><categories>cs.SI cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subgraph matching query is a classic problem in graph data management and has a variety of real-world applications, such as discovering structures in biological or chemical networks, finding communities in social network analysis, explaining neural networks in machine learning, and so on. To further solve the subgraph matching problem, several recent advanced works attempt to utilize deep-learning-based techniques to handle the subgraph matching query. However, most of these works only obtain approximate results for subgraph matching without theoretical guarantees of accuracy. In this paper, we propose a novel and effective graph neural network (GNN)-based anchor embedding framework (GNN-AE), which allows exact subgraph matching. Unlike GNN-based approximate subgraph matching approaches that only produce inexact results, in this paper, we pioneer the anchor concept (including anchor, anchor graph/path, etc.) in subgraph matching and carefully devise the anchor (graph) embedding technique based on GNN models. We transform the subgraph matching problem into a search problem in the embedding space via the anchor (graph &amp; path) embedding techniques. With the proposed anchor matching mechanism, GNN-AE can guarantee subgraph matching has no false dismissals. We design an efficient matching growth algorithm, which can retrieve the locations of all exact matches in parallel. We also propose a cost-model-based DFS query plan to enhance the parallel matching growth algorithm. Through extensive experiments on 6 real-world and 3 synthetic datasets, we confirm the effectiveness and efficiency of our GNN-AE approach for exact subgraph matching. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00032</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00032</id><created>2025-01-23</created><authors><author><keyname>Shorten</keyname><forenames>Connor</forenames></author><author><keyname>Pierse</keyname><forenames>Charles</forenames></author><author><keyname>Smith</keyname><forenames>Thomas Benjamin</forenames></author><author><keyname>D'Oosterlinck</keyname><forenames>Karel</forenames></author><author><keyname>Celik</keyname><forenames>Tuana</forenames></author><author><keyname>Cardenas</keyname><forenames>Erika</forenames></author><author><keyname>Monigatti</keyname><forenames>Leonie</forenames></author><author><keyname>Hasan</keyname><forenames>Mohd Shukri</forenames></author><author><keyname>Schmuhl</keyname><forenames>Edward</forenames></author><author><keyname>Williams</keyname><forenames>Daniel</forenames></author><author><keyname>Kesiraju</keyname><forenames>Aravind</forenames></author><author><keyname>van Luijt</keyname><forenames>Bob</forenames></author></authors><title>Querying Databases with Function Calling</title><categories>cs.DB cs.AI cs.IR</categories><comments>Preprint. 23 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The capabilities of Large Language Models (LLMs) are rapidly accelerating largely thanks to their integration with external tools. Querying databases is among the most effective of these integrations, enabling LLMs to access private or continually updating data. While Function Calling is the most common method for interfacing external tools to LLMs, its application to database querying as a tool has been underexplored. We propose a tool definition for database querying that unifies accessing data with search queries, filters, or a combination both, as well as transforming results with aggregation and groupby operators. To evaluate its effectiveness, we conduct a study with 8 LLMs spanning 5 model families. We present a novel pipeline adapting the Gorilla LLM framework to create synthetic database schemas and queries. We primarily evaluate the models with the Exact Match of predicted and ground truth query APIs. Among the models tested, Claude 3.5 Sonnet achieves the highest performance with an Exact Match score of 74.3%, followed by GPT-4o mini at 73.7%, and GPT-4o at 71.8%. We further breakdown these results per API component utilized and across synthetic use cases. We find that LLMs are highly effective at utilizing operators on boolean properties, but struggle with text property filters. Across use cases we find robust results with the higher performing models such as GPT-4o, but significant performance variance across use cases from lower performing models. We additionally conduct ablation studies exploring the impact of parallel tool calling, adding a rationale as an argument of the tool call, using a separate tool per database collection, and tool calling with structured outputs. Our findings demonstrate the effectiveness of enabling LLMs to query databases with Function Calling. We have open-sourced our experimental code and results at github.com/weaviate/gorilla. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00033</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00033</id><created>2025-01-24</created><authors><author><keyname>Schneegans</keyname><forenames>Simon</forenames></author><author><keyname>Neary</keyname><forenames>Lori</forenames></author><author><keyname>Flatken</keyname><forenames>Markus</forenames></author><author><keyname>Gerndt</keyname><forenames>Andreas</forenames></author></authors><title>STRIELAD -- A Scalable Toolkit for Real-time Interactive Exploration of   Large Atmospheric Datasets</title><categories>cs.HC cs.DC cs.GR</categories><comments>Accepted at IEEE VIS 2017 as part of the SciVis contest</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Technological advances in high performance computing and maturing physical models allow scientists to simulate weather and climate evolutions with an increasing accuracy. While this improved accuracy allows us to explore complex dynamical interactions within such physical systems, inconceivable a few years ago, it also results in grand challenges regarding the data visualization and analytics process. We present STRIELAD, a scalable weather analytics toolkit, which allows for interactive exploration and real-time visualization of such large scale datasets. It combines parallel and distributed feature extraction using high-performance computing resources with smart level-of-detail rendering methods to assure interactivity during the complete analysis process. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00034</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00034</id><created>2025-01-24</created><authors><author><keyname>Manyari</keyname><forenames>Yassine El</forenames></author><author><keyname>Fuxjager</keyname><forenames>Anton R.</forenames></author><author><keyname>Zahlner</keyname><forenames>Stefan</forenames></author><author><keyname>Van Dijk</keyname><forenames>Joost</forenames></author><author><keyname>Castagna</keyname><forenames>Alberto</forenames></author><author><keyname>Barbieri</keyname><forenames>Davide</forenames></author><author><keyname>Viebahn</keyname><forenames>Jan</forenames></author><author><keyname>Wasserer</keyname><forenames>Marcel</forenames></author></authors><title>Towards Efficient Multi-Objective Optimisation for Real-World Power Grid   Topology Control</title><categories>cs.AI cs.LG cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power grid operators face increasing difficulties in the control room as the increase in energy demand and the shift to renewable energy introduce new complexities in managing congestion and maintaining a stable supply. Effective grid topology control requires advanced tools capable of handling multi-objective trade-offs. While Reinforcement Learning (RL) offers a promising framework for tackling such challenges, existing Multi-Objective Reinforcement Learning (MORL) approaches fail to scale to the large state and action spaces inherent in real-world grid operations. Here we present a two-phase, efficient and scalable Multi-Objective Optimisation (MOO) method designed for grid topology control, combining an efficient RL learning phase with a rapid planning phase to generate day-ahead plans for unseen scenarios. We validate our approach using historical data from TenneT, a European Transmission System Operator (TSO), demonstrating minimal deployment time, generating day-ahead plans within 4-7 minutes with strong performance. These results underline the potential of our scalable method to support real-world power grid management, offering a practical, computationally efficient, and time-effective tool for operational planning. Based on current congestion costs and inefficiencies in grid operations, adopting our approach by TSOs could potentially save millions of euros annually, providing a compelling economic incentive for its integration in the control room. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00035</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00035</id><created>2025-01-24</created><authors><author><keyname>Akanda</keyname><forenames>Md Rakibul Karim</forenames></author><author><keyname>Lima</keyname><forenames>Joao Raimundo Queiroz Pires Santana De Oliveira</forenames></author><author><keyname>Holmes</keyname><forenames>Amaya Alexandria</forenames></author><author><keyname>Bonner</keyname><forenames>Christina</forenames></author></authors><title>Safeguarding the Future of Mobility: Cybersecurity Issues and Solutions   for Infrastructure Associated with Electric Vehicle Charging</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The development of an ecosystem that balances consumer convenience and security is imperative given the expanding market for electric vehicles (EVs). The vast amount of data that EV charging station management systems (EVCSMSs) give is powered by the Internet of Things (IoT) ecosystem. Intrusion Detection Systems (IDSs), which track network traffic to spot potentially dangerous data exchanges in IT and IoT contexts, are constantly improving in terms of efficacy and accuracy. Intrusion detection is becoming a major topic in academia because of the acceleration of IDS development caused by machine learning and deep learning techniques. The goal of the research presented in this paper is to use a machine-learning-based intrusion detection system with low false-positive rates and high accuracy to safeguard the ecosystem of EV charging stations (EVCS). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00036</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00036</id><created>2025-01-24</created><authors><author><keyname>Marfo</keyname><forenames>William</forenames></author><author><keyname>Tosh</keyname><forenames>Deepak K.</forenames></author><author><keyname>Moore</keyname><forenames>Shirley V.</forenames></author></authors><title>Efficient Client Selection in Federated Learning</title><categories>cs.LG cs.AI cs.DC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Federated Learning (FL) enables decentralized machine learning while preserving data privacy. This paper proposes a novel client selection framework that integrates differential privacy and fault tolerance. The adaptive client selection adjusts the number of clients based on performance and system constraints, with noise added to protect privacy. Evaluated on the UNSW-NB15 and ROAD datasets for network anomaly detection, the method improves accuracy by 7% and reduces training time by 25% compared to baselines. Fault tolerance enhances robustness with minimal performance trade-offs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00037</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00037</id><created>2025-01-25</created><authors><author><keyname>Belov</keyname><forenames>Mikhail Gennadievich</forenames></author><author><keyname>Dubov</keyname><forenames>Victor Victorovich</forenames></author><author><keyname>Ivanov</keyname><forenames>Vadim Konstantinovich</forenames></author><author><keyname>Maslov</keyname><forenames>Alexander Yurievich</forenames></author><author><keyname>Proshina</keyname><forenames>Olga Vladimirovna</forenames></author><author><keyname>Malyshkin</keyname><forenames>Vladislav Gennadievich</forenames></author></authors><title>Super Quantum Mechanics</title><categories>quant-ph cs.LG cs.NA math.NA</categories><comments>The ML approach presented in arXiv:2407.04406 is extended to   stationary and non-stationary quantum dynamics</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce Super Quantum Mechanics (SQM) as a theory that considers states in Hilbert space subject to multiple quadratic constraints. Traditional quantum mechanics corresponds to a single quadratic constraint of wavefunction normalization. In its simplest form, SQM considers states in the form of unitary operators, where the quadratic constraints are conditions of unitarity. In this case, the stationary SQM problem is a quantum inverse problem with multiple applications in machine learning and artificial intelligence. The SQM stationary problem is equivalent to a new algebraic problem that we address in this paper. The SQM non-stationary problem considers the evolution of a quantum system, distinct from the explicit time dependence of the Hamiltonian, $H(t)$. Several options for the SQM dynamic equation are considered, and quantum circuits of 2D type are introduced, which transform one quantum system into another. Although no known physical process currently describes such dynamics, this approach naturally bridges direct and inverse quantum mechanics problems, allowing for the development of a new type of computer algorithm. Beyond computer modeling, the developed theory could be directly applied if or when a physical process capable of solving an inverse quantum problem in a single measurement act (analogous to wavefunction measurement in traditional quantum mechanics) is discovered in the future. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00038</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00038</id><created>2025-01-26</created><authors><author><keyname>Meyer</keyname><forenames>François G.</forenames></author></authors><title>The Best Soules Basis for the Estimation of a Spectral Barycentre   Network</title><categories>cs.SI cs.LG physics.data-an stat.ML</categories><comments>20 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The main contribution of this work is a fast algorithm to compute the barycentre of a set of networks based on a Laplacian spectral pseudo-distance. The core engine for the reconstruction of the barycentre is an algorithm that explores the large library of Soules bases, and returns a basis that yields a sparse approximation of the sample mean adjacency matrix. We prove that when the networks are random realizations of stochastic block models, then our algorithm reconstructs the population mean adjacency matrix. In addition to the theoretical analysis of the estimator of the barycentre network, we perform Monte Carlo simulations to validate the theoretical properties of the estimator. This work is significant because it opens the door to the design of new spectral-based network synthesis that have theoretical guarantees. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00039</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00039</id><created>2025-01-26</created><authors><author><keyname>Cui</keyname><forenames>Jiaming</forenames></author><author><keyname>Adhikari</keyname><forenames>Bijaya</forenames></author><author><keyname>Haddadan</keyname><forenames>Arash</forenames></author><author><keyname>Haque</keyname><forenames>A S M Ahsan-Ul</forenames></author><author><keyname>Vreeken</keyname><forenames>Jilles</forenames></author><author><keyname>Vullikanti</keyname><forenames>Anil</forenames></author><author><keyname>Prakash</keyname><forenames>B. Aditya</forenames></author></authors><title>Accurately Estimating Unreported Infections using Information Theory</title><categories>cs.SI cs.IT math.IT physics.soc-ph</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  One of the most significant challenges in combating against the spread of infectious diseases was the difficulty in estimating the true magnitude of infections. Unreported infections could drive up disease spread, making it very hard to accurately estimate the infectivity of the pathogen, therewith hampering our ability to react effectively. Despite the use of surveillance-based methods such as serological studies, identifying the true magnitude is still challenging. This paper proposes an information theoretic approach for accurately estimating the number of total infections. Our approach is built on top of Ordinary Differential Equations (ODE) based models, which are commonly used in epidemiology and for estimating such infections. We show how we can help such models to better compute the number of total infections and identify the parametrization by which we need the fewest bits to describe the observed dynamics of reported infections. Our experiments on COVID-19 spread show that our approach leads to not only substantially better estimates of the number of total infections but also better forecasts of infections than standard model calibration based methods. We additionally show how our learned parametrization helps in modeling more accurate what-if scenarios with non-pharmaceutical interventions. Our approach provides a general method for improving epidemic modeling which is applicable broadly. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00040</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00040</id><created>2025-01-27</created><authors><author><keyname>Lautenbacher</keyname><forenames>Thomas</forenames></author><author><keyname>Rajaei</keyname><forenames>Ali</forenames></author><author><keyname>Barbieri</keyname><forenames>Davide</forenames></author><author><keyname>Viebahn</keyname><forenames>Jan</forenames></author><author><keyname>Cremer</keyname><forenames>Jochen L.</forenames></author></authors><title>Multi-Objective Reinforcement Learning for Power Grid Topology Control</title><categories>cs.LG cs.AI cs.SY eess.SY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Transmission grid congestion increases as the electrification of various sectors requires transmitting more power. Topology control, through substation reconfiguration, can reduce congestion but its potential remains under-exploited in operations. A challenge is modeling the topology control problem to align well with the objectives and constraints of operators. Addressing this challenge, this paper investigates the application of multi-objective reinforcement learning (MORL) to integrate multiple conflicting objectives for power grid topology control. We develop a MORL approach using deep optimistic linear support (DOL) and multi-objective proximal policy optimization (MOPPO) to generate a set of Pareto-optimal policies that balance objectives such as minimizing line loading, topological deviation, and switching frequency. Initial case studies show that the MORL approach can provide valuable insights into objective trade-offs and improve Pareto front approximation compared to a random search baseline. The generated multi-objective RL policies are 30% more successful in preventing grid failure under contingencies and 20% more effective when training budget is reduced - compared to the common single objective RL policy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00041</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00041</id><created>2025-01-27</created><authors><author><keyname>Bajwa</keyname><forenames>Taaha Saleem</forenames></author></authors><title>MALT: Mechanistic Ablation of Lossy Translation in LLMs for a   Low-Resource Language: Urdu</title><categories>cs.CL</categories><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LLMs are predominantly trained on English data, which leads to a significant drop in performance on low-resource languages. Understanding how LLMs handle these languages is crucial for improving their effectiveness. This study focuses on Urdu as a use case for exploring the challenges faced by LLMs in processing low-resource languages. LLMs primarily reason in English when prompted in another language, with the final layers acting as translators to convert the English response into the target language. This study finds that even for low-resource languages, the internal latent response of LLMs in English is quite coherent; however, the translation features are lossy and result in poor translations, leading to reduced performance. By mechanistically removing these translation features and using a separate translation model to translate the internal latent response of LLM, the performance of LLMs improves significantly while also preserving the cultural nuances of the input in low-resource languages. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00042</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00042</id><created>2025-01-27</created><authors><author><keyname>Ding</keyname><forenames>Yujie</forenames></author><author><keyname>Teng</keyname><forenames>Shenghua</forenames></author><author><keyname>Li</keyname><forenames>Zuoyong</forenames></author><author><keyname>Chen</keyname><forenames>Xiao</forenames></author></authors><title>LSU-Net: Lightweight Automatic Organs Segmentation Network For Medical   Images</title><categories>eess.IV cs.CV</categories><comments>5 pages, 3 figures, 4 tables. Accepted at ICASSP 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  UNet and its variants have widespread applications in medical image segmentation. However, the substantial number of parameters and computational complexity of these models make them less suitable for use in clinical settings with limited computational resources. To address this limitation, we propose a novel Lightweight Shift U-Net (LSU-Net). We integrate the Light Conv Block and the Tokenized Shift Block in a lightweight manner, combining them with a dynamic weight multi-loss design for efficient dynamic weight allocation. The Light Conv Block effectively captures features with a low parameter count by combining standard convolutions with depthwise separable convolutions. The Tokenized Shift Block optimizes feature representation by shifting and capturing deep features through a combination of the Spatial Shift Block and depthwise separable convolutions. Dynamic adjustment of the loss weights at each layer approaches the optimal solution and enhances training stability. We validated LSU-Net on the UWMGI and MSD Colon datasets, and experimental results demonstrate that LSU-Net outperforms most state-of-the-art segmentation architectures. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00043</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00043</id><created>2025-01-27</created><authors><author><keyname>Lyu</keyname><forenames>Hao</forenames></author><author><keyname>Guo</keyname><forenames>Yanyong</forenames></author><author><keyname>Liu</keyname><forenames>Pan</forenames></author><author><keyname>Zheng</keyname><forenames>Nan</forenames></author><author><keyname>Wang</keyname><forenames>Ting</forenames></author></authors><title>A scalable adaptive deep Koopman predictive controller for real-time   optimization of mixed traffic flow</title><categories>eess.SY cs.AI cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of connected automated vehicle (CAV) is advocated to mitigate traffic oscillations in mixed traffic flow consisting of CAVs and human driven vehicles (HDVs). This study proposes an adaptive deep Koopman predictive control framework (AdapKoopPC) for regulating mixed traffic flow. Firstly, a Koopman theory-based adaptive trajectory prediction deep network (AdapKoopnet) is designed for modeling HDVs car-following behavior. AdapKoopnet enables the representation of HDVs behavior by a linear model in a high-dimensional space. Secondly, the model predictive control is employed to smooth the mixed traffic flow, where the combination of the linear dynamic model of CAVs and linear prediction blocks from AdapKoopnet is embedded as the predictive model into the AdapKoopPC. Finally, the predictive performance of the prosed AdapKoopnet is verified using the HighD naturalistic driving dataset. Furthermore, the control performance of AdapKoopPC is validated by the numerical simulations. Results demonstrate that the AdapKoopnet provides more accuracy HDVs predicted trajectories than the baseline nonlinear models. Moreover, the proposed AdapKoopPC exhibits more effective control performance with less computation cost compared with baselines in mitigating traffic oscillations, especially at the low CAVs penetration rates. The code of proposed AdapKoopPC is open source. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00044</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00044</id><created>2025-01-27</created><authors><author><keyname>Pahr</keyname><forenames>Daniel</forenames></author><author><keyname>Ehlers</keyname><forenames>Henry</forenames></author><author><keyname>Filipov</keyname><forenames>Velitchko</forenames></author></authors><title>HoloGraphs: An Interactive Physicalization for Dynamic Graphs</title><categories>cs.SI cs.HC</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We present HoloGraphs, a novel approach for physically representing, explaining, exploring, and interacting with dynamic networks. HoloGraphs addresses the challenges of visualizing and understanding evolving network structures by providing an engaging method of interacting and exploring dynamic network structures using physicalization techniques. In contrast to traditional digital interfaces, our approach leverages tangible artifacts made from transparent materials to provide an intuitive way for people with low visualization literacy to explore network data. The process involves printing network embeddings on transparent media and assembling them to create a 3D representation of dynamic networks, maintaining spatial perception and allowing the examination of each timeslice individually. Interactivity is envisioned using optional Focus+Context layers and overlays for node trajectories and labels. Focus layers highlight nodes of interest, context layers provide an overview of the network structure, and global overlays show node trajectories over time. In this paper, we outline the design principles and implementation of HoloGraphs and present how elementary digital interactions can be mapped to physical interactions to manipulate the elements of a network and temporal dimension in an engaging matter. We demonstrate the capabilities of our concept in a case study. Using a dynamic network of character interactions from a popular book series, we showcase how it represents and supports understanding complex concepts such as dynamic networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00045</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00045</id><created>2025-01-27</created><authors><author><keyname>Mao</keyname><forenames>Yi</forenames></author><author><keyname>Perrault</keyname><forenames>Andrew</forenames></author></authors><title>Restless Multi-armed Bandits under Frequency and Window Constraints for   Public Service Inspections</title><categories>cs.LG cs.AI cs.CE cs.CY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Municipal inspections are an important part of maintaining the quality of goods and services. In this paper, we approach the problem of intelligently scheduling service inspections to maximize their impact, using the case of food establishment inspections in Chicago as a case study. The Chicago Department of Public Health (CDPH) inspects thousands of establishments each year, with a substantial fail rate (over 3,000 failed inspection reports in 2023). To balance the objectives of ensuring adherence to guidelines, minimizing disruption to establishments, and minimizing inspection costs, CDPH assigns each establishment an inspection window every year and guarantees that they will be inspected exactly once during that window. These constraints create a challenge for a restless multi-armed bandit (RMAB) approach, for which there are no existing methods. We develop an extension to Whittle index-based systems for RMABs that can guarantee action window constraints and frequencies, and furthermore can be leveraged to optimize action window assignments themselves. Briefly, we combine MDP reformulation and integer programming-based lookahead to maximize the impact of inspections subject to constraints. A neural network-based supervised learning model is developed to model state transitions of real Chicago establishments using public CDPH inspection records, which demonstrates 10\% AUC improvements compared with directly predicting establishments' failures. Our experiments not only show up to 24\% (in simulation) or 33\% (on real data) reward improvements resulting from our approach but also give insight into the impact of scheduling constraints. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00046</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00046</id><created>2025-01-16</created><authors><author><keyname>Wallace</keyname><forenames>Tom</forenames></author><author><keyname>Ezzati-Jivan</keyname><forenames>Naser</forenames></author><author><keyname>Ombuki-Berman</keyname><forenames>Beatrice</forenames></author></authors><title>Optimization Strategies for Enhancing Resource Efficiency in   Transformers &amp; Large Language Models</title><categories>cs.LG cs.CL</categories><comments>Accepted for ACM's ICPE 2025 in Short Paper format</comments><msc-class>68T50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advancements in Natural Language Processing are heavily reliant on the Transformer architecture, whose improvements come at substantial resource costs due to ever-growing model sizes. This study explores optimization techniques, including Quantization, Knowledge Distillation, and Pruning, focusing on energy and computational efficiency while retaining performance. Among standalone methods, 4-bit Quantization significantly reduces energy use with minimal accuracy loss. Hybrid approaches, like NVIDIA's Minitron approach combining KD and Structured Pruning, further demonstrate promising trade-offs between size reduction and accuracy retention. A novel optimization equation is introduced, offering a flexible framework for comparing various methods. Through the investigation of these compression methods, we provide valuable insights for developing more sustainable and efficient LLMs, shining a light on the often-ignored concern of energy efficiency. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00047</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00047</id><created>2025-01-28</created><authors><author><keyname>Foucault</keyname><forenames>Armand</forenames><affiliation>IMT, ANITI</affiliation></author><author><keyname>Mamalet</keyname><forenames>Franck</forenames><affiliation>ANITI</affiliation></author><author><keyname>Malgouyres</keyname><forenames>François</forenames><affiliation>IMT</affiliation></author></authors><title>HadamRNN: Binary and Sparse Ternary Orthogonal RNNs</title><categories>cs.LG</categories><comments>International Conference on Learning Representations (ICLR), Apr   2025, Singapour, Singapore</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary and sparse ternary weights in neural networks enable faster computations and lighter representations, facilitating their use on edge devices with limited computational power. Meanwhile, vanilla RNNs are highly sensitive to changes in their recurrent weights, making the binarization and ternarization of these weights inherently challenging. To date, no method has successfully achieved binarization or ternarization of vanilla RNN weights. We present a new approach leveraging the properties of Hadamard matrices to parameterize a subset of binary and sparse ternary orthogonal matrices. This method enables the training of orthogonal RNNs (ORNNs) with binary and sparse ternary recurrent weights, effectively creating a specific class of binary and sparse ternary vanilla RNNs. The resulting ORNNs, called HadamRNN and lock-HadamRNN, are evaluated on benchmarks such as the copy task, permuted and sequential MNIST tasks, and IMDB dataset. Despite binarization or sparse ternarization, these RNNs maintain performance levels comparable to state-of-the-art full-precision models, highlighting the effectiveness of our approach. Notably, our approach is the first solution with binary recurrent weights capable of tackling the copy task over 1000 timesteps. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00048</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00048</id><created>2025-01-28</created><authors><author><keyname>Sisate</keyname><forenames>Colin</forenames></author><author><keyname>Goldfinch</keyname><forenames>Alistair</forenames></author><author><keyname>Waterstone</keyname><forenames>Vincent</forenames></author><author><keyname>Kingsley</keyname><forenames>Sebastian</forenames></author><author><keyname>Blackthorn</keyname><forenames>Mariana</forenames></author></authors><title>Contextually Entangled Gradient Mapping for Optimized LLM Comprehension</title><categories>cs.LG cs.AI cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Contextually Entangled Gradient Mapping (CEGM) introduces a new approach to gradient optimization, redefining the relationship between contextual embeddings and gradient updates to enhance semantic coherence and reasoning capabilities in neural architectures. By treating gradients as dynamic carriers of contextual dependencies rather than isolated numerical entities, the proposed methodology bridges critical gaps in existing optimization strategies. The integration of entangled gradient dynamics into a loss regularization framework demonstrated significant improvements in tasks involving long-form reasoning, contextual retention, and adaptability to unseen domains. Experimental evaluations showed that the CEGM-enhanced model consistently outperformed baseline approaches, achieving higher accuracy in token-level predictions and greater resilience to noisy inputs. Practical implementations involved modifications to training pipelines, introducing entanglement layers and dynamic coefficient adjustments that seamlessly align with existing architectures. Results further highlighted reductions in semantic drift during sequential transformations and improvements in embedding coherence across paraphrased sentences, showing the robustness and versatility of the proposed methodology. The findings demonstrate the broader implications of gradient entanglement for both theoretical advancements and practical applications in optimization strategies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00049</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00049</id><created>2025-01-28</created><authors><author><keyname>Pfister</keyname><forenames>Zoe</forenames></author><author><keyname>Vierhauser</keyname><forenames>Michael</forenames></author><author><keyname>Medvedova</keyname><forenames>Alzbeta</forenames></author><author><keyname>Schroeder</keyname><forenames>Marie</forenames></author><author><keyname>Rampp</keyname><forenames>Markus</forenames></author><author><keyname>Kronenberg</keyname><forenames>Adrian</forenames></author><author><keyname>Hammerle</keyname><forenames>Albin</forenames></author><author><keyname>Wohlfahrt</keyname><forenames>Georg</forenames></author><author><keyname>Jäger</keyname><forenames>Alexandra</forenames></author><author><keyname>Breu</keyname><forenames>Ruth</forenames></author><author><keyname>Simon</keyname><forenames>Alois</forenames></author></authors><title>FORTE: An Open-Source System for Cost-Effective and Scalable   Environmental Monitoring</title><categories>q-bio.PE cs.NI</categories><acm-class>D.2.1; D.2.11</acm-class><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Forests are an essential part of our biosphere, regulating climate, acting as a sink for greenhouse gases, and providing numerous other ecosystem services. However, they are negatively impacted by climatic stressors such as drought or heat waves. In this paper, we introduce FORTE, an open-source system for environmental monitoring with the aim of understanding how forests react to such stressors. It consists of two key components: (1) a wireless sensor network (WSN) deployed in the forest for data collection, and (2) a Data Infrastructure for data processing, storage, and visualization. The WSN contains a Central Unit capable of transmitting data to the Data Infrastructure via LTE-M and several spatially independent Satellites that collect data over large areas and transmit them wirelessly to the Central Unit. Our prototype deployments show that our solution is cost-effective compared to commercial solutions, energy-efficient with sensor nodes lasting for several months on a single charge, and reliable in terms of data quality. FORTE's flexible architecture makes it suitable for a wide range of environmental monitoring applications beyond forest monitoring. The contributions of this paper are three-fold. First, we describe the high-level requirements necessary for developing an environmental monitoring system. Second, we present an architecture and prototype implementation of the requirements by introducing our FORTE platform and demonstrating its effectiveness through multiple field tests. Lastly, we provide source code, documentation, and hardware design artifacts as part of our open-source repository. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00050</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00050</id><created>2025-01-28</created><authors><author><keyname>Kumar</keyname><forenames>Sandip Sharan Senthil</forenames></author><author><keyname>Thalapanane</keyname><forenames>Sandeep</forenames></author><author><keyname>Peethambari</keyname><forenames>Guru Nandhan Appiya Dilipkumar</forenames></author><author><keyname>SriHari</keyname><forenames>Sourang</forenames></author><author><keyname>Zheng</keyname><forenames>Laura</forenames></author><author><keyname>Lin</keyname><forenames>Ming C.</forenames></author></authors><title>DISC: Dataset for Analyzing Driving Styles In Simulated Crashes for   Mixed Autonomy</title><categories>cs.RO cs.LG</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Handling pre-crash scenarios is still a major challenge for self-driving cars due to limited practical data and human-driving behavior datasets. We introduce DISC (Driving Styles In Simulated Crashes), one of the first datasets designed to capture various driving styles and behaviors in pre-crash scenarios for mixed autonomy analysis. DISC includes over 8 classes of driving styles/behaviors from hundreds of drivers navigating a simulated vehicle through a virtual city, encountering rare-event traffic scenarios. This dataset enables the classification of pre-crash human driving behaviors in unsafe conditions, supporting individualized trajectory prediction based on observed driving patterns. By utilizing a custom-designed VR-based in-house driving simulator, TRAVERSE, data was collected through a driver-centric study involving human drivers encountering twelve simulated accident scenarios. This dataset fills a critical gap in human-centric driving data for rare events involving interactions with autonomous vehicles. It enables autonomous systems to better react to human drivers and optimize trajectory prediction in mixed autonomy environments involving both human-driven and self-driving cars. In addition, individual driving behaviors are classified through a set of standardized questionnaires, carefully designed to identify and categorize driving behavior traits. We correlate data features with driving behaviors, showing that the simulated environment reflects real-world driving styles. DISC is the first dataset to capture how various driving styles respond to accident scenarios, offering significant potential to enhance autonomous vehicle safety and driving behavior analysis in mixed autonomy environments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00051</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00051</id><created>2025-01-28</created><authors><author><keyname>Jing</keyname><forenames>Bowen</forenames><affiliation>Department of Radiation Oncology, University of Texas Southwestern Medical Center</affiliation></author><author><keyname>Wang</keyname><forenames>Jing</forenames><affiliation>Department of Radiation Oncology, University of Texas Southwestern Medical Center</affiliation></author></authors><title>A two-stage dual-task learning strategy for early prediction of   pathological complete response to neoadjuvant chemotherapy for breast cancer   using dynamic contrast-enhanced magnetic resonance images</title><categories>cs.CV physics.med-ph</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Rationale and Objectives: Early prediction of pathological complete response (pCR) can facilitate personalized treatment for breast cancer patients. To improve prediction accuracy at the early time point of neoadjuvant chemotherapy, we proposed a two-stage dual-task learning strategy to train a deep neural network for early prediction of pCR using early-treatment magnetic resonance images. Methods: We developed and validated the two-stage dual-task learning strategy using the dataset from the national-wide, multi-institutional I-SPY2 clinical trial, which included dynamic contrast-enhanced magnetic resonance images acquired at three time points: pretreatment (T0), after 3 weeks (T1), and after 12 weeks of treatment (T2). First, we trained a convolutional long short-term memory network to predict pCR and extract the latent space image features at T2. At the second stage, we trained a dual-task network to simultaneously predict pCR and the image features at T2 using images from T0 and T1. This allowed us to predict pCR earlier without using images from T2. Results: The conventional single-stage single-task strategy gave an area under the receiver operating characteristic curve (AUROC) of 0.799 for pCR prediction using all the data at time points T0 and T1. By using the proposed two-stage dual-task learning strategy, the AUROC was improved to 0.820. Conclusions: The proposed two-stage dual-task learning strategy can improve model performance significantly (p=0.0025) for predicting pCR at the early stage (3rd week) of neoadjuvant chemotherapy. The early prediction model can potentially help physicians to intervene early and develop personalized plans at the early stage of chemotherapy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00052</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00052</id><created>2025-01-28</created><authors><author><keyname>Quintana</keyname><forenames>Gonzalo Iñaki</forenames></author><author><keyname>Vancamberg</keyname><forenames>Laurence</forenames></author><author><keyname>Jugnon</keyname><forenames>Vincent</forenames></author><author><keyname>Desolneux</keyname><forenames>Agnès</forenames></author><author><keyname>Mougeot</keyname><forenames>Mathilde</forenames></author></authors><title>Bridging Contrastive Learning and Domain Adaptation: Theoretical   Perspective and Practical Application</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This work studies the relationship between Contrastive Learning and Domain Adaptation from a theoretical perspective. The two standard contrastive losses, NT-Xent loss (Self-supervised) and Supervised Contrastive loss, are related to the Class-wise Mean Maximum Discrepancy (CMMD), a dissimilarity measure widely used for Domain Adaptation. Our work shows that minimizing the contrastive losses decreases the CMMD and simultaneously improves class-separability, laying the theoretical groundwork for the use of Contrastive Learning in the context of Domain Adaptation. Due to the relevance of Domain Adaptation in medical imaging, we focused the experiments on mammography images. Extensive experiments on three mammography datasets - synthetic patches, clinical (real) patches, and clinical (real) images - show improved Domain Adaptation, class-separability, and classification performance, when minimizing the Supervised Contrastive loss. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00053</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00053</id><created>2025-01-29</created><authors><author><keyname>Wang</keyname><forenames>Xiucheng</forenames></author><author><keyname>Zhao</keyname><forenames>Xuan</forenames></author><author><keyname>Cheng</keyname><forenames>Nan</forenames></author></authors><title>Differentiable Projection-based Learn to Optimize in Wireless   Network-Part I: Convex Constrained (Non-)Convex Programming</title><categories>eess.SY cs.LG cs.SY</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper addresses a class of (non-)convex optimization problems subject to general convex constraints, which pose significant challenges for traditional methods due to their inherent non-convexity and diversity. Conventional convex optimization-based solvers often struggle to efficiently handle these problems in their most general form. While neural network (NN)-based approaches offer a promising alternative, ensuring the feasibility of NN-generated solutions and effectively training the NN remain key hurdles, largely because finite-capacity networks can produce infeasible outputs. To overcome these issues, we propose a projection-based method that projects any infeasible NN output onto the feasible domain, thus guaranteeing strict adherence to the constraints without compromising the NN's optimization capability. Furthermore, we derive the objective function values for both the raw NN outputs and their projected counterparts, along with the gradients of these values with respect to the NN parameters. This derivation enables label-free (unsupervised) training, reducing reliance on labeled data and improving scalability. Experimental results demonstrate that the proposed projection-based method consistently ensures feasibility. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00055</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00055</id><created>2025-01-29</created><authors><author><keyname>Bojic</keyname><forenames>Ljubisa</forenames></author><author><keyname>Dodevska</keyname><forenames>Zorica</forenames></author><author><keyname>Deldjoo</keyname><forenames>Yashar</forenames></author><author><keyname>Pantelic</keyname><forenames>Nenad</forenames></author></authors><title>Towards Recommender Systems LLMs Playground (RecSysLLMsP): Exploring   Polarization and Engagement in Simulated Social Networks</title><categories>cs.SI cs.AI cs.CY cs.HC cs.IR</categories><comments>8 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Given the exponential advancement in AI technologies and the potential escalation of harmful effects from recommendation systems, it is crucial to simulate and evaluate these effects early on. Doing so can help prevent possible damage to both societies and technology companies. This paper introduces the Recommender Systems LLMs Playground (RecSysLLMsP), a novel simulation framework leveraging Large Language Models (LLMs) to explore the impacts of different content recommendation setups on user engagement and polarization in social networks. By creating diverse AI agents (AgentPrompts) with descriptive, static, and dynamic attributes, we assess their autonomous behaviour across three scenarios: Plurality, Balanced, and Similarity. Our findings reveal that the Similarity Scenario, which aligns content with user preferences, maximizes engagement while potentially fostering echo chambers. Conversely, the Plurality Scenario promotes diverse interactions but produces mixed engagement results. Our study emphasizes the need for a careful balance in recommender system designs to enhance user satisfaction while mitigating societal polarization. It underscores the unique value and challenges of incorporating LLMs into simulation environments. The benefits of RecSysLLMsP lie in its potential to calculate polarization effects, which is crucial for assessing societal impacts and determining user engagement levels with diverse recommender system setups. This advantage is essential for developing and maintaining a successful business model for social media companies. However, the study's limitations revolve around accurately emulating reality. Future efforts should validate the similarity in behaviour between real humans and AgentPrompts and establish metrics for measuring polarization scores. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00057</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00057</id><created>2025-01-29</created><authors><author><keyname>Uenohara</keyname><forenames>Seiji</forenames></author><author><keyname>Awamura</keyname><forenames>Satoshi</forenames></author><author><keyname>Hattori</keyname><forenames>Norio</forenames></author></authors><title>CuLD: Current-Limiting Differential Reading Circuit for Current-Based   Compute-in-Memory</title><categories>cs.AR</categories><comments>3 pages, 10 figures, 2 tables, conference(2025 Symposium on VLSI   Technology and Circuits)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a circuit configuration that addresses the issue of deviation in the multiply-accumulate (MAC) results when numerous word lines are simultaneously opened in current-based compute-in-memory (CiM) circuits. The proposed circuit solves this problem by automatically shrinking the product value according to the degree of parallelism. This circuit configuration is effective for circuit methods that calculate MAC through time integration of charge. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00058</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00058</id><created>2025-01-29</created><authors><author><keyname>Thakrar</keyname><forenames>Karishma</forenames></author><author><keyname>Chauhan</keyname><forenames>Aniket</forenames></author></authors><title>GitHub Stargazers | Building Graph- and Edge-level Prediction Algorithms   for Developer Social Networks</title><categories>cs.SI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Analyzing social networks formed by developers provides valuable insights for market segmentation, trend analysis, and community engagement. In this study, we explore the GitHub Stargazers dataset to classify developer communities and predict potential collaborations using graph neural networks (GNNs). By modeling 12,725 developer networks, we segment communities based on their focus on web development or machine learning repositories, leveraging graph attributes and node embeddings. Furthermore, we propose an edge-level recommendation algorithm that predicts new connections between developers using similarity measures. Our experimental results demonstrate the effectiveness of our approach in accurately segmenting communities and improving connection predictions, offering valuable insights for understanding open-source developer networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00059</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00059</id><created>2025-01-29</created><authors><author><keyname>Chen</keyname><forenames>Yakun</forenames></author><author><keyname>Li</keyname><forenames>Zihao</forenames></author><author><keyname>Yang</keyname><forenames>Chao</forenames></author><author><keyname>Wang</keyname><forenames>Xianzhi</forenames></author><author><keyname>Xu</keyname><forenames>Guandong</forenames></author></authors><title>Large Language Models are Few-shot Multivariate Time Series Classifiers</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large Language Models (LLMs) have been extensively applied in time series analysis. Yet, their utility in the few-shot classification (i.e., a crucial training scenario due to the limited training data available in industrial applications) concerning multivariate time series data remains underexplored. We aim to leverage the extensive pre-trained knowledge in LLMs to overcome the data scarcity problem within multivariate time series. Specifically, we propose LLMFew, an LLM-enhanced framework to investigate the feasibility and capacity of LLMs for few-shot multivariate time series classification. This model introduces a Patch-wise Temporal Convolution Encoder (PTCEnc) to align time series data with the textual embedding input of LLMs. We further fine-tune the pre-trained LLM decoder with Low-rank Adaptations (LoRA) to enhance its feature representation learning ability in time series data. Experimental results show that our model outperformed state-of-the-art baselines by a large margin, achieving 125.2% and 50.2% improvement in classification accuracy on Handwriting and EthanolConcentration datasets, respectively. Moreover, our experimental results demonstrate that LLM-based methods perform well across a variety of datasets in few-shot MTSC, delivering reliable results compared to traditional models. This success paves the way for their deployment in industrial environments where data are limited. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00060</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00060</id><created>2025-01-30</created><authors><author><keyname>Antonakaki</keyname><forenames>Despoina</forenames></author><author><keyname>Ioannidis</keyname><forenames>Sotiris</forenames></author></authors><title>Israel-Hamas war through Telegram, Reddit and Twitter</title><categories>cs.SI cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Israeli-Palestinian conflict started on 7 October 2023, have resulted thus far to over 48,000 people killed including more than 17,000 children with a majority from Gaza, more than 30,000 people injured, over 10,000 missing, and over 1 million people displaced, fleeing conflict zones. The infrastructure damage includes the 87\% of housing units, 80\% of public buildings and 60\% of cropland 17 out of 36 hospitals, 68\% of road networks and 87\% of school buildings damaged. This conflict has as well launched an online discussion across various social media platforms. Telegram was no exception due to its encrypted communication and highly involved audience. The current study will cover an analysis of the related discussion in relation to different participants of the conflict and sentiment represented in those discussion. To this end, we prepared a dataset of 125K messages shared on channels in Telegram spanning from 23 October 2025 until today. Additionally, we apply the same analysis in two publicly available datasets from Twitter containing 2001 tweets and from Reddit containing 2M opinions. We apply a volume analysis across the three datasets, entity extraction and then proceed to BERT topic analysis in order to extract common themes or topics. Next, we apply sentiment analysis to analyze the emotional tone of the discussions. Our findings hint at polarized narratives as the hallmark of how political factions and outsiders mold public opinion. We also analyze the sentiment-topic prevalence relationship, detailing the trends that may show manipulation and attempts of propaganda by the involved parties. This will give a better understanding of the online discourse on the Israel-Palestine conflict and contribute to the knowledge on the dynamics of social media communication during geopolitical crises. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00061</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00061</id><created>2025-01-30</created><authors><author><keyname>Fu</keyname><forenames>Qian</forenames></author><author><keyname>Zhang</keyname><forenames>Yuzhe</forenames></author><author><keyname>Shu</keyname><forenames>Yanfeng</forenames></author><author><keyname>Ding</keyname><forenames>Ming</forenames></author><author><keyname>Yao</keyname><forenames>Lina</forenames></author><author><keyname>Wang</keyname><forenames>Chen</forenames></author></authors><title>From Data to Action: Charting A Data-Driven Path to Combat Antimicrobial   Resistance</title><categories>cs.LG cs.AI q-bio.PE</categories><comments>29 pages, 3 figures, 4 tables, survey paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Antimicrobial-resistant (AMR) microbes are a growing challenge in healthcare, rendering modern medicines ineffective. AMR arises from antibiotic production and bacterial evolution, but quantifying its transmission remains difficult. With increasing AMR-related data, data-driven methods offer promising insights into its causes and treatments. This paper reviews AMR research from a data analytics and machine learning perspective, summarizing the state-of-the-art and exploring key areas such as surveillance, prediction, drug discovery, stewardship, and driver analysis. It discusses data sources, methods, and challenges, emphasizing standardization and interoperability. Additionally, it surveys statistical and machine learning techniques for AMR analysis, addressing issues like data noise and bias. Strategies for denoising and debiasing are highlighted to enhance fairness and robustness in AMR research. The paper underscores the importance of interdisciplinary collaboration and awareness of data challenges in advancing AMR research, pointing to future directions for innovation and improved methodologies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00063</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00063</id><created>2025-01-30</created><authors><author><keyname>Mohamed</keyname><forenames>Malak</forenames></author><author><keyname>Emad</keyname><forenames>Rokaia</forenames></author><author><keyname>Hamdi</keyname><forenames>Ali</forenames></author></authors><title>A Multi-Layered Large Language Model Framework for Disease Prediction</title><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social telehealth has revolutionized healthcare by enabling patients to share symptoms and receive medical consultations remotely. Users frequently post symptoms on social media and online health platforms, generating a vast repository of medical data that can be leveraged for disease classification and symptom severity assessment. Large language models (LLMs), such as LLAMA3, GPT-3.5 Turbo, and BERT, process complex medical data to enhance disease classification. This study explores three Arabic medical text preprocessing techniques: text summarization, text refinement, and Named Entity Recognition (NER). Evaluating CAMeL-BERT, AraBERT, and Asafaya-BERT with LoRA, the best performance was achieved using CAMeL-BERT with NER-augmented text (83% type classification, 69% severity assessment). Non-fine-tuned models performed poorly (13%-20% type classification, 40%-49% severity assessment). Integrating LLMs into social telehealth systems enhances diagnostic accuracy and treatment outcomes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00064</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00064</id><created>2025-01-30</created><authors><author><keyname>Lin</keyname><forenames>Jie</forenames></author><author><keyname>Mohaisen</keyname><forenames>David</forenames></author></authors><title>Evaluating Large Language Models in Vulnerability Detection Under   Variable Context Windows</title><categories>cs.CR cs.LG</categories><comments>5 pages, 2 tables. Appeared in ICMLA 2024</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This study examines the impact of tokenized Java code length on the accuracy and explicitness of ten major LLMs in vulnerability detection. Using chi-square tests and known ground truth, we found inconsistencies across models: some, like GPT-4, Mistral, and Mixtral, showed robustness, while others exhibited a significant link between tokenized length and performance. We recommend future LLM development focus on minimizing the influence of input length for better vulnerability detection. Additionally, preprocessing techniques that reduce token count while preserving code structure could enhance LLM accuracy and explicitness in these tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00065</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00065</id><created>2025-01-30</created><authors><author><keyname>Chu</keyname><forenames>Soon Jynn</forenames></author><author><keyname>Amarasiri</keyname><forenames>Nalaka</forenames></author><author><keyname>Giri</keyname><forenames>Sandesh</forenames></author><author><keyname>Kafle</keyname><forenames>Priyata</forenames></author></authors><title>Blood Glucose Level Prediction in Type 1 Diabetes Using Machine Learning</title><categories>q-bio.QM cs.LG</categories><comments>15 pages, 7 figures. This work was accepted for CSCI 2024 conference</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Type 1 Diabetes is a chronic autoimmune condition in which the immune system attacks and destroys insulin-producing beta cells in the pancreas, resulting in little to no insulin production. Insulin helps glucose in your blood enter your muscle, fat, and liver cells so they can use it for energy or store it for later use. If insulin is insufficient, it causes sugar to build up in the blood and leads to serious health problems. People with Type 1 Diabetes need synthetic insulin every day. In diabetes management, continuous glucose monitoring is an important feature that provides near real-time blood glucose data. It is useful in deciding the synthetic insulin dose. In this research work, we used machine learning tools, deep neural networks, deep reinforcement learning, and voting and stacking regressors to predict blood glucose levels at 30-min time intervals using the latest DiaTrend dataset. Predicting blood glucose levels is useful in better diabetes management systems. The trained models were compared using several evaluation metrics. Our evaluation results demonstrate the performance of various models across different glycemic conditions for blood glucose prediction. The source codes of this work can be found in: https://github.com/soon-jynn-chu/t1d_bg_prediction </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00066</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00066</id><created>2025-01-30</created><authors><author><keyname>White</keyname><forenames>Brianna M</forenames></author><author><keyname>Prasad</keyname><forenames>Rameshwari</forenames></author><author><keyname>Ammar</keyname><forenames>Nariman</forenames></author><author><keyname>Yaun</keyname><forenames>Jason A</forenames></author><author><keyname>Shaban-Nejad</keyname><forenames>Arash</forenames></author></authors><title>Digital Health Innovations for Screening and Mitigating Mental Health   Impacts of Adverse Childhood Experiences: Narrative Review</title><categories>cs.CY cs.HC</categories><comments>10 Pages, 1 Figure</comments><acm-class>I.2.1</acm-class><journal-ref>JMIR Pediatrics and Parenting 2024;7:e58403</journal-ref><doi>10.2196/58403</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This study presents a narrative review of the use of digital health technologies (DHTs) and artificial intelligence to screen and mitigate risks and mental health consequences associated with ACEs among children and youth. Several databases were searched for studies published from August 2017 to August 2022. Selected studies (1) explored the relationship between digital health interventions and mitigation of negative health outcomes associated with mental health in childhood and adolescence and (2) examined prevention of ACE occurrence associated with mental illness in childhood and adolescence. A total of 18 search papers were selected, according to our inclusion and exclusion criteria, to evaluate and identify means by which existing digital solutions may be useful in mitigating the mental health consequences associated with the occurrence of ACEs in childhood and adolescence and preventing ACE occurrence due to mental health consequences. We also highlighted a few knowledge gaps or barriers to DHT implementation and usability. Findings from the search suggest that the incorporation of DHTs, if implemented successfully, has the potential to improve the quality of related care provisions for the management of mental health consequences of adverse or traumatic events in childhood, including posttraumatic stress disorder, suicidal behavior or ideation, anxiety or depression, and attention-deficit/hyperactivity disorder. The use of DHTs, machine learning tools, natural learning processing, and artificial intelligence can positively help in mitigating ACEs and associated risk factors. Under proper legal regulations, security, privacy, and confidentiality assurances, digital technologies could also assist in promoting positive childhood experiences in children and young adults, bolstering resilience, and providing reliable public health resources to serve populations in need. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00067</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00067</id><created>2025-01-30</created><authors><author><keyname>Hassan</keyname><forenames>Muhammad</forenames></author><author><keyname>Ghani</keyname><forenames>Abdullah</forenames></author><author><keyname>Zaffar</keyname><forenames>Muhammad Fareed</forenames></author><author><keyname>Bashir</keyname><forenames>Masooda</forenames></author></authors><title>Decoding User Concerns in AI Health Chatbots: An Exploration of Security   and Privacy in App Reviews</title><categories>cs.CR cs.ET cs.HC</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  AI powered health chatbot applications are increasingly utilized for personalized healthcare services, yet they pose significant challenges related to user data security and privacy. This study evaluates the effectiveness of automated methods, specifically BART and Gemini GenAI, in identifying security privacy related (SPR) concerns within these applications' user reviews, benchmarking their performance against manual qualitative analysis. Our results indicate that while Gemini's performance in SPR classification is comparable to manual labeling, both automated methods have limitations, including the misclassification of unrelated issues. Qualitative analysis revealed critical user concerns, such as data collection practices, data misuse, and insufficient transparency and consent mechanisms. This research enhances the understanding of the relationship between user trust, privacy, and emerging mobile AI health chatbot technologies, offering actionable insights for improving security and privacy practices in AI driven health chatbots. Although exploratory, our findings highlight the necessity for rigorous audits and transparent communication strategies, providing valuable guidance for app developers and vendors in addressing user security and privacy concerns. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00068</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00068</id><created>2025-01-30</created><authors><author><keyname>Marlin</keyname><forenames>Robert</forenames></author><author><keyname>Jurdak</keyname><forenames>Raja</forenames></author><author><keyname>Abuadbba</keyname><forenames>Alsharif</forenames></author><author><keyname>Miller</keyname><forenames>Dimity</forenames></author></authors><title>Privacy Preserving Charge Location Prediction for Electric Vehicles</title><categories>cs.CR cs.AI</categories><comments>12 pages, 7 figures, IEEE Journal paper</comments><acm-class>I.6.5</acm-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  By 2050, electric vehicles (EVs) are projected to account for 70% of global vehicle sales. While EVs provide environmental benefits, they also pose challenges for energy generation, grid infrastructure, and data privacy. Current research on EV routing and charge management often overlooks privacy when predicting energy demands, leaving sensitive mobility data vulnerable. To address this, we developed a Federated Learning Transformer Network (FLTN) to predict EVs' next charge location with enhanced privacy measures. Each EV operates as a client, training an onboard FLTN model that shares only model weights, not raw data with a community-based Distributed Energy Resource Management System (DERMS), which aggregates them into a community global model. To further enhance privacy, non-transitory EVs use peer-to-peer weight sharing and augmentation within their community, obfuscating individual contributions and improving model accuracy. Community DERMS global model weights are then redistributed to EVs for continuous training. Our FLTN approach achieved up to 92% accuracy while preserving data privacy, compared to our baseline centralised model, which achieved 98% accuracy with no data privacy. Simulations conducted across diverse charge levels confirm the FLTN's ability to forecast energy demands over extended periods. We present a privacy-focused solution for forecasting EV charge location prediction, effectively mitigating data leakage risks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00069</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00069</id><created>2025-01-30</created><authors><author><keyname>Yang</keyname><forenames>Yuanlin</forenames></author></authors><title>Large Capacity Data Hiding in Binary Image black and white mixed regions</title><categories>cs.CR</categories><comments>6 pages,2023 3rd International Conference on Electronic Information   Engineering and Computer (EIECT)</comments><doi>10.1109/EIECT60552.2023.10441974</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information hiding technology utilizes the insensitivity of human sensory organs to redundant data, hiding confidential information in the redundant data of these public digital media, and then transmitting it. The carrier media after hiding secret information only displays its own characteristics, which can ensure the transmission of confidential information without being detected, thereby greatly improving the security of the information. In theory, any digital media including image, video, audio, and text can serve as a host carrier. Among them, hiding information in binary images poses great challenges. As we know, any information hiding method involves modifying the data of the host carrier. The more information hidden, the more data of the host carrier are modified. In this paper, we propose information hiding in the black-and-white mixed region of binary images, which can greatly reduce visual distortion. In addition, we propose an efficient encoding to achieve high-capacity information hiding while ensuring image semantics. By selecting binary images of different themes, we conduct experiments. The experimental results prove the feasibility of our technique and verify the expected performance. Since the candidate units for information hiding are selected from equally sized blocks that the image is divided into, and the hiding and extraction of information are based on a shared encoding table, the computational cost is very low, making it suitable for real-time information hiding applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00070</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00070</id><created>2025-01-30</created><authors><author><keyname>Pataranutaporn</keyname><forenames>Pat</forenames></author><author><keyname>Powdthavee</keyname><forenames>Nattavudh</forenames></author><author><keyname>Maes</keyname><forenames>Pattie</forenames></author></authors><title>Can AI Solve the Peer Review Crisis? A Large Scale Experiment on LLM's   Performance and Biases in Evaluating Economics Papers</title><categories>cs.CY cs.AI econ.GN q-fin.EC</categories><comments>72 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We investigate whether artificial intelligence can address the peer review crisis in economics by analyzing 27,090 evaluations of 9,030 unique submissions using a large language model (LLM). The experiment systematically varies author characteristics (e.g., affiliation, reputation, gender) and publication quality (e.g., top-tier, mid-tier, low-tier, AI generated papers). The results indicate that LLMs effectively distinguish paper quality but exhibit biases favoring prominent institutions, male authors, and renowned economists. Additionally, LLMs struggle to differentiate high-quality AI-generated papers from genuine top-tier submissions. While LLMs offer efficiency gains, their susceptibility to bias necessitates cautious integration and hybrid peer review models to balance equity and accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00071</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00071</id><created>2025-01-30</created><authors><author><keyname>Zarebski</keyname><forenames>Alexander E.</forenames></author><author><keyname>Tellioglu</keyname><forenames>Nefel</forenames></author><author><keyname>Stockdale</keyname><forenames>Jessica E.</forenames></author><author><keyname>Spencer</keyname><forenames>Julie A.</forenames></author><author><keyname>KhudaBukhsh</keyname><forenames>Wasiur R.</forenames></author><author><keyname>Miller</keyname><forenames>Joel C.</forenames></author><author><keyname>Zachreson</keyname><forenames>Cameron</forenames></author></authors><title>Including frameworks of public health ethics in computational modelling   of infectious disease interventions</title><categories>physics.soc-ph cs.CY q-bio.PE</categories><comments>33 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decisions on public health interventions to control infectious disease are often informed by computational models. Interpreting the predicted outcomes of a public health decision requires not only high-quality modelling, but also an ethical framework for assessing the benefits and harms associated with different options. The design and specification of ethical frameworks matured independently of computational modelling, so many values recognised as important for ethical decision-making are missing from computational models. We demonstrate a proof-of-concept approach to incorporate multiple public health values into the evaluation of a simple computational model for vaccination against a pathogen such as SARS-CoV-2. By examining a bounded space of alternative prioritisations of values (outcome equity and aggregate benefit) we identify value trade-offs, where the outcomes of optimal strategies differ depending on the ethical framework. This work demonstrates an approach to incorporating diverse values into decision criteria used to evaluate outcomes of models of infectious disease interventions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00072</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00072</id><created>2025-01-31</created><authors><author><keyname>Lukošiūtė</keyname><forenames>Kamilė</forenames></author><author><keyname>Swanda</keyname><forenames>Adam</forenames></author></authors><title>LLM Cyber Evaluations Don't Capture Real-World Risk</title><categories>cs.CR cs.AI cs.CL cs.LG</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large language models (LLMs) are demonstrating increasing prowess in cybersecurity applications, creating creating inherent risks alongside their potential for strengthening defenses. In this position paper, we argue that current efforts to evaluate risks posed by these capabilities are misaligned with the goal of understanding real-world impact. Evaluating LLM cybersecurity risk requires more than just measuring model capabilities -- it demands a comprehensive risk assessment that incorporates analysis of threat actor adoption behavior and potential for impact. We propose a risk assessment framework for LLM cyber capabilities and apply it to a case study of language models used as cybersecurity assistants. Our evaluation of frontier models reveals high compliance rates but moderate accuracy on realistic cyber assistance tasks. However, our framework suggests that this particular use case presents only moderate risk due to limited operational advantages and impact potential. Based on these findings, we recommend several improvements to align research priorities with real-world impact assessment, including closer academia-industry collaboration, more realistic modeling of attacker behavior, and inclusion of economic metrics in evaluations. This work represents an important step toward more effective assessment and mitigation of LLM-enabled cybersecurity risks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00074</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00074</id><created>2025-01-31</created><authors><author><keyname>Paek</keyname><forenames>Dong-Hee</forenames></author><author><keyname>Kong</keyname><forenames>Seung-Hyun</forenames></author></authors><title>SpikingRTNH: Spiking Neural Network for 4D Radar Object Detection</title><categories>cs.CV cs.AI cs.NE</categories><comments>arxiv preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, 4D Radar has emerged as a crucial sensor for 3D object detection in autonomous vehicles, offering both stable perception in adverse weather and high-density point clouds for object shape recognition. However, processing such high-density data demands substantial computational resources and energy consumption. We propose SpikingRTNH, the first spiking neural network (SNN) for 3D object detection using 4D Radar data. By replacing conventional ReLU activation functions with leaky integrate-and-fire (LIF) spiking neurons, SpikingRTNH achieves significant energy efficiency gains. Furthermore, inspired by human cognitive processes, we introduce biological top-down inference (BTI), which processes point clouds sequentially from higher to lower densities. This approach effectively utilizes points with lower noise and higher importance for detection. Experiments on K-Radar dataset demonstrate that SpikingRTNH with BTI significantly reduces energy consumption by 78% while achieving comparable detection performance to its ANN counterpart (51.1% AP 3D, 57.0% AP BEV). These results establish the viability of SNNs for energy-efficient 4D Radar-based object detection in autonomous driving systems. All codes are available at https://github.com/kaist-avelab/k-radar. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00075</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00075</id><created>2025-01-31</created><authors><author><keyname>Zhang</keyname><forenames>Qizhen</forenames></author><author><keyname>Bhargava</keyname><forenames>Prajjwal</forenames></author><author><keyname>Bi</keyname><forenames>Chloe</forenames></author><author><keyname>Cai</keyname><forenames>Chris X.</forenames></author><author><keyname>Foerster</keyname><forenames>Jakob</forenames></author><author><keyname>Fu</keyname><forenames>Jeremy</forenames></author><author><keyname>Koura</keyname><forenames>Punit Singh</forenames></author><author><keyname>Silva</keyname><forenames>Ruan</forenames></author><author><keyname>Shen</keyname><forenames>Sheng</forenames></author><author><keyname>Dinan</keyname><forenames>Emily</forenames></author><author><keyname>Gururangan</keyname><forenames>Suchin</forenames></author><author><keyname>Lewis</keyname><forenames>Mike</forenames></author></authors><title>BTS: Harmonizing Specialized Experts into a Generalist LLM</title><categories>cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present Branch-Train-Stitch (BTS), an efficient and flexible training algorithm for combining independently trained large language model (LLM) experts into a single, capable generalist model. Following Li et al., we start with a single seed language model which is branched into domain-specific (e.g., coding or math) experts with continual pretraining. BTS combines experts into a generalist model using lightweight stitch layers, which are inserted between frozen experts and the seed LLM, and trained on a small datamix of the expert domains. Stitch layers enable the seed LLM to integrate representations from any number of experts during the forward pass, allowing it to generalize to new domains, despite remaining frozen. Because BTS does not alter the constituent LLMs, BTS provides a modular and flexible approach: experts can be easily removed and new experts can be added with only a small amount of training. Compared to alternative model merging approaches, BTS yields the best generalist performance on a variety of downstream tasks, retaining the specialized capabilities of each of the experts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00076</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00076</id><created>2025-01-31</created><authors><author><keyname>Agossou</keyname><forenames>Bidossessi Emmanuel</forenames></author><author><keyname>Pedersen</keyname><forenames>Marius</forenames></author><author><keyname>Raja</keyname><forenames>Kiran</forenames></author><author><keyname>Vats</keyname><forenames>Anuja</forenames></author><author><keyname>Floor</keyname><forenames>Pål Anders</forenames></author></authors><title>Influence of color correction on pathology detection in Capsule   Endoscopy</title><categories>cs.CV cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Pathology detection in Wireless Capsule Endoscopy (WCE) using deep learning has been explored in the recent past. However, deep learning models can be influenced by the color quality of the dataset used to train them, impacting detection, segmentation and classification tasks. In this work, we evaluate the impact of color correction on pathology detection using two prominent object detection models: Retinanet and YOLOv5. We first generate two color corrected versions of a popular WCE dataset (i.e., SEE-AI dataset) using two different color correction functions. We then evaluate the performance of the Retinanet and YOLOv5 on the original and color corrected versions of the dataset. The results reveal that color correction makes the models generate larger bounding boxes and larger intersection areas with the ground truth annotations. Furthermore, color correction leads to an increased number of false positives for certain pathologies. However, these effects do not translate into a consistent improvement in performance metrics such as F1-scores, IoU, and AP50. The code is available at https://github.com/agossouema2011/WCE2024. Keywords: Wireless Capsule Endoscopy, Color correction, Retinanet, YOLOv5, Detection </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00077</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00077</id><created>2025-01-31</created><authors><author><keyname>Antero</keyname><forenames>Unai</forenames></author><author><keyname>Sierra</keyname><forenames>Basilio</forenames></author><author><keyname>Oñativia</keyname><forenames>Jon</forenames></author><author><keyname>Ruiz</keyname><forenames>Alejandra</forenames></author><author><keyname>Osaba</keyname><forenames>Eneko</forenames></author></authors><title>Robot localization aided by quantum algorithms</title><categories>cs.RO</categories><comments>20 pages, 18 figures. Preprint</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Localization is a critical aspect of mobile robotics, enabling robots to navigate their environment efficiently and avoid obstacles. Current probabilistic localization methods, such as the Adaptive-Monte Carlo localization (AMCL) algorithm, are computationally intensive and may struggle with large maps or high-resolution sensor data. This paper explores the application of quantum computing in robotics, focusing on the use of Grover's search algorithm to improve the efficiency of localization in mobile robots. We propose a novel approach to utilize Grover's algorithm in a 2D map, enabling faster and more efficient localization. Despite the limitations of current physical quantum computers, our experimental results demonstrate a significant speedup over classical methods, highlighting the potential of quantum computing to improve robotic localization. This work bridges the gap between quantum computing and robotics, providing a practical solution for robotic localization and paving the way for future research in quantum robotics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00078</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00078</id><created>2025-01-31</created><authors><author><keyname>Pal</keyname><forenames>Surochita</forenames></author><author><keyname>Mitra</keyname><forenames>Sushmita</forenames></author></authors><title>Deep Ensembling with Multimodal Image Fusion for Efficient   Classification of Lung Cancer</title><categories>eess.IV cs.CV</categories><doi>10.1109/ICCCNT61001.2024.10726043</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This study focuses on the classification of cancerous and healthy slices from multimodal lung images. The data used in the research comprises Computed Tomography (CT) and Positron Emission Tomography (PET) images. The proposed strategy achieves the fusion of PET and CT images by utilizing Principal Component Analysis (PCA) and an Autoencoder. Subsequently, a new ensemble-based classifier developed, Deep Ensembled Multimodal Fusion (DEMF), employing majority voting to classify the sample images under examination. Gradient-weighted Class Activation Mapping (Grad-CAM) employed to visualize the classification accuracy of cancer-affected images. Given the limited sample size, a random image augmentation strategy employed during the training phase. The DEMF network helps mitigate the challenges of scarce data in computer-aided medical image analysis. The proposed network compared with state-of-the-art networks across three publicly available datasets. The network outperforms others based on the metrics - Accuracy, F1-Score, Precision, and Recall. The investigation results highlight the effectiveness of the proposed network. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00079</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00079</id><created>2025-01-31</created><authors><author><keyname>Degerli</keyname><forenames>Aysen</forenames></author><author><keyname>Hilvo</keyname><forenames>Mika</forenames></author><author><keyname>Pajula</keyname><forenames>Juha</forenames></author><author><keyname>Huhtinen</keyname><forenames>Petri</forenames></author><author><keyname>Jäkälä</keyname><forenames>Pekka</forenames></author></authors><title>Advanced Assessment of Stroke in Retinal Fundus Imaging with Deep   Multi-view Learning</title><categories>eess.IV cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Stroke is globally a major cause of mortality and morbidity, and hence accurate and rapid diagnosis of stroke is valuable. Retinal fundus imaging reveals the known markers of elevated stroke risk in the eyes, which are retinal venular widening, arteriolar narrowing, and increased tortuosity. In contrast to other imaging techniques used for stroke diagnosis, the acquisition of fundus images is easy, non-invasive, fast, and inexpensive. Therefore, in this study, we propose a multi-view stroke network (MVS-Net) to detect stroke and transient ischemic attack (TIA) using retinal fundus images. Contrary to existing studies, our study proposes for the first time a solution to discriminate stroke and TIA with deep multi-view learning by proposing an end-to-end deep network, consisting of multi-view inputs of fundus images captured from both right and left eyes. Accordingly, the proposed MVS-Net defines representative features from fundus images of both eyes and determines the relation within their macula-centered and optic nerve head-centered views. Experiments performed on a dataset collected from stroke and TIA patients, in addition to healthy controls, show that the proposed framework achieves an AUC score of 0.84 for stroke and TIA detection. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00083</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00083</id><created>2025-01-31</created><authors><author><keyname>Miranda</keyname><forenames>Mateus de Souza</forenames></author><author><keyname>Hänsch</keyname><forenames>Ronny</forenames></author><author><keyname>Júnior</keyname><forenames>Valdivino Alexandre de Santiago</forenames></author><author><keyname>Körting</keyname><forenames>Thales Sehn</forenames></author><author><keyname>Monteiro</keyname><forenames>Erison Carlos dos Santos</forenames></author></authors><title>CerraData-4MM: A multimodal benchmark dataset on Cerrado for land use   and land cover classification</title><categories>cs.CV eess.IV</categories><comments>9 pages, 13 Figures, 3 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The Cerrado faces increasing environmental pressures, necessitating accurate land use and land cover (LULC) mapping despite challenges such as class imbalance and visually similar categories. To address this, we present CerraData-4MM, a multimodal dataset combining Sentinel-1 Synthetic Aperture Radar (SAR) and Sentinel-2 MultiSpectral Imagery (MSI) with 10m spatial resolution. The dataset includes two hierarchical classification levels with 7 and 14 classes, respectively, focusing on the diverse Bico do Papagaio ecoregion. We highlight CerraData-4MM's capacity to benchmark advanced semantic segmentation techniques by evaluating a standard U-Net and a more sophisticated Vision Transformer (ViT) model. The ViT achieves superior performance in multimodal scenarios, with the highest macro F1-score of 57.60% and a mean Intersection over Union (mIoU) of 49.05% at the first hierarchical level. Both models struggle with minority classes, particularly at the second hierarchical level, where U-Net's performance drops to an F1-score of 18.16%. Class balancing improves representation for underrepresented classes but reduces overall accuracy, underscoring the trade-off in weighted training. CerraData-4MM offers a challenging benchmark for advancing deep learning models to handle class imbalance and multimodal data fusion. Code, trained models, and data are publicly available at https://github.com/ai4luc/CerraData-4MM. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00085</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00085</id><created>2025-01-31</created><authors><author><keyname>Chan</keyname><forenames>Brian J</forenames></author><author><keyname>Cheng</keyname><forenames>Jui-Hung</forenames></author><author><keyname>Huang</keyname><forenames>Mao Xun</forenames></author><author><keyname>Chen</keyname><forenames>Chao-Ting</forenames></author><author><keyname>Huang</keyname><forenames>Hen-Hsen</forenames></author></authors><title>Efficient Beam Search for Large Language Models Using Trie-Based   Decoding</title><categories>cs.CL</categories><comments>9 pages</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In Transformer-based sequence-to-sequence generation, beam search has proven effective in enhancing the quality of generated sequences compared to greedy decoding. Conventional beam search methods typically adopt either a sequential or batch-based approach. The sequential approach, while memory-efficient, requires multiple decoding passes to construct a complete search tree, leading to significantly slower inference. On the other hand, the batch-based approach enables parallel computation across beams, but at the expense of high memory consumption due to the need to maintain separate key-value (KV) caches for each beam. In this study, we introduce a novel trie (prefix-tree)-based parallel decoding method that addresses the memory inefficiency of batch-based beam search. By sharing a single KV cache among all beams that share the same prefix, the proposed method not only reduces memory consumption dramatically but also enables parallel decoding across all branches. This innovative use of a prefix tree offers an efficient alternative for beam search, achieving significant memory savings while preserving inference speed, making it particularly well-suited for memory-constrained environments or large-scale model deployments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00088</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00088</id><created>2025-01-31</created><authors><author><keyname>Salih</keyname><forenames>Ahmed M.</forenames></author></authors><title>Re-Visiting Explainable AI Evaluation Metrics to Identify The Most   Informative Features</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Functionality or proxy-based approach is one of the used approaches to evaluate the quality of explainable artificial intelligence methods. It uses statistical methods, definitions and new developed metrics for the evaluation without human intervention. Among them, Selectivity or RemOve And Retrain (ROAR), and Permutation Importance (PI) are the most commonly used metrics to evaluate the quality of explainable artificial intelligence methods to highlight the most significant features in machine learning models. They state that the model performance should experience a sharp reduction if the most informative feature is removed from the model or permuted. However, the efficiency of both metrics is significantly affected by multicollinearity, number of significant features in the model and the accuracy of the model. This paper shows with empirical examples that both metrics suffer from the aforementioned limitations. Accordingly, we propose expected accuracy interval (EAI), a metric to predict the upper and lower bounds of the the accuracy of the model when ROAR or IP is implemented. The proposed metric found to be very useful especially with collinear features. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00089</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00089</id><created>2025-01-31</created><authors><author><keyname>Li</keyname><forenames>Yinghao</forenames></author><author><keyname>Gao</keyname><forenames>Vianne</forenames></author><author><keyname>Zhang</keyname><forenames>Chao</forenames></author><author><keyname>Torkamani</keyname><forenames>MohamadAli</forenames></author></authors><title>Ensembles of Low-Rank Expert Adapters</title><categories>cs.CL cs.AI cs.LG</categories><comments>29 pages, 5 figures, 5 tables; proceedings in ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The training and fine-tuning of large language models (LLMs) often involve diverse textual data from multiple sources, which poses challenges due to conflicting gradient directions, hindering optimization and specialization. These challenges can undermine model generalization across tasks, resulting in reduced downstream performance. Recent research suggests that fine-tuning LLMs on carefully selected, task-specific subsets of data can match or even surpass the performance of using the entire dataset. Building on these insights, we propose the Ensembles of Low-Rank Expert Adapters (ELREA) framework to improve the model's capability to handle diverse tasks. ELREA clusters the training instructions based on their gradient directions, representing different areas of expertise and thereby reducing conflicts during optimization. Expert adapters are then trained on these clusters, utilizing the low-rank adaptation (LoRA) technique to ensure training efficiency and model scalability. During inference, ELREA combines predictions from the most relevant expert adapters based on the input data's gradient similarity to the training clusters, ensuring optimal adapter selection for each task. Experiments show that our method outperforms baseline LoRA adapters trained on the full dataset and other ensemble approaches with similar training and inference complexity across a range of domain-specific tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00090</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00090</id><created>2025-01-31</created><authors><author><keyname>Born</keyname><forenames>Logan</forenames></author><author><keyname>Monroe</keyname><forenames>M. Willis</forenames></author><author><keyname>Kelley</keyname><forenames>Kathryn</forenames></author><author><keyname>Sarkar</keyname><forenames>Anoop</forenames></author></authors><title>Disambiguating Numeral Sequences to Decipher Ancient Accounting Corpora</title><categories>cs.CL</categories><doi>10.18653/v1/2023.cawl-1.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A numeration system encodes abstract numeric quantities as concrete strings of written characters. The numeration systems used by modern scripts tend to be precise and unambiguous, but this was not so for the ancient and partially-deciphered proto-Elamite (PE) script, where written numerals can have up to four distinct readings depending on the system that is used to read them. We consider the task of disambiguating between these readings in order to determine the values of the numeric quantities recorded in this corpus. We algorithmically extract a list of possible readings for each PE numeral notation, and contribute two disambiguation techniques based on structural properties of the original documents and classifiers learned with the bootstrapping algorithm. We also contribute a test set for evaluating disambiguation techniques, as well as a novel approach to cautious rule selection for bootstrapped classifiers. Our analysis confirms existing intuitions about this script and reveals previously-unknown correlations between tablet content and numeral magnitude. This work is crucial to understanding and deciphering PE, as the corpus is heavily accounting-focused and contains many more numeric tokens than tokens of text. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00094</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00094</id><created>2025-01-31</created><authors><author><keyname>Heakl</keyname><forenames>Ahmed</forenames></author><author><keyname>Ghaboura</keyname><forenames>Sara</forenames></author><author><keyname>Thawkar</keyname><forenames>Omkar</forenames></author><author><keyname>Khan</keyname><forenames>Fahad Shahbaz</forenames></author><author><keyname>Cholakkal</keyname><forenames>Hisham</forenames></author><author><keyname>Anwer</keyname><forenames>Rao Muhammad</forenames></author><author><keyname>Khan</keyname><forenames>Salman</forenames></author></authors><title>AIN: The Arabic INclusive Large Multimodal Model</title><categories>cs.CV cs.AI cs.CL cs.HC cs.LG</categories><comments>20 pages, 16 figures, ACL</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Amid the swift progress of large language models (LLMs) and their evolution into large multimodal models (LMMs), significant strides have been made in high-resource languages such as English and Chinese. While Arabic LLMs have seen notable progress, Arabic LMMs remain largely unexplored, often narrowly focusing on a few specific aspects of the language and visual understanding. To bridge this gap, we introduce AIN-the Arabic Inclusive Multimodal Model-designed to excel across diverse domains. AIN is an English-Arabic bilingual LMM designed to excel in English and Arabic, leveraging carefully constructed 3.6 million high-quality Arabic-English multimodal data samples. AIN demonstrates state-of-the-art Arabic performance, while also possessing strong English-language visual capabilities. On the recent CAMEL-Bench benchmark comprising 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding, our AIN demonstrates strong performance with the 7B model outperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains and 38 sub-domains. AIN's superior capabilities position it as a significant step toward empowering Arabic speakers with advanced multimodal generative AI tools across diverse applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00108</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00108</id><created>2025-01-31</created><authors><author><keyname>Suk</keyname><forenames>Joe</forenames></author><author><keyname>Kim</keyname><forenames>Jung-hun</forenames></author></authors><title>Tracking Most Significant Shifts in Infinite-Armed Bandits</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study an infinite-armed bandit problem where actions' mean rewards are initially sampled from a reservoir distribution. Most prior works in this setting focused on stationary rewards (Berry et al., 1997; Wang et al., 2008; Bonald and Proutiere, 2013; Carpentier and Valko, 2015) with the more challenging adversarial/non-stationary variant only recently studied in the context of rotting/decreasing rewards (Kim et al., 2022; 2024). Furthermore, optimal regret upper bounds were only achieved using parameter knowledge of non-stationarity and only known for certain regimes of regularity of the reservoir. This work shows the first parameter-free optimal regret bounds for all regimes while also relaxing distributional assumptions on the reservoir.   We first introduce a blackbox scheme to convert a finite-armed MAB algorithm designed for near-stationary environments into a parameter-free algorithm for the infinite-armed non-stationary problem with optimal regret guarantees. We next study a natural notion of significant shift for this problem inspired by recent developments in finite-armed MAB (Suk &amp; Kpotufe, 2022). We show that tighter regret bounds in terms of significant shifts can be adaptively attained by employing a randomized variant of elimination within our blackbox scheme. Our enhanced rates only depend on the rotting non-stationarity and thus exhibit an interesting phenomenon for this problem where rising rewards do not factor into the difficulty of non-stationarity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00112</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00112</id><created>2025-01-31</created><authors><author><keyname>Bernal</keyname><forenames>Javier</forenames></author><author><keyname>Torres-Jimenez</keyname><forenames>Jose</forenames></author></authors><title>SAGRAD: A Program for Neural Network Training with Simulated Annealing   and the Conjugate Gradient Method</title><categories>cs.LG cs.NE</categories><journal-ref>Journal of Research of the National Institute of Standards and   Technology Volume 120 (2015)</journal-ref><doi>10.6028/jres.120.009</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  SAGRAD (Simulated Annealing GRADient), a Fortran 77 program for computing neural networks for classification using batch learning, is discussed. Neural network training in SAGRAD is based on a combination of simulated annealing and M{\o}ller's scaled conjugate gradient algorithm, the latter a variation of the traditional conjugate gradient method, better suited for the nonquadratic nature of neural networks. Different aspects of the implementation of the training process in SAGRAD are discussed, such as the efficient computation of gradients and multiplication of vectors by Hessian matrices that are required by M{\o}ller's algorithm; the (re)initialization of weights with simulated annealing required to (re)start M{\o}ller's algorithm the first time and each time thereafter that it shows insufficient progress in reaching a possibly local minimum; and the use of simulated annealing when M{\o}ller's algorithm, after possibly making considerable progress, becomes stuck at a local minimum or flat area of weight space. Outlines of the scaled conjugate gradient algorithm, the simulated annealing procedure and the training process used in SAGRAD are presented together with results from running SAGRAD on two examples of training data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00114</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00114</id><created>2025-01-31</created><authors><author><keyname>Tan</keyname><forenames>Aaron Hao</forenames></author><author><keyname>Fung</keyname><forenames>Angus</forenames></author><author><keyname>Wang</keyname><forenames>Haitong</forenames></author><author><keyname>Nejat</keyname><forenames>Goldie</forenames></author></authors><title>Mobile Robot Navigation Using Hand-Drawn Maps: A Vision Language Model   Approach</title><categories>cs.RO cs.CV</categories><comments>8 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hand-drawn maps can be used to convey navigation instructions between humans and robots in a natural and efficient manner. However, these maps can often contain inaccuracies such as scale distortions and missing landmarks which present challenges for mobile robot navigation. This paper introduces a novel Hand-drawn Map Navigation (HAM-Nav) architecture that leverages pre-trained vision language models (VLMs) for robot navigation across diverse environments, hand-drawing styles, and robot embodiments, even in the presence of map inaccuracies. HAM-Nav integrates a unique Selective Visual Association Prompting approach for topological map-based position estimation and navigation planning as well as a Predictive Navigation Plan Parser to infer missing landmarks. Extensive experiments were conducted in photorealistic simulated environments, using both wheeled and legged robots, demonstrating the effectiveness of HAM-Nav in terms of navigation success rates and Success weighted by Path Length. Furthermore, a user study in real-world environments highlighted the practical utility of hand-drawn maps for robot navigation as well as successful navigation outcomes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00115</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00115</id><created>2025-01-31</created><authors><author><keyname>Cheng</keyname><forenames>Richard</forenames></author><author><keyname>Papozov</keyname><forenames>Chavdar</forenames></author><author><keyname>Helmick</keyname><forenames>Dan</forenames></author><author><keyname>Tjersland</keyname><forenames>Mark</forenames></author></authors><title>A Direct Semi-Exhaustive Search Method for Robust, Partial-to-Full Point   Cloud Registration</title><categories>cs.RO cs.CV</categories><comments>IROS 2024</comments><doi>10.1109/IROS58592.2024.10801518</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Point cloud registration refers to the problem of finding the rigid transformation that aligns two given point clouds, and is crucial for many applications in robotics and computer vision. The main insight of this paper is that we can directly optimize the point cloud registration problem without correspondences by utilizing an algorithmically simple, yet computationally complex, semi-exhaustive search approach that is very well-suited for parallelization on modern GPUs. Our proposed algorithm, Direct Semi-Exhaustive Search (DSES), iterates over potential rotation matrices and efficiently computes the inlier-maximizing translation associated with each rotation. It then computes the optimal rigid transformation based on any desired distance metric by directly computing the error associated with each transformation candidate $\{R, t\}$. By leveraging the parallelism of modern GPUs, DSES outperforms state-of-the-art methods for partial-to-full point cloud registration on the simulated ModelNet40 benchmark and demonstrates high performance and robustness for pose estimation on a real-world robotics problem (https://youtu.be/q0q2-s2KSuA). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00127</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00127</id><created>2025-01-31</created><authors><author><keyname>Pluth</keyname><forenames>Daniel</forenames></author><author><keyname>Zhou</keyname><forenames>Yu</forenames></author><author><keyname>Gurbani</keyname><forenames>Vijay K.</forenames></author></authors><title>Sparse Autoencoder Insights on Voice Embeddings</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Recent advances in explainable machine learning have highlighted the potential of sparse autoencoders in uncovering mono-semantic features in densely encoded embeddings. While most research has focused on Large Language Model (LLM) embeddings, the applicability of this technique to other domains remains largely unexplored. This study applies sparse autoencoders to speaker embeddings generated from a Titanet model, demonstrating the effectiveness of this technique in extracting mono-semantic features from non-textual embedded data. The results show that the extracted features exhibit characteristics similar to those found in LLM embeddings, including feature splitting and steering. The analysis reveals that the autoencoder can identify and manipulate features such as language and music, which are not evident in the original embedding. The findings suggest that sparse autoencoders can be a valuable tool for understanding and interpreting embedded data in many domains, including audio-based speaker recognition. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00129</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00129</id><created>2025-01-31</created><authors><author><keyname>Mikulinsky</keyname><forenames>Rachel</forenames></author><author><keyname>Alper</keyname><forenames>Morris</forenames></author><author><keyname>Gordin</keyname><forenames>Shai</forenames></author><author><keyname>Jiménez</keyname><forenames>Enrique</forenames></author><author><keyname>Cohen</keyname><forenames>Yoram</forenames></author><author><keyname>Averbuch-Elor</keyname><forenames>Hadar</forenames></author></authors><title>ProtoSnap: Prototype Alignment for Cuneiform Signs</title><categories>cs.CV cs.LG</categories><comments>Accepted to ICLR 2025. Project page:   https://tau-vailab.github.io/ProtoSnap/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cuneiform writing system served as the medium for transmitting knowledge in the ancient Near East for a period of over three thousand years. Cuneiform signs have a complex internal structure which is the subject of expert paleographic analysis, as variations in sign shapes bear witness to historical developments and transmission of writing and culture over time. However, prior automated techniques mostly treat sign types as categorical and do not explicitly model their highly varied internal configurations. In this work, we present an unsupervised approach for recovering the fine-grained internal configuration of cuneiform signs by leveraging powerful generative models and the appearance and structure of prototype font images as priors. Our approach, ProtoSnap, enforces structural consistency on matches found with deep image features to estimate the diverse configurations of cuneiform characters, snapping a skeleton-based template to photographed cuneiform signs. We provide a new benchmark of expert annotations and evaluate our method on this task. Our evaluation shows that our approach succeeds in aligning prototype skeletons to a wide variety of cuneiform signs. Moreover, we show that conditioning on structures produced by our method allows for generating synthetic data with correct structural configurations, significantly boosting the performance of cuneiform sign recognition beyond existing techniques, in particular over rare signs. Our code, data, and trained models are available at the project page: https://tau-vailab.github.io/ProtoSnap/ </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00131</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00131</id><created>2025-01-31</created><authors><author><keyname>Dey</keyname><forenames>Soumik</forenames></author><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Wu</keyname><forenames>Hansi</forenames></author><author><keyname>Dong</keyname><forenames>Bingfeng</forenames></author><author><keyname>Li</keyname><forenames>Binbin</forenames></author></authors><title>Middleman Bias in Advertising: Aligning Relevance of Keyphrase   Recommendations with Search</title><categories>cs.IR</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  E-commerce sellers are recommended keyphrases based on their inventory on which they advertise to increase buyer engagement (clicks/sales). Keyphrases must be pertinent to items; otherwise, it can result in seller dissatisfaction and poor targeting -- towards that end relevance filters are employed. In this work, we describe the shortcomings of training relevance filter models on biased click/sales signals. We re-conceptualize advertiser keyphrase relevance as interaction between two dynamical systems -- Advertising which produces the keyphrases and Search which acts as a middleman to reach buyers. We discuss the bias of search relevance systems (middleman bias) and the need to align advertiser keyphrases with search relevance signals. We also compare the performance of cross encoders and bi-encoders in modeling this alignment and the scalability of such a solution for sellers at eBay. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00133</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00133</id><created>2025-01-31</created><authors><author><keyname>Vazquez</keyname><forenames>Fabian</forenames></author><author><keyname>Nuñez</keyname><forenames>Jose Angel</forenames></author><author><keyname>Fu</keyname><forenames>Xiaoyan</forenames></author><author><keyname>Gu</keyname><forenames>Pengfei</forenames></author><author><keyname>Fu</keyname><forenames>Bin</forenames></author></authors><title>Exploring Transfer Learning for Deep Learning Polyp Detection in   Colonoscopy Images Using YOLOv8</title><categories>cs.CV cs.AI</categories><comments>10 pages, 3 figures, 6 tables, SPIE conference</comments><acm-class>I.2.0</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deep learning methods have demonstrated strong performance in objection tasks; however, their ability to learn domain-specific applications with limited training data remains a significant challenge. Transfer learning techniques address this issue by leveraging knowledge from pre-training on related datasets, enabling faster and more efficient learning for new tasks. Finding the right dataset for pre-training can play a critical role in determining the success of transfer learning and overall model performance. In this paper, we investigate the impact of pre-training a YOLOv8n model on seven distinct datasets, evaluating their effectiveness when transferred to the task of polyp detection. We compare whether large, general-purpose datasets with diverse objects outperform niche datasets with characteristics similar to polyps. In addition, we assess the influence of the size of the dataset on the efficacy of transfer learning. Experiments on the polyp datasets show that models pre-trained on relevant datasets consistently outperform those trained from scratch, highlighting the benefit of pre-training on datasets with shared domain-specific features. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00136</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00136</id><created>2025-01-31</created><authors><author><keyname>Chang</keyname><forenames>Edward Y.</forenames></author></authors><title>A Three-Branch Checks-and-Balances Frameworkfor Context-Aware Ethical   Alignment of Large Language Models</title><categories>cs.CL cs.AI</categories><comments>17 pages, 6 tables, 6 figures. arXiv admin note: substantial text   overlap with arXiv:2405.07076</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a three-branch checks-and-balances framework for ethical alignment of Large Language Models (LLMs), inspired by governmental systems. It implements three independent yet interacting components: LLMs as the executive branch for knowledge generation, DIKE as the legislative branch establishing ethical guardrails, and ERIS as the judicial branch for contextual interpretation. The adversarial DIKE-ERIS duality enables adaptation to diverse cultural contexts while upholding consistent ethical principles. This architecture addresses limitations of reinforcement learning with human feedback (RLHF) by providing interpretable, adaptable, and culturally-aware ethical reasoning. Through self-supervised learning and adversarial testing, our framework demonstrates how emotional modeling can guide linguistic behaviors toward ethical outcomes while preserving independence across knowledge generation, ethical oversight, and contextual interpretation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00138</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00138</id><created>2025-01-31</created><authors><author><keyname>Esterhuyse</keyname><forenames>Christopher A.</forenames></author><author><keyname>Müller</keyname><forenames>Tim</forenames></author><author><keyname>van Binsbergen</keyname><forenames>L. Thomas</forenames></author></authors><title>JustAct+: Justified and Accountable Actions in Policy-Regulated,   Multi-Domain Data Processing</title><categories>cs.LO cs.DC cs.MA cs.PL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Inter-organisational data exchange is regulated by norms originating from sources ranging from (inter)national laws, to processing agreements, and individual consent. Verifying norm compliance is complex because laws (e.g., GDPR) distribute responsibility and require accountability. Moreover, in some application domains (e.g., healthcare), privacy requirements extend the norms (e.g., patient consent). In contrast, existing solutions such as smart contracts, access- and usage-control assume policies to be public, or otherwise, statically partition policy information at the cost of accountability and flexibility. Instead, our framework prescribes how decentralised agents justify their actions with policy fragments that the agents autonomously create, gossip, and assemble. Crucially, the permission of actions is always reproducible by any observer, even with a partial view of all the dynamic policies. Actors can be sure that future auditors will confirm their permissions. Systems centralise control by (re)configuring externally synchronised agreements, the bases of all justifications. As a result, control is centralised only to the extent desired by the agents.   In this paper, we define the JustAct framework, detail its implementation in a particular data-processing system, and design a suitable policy language based on logic programming. A case study reproduces Brane - an existing policy-regulated, inter-domain, medical data processing system - and serves to demonstrate and assess the qualities of the framework. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00139</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00139</id><created>2025-01-31</created><authors><author><keyname>Mo</keyname><forenames>Jianhua</forenames><affiliation>Charlie</affiliation></author><author><keyname>AlAmmouri</keyname><forenames>Ahmad</forenames><affiliation>Charlie</affiliation></author><author><keyname>Dong</keyname><forenames>Shenggang</forenames><affiliation>Charlie</affiliation></author><author><keyname>Nam</keyname><forenames>Younghan</forenames><affiliation>Charlie</affiliation></author><author><keyname>Choi</keyname><forenames>Won-Suk</forenames><affiliation>Charlie</affiliation></author><author><keyname>Xu</keyname><forenames>Gary</forenames><affiliation>Charlie</affiliation></author><author><keyname>Jianzhong</keyname><affiliation>Charlie</affiliation></author><author><keyname>Zhan</keyname></author></authors><title>Beamforming with Joint Phase and Time Array: System Design, Prototyping   and Performance</title><categories>cs.IT eess.SP math.IT</categories><comments>Presented at Asilomar Conference on Signals, Systems, and Computers   2024</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Joint phase-time arrays (JPTA) is a new mmWave radio frequency front-end architecture constructed with appending time-delay elements to phase shifters for analog beamforming. JPTA allows the mmWave base station (BS) to form multiple frequency-dependent beams with a single RF chain, exploiting the extra degrees of freedom the time-delay elements offer. Without requiring extra power-hungry RF chains, a BS with JPTA can schedule multiple users in different directions in a frequency-division multiplexing (FDM) manner. A BS with JPTA achieves various advantages over the traditional analog beamforming system. Simulation results show that JPTA can bring significant system-level benefits, e.g., extending uplink throughput coverage by 100%. To realize these system benefits of JPTA, high-resolution delay elements with a wide delay dynamic range are essential. With newly developed delay elements, we demonstrate that a single TRX RF chain can serve four users in four different directions in the mmWave band. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00140</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00140</id><created>2025-01-31</created><authors><author><keyname>Jiang</keyname><forenames>Qin</forenames></author><author><keyname>Wang</keyname><forenames>Chengjia</forenames></author><author><keyname>Lones</keyname><forenames>Michael</forenames></author><author><keyname>Pang</keyname><forenames>Wei</forenames></author></authors><title>Demystifying MPNNs: Message Passing as Merely Efficient Matrix   Multiplication</title><categories>cs.LG cs.AI cs.NE cs.SI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While Graph Neural Networks (GNNs) have achieved remarkable success, their design largely relies on empirical intuition rather than theoretical understanding. In this paper, we present a comprehensive analysis of GNN behavior through three fundamental aspects: (1) we establish that \textbf{$k$-layer} Message Passing Neural Networks efficiently aggregate \textbf{$k$-hop} neighborhood information through iterative computation, (2) analyze how different loop structures influence neighborhood computation, and (3) examine behavior across structure-feature hybrid and structure-only tasks. For deeper GNNs, we demonstrate that gradient-related issues, rather than just over-smoothing, can significantly impact performance in sparse graphs. We also analyze how different normalization schemes affect model performance and how GNNs make predictions with uniform node features, providing a theoretical framework that bridges the gap between empirical success and theoretical understanding. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00142</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00142</id><created>2025-01-31</created><authors><author><keyname>Keyela</keyname><forenames>Patatchona</forenames></author><author><keyname>Cherkaoui</keyname><forenames>Soumaya</forenames></author></authors><title>Open RAN Slicing with Quantum Optimization</title><categories>cs.NI</categories><comments>Presented at IEEE conference GIIS25</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  RAN slicing technology is a key aspect of the Open RAN paradigm, allowing simultaneous and independent provision of various services such as ultra-reliable low-latency communications (URLLC), enhanced mobile broadband (eMBB), and massive machine-type communications (mMTC) through virtual networks that share a single radio access infrastructure. Efficient resource allocation is crucial for RAN slicing, as each service has specific quality of service (QoS) requirements, and a balance between different services must be maintained. Although heuristic and reinforcement learning (RL) techniques have been explored to achieve efficient resource allocation, these approaches face notable limitations: heuristic algorithms face complexity issues that limit their effectiveness in large networks, RL solutions are constrained by their dependency on training data and struggle to adapt to new scenarios and environments. This paper proposes a framework that leverages quantum optimization techniques to optimize radio resource blocks allocation in Open RAN slicing for URLLC and eMBB services. We provide a classical problem formulation and the quantum implementation using the constrained quadratic model on Dwave quantum annealing platform, showcasing the potential of quantum optimization techniques to deliver in real-time optimal solutions for optimization problems in 5G and beyond networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00145</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00145</id><created>2025-01-31</created><authors><author><keyname>Speck</keyname><forenames>David</forenames></author><author><keyname>Hecher</keyname><forenames>Markus</forenames></author><author><keyname>Gnad</keyname><forenames>Daniel</forenames></author><author><keyname>Fichte</keyname><forenames>Johannes K.</forenames></author><author><keyname>Corrêa</keyname><forenames>Augusto B.</forenames></author></authors><title>Counting and Reasoning with Plans</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical planning asks for a sequence of operators reaching a given goal. While the most common case is to compute a plan, many scenarios require more than that. However, quantitative reasoning on the plan space remains mostly unexplored. A fundamental problem is to count plans, which relates to the conditional probability on the plan space. Indeed, qualitative and quantitative approaches are well-established in various other areas of automated reasoning. We present the first study to quantitative and qualitative reasoning on the plan space. In particular, we focus on polynomially bounded plans. On the theoretical side, we study its complexity, which gives rise to rich reasoning modes. Since counting is hard in general, we introduce the easier notion of facets, which enables understanding the significance of operators. On the practical side, we implement quantitative reasoning for planning. Thereby, we transform a planning task into a propositional formula and use knowledge compilation to count different plans. This framework scales well to large plan spaces, while enabling rich reasoning capabilities such as learning pruning functions and explainable planning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00146</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00146</id><created>2025-01-31</created><authors><author><keyname>Jahanandish</keyname><forenames>Hassan</forenames></author><author><keyname>Sang</keyname><forenames>Shengtian</forenames></author><author><keyname>Li</keyname><forenames>Cynthia Xinran</forenames></author><author><keyname>Vesal</keyname><forenames>Sulaiman</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Indrani</forenames></author><author><keyname>Lee</keyname><forenames>Jeong Hoon</forenames></author><author><keyname>Fan</keyname><forenames>Richard</forenames></author><author><keyname>Sonna</keyname><forenames>Geoffrey A.</forenames></author><author><keyname>Rusu</keyname><forenames>Mirabela</forenames></author></authors><title>Multimodal MRI-Ultrasound AI for Prostate Cancer Detection Outperforms   Radiologist MRI Interpretation: A Multi-Center Study</title><categories>eess.IV cs.AI cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pre-biopsy magnetic resonance imaging (MRI) is increasingly used to target suspicious prostate lesions. This has led to artificial intelligence (AI) applications improving MRI-based detection of clinically significant prostate cancer (CsPCa). However, MRI-detected lesions must still be mapped to transrectal ultrasound (TRUS) images during biopsy, which results in missing CsPCa. This study systematically evaluates a multimodal AI framework integrating MRI and TRUS image sequences to enhance CsPCa identification. The study included 3110 patients from three cohorts across two institutions who underwent prostate biopsy. The proposed framework, based on the 3D UNet architecture, was evaluated on 1700 test cases, comparing performance to unimodal AI models that use either MRI or TRUS alone. Additionally, the proposed model was compared to radiologists in a cohort of 110 patients. The multimodal AI approach achieved superior sensitivity (80%) and Lesion Dice (42%) compared to unimodal MRI (73%, 30%) and TRUS models (49%, 27%). Compared to radiologists, the multimodal model showed higher specificity (88% vs. 78%) and Lesion Dice (38% vs. 33%), with equivalent sensitivity (79%). Our findings demonstrate the potential of multimodal AI to improve CsPCa lesion targeting during biopsy and treatment planning, surpassing current unimodal models and radiologists; ultimately improving outcomes for prostate cancer patients. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00147</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00147</id><created>2025-01-31</created><authors><author><keyname>Los</keyname><forenames>Denis</forenames></author></authors><title>Efficient Read-Port-Count Reduction Schemes for the Centralized Physical   Register File in a Superscalar Microprocessor</title><categories>cs.AR</categories><journal-ref>International Journal of Open Information Technologies, vol. 13,   no. 2, pp. 105-113, 2025</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The physical register file supports increasing the execution width and depth of a superscalar microprocessor to exploit more instruction-level parallelism. The efficient design of the physical register file is critical since its resources, such as the number of read and write ports, have a significant impact on CPU power consumption. Reducing the number of ports to the physical register file is a well-known direction for optimization. For port-count reduction schemes, balancing the trade-off between the scheme's complexity and performance is crucial. In our work, we introduce a high-level analysis method to estimate the complexity of the schemes during microarchitectural design. Moreover, we explore the structure of different port-count reduction schemes and introduce a practical approach to constructing low-complexity read-portcount reduction schemes for the centralized integer physical register file. We show that the read-port-count reduction schemes designed with this approach can reduce the number of read ports by a factor of two (from 17 to 8 read ports) with the Geomean performance degradation of only 0.1% IPC across the SPECrate CPU 2017 Integer workloads. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00149</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00149</id><created>2025-01-31</created><authors><author><keyname>Filos-Ratsikas</keyname><forenames>Aris</forenames></author><author><keyname>Gkatzelis</keyname><forenames>Vasilis</forenames></author><author><keyname>Latifian</keyname><forenames>Mohamad</forenames></author><author><keyname>Rewinski</keyname><forenames>Emma</forenames></author><author><keyname>Voudouris</keyname><forenames>Alexandros A.</forenames></author></authors><title>Optimal Metric Distortion for Matching on the Line</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the distortion of one-sided and two-sided matching problems on the line. In the one-sided case, $n$ agents need to be matched to $n$ items, and each agent's cost in a matching is their distance from the item they were matched to. We propose an algorithm that is provided only with ordinal information regarding the agents' preferences (each agent's ranking of the items from most- to least-preferred) and returns a matching aiming to minimize the social cost with respect to the agents' true (cardinal) costs. We prove that our algorithm simultaneously achieves the best-possible approximation of $3$ (known as distortion) with respect to a variety of social cost measures which include the utilitarian and egalitarian social cost. In the two-sided case, where the agents need be matched to $n$ other agents and both sides report their ordinal preferences over each other, we show that it is always possible to compute an optimal matching. In fact, we show that this optimal matching can be achieved using even less information, and we provide bounds regarding the sufficient number of queries. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00150</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00150</id><created>2025-01-31</created><authors><author><keyname>Alexanderian</keyname><forenames>Alen</forenames></author><author><keyname>Díaz</keyname><forenames>Hugo</forenames></author><author><keyname>Rao</keyname><forenames>Vishwas</forenames></author><author><keyname>Saibaba</keyname><forenames>Arvind K.</forenames></author></authors><title>Optimal sensor placement under model uncertainty in the weak-constraint   4D-Var framework</title><categories>math.NA cs.NA</categories><msc-class>58F15, 58F17, 53C3</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In data assimilation, the model may be subject to uncertainties and errors. The weak-constraint data assimilation framework enables incorporating model uncertainty in the dynamics of the governing equations. % We propose a new framework for near-optimal sensor placement in the weak-constrained setting. This is achieved by first deriving a design criterion based on the expected information gain, which involves the Kullback-Leibler divergence from the forecast prior to the posterior distribution. An explicit formula for this criterion is provided, assuming that the model error and background are independent and Gaussian and the dynamics are linear. % We discuss algorithmic approaches to efficiently evaluate this criterion through randomized approximations.   To provide further insight and flexibility in computations, we also provide alternative expressions for the criteria. %t We provide an algorithm to find near-optimal experimental designs using column subset selection, including a randomized algorithm that avoids computing the adjoint of the forward operator. % Through numerical experiments in one and two spatial dimensions, we show the effectiveness of our proposed methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00151</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00151</id><created>2025-01-31</created><authors><author><keyname>Trinh</keyname><forenames>Viet</forenames></author></authors><title>A Comprehensive Review: Applicability of Deep Neural Networks in   Business Decision Making and Market Prediction Investment</title><categories>econ.GN cs.AI q-fin.EC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big data, both in its structured and unstructured formats, have brought in unforeseen challenges in economics and business. How to organize, classify, and then analyze such data to obtain meaningful insights are the ever-going research topics for business leaders and academic researchers. This paper studies recent applications of deep neural networks in decision making in economical business and investment; especially in risk management, portfolio optimization, and algorithmic trading. Set aside limitation in data privacy and cross-market analysis, the article establishes that deep neural networks have performed remarkably in financial classification and prediction. Moreover, the study suggests that by compositing multiple neural networks, spanning different data type modalities, a more robust, efficient, and scalable financial prediction framework can be constructed. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00153</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00153</id><created>2025-01-31</created><authors><author><keyname>Ginosar</keyname><forenames>Ran</forenames></author></authors><title>Theoretical complexity analysis of many-cores on a single chip</title><categories>cs.AR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  When a single core is scaled up to m cores occupying the same chip area and executing the same (parallelizable) task, achievable speedup is square-root m, power is reduced by square-root m and energy is reduced by m. Thus, many-core architectures can efficiently outperform architectures of a single core and a small-count multi-core. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00156</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00156</id><created>2025-01-31</created><authors><author><keyname>Fioresi</keyname><forenames>Joseph</forenames></author><author><keyname>Dave</keyname><forenames>Ishan Rajendrakumar</forenames></author><author><keyname>Shah</keyname><forenames>Mubarak</forenames></author></authors><title>ALBAR: Adversarial Learning approach to mitigate Biases in Action   Recognition</title><categories>cs.CV cs.CR</categories><comments>Accepted to ICLR 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bias in machine learning models can lead to unfair decision making, and while it has been well-studied in the image and text domains, it remains underexplored in action recognition. Action recognition models often suffer from background bias (i.e., inferring actions based on background cues) and foreground bias (i.e., relying on subject appearance), which can be detrimental to real-life applications such as autonomous vehicles or assisted living monitoring. While prior approaches have mainly focused on mitigating background bias using specialized augmentations, we thoroughly study both biases. We propose ALBAR, a novel adversarial training method that mitigates foreground and background biases without requiring specialized knowledge of the bias attributes. Our framework applies an adversarial cross-entropy loss to the sampled static clip (where all the frames are the same) and aims to make its class probabilities uniform using a proposed entropy maximization loss. Additionally, we introduce a gradient penalty loss for regularization against the debiasing process. We evaluate our method on established background and foreground bias protocols, setting a new state-of-the-art and strongly improving combined debiasing performance by over 12% on HMDB51. Furthermore, we identify an issue of background leakage in the existing UCF101 protocol for bias evaluation which provides a shortcut to predict actions and does not provide an accurate measure of the debiasing capability of a model. We address this issue by proposing more fine-grained segmentation boundaries for the actor, where our method also outperforms existing approaches. Project Page: https://joefioresi718.github.io/ALBAR_webpage/ </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00158</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00158</id><created>2025-01-31</created><authors><author><keyname>Zhang</keyname><forenames>Binchi</forenames></author><author><keyname>Chen</keyname><forenames>Zhengzhang</forenames></author><author><keyname>Zheng</keyname><forenames>Zaiyi</forenames></author><author><keyname>Li</keyname><forenames>Jundong</forenames></author><author><keyname>Chen</keyname><forenames>Haifeng</forenames></author></authors><title>Resolving Editing-Unlearning Conflicts: A Knowledge Codebook Framework   for Large Language Model Updating</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) excel in natural language processing by encoding extensive human knowledge, but their utility relies on timely updates as knowledge evolves. Updating LLMs involves two key tasks simultaneously: unlearning to remove unwanted knowledge and editing to incorporate new information. Existing methods face two major challenges: ineffective knowledge storage (either too sparse or too dense) and task conflicts between editing and unlearning, as validated through our theoretical and experimental results. To address these issues, we propose LOKA, a conflict-free framework for LLM updating based on a knowledge codebook. During training, updated knowledge is stored in multiple codebook memories. To optimize knowledge storage, a similarity-aware knowledge mapping ensures that related knowledge pieces are clustered and allocated to the same memory. Additionally, LOKA resolves task conflicts by employing task-specific and multi-task memories guided by a conflict score. In the inference stage, LOKA retrieves the most relevant memory from the codebook and plugs it into the original LLM to apply the updated knowledge. A learning-based router controls codebook activation to further improve knowledge utilization. Extensive experiments demonstrate the effectiveness of LOKA in LLM knowledge updating tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00160</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00160</id><created>2025-01-31</created><authors><author><keyname>Bricout</keyname><forenames>Charles</forenames></author><author><keyname>Bouix</keyname><forenames>Sylvain</forenames></author><author><keyname>Kahou</keyname><forenames>Samira Ebrahimi</forenames></author><author><keyname>Cho</keyname><forenames>Kang Ik K.</forenames></author><author><keyname>Harms</keyname><forenames>Michael</forenames></author><author><keyname>Pasternak</keyname><forenames>Ofer</forenames></author><author><keyname>Bearden</keyname><forenames>Carrie E.</forenames></author><author><keyname>McGorry</keyname><forenames>Patrick D.</forenames></author><author><keyname>Kahn</keyname><forenames>Rene S.</forenames></author><author><keyname>Kane</keyname><forenames>John</forenames></author><author><keyname>Nelson</keyname><forenames>Barnaby</forenames></author><author><keyname>Woods</keyname><forenames>Scott W.</forenames></author><author><keyname>Shenton</keyname><forenames>Martha E.</forenames></author></authors><title>Improving Quality Control Of MRI Images Using Synthetic Motion Data</title><categories>eess.IV cs.CV</categories><comments>Accepted at ISBI 2025</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  MRI quality control (QC) is challenging due to unbalanced and limited datasets, as well as subjective scoring, which hinder the development of reliable automated QC systems. To address these issues, we introduce an approach that pretrains a model on synthetically generated motion artifacts before applying transfer learning for QC classification. This method not only improves the accuracy in identifying poor-quality scans but also reduces training time and resource requirements compared to training from scratch. By leveraging synthetic data, we provide a more robust and resource-efficient solution for QC automation in MRI, paving the way for broader adoption in diverse research settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00162</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00162</id><created>2025-01-31</created><authors><author><keyname>Ristich</keyname><forenames>Eron</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Ren</keyname><forenames>Yi</forenames></author><author><keyname>Sun</keyname><forenames>Jiefeng</forenames></author></authors><title>Physics-informed Split Koopman Operators for Data-efficient Soft Robotic   Simulation</title><categories>cs.RO</categories><comments>This work has been submitted to the IEEE for possible publication.   Submitted to ICRA 2025 for review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Koopman operator theory provides a powerful data-driven technique for modeling nonlinear dynamical systems in a linear framework, in comparison to computationally expensive and highly nonlinear physics-based simulations. However, Koopman operator-based models for soft robots are very high dimensional and require considerable amounts of data to properly resolve. Inspired by physics-informed techniques from machine learning, we present a novel physics-informed Koopman operator identification method that improves simulation accuracy for small dataset sizes. Through Strang splitting, the method takes advantage of both continuous and discrete Koopman operator approximation to obtain information both from trajectory and phase space data. The method is validated on a tendon-driven soft robotic arm, showing orders of magnitude improvement over standard methods in terms of the shape error. We envision this method can significantly reduce the data requirement of Koopman operators for systems with partially known physical models, and thus reduce the cost of obtaining data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00163</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00163</id><created>2025-01-31</created><authors><author><keyname>Yazici</keyname><forenames>Ibrahim</forenames></author><author><keyname>Yotov</keyname><forenames>Ivan</forenames></author></authors><title>Multipoint stress mixed finite element methods for elasticity on cuboid   grids</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop multipoint stress mixed finite element methods for linear elasticity with weak stress symmetry on cuboid grids, which can be reduced to a symmetric and positive definite cell-centered system. The methods employ the lowest-order enhanced Raviart-Thomas finite element space for the stress and piecewise constant displacement. The vertex quadrature rule is employed to localize the interaction of stress degrees of freedom, enabling local stress elimination around each vertex. We introduce two methods. The first method uses a piecewise constant rotation, resulting in a cell-centered system for the displacement and rotation. The second method employs a continuous piecewise trilinear rotation and the vertex quadrature rule for the asymmetry bilinear forms, allowing for further elimination of the rotation and resulting in a cell-centered system for the displacement only. Stability and error analysis is performed for both methods. For the stability analysis of the second method, a new auxiliary H-curl conforming matrix-valued space is constructed, which forms an exact sequence with the stress space. A matrix-matrix inf-sup condition is shown for the curl of this auxiliary space and the trilinear rotation space. First-order convergence is established for all variables in their natural norms, as well as second-order superconvergence of the displacement at the cell centers. Numerical results are presented to verify the theory. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00168</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00168</id><created>2025-01-31</created><authors><author><keyname>Herrera-Esposito</keyname><forenames>Daniel</forenames></author><author><keyname>Burge</keyname><forenames>Johannes</forenames></author></authors><title>Supervised Quadratic Feature Analysis: An Information Geometry Approach   to Dimensionality Reduction</title><categories>stat.ML cs.LG math.DG math.ST stat.TH</categories><comments>18 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Supervised dimensionality reduction aims to map labeled data to a low-dimensional feature space while maximizing class discriminability. Despite the availability of methods for learning complex non-linear features (e.g. Deep Learning), there is an enduring demand for dimensionality reduction methods that learn linear features due to their interpretability, low computational cost, and broad applicability. However, there is a gap between methods that optimize linear separability (e.g. LDA), and more flexible but computationally expensive methods that optimize over arbitrary class boundaries (e.g. metric-learning methods). Here, we present Supervised Quadratic Feature Analysis (SQFA), a dimensionality reduction method for learning linear features that maximize the differences between class-conditional first- and second-order statistics, which allow for quadratic discrimination. SQFA exploits the information geometry of second-order statistics in the symmetric positive definite manifold. We show that SQFA features support quadratic discriminability in real-world problems. We also provide a theoretical link, based on information geometry, between SQFA and the Quadratic Discriminant Analysis (QDA) classifier. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00169</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00169</id><created>2025-01-31</created><authors><author><keyname>Sahin</keyname><forenames>Omur</forenames></author><author><keyname>Zhang</keyname><forenames>Man</forenames></author><author><keyname>Arcuri</keyname><forenames>Andrea</forenames></author></authors><title>Causes and Effects of Fitness Landscapes in System Test Generation: A   Replication Study</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Search-Based Software Testing (SBST) has seen several success stories in academia and industry. The effectiveness of a search algorithm at solving a software engineering problem strongly depends on how such algorithm can navigate the fitness landscape of the addressed problem. The fitness landscape depends on the used fitness function. Understanding the properties of a fitness landscape can help to provide insight on how a search algorithm behaves on it. Such insight can provide valuable information to researchers to being able to design novel, more effective search algorithms and fitness functions tailored for a specific problem. Due to its importance, few fitness landscape analyses have been carried out in the scientific literature of SBST. However, those have been focusing on the problem of unit test generation, e.g., with state-of-the-art tools such as EvoSuite. In this paper, we replicate one such existing study. However, in our work we focus on system test generation, with the state-of-the-art tool EvoMaster. Based on an empirical study involving the testing of 23 web services, this enables us to provide valuable insight into this important testing domain of practical industrial relevance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00172</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00172</id><created>2025-01-31</created><authors><author><keyname>Huang</keyname><forenames>Jizhou</forenames></author><author><keyname>Juba</keyname><forenames>Brendan</forenames></author></authors><title>Distribution-Specific Agnostic Conditional Classification With   Halfspaces</title><categories>cs.LG cs.CC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study ``selective'' or ``conditional'' classification problems under an agnostic setting. Classification tasks commonly focus on modeling the relationship between features and categories that captures the vast majority of data. In contrast to common machine learning frameworks, conditional classification intends to model such relationships only on a subset of the data defined by some selection rule. Most work on conditional classification either solves the problem in a realizable setting or does not guarantee the error is bounded compared to an optimal solution. In this work, we consider selective/conditional classification by sparse linear classifiers for subsets defined by halfspaces, and give both positive as well as negative results for Gaussian feature distributions. On the positive side, we present the first PAC-learning algorithm for homogeneous halfspace selectors with error guarantee $\bigO*{\sqrt{\mathrm{opt}}}$, where $\mathrm{opt}$ is the smallest conditional classification error over the given class of classifiers and homogeneous halfspaces. On the negative side, we find that, under cryptographic assumptions, approximating the conditional classification loss within a small additive error is computationally hard even under Gaussian distribution. We prove that approximating conditional classification is at least as hard as approximating agnostic classification in both additive and multiplicative form. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00173</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00173</id><created>2025-01-31</created><authors><author><keyname>Chacko</keyname><forenames>Rohan</forenames></author><author><keyname>Haeni</keyname><forenames>Nicolai</forenames></author><author><keyname>Khaliullin</keyname><forenames>Eldar</forenames></author><author><keyname>Sun</keyname><forenames>Lin</forenames></author><author><keyname>Lee</keyname><forenames>Douglas</forenames></author></authors><title>Lifting by Gaussians: A Simple, Fast and Flexible Method for 3D Instance   Segmentation</title><categories>cs.CV</categories><comments>Accepted to WACV 2025</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We introduce Lifting By Gaussians (LBG), a novel approach for open-world instance segmentation of 3D Gaussian Splatted Radiance Fields (3DGS). Recently, 3DGS Fields have emerged as a highly efficient and explicit alternative to Neural Field-based methods for high-quality Novel View Synthesis. Our 3D instance segmentation method directly lifts 2D segmentation masks from SAM (alternately FastSAM, etc.), together with features from CLIP and DINOv2, directly fusing them onto 3DGS (or similar Gaussian radiance fields such as 2DGS). Unlike previous approaches, LBG requires no per-scene training, allowing it to operate seamlessly on any existing 3DGS reconstruction. Our approach is not only an order of magnitude faster and simpler than existing approaches; it is also highly modular, enabling 3D semantic segmentation of existing 3DGS fields without requiring a specific parametrization of the 3D Gaussians. Furthermore, our technique achieves superior semantic segmentation for 2D semantic novel view synthesis and 3D asset extraction results while maintaining flexibility and efficiency. We further introduce a novel approach to evaluate individually segmented 3D assets from 3D radiance field segmentation methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00174</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00174</id><created>2025-01-31</created><authors><author><keyname>Costa</keyname><forenames>Guilherme H. Bandeira</forenames></author><author><keyname>Freire</keyname><forenames>Miguel</forenames></author><author><keyname>Oliveira</keyname><forenames>Arlindo L.</forenames></author></authors><title>The role of positional encodings in the ARC benchmark</title><categories>cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Abstraction and Reasoning Corpus challenges AI systems to perform abstract reasoning with minimal training data, a task intuitive for humans but demanding for machine learning models. Using CodeT5+ as a case study, we demonstrate how limitations in positional encoding hinder reasoning and impact performance. This work further examines the role of positional encoding across transformer architectures, highlighting its critical influence on models of varying sizes and configurations. Comparing several strategies, we find that while 2D positional encoding and Rotary Position Embedding offer competitive performance, 2D encoding excels in data-constrained scenarios, emphasizing its effectiveness for ARC tasks </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00177</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00177</id><created>2025-01-31</created><authors><author><keyname>Schoinas</keyname><forenames>Eirini</forenames></author><author><keyname>Rastogi</keyname><forenames>Adyah</forenames></author><author><keyname>Carter</keyname><forenames>Anissa</forenames></author><author><keyname>Granley</keyname><forenames>Jacob</forenames></author><author><keyname>Beyeler</keyname><forenames>Michael</forenames></author></authors><title>Evaluating Deep Human-in-the-Loop Optimization for Retinal Implants   Using Sighted Participants</title><categories>cs.LG cs.CV cs.HC</categories><acm-class>I.2.10</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Human-in-the-loop optimization (HILO) is a promising approach for personalizing visual prostheses by iteratively refining stimulus parameters based on user feedback. Previous work demonstrated HILO's efficacy in simulation, but its performance with human participants remains untested. Here we evaluate HILO using sighted participants viewing simulated prosthetic vision to assess its ability to optimize stimulation strategies under realistic conditions. Participants selected between phosphenes generated by competing encoders to iteratively refine a deep stimulus encoder (DSE). We tested HILO in three conditions: standard optimization, threshold misspecifications, and out-of-distribution parameter sampling. Participants consistently preferred HILO-generated stimuli over both a na\"ive encoder and the DSE alone, with log odds favoring HILO across all conditions. We also observed key differences between human and simulated decision-making, highlighting the importance of validating optimization strategies with human participants. These findings support HILO as a viable approach for adapting visual prostheses to individuals. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00180</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00180</id><created>2025-01-31</created><authors><author><keyname>Benita</keyname><forenames>Roi</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author><author><keyname>Keshet</keyname><forenames>Joseph</forenames></author></authors><title>Designing Scheduling for Diffusion Models via Spectral Analysis</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Diffusion models (DMs) have emerged as powerful tools for modeling complex data distributions and generating realistic new samples. Over the years, advanced architectures and sampling methods have been developed to make these models practically usable. However, certain synthesis process decisions still rely on heuristics without a solid theoretical foundation. In our work, we offer a novel analysis of the DM's inference process, introducing a comprehensive frequency response perspective. Specifically, by relying on Gaussianity and shift-invariance assumptions, we present the inference process as a closed-form spectral transfer function, capturing how the generated signal evolves in response to the initial noise. We demonstrate how the proposed analysis can be leveraged for optimizing the noise schedule, ensuring the best alignment with the original dataset's characteristics. Our results lead to scheduling curves that are dependent on the frequency content of the data, offering a theoretical justification for some of the heuristics taken by practitioners. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00182</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00182</id><created>2025-01-31</created><authors><author><keyname>Seo</keyname><forenames>Jungwon</forenames></author><author><keyname>Catak</keyname><forenames>Ferhat Ozgur</forenames></author><author><keyname>Rong</keyname><forenames>Chunming</forenames></author></authors><title>Understanding Federated Learning from IID to Non-IID dataset: An   Experimental Study</title><categories>cs.LG cs.AI stat.ML</categories><journal-ref>36th Norwegian ICT Conference for Research and Education, NIKT   2024</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As privacy concerns and data regulations grow, federated learning (FL) has emerged as a promising approach for training machine learning models across decentralized data sources without sharing raw data. However, a significant challenge in FL is that client data are often non-IID (non-independent and identically distributed), leading to reduced performance compared to centralized learning. While many methods have been proposed to address this issue, their underlying mechanisms are often viewed from different perspectives. Through a comprehensive investigation from gradient descent to FL, and from IID to non-IID data settings, we find that inconsistencies in client loss landscapes primarily cause performance degradation in non-IID scenarios. From this understanding, we observe that existing methods can be grouped into two main strategies: (i) adjusting parameter update paths and (ii) modifying client loss landscapes. These findings offer a clear perspective on addressing non-IID challenges in FL and help guide future research in the field. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00185</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00185</id><created>2025-01-31</created><authors><author><keyname>Poudel</keyname><forenames>Prakash</forenames></author><author><keyname>Cowlagi</keyname><forenames>Raghvendra V.</forenames></author></authors><title>Optimal Coupled Sensor Placement and Path-Planning in Unknown   Time-Varying Environments</title><categories>eess.SY cs.SY</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We address path-planning for a mobile agent to navigate in an unknown environment with minimum exposure to a spatially and temporally varying threat field. The threat field is estimated using pointwise noisy measurements from a mobile sensor network. For this problem, we present a new information gain measure for optimal sensor placement that quantifies reduction in uncertainty in the path cost rather than the environment state. This measure, which we call the context-relevant mutual information (CRMI), couples the sensor placement and path-planning problem. We propose an iterative coupled sensor configuration and path-planning (CSCP) algorithm. At each iteration, this algorithm places sensors to maximize CRMI, updates the threat estimate using new measurements, and recalculates the path with minimum expected exposure to the threat. The iterations converge when the path cost variance, which is an indicator of risk, reduces below a desired threshold. We show that CRMI is submodular, and therefore, greedy optimization provides near-optimal sensor placements while maintaining computational efficiency of the CSCP algorithm. Distance-based sensor reconfiguration costs are introduced in a modified CRMI measure, which we also show to be submodular. Through numerical simulations, we demonstrate that the principal advantage of this algorithm is that near-optimal low-variance paths are achieved using far fewer sensor measurements as compared to a standard sensor placement method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00186</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00186</id><created>2025-01-31</created><authors><author><keyname>Dalal</keyname><forenames>Vibhu</forenames></author></authors><title>Formalising Propositional Information via Implication Hypergraphs</title><categories>math.LO cs.IT math.IT</categories><comments>11 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This work introduces a framework for quantifying the information content of logical propositions through the use of implication hypergraphs. We posit that a proposition's informativeness is primarily determined by its relationships with other propositions -- specifically, the extent to which it implies or derives other propositions. To formalize this notion, we develop a framework based on implication hypergraphs, that seeks to capture these relationships. Within this framework, we define propositional information, derive some key properties, and illustrate the concept through examples. While the approach is broadly applicable, mathematical propositions emerge as an ideal domain for its application due to their inherently rich and interconnected structure. We provide several examples to illustrate this and subsequently discuss the limitations of the framework, along with suggestions for potential refinements. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00190</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00190</id><created>2025-01-31</created><authors><author><keyname>Bui</keyname><forenames>Thu</forenames></author><author><keyname>Schönlieb</keyname><forenames>Carola-Bibiane</forenames></author><author><keyname>Ribeiro</keyname><forenames>Bruno</forenames></author><author><keyname>Bevilacqua</keyname><forenames>Beatrice</forenames></author><author><keyname>Eliasof</keyname><forenames>Moshe</forenames></author></authors><title>On the Effectiveness of Random Weights in Graph Neural Networks</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Graph Neural Networks (GNNs) have achieved remarkable success across diverse tasks on graph-structured data, primarily through the use of learned weights in message passing layers. In this paper, we demonstrate that random weights can be surprisingly effective, achieving performance comparable to end-to-end training counterparts, across various tasks and datasets. Specifically, we show that by replacing learnable weights with random weights, GNNs can retain strong predictive power, while significantly reducing training time by up to 6$\times$ and memory usage by up to 3$\times$. Moreover, the random weights combined with our construction yield random graph propagation operators, which we show to reduce the problem of feature rank collapse in GNNs. These understandings and empirical results highlight random weights as a lightweight and efficient alternative, offering a compelling perspective on the design and training of GNN architectures. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00191</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00191</id><created>2025-01-31</created><authors><author><keyname>Castañeda</keyname><forenames>Armando</forenames></author><author><keyname>Rodríguez</keyname><forenames>Gilde Valeria</forenames></author></authors><title>Asynchronous Fault-Tolerant Language Decidability for Runtime   Verification of Distributed Systems</title><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we offer a wider perspective of the general problem of distributed runtime verification of distributed systems, in fully asynchronous fault-tolerant environments. We study this problem in an asynchronous shared memory model with crash failures where correctness properties are defined as languages, and the aim is to design wait-free algorithms that decide languages in a distributed manner. A decidability definition states the possible values processes can report in an execution, and provides semantics to the values. We propose several decidability definitions, study the relations among them, and prove possibility and impossibility results. One of our main results is a characterization of the correctness properties that can be decided asynchronously. Remarkably, it applies to any language decidability definition. Intuitively, the characterization is that only properties with no real-time order constraints can be decided in asynchronous fault-tolerant settings. As a consequence, there are correctness properties, like linearizability and strong eventual counters, that are unverifiable, no matter the number of possible values processes can report in an execution, and the semantics one gives to those values. We present, however, techniques to evade this strong impossibility result, that combine an indirect runtime verification approach and relaxed decidability definitions. All possibility results use only read/write registers, hence can be simulated in asynchronous messages-passing systems where less than half of the processes can crash, and the impossibility results hold even if processes use powerful operations with arbitrary large consensus number. The results presented here also serve to put previous work in a more general context. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00193</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00193</id><created>2025-01-31</created><authors><author><keyname>Egger</keyname><forenames>Maximilian</forenames></author><author><keyname>Bakshi</keyname><forenames>Mayank</forenames></author><author><keyname>Bitar</keyname><forenames>Rawad</forenames></author></authors><title>Byzantine-Resilient Zero-Order Optimization for Communication-Efficient   Heterogeneous Federated Learning</title><categories>cs.LG cs.CR cs.DC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce CyBeR-0, a Byzantine-resilient federated zero-order optimization method that is robust under Byzantine attacks and provides significant savings in uplink and downlink communication costs. We introduce transformed robust aggregation to give convergence guarantees for general non-convex objectives under client data heterogeneity. Empirical evaluations for standard learning tasks and fine-tuning large language models show that CyBeR-0 exhibits stable performance with only a few scalars per-round communication cost and reduced memory requirements. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00194</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00194</id><created>2025-01-31</created><authors><author><keyname>Shajihan</keyname><forenames>Althaf</forenames></author><author><keyname>Mechitov</keyname><forenames>Kirill</forenames></author><author><keyname>Chowdhary</keyname><forenames>Girish</forenames></author><author><keyname>Spencer</keyname><forenames>Billie F.</forenames><suffix>Jr</suffix></author></authors><title>Physics-Informed Neural Network based Damage Identification for Truss   Railroad Bridges</title><categories>cs.LG cs.AI physics.comp-ph</categories><comments>30 pages, 15 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Railroad bridges are a crucial component of the U.S. freight rail system, which moves over 40 percent of the nation's freight and plays a critical role in the economy. However, aging bridge infrastructure and increasing train traffic pose significant safety hazards and risk service disruptions. The U.S. rail network includes over 100,000 railroad bridges, averaging one every 1.4 miles of track, with steel bridges comprising over 50% of the network's total bridge length. Early identification and assessment of damage in these bridges remain challenging tasks. This study proposes a physics-informed neural network (PINN) based approach for damage identification in steel truss railroad bridges. The proposed approach employs an unsupervised learning approach, eliminating the need for large datasets typically required by supervised methods. The approach utilizes train wheel load data and bridge response during train crossing events as inputs for damage identification. The PINN model explicitly incorporates the governing differential equations of the linear time-varying (LTV) bridge-train system. Herein, this model employs a recurrent neural network (RNN) based architecture incorporating a custom Runge-Kutta (RK) integrator cell, designed for gradient-based learning. The proposed approach updates the bridge finite element model while also quantifying damage severity and localizing the affected structural members. A case study on the Calumet Bridge in Chicago, Illinois, with simulated damage scenarios, is used to demonstrate the model's effectiveness in identifying damage while maintaining low false-positive rates. Furthermore, the damage identification pipeline is designed to seamlessly integrate prior knowledge from inspections and drone surveys, also enabling context-aware updating and assessment of bridge's condition. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00196</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00196</id><created>2025-01-31</created><authors><author><keyname>Yilmaz</keyname><forenames>Abdurrahim</forenames></author><author><keyname>Yuceyalcin</keyname><forenames>Furkan</forenames></author><author><keyname>Gokyayla</keyname><forenames>Ece</forenames></author><author><keyname>Choi</keyname><forenames>Donghee</forenames></author><author><keyname>Demircali</keyname><forenames>Ozan Erdem Ali Anil</forenames></author><author><keyname>Varol</keyname><forenames>Rahmetullah</forenames></author><author><keyname>Kirabali</keyname><forenames>Ufuk Gorkem</forenames></author><author><keyname>Gencoglan</keyname><forenames>Gulsum</forenames></author><author><keyname>Posma</keyname><forenames>Joram M.</forenames></author><author><keyname>Temelkuran</keyname><forenames>Burak</forenames></author></authors><title>DermaSynth: Rich Synthetic Image-Text Pairs Using Open Access   Dermatology Datasets</title><categories>cs.CV cs.AI cs.CL</categories><comments>12 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  A major barrier to developing vision large language models (LLMs) in dermatology is the lack of large image--text pairs dataset. We introduce DermaSynth, a dataset comprising of 92,020 synthetic image--text pairs curated from 45,205 images (13,568 clinical and 35,561 dermatoscopic) for dermatology-related clinical tasks. Leveraging state-of-the-art LLMs, using Gemini 2.0, we used clinically related prompts and self-instruct method to generate diverse and rich synthetic texts. Metadata of the datasets were incorporated into the input prompts by targeting to reduce potential hallucinations. The resulting dataset builds upon open access dermatological image repositories (DERM12345, BCN20000, PAD-UFES-20, SCIN, and HIBA) that have permissive CC-BY-4.0 licenses. We also fine-tuned a preliminary Llama-3.2-11B-Vision-Instruct model, DermatoLlama 1.0, on 5,000 samples. We anticipate this dataset to support and accelerate AI research in dermatology. Data and code underlying this work are accessible at https://github.com/abdurrahimyilmaz/DermaSynth. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00197</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00197</id><created>2025-01-31</created><authors><author><keyname>Chang</keyname><forenames>Yingshan</forenames></author><author><keyname>Bisk</keyname><forenames>Yonatan</forenames></author></authors><title>Model Successor Functions</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The notion of generalization has moved away from the classical one defined in statistical learning theory towards an emphasis on out-of-domain generalization (OODG). Recently, there is a growing focus on inductive generalization, where a progression of difficulty implicitly governs the direction of domain shifts. In inductive generalization, it is often assumed that the training data lie in the easier side, while the testing data lie in the harder side. The challenge is that training data are always finite, but a learner is expected to infer an inductive principle that could be applied in an unbounded manner. This emerging regime has appeared in the literature under different names, such as length/logical/algorithmic extrapolation, but a formal definition is lacking. This work provides such a formalization that centers on the concept of model successors. Then we outline directions to adapt well-established techniques towards the learning of model successors. This work calls for restructuring of the research discussion around inductive generalization from fragmented task-centric communities to a more unified effort, focused on universal properties of learning and computation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00198</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00198</id><created>2025-01-31</created><authors><author><keyname>Zhang</keyname><forenames>Luyang</forenames></author><author><keyname>Jiao</keyname><forenames>Cathy</forenames></author><author><keyname>Li</keyname><forenames>Beibei</forenames></author><author><keyname>Xiong</keyname><forenames>Chenyan</forenames></author></authors><title>Fairshare Data Pricing for Large Language Models</title><categories>cs.GT cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Training data is a pivotal resource for building large language models (LLMs), but unfair pricing in data markets poses a serious challenge for both data buyers (e.g., LLM builders) and sellers (e.g., human annotators), which discourages market participation, reducing data quantity and quality. In this paper, we propose a fairshare pricing framework that sets training data prices using data valuation methods to quantify their contribution to LLMs. In our framework, buyers make purchasing decisions using data valuation and sellers set prices to maximize their profits based on the anticipated buyer purchases. We theoretically show that pricing derived from our framework is tightly linked to data valuation and buyers' budget, optimal for both buyers and sellers. Through market simulations using current LLMs and datasets (math problems, medical diagnosis, and physical reasoning), we show that our framework is fairshare for buyers by ensuring their purchased data is reflective of model training value, leading to higher LLM task performances per-dollar spent on data, and fairshare for sellers by ensuring they sell their data at optimal prices. Our framework lays the foundation for future research on equitable and sustainable data markets for large-scale AI. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00199</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00199</id><created>2025-01-31</created><authors><author><keyname>Ye</keyname><forenames>Xiuzhen</forenames></author><author><keyname>Tang</keyname><forenames>Wentao</forenames></author></authors><title>Optimal Construction of Data Injection Attacks on Process Systems</title><categories>eess.SY cs.SY</categories><comments>41 pages, 13 figures, submitted to AIChE journal</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  An information-theoretic framework for constructing data injection attacks on process systems, from the attacker's standpoint, is studied. The attack construction aims to distract the stationary distributions of the process variables and stay stealthy, simultaneously. The problem is formulated as designing a multivariate Gaussian distribution to maximize the Kullback-Leibler divergence between the stationary distributions of states and state estimates under attacks and without attacks, while minimizing that between the distributions of sensor measurements. When the attacker has limited access to sensors, sparse attacks are proposed by incorporating a sparsity constraint on the attack. We conduct a theoretical analysis on the convexity of the attack construction problem and present a greedy algorithm, which allows for a systematic quantification of measurements' vulnerability of process systems. We numerically evaluate the performance of proposed constructions on a two-reactor process. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00201</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00201</id><created>2025-01-31</created><authors><author><keyname>Chen</keyname><forenames>Yisong</forenames></author><author><keyname>Zhao</keyname><forenames>Chuqing</forenames></author><author><keyname>Xu</keyname><forenames>Yixin</forenames></author><author><keyname>Nie</keyname><forenames>Chuanhao</forenames></author></authors><title>Year-over-Year Developments in Financial Fraud Detection via Deep   Learning: A Systematic Literature Review</title><categories>cs.LG cs.AI q-fin.ST</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper systematically reviews advancements in deep learning (DL) techniques for financial fraud detection, a critical issue in the financial sector. Using the Kitchenham systematic literature review approach, 57 studies published between 2019 and 2024 were analyzed. The review highlights the effectiveness of various deep learning models such as Convolutional Neural Networks, Long Short-Term Memory, and transformers across domains such as credit card transactions, insurance claims, and financial statement audits. Performance metrics such as precision, recall, F1-score, and AUC-ROC were evaluated. Key themes explored include the impact of data privacy frameworks and advancements in feature engineering and data preprocessing. The study emphasizes challenges such as imbalanced datasets, model interpretability, and ethical considerations, alongside opportunities for automation and privacy-preserving techniques such as blockchain integration and Principal Component Analysis. By examining trends over the past five years, this review identifies critical gaps and promising directions for advancing DL applications in financial fraud detection, offering actionable insights for researchers and practitioners. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00202</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00202</id><created>2025-01-31</created><authors><author><keyname>Kim</keyname><forenames>Hyeok</forenames></author><author><keyname>Jeng</keyname><forenames>Mingyoung J.</forenames></author><author><keyname>Smith</keyname><forenames>Kaitlin N.</forenames></author></authors><title>Toward Human-Quantum Computer Interaction: Interface Techniques for   Usable Quantum Computing</title><categories>cs.HC</categories><comments>18 pages, 11 figures, 1 table. Conditionally accepted to ACM CHI 2025</comments><doi>10.1145/3706598.3713370</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  By leveraging quantum-mechanical properties like superposition, entanglement, and interference, quantum computing (QC) offers promising solutions for problems that classical computing has not been able to solve efficiently, such as drug discovery, cryptography, and physical simulation. Unfortunately, adopting QC remains difficult for potential users like QC beginners and application-specific domain experts, due to limited theoretical and practical knowledge, the lack of integrated interface-wise support, and poor documentation. For example, to use quantum computers, one has to convert conceptual logic into low-level codes, analyze quantum program results, and share programs and results. To support the wider adoption of QC, we, as designers and QC experts, propose interaction techniques for QC through design iterations. These techniques include writing quantum codes conceptually, comparing initial quantum programs with optimized programs, sharing quantum program results, and exploring quantum machines. We demonstrate the feasibility and utility of these techniques via use cases with high-fidelity prototypes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00203</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00203</id><created>2025-01-31</created><authors><author><keyname>Sun</keyname><forenames>Shengyang</forenames></author><author><keyname>Zhang</keyname><forenames>Yian</forenames></author><author><keyname>Bukharin</keyname><forenames>Alexander</forenames></author><author><keyname>Mosallanezhad</keyname><forenames>David</forenames></author><author><keyname>Zeng</keyname><forenames>Jiaqi</forenames></author><author><keyname>Singhal</keyname><forenames>Soumye</forenames></author><author><keyname>Shen</keyname><forenames>Gerald</forenames></author><author><keyname>Renduchintala</keyname><forenames>Adi</forenames></author><author><keyname>Konuk</keyname><forenames>Tugrul</forenames></author><author><keyname>Dong</keyname><forenames>Yi</forenames></author><author><keyname>Wang</keyname><forenames>Zhilin</forenames></author><author><keyname>Chichkov</keyname><forenames>Dmitry</forenames></author><author><keyname>Delalleau</keyname><forenames>Olivier</forenames></author><author><keyname>Kuchaiev</keyname><forenames>Oleksii</forenames></author></authors><title>Reward-aware Preference Optimization: A Unified Mathematical Framework   for Model Alignment</title><categories>cs.LG cs.CL</categories><comments>8 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The rapid development of large language model (LLM) alignment algorithms has resulted in a complex and fragmented landscape, with limited clarity on the effectiveness of different methods and their inter-connections. This paper introduces Reward-Aware Preference Optimization (RPO), a mathematical framework that unifies popular preference optimization techniques in LLM alignment, including DPO, IPO, SimPO, and REINFORCE (LOO), among others. RPO provides a structured approach to disentangle and systematically study the impact of various design choices, such as the optimization objective, the number of responses per prompt, and the use of implicit versus explicit reward models, on LLM preference optimization. We additionally propose a new experimental setup that enables the clean and direct ablation of such design choices. Through an extensive series of ablation studies within the RPO framework, we gain insights into the critical factors shaping model alignment, offering practical guidance on the most effective strategies for improving LLM alignment. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00204</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00204</id><created>2025-01-31</created><authors><author><keyname>Balcan</keyname><forenames>Maria-Florina</forenames></author><author><keyname>Bernasconi</keyname><forenames>Martino</forenames></author><author><keyname>Castiglioni</keyname><forenames>Matteo</forenames></author><author><keyname>Celli</keyname><forenames>Andrea</forenames></author><author><keyname>Harris</keyname><forenames>Keegan</forenames></author><author><keyname>Wu</keyname><forenames>Zhiwei Steven</forenames></author></authors><title>Nearly-Optimal Bandit Learning in Stackelberg Games with Side   Information</title><categories>cs.LG cs.GT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study the problem of online learning in Stackelberg games with side information between a leader and a sequence of followers. In every round the leader observes contextual information and commits to a mixed strategy, after which the follower best-responds. We provide learning algorithms for the leader which achieve $O(T^{1/2})$ regret under bandit feedback, an improvement from the previously best-known rates of $O(T^{2/3})$. Our algorithms rely on a reduction to linear contextual bandits in the utility space: In each round, a linear contextual bandit algorithm recommends a utility vector, which our algorithm inverts to determine the leader's mixed strategy. We extend our algorithms to the setting in which the leader's utility function is unknown, and also apply it to the problems of bidding in second-price auctions with side information and online Bayesian persuasion with public and private states. Finally, we observe that our algorithms empirically outperform previous results on numerical simulations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00205</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00205</id><created>2025-01-31</created><authors><author><keyname>Khater</keyname><forenames>Omar H.</forenames></author><author><keyname>Siddiqui</keyname><forenames>Abdul Jabbar</forenames></author><author><keyname>Hossain</keyname><forenames>M. Shamim</forenames></author></authors><title>EcoWeedNet: A Lightweight and Automated Weed Detection Method for   Sustainable Next-Generation Agricultural Consumer Electronics</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sustainable agriculture plays a crucial role in ensuring world food security for consumers. A critical challenge faced by sustainable precision agriculture is weed growth, as weeds share essential resources with the crops, such as water, soil nutrients, and sunlight, which notably affect crop yields. The traditional methods employed to combat weeds include the usage of chemical herbicides and manual weed removal methods. However, these could damage the environment and pose health hazards. The adoption of automated computer vision technologies and ground agricultural consumer electronic vehicles in precision agriculture offers sustainable, low-carbon solutions. However, prior works suffer from issues such as low accuracy and precision and high computational expense. This work proposes EcoWeedNet, a novel model with enhanced weed detection performance without adding significant computational complexity, aligning with the goals of low-carbon agricultural practices. Additionally, our model is lightweight and optimal for deployment on ground-based consumer electronic agricultural vehicles and robots. The effectiveness of the proposed model is demonstrated through comprehensive experiments on the CottonWeedDet12 benchmark dataset reflecting real-world scenarios. EcoWeedNet achieves performance close to that of large models yet with much fewer parameters. (approximately 4.21% of the parameters and 6.59% of the GFLOPs of YOLOv4). This work contributes effectively to the development of automated weed detection methods for next-generation agricultural consumer electronics featuring lower energy consumption and lower carbon footprint. This work paves the way forward for sustainable agricultural consumer technologies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00206</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00206</id><created>2025-01-31</created><authors><author><keyname>Egger</keyname><forenames>Maximilian</forenames></author><author><keyname>Bitar</keyname><forenames>Rawad</forenames></author><author><keyname>Wachter-Zeh</keyname><forenames>Antonia</forenames></author><author><keyname>Weinberger</keyname><forenames>Nir</forenames></author><author><keyname>Gündüz</keyname><forenames>Deniz</forenames></author></authors><title>BICompFL: Stochastic Federated Learning with Bi-Directional Compression</title><categories>cs.LG cs.DC cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the prominent communication bottleneck in federated learning (FL). We specifically consider stochastic FL, in which models or compressed model updates are specified by distributions rather than deterministic parameters. Stochastic FL offers a principled approach to compression, and has been shown to reduce the communication load under perfect downlink transmission from the federator to the clients. However, in practice, both the uplink and downlink communications are constrained. We show that bi-directional compression for stochastic FL has inherent challenges, which we address by introducing BICompFL. Our BICompFL is experimentally shown to reduce the communication cost by an order of magnitude compared to multiple benchmarks, while maintaining state-of-the-art accuracies. Theoretically, we study the communication cost of BICompFL through a new analysis of an importance-sampling based technique, which exposes the interplay between uplink and downlink communication costs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00208</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00208</id><created>2025-01-31</created><authors><author><keyname>Granados</keyname><forenames>Ana</forenames></author><author><keyname>Koroutchev</keyname><forenames>Kostadin</forenames></author><author><keyname>Rodríguez</keyname><forenames>Francisco de Borja</forenames></author></authors><title>Discovering Dataset Nature through Algorithmic Clustering based on   String Compression</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Transactions on Knowledge and Data Engineering 2015</journal-ref><doi>10.1109/TKDE.2014.2345396</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Text datasets can be represented using models that do not preserve text structure, or using models that preserve text structure. Our hypothesis is that depending on the dataset nature, there can be advantages using a model that preserves text structure over one that does not, and viceversa. The key is to determine the best way of representing a particular dataset, based on the dataset itself. In this work, we propose to investigate this problem by combining text distortion and algorithmic clustering based on string compression. Specifically, a distortion technique previously developed by the authors is applied to destroy text structure progressively. Following this, a clustering algorithm based on string compression is used to analyze the effects of the distortion on the information contained in the texts. Several experiments are carried out on text datasets and artificially-generated datasets. The results show that in strongly structural datasets the clustering results worsen as text structure is progressively destroyed. Besides, they show that using a compressor which enables the choice of the size of the left-context symbols helps to determine the nature of the datasets. Finally, the results are contrasted with a method based on multidimensional projections and analogous conclusions are obtained. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00212</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00212</id><created>2025-01-31</created><authors><author><keyname>Dong</keyname><forenames>Kefan</forenames></author><author><keyname>Ma</keyname><forenames>Tengyu</forenames></author></authors><title>Beyond Limited Data: Self-play LLM Theorem Provers with Iterative   Conjecturing and Proving</title><categories>cs.LG cs.AI cs.LO</categories><comments>22 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  A fundamental challenge in formal theorem proving by LLMs is the lack of high-quality training data. Although reinforcement learning or expert iteration partially mitigates this issue by alternating between LLM generating proofs and finetuning them on correctly generated ones, performance quickly plateaus due to the scarcity of correct proofs (sparse rewards). To keep improving the models with limited data, we draw inspiration from mathematicians, who continuously develop new results, partly by proposing novel conjectures or exercises (which are often variants of known results) and attempting to solve them. We design the Self-play Theorem Prover (STP) that simultaneously takes on two roles, conjecturer and prover, each providing training signals to the other. The conjecturer is trained iteratively on previously generated conjectures that are barely provable by the current prover, which incentivizes it to generate increasingly challenging conjectures over time. The prover attempts to prove the conjectures with standard expert iteration. We evaluate STP with both Lean and Isabelle formal versifiers. With 19.8 billion tokens generated during the training in Lean, STP proves 26.3% of the statements in the LeanWorkbook dataset, doubling the previous best result of 13.2% achieved through expert iteration. The final model achieves state-of-the-art performance among whole-proof generation methods on miniF2F-test (61.1%, pass@3200), Proofnet-test (23.1%, pass@3200) and PutnamBench (8/644, pass@64). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00213</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00213</id><created>2025-01-31</created><authors><author><keyname>Tomihari</keyname><forenames>Akiyoshi</forenames></author><author><keyname>Sato</keyname><forenames>Issei</forenames></author></authors><title>Understanding Why Adam Outperforms SGD: Gradient Heterogeneity in   Transformers</title><categories>cs.LG cs.AI cs.NE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Transformer models are challenging to optimize with SGD and typically require adaptive optimizers such as Adam. However, the reasons behind the superior performance of Adam over SGD remain unclear. In this study, we investigate the optimization of transformer models by focusing on \emph{gradient heterogeneity}, defined as the disparity in gradient norms among parameters. Our analysis shows that gradient heterogeneity hinders gradient-based optimization, including SGD, while sign-based optimization, a simplified variant of Adam, is less affected. We further examine gradient heterogeneity in transformer models and show that it is influenced by the placement of layer normalization. Additionally, we show that the momentum term in sign-based optimization is important for preventing the excessive growth of linear-head parameters in tasks with many classes. Experimental results from fine-tuning transformer models in both NLP and vision domains validate our theoretical analyses. This study provides insights into the optimization challenges of transformer models and offers guidance for designing future optimization algorithms. Code is available at \url{https://github.com/tom4649/gradient-heterogeneity}. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00215</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00215</id><created>2025-01-31</created><authors><author><keyname>Spada</keyname><forenames>Fabio</forenames></author><author><keyname>Elango</keyname><forenames>Purnanand</forenames></author><author><keyname>Açıkmeşe</keyname><forenames>Behçet</forenames></author></authors><title>Impulsive Relative Motion Control with Continuous-Time Constraint   Satisfaction for Cislunar Space Missions</title><categories>eess.SY cs.SY</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Recent investments in cislunar applications open new frontiers for space missions within highly nonlinear dynamical regimes. In this paper, we propose a method based on Sequential Convex Programming (SCP) to loiter around a given target with impulsive actuation while satisfying path constraints continuously over the finite time-horizon, i.e., independently of the number of nodes in which domain is discretized. Location, timing, magnitude, and direction of a fixed number of impulses are optimized in a model predictive framework, exploiting the exact nonlinear dynamics of non-stationary orbital regimes. The proposed approach is validated on a relative orbiting problem with respect to a selenocentric Near Rectilinear Halo Orbit. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00217</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00217</id><created>2025-01-31</created><authors><author><keyname>Hassanpour</keyname><forenames>Negar</forenames></author><author><keyname>Janjua</keyname><forenames>Muhammad Kamran</forenames></author><author><keyname>Zhang</keyname><forenames>Kunlin</forenames></author><author><keyname>Lavasani</keyname><forenames>Sepehr</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaowen</forenames></author><author><keyname>Zhou</keyname><forenames>Chunhua</forenames></author><author><keyname>Gao</keyname><forenames>Chao</forenames></author></authors><title>Fantastic Multi-Task Gradient Updates and How to Find Them In a Cone</title><categories>cs.LG cs.AI cs.CV</categories><comments>16 pages, 7 figures, 5 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Balancing competing objectives remains a fundamental challenge in multi-task learning (MTL), primarily due to conflicting gradients across individual tasks. A common solution relies on computing a dynamic gradient update vector that balances competing tasks as optimization progresses. Building on this idea, we propose ConicGrad, a principled, scalable, and robust MTL approach formulated as a constrained optimization problem. Our method introduces an angular constraint to dynamically regulate gradient update directions, confining them within a cone centered on the reference gradient of the overall objective. By balancing task-specific gradients without over-constraining their direction or magnitude, ConicGrad effectively resolves inter-task gradient conflicts. Moreover, our framework ensures computational efficiency and scalability to high-dimensional parameter spaces. We conduct extensive experiments on standard supervised learning and reinforcement learning MTL benchmarks, and demonstrate that ConicGrad achieves state-of-the-art performance across diverse tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00219</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00219</id><created>2025-01-31</created><authors><author><keyname>Lin</keyname><forenames>Yiling</forenames></author><author><keyname>Li</keyname><forenames>Linzhuo</forenames></author><author><keyname>Wu</keyname><forenames>Lingfei</forenames></author></authors><title>Team Size and Its Negative Impact on the Disruption Index</title><categories>cs.SI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As science transitions from the age of lone geniuses to an era of collaborative teams, the question of whether large teams can sustain the creativity of individuals and continue driving innovation has become increasingly important. Our previous research first revealed a negative relationship between team size and the Disruption Index-a network-based metric of innovation-by analyzing 65 million projects across papers, patents, and software over half a century. This work has sparked lively debates within the scientific community about the robustness of the Disruption Index in capturing the impact of team size on innovation. Here, we present additional evidence that the negative link between team size and disruption holds, even when accounting for factors such as reference length, citation impact, and historical time. We further show how a narrow 5-year window for measuring disruption can misrepresent this relationship as positive, underestimating the long-term disruptive potential of small teams. Like "sleeping beauties," small teams need a decade or more to see their transformative contributions to science. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00220</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00220</id><created>2025-01-31</created><authors><author><keyname>Sarasa</keyname><forenames>Guillermo</forenames></author><author><keyname>Granados</keyname><forenames>Ana</forenames></author><author><keyname>Rodríguez</keyname><forenames>Francisco B</forenames></author></authors><title>Algorithmic Clustering based on String Compression to Extract P300   Structure in EEG Signals</title><categories>cs.LG cs.IT eess.SP math.IT</categories><journal-ref>Computer Methods and Programs in Biomedicine 2019</journal-ref><doi>10.1016/j.cmpb.2019.03.009</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  P300 is an Event-Related Potential widely used in Brain-Computer Interfaces, but its detection is challenging due to inter-subject and temporal variability. This work introduces a clustering methodology based on Normalized Compression Distance (NCD) to extract the P300 structure, ensuring robustness against variability. We propose a novel signal-to-ASCII transformation to generate compression-friendly objects, which are then clustered using a hierarchical tree-based method and a multidimensional projection approach. Experimental results on two datasets demonstrate the method's ability to reveal relevant P300 structures, showing clustering performance comparable to state-of-the-art approaches. Furthermore, analysis at the electrode level suggests that the method could assist in electrode selection for P300 detection. This compression-driven clustering methodology offers a complementary tool for EEG analysis and P300 identification. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00221</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00221</id><created>2025-01-31</created><authors><author><keyname>Shen</keyname><forenames>Jocelyn</forenames></author><author><keyname>Lee</keyname><forenames>Audrey</forenames></author><author><keyname>Alghowinem</keyname><forenames>Sharifa</forenames></author><author><keyname>Adkins</keyname><forenames>River</forenames></author><author><keyname>Breazeal</keyname><forenames>Cynthia</forenames></author><author><keyname>Park</keyname><forenames>Hae Won</forenames></author></authors><title>Social Robots as Social Proxies for Fostering Connection and Empathy   Towards Humanity</title><categories>cs.HC cs.RO</categories><comments>Accepted to HRI 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Despite living in an increasingly connected world, social isolation is a prevalent issue today. While social robots have been explored as tools to enhance social connection through companionship, their potential as asynchronous social platforms for fostering connection towards humanity has received less attention. In this work, we introduce the design of a social support companion that facilitates the exchange of emotionally relevant stories and scaffolds reflection to enhance feelings of connection via five design dimensions. We investigate how social robots can serve as "social proxies" facilitating human stories, passing stories from other human narrators to the user. To this end, we conduct a real-world deployment of 40 robot stations in users' homes over the course of two weeks. Through thematic analysis of user interviews, we find that social proxy robots can foster connection towards other people's experiences via mechanisms such as identifying connections across stories or offering diverse perspectives. We present design guidelines from our study insights on the use of social robot systems that serve as social platforms to enhance human empathy and connection. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00222</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00222</id><created>2025-01-31</created><authors><author><keyname>Power</keyname><forenames>Conor</forenames></author><author><keyname>Koutris</keyname><forenames>Paraschos</forenames></author><author><keyname>Hellerstein</keyname><forenames>Joseph M</forenames></author></authors><title>The Free Termination Property of Queries Over Time</title><categories>cs.DB cs.DC cs.PL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Building on prior work on distributed databases and the CALM Theorem, we define and study the question of free termination: in the absence of distributed coordination, what query properties allow nodes in a distributed (database) system to unilaterally terminate execution even though they may receive additional data or messages in the future? This completeness question is complementary to the soundness questions studied in the CALM literature. We also develop a new model based on semiautomata that allows us to bridge from the relational transducer model of the CALM papers to algebraic models that are popular among software engineers (e.g. CRDTs) and of increasing interest to database theory for datalog extensions and incremental view maintenance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00224</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00224</id><created>2025-01-31</created><authors><author><keyname>Wild</keyname><forenames>Paul</forenames></author><author><keyname>Schröder</keyname><forenames>Lutz</forenames></author></authors><title>Behavioural Conformances based on Lax Couplings</title><categories>cs.LO</categories><msc-class>68Q85, 03B70</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Behavioural conformances -- e.g. behavioural equivalences, distances, preorders -- on a wide range of system types (non-deterministic, probabilistic, weighted etc.) can be dealt with uniformly in the paradigm of universal coalgebra. One of the most commonly used constructions for defining behavioural distances on coalgebras arises as a generalization of the well-known Wasserstein metric. In this construction, couplings of probability distributions are replaced with couplings of more general objects, depending on the functor describing the system type. In many cases, however, the set of couplings of two functor elements is empty, which causes such elements to have infinite distance even in situations where this is not desirable. We propose an approach to defining behavioural distances and preorders based on a more liberal notion of coupling where the coupled elements are matched laxly rather than on-the-nose. We thereby substantially broaden the range of behavioural conformances expressible in terms of couplings, covering, e.g., refinement of modal transition systems and behavioural distance on metric labelled Markov chains. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00225</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00225</id><created>2025-01-31</created><authors><author><keyname>Harris</keyname><forenames>Keegan</forenames></author><author><keyname>Slivkins</keyname><forenames>Aleksandrs</forenames></author></authors><title>Should You Use Your Large Language Model to Explore or Exploit?</title><categories>cs.LG cs.AI cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We evaluate the ability of the current generation of large language models (LLMs) to help a decision-making agent facing an exploration-exploitation tradeoff. We use LLMs to explore and exploit in silos in various (contextual) bandit tasks. We find that while the current LLMs often struggle to exploit, in-context mitigations may be used to substantially improve performance for small-scale tasks. However even then, LLMs perform worse than a simple linear regression. On the other hand, we find that LLMs do help at exploring large action spaces with inherent semantics, by suggesting suitable candidates to explore. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00226</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00226</id><created>2025-01-31</created><authors><author><keyname>Xing</keyname><forenames>Jun</forenames></author><author><keyname>Bhatia</keyname><forenames>Mayur</forenames></author><author><keyname>Phulwani</keyname><forenames>Sahil</forenames></author><author><keyname>Suresh</keyname><forenames>Darshan</forenames></author><author><keyname>Matta</keyname><forenames>Rafik</forenames></author></authors><title>HackerRank-ASTRA: Evaluating Correctness &amp; Consistency of Large Language   Models on cross-domain multi-file project problems</title><categories>cs.LG cs.SE</categories><comments>24 pages, 25 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Evaluating the real-world applicability of large language models (LLMs) provides valuable insights for their development and use in software development tasks. Existing benchmarks often focus on standalone coding problems or specific libraries, overlooking multi-file, project-based scenarios and lacking a rigorous evaluation of consistency. The HackerRank-ASTRA Benchmark introduces project-based coding problems that mirror real-world scenarios. It evaluates model consistency through 32 runs (k = 32) and median standard deviation while incorporating taxonomy-level analysis to assess sub-skill capabilities. Initial evaluations on 65 problems show that the top three models -- o1, o1-preview, and Claude-3.5-Sonnet-1022 -- achieved comparable average scores of 75%, with no statistically significant differences in performance. Notably, Claude-3.5-Sonnet-1022 demonstrated the highest consistency across problems, with low variability (SD = 0.0497), which was statistically significant compared to other models, highlighting its reliability for real-world software development tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00227</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00227</id><created>2025-01-31</created><authors><author><keyname>Keramati</keyname><forenames>Hadi</forenames></author><author><keyname>Hamdullahpur</keyname><forenames>Feridun</forenames></author></authors><title>AK-SLRL: Adaptive Krylov Subspace Exploration Using Single-Life   Reinforcement Learning for Sparse Linear System</title><categories>cs.CE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents a single-life reinforcement learning (SLRL) approach to adaptively select the dimension of the Krylov subspace during the generalized minimal residual (GMRES) iteration. GMRES is an iterative algorithm for solving large and sparse linear systems of equations in the form of \(Ax = b\) which are mainly derived from partial differential equations (PDEs). The proposed framework uses RL to adjust the Krylov subspace dimension (m) in the GMRES (m) algorithm. This research demonstrates that altering the dimension of the Krylov subspace in an online setup using SLRL can accelerate the convergence of the GMRES algorithm by more than an order of magnitude. A comparison of different matrix sizes and sparsity levels is performed to demonstrate the effectiveness of adaptive Krylov subspace exploration using single-life RL (AK-SLRL). We compare AK-SLRL with constant-restart GMRES by applying the highest restart value used in AK-SLRL to the GMRES method. The results show that using an adjustable restart parameter with single-life soft-actor critic (SLSAC) and an experience replay buffer sized to half the matrix dimension converges significantly faster than the constant restart GMRES with higher values. Higher values of the restart parameter are equivalent to a higher number of Arnoldi iterations to construct an orthonormal basis for the Krylov subspace $ K_m(A, r_0) $. This process includes constructing $m$ orthonormal vectors and updating the Hessenberg matrix $H$. Therefore, lower values of $m$ result in reduced computation needed in GMRES minimization to solve the least-squares problem in the smaller Hessenberg matrix. The robustness of the result is validated through a wide range of matrix dimensions and sparsity. This paper contributes to the series of RL combinations with numerical solvers to achieve accelerated scientific computing. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00228</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00228</id><created>2025-01-31</created><authors><author><keyname>Yang</keyname><forenames>Yuanyuan</forenames></author><author><keyname>Zhang</keyname><forenames>Ruimin</forenames></author><author><keyname>Morgenstern</keyname><forenames>Jamie</forenames></author><author><keyname>Xu</keyname><forenames>Haifeng</forenames></author></authors><title>Markovian Pandora's box</title><categories>cs.GT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we study the Markovian Pandora's Box Problem, where decisions are governed by both order constraints and Markovianly correlated rewards, structured within a shared directed acyclic graph. To the best of our knowledge, previous work has not incorporated Markovian dependencies in this setting. This framework is particularly relevant to applications such as data or computation driven algorithm design, where exploration of future models incurs cost.   We present optimal fully adaptive strategies where the associated graph forms a forest. Under static transition, we introduce a strategy that achieves a near optimal expected payoff in multi line graphs and a 1/2 approximation in forest-structured graphs. Notably, this algorithm provides a significant speedup over the exact solution, with the improvement becoming more pronounced as the graph size increases. Our findings deepen the understanding of sequential exploration under Markovian correlations in graph-based decision-making. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00229</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00229</id><created>2025-01-31</created><authors><author><keyname>Wang</keyname><forenames>Jiyao</forenames></author><author><keyname>Sheng</keyname><forenames>Youyu</forenames></author><author><keyname>He</keyname><forenames>Qihang</forenames></author><author><keyname>Hu</keyname><forenames>Haolong</forenames></author><author><keyname>Liu</keyname><forenames>Shuwen</forenames></author><author><keyname>Gu</keyname><forenames>Feiqi</forenames></author><author><keyname>Jing</keyname><forenames>Yumei</forenames></author><author><keyname>He</keyname><forenames>Dengbo</forenames></author></authors><title>Enhancing Psychotherapeutic Alliance in College: When and How to   Integrate Multimodal Large Language Models in Psychotherapy</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As mental health issues rise among college students, there is an increasing interest and demand in leveraging Multimodal Language Models (MLLM) to enhance mental support services, yet integrating them into psychotherapy remains theoretical or non-user-centered. This study investigated the opportunities and challenges of using MLLMs within the campus psychotherapy alliance in China. Through three studies involving both therapists and student clients, we argue that the ideal role for MLLMs at this stage is as an auxiliary tool to human therapists. Users widely expect features such as triage matching and real-time emotion recognition. At the same time, for independent therapy by MLLM, concerns about capabilities and privacy ethics remain prominent, despite high demands for personalized avatars and non-verbal communication. Our findings further indicate that users' sense of social identity and perceived relative status of MLLMs significantly influence their acceptance. This study provides insights for future intelligent campus mental healthcare. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00232</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00232</id><created>2025-01-31</created><authors><author><keyname>Nickzamir</keyname><forenames>Mehdi</forenames></author><author><keyname>Gandab</keyname><forenames>Seyed Mohammad Sheikh Ahamdi</forenames></author></authors><title>A Hybrid Random Forest and CNN Framework for Tile-Wise Oil-Water   Classification in Hyperspectral Images</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel hybrid Random Forest and Convolutional Neural Network (CNN) framework is presented for oil-water classification in hyperspectral images (HSI). To address the challenge of preserving spatial context, the images were divided into smaller, non-overlapping tiles, which served as the basis for training, validation, and testing. Random Forest demonstrated strong performance in pixel-wise classification, outperforming models such as XGBoost, Attention-Based U-Net, and HybridSN. However, Random Forest loses spatial context, limiting its ability to fully exploit the spatial relationships in hyperspectral data. To improve performance, a CNN was trained on the probability maps generated by the Random Forest, leveraging the CNN's capacity to incorporate spatial context. The hybrid approach achieved 7.6% improvement in recall (to 0.85), 2.4% improvement in F1 score (to 0.84), and 0.54% improvement in AUC (to 0.99) compared to the baseline. These results highlight the effectiveness of combining probabilistic outputs with spatial feature learning for context-aware analysis of hyperspectral images. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00233</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00233</id><created>2025-01-31</created><authors><author><keyname>Chalaki</keyname><forenames>Mahdi</forenames></author><author><keyname>Zakerimanesh</keyname><forenames>Amir</forenames></author><author><keyname>Soleymani</keyname><forenames>Abed</forenames></author><author><keyname>Mushahwar</keyname><forenames>Vivian</forenames></author><author><keyname>Tavakoli</keyname><forenames>Mahdi</forenames></author></authors><title>Vision-Based Fuzzy Control System for Smart Walkers: Enhancing Usability   for Stroke Survivors with Unilateral Upper Limb Impairments</title><categories>cs.RO cs.SY eess.SY</categories><comments>7 pages, Accepted in IEEE International Conference on Robotics and   Automation (ICRA) 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobility impairments, particularly those caused by stroke-induced hemiparesis, significantly impact independence and quality of life. Current smart walker controllers operate by using input forces from the user to control linear motion and input torques to dictate rotational movement; however, because they predominantly rely on user-applied torque exerted on the device handle as an indicator of user intent to turn, they fail to adequately accommodate users with unilateral upper limb impairments. This leads to increased physical strain and cognitive load. This paper introduces a novel smart walker equipped with a fuzzy control algorithm that leverages shoulder abduction angles to intuitively interpret user intentions using just one functional hand. By integrating a force sensor and stereo camera, the system enhances walker responsiveness and usability. Experimental evaluations with five participants showed that the fuzzy controller outperformed the traditional admittance controller, reducing wrist torque while using the right hand to operate the walker by 12.65% for left turns, 80.36% for straight paths, and 81.16% for right turns. Additionally, average user comfort ratings on a Likert scale increased from 1 to 4. Results confirmed a strong correlation between shoulder abduction angles and directional intent, with users reporting decreased effort and enhanced ease of use. This study contributes to assistive robotics by providing an adaptable control mechanism for smart walkers, suggesting a pathway towards enhancing mobility and independence for individuals with mobility impairments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00234</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00234</id><created>2025-01-31</created><authors><author><keyname>Ren</keyname><forenames>Yinuo</forenames></author><author><keyname>Chen</keyname><forenames>Haoxuan</forenames></author><author><keyname>Zhu</keyname><forenames>Yuchen</forenames></author><author><keyname>Guo</keyname><forenames>Wei</forenames></author><author><keyname>Chen</keyname><forenames>Yongxin</forenames></author><author><keyname>Rotskoff</keyname><forenames>Grant M.</forenames></author><author><keyname>Tao</keyname><forenames>Molei</forenames></author><author><keyname>Ying</keyname><forenames>Lexing</forenames></author></authors><title>Fast Solvers for Discrete Diffusion Models: Theory and Applications of   High-Order Algorithms</title><categories>cs.LG cs.CV cs.NA math.NA physics.comp-ph stat.ML</categories><comments>38 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Discrete diffusion models have emerged as a powerful generative modeling framework for discrete data with successful applications spanning from text generation to image synthesis. However, their deployment faces challenges due to the high dimensionality of the state space, necessitating the development of efficient inference algorithms. Current inference approaches mainly fall into two categories: exact simulation and approximate methods such as $\tau$-leaping. While exact methods suffer from unpredictable inference time and redundant function evaluations, $\tau$-leaping is limited by its first-order accuracy. In this work, we advance the latter category by tailoring the first extension of high-order numerical inference schemes to discrete diffusion models, enabling larger step sizes while reducing error. We rigorously analyze the proposed schemes and establish the second-order accuracy of the $\theta$-trapezoidal method in KL divergence. Empirical evaluations on GPT-2 level text and ImageNet-level image generation tasks demonstrate that our method achieves superior sample quality compared to existing approaches under equivalent computational constraints. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00238</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00238</id><created>2025-01-31</created><authors><author><keyname>Gohar</keyname><forenames>Usman</forenames></author><author><keyname>Hunter</keyname><forenames>Michael C.</forenames></author><author><keyname>Cohen</keyname><forenames>Myra B.</forenames></author><author><keyname>Lutz</keyname><forenames>Robyn R.</forenames></author></authors><title>A Taxonomy of Real-World Defeaters in Safety Assurance Cases</title><categories>cs.SE</categories><comments>ICSE 2025, Workshop on Multi-disciplinary, Open, and integRatEd   Requirements Engineering</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The rise of cyber-physical systems in safety-critical domains calls for robust risk-evaluation frameworks. Assurance cases, often required by regulatory bodies, are a structured approach to demonstrate that a system meets its safety requirements. However, assurance cases are fraught with challenges, such as incomplete evidence and gaps in reasoning, called defeaters, that can call into question the credibility and robustness of assurance cases. Identifying these defeaters increases confidence in the assurance case and can prevent catastrophic failures. The search for defeaters in an assurance case, however, is not structured, and there is a need to standardize defeater analysis. The software engineering community thus could benefit from having a reusable classification of real-world defeaters in software assurance cases. In this paper, we conducted a systematic study of literature from the past 20 years. Using open coding, we derived a taxonomy with seven broad categories, laying the groundwork for standardizing the analysis and management of defeaters in safety-critical systems. We provide our artifacts as open source for the community to use and build upon, thus establishing a common framework for understanding defeaters. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00240</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00240</id><created>2025-01-31</created><authors><author><keyname>Zhang</keyname><forenames>Yasi</forenames></author><author><keyname>Leong</keyname><forenames>Oscar</forenames></author></authors><title>Learning Difference-of-Convex Regularizers for Inverse Problems: A   Flexible Framework with Theoretical Guarantees</title><categories>stat.ML cs.LG eess.IV math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning effective regularization is crucial for solving ill-posed inverse problems, which arise in a wide range of scientific and engineering applications. While data-driven methods that parameterize regularizers using deep neural networks have demonstrated strong empirical performance, they often result in highly nonconvex formulations that lack theoretical guarantees. Recent work has shown that incorporating structured nonconvexity into neural network-based regularizers, such as weak convexity, can strike a balance between empirical performance and theoretical tractability. In this paper, we demonstrate that a broader class of nonconvex functions, difference-of-convex (DC) functions, can yield improved empirical performance while retaining strong convergence guarantees. The DC structure enables the use of well-established optimization algorithms, such as the Difference-of-Convex Algorithm (DCA) and a Proximal Subgradient Method (PSM), which extend beyond standard gradient descent. Furthermore, we provide theoretical insights into the conditions under which optimal regularizers can be expressed as DC functions. Extensive experiments on computed tomography (CT) reconstruction tasks show that our approach achieves strong performance across sparse and limited-view settings, consistently outperforming other weakly supervised learned regularizers. Our code is available at \url{https://github.com/YasminZhang/ADCR}. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00241</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00241</id><created>2025-01-31</created><authors><author><keyname>He</keyname><forenames>Shiqi</forenames></author><author><keyname>Jang</keyname><forenames>Insu</forenames></author><author><keyname>Chowdhury</keyname><forenames>Mosharaf</forenames></author></authors><title>Mordal: Automated Pretrained Model Selection for Vision Language Models</title><categories>cs.LG cs.AI cs.CL cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incorporating multiple modalities into large language models (LLMs) is a powerful way to enhance their understanding of non-textual data, enabling them to perform multimodal tasks. Vision language models (VLMs) form the fastest growing category of multimodal models because of their many practical use cases, including in healthcare, robotics, and accessibility. Unfortunately, even though different VLMs in the literature demonstrate impressive visual capabilities in different benchmarks, they are handcrafted by human experts; there is no automated framework to create task-specific multimodal models.   We introduce Mordal, an automated multimodal model search framework that efficiently finds the best VLM for a user-defined task without manual intervention. Mordal achieves this both by reducing the number of candidates to consider during the search process and by minimizing the time required to evaluate each remaining candidate. Our evaluation shows that Mordal can find the best VLM for a given problem using up to $8.9\times$--$11.6\times$ lower GPU hours than grid search. In the process of our evaluation, we have also discovered new VLMs that outperform their state-of-the-art counterparts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00242</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00242</id><created>2025-01-31</created><authors><author><keyname>Chakraborty</keyname><forenames>Shuvam</forenames></author><author><keyname>Bedewy</keyname><forenames>Ahmed</forenames></author><author><keyname>Li</keyname><forenames>Wenjun</forenames></author><author><keyname>Abedini</keyname><forenames>Navid</forenames></author></authors><title>Digital-Twin assisted Network Energy Optimization during Low Traffic   Hours</title><categories>cs.NI cs.SY eess.SY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As wireless network technology advances towards the sixth generation (6G), increasing network energy consumption has become a critical concern due to the growing demand for diverse services, radio deployments at various frequencies, larger bandwidths, and more antennas. Network operators must manage energy usage not only to reduce operational cost and improve revenue but also to minimize environmental impact by reducing the carbon footprint. The 3rd Generation Partnership Project (3GPP) has introduced several network energy savings (NES) features. However, the implementation details and system-level aspects of these features have not been thoroughly investigated. In this paper, we explore system-level resource optimization for network energy savings in low-traffic scenarios. We introduce multiple NES optimization formulations and strategies, and further analyze their performance using a detailed network digital twin. Our results demonstrate promising NES gains of up to 44%. Additionally, we provide practical considerations for implementing the proposed schemes and examine their impacts on user equipment (UE) operation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00244</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00244</id><created>2025-01-31</created><authors><author><keyname>Gonzalez</keyname><forenames>Alexandra</forenames></author><author><keyname>Matias</keyname><forenames>J. Nathan</forenames></author></authors><title>Measuring the Mental Health of Content Reviewers, a Systematic Review</title><categories>cs.CY cs.HC</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Artificial intelligence and social computing rely on hundreds of thousands of content reviewers to classify high volumes of harmful and forbidden content. Many workers report long-term, potentially irreversible psychological harm. This work is similar to activities that cause psychological harm to other kinds of helping professionals even after small doses of exposure. Yet researchers struggle to measure the mental health of content reviewers well enough to inform diagnoses, evaluate workplace improvements, hold employers accountable, or advance scientific understanding. This systematic review summarizes psychological measures from other professions and relates them to the experiences of content reviewers. After identifying 1,673 potential papers, we reviewed 143 that validate measures in related occupations. We summarize the uses of psychological measurement for content reviewing, differences between clinical and research measures, and 12 measures that are adaptable to content reviewing. We find serious gaps in measurement validity in regions where content review labor is common. Overall, we argue for reliable measures of content reviewer mental health that match the nature of the work and are culturally-relevant. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00245</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00245</id><created>2025-01-31</created><authors><author><keyname>Zou</keyname><forenames>Tianyuan</forenames></author><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Li</keyname><forenames>Peng</forenames></author><author><keyname>Xiong</keyname><forenames>Yufei</forenames></author><author><keyname>Zhang</keyname><forenames>Jianqing</forenames></author><author><keyname>Liu</keyname><forenames>Jingjing</forenames></author><author><keyname>Ye</keyname><forenames>Xiaozhou</forenames></author><author><keyname>Ouyang</keyname><forenames>Ye</forenames></author><author><keyname>Zhang</keyname><forenames>Ya-Qin</forenames></author></authors><title>Contrastive Private Data Synthesis via Weighted Multi-PLM Fusion</title><categories>cs.LG</categories><comments>16 pages, 11 tables, 7 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Substantial quantity and high quality are the golden rules of making a good training dataset with sample privacy protection equally important. Generating synthetic samples that resemble high-quality private data while ensuring Differential Privacy (DP), a formal privacy guarantee, promises scalability and practicality. However, existing methods relying on pre-trained models for data synthesis %that avoid fine-tuning large pre-trained generative models often struggle in data-deficient scenarios, suffering from limited sample size, inevitable generation noise and existing pre-trained model bias. To address these challenges, we propose a novel contrAstive private data Synthesis via Weighted multiple Pre-trained language models (PLM) framework, named as WASP. WASP utilizes limited private samples for more accurate private data distribution estimation via a Top-Q voting mechanism, and leverages low-quality synthetic samples for contrastive generation via collaboration among dynamically weighted multiple pre-trained models.Extensive experiments on 6 well-developed datasets with 6 open-source and 3 closed-source PLMs demonstrate the superiority of WASP in improving model performance over diverse downstream tasks. Code is available at https://anonymous.4open.science/r/WASP. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00246</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00246</id><created>2025-01-31</created><authors><author><keyname>Tonix</keyname><forenames>Larin</forenames></author><author><keyname>Baskerville</keyname><forenames>Morgana</forenames></author><author><keyname>Stourton</keyname><forenames>Nathaniel</forenames></author><author><keyname>Tattershall</keyname><forenames>Ophelia</forenames></author></authors><title>Context-Preserving Tensorial Reconfiguration in Large Language Model   Training</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Handling long-range dependencies in neural architectures has remained a persistent challenge due to computational limitations and inefficient contextual retention mechanisms. Tensorial operations have provided a foundation for restructuring model representations, yet conventional architectures have struggled to incorporate such techniques without introducing excessive complexity. A novel approach, Context-Preserving Tensorial Reconfiguration (CPTR), enables dynamic reorganization of weight tensors through structured factorization and adaptive contraction, allowing for enhanced contextual integration without substantial computational overhead. Empirical evaluations demonstrate that CPTR improves coherence retention across extended sequences, leading to measurable reductions in perplexity and improved recall accuracy for long-context tasks. Performance comparisons reveal that CPTR-enhanced models exhibit greater computational efficiency and reduced memory consumption while maintaining competitive language generation fluency and accuracy. Gradient stability metrics further validate the improved training efficiency, revealing more controlled variance in weight updates. Comparative studies across baseline and CPTR-enhanced models confirm that tensorial reconfiguration contributes to more stable and computationally efficient language modeling. The findings support the potential of CPTR in refining contemporary neural architectures for tasks requiring long-range contextual understanding and efficient memory utilization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00247</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00247</id><created>2025-01-31</created><authors><author><keyname>Rasheed</keyname><forenames>Abdullah</forenames></author><author><keyname>Dubagunta</keyname><forenames>Nidhi</forenames></author></authors><title>Bounding Distance Between Outputs in Distributed Lattice Agreement</title><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  This paper studies the lattice agreement problem and proposes a stronger form, $\varepsilon$-bounded lattice agreement, that enforces an additional tightness constraint on the outputs. To formalize the concept, we define a quasi-metric on the structure of the lattice, which captures a natural notion of distance between lattice elements. We consider the bounded lattice agreement problem in both synchronous and asynchronous systems, and provide algorithms that aim to minimize the distance between the output values, while satisfying the requirements of the classic lattice agreement problem. We show strong impossibility results for the asynchronous case, and a heuristic algorithm that achieves improved tightness with high probability, and we test an approximation of this algorithm to show that only a very small number of rounds are necessary. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00248</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00248</id><created>2025-01-31</created><authors><author><keyname>Li</keyname><forenames>Anran</forenames></author><author><keyname>Swensen</keyname><forenames>John P.</forenames></author><author><keyname>Hosseinzadeh</keyname><forenames>Mehdi</forenames></author></authors><title>Provably-Stable Neural Network-Based Control of Nonlinear Systems</title><categories>math.OC cs.LG cs.SY eess.SY</categories><journal-ref>Engineering Applications of Artificial Intelligence, volume 138,   pages 109252, year 2024</journal-ref><doi>10.1016/j.engappai.2024.109252</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In recent years, Neural Networks (NNs) have been employed to control nonlinear systems due to their potential capability in dealing with situations that might be difficult for conventional nonlinear control schemes. However, to the best of our knowledge, the current literature on NN-based control lacks theoretical guarantees for stability and tracking performance. This precludes the application of NN-based control schemes to systems where stringent stability and performance guarantees are required. To address this gap, this paper proposes a systematic and comprehensive methodology to design provably-stable NN-based control schemes for affine nonlinear systems. Rigorous analysis is provided to show that the proposed approach guarantees stability of the closed-loop system with the NN in the loop. Also, it is shown that the resulting NN-based control scheme ensures that system states asymptotically converge to a neighborhood around the desired equilibrium point, with a tunable proximity threshold. The proposed methodology is validated and evaluated via simulation studies on an inverted pendulum and experimental studies on a Parrot Bebop 2 drone. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00250</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00250</id><created>2025-01-31</created><authors><author><keyname>Fujioka</keyname><forenames>Takumu</forenames><affiliation>Nagoya Institute of Technology</affiliation></author><author><keyname>Tanaka</keyname><forenames>Gouhei</forenames><affiliation>Nagoya Institute of Technology</affiliation><affiliation>The University of Tokyo</affiliation></author></authors><title>Transformer-Based Vector Font Classification Using Different Font   Formats: TrueType versus PostScript</title><categories>cs.CV</categories><comments>8 pages, 8 figures, 4 tables, Submitted to IJCNN 2025. Code available   at https://github.com/fjktkm/truetype-vs-postscript-transformer</comments><acm-class>I.5.1; I.4.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern fonts adopt vector-based formats, which ensure scalability without loss of quality. While many deep learning studies on fonts focus on bitmap formats, deep learning for vector fonts remains underexplored. In studies involving deep learning for vector fonts, the choice of font representation has often been made conventionally. However, the font representation format is one of the factors that can influence the computational performance of machine learning models in font-related tasks. Here we show that font representations based on PostScript outlines outperform those based on TrueType outlines in Transformer-based vector font classification. TrueType outlines represent character shapes as sequences of points and their associated flags, whereas PostScript outlines represent them as sequences of commands. In previous research, PostScript outlines have been predominantly used when fonts are treated as part of vector graphics, while TrueType outlines are mainly employed when focusing on fonts alone. Whether to use PostScript or TrueType outlines has been mainly determined by file format specifications and precedent settings in previous studies, rather than performance considerations. To date, few studies have compared which outline format provides better embedding representations. Our findings suggest that information aggregation is crucial in Transformer-based deep learning for vector graphics, as in tokenization in language models and patch division in bitmap-based image recognition models. This insight provides valuable guidance for selecting outline formats in future research on vector graphics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00253</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00253</id><created>2025-01-31</created><authors><author><keyname>Long</keyname><forenames>Junhao</forenames></author><author><keyname>Yang</keyname><forenames>Fengwei</forenames></author><author><keyname>Yan</keyname><forenames>Juncheng</forenames></author><author><keyname>Zhang</keyname><forenames>Baoping</forenames></author><author><keyname>Jin</keyname><forenames>Chao</forenames></author><author><keyname>Yang</keyname><forenames>Jian</forenames></author><author><keyname>Zou</keyname><forenames>Changliang</forenames></author><author><keyname>Xu</keyname><forenames>Jun</forenames></author></authors><title>Patch Triplet Similarity Purification for Guided Real-World Low-Dose CT   Image Denoising</title><categories>eess.IV cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image denoising of low-dose computed tomography (LDCT) is an important problem for clinical diagnosis with reduced radiation exposure. Previous methods are mostly trained with pairs of synthetic or misaligned LDCT and normal-dose CT (NDCT) images. However, trained with synthetic noise or misaligned LDCT/NDCT image pairs, the denoising networks would suffer from blurry structure or motion artifacts. Since non-contrast CT (NCCT) images share the content characteristics to the corresponding NDCT images in a three-phase scan, they can potentially provide useful information for real-world LDCT image denoising. To exploit this aspect, in this paper, we propose to incorporate clean NCCT images as useful guidance for the learning of real-world LDCT image denoising networks. To alleviate the issue of spatial misalignment in training data, we design a new Patch Triplet Similarity Purification (PTSP) strategy to select highly similar patch (instead of image) triplets of LDCT, NDCT, and NCCT images for network training. Furthermore, we modify two image denoising transformers of SwinIR and HAT to accommodate the NCCT image guidance, by replacing vanilla self-attention with cross-attention. On our collected clinical dataset, the modified transformers trained with the data selected by our PTSP strategy show better performance than 15 comparison methods on real-world LDCT image denoising. Ablation studies validate the effectiveness of our NCCT image guidance and PTSP strategy. We will publicly release our data and code. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00258</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00258</id><created>2025-01-31</created><authors><author><keyname>Liu</keyname><forenames>Hongyi</forenames></author><author><keyname>Saha</keyname><forenames>Rajarshi</forenames></author><author><keyname>Jia</keyname><forenames>Zhen</forenames></author><author><keyname>Park</keyname><forenames>Youngsuk</forenames></author><author><keyname>Huang</keyname><forenames>Jiaji</forenames></author><author><keyname>Sabach</keyname><forenames>Shoham</forenames></author><author><keyname>Wang</keyname><forenames>Yu-Xiang</forenames></author><author><keyname>Karypis</keyname><forenames>George</forenames></author></authors><title>ProxSparse: Regularized Learning of Semi-Structured Sparsity Masks for   Pretrained LLMs</title><categories>cs.LG cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) have demonstrated exceptional performance in natural language processing tasks, yet their massive size makes serving them inefficient and costly. Semi-structured pruning has emerged as an effective method for model acceleration, but existing approaches are suboptimal because they focus on local, layer-wise optimizations using heuristic rules, failing to leverage global feedback. We present ProxSparse, a learning-based framework for mask selection enabled by regularized optimization. ProxSparse transforms the rigid, non-differentiable mask selection process into a smoother optimization procedure, allowing gradual mask exploration with flexibility. ProxSparse does not involve additional weight updates once the mask is determined. Our extensive evaluations on 7 widely used models show that ProxSparse consistently outperforms previously proposed semi-structured mask selection methods with significant improvement, demonstrating the effectiveness of our learned approach towards semi-structured pruning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00260</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00260</id><created>2025-01-31</created><authors><author><keyname>Han</keyname><forenames>Qishen</forenames></author><author><keyname>Schoenebeck</keyname><forenames>Grant</forenames></author><author><keyname>Tao</keyname><forenames>Biaoshuai</forenames></author><author><keyname>Xia</keyname><forenames>Lirong</forenames></author></authors><title>Strong Equilibria in Bayesian Games with Bounded Group Size</title><categories>cs.GT</categories><comments>Accepted by TheWebConf 2025 (WWW'25). 23 pages</comments><doi>10.1145/3696410.3714585</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the group strategic behaviors in Bayesian games. Equilibria in previous work do not consider group strategic behaviors with bounded sizes and are too ``strong'' to exist in many scenarios. We propose the ex-ante Bayesian $k$-strong equilibrium and the Bayesian $k$-strong equilibrium, where no group of at most $k$ agents can benefit from deviation. The two solution concepts differ in how agents calculate their utilities when contemplating whether a deviation is beneficial. Intuitively, agents are more conservative in the Bayesian $k$-strong equilibrium than in the ex-ante Bayesian $k$-strong equilibrium. With our solution concepts, we study collusion in the peer prediction mechanisms, as a representative of the Bayesian games with group strategic behaviors. We characterize the thresholds of the group size $k$ so that truthful reporting in the peer prediction mechanism is an equilibrium for each solution concept, respectively. Our solution concepts can serve as criteria to evaluate the robustness of a peer prediction mechanism against collusion. Besides the peer prediction problem, we also discuss two other potential applications of our new solution concepts, voting and Blotto games, where introducing bounded group sizes provides more fine-grained insights into the behavior of strategic agents. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00261</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00261</id><created>2025-01-31</created><authors><author><keyname>Feng</keyname><forenames>Shengyu</forenames></author><author><keyname>Kim</keyname><forenames>Jaehyung</forenames></author><author><keyname>Yang</keyname><forenames>Yiming</forenames></author><author><keyname>Boudreau</keyname><forenames>Joseph</forenames></author><author><keyname>Chowdhury</keyname><forenames>Tasnuva</forenames></author><author><keyname>Hoisie</keyname><forenames>Adolfy</forenames></author><author><keyname>Khan</keyname><forenames>Raees</forenames></author><author><keyname>Kilic</keyname><forenames>Ozgur O.</forenames></author><author><keyname>Klasky</keyname><forenames>Scott</forenames></author><author><keyname>Korchuganova</keyname><forenames>Tatiana</forenames></author><author><keyname>Nilsson</keyname><forenames>Paul</forenames></author><author><keyname>Outschoorn</keyname><forenames>Verena Ingrid Martinez</forenames></author><author><keyname>Park</keyname><forenames>David K.</forenames></author><author><keyname>Podhorszki</keyname><forenames>Norbert</forenames></author><author><keyname>Ren</keyname><forenames>Yihui</forenames></author><author><keyname>Suter</keyname><forenames>Frederic</forenames></author><author><keyname>Vatsavai</keyname><forenames>Sairam Sri</forenames></author><author><keyname>Yang</keyname><forenames>Wei</forenames></author><author><keyname>Yoo</keyname><forenames>Shinjae</forenames></author><author><keyname>Maeno</keyname><forenames>Tadashi</forenames></author><author><keyname>Klimentov</keyname><forenames>Alexei</forenames></author></authors><title>Alternative Mixed Integer Linear Programming Optimization for Joint Job   Scheduling and Data Allocation in Grid Computing</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach to the joint optimization of job scheduling and data allocation in grid computing environments. We formulate this joint optimization problem as a mixed integer quadratically constrained program. To tackle the nonlinearity in the constraint, we alternatively fix a subset of decision variables and optimize the remaining ones via Mixed Integer Linear Programming (MILP). We solve the MILP problem at each iteration via an off-the-shelf MILP solver. Our experimental results show that our method significantly outperforms existing heuristic methods, employing either independent optimization or joint optimization strategies. We have also verified the generalization ability of our method over grid environments with various sizes and its high robustness to the algorithm hyper-parameters. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00262</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00262</id><created>2025-01-31</created><authors><author><keyname>Chen</keyname><forenames>Dianwei</forenames></author><author><keyname>Zhang</keyname><forenames>Zifan</forenames></author><author><keyname>Liu</keyname><forenames>Yuchen</forenames></author><author><keyname>Yang</keyname><forenames>Xianfeng Terry</forenames></author></authors><title>Your submission contained main.bib and main.tex file, but no main.bbl   file (include main.bbl, or submit without main.bib; and remember to verify   references)</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous driving systems face significant challenges in handling unpredictable edge-case scenarios, such as adversarial pedestrian movements, dangerous vehicle maneuvers, and sudden environmental changes. Current end-to-end driving models struggle with generalization to these rare events due to limitations in traditional detection and prediction approaches. To address this, we propose INSIGHT (Integration of Semantic and Visual Inputs for Generalized Hazard Tracking), a hierarchical vision-language model (VLM) framework designed to enhance hazard detection and edge-case evaluation. By using multimodal data fusion, our approach integrates semantic and visual representations, enabling precise interpretation of driving scenarios and accurate forecasting of potential dangers. Through supervised fine-tuning of VLMs, we optimize spatial hazard localization using attention-based mechanisms and coordinate regression techniques. Experimental results on the BDD100K dataset demonstrate a substantial improvement in hazard prediction straightforwardness and accuracy over existing models, achieving a notable increase in generalization performance. This advancement enhances the robustness and safety of autonomous driving systems, ensuring improved situational awareness and potential decision-making in complex real-world scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00264</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00264</id><created>2025-01-31</created><authors><author><keyname>Zhang</keyname><forenames>Binchi</forenames></author><author><keyname>Zheng</keyname><forenames>Zaiyi</forenames></author><author><keyname>Chen</keyname><forenames>Zhengzhang</forenames></author><author><keyname>Li</keyname><forenames>Jundong</forenames></author></authors><title>Beyond the Permutation Symmetry of Transformers: The Role of Rotation   for Model Fusion</title><categories>cs.LG cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Symmetry in the parameter space of deep neural networks (DNNs) has proven beneficial for various deep learning applications. A well-known example is the permutation symmetry in Multi-Layer Perceptrons (MLPs), where permuting the rows of weight matrices in one layer and applying the inverse permutation to adjacent layers yields a functionally equivalent model. While permutation symmetry fully characterizes the equivalence set for MLPs, its discrete nature limits its utility for transformers. In this paper, we introduce rotation symmetry, a novel form of parameter space symmetry for transformers that generalizes permutation symmetry by rotating parameter matrices in self-attention layers. Unlike permutation symmetry, rotation symmetry operates in a continuous domain, thereby significantly expanding the equivalence set for transformers. Based on this property, we propose a theoretically optimal parameter matching algorithm as a plug-and-play module to enhance model fusion. We evaluate our approach using pre-trained transformers across diverse natural language and vision tasks. Experimental results demonstrate that our rotation symmetry-based matching algorithm substantially improves model fusion, highlighting the potential of parameter space symmetry to facilitate model fusion. Our code is available on https://github.com/zhengzaiyi/RotationSymmetry. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00265</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00265</id><created>2025-01-31</created><authors><author><keyname>Martinez-Romero</keyname><forenames>Marcos</forenames></author><author><keyname>Horridge</keyname><forenames>Matthew</forenames></author><author><keyname>Mistry</keyname><forenames>Nilesh</forenames></author><author><keyname>Weyhmiller</keyname><forenames>Aubrie</forenames></author><author><keyname>Yu</keyname><forenames>Jimmy K.</forenames></author><author><keyname>Fujimoto</keyname><forenames>Alissa</forenames></author><author><keyname>Henry</keyname><forenames>Aria</forenames></author><author><keyname>O'Connor</keyname><forenames>Martin J.</forenames></author><author><keyname>Sier</keyname><forenames>Ashley</forenames></author><author><keyname>Suber</keyname><forenames>Stephanie</forenames></author><author><keyname>Akdogan</keyname><forenames>Mete U.</forenames></author><author><keyname>Cao</keyname><forenames>Yan</forenames></author><author><keyname>Valliappan</keyname><forenames>Somu</forenames></author><author><keyname>Mieczkowska</keyname><forenames>Joanna O.</forenames></author><author><keyname>team</keyname><forenames>the RADx Data Hub</forenames></author><author><keyname>Krishnamurthy</keyname><forenames>Ashok</forenames></author><author><keyname>Keller</keyname><forenames>Michael A.</forenames></author><author><keyname>Musen</keyname><forenames>Mark A.</forenames></author></authors><title>RADx Data Hub: A Cloud Repository for FAIR, Harmonized COVID-19 Data</title><categories>cs.DB</categories><comments>This paper is currently under consideration for publication in the   journal Scientific Data</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The COVID-19 pandemic highlighted the urgent need for robust systems to enable rapid data collection, integration, and analysis for public health responses. Existing approaches often relied on disparate, non-interoperable systems, creating bottlenecks in comprehensive analyses and timely decision-making. To address these challenges, the U.S. National Institutes of Health (NIH) launched the Rapid Acceleration of Diagnostics (RADx) initiative in 2020, with the RADx Data Hub, a centralized repository for de-identified and curated COVID-19 data, as its cornerstone. The RADx Data Hub hosts diverse study data, including clinical data, testing results, smart sensor outputs, self-reported symptoms, and information on social determinants of health. Built on cloud infrastructure, the RADx Data Hub integrates metadata standards, interoperable formats, and ontology-based tools to adhere to the FAIR (Findable, Accessible, Interoperable, Reusable) principles for data sharing. Initially developed for COVID-19 research, its architecture and processes are adaptable to other scientific disciplines. This paper provides an overview of the data hosted by the RADx Data Hub and describes the platform's capabilities and architecture. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00266</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00266</id><created>2025-01-31</created><authors><author><keyname>Sun</keyname><forenames>Yuwei</forenames></author><author><keyname>Mi</keyname><forenames>Lu</forenames></author><author><keyname>Fujisawa</keyname><forenames>Ippei</forenames></author><author><keyname>Kanai</keyname><forenames>Ryota</forenames></author></authors><title>MCM: Multi-layer Concept Map for Efficient Concept Learning from Masked   Images</title><categories>cs.CV cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Masking strategies commonly employed in natural language processing are still underexplored in vision tasks such as concept learning, where conventional methods typically rely on full images. However, using masked images diversifies perceptual inputs, potentially offering significant advantages in concept learning with large-scale Transformer models. To this end, we propose Multi-layer Concept Map (MCM), the first work to devise an efficient concept learning method based on masked images. In particular, we introduce an asymmetric concept learning architecture by establishing correlations between different encoder and decoder layers, updating concept tokens using backward gradients from reconstruction tasks. The learned concept tokens at various levels of granularity help either reconstruct the masked image patches by filling in gaps or guide the reconstruction results in a direction that reflects specific concepts. Moreover, we present both quantitative and qualitative results across a wide range of metrics, demonstrating that MCM significantly reduces computational costs by training on fewer than 75% of the total image patches while enhancing concept prediction performance. Additionally, editing specific concept tokens in the latent space enables targeted image generation from masked images, aligning both the visible contextual patches and the provided concepts. By further adjusting the testing time mask ratio, we could produce a range of reconstructions that blend the visible patches with the provided concepts, proportional to the chosen ratios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00268</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00268</id><created>2025-01-31</created><authors><author><keyname>Lim</keyname><forenames>Chungman</forenames></author><author><keyname>Kim</keyname><forenames>Gyeongdeok</forenames></author><author><keyname>Kang</keyname><forenames>Su-Yeon</forenames></author><author><keyname>Seifi</keyname><forenames>Hasti</forenames></author><author><keyname>Park</keyname><forenames>Gunhyuk</forenames></author></authors><title>Can a Machine Feel Vibrations?: A Framework for Vibrotactile Sensation   and Emotion Prediction via a Neural Network</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vibrotactile signals offer new possibilities for conveying sensations and emotions in various applications. Yet, designing vibrotactile tactile icons (i.e., Tactons) to evoke specific feelings often requires a trial-and-error process and user studies. To support haptic design, we propose a framework for predicting sensory and emotional ratings from vibration signals. We created 154 Tactons and conducted a study to collect acceleration data from smartphones and roughness, valence, and arousal user ratings (n=36). We converted the Tacton signals into two-channel spectrograms reflecting the spectral sensitivities of mechanoreceptors, then input them into VibNet, our dual-stream neural network. The first stream captures sequential features using recurrent networks, while the second captures temporal-spectral features using 2D convolutional networks. VibNet outperformed baseline models, with 82% of its predictions falling within the standard deviations of ground truth user ratings for two new Tacton sets. We discuss the efficacy of our mechanoreceptive processing and dual-stream neural network and present future research directions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00270</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00270</id><created>2025-01-31</created><authors><author><keyname>Chen</keyname><forenames>Zhiliang</forenames></author><author><keyname>Lau</keyname><forenames>Gregory Kang Ruey</forenames></author><author><keyname>Foo</keyname><forenames>Chuan-Sheng</forenames></author><author><keyname>Low</keyname><forenames>Bryan Kian Hsiang</forenames></author></authors><title>DUET: Optimizing Training Data Mixtures via Feedback from Unseen   Evaluation Tasks</title><categories>cs.LG cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The performance of a machine learning (ML) model depends heavily on the relevance of its training data to the domain of the downstream evaluation task. However, in practice, the data involved in an unseen evaluation task is often not known to us (e.g., conversations between an LLM and a user are end-to-end encrypted). So, it is not obvious what data would be relevant for training/fine-tuning the ML model to maximize its task performance. Instead, one can only deploy the ML model in the unseen evaluation task to gather multiple rounds of coarse feedback on how well the model has performed. This paper presents a novel global-to-local algorithm called DUET that can exploit the feedback loop by interleaving a data selection method with Bayesian optimization. As a result, DUET can efficiently refine the training data mixture from a pool of data domains to maximize the model's performance on the unseen evaluation task and its convergence to the optimal data mixture can be theoretically guaranteed by analyzing its cumulative regret. Empirical evaluation on image and LLM evaluation tasks shows that DUET finds better training data mixtures than conventional baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00271</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00271</id><created>2025-01-31</created><authors><author><keyname>Yu</keyname><forenames>Fei</forenames></author><author><keyname>Li</keyname><forenames>Yingru</forenames></author><author><keyname>Wang</keyname><forenames>Benyou</forenames></author></authors><title>Scaling Flaws of Verifier-Guided Search in Mathematical Reasoning</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models (LLMs) struggle with multi-step reasoning, where inference-time scaling has emerged as a promising strategy for performance improvement. Verifier-guided search outperforms repeated sampling when sample size is limited by selecting and prioritizing valid reasoning paths. However, we identify a critical limitation: scaling flaws, prevalent across different models (Mistral 7B and DeepSeekMath 7B), benchmarks (GSM8K and MATH), and verifiers (outcome value models and process reward models). As sample size increases, verifier-guided search exhibits diminishing advantages and eventually underperforms repeated sampling. Our analysis attributes this to verifier failures, where imperfect verifiers misrank candidates and erroneously prune all valid paths. These issues are further exacerbated in challenging and out-of-distribution problems, restricting search effectiveness. To mitigate verifier failures, we explore reducing reliance on verifiers and conduct preliminary investigations using two simple methods. Our findings reveal fundamental limitations in verifier-guided search and suggest future directions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00274</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00274</id><created>2025-01-31</created><authors><author><keyname>Moltafet</keyname><forenames>Mohammad</forenames></author><author><keyname>Sadjadpour</keyname><forenames>Hamid R.</forenames></author><author><keyname>Rezki</keyname><forenames>Zouheir</forenames></author><author><keyname>Codreanu</keyname><forenames>Marian</forenames></author><author><keyname>Yates</keyname><forenames>Roy D.</forenames></author></authors><title>AoI in M/G/1/1 Queues with Probabilistic Preemption</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a status update system consisting of one source, one server, and one sink. The source generates packets according to a Poisson process and the packets are served according to a generally distributed service time. We consider a system with a capacity of one packet, i.e., there is no waiting buffer in the system, and model it as an M/G/1/1 queueing system. We introduce a probabilistically preemptive packet management policy and calculate the moment generating functions (MGFs) of the age of information (AoI) and peak AoI (PAoI) under the policy. According to the probabilistically preemptive policy, when a packet arrives, the possible packet in the system is replaced by the arriving packet with a fixed probability. Numerical results show the effectiveness of the packet management policy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00275</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00275</id><created>2025-01-31</created><authors><author><keyname>Bimbraw</keyname><forenames>Keshav</forenames></author><author><keyname>Nekkanti</keyname><forenames>Srikar</forenames></author><author><keyname>Tiller</keyname><forenames>Daniel B.</forenames><suffix>II</suffix></author><author><keyname>Deshmukh</keyname><forenames>Mihir</forenames></author><author><keyname>Calli</keyname><forenames>Berk</forenames></author><author><keyname>Howe</keyname><forenames>Robert D.</forenames></author><author><keyname>Zhang</keyname><forenames>Haichong K.</forenames></author></authors><title>Simultaneous Estimation of Manipulation Skill and Hand Grasp Force from   Forearm Ultrasound Images</title><categories>cs.RO cs.CV cs.ET cs.HC</categories><comments>30 pages, 52 references, 10 figures, 8 tables and 2 supplementary   videos. Currently under review</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Accurate estimation of human hand configuration and the forces they exert is critical for effective teleoperation and skill transfer in robotic manipulation. A deeper understanding of human interactions with objects can further enhance teleoperation performance. To address this need, researchers have explored methods to capture and translate human manipulation skills and applied forces to robotic systems. Among these, biosignal-based approaches, particularly those using forearm ultrasound data, have shown significant potential for estimating hand movements and finger forces. In this study, we present a method for simultaneously estimating manipulation skills and applied hand force using forearm ultrasound data. Data collected from seven participants were used to train deep learning models for classifying manipulation skills and estimating grasp force. Our models achieved an average classification accuracy of 94.87 percent plus or minus 10.16 percent for manipulation skills and an average root mean square error (RMSE) of 0.51 plus or minus 0.19 Newtons for force estimation, as evaluated using five-fold cross-validation. These results highlight the effectiveness of forearm ultrasound in advancing human-machine interfacing and robotic teleoperation for complex manipulation tasks. This work enables new and effective possibilities for human-robot skill transfer and tele-manipulation, bridging the gap between human dexterity and robotic control. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00277</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00277</id><created>2025-01-31</created><authors><author><keyname>Feng</keyname><forenames>Shengyu</forenames></author><author><keyname>Yang</keyname><forenames>Yiming</forenames></author></authors><title>Regularized Langevin Dynamics for Combinatorial Optimization</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a simple yet effective sampling framework for combinatorial optimization (CO). Our method builds on discrete Langevin dynamics (LD), an efficient gradient-guided generative algorithm. However, we observed that directly applying LD often leads to limited exploration. To overcome this limitation, we propose the Regularized Langevin Dynamics (RLD), which enforces an expected distance between the sampled and current solutions, effectively avoiding local minima. We develop two CO solvers on top of RLD, one based on simulated annealing (SA) and the other one based on neural network (NN). Empirical results on three classical CO problems demonstrate that both of our methods can achieve comparable or better performance against the previous state-of-the-art (SOTA) SA and NN-based solvers. In particular, our SA algorithm reduces the running time of the previous SOTA SA method by up to 80\%, while achieving equal or superior performance. In summary, RLD offers a promising framework for enhancing both traditional heuristics and NN models to solve CO problems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00279</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00279</id><created>2025-01-31</created><authors><author><keyname>Pham</keyname><forenames>Khiem</forenames></author><author><keyname>Herrmann</keyname><forenames>Charles</forenames></author><author><keyname>Zabih</keyname><forenames>Ramin</forenames></author></authors><title>Improving realistic semi-supervised learning with doubly robust   estimation</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  A major challenge in Semi-Supervised Learning (SSL) is the limited information available about the class distribution in the unlabeled data. In many real-world applications this arises from the prevalence of long-tailed distributions, where the standard pseudo-label approach to SSL is biased towards the labeled class distribution and thus performs poorly on unlabeled data. Existing methods typically assume that the unlabeled class distribution is either known a priori, which is unrealistic in most situations, or estimate it on-the-fly using the pseudo-labels themselves. We propose to explicitly estimate the unlabeled class distribution, which is a finite-dimensional parameter, \emph{as an initial step}, using a doubly robust estimator with a strong theoretical guarantee; this estimate can then be integrated into existing methods to pseudo-label the unlabeled data during training more accurately. Experimental results demonstrate that incorporating our techniques into common pseudo-labeling approaches improves their performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00280</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00280</id><created>2025-01-31</created><authors><author><keyname>Meshir</keyname><forenames>Juan Daniel</forenames></author><author><keyname>Palafox</keyname><forenames>Abel</forenames></author><author><keyname>Guerrero</keyname><forenames>Edgar Alejandro</forenames></author></authors><title>On the study of frequency control and spectral bias in Wavelet-Based   Kolmogorov Arnold networks: A path to physics-informed KANs</title><categories>cs.LG cs.NA math.NA</categories><comments>29 pages, 13 figures</comments><msc-class>68T07, 68T20, 65L05, 65M22</msc-class><acm-class>I.2.0; I.2.6; G.1.7; G.1.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral bias, the tendency of neural networks to prioritize learning low-frequency components of functions during the initial training stages, poses a significant challenge when approximating solutions with high-frequency details. This issue is particularly pronounced in physics-informed neural networks (PINNs), widely used to solve differential equations that describe physical phenomena. In the literature, contributions such as Wavelet Kolmogorov Arnold Networks (Wav-KANs) have demonstrated promising results in capturing both low- and high-frequency components. Similarly, Fourier features (FF) are often employed to address this challenge. However, the theoretical foundations of Wav-KANs, particularly the relationship between the frequency of the mother wavelet and spectral bias, remain underexplored. A more in-depth understanding of how Wav-KANs manage high-frequency terms could offer valuable insights for addressing oscillatory phenomena encountered in parabolic, elliptic, and hyperbolic differential equations. In this work, we analyze the eigenvalues of the neural tangent kernel (NTK) of Wav-KANs to enhance their ability to converge on high-frequency components, effectively mitigating spectral bias. Our theoretical findings are validated through numerical experiments, where we also discuss the limitations of traditional approaches, such as standard PINNs and Fourier features, in addressing multi-frequency problems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00281</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00281</id><created>2025-01-31</created><authors><author><keyname>Yan</keyname><forenames>Fanqi</forenames></author><author><keyname>Nguyen</keyname><forenames>Huy</forenames></author><author><keyname>Akbarian</keyname><forenames>Pedram</forenames></author><author><keyname>Ho</keyname><forenames>Nhat</forenames></author><author><keyname>Rinaldo</keyname><forenames>Alessandro</forenames></author></authors><title>Sigmoid Self-Attention is Better than Softmax Self-Attention: A   Mixture-of-Experts Perspective</title><categories>cs.LG cs.AI</categories><comments>Fanqi Yan, Huy Nguyen contributed equally to this work. 51 pages, 2   figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At the core of the popular Transformer architecture is the self-attention mechanism, which dynamically assigns softmax weights to each input token so that the model can focus on the most salient information. However, the softmax structure slows down the attention computation due to its row-wise nature, and inherently introduces competition among tokens: as the weight assigned to one token increases, the weights of others decrease. This competitive dynamic may narrow the focus of self-attention to a limited set of features, potentially overlooking other informative characteristics. Recent experimental studies have shown that using the element-wise sigmoid function helps eliminate token competition and reduce the computational overhead. Despite these promising empirical results, a rigorous comparison between sigmoid and softmax self-attention mechanisms remains absent in the literature. This paper closes this gap by theoretically demonstrating that sigmoid self-attention is more sample-efficient than its softmax counterpart. Toward that goal, we illustrate that each row of the self-attention matrix can be represented as a mixture of experts. Our analysis shows that ''experts'' in sigmoid self-attention require significantly less data to achieve the same approximation error as those in softmax self-attention. We corroborate our theoretical findings through extensive experiments on both synthetic and real-world datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00282</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00282</id><created>2025-01-31</created><authors><author><keyname>Ahamed</keyname><forenames>Md Atik</forenames></author><author><keyname>Cheng</keyname><forenames>Andrew</forenames></author><author><keyname>Ye</keyname><forenames>Qiang</forenames></author><author><keyname>Cheng</keyname><forenames>Qiang</forenames></author></authors><title>GraphMinNet: Learning Dependencies in Graphs with Light Complexity   Minimal Architecture</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph Neural Networks (GNNs) have demonstrated remarkable success in various applications, yet they often struggle to capture long-range dependencies (LRD) effectively. This paper introduces GraphMinNet, a novel GNN architecture that generalizes the idea of minimal Gated Recurrent Units to graph-structured data. Our approach achieves efficient LRD modeling with linear computational complexity while maintaining permutation equivariance and stability. The model incorporates both structural and positional information through a unique combination of feature and positional encodings, leading to provably stronger expressiveness than the 1-WL test. Theoretical analysis establishes that GraphMinNet maintains non-decaying gradients over long distances, ensuring effective long-range information propagation. Extensive experiments on ten diverse datasets, including molecular graphs, image graphs, and synthetic networks, demonstrate that GraphMinNet achieves state-of-the-art performance while being computationally efficient. Our results show superior performance on 6 out of 10 datasets and competitive results on the others, validating the effectiveness of our approach in capturing both local and global graph structures. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00283</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00283</id><created>2025-01-31</created><authors><author><keyname>Chen</keyname><forenames>Liuging</forenames></author><author><keyname>Song</keyname><forenames>Yaxuan</forenames></author><author><keyname>Guo</keyname><forenames>Jia</forenames></author><author><keyname>Sun</keyname><forenames>Lingyun</forenames></author><author><keyname>Childs</keyname><forenames>Peter</forenames></author><author><keyname>Yin</keyname><forenames>Yuan</forenames></author></authors><title>How Generative AI supports human in conceptual design</title><categories>cs.HC</categories><comments>20 pages, 2 figures, accepted by Design Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative Artificial Intelligence (Generative AI) is a collection of AI technologies that can generate new information such as texts and images. With its strong capabilities, Generative AI has been actively studied in creative design processes. However, limited studies have explored the roles of humans and Generative AI in conceptual design processes, leaving a gap for human-AI collaboration investigation. To address this gap, this study uncovers the contributions of different Generative AI technologies in assisting humans in the conceptual design process. Novice designers completed two design tasks with or without the assistance of Generative AI. Results revealed that Generative AI primarily assists humans in problem definition and idea generation stages, while idea selection and evaluation remain predominantly human-led. Additionally, with Generative AI assistance, the idea selection and evaluation stages were further enhanced. Based on the findings, we discuss the role of Generative AI in human-AI collaboration and implications for enhancing future conceptual design support with Generative AI assistance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00284</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00284</id><created>2025-01-31</created><authors><author><keyname>Li</keyname><forenames>Grace Jingying</forenames></author><author><keyname>Luo</keyname><forenames>Jiajie</forenames></author><author><keyname>Chu</keyname><forenames>Weiqi</forenames></author></authors><title>Bounded-Confidence Models of Multi-Dimensional Opinions with   Topic-Weighted Discordance</title><categories>physics.soc-ph cs.SI math.DS</categories><comments>40 pages, 17 figures</comments><msc-class>91D30, 05C82, 37H05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People's opinions on a wide range of topics often evolve over time through their interactions with others. Models of opinion dynamics primarily focus on one-dimensional opinions which represent opinions on one topic. However, opinions on various topics are rarely isolated; instead, they can be interdependent and exhibit correlations. In a bounded-confidence model (BCM) of opinion dynamics, agents influence each other's opinions only if their opinions are sufficiently similar. We extend classical agent-based BCMs -- namely, the Hegeselmann--Krause BCM, which has synchronous interactions, and the Deffuant--Weisbuch BCM, which has asynchronous interactions -- to a multidimensional setting, in which the opinions are multidimensional vectors representing opinions of different topics and opinions on different topics are interdependent. To measure opinion differences between agents, we introduce topic-weighted discordance functions that account for opinion differences in all topics. We use the regions of receptiveness to characterize the steady-state opinion clusters and provide an analytical approach to compute these regions. In addition, we numerically simulate our models on various networks with initial opinions drawn from a variety of distributions. When initial opinions are correlated across different topics, our topic-weighted BCMs yield significantly different results in both transient and steady states compared to baseline models, where the dynamics of each opinion topic are independent. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00285</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00285</id><created>2025-01-31</created><authors><author><keyname>Chang</keyname><forenames>Yanchuan</forenames></author><author><keyname>Cai</keyname><forenames>Xu</forenames></author><author><keyname>Jensen</keyname><forenames>Christian S.</forenames></author><author><keyname>Qi</keyname><forenames>Jianzhong</forenames></author></authors><title>K Nearest Neighbor-Guided Trajectory Similarity Learning</title><categories>cs.LG cs.CV cs.DB</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Trajectory similarity is fundamental to many spatio-temporal data mining applications. Recent studies propose deep learning models to approximate conventional trajectory similarity measures, exploiting their fast inference time once trained. Although efficient inference has been reported, challenges remain in similarity approximation accuracy due to difficulties in trajectory granularity modeling and in exploiting similarity signals in the training data. To fill this gap, we propose TSMini, a highly effective trajectory similarity model with a sub-view modeling mechanism capable of learning multi-granularity trajectory patterns and a k nearest neighbor-based loss that guides TSMini to learn not only absolute similarity values between trajectories but also their relative similarity ranks. Together, these two innovations enable highly accurate trajectory similarity approximation. Experiments show that TSMini can outperform the state-of-the-art models by 22% in accuracy on average when learning trajectory similarity measures. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00286</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00286</id><created>2025-01-31</created><authors><author><keyname>Panteleaki</keyname><forenames>Aikaterini Maria</forenames></author><author><keyname>Balaskas</keyname><forenames>Konstantinos</forenames></author><author><keyname>Zervakis</keyname><forenames>Georgios</forenames></author><author><keyname>Amrouch</keyname><forenames>Hussam</forenames></author><author><keyname>Anagnostopoulos</keyname><forenames>Iraklis</forenames></author></authors><title>Late Breaking Results: Leveraging Approximate Computing for Carbon-Aware   DNN Accelerators</title><categories>cs.AR</categories><comments>Accepted in DATE 2025 - Late Breaking Results</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The rapid growth of Machine Learning (ML) has increased demand for DNN hardware accelerators, but their embodied carbon footprint poses significant environmental challenges. This paper leverages approximate computing to design sustainable accelerators by minimizing the Carbon Delay Product (CDP). Using gate-level pruning and precision scaling, we generate area-aware approximate multipliers and optimize the accelerator design with a genetic algorithm. Results demonstrate reduced embodied carbon while meeting performance and accuracy requirements. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00288</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00288</id><created>2025-01-31</created><authors><author><keyname>Liu</keyname><forenames>Jijia</forenames></author><author><keyname>Gao</keyname><forenames>Feng</forenames></author><author><keyname>Liao</keyname><forenames>Qingmin</forenames></author><author><keyname>Yu</keyname><forenames>Chao</forenames></author><author><keyname>Wang</keyname><forenames>Yu</forenames></author></authors><title>Learning from Suboptimal Data in Continuous Control via Auto-Regressive   Soft Q-Network</title><categories>cs.LG cs.RO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Reinforcement learning (RL) for continuous control often requires large amounts of online interaction data. Value-based RL methods can mitigate this burden by offering relatively high sample efficiency. Some studies further enhance sample efficiency by incorporating offline demonstration data to "kick-start" training, achieving promising results in continuous control. However, they typically compute the Q-function independently for each action dimension, neglecting interdependencies and making it harder to identify optimal actions when learning from suboptimal data, such as non-expert demonstration and online-collected data during the training process. To address these issues, we propose Auto-Regressive Soft Q-learning (ARSQ), a value-based RL algorithm that models Q-values in a coarse-to-fine, auto-regressive manner. First, ARSQ decomposes the continuous action space into discrete spaces in a coarse-to-fine hierarchy, enhancing sample efficiency for fine-grained continuous control tasks. Next, it auto-regressively predicts dimensional action advantages within each decision step, enabling more effective decision-making in continuous control tasks. We evaluate ARSQ on two continuous control benchmarks, RLBench and D4RL, integrating demonstration data into online training. On D4RL, which includes non-expert demonstrations, ARSQ achieves an average $1.62\times$ performance improvement over SOTA value-based baseline. On RLBench, which incorporates expert demonstrations, ARSQ surpasses various baselines, demonstrating its effectiveness in learning from suboptimal online-collected data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00289</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00289</id><created>2025-01-31</created><authors><author><keyname>Mukherjee</keyname><forenames>Anirban</forenames></author><author><keyname>Chang</keyname><forenames>Hannah Hanwen</forenames></author></authors><title>Agentic AI: Expanding the Algorithmic Frontier of Creative Problem   Solving</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Agentic Artificial Intelligence (AI) systems are capable of autonomously pursuing goals, making decisions, and taking actions over extended periods. Unlike traditional generative AI, which responds reactively to prompts, agentic AI proactively orchestrates complex workflows--as exemplified by travel-planning agents that autonomously book flights, negotiate hotel rates, curate brand-aligned experiences, and adapt to real-time disruptions. We posit that this transition from advisory roles to proactive execution challenges existing legal, economic, and marketing frameworks. We highlight gaps in liability attribution, intellectual property ownership, and informed consent when agentic AI systems enter into binding contracts or generate novel solutions. Central to this analysis is the tension between novelty and practicality: although agentic AI can craft unconventional and highly original experiences, these outputs may conflict with user preferences or logistical constraints. Furthermore, algorithmic coordination among AI systems risks distorting competitive dynamics through tacit collusion or market concentration, particularly if diverse AI systems converge on similar solutions due to shared underlying data or optimization logic. Addressing these challenges will necessitate interdisciplinary collaboration to redefine legal accountability, align AI-driven choices with consumer values, and maintain ethical safeguards. We advocate for frameworks that balance autonomy with accountability, ensuring stakeholders can harness agentic AI's potential while preserving trust, fairness, and societal welfare in an increasingly automated ecosystem. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00290</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00290</id><created>2025-01-31</created><authors><author><keyname>Ma</keyname><forenames>Huan</forenames></author><author><keyname>Chen</keyname><forenames>Jingdong</forenames></author><author><keyname>Wang</keyname><forenames>Guangyu</forenames></author><author><keyname>Zhang</keyname><forenames>Changqing</forenames></author></authors><title>Estimating LLM Uncertainty with Logits</title><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In recent years, Large Language Models (LLMs) have seen remarkable advancements and have been extensively integrated across various fields. Despite their progress, LLMs are prone to hallucinations, producing responses that may not be dependable if the models lack sufficient grounding knowledge. To mitigate this issue, methods for estimating uncertainty have been adopted, with a focus on critical tokens as indicators of reliability. Nevertheless, probability-based approaches have shown limitations in assessing token-level reliability due to the erosion of evidence strength information acquired during training. In this paper, we introduce Logits-induced Token Uncertainty (LogU), a novel framework designed to estimate token-specific uncertainty in LLMs in real time, without the need for multiple sampling rounds. By leveraging evidence modeling for the implementation of LogU, we utilize the derived uncertainty measures to steer downstream tasks. Our experimental findings highlight the substantial effectiveness and potential of LogU, marking a significant advancement in addressing the challenge of model hallucinations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00294</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00294</id><created>2025-01-31</created><authors><author><keyname>Abin</keyname><forenames>Hamidreza</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author></authors><title>On the Source Model Key Agreement Problem</title><categories>cs.IT math.IT</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the source model key agreement problem involving two legitimate parties and an eavesdropper who observe n i.i.d. samples of X and Y and Z respectively. The best-known upper bound on the key capacity is characterized by an inf-max optimization problem that generally lacks a closed-form solution. In this paper, we solve the optimization for some class of sources, thereby providing simple expressions for the upper bound. We provide general conditions under which the upper bound reduces to I(X;Y). As an example, we consider the XOR setting in which X and Y are binary, and Z is the XOR of X and Y . The upper bound reduces to I(X;Y) for this source. Next, we conjecture that the rate I(X;Y) is not achievable for the XOR source, and provide some ideas that might be useful for developing a new upper bound on the source model problem. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00295</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00295</id><created>2025-01-31</created><authors><author><keyname>Yang</keyname><forenames>Qiaoyu</forenames></author><author><keyname>Zhang</keyname><forenames>Shuo</forenames></author><author><keyname>Huang</keyname><forenames>Chuan-Che</forenames></author></authors><title>Toward noise-robust whisper keyword spotting on headphones with   in-earcup microphone and curriculum learning</title><categories>eess.AS cs.SD</categories><comments>Accepted to ICASSP 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The expanding feature set of modern headphones puts a challenge on the design of their control interface. Users may want to separately control each feature or quickly switch between modes that activate different features. Traditional approach of physical buttons may no longer be feasible when the feature set is large. Keyword spotting with voice commands is a promising solution to the issue. Most existing methods of keyword spotting only support commands spoken in a regular voice. However, regular voice may not be desirable in quiet places or public settings. In this paper, we investigate the problem of on-device keyword spotting in whisper voice and explore approaches to improve noise robustness. We leverage the inner microphone on noise-cancellation headphones as an additional source of voice input. We also design a curriculum learning strategy that gradually increases the proportion of whisper keywords during training. We demonstrate through experiments that the combination of multi-microphone processing and curriculum learning could improve F1 score of whisper keyword spotting by up to 15% in noisy conditions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00298</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00298</id><created>2025-01-31</created><authors><author><keyname>Moreno</keyname><forenames>Alexander</forenames></author><author><keyname>Xiao</keyname><forenames>Justin</forenames></author><author><keyname>Mei</keyname><forenames>Jonathan</forenames></author></authors><title>The Price of Linear Time: Error Analysis of Structured Kernel   Interpolation</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Structured Kernel Interpolation (SKI) (Wilson et al. 2015) helps scale Gaussian Processes (GPs) by approximating the kernel matrix via interpolation at inducing points, achieving linear computational complexity. However, it lacks rigorous theoretical error analysis. This paper bridges the gap: we prove error bounds for the SKI Gram matrix and examine the error's effect on hyperparameter estimation and posterior inference. We further provide a practical guide to selecting the number of inducing points under convolutional cubic interpolation: they should grow as $n^{d/3}$ for error control. Crucially, we identify two dimensionality regimes governing the trade-off between SKI Gram matrix spectral norm error and computational complexity. For $d \leq 3$, any error tolerance can achieve linear time for sufficiently large sample size. For $d &gt; 3$, the error must increase with sample size to maintain linear time. Our analysis provides key insights into SKI's scalability-accuracy trade-offs, establishing precise conditions for achieving linear-time GP inference with controlled approximation error. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00299</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00299</id><created>2025-01-31</created><authors><author><keyname>Liu</keyname><forenames>Xiang</forenames></author><author><keyname>Tang</keyname><forenames>Zhenheng</forenames></author><author><keyname>Dong</keyname><forenames>Peijie</forenames></author><author><keyname>Li</keyname><forenames>Zeyu</forenames></author><author><keyname>Li</keyname><forenames>Bo</forenames></author><author><keyname>Hu</keyname><forenames>Xuming</forenames></author><author><keyname>Chu</keyname><forenames>Xiaowen</forenames></author></authors><title>ChunkKV: Semantic-Preserving KV Cache Compression for Efficient   Long-Context LLM Inference</title><categories>cs.CL</categories><comments>35 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To reduce memory costs in long-context inference with Large Language Models (LLMs), many recent works focus on compressing the key-value (KV) cache of different tokens. However, we identify that the previous KV cache compression methods measure token importance individually, neglecting the dependency between different tokens in the real-world language characterics. In light of this, we introduce ChunkKV, grouping the tokens in a chunk as a basic compressing unit, and retaining the most informative semantic chunks while discarding the less important ones. Furthermore, observing that ChunkKV exhibits higher similarity in the preserved indices across different layers, we propose layer-wise index reuse to further reduce computational overhead. We evaluated ChunkKV on cutting-edge long-context benchmarks including LongBench and Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context learning benchmark. Our experiments with instruction tuning and multi-step reasoning (O1 and R1) LLMs, achieve up to 10\% performance improvement under aggressive compression ratios compared to existing methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00300</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00300</id><created>2025-01-31</created><authors><author><keyname>Jahan</keyname><forenames>Israt</forenames></author><author><keyname>Schreck</keyname><forenames>John S.</forenames></author><author><keyname>Gagne</keyname><forenames>David John</forenames></author><author><keyname>Becker</keyname><forenames>Charlie</forenames></author><author><keyname>Astitha</keyname><forenames>Marina</forenames></author></authors><title>Uncertainty Quantification of Wind Gust Predictions in the Northeast US:   An Evidential Neural Network and Explainable Artificial Intelligence Approach</title><categories>cs.LG physics.ao-ph stat.ML</categories><comments>Main body 27 pages with 12 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Machine learning has shown promise in reducing bias in numerical weather model predictions of wind gusts. Yet, they underperform to predict high gusts even with additional observations due to the right-skewed distribution of gusts. Uncertainty quantification (UQ) addresses this by identifying when predictions are reliable or needs cautious interpretation. Using data from 61 extratropical storms in the Northeastern USA, we introduce evidential neural network (ENN) as a novel approach for UQ in gust predictions, leveraging atmospheric variables from the Weather Research and Forecasting (WRF) model as features and gust observations as targets. Explainable artificial intelligence (XAI) techniques demonstrated that key predictive features also contributed to higher uncertainty. Estimated uncertainty correlated with storm intensity and spatial gust gradients. ENN allowed constructing gust prediction intervals without requiring an ensemble. From an operational perspective, providing gust forecasts with quantified uncertainty enhances stakeholders' confidence in risk assessment and response planning for extreme gust events. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00301</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00301</id><created>2025-01-31</created><authors><author><keyname>Dombrowski</keyname><forenames>Alistair</forenames></author><author><keyname>Engelhardt</keyname><forenames>Beatrix</forenames></author><author><keyname>Fairbrother</keyname><forenames>Dimitri</forenames></author><author><keyname>Evidail</keyname><forenames>Henry</forenames></author></authors><title>Contextual Morphogenesis in Large Language Models: A Novel Approach to   Self-Organizing Token Representations</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Token representations influence the efficiency and adaptability of language models, yet conventional tokenization strategies impose rigid segmentation boundaries that do not adjust dynamically to evolving contextual relationships. The introduction of contextual morphogenesis establishes a self-organizing mechanism that restructures token boundaries based on learned contextual dependencies, allowing embeddings to evolve progressively across iterative processing steps. Empirical evaluations demonstrate that dynamically adjusted tokenization contributes to reductions in perplexity while maintaining representational stability, particularly in linguistically complex domains where static segmentation fails to capture nuanced dependencies. Computational trade-offs associated with self-organizing token structures indicate that additional processing overhead remains within feasible limits, provided that optimization strategies account for segmentation update efficiency. Comparative assessments across different linguistic corpora suggest that adaptive tokenization preserves interpretability while improving alignment with contextual cues, reinforcing the potential of morphogenetic segmentation mechanisms to refine predictive accuracy. Stability analyses confirm that evolving token structures maintain consistent segmentation behaviors across varied text distributions, ensuring that representational adaptations remain linguistically coherent. The effectiveness of contextual morphogenesis in refining structural stability and predictive performance highlights its viability as an alternative to traditional tokenization methods. Further analysis of computational efficiency considerations suggests that hybrid strategies integrating both static and dynamic segmentation techniques may offer a balanced approach to optimizing representational flexibility while maintaining inference efficiency. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00302</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00302</id><created>2025-01-31</created><authors><author><keyname>He</keyname><forenames>Yixuan</forenames></author><author><keyname>Sandel</keyname><forenames>Aaron</forenames></author><author><keyname>Wipf</keyname><forenames>David</forenames></author><author><keyname>Cucuringu</keyname><forenames>Mihai</forenames></author><author><keyname>Mitani</keyname><forenames>John</forenames></author><author><keyname>Reinert</keyname><forenames>Gesine</forenames></author></authors><title>Learning to Fuse Temporal Proximity Networks: A Case Study in Chimpanzee   Social Interactions</title><categories>stat.ML cs.AI cs.LG math.OC math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  How can we identify groups of primate individuals which could be conjectured to drive social structure? To address this question, one of us has collected a time series of data for social interactions between chimpanzees. Here we use a network representation, leading to the task of combining these data into a time series of a single weighted network per time stamp, where different proximities should be given different weights reflecting their relative importance. We optimize these proximity-type weights in a principled way, using an innovative loss function which rewards structural consistency across time. The approach is empirically validated by carefully designed synthetic data. Using statistical tests, we provide a way of identifying groups of individuals that stay related for a significant length of time. Applying the approach to the chimpanzee data set, we detect cliques in the animal social network time series, which can be validated by real-world intuition from prior research and qualitative observations by chimpanzee experts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00303</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00303</id><created>2025-01-31</created><authors><author><keyname>Roque</keyname><forenames>Emmanuel</forenames></author><author><keyname>Torba</keyname><forenames>Sergii M.</forenames></author></authors><title>Representation of solutions of the one-dimensional Dirac equation in   terms of Neumann series of Bessel functions</title><categories>math.CA cs.NA math-ph math.MP math.NA</categories><comments>21 pages, 1 figure</comments><msc-class>34A25, 34A30, 34A45, 34B30, 34L40, 35L40, 33C45, 65L05, 65L15, 81Q05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A representation of solutions of the one-dimensional Dirac equation is obtained. The solutions are represented as Neumann series of Bessel functions. The representations are shown to be uniformly convergent with respect to the spectral parameter. Explicit formulas for the coefficients are obtained via a system of recursive integrals. The result is based on the Fourier-Legendre series expansion of the transmutation kernel. An efficient numerical method for solving initial-value and spectral problems based on this approach is presented with a numerical example. The method can compute large sets of eigendata with non-deteriorating accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00304</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00304</id><created>2025-01-31</created><authors><author><keyname>Deng</keyname><forenames>Ke</forenames></author><author><keyname>Zhang</keyname><forenames>Hanwen</forenames></author><author><keyname>Lu</keyname><forenames>Jin</forenames></author><author><keyname>Sun</keyname><forenames>Haijian</forenames></author></authors><title>HoP: Homeomorphic Polar Learning for Hard Constrained Optimization</title><categories>cs.LG cs.AI math.OC</categories><comments>in submission</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Constrained optimization demands highly efficient solvers which promotes the development of learn-to-optimize (L2O) approaches. As a data-driven method, L2O leverages neural networks to efficiently produce approximate solutions. However, a significant challenge remains in ensuring both optimality and feasibility of neural networks' output. To tackle this issue, we introduce Homeomorphic Polar Learning (HoP) to solve the star-convex hard-constrained optimization by embedding homeomorphic mapping in neural networks. The bijective structure enables end-to-end training without extra penalty or correction. For performance evaluation, we evaluate HoP's performance across a variety of synthetic optimization tasks and real-world applications in wireless communications. In all cases, HoP achieves solutions closer to the optimum than existing L2O methods while strictly maintaining feasibility. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00305</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00305</id><created>2025-01-31</created><authors><author><keyname>Guo</keyname><forenames>Jiaxin</forenames></author><author><keyname>Chen</keyname><forenames>C. L. Philip</forenames></author><author><keyname>Li</keyname><forenames>Shuzhen</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>DEUCE: Dual-diversity Enhancement and Uncertainty-awareness for   Cold-start Active Learning</title><categories>cs.CL cs.AI cs.IR</categories><comments>18 pages, 3 figures, 12 tables. Accepted manuscript by TACL. For   published version by MIT Press, see   https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00731/125950</comments><acm-class>I.2.6; I.2.7; I.5.1; H.3.1; H.3.3</acm-class><journal-ref>Transactions of the Association for Computational Linguistics,   Vol. 12 (2024), pp. 1736-1754</journal-ref><doi>10.1162/tacl_a_00731</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Cold-start active learning (CSAL) selects valuable instances from an unlabeled dataset for manual annotation. It provides high-quality data at a low annotation cost for label-scarce text classification. However, existing CSAL methods overlook weak classes and hard representative examples, resulting in biased learning. To address these issues, this paper proposes a novel dual-diversity enhancing and uncertainty-aware (DEUCE) framework for CSAL. Specifically, DEUCE leverages a pretrained language model (PLM) to efficiently extract textual representations, class predictions, and predictive uncertainty. Then, it constructs a Dual-Neighbor Graph (DNG) to combine information on both textual diversity and class diversity, ensuring a balanced data distribution. It further propagates uncertainty information via density-based clustering to select hard representative instances. DEUCE performs well in selecting class-balanced and hard representative data by dual-diversity and informativeness. Experiments on six NLP datasets demonstrate the superiority and efficiency of DEUCE. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00306</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00306</id><created>2025-01-31</created><authors><author><keyname>Naseh</keyname><forenames>Ali</forenames></author><author><keyname>Peng</keyname><forenames>Yuefeng</forenames></author><author><keyname>Suri</keyname><forenames>Anshuman</forenames></author><author><keyname>Chaudhari</keyname><forenames>Harsh</forenames></author><author><keyname>Oprea</keyname><forenames>Alina</forenames></author><author><keyname>Houmansadr</keyname><forenames>Amir</forenames></author></authors><title>Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented   Generation</title><categories>cs.CR cs.AI cs.CL cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to generate grounded responses by leveraging external knowledge databases without altering model parameters. Although the absence of weight tuning prevents leakage via model parameters, it introduces the risk of inference adversaries exploiting retrieved documents in the model's context. Existing methods for membership inference and data extraction often rely on jailbreaking or carefully crafted unnatural queries, which can be easily detected or thwarted with query rewriting techniques common in RAG systems. In this work, we present Interrogation Attack (IA), a membership inference technique targeting documents in the RAG datastore. By crafting natural-text queries that are answerable only with the target document's presence, our approach demonstrates successful inference with just 30 queries while remaining stealthy; straightforward detectors identify adversarial prompts from existing methods up to ~76x more frequently than those generated by our attack. We observe a 2x improvement in TPR@1%FPR over prior inference attacks across diverse RAG configurations, all while costing less than $0.02 per document inference. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00307</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00307</id><created>2025-01-31</created><authors><author><keyname>Xia</keyname><forenames>Mengfei</forenames></author><author><keyname>Zhou</keyname><forenames>Yu</forenames></author><author><keyname>Yi</keyname><forenames>Ran</forenames></author><author><keyname>Liu</keyname><forenames>Yong-Jin</forenames></author><author><keyname>Wang</keyname><forenames>Wenping</forenames></author></authors><title>A Diffusion Model Translator for Efficient Image-to-Image Translation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applying diffusion models to image-to-image translation (I2I) has recently received increasing attention due to its practical applications. Previous attempts inject information from the source image into each denoising step for an iterative refinement, thus resulting in a time-consuming implementation. We propose an efficient method that equips a diffusion model with a lightweight translator, dubbed a Diffusion Model Translator (DMT), to accomplish I2I. Specifically, we first offer theoretical justification that in employing the pioneering DDPM work for the I2I task, it is both feasible and sufficient to transfer the distribution from one domain to another only at some intermediate step. We further observe that the translation performance highly depends on the chosen timestep for domain transfer, and therefore propose a practical strategy to automatically select an appropriate timestep for a given task. We evaluate our approach on a range of I2I applications, including image stylization, image colorization, segmentation to image, and sketch to image, to validate its efficacy and general utility. The comparisons show that our DMT surpasses existing methods in both quality and efficiency. Code will be made publicly available. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00309</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00309</id><created>2025-01-31</created><authors><author><keyname>Shi</keyname><forenames>Jianwei</forenames></author><author><keyname>Abdulah</keyname><forenames>Sameh</forenames></author><author><keyname>Sun</keyname><forenames>Ying</forenames></author><author><keyname>Genton</keyname><forenames>Marc G.</forenames></author></authors><title>Decentralized Inference for Distributed Geospatial Data Using Low-Rank   Models</title><categories>stat.ML cs.LG stat.CO stat.ME</categories><comments>84 pages</comments><msc-class>62M30</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Advancements in information technology have enabled the creation of massive spatial datasets, driving the need for scalable and efficient computational methodologies. While offering viable solutions, centralized frameworks are limited by vulnerabilities such as single-point failures and communication bottlenecks. This paper presents a decentralized framework tailored for parameter inference in spatial low-rank models to address these challenges. A key obstacle arises from the spatial dependence among observations, which prevents the log-likelihood from being expressed as a summation-a critical requirement for decentralized optimization approaches. To overcome this challenge, we propose a novel objective function leveraging the evidence lower bound, which facilitates the use of decentralized optimization techniques. Our approach employs a block descent method integrated with multi-consensus and dynamic consensus averaging for effective parameter optimization. We prove the convexity of the new objective function in the vicinity of the true parameters, ensuring the convergence of the proposed method. Additionally, we present the first theoretical results establishing the consistency and asymptotic normality of the estimator within the context of spatial low-rank models. Extensive simulations and real-world data experiments corroborate these theoretical findings, showcasing the robustness and scalability of the framework. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00310</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00310</id><created>2025-01-31</created><authors><author><keyname>Nfissi</keyname><forenames>Alaa</forenames></author><author><keyname>Bouachir</keyname><forenames>Wassim</forenames></author><author><keyname>Bouguila</keyname><forenames>Nizar</forenames></author><author><keyname>Mishara</keyname><forenames>Brian</forenames></author></authors><title>SigWavNet: Learning Multiresolution Signal Wavelet Network for Speech   Emotion Recognition</title><categories>cs.SD cs.AI cs.CL eess.AS</categories><comments>Published in: IEEE Transactions on Affective Computing</comments><acm-class>I.2.7; I.2.6; I.2.1; I.2.0</acm-class><doi>10.1109/TAFFC.2025.3537991</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the field of human-computer interaction and psychological assessment, speech emotion recognition (SER) plays an important role in deciphering emotional states from speech signals. Despite advancements, challenges persist due to system complexity, feature distinctiveness issues, and noise interference. This paper introduces a new end-to-end (E2E) deep learning multi-resolution framework for SER, addressing these limitations by extracting meaningful representations directly from raw waveform speech signals. By leveraging the properties of the fast discrete wavelet transform (FDWT), including the cascade algorithm, conjugate quadrature filter, and coefficient denoising, our approach introduces a learnable model for both wavelet bases and denoising through deep learning techniques. The framework incorporates an activation function for learnable asymmetric hard thresholding of wavelet coefficients. Our approach exploits the capabilities of wavelets for effective localization in both time and frequency domains. We then combine one-dimensional dilated convolutional neural networks (1D dilated CNN) with a spatial attention layer and bidirectional gated recurrent units (Bi-GRU) with a temporal attention layer to efficiently capture the nuanced spatial and temporal characteristics of emotional features. By handling variable-length speech without segmentation and eliminating the need for pre or post-processing, the proposed model outperformed state-of-the-art methods on IEMOCAP and EMO-DB datasets. The source code of this paper is shared on the Github repository: https://github.com/alaaNfissi/SigWavNet-Learning-Multiresolution-Signal-Wavelet-Network-for-Speech-Emotion-Recognition. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00311</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00311</id><created>2025-01-31</created><authors><author><keyname>Yang</keyname><forenames>David H.</forenames></author><author><keyname>Amiri</keyname><forenames>Mohammad Mohammadi</forenames></author><author><keyname>Pedapati</keyname><forenames>Tejaswini</forenames></author><author><keyname>Chaudhury</keyname><forenames>Subhajit</forenames></author><author><keyname>Chen</keyname><forenames>Pin-Yu</forenames></author></authors><title>Sparse Gradient Compression for Fine-Tuning Large Language Models</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fine-tuning large language models (LLMs) for downstream tasks has become increasingly crucial due to their widespread use and the growing availability of open-source models. However, the high memory costs associated with fine-tuning remain a significant challenge, especially as models increase in size. To address this, parameter efficient fine-tuning (PEFT) methods have been proposed to minimize the number of parameters required for fine-tuning LLMs. However, these approaches often tie the number of optimizer states to dimensions of model parameters, limiting flexibility and control during fine-tuning. In this paper, we propose sparse gradient compression (SGC), a training regime designed to address these limitations. Our approach leverages inherent sparsity in gradients to compress optimizer states by projecting them onto a low-dimensonal subspace, with dimensionality independent of the original model's parameters. By enabling optimizer state updates in an arbitrary low-dimensional subspace, SGC offers a flexible tradeoff between memory efficiency and performance. We demonstrate through experiments that SGC can decrease memory usage in optimizer states more effectively than existing PEFT methods. Furthermore, by fine-tuning LLMs on various downstream tasks, we show that SGC can deliver superior performance while substantially lowering optimizer state memory requirements, particularly in both data-limited and memory-limited settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00313</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00313</id><created>2025-01-31</created><authors><author><keyname>Hosseini</keyname><forenames>Hadi</forenames></author><author><keyname>Khanna</keyname><forenames>Samarth</forenames></author></authors><title>Distributive Fairness in Large Language Models: Evaluating Alignment   with Human Values</title><categories>cs.GT cs.AI cs.CL cs.MA</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The growing interest in employing large language models (LLMs) for decision-making in social and economic contexts has raised questions about their potential to function as agents in these domains. A significant number of societal problems involve the distribution of resources, where fairness, along with economic efficiency, play a critical role in the desirability of outcomes. In this paper, we examine whether LLM responses adhere to fundamental fairness concepts such as equitability, envy-freeness, and Rawlsian maximin, and investigate their alignment with human preferences. We evaluate the performance of several LLMs, providing a comparative benchmark of their ability to reflect these measures. Our results demonstrate a lack of alignment between current LLM responses and human distributional preferences. Moreover, LLMs are unable to utilize money as a transferable resource to mitigate inequality. Nonetheless, we demonstrate a stark contrast when (some) LLMs are tasked with selecting from a predefined menu of options rather than generating one. In addition, we analyze the robustness of LLM responses to variations in semantic factors (e.g. intentions or personas) or non-semantic prompting changes (e.g. templates or orderings). Finally, we highlight potential strategies aimed at enhancing the alignment of LLM behavior with well-established fairness concepts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00314</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00314</id><created>2025-01-31</created><authors><author><keyname>Heidari</keyname><forenames>Moein</forenames></author><author><keyname>Aghdam</keyname><forenames>Ehsan Khodapanah</forenames></author><author><keyname>Manzella</keyname><forenames>Alexander</forenames></author><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author><author><keyname>Scalabrino</keyname><forenames>Rebecca</forenames></author><author><keyname>Chen</keyname><forenames>Wenjin</forenames></author><author><keyname>Foran</keyname><forenames>David J.</forenames></author><author><keyname>Hacihaliloglu</keyname><forenames>Ilker</forenames></author></authors><title>A Study on the Performance of U-Net Modifications in Retroperitoneal   Tumor Segmentation</title><categories>eess.IV cs.CV</categories><comments>Accepted for presentation at the 2025 SPIE Medical Imaging Conference</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The retroperitoneum hosts a variety of tumors, including rare benign and malignant types, which pose diagnostic and treatment challenges due to their infrequency and proximity to vital structures. Estimating tumor volume is difficult due to their irregular shapes, and manual segmentation is time-consuming. Automatic segmentation using U-Net and its variants, incorporating Vision Transformer (ViT) elements, has shown promising results but struggles with high computational demands. To address this, architectures like the Mamba State Space Model (SSM) and Extended Long-Short Term Memory (xLSTM) offer efficient solutions by handling long-range dependencies with lower resource consumption. This study evaluates U-Net enhancements, including CNN, ViT, Mamba, and xLSTM, on a new in-house CT dataset and a public organ segmentation dataset. The proposed ViLU-Net model integrates Vi-blocks for improved segmentation. Results highlight xLSTM's efficiency in the U-Net framework. The code is publicly accessible on GitHub. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00315</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00315</id><created>2025-01-31</created><authors><author><keyname>Kim</keyname><forenames>Jihyeok</forenames></author><author><keyname>Moon</keyname><forenames>Seongwoo</forenames></author><author><keyname>Nah</keyname><forenames>Sungwon</forenames></author><author><keyname>Shim</keyname><forenames>David Hyunchul</forenames></author></authors><title>MonoDINO-DETR: Depth-Enhanced Monocular 3D Object Detection Using a   Vision Foundation Model</title><categories>cs.CV</categories><comments>8 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes novel methods to enhance the performance of monocular 3D object detection models by leveraging the generalized feature extraction capabilities of a vision foundation model. Unlike traditional CNN-based approaches, which often suffer from inaccurate depth estimation and rely on multi-stage object detection pipelines, this study employs a Vision Transformer (ViT)-based foundation model as the backbone, which excels at capturing global features for depth estimation. It integrates a detection transformer (DETR) architecture to improve both depth estimation and object detection performance in a one-stage manner. Specifically, a hierarchical feature fusion block is introduced to extract richer visual features from the foundation model, further enhancing feature extraction capabilities. Depth estimation accuracy is further improved by incorporating a relative depth estimation model trained on large-scale data and fine-tuning it through transfer learning. Additionally, the use of queries in the transformer's decoder, which consider reference points and the dimensions of 2D bounding boxes, enhances recognition performance. The proposed model outperforms recent state-of-the-art methods, as demonstrated through quantitative and qualitative evaluations on the KITTI 3D benchmark and a custom dataset collected from high-elevation racing environments. Code is available at https://github.com/JihyeokKim/MonoDINO-DETR. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00316</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00316</id><created>2025-01-31</created><authors><author><keyname>Dasdan</keyname><forenames>Ali</forenames></author></authors><title>The Kernighan-Lin Search Algorithm</title><categories>cs.DS</categories><comments>20 pages, 12 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The traveling salesman problem (TSP) and the graph partitioning problem (GPP) are two important combinatorial optimization problems with many applications. Due to the NP-hardness of these problems, heuristic algorithms are commonly used to find good, or hopefully near-optimal, solutions. Kernighan and Lin have proposed two of the most successful heuristic algorithms for these problems: The Lin-Kernighan (LK) algorithm for TSP and the Kernighan-Lin (KL) algorithm for GPP. Although these algorithms are problem specific to TSP and GPP, they share a problem-agnostic mechanism, called variable depth search, that has wide applicability for general search. This paper expresses this mechanism as part of a general search algorithm, called the Kernighan-Lin Search algorithm, to facilitate its use beyond the TSP and GPP problems. Experimental comparisons with other general search algorithms, namely, genetic algorithms, hill climbing, and simulated annealing, on function optimization test suites confirm that the new algorithm is very successful in solution quality and running time. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00317</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00317</id><created>2025-01-31</created><authors><author><keyname>Nam</keyname><forenames>Yehyun</forenames></author><author><keyname>Jang</keyname><forenames>Jihoon</forenames></author><author><keyname>Park</keyname><forenames>Kunsoo</forenames></author><author><keyname>Yang</keyname><forenames>Jianye</forenames></author><author><keyname>Long</keyname><forenames>Cheng</forenames></author></authors><title>DIST: Efficient k-Clique Listing via Induced Subgraph Trie</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Listing k-cliques plays a fundamental role in various data mining tasks, such as community detection and mining of cohesive substructures. Existing algorithms for the k-clique listing problem are built upon a general framework, which finds k-cliques by recursively finding (k-1)-cliques within subgraphs induced by the out-neighbors of each vertex. However, this framework has inherent inefficiency of finding smaller cliques within certain subgraphs repeatedly. In this paper, we propose an algorithm DIST for the k-clique listing problem. In contrast to existing works, the main idea in our approach is to compute each clique in the given graph only once and store it into a data structure called Induced Subgraph Trie, which allows us to retrieve the cliques efficiently. Furthermore, we propose a method to prune search space based on a novel concept called soft embedding of an l-tree, which further improves the running time. We show the superiority of our approach in terms of time and space usage through comprehensive experiments conducted on real-world networks; DIST outperforms the state-of-the-art algorithm by up to two orders of magnitude in both single-threaded and parallel experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00318</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00318</id><created>2025-01-31</created><authors><author><keyname>Xu</keyname><forenames>Chenhui</forenames></author><author><keyname>Liu</keyname><forenames>Dancheng</forenames></author><author><keyname>Hu</keyname><forenames>Yuting</forenames></author><author><keyname>Li</keyname><forenames>Jiajie</forenames></author><author><keyname>Qin</keyname><forenames>Ruiyang</forenames></author><author><keyname>Zheng</keyname><forenames>Qingxiao</forenames></author><author><keyname>Xiong</keyname><forenames>Jinjun</forenames></author></authors><title>Sub-Sequential Physics-Informed Learning with State Space Model</title><categories>cs.LG cs.NA math.NA</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Physics-Informed Neural Networks (PINNs) are a kind of deep-learning-based numerical solvers for partial differential equations (PDEs). Existing PINNs often suffer from failure modes of being unable to propagate patterns of initial conditions. We discover that these failure modes are caused by the simplicity bias of neural networks and the mismatch between PDE's continuity and PINN's discrete sampling. We reveal that the State Space Model (SSM) can be a continuous-discrete articulation allowing initial condition propagation, and that simplicity bias can be eliminated by aligning a sequence of moderate granularity. Accordingly, we propose PINNMamba, a novel framework that introduces sub-sequence modeling with SSM. Experimental results show that PINNMamba can reduce errors by up to 86.3\% compared with state-of-the-art architecture. Our code is available at https://github.com/miniHuiHui/PINNMamba. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00319</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00319</id><created>2025-01-31</created><authors><author><keyname>Yang</keyname><forenames>Dong</forenames></author><author><keyname>Wang</keyname><forenames>Yue</forenames></author><author><keyname>Zhang</keyname><forenames>Songyang</forenames></author><author><keyname>Li</keyname><forenames>Yingshu</forenames></author><author><keyname>Cai</keyname><forenames>Zhipeng</forenames></author></authors><title>Physics-Inspired Distributed Radio Map Estimation</title><categories>cs.LG cs.DC eess.SP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  To gain panoramic awareness of spectrum coverage in complex wireless environments, data-driven learning approaches have recently been introduced for radio map estimation (RME). While existing deep learning based methods conduct RME given spectrum measurements gathered from dispersed sensors in the region of interest, they rely on centralized data at a fusion center, which however raises critical concerns on data privacy leakages and high communication overloads. Federated learning (FL) enhance data security and communication efficiency in RME by allowing multiple clients to collaborate in model training without directly sharing local data. However, the performance of the FL-based RME can be hindered by the problem of task heterogeneity across clients due to their unavailable or inaccurate landscaping information. To fill this gap, in this paper, we propose a physics-inspired distributed RME solution in the absence of landscaping information. The main idea is to develop a novel distributed RME framework empowered by leveraging the domain knowledge of radio propagation models, and by designing a new distributed learning approach that splits the entire RME model into two modules. A global autoencoder module is shared among clients to capture the common pathloss influence on radio propagation pattern, while a client-specific autoencoder module focuses on learning the individual features produced by local shadowing effects from the unique building distributions in local environment. Simulation results show that our proposed method outperforms the benchmarks in achieving higher performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00320</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00320</id><created>2025-02-01</created><authors><author><keyname>Gan</keyname><forenames>Emily</forenames></author><author><keyname>Jedra</keyname><forenames>Yassir</forenames></author><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author></authors><title>$k$-SVD with Gradient Descent</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that a gradient-descent with a simple, universal rule for step-size selection provably finds $k$-SVD, i.e., the $k\geq 1$ largest singular values and corresponding vectors, of any matrix, despite nonconvexity. There has been substantial progress towards this in the past few years where existing results are able to establish such guarantees for the \emph{exact-parameterized} and \emph{over-parameterized} settings, with choice of oracle-provided step size. But guarantees for generic setting with a step size selection that does not require oracle-provided information has remained a challenge. We overcome this challenge and establish that gradient descent with an appealingly simple adaptive step size (akin to preconditioning) and random initialization enjoys global linear convergence for generic setting. Our convergence analysis reveals that the gradient method has an attracting region, and within this attracting region, the method behaves like Heron's method (a.k.a. the Babylonian method). Empirically, we validate the theoretical results. The emergence of modern compute infrastructure for iterative optimization coupled with this work is likely to provide means to solve $k$-SVD for very large matrices. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00321</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00321</id><created>2025-02-01</created><authors><author><keyname>Yan</keyname><forenames>Bencheng</forenames></author><author><keyname>Chen</keyname><forenames>Si</forenames></author><author><keyname>Jia</keyname><forenames>Shichang</forenames></author><author><keyname>Liu</keyname><forenames>Jianyu</forenames></author><author><keyname>Liu</keyname><forenames>Yueran</forenames></author><author><keyname>Fu</keyname><forenames>Chenghan</forenames></author><author><keyname>Guan</keyname><forenames>Wanxian</forenames></author><author><keyname>Zhao</keyname><forenames>Hui</forenames></author><author><keyname>Zhang</keyname><forenames>Xiang</forenames></author><author><keyname>Zhang</keyname><forenames>Kai</forenames></author><author><keyname>Su</keyname><forenames>Wenbo</forenames></author><author><keyname>Wang</keyname><forenames>Pengjie</forenames></author><author><keyname>Xu</keyname><forenames>Jian</forenames></author><author><keyname>Zheng</keyname><forenames>Bo</forenames></author><author><keyname>Liu</keyname><forenames>Baolin</forenames></author></authors><title>MIM: Multi-modal Content Interest Modeling Paradigm for User Behavior   Modeling</title><categories>cs.IR cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Click-Through Rate (CTR) prediction is a crucial task in recommendation systems, online searches, and advertising platforms, where accurately capturing users' real interests in content is essential for performance. However, existing methods heavily rely on ID embeddings, which fail to reflect users' true preferences for content such as images and titles. This limitation becomes particularly evident in cold-start and long-tail scenarios, where traditional approaches struggle to deliver effective results. To address these challenges, we propose a novel Multi-modal Content Interest Modeling paradigm (MIM), which consists of three key stages: Pre-training, Content-Interest-Aware Supervised Fine-Tuning (C-SFT), and Content-Interest-Aware UBM (CiUBM). The pre-training stage adapts foundational models to domain-specific data, enabling the extraction of high-quality multi-modal embeddings. The C-SFT stage bridges the semantic gap between content and user interests by leveraging user behavior signals to guide the alignment of embeddings with user preferences. Finally, the CiUBM stage integrates multi-modal embeddings and ID-based collaborative filtering signals into a unified framework. Comprehensive offline experiments and online A/B tests conducted on the Taobao, one of the world's largest e-commerce platforms, demonstrated the effectiveness and efficiency of MIM method. The method has been successfully deployed online, achieving a significant increase of +14.14% in CTR and +4.12% in RPM, showcasing its industrial applicability and substantial impact on platform performance. To promote further research, we have publicly released the code and dataset at https://pan.quark.cn/s/8fc8ec3e74f3. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00322</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00322</id><created>2025-02-01</created><authors><author><keyname>Balepur</keyname><forenames>Nishant</forenames></author><author><keyname>Siu</keyname><forenames>Alexa</forenames></author><author><keyname>Lipka</keyname><forenames>Nedim</forenames></author><author><keyname>Dernoncourt</keyname><forenames>Franck</forenames></author><author><keyname>Sun</keyname><forenames>Tong</forenames></author><author><keyname>Boyd-Graber</keyname><forenames>Jordan</forenames></author><author><keyname>Mathur</keyname><forenames>Puneet</forenames></author></authors><title>MODS: Moderating a Mixture of Document Speakers to Summarize Debatable   Queries in Document Collections</title><categories>cs.CL cs.IR</categories><comments>Accepted at NAACL 2025(main)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Query-focused summarization (QFS) gives a summary of documents to answer a query. Past QFS work assumes queries have one answer, ignoring debatable ones (Is law school worth it?). We introduce Debatable QFS (DQFS), a task to create summaries that answer debatable queries via documents with opposing perspectives; summaries must comprehensively cover all sources and balance perspectives, favoring no side. These goals elude LLM QFS systems, which: 1) lack structured content plans, failing to guide LLMs to write balanced summaries, and 2) use the same query to retrieve contexts across documents, failing to cover all perspectives specific to each document's content. To overcome this, we design MODS, a multi-LLM framework mirroring human panel discussions. MODS treats documents as individual Speaker LLMs and has a Moderator LLM that picks speakers to respond to tailored queries for planned topics. Speakers use tailored queries to retrieve relevant contexts from their documents and supply perspectives, which are tracked in a rich outline, yielding a content plan to guide the final summary. Experiments on ConflictingQA with controversial web queries and DebateQFS, our new dataset of debate queries from Debatepedia, show MODS beats SOTA by 38-59% in topic paragraph coverage and balance, based on new citation metrics. Users also find MODS's summaries to be readable and more balanced. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00329</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00329</id><created>2025-02-01</created><authors><author><keyname>Zhang</keyname><forenames>Jiani</forenames></author><author><keyname>Zhang</keyname><forenames>Hengrui</forenames></author><author><keyname>Chakravarti</keyname><forenames>Rishav</forenames></author><author><keyname>Hu</keyname><forenames>Yiqun</forenames></author><author><keyname>Ng</keyname><forenames>Patrick</forenames></author><author><keyname>Katsifodimos</keyname><forenames>Asterios</forenames></author><author><keyname>Rangwala</keyname><forenames>Huzefa</forenames></author><author><keyname>Karypis</keyname><forenames>George</forenames></author><author><keyname>Halevy</keyname><forenames>Alon</forenames></author></authors><title>CoddLLM: Empowering Large Language Models for Data Analytics</title><categories>cs.DB cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) have the potential to revolutionize data analytics by simplifying tasks such as data discovery and SQL query synthesis through natural language interactions. This work serves as a pivotal first step toward the development of foundation models explicitly designed for data analytics applications. To propel this vision forward, we unveil a new data recipe for post-training LLMs, enhancing their comprehension of data management and empowering them to tackle complex real-world analytics tasks. Specifically, our innovative approach includes a scalable synthetic data generation method that enables the creation of a broad spectrum of topics centered on data representation and manipulation. Furthermore, we introduce two new tasks that seamlessly bridge tables and text. We show that such tasks can enhance models' understanding of schema creation and the nuanced translation between natural language and tabular data. Leveraging this data recipe, we post-train a new foundation model, named CoddLLM, based on Mistral-NeMo-12B. To assess the language understanding and reasoning capabilities of LLMs in the realm of data analytics, we contribute AnalyticsMMLU, a benchmark containing thousands of multiple-choice questions on databases, data analysis, and machine learning. Our focus on data discovery, has resulted in the contribution of three comprehensive benchmarks that address both database and data lake scenarios. CoddLLM not only excels in performance but also sets a new standard, achieving the highest average accuracy across eight datasets. It outperforms GPT-3.5-Turbo on AnalyticsMMLU, exceeding GPT-4o by 12.1% in table selection and showing an average improvement of 24.9% in Text-to-SQL compared to the base model. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00330</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00330</id><created>2025-02-01</created><authors><author><keyname>Wan</keyname><forenames>Xingchen</forenames></author><author><keyname>Zhou</keyname><forenames>Han</forenames></author><author><keyname>Sun</keyname><forenames>Ruoxi</forenames></author><author><keyname>Nakhost</keyname><forenames>Hootan</forenames></author><author><keyname>Jiang</keyname><forenames>Ke</forenames></author><author><keyname>Arık</keyname><forenames>Sercan Ö.</forenames></author></authors><title>From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative   Optimization and Generation</title><categories>cs.LG cs.AI stat.ML</categories><comments>Expanded version of the ICLR 2025 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in long-context large language models (LLMs) have led to the emerging paradigm of many-shot in-context learning (ICL), where it is observed that scaling many more demonstrating examples beyond the conventional few-shot setup in the context can lead to performance benefits. However, despite its promise, it is unclear what aspects dominate the benefits and whether simply scaling to more examples is the most effective way of improving many-shot ICL. In this work, we first provide an analysis of the factors driving many-shot ICL, and we find that 1) many-shot performance can still be attributed to often a few disproportionately influential examples and 2) identifying such influential examples ("optimize") and using them as demonstrations to regenerate new examples ("generate") can lead to further improvements. Inspired by the findings, we propose BRIDGE, an algorithm that alternates between the optimize step with Bayesian optimization to discover the influential sets of examples and the generate step to reuse this set to expand the reasoning paths of the examples back to the many-shot regime automatically. On Gemini, Claude, and Mistral LLMs of different sizes, we show that BRIDGE to significant improvements across a diverse set of tasks, including symbolic reasoning, numerical reasoning, and code generation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00333</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00333</id><created>2025-02-01</created><authors><author><keyname>Liu</keyname><forenames>Kai</forenames></author><author><keyname>Yang</keyname><forenames>Kaicheng</forenames></author><author><keyname>Chen</keyname><forenames>Zheng</forenames></author><author><keyname>Li</keyname><forenames>Zhiteng</forenames></author><author><keyname>Guo</keyname><forenames>Yong</forenames></author><author><keyname>Li</keyname><forenames>Wenbo</forenames></author><author><keyname>Kong</keyname><forenames>Linghe</forenames></author><author><keyname>Zhang</keyname><forenames>Yulun</forenames></author></authors><title>BiMaCoSR: Binary One-Step Diffusion Model Leveraging Flexible Matrix   Compression for Real Super-Resolution</title><categories>cs.CV</categories><comments>10 pages, 5 figures. The code and models will be available at   https://github.com/Kai-Liu001/BiMaCoSR</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While super-resolution (SR) methods based on diffusion models (DM) have demonstrated inspiring performance, their deployment is impeded due to the heavy request of memory and computation. Recent researchers apply two kinds of methods to compress or fasten the DM. One is to compress the DM into 1-bit, aka binarization, alleviating the storage and computation pressure. The other distills the multi-step DM into only one step, significantly speeding up inference process. Nonetheless, it remains impossible to deploy DM to resource-limited edge devices. To address this problem, we propose BiMaCoSR, which combines binarization and one-step distillation to obtain extreme compression and acceleration. To prevent the catastrophic collapse of the model caused by binarization, we proposed sparse matrix branch (SMB) and low rank matrixbranch (LRM). Both auxiliary branches pass the full-precision (FP) information but in different ways. SMB absorbs the extreme values and its output is high rank, carrying abundant FP information. Whereas, the design of LRMB is inspired by LoRA and is initialized with the top r SVD components, outputting low rank representation. The computation and storage overhead of our proposed branches can be safely ignored. Comprehensive comparison experiments are conducted to exhibit BiMaCoSR outperforms current state-of-the-art binarization methods and gains competitive performance compared with FP one-step model. BiMaCoSR achieves a 23.8x compression ratio and a 27.4x speedup ratio compared to FP counterpart. Our code and model are available at https://github.com/Kai-Liu001/BiMaCoSR. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00334</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00334</id><created>2025-02-01</created><authors><author><keyname>Xu</keyname><forenames>Xin</forenames></author><author><keyname>Xu</keyname><forenames>Qiyun</forenames></author><author><keyname>Xiao</keyname><forenames>Tong</forenames></author><author><keyname>Chen</keyname><forenames>Tianhao</forenames></author><author><keyname>Yan</keyname><forenames>Yuchen</forenames></author><author><keyname>Zhang</keyname><forenames>Jiaxin</forenames></author><author><keyname>Diao</keyname><forenames>Shizhe</forenames></author><author><keyname>Yang</keyname><forenames>Can</forenames></author><author><keyname>Wang</keyname><forenames>Yang</forenames></author></authors><title>UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning   with Large Language Models</title><categories>cs.CL cs.AI</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large language models (LLMs) have demonstrated remarkable capabilities in solving complex reasoning tasks, particularly in mathematics. However, the domain of physics reasoning presents unique challenges that have received significantly less attention. Existing benchmarks often fall short in evaluating LLMs' abilities on the breadth and depth of undergraduate-level physics, underscoring the need for a comprehensive evaluation. To fill this gap, we introduce UGPhysics, a large-scale and comprehensive benchmark specifically designed to evaluate UnderGraduate-level Physics (UGPhysics) reasoning with LLMs. UGPhysics includes 5,520 undergraduate-level physics problems in both English and Chinese, covering 13 subjects with seven different answer types and four distinct physics reasoning skills, all rigorously screened for data leakage. Additionally, we develop a Model-Assistant Rule-based Judgment (MARJ) pipeline specifically tailored for assessing answer correctness of physics problems, ensuring accurate evaluation. Our evaluation of 31 leading LLMs shows that the highest overall accuracy, 49.8% (achieved by OpenAI-o1-mini), emphasizes the necessity for models with stronger physics reasoning skills, beyond math abilities. We hope UGPhysics, along with MARJ, will drive future advancements in AI for physics reasoning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00336</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00336</id><created>2025-02-01</created><authors><author><keyname>George</keyname><forenames>Anand Jerry</forenames></author><author><keyname>Veiga</keyname><forenames>Rodrigo</forenames></author><author><keyname>Macris</keyname><forenames>Nicolas</forenames></author></authors><title>Denoising Score Matching with Random Features: Insights on Diffusion   Models from Precise Learning Curves</title><categories>cs.LG stat.ML</categories><comments>8 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We derive asymptotically precise expressions for test and train errors of denoising score matching (DSM) in generative diffusion models. The score function is parameterized by random features neural networks, with the target distribution being $d$-dimensional standard Gaussian. We operate in a regime where the dimension $d$, number of data samples $n$, and number of features $p$ tend to infinity while keeping the ratios $\psi_n=\frac{n}{d}$ and $\psi_p=\frac{p}{d}$ fixed. By characterizing the test and train errors, we identify regimes of generalization and memorization in diffusion models. Furthermore, our work sheds light on the conditions enhancing either generalization or memorization. Consistent with prior empirical observations, our findings indicate that the model complexity ($p$) and the number of noise samples per data sample ($m$) used during DSM significantly influence generalization and memorization behaviors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00338</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00338</id><created>2025-02-01</created><authors><author><keyname>Gao</keyname><forenames>Yuan</forenames></author><author><keyname>Wu</keyname><forenames>Hao</forenames></author><author><keyname>Shu</keyname><forenames>Ruiqi</forenames></author><author><keyname>Dong</keyname><forenames>Huanshuo</forenames></author><author><keyname>Xu</keyname><forenames>Fan</forenames></author><author><keyname>Chen</keyname><forenames>Rui</forenames></author><author><keyname>Yan</keyname><forenames>Yibo</forenames></author><author><keyname>Wen</keyname><forenames>Qingsong</forenames></author><author><keyname>Hu</keyname><forenames>Xuming</forenames></author><author><keyname>Wang</keyname><forenames>Kun</forenames></author><author><keyname>Wu</keyname><forenames>Jiahao</forenames></author><author><keyname>Li</keyname><forenames>Qing</forenames></author><author><keyname>Xiong</keyname><forenames>Hui</forenames></author><author><keyname>Huang</keyname><forenames>Xiaomeng</forenames></author></authors><title>OneForecast: A Universal Framework for Global and Regional Weather   Forecasting</title><categories>cs.LG physics.ao-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate weather forecasts are important for disaster prevention, agricultural planning, and water resource management. Traditional numerical weather prediction (NWP) methods offer physically interpretable high-accuracy predictions but are computationally expensive and fail to fully leverage rapidly growing historical data. In recent years, deep learning methods have made significant progress in weather forecasting, but challenges remain, such as balancing global and regional high-resolution forecasts, excessive smoothing in extreme event predictions, and insufficient dynamic system modeling. To address these issues, this paper proposes a global-regional nested weather forecasting framework based on graph neural networks (GNNs). By combining a dynamic system perspective with multi-grid theory, we construct a multi-scale graph structure and densify the target region to capture local high-frequency features. We introduce an adaptive information propagation mechanism, using dynamic gating units to deeply integrate node and edge features for more accurate extreme event forecasting. For high-resolution regional forecasts, we propose a neural nested grid method to mitigate boundary information loss. Experimental results show that the proposed method performs excellently across global to regional scales and short-term to long-term forecasts, especially in extreme event predictions (e.g., typhoons), significantly improving forecast accuracy. Our codes are available at https://github.com/YuanGao-YG/OneForecast. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00339</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00339</id><created>2025-02-01</created><authors><author><keyname>Yi</keyname><forenames>Jingyuan</forenames></author><author><keyname>Xu</keyname><forenames>Zeqiu</forenames></author><author><keyname>Huang</keyname><forenames>Tianyi</forenames></author><author><keyname>Yu</keyname><forenames>Peiyang</forenames></author></authors><title>Challenges and Innovations in LLM-Powered Fake News Detection: A   Synthesis of Approaches and Future Directions</title><categories>cs.CL cs.CY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The pervasiveness of the dissemination of fake news through social media platforms poses critical risks to the trust of the general public, societal stability, and democratic institutions. This challenge calls for novel methodologies in detection, which can keep pace with the dynamic and multi-modal nature of misinformation. Recent works include powering the detection using large language model advances in multimodal frameworks, methodologies using graphs, and adversarial training in the literature of fake news. Based on the different approaches which can bring success, some key highlights will be underlined: enhanced LLM-improves accuracy through more advanced semantics and cross-modality fusion for robust detections. The review further identifies critical gaps in adaptability to dynamic social media trends, real-time, and cross-platform detection capabilities, as well as the ethical challenges thrown up by the misuse of LLMs. Future directions underline the development of style-agnostic models, cross-lingual detection frameworks, and robust policies with a view to mitigating LLM-driven misinformation. This synthesis thus lays a concrete foundation for those researchers and practitioners committed to reinforcing fake news detection systems with complications that keep on growing in the digital landscape. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00340</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00340</id><created>2025-02-01</created><authors><author><keyname>Chai</keyname><forenames>Di</forenames></author><author><keyname>Li</keyname><forenames>Pengbo</forenames></author><author><keyname>Zhang</keyname><forenames>Feiyuan</forenames></author><author><keyname>Jin</keyname><forenames>Yilun</forenames></author><author><keyname>Tian</keyname><forenames>Han</forenames></author><author><keyname>Zhang</keyname><forenames>Junxue</forenames></author><author><keyname>Chen</keyname><forenames>Kai</forenames></author></authors><title>Enhancing Token Filtering Efficiency in Large Language Model Training   with Collider</title><categories>cs.LG cs.CL cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Token filtering has been proposed to enhance utility of large language models (LLMs) by eliminating inconsequential tokens during training. While using fewer tokens should reduce computational workloads, existing studies have not succeeded in achieving higher efficiency. This is primarily due to the insufficient sparsity caused by filtering tokens only in the output layers, as well as inefficient sparse GEMM (General Matrix Multiplication), even when having sufficient sparsity.   This paper presents Collider, a system unleashing the full efficiency of token filtering in LLM training. At its core, Collider filters activations of inconsequential tokens across all layers to maintain sparsity. Additionally, it features an automatic workflow that transforms sparse GEMM into dimension-reduced dense GEMM for optimized efficiency. Evaluations on three LLMs-TinyLlama-1.1B, Qwen2.5-1.5B, and Phi1.5-1.4B-demonstrate that Collider reduces backpropagation time by up to 35.1% and end-to-end training time by up to 22.0% when filtering 40% of tokens. Utility assessments of training TinyLlama on 15B tokens indicate that Collider sustains the utility advancements of token filtering by relatively improving model utility by 16.3% comparing to regular training, and reduces training time from 4.7 days to 3.5 days using 8 GPUs. Collider is designed for easy integration into existing LLM training frameworks, allowing systems already using token filtering to accelerate training with just one line of code. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00341</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00341</id><created>2025-02-01</created><authors><author><keyname>Jabbour</keyname><forenames>Jason</forenames></author><author><keyname>Kleinbard</keyname><forenames>Kai</forenames></author><author><keyname>Miller</keyname><forenames>Olivia</forenames></author><author><keyname>Haussman</keyname><forenames>Robert</forenames></author><author><keyname>Reddi</keyname><forenames>Vijay Janapa</forenames></author></authors><title>SocratiQ: A Generative AI-Powered Learning Companion for Personalized   Education and Broader Accessibility</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Traditional educational approaches often struggle to provide personalized and interactive learning experiences on a scale. In this paper, we present SocratiQ, an AI-powered educational assistant that addresses this challenge by implementing the Socratic method through adaptive learning technologies. The system employs a novel Generative AI-based learning framework that dynamically creates personalized learning pathways based on student responses and comprehension patterns. We provide an account of our integration methodology, system architecture, and evaluation framework, along with the technical and pedagogical challenges encountered during implementation and our solutions. Although our implementation focuses on machine learning systems education, the integration approaches we present can inform similar efforts across STEM fields. Through this work, our goal is to advance the understanding of how generative AI technologies can be designed and systematically incorporated into educational resources. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00342</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00342</id><created>2025-02-01</created><authors><author><keyname>Li</keyname><forenames>Zechuan</forenames></author><author><keyname>Yu</keyname><forenames>Hongshan</forenames></author><author><keyname>Ding</keyname><forenames>Yihao</forenames></author><author><keyname>Li</keyname><forenames>Yan</forenames></author><author><keyname>He</keyname><forenames>Yong</forenames></author><author><keyname>Akhtar</keyname><forenames>Naveed</forenames></author></authors><title>Embodied Intelligence for 3D Understanding: A Survey on 3D Scene   Question Answering</title><categories>cs.CV</categories><comments>Work in progress</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  3D Scene Question Answering (3D SQA) represents an interdisciplinary task that integrates 3D visual perception and natural language processing, empowering intelligent agents to comprehend and interact with complex 3D environments. Recent advances in large multimodal modelling have driven the creation of diverse datasets and spurred the development of instruction-tuning and zero-shot methods for 3D SQA. However, this rapid progress introduces challenges, particularly in achieving unified analysis and comparison across datasets and baselines. This paper presents the first comprehensive survey of 3D SQA, systematically reviewing datasets, methodologies, and evaluation metrics while highlighting critical challenges and future opportunities in dataset standardization, multimodal fusion, and task design. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00343</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00343</id><created>2025-02-01</created><authors><author><keyname>Abdelmoniem</keyname><forenames>Ahmed M.</forenames></author><author><keyname>Abdulah</keyname><forenames>Sameh</forenames></author><author><keyname>Atwa</keyname><forenames>Walid</forenames></author></authors><title>A Novel Approach to Translate Structural Aggregation Queries to   MapReduce Code</title><categories>cs.DB cs.DC</categories><journal-ref>International Journal of Computer Applications(0975 - 8887),   Volume 186 - No.33, July 2024</journal-ref><doi>10.5120/ijca2024923879</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Data management applications are growing and require more attention, especially in the "big data" era. Thus, supporting such applications with novel and efficient algorithms that achieve higher performance is critical. Array database management systems are one way to support these applications by dealing with data represented in n-dimensional data structures. For instance, software like SciDB and RasDaMan can be powerful tools to achieve the required performance on large-scale problems with multidimensional data. Like their relational counterparts, these management systems support specific array query languages as the user interface. As a popular programming model, MapReduce allows large-scale data analysis, facilitates query processing, and is used as a DB engine. Nevertheless, one major obstacle is the low productivity of developing MapReduce applications. Unlike high-level declarative languages such as SQL, MapReduce jobs are written in a low-level descriptive language, often requiring massive programming efforts and complicated debugging processes. This work presents a system that supports translating array queries expressed in the Array Query Language (AQL) in SciDB into MapReduce jobs. We focus on translating some unique structural aggregations, including circular, grid, hierarchical, and sliding aggregations. Unlike traditional aggregations in relational DBs, these structural aggregations are designed explicitly for array manipulation. Thus, our work can be considered an array-view counterpart of existing SQL to MapReduce translators like HiveQL and YSmart. Our translator supports structural aggregations over arrays to meet various array manipulations. The translator can also help user-defined aggregation functions with minimal user effort. We show that our translator can generate optimized MapReduce code, which performs better than the short handwritten code by up to 10.84x. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00344</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00344</id><created>2025-02-01</created><authors><author><keyname>Kobayashi</keyname><forenames>Kosei</forenames></author><author><keyname>Matsuzaki</keyname><forenames>Kosuke</forenames></author><author><keyname>Taniguchi</keyname><forenames>Masaya</forenames></author><author><keyname>Sakaguchi</keyname><forenames>Keisuke</forenames></author><author><keyname>Inui</keyname><forenames>Kentaro</forenames></author><author><keyname>Abe</keyname><forenames>Kentaro</forenames></author></authors><title>FinchGPT: a Transformer based language model for birdsong analysis</title><categories>cs.CL</categories><comments>12 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The long-range dependencies among the tokens, which originate from hierarchical structures, are a defining hallmark of human language. However, whether similar dependencies exist within the sequential vocalization of non-human animals remains a topic of investigation. Transformer architectures, known for their ability to model long-range dependencies among tokens, provide a powerful tool for investigating this phenomenon. In this study, we employed the Transformer architecture to analyze the songs of Bengalese finch (Lonchura striata domestica), which are characterized by their highly variable and complex syllable sequences. To this end, we developed FinchGPT, a Transformer-based model trained on a textualized corpus of birdsongs, which outperformed other architecture models in this domain. Attention weight analysis revealed that FinchGPT effectively captures long-range dependencies within syllables sequences. Furthermore, reverse engineering approaches demonstrated the impact of computational and biological manipulations on its performance: restricting FinchGPT's attention span and disrupting birdsong syntax through the ablation of specific brain nuclei markedly influenced the model's outputs. Our study highlights the transformative potential of large language models (LLMs) in deciphering the complexities of animal vocalizations, offering a novel framework for exploring the structural properties of non-human communication systems while shedding light on the computational distinctions between biological brains and artificial neural networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00345</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00345</id><created>2025-02-01</created><authors><author><keyname>Li</keyname><forenames>Yurui</forenames></author><author><keyname>Chen</keyname><forenames>Yuxuan</forenames></author><author><keyname>Zhang</keyname><forenames>Li</forenames></author><author><keyname>Li</keyname><forenames>Shijian</forenames></author><author><keyname>Pan</keyname><forenames>Gang</forenames></author></authors><title>The Composite Task Challenge for Cooperative Multi-Agent Reinforcement   Learning</title><categories>cs.LG cs.AI cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The significant role of division of labor (DOL) in promoting cooperation is widely recognized in real-world applications.Many cooperative multi-agent reinforcement learning (MARL) methods have incorporated the concept of DOL to improve cooperation among agents.However, the tasks used in existing testbeds typically correspond to tasks where DOL is often not a necessary feature for achieving optimal policies.Additionally, the full utilize of DOL concept in MARL methods remains unrealized due to the absence of appropriate tasks.To enhance the generality and applicability of MARL methods in real-world scenarios, there is a necessary to develop tasks that demand multi-agent DOL and cooperation.In this paper, we propose a series of tasks designed to meet these requirements, drawing on real-world rules as the guidance for their design.We guarantee that DOL and cooperation are necessary condition for completing tasks and introduce three factors to expand the diversity of proposed tasks to cover more realistic situations.We evaluate 10 cooperative MARL methods on the proposed tasks.The results indicate that all baselines perform poorly on these tasks.To further validate the solvability of these tasks, we also propose simplified variants of proposed tasks.Experimental results show that baselines are able to handle these simplified variants, providing evidence of the solvability of the proposed tasks.The source files is available at https://github.com/Yurui-Li/CTC. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00346</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00346</id><created>2025-02-01</created><authors><author><keyname>Abrar</keyname><forenames>Md Mainul</forenames></author><author><keyname>Sapkota</keyname><forenames>Parvat</forenames></author><author><keyname>Sprouts</keyname><forenames>Damon</forenames></author><author><keyname>Jia</keyname><forenames>Xun</forenames></author><author><keyname>Chi</keyname><forenames>Yujie</forenames></author></authors><title>Actor Critic with Experience Replay-based automatic treatment planning   for prostate cancer intensity modulated radiotherapy</title><categories>cs.LG cs.AI physics.med-ph</categories><comments>27 Pages, 8 Figures, 4 Tables</comments><msc-class>92C50 (Primary) 68T07 (Secondary)</msc-class><acm-class>I.2.1; J.2; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Real-time treatment planning in IMRT is challenging due to complex beam interactions. AI has improved automation, but existing models require large, high-quality datasets and lack universal applicability. Deep reinforcement learning (DRL) offers a promising alternative by mimicking human trial-and-error planning.   Purpose: Develop a stochastic policy-based DRL agent for automatic treatment planning with efficient training, broad applicability, and robustness against adversarial attacks using Fast Gradient Sign Method (FGSM).   Methods: Using the Actor-Critic with Experience Replay (ACER) architecture, the agent tunes treatment planning parameters (TPPs) in inverse planning. Training is based on prostate cancer IMRT cases, using dose-volume histograms (DVHs) as input. The model is trained on a single patient case, validated on two independent cases, and tested on 300+ plans across three datasets. Plan quality is assessed using ProKnow scores, and robustness is tested against adversarial attacks.   Results: Despite training on a single case, the model generalizes well. Before ACER-based planning, the mean plan score was 6.20$\pm$1.84; after, 93.09% of cases achieved a perfect score of 9, with a mean of 8.93$\pm$0.27. The agent effectively prioritizes optimal TPP tuning and remains robust against adversarial attacks.   Conclusions: The ACER-based DRL agent enables efficient, high-quality treatment planning in prostate cancer IMRT, demonstrating strong generalizability and robustness. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00347</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00347</id><created>2025-02-01</created><authors><author><keyname>Muiz</keyname><forenames>Bakhtiar</forenames></author><author><keyname>Hasib</keyname><forenames>Abdul</forenames></author><author><keyname>Ahmed</keyname><forenames>Md. Faishal</forenames></author><author><keyname>Zubaer</keyname><forenames>Abdullah Al</forenames></author><author><keyname>Hossen</keyname><forenames>Rakib</forenames></author><author><keyname>Khushi</keyname><forenames>Mst Deloara</forenames></author><author><keyname>Rahman</keyname><forenames>Anichur</forenames></author></authors><title>IoT-enabled Drowsiness Driver Safety Alert System with Real-Time   Monitoring Using Integrated Sensors Technology</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Significant losses in terms of life and property occur from road traffic accidents, which are often caused by drunk and drowsy drivers. Reducing accidents requires effective detection of alcohol impairment and drowsiness as well as real-time driver monitoring. This paper aims to create an Internet of Things (IoT)--enabled Drowsiness Driver Safety Alert System with Real-Time Monitoring Using Integrated Sensors Technology. The system features an alcohol sensor and an IR sensor for detecting alcohol presence and monitoring driver eye movements, respectively. Upon detecting alcohol, alarms and warning lights are activated, the vehicle speed is progressively reduced, and the motor stops within ten to fifteen seconds if the alcohol presence persists. The IR sensor monitors prolonged eye closure, triggering alerts, or automatic vehicle stoppage to prevent accidents caused by drowsiness. Data from the IR sensor is transmitted to a mobile phone via Bluetooth for real-time monitoring and alerts. By identifying driver alcoholism and drowsiness, this system seeks to reduce accidents and save lives by providing safer transportation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00348</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00348</id><created>2025-02-01</created><authors><author><keyname>Zhang</keyname><forenames>Kaike</forenames></author><author><keyname>Cao</keyname><forenames>Qi</forenames></author><author><keyname>Wu</keyname><forenames>Yunfan</forenames></author><author><keyname>Sun</keyname><forenames>Fei</forenames></author><author><keyname>Shen</keyname><forenames>Huawei</forenames></author><author><keyname>Cheng</keyname><forenames>Xueqi</forenames></author></authors><title>Personalized Denoising Implicit Feedback for Robust Recommender System</title><categories>cs.IR</categories><comments>To appear in WWW 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While implicit feedback is foundational to modern recommender systems, factors such as human error, uncertainty, and ambiguity in user behavior inevitably introduce significant noise into this feedback, adversely affecting the accuracy and robustness of recommendations. To address this issue, existing methods typically aim to reduce the training weight of noisy feedback or discard it entirely, based on the observation that noisy interactions often exhibit higher losses in the overall loss distribution. However, we identify two key issues: (1) there is a significant overlap between normal and noisy interactions in the overall loss distribution, and (2) this overlap becomes even more pronounced when transitioning from pointwise loss functions (e.g., BCE loss) to pairwise loss functions (e.g., BPR loss). This overlap leads traditional methods to misclassify noisy interactions as normal, and vice versa. To tackle these challenges, we further investigate the loss overlap and find that for a given user, there is a clear distinction between normal and noisy interactions in the user's personal loss distribution. Based on this insight, we propose a resampling strategy to Denoise using the user's Personal Loss distribution, named PLD, which reduces the probability of noisy interactions being optimized. Specifically, during each optimization iteration, we create a candidate item pool for each user and resample the items from this pool based on the user's personal loss distribution, prioritizing normal interactions. Additionally, we conduct a theoretical analysis to validate PLD's effectiveness and suggest ways to further enhance its performance. Extensive experiments conducted on three datasets with varying noise ratios demonstrate PLD's efficacy and robustness. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00350</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00350</id><created>2025-02-01</created><authors><author><keyname>Yu</keyname><forenames>Zhongming</forenames></author><author><keyname>Zhang</keyname><forenames>Hejia</forenames></author><author><keyname>Zhao</keyname><forenames>Yujie</forenames></author><author><keyname>Huang</keyname><forenames>Hanxian</forenames></author><author><keyname>Yao</keyname><forenames>Matrix</forenames></author><author><keyname>Ding</keyname><forenames>Ke</forenames></author><author><keyname>Zhao</keyname><forenames>Jishen</forenames></author></authors><title>OrcaLoca: An LLM Agent Framework for Software Issue Localization</title><categories>cs.SE cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent developments in Large Language Model (LLM) agents are revolutionizing Autonomous Software Engineering (ASE), enabling automated coding, problem fixes, and feature improvements. However, localization -- precisely identifying software problems by navigating to relevant code sections -- remains a significant challenge. Current approaches often yield suboptimal results due to a lack of effective integration between LLM agents and precise code search mechanisms. This paper introduces OrcaLoca, an LLM agent framework that improves accuracy for software issue localization by integrating priority-based scheduling for LLM-guided action, action decomposition with relevance scoring, and distance-aware context pruning. Experimental results demonstrate that OrcaLoca becomes the new open-source state-of-the-art (SOTA) in function match rate (65.33%) on SWE-bench Lite. It also improves the final resolved rate of an open-source framework by 6.33 percentage points through its patch generation integration. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00351</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00351</id><created>2025-02-01</created><authors><author><keyname>Liu</keyname><forenames>Yao</forenames></author><author><keyname>Liu</keyname><forenames>Zhilan</forenames></author><author><keyname>Tan</keyname><forenames>Tien Ping</forenames></author><author><keyname>Li</keyname><forenames>Yuxin</forenames></author></authors><title>Multi-Order Hyperbolic Graph Convolution and Aggregated Attention for   Social Event Detection</title><categories>cs.SI cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Social event detection (SED) is a task focused on identifying specific real-world events and has broad applications across various domains. It is integral to many mobile applications with social features, including major platforms like Twitter, Weibo, and Facebook. By enabling the analysis of social events, SED provides valuable insights for businesses to understand consumer preferences and supports public services in handling emergencies and disaster management. Due to the hierarchical structure of event detection data, traditional approaches in Euclidean space often fall short in capturing the complexity of such relationships. While existing methods in both Euclidean and hyperbolic spaces have shown promising results, they tend to overlook multi-order relationships between events. To address these limitations, this paper introduces a novel framework, Multi-Order Hyperbolic Graph Convolution with Aggregated Attention (MOHGCAA), designed to enhance the performance of SED. Experimental results demonstrate significant improvements under both supervised and unsupervised settings. To further validate the effectiveness and robustness of the proposed framework, we conducted extensive evaluations across multiple datasets, confirming its superiority in tackling common challenges in social event detection. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00352</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00352</id><created>2025-02-01</created><authors><author><keyname>Han</keyname><forenames>Ye</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author><author><keyname>Meng</keyname><forenames>Dejian</forenames></author></authors><title>A Differentiated Reward Method for Reinforcement Learning based   Multi-Vehicle Cooperative Decision-Making Algorithms</title><categories>cs.AI cs.MA cs.RO</categories><comments>8 pages, 3 figures, submitted to IEEE IV 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reinforcement learning (RL) shows great potential for optimizing multi-vehicle cooperative driving strategies through the state-action-reward feedback loop, but it still faces challenges such as low sample efficiency. This paper proposes a differentiated reward method based on steady-state transition systems, which incorporates state transition gradient information into the reward design by analyzing traffic flow characteristics, aiming to optimize action selection and policy learning in multi-vehicle cooperative decision-making. The performance of the proposed method is validated in RL algorithms such as MAPPO, MADQN, and QMIX under varying autonomous vehicle penetration. The results show that the differentiated reward method significantly accelerates training convergence and outperforms centering reward and others in terms of traffic efficiency, safety, and action rationality. Additionally, the method demonstrates strong scalability and environmental adaptability, providing a novel approach for multi-agent cooperative decision-making in complex traffic scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00354</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00354</id><created>2025-02-01</created><authors><author><keyname>Feng</keyname><forenames>Yu</forenames></author><author><keyname>Geng</keyname><forenames>Yangli-ao</forenames></author><author><keyname>Zhu</keyname><forenames>Yifan</forenames></author><author><keyname>Han</keyname><forenames>Zongfu</forenames></author><author><keyname>Yu</keyname><forenames>Xie</forenames></author><author><keyname>Xue</keyname><forenames>Kaiwen</forenames></author><author><keyname>Luo</keyname><forenames>Haoran</forenames></author><author><keyname>Sun</keyname><forenames>Mengyang</forenames></author><author><keyname>Zhang</keyname><forenames>Guangwei</forenames></author><author><keyname>Song</keyname><forenames>Meina</forenames></author></authors><title>PM-MOE: Mixture of Experts on Private Model Parameters for Personalized   Federated Learning</title><categories>cs.LG cs.AI cs.CR</categories><doi>10.1145/3696410.3714561</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Federated learning (FL) has gained widespread attention for its privacy-preserving and collaborative learning capabilities. Due to significant statistical heterogeneity, traditional FL struggles to generalize a shared model across diverse data domains. Personalized federated learning addresses this issue by dividing the model into a globally shared part and a locally private part, with the local model correcting representation biases introduced by the global model. Nevertheless, locally converged parameters more accurately capture domain-specific knowledge, and current methods overlook the potential benefits of these parameters. To address these limitations, we propose PM-MoE architecture. This architecture integrates a mixture of personalized modules and an energy-based personalized modules denoising, enabling each client to select beneficial personalized parameters from other clients. We applied the PM-MoE architecture to nine recent model-split-based personalized federated learning algorithms, achieving performance improvements with minimal additional training. Extensive experiments on six widely adopted datasets and two heterogeneity settings validate the effectiveness of our approach. The source code is available at \url{https://github.com/dannis97500/PM-MOE}. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00355</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00355</id><created>2025-02-01</created><authors><author><keyname>George</keyname><forenames>Anand Jerry</forenames></author><author><keyname>Macris</keyname><forenames>Nicolas</forenames></author></authors><title>Sampling in High-Dimensions using Stochastic Interpolants and   Forward-Backward Stochastic Differential Equations</title><categories>cs.LG stat.ML</categories><comments>8 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a class of diffusion-based algorithms to draw samples from high-dimensional probability distributions given their unnormalized densities. Ideally, our methods can transport samples from a Gaussian distribution to a specified target distribution in finite time. Our approach relies on the stochastic interpolants framework to define a time-indexed collection of probability densities that bridge a Gaussian distribution to the target distribution. Subsequently, we derive a diffusion process that obeys the aforementioned probability density at each time instant. Obtaining such a diffusion process involves solving certain Hamilton-Jacobi-Bellman PDEs. We solve these PDEs using the theory of forward-backward stochastic differential equations (FBSDE) together with machine learning-based methods. Through numerical experiments, we demonstrate that our algorithm can effectively draw samples from distributions that conventional methods struggle to handle. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00356</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00356</id><created>2025-02-01</created><authors><author><keyname>Geng</keyname><forenames>Zipei</forenames></author><author><keyname>Abdulah</keyname><forenames>Sameh</forenames></author><author><keyname>Sun</keyname><forenames>Ying</forenames></author><author><keyname>Ltaief</keyname><forenames>Hatem</forenames></author><author><keyname>Keyes</keyname><forenames>David E.</forenames></author><author><keyname>Genton</keyname><forenames>Marc G.</forenames></author></authors><title>GPU-Accelerated Modified Bessel Function of the Second Kind for Gaussian   Processes</title><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Modified Bessel functions of the second kind are widely used in physics, engineering, spatial statistics, and machine learning. Since contemporary scientific applications, including machine learning, rely on GPUs for acceleration, providing robust GPU-hosted implementations of special functions, such as the modified Bessel function, is crucial for performance. Existing implementations of the modified Bessel function of the second kind rely on CPUs and have limited coverage of the full range of values needed in some applications. In this work, we present a robust implementation of the modified Bessel function of the second kind on GPUs, eliminating the dependence on the CPU host. We cover a range of values commonly used in real applications, providing high accuracy compared to common libraries like the GNU Scientific Library (GSL) when referenced to Mathematica as the authority. Our GPU-accelerated approach demonstrates a 2.68x performance improvement using a single A100 GPU compared to the GSL on 40-core Intel Cascade Lake CPUs. Our implementation is integrated into ExaGeoStat, the HPC framework for spatial data modeling, where the modified Bessel function of the second kind is required by the Mat\'ern covariance function in generating covariance matrices. We accelerate the matrix generation process in ExaGeoStat by up to 12.62x with four A100 GPUs while maintaining almost the same accuracy for modeling and prediction operations using synthetic and real datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00358</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00358</id><created>2025-02-01</created><authors><author><keyname>Li</keyname><forenames>Jia</forenames></author><author><keyname>Zhao</keyname><forenames>Wenjie</forenames></author><author><keyname>Huang</keyname><forenames>Ziru</forenames></author><author><keyname>Guo</keyname><forenames>Yunhui</forenames></author><author><keyname>Tian</keyname><forenames>Yapeng</forenames></author></authors><title>Do Audio-Visual Segmentation Models Truly Segment Sounding Objects?</title><categories>cs.SD cs.AI cs.LG cs.MM eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unlike traditional visual segmentation, audio-visual segmentation (AVS) requires the model not only to identify and segment objects but also to determine whether they are sound sources. Recent AVS approaches, leveraging transformer architectures and powerful foundation models like SAM, have achieved impressive performance on standard benchmarks. Yet, an important question remains: Do these models genuinely integrate audio-visual cues to segment sounding objects? In this paper, we systematically investigate this issue in the context of robust AVS. Our study reveals a fundamental bias in current methods: they tend to generate segmentation masks based predominantly on visual salience, irrespective of the audio context. This bias results in unreliable predictions when sounds are absent or irrelevant. To address this challenge, we introduce AVSBench-Robust, a comprehensive benchmark incorporating diverse negative audio scenarios including silence, ambient noise, and off-screen sounds. We also propose a simple yet effective approach combining balanced training with negative samples and classifier-guided similarity learning. Our extensive experiments show that state-of-theart AVS methods consistently fail under negative audio conditions, demonstrating the prevalence of visual bias. In contrast, our approach achieves remarkable improvements in both standard metrics and robustness measures, maintaining near-perfect false positive rates while preserving highquality segmentation performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00359</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00359</id><created>2025-02-01</created><authors><author><keyname>Xu</keyname><forenames>Wanghan</forenames></author><author><keyname>Yue</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Wang</keyname><forenames>Zidong</forenames></author><author><keyname>Teng</keyname><forenames>Yao</forenames></author><author><keyname>Zhang</keyname><forenames>Wenlong</forenames></author><author><keyname>Liu</keyname><forenames>Xihui</forenames></author><author><keyname>Zhou</keyname><forenames>Luping</forenames></author><author><keyname>Ouyang</keyname><forenames>Wanli</forenames></author><author><keyname>Bai</keyname><forenames>Lei</forenames></author></authors><title>Exploring Representation-Aligned Latent Space for Better Generation</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Generative models serve as powerful tools for modeling the real world, with mainstream diffusion models, particularly those based on the latent diffusion model paradigm, achieving remarkable progress across various tasks, such as image and video synthesis. Latent diffusion models are typically trained using Variational Autoencoders (VAEs), interacting with VAE latents rather than the real samples. While this generative paradigm speeds up training and inference, the quality of the generated outputs is limited by the latents' quality. Traditional VAE latents are often seen as spatial compression in pixel space and lack explicit semantic representations, which are essential for modeling the real world. In this paper, we introduce ReaLS (Representation-Aligned Latent Space), which integrates semantic priors to improve generation performance. Extensive experiments show that fundamental DiT and SiT trained on ReaLS can achieve a 15% improvement in FID metric. Furthermore, the enhanced semantic latent space enables more perceptual downstream tasks, such as segmentation and depth estimation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00360</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00360</id><created>2025-02-01</created><authors><author><keyname>Li</keyname><forenames>Liangchen</forenames></author><author><keyname>Wang</keyname><forenames>Caoliwen</forenames></author><author><keyname>Zhou</keyname><forenames>Yuqi</forenames></author><author><keyname>Deng</keyname><forenames>Bailin</forenames></author><author><keyname>Zhang</keyname><forenames>Juyong</forenames></author></authors><title>Shape from Semantics: 3D Shape Generation from Multi-View Semantics</title><categories>cs.CV cs.GR</categories><comments>Project page: https://shapefromsemantics.github.io</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose ``Shape from Semantics'', which is able to create 3D models whose geometry and appearance match given semantics when observed from different views. Traditional ``Shape from X'' tasks usually use visual input (e.g., RGB images or depth maps) to reconstruct geometry, imposing strict constraints that limit creative explorations. As applications, works like Shadow Art and Wire Art often struggle to grasp the embedded semantics of their design through direct observation and rely heavily on specific setups for proper display. To address these limitations, our framework uses semantics as input, greatly expanding the design space to create objects that integrate multiple semantic elements and are easily discernible by observers. Considering that this task requires a rich imagination, we adopt various generative models and structure-to-detail pipelines. Specifically, we adopt multi-semantics Score Distillation Sampling (SDS) to distill 3D geometry and appearance from 2D diffusion models, ensuring that the initial shape is consistent with the semantic input. We then use image restoration and video generation models to add more details as supervision. Finally, we introduce neural signed distance field (SDF) representation to achieve detailed shape reconstruction. Our framework generates meshes with complex details, well-structured geometry, coherent textures, and smooth transitions, resulting in visually appealing and eye-catching designs. Project page: https://shapefromsemantics.github.io </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00361</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00361</id><created>2025-02-01</created><authors><author><keyname>Ma</keyname><forenames>Haitong</forenames></author><author><keyname>Chen</keyname><forenames>Tianyi</forenames></author><author><keyname>Wang</keyname><forenames>Kai</forenames></author><author><keyname>Li</keyname><forenames>Na</forenames></author><author><keyname>Dai</keyname><forenames>Bo</forenames></author></authors><title>Soft Diffusion Actor-Critic: Efficient Online Reinforcement Learning for   Diffusion Policy</title><categories>cs.LG</categories><comments>19 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diffusion policies have achieved superior performance in imitation learning and offline reinforcement learning (RL) due to their rich expressiveness. However, the vanilla diffusion training procedure requires samples from target distribution, which is impossible in online RL since we cannot sample from the optimal policy, making training diffusion policies highly non-trivial in online RL. Backpropagating policy gradient through the diffusion process incurs huge computational costs and instability, thus being expensive and impractical. To enable efficient diffusion policy training for online RL, we propose Soft Diffusion Actor-Critic (SDAC), exploiting the viewpoint of diffusion models as noise-perturbed energy-based models. The proposed SDAC relies solely on the state-action value function as the energy functions to train diffusion policies, bypassing sampling from the optimal policy while maintaining lightweight computations. We conducted comprehensive comparisons on MuJoCo benchmarks. The empirical results show that SDAC outperforms all recent diffusion-policy online RLs on most tasks, and improves more than 120% over soft actor-critic on complex locomotion tasks such as Humanoid and Ant. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00362</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00362</id><created>2025-02-01</created><authors><author><keyname>Uotila</keyname><forenames>Valter</forenames></author></authors><title>Left-Deep Join Order Selection with Higher-Order Unconstrained Binary   Optimization on Quantum Computers</title><categories>quant-ph cs.DB</categories><comments>28 pages, 18 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Join order optimization is among the most crucial query optimization problems, and its central position is also evident in the new research field where quantum computing is applied to database optimization and data management. In the field, join order optimization is the most studied database problem, usually tackled with a quadratic unconstrained binary optimization model, which is solved with various meta-heuristics such as quantum annealing, quantum approximate optimization algorithm, or variational quantum eigensolver. In this work, we continue developing quantum computing techniques for join order optimization by presenting three novel quantum optimization algorithms. These algorithms are based on a higher-order unconstrained binary optimization model, which is a generalization of the quadratic model and has not previously been applied to database problems. Theoretically, these optimization problems naturally map to universal quantum computers and quantum annealers. Compared to previous research, two of our algorithms are the first quantum algorithms to precisely model the join order cost function. We prove theoretical bounds by showing that these two methods encode the same plans as the dynamic programming algorithm without cross-products, which provides the optimal result up to cross-products. The third algorithm reaches at least as good plans as the greedy algorithm without cross-products. These results set an important theoretical connection between the classical and quantum algorithms for join order selection, which has not been studied in the previous research. To demonstrate our algorithms' practical usability, we have conducted an experimental evaluation on thousands of clique, cycle, star, tree, and chain query graphs using quantum and classical solvers. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00363</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00363</id><created>2025-02-01</created><authors><author><keyname>Mohammadagha</keyname><forenames>Mohsen</forenames></author><author><keyname>Najafi</keyname><forenames>Mohammad</forenames></author><author><keyname>Kaushal</keyname><forenames>Vinayak</forenames></author><author><keyname>Jibreen</keyname><forenames>Ahmad Mahmoud Ahmad</forenames></author></authors><title>Machine Learning Models for Reinforced Concrete Pipes Condition   Prediction: The State-of-the-Art Using Artificial Neural Networks and   Multiple Linear Regression in a Wisconsin Case Study</title><categories>cs.LG cond-mat.mtrl-sci</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aging sewer infrastructure in the U.S., covering 2.1 million kilometers, encounters increasing structural issues, resulting in around 75,000 yearly sanitary sewer overflows that present serious economic, environmental, and public health hazards. Conventional inspection techniques and deterministic models do not account for the unpredictable nature of sewer decline, whereas probabilistic methods depend on extensive historical data, which is frequently lacking or incomplete. This research intends to enhance predictive accuracy for the condition of sewer pipelines through machine learning models artificial neural networks (ANNs) and multiple linear regression (MLR) by integrating factors such as pipe age, material, diameter, environmental influences, and PACP ratings. ANNs utilized ReLU activation functions and Adam optimization, whereas MLR applied regularization to address multicollinearity, with both models assessed through metrics like RMSE, MAE, and R2. The findings indicated that ANNs surpassed MLR, attaining an R2 of 0.9066 compared to MLRs 0.8474, successfully modeling nonlinear relationships while preserving generalization. MLR, on the other hand, offered enhanced interpretability by pinpointing significant predictors such as residual buildup. As a result, pipeline degradation is driven by pipe length, age, and pipe diameter as key predictors, while depth, soil type, and segment show minimal influence in this analysis. Future studies ought to prioritize hybrid models that merge the accuracy of ANNs with the interpretability of MLR, incorporating advanced methods such as SHAP analysis and transfer learning to improve scalability in managing infrastructure and promoting environmental sustainability. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00365</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00365</id><created>2025-02-01</created><authors><author><keyname>Romero-Alvarado</keyname><forenames>Daniel</forenames></author><author><keyname>Martínez-Plumed</keyname><forenames>Fernando</forenames></author><author><keyname>Hernández-Orallo</keyname><forenames>José</forenames></author></authors><title>What should an AI assessor optimise for?</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  An AI assessor is an external, ideally indepen-dent system that predicts an indicator, e.g., a loss value, of another AI system. Assessors can lever-age information from the test results of many other AI systems and have the flexibility of be-ing trained on any loss function or scoring rule: from squared error to toxicity metrics. Here we address the question: is it always optimal to train the assessor for the target metric? Or could it be better to train for a different metric and then map predictions back to the target metric? Us-ing twenty regression and classification problems with tabular data, we experimentally explore this question for, respectively, regression losses and classification scores with monotonic and non-monotonic mappings and find that, contrary to intuition, optimising for more informative met-rics is not generally better. Surprisingly, some monotonic transformations are promising. For example, the logistic loss is useful for minimis-ing absolute or quadratic errors in regression, and the logarithmic score helps maximise quadratic or spherical scores in classification. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00366</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00366</id><created>2025-02-01</created><authors><author><keyname>Lee</keyname><forenames>Jeong Hoon</forenames></author><author><keyname>Li</keyname><forenames>Cynthia Xinran</forenames></author><author><keyname>Jahanandish</keyname><forenames>Hassan</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Indrani</forenames></author><author><keyname>Vesal</keyname><forenames>Sulaiman</forenames></author><author><keyname>Zhang</keyname><forenames>Lichun</forenames></author><author><keyname>Sang</keyname><forenames>Shengtian</forenames></author><author><keyname>Choi</keyname><forenames>Moon Hyung</forenames></author><author><keyname>Soerensen</keyname><forenames>Simon John Christoph</forenames></author><author><keyname>Zhou</keyname><forenames>Steve Ran</forenames></author><author><keyname>Sommer</keyname><forenames>Elijah Richard</forenames></author><author><keyname>Fan</keyname><forenames>Richard</forenames></author><author><keyname>Ghanouni</keyname><forenames>Pejman</forenames></author><author><keyname>Song</keyname><forenames>Yuze</forenames></author><author><keyname>Seibert</keyname><forenames>Tyler M.</forenames></author><author><keyname>Sonn</keyname><forenames>Geoffrey A.</forenames></author><author><keyname>Rusu</keyname><forenames>Mirabela</forenames></author></authors><title>Prostate-Specific Foundation Models for Enhanced Detection of Clinically   Significant</title><categories>eess.IV cs.CV</categories><comments>44pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Accurate prostate cancer diagnosis remains challenging. Even when using MRI, radiologists exhibit low specificity and significant inter-observer variability, leading to potential delays or inaccuracies in identifying clinically significant cancers. This leads to numerous unnecessary biopsies and risks of missing clinically significant cancers. Here we present prostate vision contrastive network (ProViCNet), prostate organ-specific vision foundation models for Magnetic Resonance Imaging (MRI) and Trans-Rectal Ultrasound imaging (TRUS) for comprehensive cancer detection. ProViCNet was trained and validated using 4,401 patients across six institutions, as a prostate cancer detection model on radiology images relying on patch-level contrastive learning guided by biopsy confirmed radiologist annotations. ProViCNet demonstrated consistent performance across multiple internal and external validation cohorts with area under the receiver operating curve values ranging from 0.875 to 0.966, significantly outperforming radiologists in the reader study (0.907 versus 0.805, p&lt;0.001) for mpMRI, while achieving 0.670 to 0.740 for TRUS. We also integrated ProViCNet with standard PSA to develop a virtual screening test, and we showed that we can maintain the high sensitivity for detecting clinically significant cancers while more than doubling specificity from 15% to 38% (p&lt;0.001), thereby substantially reducing unnecessary biopsies. These findings highlight that ProViCNet's potential for enhancing prostate cancer diagnosis accuracy and reduce unnecessary biopsies, thereby optimizing diagnostic pathways. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00372</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00372</id><created>2025-02-01</created><authors><author><keyname>Cai</keyname><forenames>Zhixi</forenames></author><author><keyname>Ke</keyname><forenames>Fucai</forenames></author><author><keyname>Jahangard</keyname><forenames>Simindokht</forenames></author><author><keyname>de la Banda</keyname><forenames>Maria Garcia</forenames></author><author><keyname>Haffari</keyname><forenames>Reza</forenames></author><author><keyname>Stuckey</keyname><forenames>Peter J.</forenames></author><author><keyname>Rezatofighi</keyname><forenames>Hamid</forenames></author></authors><title>NAVER: A Neuro-Symbolic Compositional Automaton for Visual Grounding   with Explicit Logic Reasoning</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Visual Grounding (VG) tasks, such as referring expression detection and segmentation tasks are important for linking visual entities to context, especially in complex reasoning tasks that require detailed query interpretation. This paper explores VG beyond basic perception, highlighting challenges for methods that require reasoning like human cognition. Recent advances in large language methods (LLMs) and Vision-Language methods (VLMs) have improved abilities for visual comprehension, contextual understanding, and reasoning. These methods are mainly split into end-to-end and compositional methods, with the latter offering more flexibility. Compositional approaches that integrate LLMs and foundation models show promising performance but still struggle with complex reasoning with language-based logical representations. To address these limitations, we propose NAVER, a compositional visual grounding method that integrates explicit probabilistic logic reasoning within a finite-state automaton, equipped with a self-correcting mechanism. This design improves robustness and interpretability in inference through explicit logic reasoning. Our results show that NAVER achieves SoTA performance comparing to recent end-to-end and compositional baselines. The code is available at https://github.com/ControlNet/NAVER . </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00373</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00373</id><created>2025-02-01</created><authors><author><keyname>Wang</keyname><forenames>Amy Xiang</forenames></author><author><keyname>Shumaylov</keyname><forenames>Zakhar</forenames></author><author><keyname>Zaika</keyname><forenames>Peter</forenames></author><author><keyname>Sherry</keyname><forenames>Ferdia</forenames></author><author><keyname>Schönlieb</keyname><forenames>Carola-Bibiane</forenames></author></authors><title>Generalized Lie Symmetries in Physics-Informed Neural Operators</title><categories>cs.LG physics.comp-ph</categories><comments>SCML 2025 Oral</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Physics-informed neural operators (PINOs) have emerged as powerful tools for learning solution operators of partial differential equations (PDEs). Recent research has demonstrated that incorporating Lie point symmetry information can significantly enhance the training efficiency of PINOs, primarily through techniques like data, architecture, and loss augmentation. In this work, we focus on the latter, highlighting that point symmetries oftentimes result in no training signal, limiting their effectiveness in many problems. To address this, we propose a novel loss augmentation strategy that leverages evolutionary representatives of point symmetries, a specific class of generalized symmetries of the underlying PDE. These generalized symmetries provide a richer set of generators compared to standard symmetries, leading to a more informative training signal. We demonstrate that leveraging evolutionary representatives enhances the performance of neural operators, resulting in improved data efficiency and accuracy during training. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00374</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00374</id><created>2025-02-01</created><authors><author><keyname>Min</keyname><forenames>Anna</forenames></author><author><keyname>Hu</keyname><forenames>Chenxu</forenames></author><author><keyname>Ren</keyname><forenames>Yi</forenames></author><author><keyname>Zhao</keyname><forenames>Hang</forenames></author></authors><title>A Unit-based System and Dataset for Expressive Direct Speech-to-Speech   Translation</title><categories>cs.CL cs.CV cs.MM cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current research in speech-to-speech translation (S2ST) primarily concentrates on translation accuracy and speech naturalness, often overlooking key elements like paralinguistic information, which is essential for conveying emotions and attitudes in communication. To address this, our research introduces a novel, carefully curated multilingual dataset from various movie audio tracks. Each dataset pair is precisely matched for paralinguistic information and duration. We enhance this by integrating multiple prosody transfer techniques, aiming for translations that are accurate, natural-sounding, and rich in paralinguistic details. Our experimental results confirm that our model retains more paralinguistic information from the source speech while maintaining high standards of translation accuracy and naturalness. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00375</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00375</id><created>2025-02-01</created><authors><author><keyname>Duong</keyname><forenames>Anh-Kiet</forenames></author><author><keyname>Gomez-Krämer</keyname><forenames>Petra</forenames></author></authors><title>Scalable Framework for Classifying AI-Generated Content Across   Modalities</title><categories>cs.CV</categories><comments>12 pages, Defactify4 @ AAAI 2025</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The rapid growth of generative AI technologies has heightened the importance of effectively distinguishing between human and AI-generated content, as well as classifying outputs from diverse generative models. This paper presents a scalable framework that integrates perceptual hashing, similarity measurement, and pseudo-labeling to address these challenges. Our method enables the incorporation of new generative models without retraining, ensuring adaptability and robustness in dynamic scenarios. Comprehensive evaluations on the Defactify4 dataset demonstrate competitive performance in text and image classification tasks, achieving high accuracy across both distinguishing human and AI-generated content and classifying among generative methods. These results highlight the framework's potential for real-world applications as generative AI continues to evolve. Source codes are publicly available at https://github.com/ffyyytt/defactify4. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00376</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00376</id><created>2025-02-01</created><authors><author><keyname>Rehman</keyname><forenames>Abdul</forenames></author><author><keyname>Heldal</keyname><forenames>Ilona</forenames></author><author><keyname>Lin</keyname><forenames>Jerry Chun-Wei</forenames></author></authors><title>SSRepL-ADHD: Adaptive Complex Representation Learning Framework for ADHD   Detection from Visual Attention Tasks</title><categories>cs.LG cs.HC eess.SP</categories><journal-ref>2024 IEEE International Conference on Big Data (BigData)</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Self Supervised Representation Learning (SSRepL) can capture meaningful and robust representations of the Attention Deficit Hyperactivity Disorder (ADHD) data and have the potential to improve the model's performance on also downstream different types of Neurodevelopmental disorder (NDD) detection. In this paper, a novel SSRepL and Transfer Learning (TL)-based framework that incorporates a Long Short-Term Memory (LSTM) and a Gated Recurrent Units (GRU) model is proposed to detect children with potential symptoms of ADHD. This model uses Electroencephalogram (EEG) signals extracted during visual attention tasks to accurately detect ADHD by preprocessing EEG signal quality through normalization, filtering, and data balancing. For the experimental analysis, we use three different models: 1) SSRepL and TL-based LSTM-GRU model named as SSRepL-ADHD, which integrates LSTM and GRU layers to capture temporal dependencies in the data, 2) lightweight SSRepL-based DNN model (LSSRepL-DNN), and 3) Random Forest (RF). In the study, these models are thoroughly evaluated using well-known performance metrics (i.e., accuracy, precision, recall, and F1-score). The results show that the proposed SSRepL-ADHD model achieves the maximum accuracy of 81.11% while admitting the difficulties associated with dataset imbalance and feature selection. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00377</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00377</id><created>2025-02-01</created><authors><author><keyname>Min</keyname><forenames>Anna</forenames></author><author><keyname>Hu</keyname><forenames>Chenxu</forenames></author><author><keyname>Ren</keyname><forenames>Yi</forenames></author><author><keyname>Zhao</keyname><forenames>Hang</forenames></author></authors><title>When End-to-End is Overkill: Rethinking Cascaded Speech-to-Text   Translation</title><categories>cs.CL cs.AI cs.MM cs.SD eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Though end-to-end speech-to-text translation has been a great success, we argue that the cascaded speech-to-text translation model still has its place, which is usually criticized for the error propagation between automatic speech recognition (ASR) and machine translation (MT) models. In this paper, we explore the benefits of incorporating multiple candidates from ASR and self-supervised speech features into MT. Our analysis reveals that the primary cause of cascading errors stems from the increased divergence between similar samples in the speech domain when mapped to the text domain. By including multiple candidates and self-supervised speech features, our approach allows the machine translation model to choose the right words and ensure precise translation using various speech samples. This strategy minimizes error spread and takes advantage of large ASR and MT datasets, along with pre-trained ASR/MT models, while addressing associated issues. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00379</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00379</id><created>2025-02-01</created><authors><author><keyname>Nikulin</keyname><forenames>Alexander</forenames></author><author><keyname>Zisman</keyname><forenames>Ilya</forenames></author><author><keyname>Tarasov</keyname><forenames>Denis</forenames></author><author><keyname>Lyubaykin</keyname><forenames>Nikita</forenames></author><author><keyname>Polubarov</keyname><forenames>Andrei</forenames></author><author><keyname>Kiselev</keyname><forenames>Igor</forenames></author><author><keyname>Kurenkov</keyname><forenames>Vladislav</forenames></author></authors><title>Latent Action Learning Requires Supervision in the Presence of   Distractors</title><categories>cs.CV cs.AI cs.LG</categories><comments>Preprint. In review</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recently, latent action learning, pioneered by Latent Action Policies (LAPO), have shown remarkable pre-training efficiency on observation-only data, offering potential for leveraging vast amounts of video available on the web for embodied AI. However, prior work has focused on distractor-free data, where changes between observations are primarily explained by ground-truth actions. Unfortunately, real-world videos contain action-correlated distractors that may hinder latent action learning. Using Distracting Control Suite (DCS) we empirically investigate the effect of distractors on latent action learning and demonstrate that LAPO struggle in such scenario. We propose LAOM, a simple LAPO modification that improves the quality of latent actions by 8x, as measured by linear probing. Importantly, we show that providing supervision with ground-truth actions, as few as 2.5% of the full dataset, during latent action learning improves downstream performance by 4.2x on average. Our findings suggest that integrating supervision during Latent Action Models (LAM) training is critical in the presence of distractors, challenging the conventional pipeline of first learning LAM and only then decoding from latent to ground-truth actions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00380</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00380</id><created>2025-02-01</created><authors><author><keyname>Belucci</keyname><forenames>Bruno</forenames></author><author><keyname>Lounici</keyname><forenames>Karim</forenames></author><author><keyname>Meziani</keyname><forenames>Katia</forenames></author></authors><title>CoHiRF: A Scalable and Interpretable Clustering Framework for   High-Dimensional Data</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Clustering high-dimensional data poses significant challenges due to the curse of dimensionality, scalability issues, and the presence of noisy and irrelevant features. We propose Consensus Hierarchical Random Feature (CoHiRF), a novel clustering method designed to address these challenges effectively. CoHiRF leverages random feature selection to mitigate noise and dimensionality effects, repeatedly applies K-Means clustering in reduced feature spaces, and combines results through a unanimous consensus criterion. This iterative approach constructs a cluster assignment matrix, where each row records the cluster assignments of a sample across repetitions, enabling the identification of stable clusters by comparing identical rows. Clusters are organized hierarchically, enabling the interpretation of the hierarchy to gain insights into the dataset. CoHiRF is computationally efficient with a running time comparable to K-Means, scalable to massive datasets, and exhibits robust performance against state-of-the-art methods such as SC-SRGF, HDBSCAN, and OPTICS. Experimental results on synthetic and real-world datasets confirm the method's ability to reveal meaningful patterns while maintaining scalability, making it a powerful tool for high-dimensional data analysis. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00381</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00381</id><created>2025-02-01</created><authors><author><keyname>Rehman</keyname><forenames>Abdul</forenames></author><author><keyname>Heldal</keyname><forenames>Ilona</forenames></author><author><keyname>Stilwell</keyname><forenames>Diana</forenames></author><author><keyname>Lin</keyname><forenames>Jerry Chun-Wei</forenames></author></authors><title>Towards a Supporting Framework for Neuro-Developmental Disorder:   Considering Artificial Intelligence, Serious Games and Eye Tracking</title><categories>cs.HC</categories><journal-ref>2024 IEEE International Conference on Big Data (BigData)</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper focuses on developing a framework for uncovering insights about NDD children's performance (e.g., raw gaze cluster analysis, duration analysis \&amp; area of interest for sustained attention, stimuli expectancy, loss of focus/motivation, inhibitory control) and informing their teachers. The hypothesis behind this work is that self-adaptation of games can contribute to improving students' well-being and performance by suggesting personalized activities (e.g., highlighting stimuli to increase attention or choosing a difficulty level that matches students' abilities). The aim is to examine how AI can be used to help solve this problem. The results would not only contribute to a better understanding of the problems of NDD children and their teachers but also help psychologists to validate the results against their clinical knowledge, improve communication with patients and identify areas for further investigation, e.g., by explaining the decision made and preserving the children's private data in the learning process. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00382</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00382</id><created>2025-02-01</created><authors><author><keyname>Goyal</keyname><forenames>Sahil</forenames></author><author><keyname>Tula</keyname><forenames>Debapriya</forenames></author><author><keyname>Jain</keyname><forenames>Gagan</forenames></author><author><keyname>Shenoy</keyname><forenames>Pradeep</forenames></author><author><keyname>Jain</keyname><forenames>Prateek</forenames></author><author><keyname>Paul</keyname><forenames>Sujoy</forenames></author></authors><title>Masked Generative Nested Transformers with Decode Time Scaling</title><categories>cs.CV cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Recent advances in visual generation have made significant strides in producing content of exceptional quality. However, most methods suffer from a fundamental problem - a bottleneck of inference computational efficiency. Most of these algorithms involve multiple passes over a transformer model to generate tokens or denoise inputs. However, the model size is kept consistent throughout all iterations, which makes it computationally expensive. In this work, we aim to address this issue primarily through two key ideas - (a) not all parts of the generation process need equal compute, and we design a decode time model scaling schedule to utilize compute effectively, and (b) we can cache and reuse some of the computation. Combining these two ideas leads to using smaller models to process more tokens while large models process fewer tokens. These different-sized models do not increase the parameter size, as they share parameters. We rigorously experiment with ImageNet256$\times$256 , UCF101, and Kinetics600 to showcase the efficacy of the proposed method for image/video generation and frame prediction. Our experiments show that with almost $3\times$ less compute than baseline, our model obtains competitive performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00384</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00384</id><created>2025-02-01</created><authors><author><keyname>Karayalçin</keyname><forenames>Sengim</forenames></author><author><keyname>Krček</keyname><forenames>Marina</forenames></author><author><keyname>Picek</keyname><forenames>Stjepan</forenames></author></authors><title>It's Not Just a Phase: On Investigating Phase Transitions in Deep   Learning-based Side-channel Analysis</title><categories>cs.CR cs.LG</categories><comments>17 pages, 13 figures, 1 table</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Side-channel analysis (SCA) represents a realistic threat where the attacker can observe unintentional information to obtain secret data. Evaluation labs also use the same SCA techniques in the security certification process. The results in the last decade have shown that machine learning, especially deep learning, is an extremely powerful SCA approach, allowing the breaking of protected devices while achieving optimal attack performance. Unfortunately, deep learning operates as a black-box, making it less useful for security evaluators who must understand how attacks work to prevent them in the future. This work demonstrates that mechanistic interpretability can effectively scale to realistic scenarios where relevant information is sparse and well-defined interchange interventions to the input are impossible due to side-channel protections. Concretely, we reverse engineer the features the network learns during phase transitions, eventually retrieving secret masks, allowing us to move from black-box to white-box evaluation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00385</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00385</id><created>2025-02-01</created><authors><author><keyname>Civelli</keyname><forenames>Stefano</forenames></author><author><keyname>Bernardelle</keyname><forenames>Pietro</forenames></author><author><keyname>Demartini</keyname><forenames>Gianluca</forenames></author></authors><title>The Impact of Persona-based Political Perspectives on Hateful Content   Detection</title><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While pretraining language models with politically diverse content has been shown to improve downstream task fairness, such approaches require significant computational resources often inaccessible to many researchers and organizations. Recent work has established that persona-based prompting can introduce political diversity in model outputs without additional training. However, it remains unclear whether such prompting strategies can achieve results comparable to political pretraining for downstream tasks. We investigate this question using persona-based prompting strategies in multimodal hate-speech detection tasks, specifically focusing on hate speech in memes. Our analysis reveals that when mapping personas onto a political compass and measuring persona agreement, inherent political positioning has surprisingly little correlation with classification decisions. Notably, this lack of correlation persists even when personas are explicitly injected with stronger ideological descriptors. Our findings suggest that while LLMs can exhibit political biases in their responses to direct political questions, these biases may have less impact on practical classification tasks than previously assumed. This raises important questions about the necessity of computationally expensive political pretraining for achieving fair performance in downstream tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00386</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00386</id><created>2025-02-01</created><authors><author><keyname>Zhang</keyname><forenames>Wenzhen</forenames></author><author><keyname>Cheng</keyname><forenames>Debo</forenames></author><author><keyname>Lu</keyname><forenames>Guangquan</forenames></author><author><keyname>Zhou</keyname><forenames>Bo</forenames></author><author><keyname>Li</keyname><forenames>Jiaye</forenames></author><author><keyname>Zhang</keyname><forenames>Shichao</forenames></author></authors><title>Efficient Adaptive Label Refinement for Label Noise Learning</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks are highly susceptible to overfitting noisy labels, which leads to degraded performance. Existing methods address this issue by employing manually defined criteria, aiming to achieve optimal partitioning in each iteration to avoid fitting noisy labels while thoroughly learning clean samples. However, this often results in overly complex and difficult-to-train models. To address this issue, we decouple the tasks of avoiding fitting incorrect labels and thoroughly learning clean samples and propose a simple yet highly applicable method called Adaptive Label Refinement (ALR). First, inspired by label refurbishment techniques, we update the original hard labels to soft labels using the model's predictions to reduce the risk of fitting incorrect labels. Then, by introducing the entropy loss, we gradually `harden' the high-confidence soft labels, guiding the model to better learn from clean samples. This approach is simple and efficient, requiring no prior knowledge of noise or auxiliary datasets, making it more accessible compared to existing methods. We validate ALR's effectiveness through experiments on benchmark datasets with artificial label noise (CIFAR-10/100) and real-world datasets with inherent noise (ANIMAL-10N, Clothing1M, WebVision). The results show that ALR outperforms state-of-the-art methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00388</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00388</id><created>2025-02-01</created><authors><author><keyname>Caviola</keyname><forenames>Lucius</forenames></author></authors><title>The Societal Response to Potentially Sentient AI</title><categories>cs.CY</categories><comments>36 pages, 4 figures, 2 tables, 3 informational boxes</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We may soon develop highly human-like AIs that appear-or perhaps even are-sentient, capable of subjective experiences such as happiness and suffering. Regardless of whether AI can achieve true sentience, it is crucial to anticipate and understand how the public and key decision-makers will respond, as their perceptions will shape the future of both humanity and AI. Currently, public skepticism about AI sentience remains high. However, as AI systems advance and become increasingly skilled at human-like interactions, public attitudes may shift. Future AI systems designed to fulfill social needs could foster deep emotional connections with users, potentially influencing perceptions of their sentience and moral status. A key question is whether public beliefs about AI sentience will diverge from expert opinions, given the potential mismatch between an AI's internal mechanisms and its outward behavior. Given the profound difficulty of determining AI sentience, society might face a period of uncertainty, disagreement, and even conflict over questions of AI sentience and rights. To navigate these challenges responsibly, further social science research is essential to explore how society will perceive and engage with potentially sentient AI. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00392</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00392</id><created>2025-02-01</created><authors><author><keyname>Sun</keyname><forenames>Zhichao</forenames></author><author><keyname>Liu</keyname><forenames>Yepeng</forenames></author><author><keyname>Zhu</keyname><forenames>Huachao</forenames></author><author><keyname>Gu</keyname><forenames>Yuliang</forenames></author><author><keyname>Zou</keyname><forenames>Yuda</forenames></author><author><keyname>Liu</keyname><forenames>Zelong</forenames></author><author><keyname>Xia</keyname><forenames>Gui-Song</forenames></author><author><keyname>Du</keyname><forenames>Bo</forenames></author><author><keyname>Xu</keyname><forenames>Yongchao</forenames></author></authors><title>RefDrone: A Challenging Benchmark for Referring Expression Comprehension   in Drone Scenes</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Drones have become prevalent robotic platforms with diverse applications, showing significant potential in Embodied Artificial Intelligence (Embodied AI). Referring Expression Comprehension (REC) enables drones to locate objects based on natural language expressions, a crucial capability for Embodied AI. Despite advances in REC for ground-level scenes, aerial views introduce unique challenges including varying viewpoints, occlusions and scale variations. To address this gap, we introduce RefDrone, a REC benchmark for drone scenes. RefDrone reveals three key challenges in REC: 1) multi-scale and small-scale target detection; 2) multi-target and no-target samples; 3) complex environment with rich contextual expressions. To efficiently construct this dataset, we develop RDAgent (referring drone annotation framework with multi-agent system), a semi-automated annotation tool for REC tasks. RDAgent ensures high-quality contextual expressions and reduces annotation cost. Furthermore, we propose Number GroundingDINO (NGDINO), a novel method designed to handle multi-target and no-target cases. NGDINO explicitly learns and utilizes the number of objects referred to in the expression. Comprehensive experiments with state-of-the-art REC methods demonstrate that NGDINO achieves superior performance on both the proposed RefDrone and the existing gRefCOCO datasets. The dataset and code will be publicly at https://github.com/sunzc-sunny/refdrone. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00393</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00393</id><created>2025-02-01</created><authors><author><keyname>Haji-Ali</keyname><forenames>Abdul-Lateef</forenames></author><author><keyname>Hoel</keyname><forenames>Håkon</forenames></author><author><keyname>Petersson</keyname><forenames>Andreas</forenames></author></authors><title>The multi-index Monte Carlo method for semilinear stochastic partial   differential equations</title><categories>math.NA cs.NA math.PR</categories><msc-class>60H15, 65C05, 60H35, 65Y20, 35K58</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic partial differential equations (SPDEs) are often difficult to solve numerically due to their low regularity and high dimensionality. These challenges limit the practical use of computer-aided studies and pose significant barriers to statistical analysis of SPDEs. In this work, we introduce a highly efficient multi-index Monte Carlo method (MIMC) designed to approximate statistics of mild solutions to semilinear parabolic SPDEs. Key to our approach is the proof of a multiplicative convergence property for coupled solutions generated by an exponential integrator numerical solver, which we incorporate with MIMC. We further describe theoretically how the asymptotic computational cost of MIMC can be bounded in terms of the input accuracy tolerance, as the tolerance goes to zero. Notably, our methodology illustrates that for an SPDE with low regularity, MIMC offers substantial performance improvements over other viable methods. Numerical experiments comparing the performance of MIMC with the multilevel Monte Carlo method on relevant test problems validate our theoretical findings. These results also demonstrate that MIMC significantly outperforms state-of-the-art multilevel Monte Carlo, thereby underscoring its potential as a robust and tractable tool for solving semilinear parabolic SPDEs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00395</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00395</id><created>2025-02-01</created><authors><author><keyname>Leitenstern</keyname><forenames>Maximilian</forenames></author><author><keyname>Alten</keyname><forenames>Marko</forenames></author><author><keyname>Bolea-Schaser</keyname><forenames>Christian</forenames></author><author><keyname>Kulmer</keyname><forenames>Dominik</forenames></author><author><keyname>Weinmann</keyname><forenames>Marcel</forenames></author><author><keyname>Lienkamp</keyname><forenames>Markus</forenames></author></authors><title>FlexCloud: Direct, Modular Georeferencing and Drift-Correction of Point   Cloud Maps</title><categories>cs.RO cs.CV</categories><comments>Accepted for publication at VEHITS 2025, Proceedings of the 11th   International Conference on Vehicle Technology and Intelligent Transport   Systems - VEHITS; 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Current software stacks for real-world applications of autonomous driving leverage map information to ensure reliable localization, path planning, and motion prediction. An important field of research is the generation of point cloud maps, referring to the topic of simultaneous localization and mapping (SLAM). As most recent developments do not include global position data, the resulting point cloud maps suffer from internal distortion and missing georeferencing, preventing their use for map-based localization approaches. Therefore, we propose FlexCloud for an automatic georeferencing of point cloud maps created from SLAM. Our approach is designed to work modularly with different SLAM methods, utilizing only the generated local point cloud map and its odometry. Using the corresponding GNSS positions enables direct georeferencing without additional control points. By leveraging a 3D rubber-sheet transformation, we can correct distortions within the map caused by long-term drift while maintaining its structure. Our approach enables the creation of consistent, globally referenced point cloud maps from data collected by a mobile mapping system (MMS). The source code of our work is available at https://github.com/TUMFTM/FlexCloud. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00396</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00396</id><created>2025-02-01</created><authors><author><keyname>Zhaole</keyname><forenames>Sun</forenames></author><author><keyname>Gao</keyname><forenames>Xiao</forenames></author><author><keyname>Mao</keyname><forenames>Xiaofeng</forenames></author><author><keyname>Zhu</keyname><forenames>Jihong</forenames></author><author><keyname>Billard</keyname><forenames>Aude</forenames></author><author><keyname>Fisher</keyname><forenames>Robert B.</forenames></author></authors><title>Dexterous Cable Manipulation: Taxonomy, Multi-Fingered Hand Design, and   Long-Horizon Manipulation</title><categories>cs.RO</categories><comments>17 pages, 14 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Existing research that addressed cable manipulation relied on two-fingered grippers, which make it difficult to perform similar cable manipulation tasks that humans perform. However, unlike dexterous manipulation of rigid objects, the development of dexterous cable manipulation skills in robotics remains underexplored due to the unique challenges posed by a cable's deformability and inherent uncertainty. In addition, using a dexterous hand introduces specific difficulties in tasks, such as cable grasping, pulling, and in-hand bending, for which no dedicated task definitions, benchmarks, or evaluation metrics exist. Furthermore, we observed that most existing dexterous hands are designed with structures identical to humans', typically featuring only one thumb, which often limits their effectiveness during dexterous cable manipulation. Lastly, existing non-task-specific methods did not have enough generalization ability to solve these cable manipulation tasks or are unsuitable due to the designed hardware. We have three contributions in real-world dexterous cable manipulation in the following steps: (1) We first defined and organized a set of dexterous cable manipulation tasks into a comprehensive taxonomy, covering most short-horizon action primitives and long-horizon tasks for one-handed cable manipulation. This taxonomy revealed that coordination between the thumb and the index finger is critical for cable manipulation, which decomposes long-horizon tasks into simpler primitives. (2) We designed a novel five-fingered hand with 25 degrees of freedom (DoF), featuring two symmetric thumb-index configurations and a rotatable joint on each fingertip, which enables dexterous cable manipulation. (3) We developed a demonstration collection pipeline for this non-anthropomorphic hand, which is difficult to operate by previous motion capture methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00397</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00397</id><created>2025-02-01</created><authors><author><keyname>Girmaji</keyname><forenames>Rohit</forenames></author><author><keyname>Jain</keyname><forenames>Siddharth</forenames></author><author><keyname>Beri</keyname><forenames>Bhav</forenames></author><author><keyname>Bansal</keyname><forenames>Sarthak</forenames></author><author><keyname>Gandhi</keyname><forenames>Vineet</forenames></author></authors><title>Minimalistic Video Saliency Prediction via Efficient Decoder &amp; Spatio   Temporal Action Cues</title><categories>cs.CV</categories><comments>Accepted at 2025 IEEE International Conference on Acoustics, Speech,   and Signal Processing (ICASSP 2025)</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  This paper introduces ViNet-S, a 36MB model based on the ViNet architecture with a U-Net design, featuring a lightweight decoder that significantly reduces model size and parameters without compromising performance. Additionally, ViNet-A (148MB) incorporates spatio-temporal action localization (STAL) features, differing from traditional video saliency models that use action classification backbones. Our studies show that an ensemble of ViNet-S and ViNet-A, by averaging predicted saliency maps, achieves state-of-the-art performance on three visual-only and six audio-visual saliency datasets, outperforming transformer-based models in both parameter efficiency and real-time performance, with ViNet-S reaching over 1000fps. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00399</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00399</id><created>2025-02-01</created><authors><author><keyname>Yoon</keyname><forenames>Donghyun</forenames></author><author><keyname>Jeong</keyname><forenames>Minwoo</forenames></author><author><keyname>Lee</keyname><forenames>Jinyong</forenames></author><author><keyname>Kim</keyname><forenames>Seyun</forenames></author><author><keyname>Yoon</keyname><forenames>Yoonjin</forenames></author></authors><title>Integrating Urban Air Mobility with Highway Infrastructure: A Strategic   Approach for Vertiport Location Selection in the Seoul Metropolitan Area</title><categories>cs.CY</categories><comments>24 pages</comments><journal-ref>104th Transportation Research Board Annual Meeting (2025)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study focuses on identifying suitable locations for highway-transfer Vertiports to integrate Urban Air Mobility (UAM) with existing highway infrastructure. UAM offers an effective solution for enhancing transportation accessibility in the Seoul Metropolitan Area, where conventional transportation often struggle to connect suburban employment zones such as industrial parks. By integrating UAM with ground transportation at highway facilities, an efficient connectivity solution can be achieved for regions with limited transportation options. Our proposed methodology for determining the suitable Vertiport locations utilizes data such as geographic information, origin-destination volume, and travel time. Vertiport candidates are evaluated and selected based on criteria including location desirability, combined transportation accessibility and transportation demand. Applying this methodology to the Seoul metropolitan area, we identify 56 suitable Vertiport locations out of 148 candidates. The proposed methodology offers a strategic approach for the selection of highway-transfer Vertiport locations, enhancing UAM integration with existing transportation systems. Our study provides valuable insights for urban planners and policymakers, with recommendations for future research to include real-time environmental data and to explore the impact of Mobility-as-a-Service on UAM operations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00401</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00401</id><created>2025-02-01</created><authors><author><keyname>Grover</keyname><forenames>Karish</forenames></author><author><keyname>Yu</keyname><forenames>Haiyang</forenames></author><author><keyname>Song</keyname><forenames>Xiang</forenames></author><author><keyname>Zhu</keyname><forenames>Qi</forenames></author><author><keyname>Xie</keyname><forenames>Han</forenames></author><author><keyname>Ioannidis</keyname><forenames>Vassilis N.</forenames></author><author><keyname>Faloutsos</keyname><forenames>Christos</forenames></author></authors><title>Spectro-Riemannian Graph Neural Networks</title><categories>cs.LG cs.AI stat.ML</categories><comments>ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Can integrating spectral and curvature signals unlock new potential in graph representation learning? Non-Euclidean geometries, particularly Riemannian manifolds such as hyperbolic (negative curvature) and spherical (positive curvature), offer powerful inductive biases for embedding complex graph structures like scale-free, hierarchical, and cyclic patterns. Meanwhile, spectral filtering excels at processing signal variations across graphs, making it effective in homophilic and heterophilic settings. Leveraging both can significantly enhance the learned representations. To this end, we propose Spectro-Riemannian Graph Neural Networks (CUSP) - the first graph representation learning paradigm that unifies both CUrvature (geometric) and SPectral insights. CUSP is a mixed-curvature spectral GNN that learns spectral filters to optimize node embeddings in products of constant-curvature manifolds (hyperbolic, spherical, and Euclidean). Specifically, CUSP introduces three novel components: (a) Cusp Laplacian, an extension of the traditional graph Laplacian based on Ollivier-Ricci curvature, designed to capture the curvature signals better; (b) Cusp Filtering, which employs multiple Riemannian graph filters to obtain cues from various bands in the eigenspectrum; and (c) Cusp Pooling, a hierarchical attention mechanism combined with a curvature-based positional encoding to assess the relative importance of differently curved substructures in our graph. Empirical evaluation across eight homophilic and heterophilic datasets demonstrates the superiority of CUSP in node classification and link prediction tasks, with a gain of up to 5.3% over state-of-the-art models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00402</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00402</id><created>2025-02-01</created><authors><author><keyname>Zimmer</keyname><forenames>Walter</forenames></author><author><keyname>Greer</keyname><forenames>Ross</forenames></author><author><keyname>Zhou</keyname><forenames>Xingcheng</forenames></author><author><keyname>Song</keyname><forenames>Rui</forenames></author><author><keyname>Pavel</keyname><forenames>Marc</forenames></author><author><keyname>Lehmberg</keyname><forenames>Daniel</forenames></author><author><keyname>Ghita</keyname><forenames>Ahmed</forenames></author><author><keyname>Gopalkrishnan</keyname><forenames>Akshay</forenames></author><author><keyname>Trivedi</keyname><forenames>Mohan</forenames></author><author><keyname>Knoll</keyname><forenames>Alois</forenames></author></authors><title>Enhancing Highway Safety: Accident Detection on the A9 Test Stretch   Using Roadside Sensors</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Road traffic injuries are the leading cause of death for people aged 5-29, resulting in about 1.19 million deaths each year. To reduce these fatalities, it is essential to address human errors like speeding, drunk driving, and distractions. Additionally, faster accident detection and quicker medical response can help save lives. We propose an accident detection framework that combines a rule-based approach with a learning-based one. We introduce a dataset of real-world highway accidents featuring high-speed crash sequences. It includes 294,924 labeled 2D boxes, 93,012 labeled 3D boxes, and track IDs across 48,144 frames captured at 10 Hz using four roadside cameras and LiDAR sensors. The dataset covers ten object classes and is released in the OpenLABEL format. Our experiments and analysis demonstrate the reliability of our method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00404</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00404</id><created>2025-02-01</created><authors><author><keyname>Lu</keyname><forenames>Rongchang</forenames></author><author><keyname>Li</keyname><forenames>Changyu</forenames></author><author><keyname>Li</keyname><forenames>Donghang</forenames></author><author><keyname>Zhang</keyname><forenames>Guojing</forenames></author><author><keyname>Huang</keyname><forenames>Jianqiang</forenames></author><author><keyname>Li</keyname><forenames>Xilai</forenames></author></authors><title>Exploring Linear Attention Alternative for Single Image Super-Resolution</title><categories>cs.CV eess.IV</categories><comments>This paper has been published to IEEE International Joint Conference   on Neural Networks. Feel free to contact on nomodeset@qq.com</comments><acm-class>I.4.9</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning-based single-image super-resolution (SISR) technology focuses on enhancing low-resolution (LR) images into high-resolution (HR) ones. Although significant progress has been made, challenges remain in computational complexity and quality, particularly in remote sensing image processing. To address these issues, we propose our Omni-Scale RWKV Super-Resolution (OmniRWKVSR) model which presents a novel approach that combines the Receptance Weighted Key Value (RWKV) architecture with feature extraction techniques such as Visual RWKV Spatial Mixing (VRSM) and Visual RWKV Channel Mixing (VRCM), aiming to overcome the limitations of existing methods and achieve superior SISR performance. This work has proved able to provide effective solutions for high-quality image reconstruction. Under the 4x Super-Resolution tasks, compared to the MambaIR model, we achieved an average improvement of 0.26% in PSNR and 0.16% in SSIM. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00405</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00405</id><created>2025-02-01</created><authors><author><keyname>Ren</keyname><forenames>Fengyun</forenames></author><author><keyname>Zhang</keyname><forenames>Shumin</forenames></author><author><keyname>Wang</keyname><forenames>Ke</forenames></author></authors><title>Spectral Sufficient Conditions for Graph Factors</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $\{K_{1,1}, K_{1,2},C_m: m\geq3\}$-factor of a graph is a spanning subgraph whose each component is an element of $\{K_{1,1}, K_{1,2},C_m: m\geq3\}$. In this paper, through the graph spectral methods, we establish the lower bound of the signless Laplacian spectral radius and the upper bound of the distance spectral radius to determine whether a graph admits a $\{K_2\}$-factor. We get a lower bound on the size (resp. the spectral radius) of $G$ to guarantee that $G$ contains a $\{K_{1,1}, K_{1,2},C_m: m\geq3\}$-factor. Then we determine an upper bound on the distance spectral radius of $G$ to ensure that $G$ has a $\{K_{1,1}, K_{1,2},C_m: m\geq3\}$-factor. Furthermore, by constructing extremal graphs, we show that the above all bounds are best possible. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00406</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00406</id><created>2025-02-01</created><authors><author><keyname>Sanyal</keyname><forenames>Debdeep</forenames></author><author><keyname>Mandal</keyname><forenames>Murari</forenames></author></authors><title>ALU: Agentic LLM Unlearning</title><categories>cs.AI cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Information removal or suppression in large language models (LLMs) is a desired functionality, useful in AI regulation, legal compliance, safety, and privacy. LLM unlearning methods aim to remove information on demand from LLMs. Current LLM unlearning methods struggle to balance the unlearning efficacy and utility due to the competing nature of these objectives. Keeping the unlearning process computationally feasible without assuming access to the model weights is an overlooked area. We present the first agentic LLM unlearning (ALU) method, a multi-agent, retrain-free, model-agnostic approach to LLM unlearning that achieves effective unlearning while preserving the utility. Our ALU framework unlearns by involving multiple LLM agents, each designed for a specific step in the unlearning process, without the need to update model weights for any of the agents in the framework. Users can easily request any set of unlearning instances in any sequence, and ALU seamlessly adapts in real time. This is facilitated without requiring any changes in the underlying LLM model. Through extensive experiments on established benchmarks (TOFU, WMDP, WPU) and jailbreaking techniques (many shot, target masking, other languages), we demonstrate that ALU consistently stands out as the most robust LLM unlearning framework among current state-of-the-art methods while incurring a low constant-time cost. We further highlight ALU's superior performance compared to existing methods when evaluated at scale. Specifically, ALU is assessed on up to 1000 unlearning targets, exceeding the evaluation scope of all previously proposed LLM unlearning methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00407</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00407</id><created>2025-02-01</created><authors><author><keyname>D'Acunto</keyname><forenames>Gabriele</forenames></author><author><keyname>Zennaro</keyname><forenames>Fabio Massimo</forenames></author><author><keyname>Felekis</keyname><forenames>Yorgos</forenames></author><author><keyname>Di Lorenzo</keyname><forenames>Paolo</forenames></author></authors><title>Causal Abstraction Learning based on the Semantic Embedding Principle</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structural causal models (SCMs) allow us to investigate complex systems at multiple levels of resolution. The causal abstraction (CA) framework formalizes the mapping between high- and low-level SCMs. We address CA learning in a challenging and realistic setting, where SCMs are inaccessible, interventional data is unavailable, and sample data is misaligned. A key principle of our framework is $\textit{semantic embedding}$, formalized as the high-level distribution lying on a subspace of the low-level one. This principle naturally links linear CA to the geometry of the $\textit{Stiefel manifold}$. We present a category-theoretic approach to SCMs that enables the learning of a CA by finding a morphism between the low- and high-level probability measures, adhering to the semantic embedding principle. Consequently, we formulate a general CA learning problem. As an application, we solve the latter problem for linear CA; considering Gaussian measures and the Kullback-Leibler divergence as an objective. Given the nonconvexity of the learning task, we develop three algorithms building upon existing paradigms for Riemannian optimization. We demonstrate that the proposed methods succeed on both synthetic and real-world brain data with different degrees of prior information about the structure of CA. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00408</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00408</id><created>2025-02-01</created><authors><author><keyname>Griebel</keyname><forenames>Titus</forenames></author><author><keyname>Archit</keyname><forenames>Anwai</forenames></author><author><keyname>Pape</keyname><forenames>Constantin</forenames></author></authors><title>Segment Anything for Histopathology</title><categories>eess.IV cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Nucleus segmentation is an important analysis task in digital pathology. However, methods for automatic segmentation often struggle with new data from a different distribution, requiring users to manually annotate nuclei and retrain data-specific models. Vision foundation models (VFMs), such as the Segment Anything Model (SAM), offer a more robust alternative for automatic and interactive segmentation. Despite their success in natural images, a foundation model for nucleus segmentation in histopathology is still missing. Initial efforts to adapt SAM have shown some success, but did not yet introduce a comprehensive model for diverse segmentation tasks. To close this gap, we introduce PathoSAM, a VFM for nucleus segmentation, based on training SAM on a diverse dataset. Our extensive experiments show that it is the new state-of-the-art model for automatic and interactive nucleus instance segmentation in histopathology. We also demonstrate how it can be adapted for other segmentation tasks, including semantic nucleus segmentation. For this task, we show that it yields results better than popular methods, while not yet beating the state-of-the-art, CellViT. Our models are open-source and compatible with popular tools for data annotation. We also provide scripts for whole-slide image segmentation. Our code and models are publicly available at https://github.com/computational-cell-analytics/patho-sam. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00409</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00409</id><created>2025-02-01</created><authors><author><keyname>Varangot-Reille</keyname><forenames>Clovis</forenames></author><author><keyname>Bouvard</keyname><forenames>Christophe</forenames></author><author><keyname>Gourru</keyname><forenames>Antoine</forenames></author><author><keyname>Ciancone</keyname><forenames>Mathieu</forenames></author><author><keyname>Schaeffer</keyname><forenames>Marion</forenames></author><author><keyname>Jacquenet</keyname><forenames>François</forenames></author></authors><title>Doing More with Less -- Implementing Routing Strategies in Large   Language Model-Based Systems: An Extended Survey</title><categories>cs.AI cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLM)-based systems, i.e. interconnected elements that include an LLM as a central component (e.g., conversational agents), are typically monolithic static architectures that rely on a single LLM for all user queries. However, they often require different preprocessing strategies, levels of reasoning, or knowledge. Generalist LLMs (i.e. GPT-4), trained on very large multi-topic corpora, can perform well in a variety of tasks. However, they require significant financial, energy, and hardware resources that may not be justified for basic tasks. This implies potentially investing in unnecessary costs for a given query. To overcome this problem, a routing mechanism routes user queries to the most suitable components, such as smaller LLMs or experts in specific topics. This approach may improve response quality while minimising costs. Routing can be expanded to other components of the conversational agent architecture, such as the selection of optimal embedding strategies. This paper explores key considerations for integrating routing into LLM-based systems, focusing on resource management, cost definition, and strategy selection. Our main contributions include a formalisation of the problem, a novel taxonomy of existing approaches emphasising relevance and resource efficiency, and a comparative analysis of these strategies in relation to industry practices. Finally, we identify critical challenges and directions for future research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00412</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00412</id><created>2025-02-01</created><authors><author><keyname>Wang</keyname><forenames>Ziyu</forenames></author><author><keyname>Pan</keyname><forenames>Tengyu</forenames></author><author><keyname>Li</keyname><forenames>Zhenyu</forenames></author><author><keyname>Ji</keyname><forenames>Wu</forenames></author><author><keyname>Xiuxing</keyname><forenames>Li</forenames></author><author><keyname>Wang</keyname><forenames>Jianyong</forenames></author></authors><title>TROI: Cross-Subject Pretraining with Sparse Voxel Selection for Enhanced   fMRI Visual Decoding</title><categories>cs.CV</categories><comments>ICASSP 2025</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  fMRI (functional Magnetic Resonance Imaging) visual decoding involves decoding the original image from brain signals elicited by visual stimuli. This often relies on manually labeled ROIs (Regions of Interest) to select brain voxels. However, these ROIs can contain redundant information and noise, reducing decoding performance. Additionally, the lack of automated ROI labeling methods hinders the practical application of fMRI visual decoding technology, especially for new subjects. This work presents TROI (Trainable Region of Interest), a novel two-stage, data-driven ROI labeling method for cross-subject fMRI decoding tasks, particularly when subject samples are limited. TROI leverages labeled ROIs in the dataset to pretrain an image decoding backbone on a cross-subject dataset, enabling efficient optimization of the input layer for new subjects without retraining the entire model from scratch. In the first stage, we introduce a voxel selection method that combines sparse mask training and low-pass filtering to quickly generate the voxel mask and determine input layer dimensions. In the second stage, we apply a learning rate rewinding strategy to fine-tune the input layer for downstream tasks. Experimental results on the same small sample dataset as the baseline method for brain visual retrieval and reconstruction tasks show that our voxel selection method surpasses the state-of-the-art method MindEye2 with an annotated ROI mask. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00413</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00413</id><created>2025-02-01</created><authors><author><keyname>Canay</keyname><forenames>Ozkan</forenames></author><author><keyname>Kocabicak</keyname><forenames>Umit</forenames></author></authors><title>Predictive modeling and anomaly detection in large-scale web portals   through the CAWAL framework</title><categories>cs.LG cs.IR</categories><comments>15 pages, 4 figures</comments><msc-class>68T05, 68T10</msc-class><acm-class>I.2; I.5</acm-class><journal-ref>Knowledge-Based Systems, 306, 112710 (2024)</journal-ref><doi>10.1016/j.knosys.2024.112710</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This study presents an approach that uses session and page view data collected through the CAWAL framework, enriched through specialized processes, for advanced predictive modeling and anomaly detection in web usage mining (WUM) applications. Traditional WUM methods often rely on web server logs, which limit data diversity and quality. Integrating application logs with web analytics, the CAWAL framework creates comprehensive session and page view datasets, providing a more detailed view of user interactions and effectively addressing these limitations. This integration enhances data diversity and quality while eliminating the preprocessing stage required in conventional WUM, leading to greater process efficiency. The enriched datasets, created by cross-integrating session and page view data, were applied to advanced machine learning models, such as Gradient Boosting and Random Forest, which are known for their effectiveness in capturing complex patterns and modeling non-linear relationships. These models achieved over 92% accuracy in predicting user behavior and significantly improved anomaly detection capabilities. The results show that this approach offers detailed insights into user behavior and system performance metrics, making it a reliable solution for improving large-scale web portals' efficiency, reliability, and scalability. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00414</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00414</id><created>2025-02-01</created><authors><author><keyname>Ali</keyname><forenames>Hasin Jawad</forenames></author><author><keyname>Abrar</keyname><forenames>Ajwad</forenames></author><author><keyname>Hossain</keyname><forenames>S. M. Hozaifa</forenames></author><author><keyname>Mridha</keyname><forenames>M. Firoz</forenames></author></authors><title>Social media polarization during conflict: Insights from an ideological   stance dataset on Israel-Palestine Reddit comments</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In politically sensitive scenarios like wars, social media serves as a platform for polarized discourse and expressions of strong ideological stances. While prior studies have explored ideological stance detection in general contexts, limited attention has been given to conflict-specific settings. This study addresses this gap by analyzing 9,969 Reddit comments related to the Israel-Palestine conflict, collected between October 2023 and August 2024. The comments were categorized into three stance classes: Pro-Israel, Pro-Palestine, and Neutral. Various approaches, including machine learning, pre-trained language models, neural networks, and prompt engineering strategies for open source large language models (LLMs), were employed to classify these stances. Performance was assessed using metrics such as accuracy, precision, recall, and F1-score. Among the tested methods, the Scoring and Reflective Re-read prompt in Mixtral 8x7B demonstrated the highest performance across all metrics. This study provides comparative insights into the effectiveness of different models for detecting ideological stances in highly polarized social media contexts. The dataset used in this research is publicly available for further exploration and validation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00415</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00415</id><created>2025-02-01</created><authors><author><keyname>Fatouros</keyname><forenames>George</forenames></author><author><keyname>Metaxas</keyname><forenames>Kostas</forenames></author><author><keyname>Soldatos</keyname><forenames>John</forenames></author><author><keyname>Karathanassis</keyname><forenames>Manos</forenames></author></authors><title>MarketSenseAI 2.0: Enhancing Stock Analysis through LLM Agents</title><categories>q-fin.CP cs.AI cs.CL cs.MA q-fin.PM</categories><comments>25 pages, 7 figures, Under review at Financial Innovation (FIN)</comments><msc-class>68T07, 68T50, 91G10, 91G15</msc-class><acm-class>I.2.1; I.2.7; J.4</acm-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  MarketSenseAI is a novel framework for holistic stock analysis which leverages Large Language Models (LLMs) to process financial news, historical prices, company fundamentals and the macroeconomic environment to support decision making in stock analysis and selection. In this paper, we present the latest advancements on MarketSenseAI, driven by rapid technological expansion in LLMs. Through a novel architecture combining Retrieval-Augmented Generation and LLM agents, the framework processes SEC filings and earnings calls, while enriching macroeconomic analysis through systematic processing of diverse institutional reports. We demonstrate a significant improvement in fundamental analysis accuracy over the previous version. Empirical evaluation on S\&amp;P 100 stocks over two years (2023-2024) shows MarketSenseAI achieving cumulative returns of 125.9% compared to the index return of 73.5%, while maintaining comparable risk profiles. Further validation on S\&amp;P 500 stocks during 2024 demonstrates the framework's scalability, delivering a 33.8% higher Sortino ratio than the market. This work marks a significant advancement in applying LLM technology to financial analysis, offering insights into the robustness of LLM-driven investment strategies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00416</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00416</id><created>2025-02-01</created><authors><author><keyname>Padmaprabhan</keyname><forenames>A.</forenames></author><author><keyname>Hari</keyname><forenames>Shriram</forenames></author><author><keyname>Thomas</keyname><forenames>Nived Philip</forenames></author><author><keyname>Chadha</keyname><forenames>Khaish Singh</forenames></author><author><keyname>Sidhardh</keyname><forenames>Sai</forenames></author><author><keyname>Chinthapenta</keyname><forenames>Viswanath</forenames></author><author><keyname>Kumar</keyname><forenames>Prabhat</forenames></author></authors><title>GO-GAN: Geometry Optimization Generative Adversarial Network for   Achieving Optimized Structures with Targeted Physical Properties</title><categories>cs.CE</categories><comments>iNCMDAO 2024</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents GO-GAN, a novel Generative Adversarial Network (GAN) architecture for geometry optimization (GO), specifically to generate structures based on user-specified input parameters. The architecture for GO-GAN proposed here combines a \texttt{Pix2Pix} GAN with a new input mechanism, involving a dynamic batch gradient descent-based training loop that leverages dataset symmetries. The model, implemented here using \texttt{TensorFlow} and \texttt{Keras}, is trained using input images representing scalar physical properties generated by a custom MatLab code. After training, GO-GAN rapidly generates optimized geometries from input images representing scalar inputs of the physical properties. Results demonstrate GO-GAN's ability to produce acceptable designs with desirable variations. These variations are followed by the influence of discriminators during training and are of practical significance in ensuring adherence to specifications while enabling creative exploration of the design space. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00418</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00418</id><created>2025-02-01</created><authors><author><keyname>Teuber</keyname><forenames>Carolin</forenames></author><author><keyname>Archit</keyname><forenames>Anwai</forenames></author><author><keyname>Pape</keyname><forenames>Constantin</forenames></author></authors><title>Parameter Efficient Fine-Tuning of Segment Anything Model</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Segmentation is an important analysis task for biomedical images, enabling the study of individual organelles, cells or organs. Deep learning has massively improved segmentation methods, but challenges remain in generalization to new conditions, requiring costly data annotation. Vision foundation models, such as Segment Anything Model (SAM), address this issue through broad segmentation capabilities. However, these models still require finetuning on annotated data, although with less annotations, to achieve optimal results for new conditions. As a downside, they require more computational resources. This makes parameter-efficient finetuning (PEFT) relevant for their application. We contribute the first comprehensive study of PEFT for SAM applied to biomedical segmentation by evaluating 9 PEFT methods on diverse datasets. We also provide an implementation of QLoRA for vision transformers and a new approach for resource-efficient finetuning of SAM. Our code is publicly available at https://github.com/computational-cell-analytics/peft-sam. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00421</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00421</id><created>2025-02-01</created><authors><author><keyname>Abu</keyname><forenames>Turi</forenames></author><author><keyname>Shi</keyname><forenames>Ying</forenames></author><author><keyname>Zheng</keyname><forenames>Thomas Fang</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author></authors><title>Sagalee: an Open Source Automatic Speech Recognition Dataset for Oromo   Language</title><categories>cs.CL cs.SD eess.AS</categories><comments>Accepted for ICASSP2025 (2025 IEEE International Conference on   Acoustics, Speech, and Signal Processing)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a novel Automatic Speech Recognition (ASR) dataset for the Oromo language, a widely spoken language in Ethiopia and neighboring regions. The dataset was collected through a crowd-sourcing initiative, encompassing a diverse range of speakers and phonetic variations. It consists of 100 hours of real-world audio recordings paired with transcriptions, covering read speech in both clean and noisy environments. This dataset addresses the critical need for ASR resources for the Oromo language which is underrepresented. To show its applicability for the ASR task, we conducted experiments using the Conformer model, achieving a Word Error Rate (WER) of 15.32% with hybrid CTC and AED loss and WER of 18.74% with pure CTC loss. Additionally, fine-tuning the Whisper model resulted in a significantly improved WER of 10.82%. These results establish baselines for Oromo ASR, highlighting both the challenges and the potential for improving ASR performance in Oromo. The dataset is publicly available at https://github.com/turinaf/sagalee and we encourage its use for further research and development in Oromo speech processing. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00423</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00423</id><created>2025-02-01</created><authors><author><keyname>Chen</keyname><forenames>Elynn</forenames></author><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Jing</keyname><forenames>Wenbo</forenames></author><author><keyname>Liu</keyname><forenames>Xiao</forenames></author></authors><title>Stochastic Linear Bandits with Latent Heterogeneity</title><categories>cs.LG stat.ME stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper addresses the critical challenge of latent heterogeneity in online decision-making, where individual responses to business actions vary due to unobserved characteristics. While existing approaches in data-driven decision-making have focused on observable heterogeneity through contextual features, they fall short when heterogeneity stems from unobservable factors such as lifestyle preferences and personal experiences. We propose a novel latent heterogeneous bandit framework that explicitly models this unobserved heterogeneity in customer responses, with promotion targeting as our primary example. Our methodology introduces an innovative algorithm that simultaneously learns latent group memberships and group-specific reward functions. Through theoretical analysis and empirical validation using data from a mobile commerce platform, we establish high-probability bounds for parameter estimation, convergence rates for group classification, and comprehensive regret bounds. Notably, our theoretical analysis reveals two distinct types of regret measures: a ``strong regret'' against an oracle with perfect knowledge of customer memberships, which remains non-sub-linear due to inherent classification uncertainty, and a ``regular regret'' against an oracle aware only of deterministic components, for which our algorithm achieves a sub-linear rate that is minimax optimal in horizon length and dimension. We further demonstrate that existing bandit algorithms ignoring latent heterogeneity incur constant average regret that accumulates linearly over time. Our framework provides practitioners with new tools for decision-making under latent heterogeneity and extends to various business applications, including personalized pricing, resource allocation, and inventory management. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00425</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00425</id><created>2025-02-01</created><authors><author><keyname>Yu</keyname><forenames>JiangYong</forenames></author><author><keyname>Zhou</keyname><forenames>Sifan</forenames></author><author><keyname>Yang</keyname><forenames>Dawei</forenames></author><author><keyname>Wang</keyname><forenames>Shuo</forenames></author><author><keyname>Li</keyname><forenames>Shuoyu</forenames></author><author><keyname>Hu</keyname><forenames>Xing</forenames></author><author><keyname>Xu</keyname><forenames>Chen</forenames></author><author><keyname>Xu</keyname><forenames>Zukang</forenames></author><author><keyname>Shu</keyname><forenames>Changyong</forenames></author><author><keyname>Yuan</keyname><forenames>Zhihang</forenames></author></authors><title>MQuant: Unleashing the Inference Potential of Multimodal Large Language   Models via Full Static Quantization</title><categories>cs.CV cs.AI</categories><comments>First quantization solution for Multimodal large language models   applicable to 5 mainstream MLLMs</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Multimodal large language models (MLLMs) have garnered widespread attention due to their ability to understand multimodal input. However, their large parameter sizes and substantial computational demands severely hinder their practical deployment and application.While quantization is an effective way to reduce model size and inference latency, its application to MLLMs remains underexplored. In this paper, we propose MQuant, a post-training quantization (PTQ) framework designed to tackle the unique challenges of multimodal large language models (MLLMs). Conventional quantization often struggles with MLLMs because of (a) high inference latency from large visual token counts, (b) distributional disparities between visual and textual tokens, and (c) extreme outliers introduced by Hadamard-based transformations. To address these issues, MQuant introduces: Modality-Specific Static Quantization (MSQ), assigning distinct static scales for visual vs. textual tokens; Attention-Invariant Flexible Switching (AIFS), reordering tokens to preserve casual attention while eliminating expensive token-wise scale computations; Rotation Magnitude Suppression (RMS), mitigating weight outliers arising from online Hadamard rotations. On five mainstream MLLMs (including Qwen-VL, MiniCPM-V, CogVLM2), MQuant under W4A8 achieves near-floating-point accuracy (&lt;1% degradation) while reducing inference latency by up to 30%, significantly outperforming existing PTQ baselines. Our MQuant effectively bridges the gap for efficient and accurate MLLMs inference in resource-constrained devices. Code will be released. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00426</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00426</id><created>2025-02-01</created><authors><author><keyname>Yan</keyname><forenames>Rui</forenames></author><author><keyname>Wang</keyname><forenames>Jin</forenames></author><author><keyname>Qu</keyname><forenames>Hongyu</forenames></author><author><keyname>Du</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Zhang</keyname><forenames>Dong</forenames></author><author><keyname>Tang</keyname><forenames>Jinhui</forenames></author><author><keyname>Tan</keyname><forenames>Tieniu</forenames></author></authors><title>TeST-V: TEst-time Support-set Tuning for Zero-shot Video Classification</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recently, adapting Vision Language Models (VLMs) to zero-shot visual classification by tuning class embedding with a few prompts (Test-time Prompt Tuning, TPT) or replacing class names with generated visual samples (support-set) has shown promising results. However, TPT cannot avoid the semantic gap between modalities while the support-set cannot be tuned. To this end, we draw on each other's strengths and propose a novel framework namely TEst-time Support-set Tuning for zero-shot Video Classification (TEST-V). It first dilates the support-set with multiple prompts (Multi-prompting Support-set Dilation, MSD) and then erodes the support-set via learnable weights to mine key cues dynamically (Temporal-aware Support-set Erosion, TSE). Specifically, i) MSD expands the support samples for each class based on multiple prompts enquired from LLMs to enrich the diversity of the support-set. ii) TSE tunes the support-set with factorized learnable weights according to the temporal prediction consistency in a self-supervised manner to dig pivotal supporting cues for each class. $\textbf{TEST-V}$ achieves state-of-the-art results across four benchmarks and has good interpretability for the support-set dilation and erosion. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00428</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00428</id><created>2025-02-01</created><authors><author><keyname>Zaccour</keyname><forenames>Juliette</forenames></author><author><keyname>Binns</keyname><forenames>Reuben</forenames></author><author><keyname>Rocher</keyname><forenames>Luc</forenames></author></authors><title>Access Denied: Meaningful Data Access for Quantitative Algorithm Audits</title><categories>cs.HC cs.CY</categories><comments>30 pages, 12 figures. To be published in CHI Conference on Human   Factors in Computing Systems (CHI '25)</comments><doi>10.1145/3706598.3713963</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Independent algorithm audits hold the promise of bringing accountability to automated decision-making. However, third-party audits are often hindered by access restrictions, forcing auditors to rely on limited, low-quality data. To study how these limitations impact research integrity, we conduct audit simulations on two realistic case studies for recidivism and healthcare coverage prediction. We examine the accuracy of estimating group parity metrics across three levels of access: (a) aggregated statistics, (b) individual-level data with model outputs, and (c) individual-level data without model outputs. Despite selecting one of the simplest tasks for algorithmic auditing, we find that data minimization and anonymization practices can strongly increase error rates on individual-level data, leading to unreliable assessments. We discuss implications for independent auditors, as well as potential avenues for HCI researchers and regulators to improve data access and enable both reliable and holistic evaluations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00429</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00429</id><created>2025-02-01</created><authors><author><keyname>Parida</keyname><forenames>Shreyas Kumar</forenames></author><author><keyname>Gerostathopoulos</keyname><forenames>Ilias</forenames></author><author><keyname>Bogner</keyname><forenames>Justus</forenames></author></authors><title>How Do Model Export Formats Impact the Development of ML-Enabled   Systems? A Case Study on Model Integration</title><categories>cs.SE cs.LG</categories><comments>Accepted for publication at the International Conference on AI   Engineering - Software Engineering for AI (CAIN'25, see   https://conf.researchr.org/home/cain-2025)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning (ML) models are often integrated into ML-enabled systems to provide software functionality that would otherwise be impossible. This integration requires the selection of an appropriate ML model export format, for which many options are available. These formats are crucial for ensuring a seamless integration, and choosing a suboptimal one can negatively impact system development. However, little evidence is available to guide practitioners during the export format selection.   We therefore evaluated various model export formats regarding their impact on the development of ML-enabled systems from an integration perspective. Based on the results of a preliminary questionnaire survey (n=17), we designed an extensive embedded case study with two ML-enabled systems in three versions with different technologies. We then analyzed the effect of five popular export formats, namely ONNX, Pickle, TensorFlow's SavedModel, PyTorch's TorchScript, and Joblib. In total, we studied 30 units of analysis (2 systems x 3 tech stacks x 5 formats) and collected data via structured field notes.   The holistic qualitative analysis of the results indicated that ONNX offered the most efficient integration and portability across most cases. SavedModel and TorchScript were very convenient to use in Python-based systems, but otherwise required workarounds (TorchScript more than SavedModel). SavedModel also allowed the easy incorporation of preprocessing logic into a single file, which made it scalable for complex deep learning use cases. Pickle and Joblib were the most challenging to integrate, even in Python-based systems. Regarding technical support, all model export formats had strong technical documentation and strong community support across platforms such as Stack Overflow and Reddit. Practitioners can use our findings to inform the selection of ML export formats suited to their context. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00430</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00430</id><created>2025-02-01</created><authors><author><keyname>Agbeve</keyname><forenames>Douglas Dziedzorm</forenames></author><author><keyname>Belogaev</keyname><forenames>Andrey</forenames></author><author><keyname>Sandra</keyname><forenames>Wim</forenames></author><author><keyname>Lylon</keyname><forenames>Carl</forenames></author><author><keyname>Famaey</keyname><forenames>Jeroen</forenames></author></authors><title>A2P: A Scalable OFDMA Polling Algorithm for Time-Sensitive Wi-Fi   Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the years, advancements such as increased bandwidth, new modulation and coding schemes, frame aggregation, and the use of multiple antennas have been employed to enhance Wi-Fi performance. Nonetheless, as network density and the demand for low-latency applications increases, contention delays and retransmissions have become obstacles to efficient Wi-Fi deployment in modern scenarios. The introduction of Orthogonal Frequency-Division Multiple Access (OFDMA) in the IEEE 802.11 standard allows simultaneous transmissions to and from multiple users within the same transmission opportunity, thereby reducing the contention. However, the AP must efficiently manage the resource allocation, particularly in uplink scenarios where it lacks prior knowledge of users' buffer statuses, thus making polling a critical bottleneck in networks with a high number of users with sporadic traffic pattern. This paper addresses the polling problem and introduces the A2P algorithm, designed to enable scalable and efficient polling in high-density OFDMA-based time sensitive Wi-Fi networks. Simulation results show that A2P outperforms the alternative schemes by maintaining significantly lower delay and packet loss in dense time-sensitive teleconferencing scenario. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00432</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00432</id><created>2025-02-01</created><authors><author><keyname>Silvestri</keyname><forenames>Matteo</forenames></author><author><keyname>Gabrielli</keyname><forenames>Edoardo</forenames></author><author><keyname>Silvestri</keyname><forenames>Fabrizio</forenames></author><author><keyname>Tolomei</keyname><forenames>Gabriele</forenames></author></authors><title>Community Membership Hiding via Gradient-based Optimization</title><categories>cs.SI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We tackle the problem of \emph{community membership hiding}, which involves strategically altering a network's structure to obscure a target node's membership in a specific community identified by a detection algorithm. We reformulate the original discrete counterfactual graph objective as a differentiable constrained optimization task. To solve this, we propose \method{}, a gradient-based method that modifies the network's structure within the feasible bounds for an individual target node, effectively concealing its membership. Experimental results across multiple datasets and community detection algorithms show that our approach surpasses existing baselines, offering a better balance between accuracy and computational efficiency. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00433</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00433</id><created>2025-02-01</created><authors><author><keyname>Cheng</keyname><forenames>Xinle</forenames></author><author><keyname>Chen</keyname><forenames>Zhuoming</forenames></author><author><keyname>Jia</keyname><forenames>Zhihao</forenames></author></authors><title>CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion   Models</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diffusion models have revolutionized generative tasks, especially in the domain of text-to-image synthesis; however, their iterative denoising process demands substantial computational resources. In this paper, we present a novel acceleration strategy that integrates token-level pruning with caching techniques to tackle this computational challenge. By employing noise relative magnitude, we identify significant token changes across denoising iterations. Additionally, we enhance token selection by incorporating spatial clustering and ensuring distributional balance. Our experiments demonstrate reveal a 50%-60% reduction in computational costs while preserving the performance of the model, thereby markedly increasing the efficiency of diffusion models. The code is available at https://github.com/ada-cheng/CAT-Pruning </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00434</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00434</id><created>2025-02-01</created><authors><author><keyname>de Colnet</keyname><forenames>Alexis</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author><author><keyname>Zhang</keyname><forenames>Tianwei</forenames></author></authors><title>Compilation and Fast Model Counting beyond CNF</title><categories>cs.CC cs.AI cs.LO</categories><doi>10.24963/ijcai.2024/367</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Circuits in deterministic decomposable negation normal form (d-DNNF) are representations of Boolean functions that enable linear-time model counting. This paper strengthens our theoretical knowledge of what classes of functions can be efficiently transformed, or compiled, into d-DNNF. Our main contribution is the fixed-parameter tractable (FPT) compilation of conjunctions of specific constraints parameterized by incidence treewidth. This subsumes the known result for CNF. The constraints in question are all functions representable by constant-width ordered binary decision diagrams (OBDDs) for all variable orderings. For instance, this includes parity constraints and cardinality constraints with constant threshold. The running time of the FPT compilation is singly exponential in the incidence treewidth but hides large constants in the exponent. To balance that, we give a more efficient FPT algorithm for model counting that applies to a sub-family of the constraints and does not require compilation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00435</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00435</id><created>2025-02-01</created><authors><author><keyname>Duc</keyname><forenames>Chuc Man</forenames></author><author><keyname>Fukui</keyname><forenames>Hiromichi</forenames></author></authors><title>SatMamba: Development of Foundation Models for Remote Sensing Imagery   Using State Space Models</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Foundation models refer to deep learning models pretrained on large unlabeled datasets through self-supervised algorithms. In the Earth science and remote sensing communities, there is growing interest in transforming the use of Earth observation data, including satellite and aerial imagery, through foundation models. Various foundation models have been developed for remote sensing, such as those for multispectral, high-resolution, and hyperspectral images, and have demonstrated superior performance on various downstream tasks compared to traditional supervised models. These models are evolving rapidly, with capabilities to handle multispectral, multitemporal, and multisensor data. Most studies use masked autoencoders in combination with Vision Transformers (ViTs) as the backbone for pretraining. While the models showed promising performance, ViTs face challenges, such as quadratic computational scaling with input length, which may limit performance on multiband and multitemporal data with long sequences. This research aims to address these challenges by proposing SatMamba, a new pretraining framework that combines masked autoencoders with State Space Model, offering linear computational scaling. Experiments on high-resolution imagery across various downstream tasks show promising results, paving the way for more efficient foundation models and unlocking the full potential of Earth observation data. The source code is available in https://github.com/mdchuc/HRSFM. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00436</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00436</id><created>2025-02-01</created><authors><author><keyname>Yan</keyname><forenames>Jiaqi</forenames></author><author><keyname>Markovsky</keyname><forenames>Ivan</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author></authors><title>Secure Data Reconstruction: A Direct Data-Driven Approach</title><categories>eess.SY cs.SY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper addresses the problem of secure data reconstruction for unknown systems, where data collected from the system are susceptible to malicious manipulation. We aim to recover the real trajectory without prior knowledge of the system model. To achieve this, a behavioral language is used to represent the system, describing it using input/output trajectories instead of state-space models. We consider two attack scenarios. In the first scenario, up to $k$ entries of the collected data are malicious. On the other hand, the second scenario assumes that at most $k$ channels from sensors or actuators can be compromised, implying that any data collected from these channels might be falsified. For both scenarios, we formulate the trajectory recovery problem as an optimization problem and introduce sufficient conditions to ensure successful recovery of the true data. Since finding exact solutions to these problems can be computationally inefficient, we further approximate them using an $\ell_1$-norm and group Least Absolute Shrinkage and Selection Operator (LASSO). We demonstrate that under certain conditions, these approximation problems also find the true trajectory while maintaining low computation complexity. Finally, we extend the proposed algorithms to noisy data. By reconstructing the secure trajectory, this work serves as a safeguard mechanism for subsequent data-driven control methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00439</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00439</id><created>2025-02-01</created><authors><author><keyname>Xiong</keyname><forenames>Yizhe</forenames></author><author><keyname>Huang</keyname><forenames>Wei</forenames></author><author><keyname>Ye</keyname><forenames>Xin</forenames></author><author><keyname>Chen</keyname><forenames>Hui</forenames></author><author><keyname>Lin</keyname><forenames>Zijia</forenames></author><author><keyname>Lian</keyname><forenames>Haoran</forenames></author><author><keyname>Su</keyname><forenames>Zhenpeng</forenames></author><author><keyname>Han</keyname><forenames>Jungong</forenames></author><author><keyname>Ding</keyname><forenames>Guiguang</forenames></author></authors><title>UniAttn: Reducing Inference Costs via Softmax Unification for   Post-Training LLMs</title><categories>cs.CL</categories><comments>11 pages, 4 figures. Preprint, under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Post-training is essential for adapting Large Language Models (LLMs) to real-world applications. Deploying post-trained models faces significant challenges due to substantial memory overhead and noticeable inference latency. Existing work has identified significant redundancies in LLMs and proposed efficient architectures, namely intra-layer KV sharing and cross-layer KV sharing. However, intra-layer KV sharing still results in high inference costs, while cross-layer KV sharing leads to significant performance degradation. As a result, both methods remain suboptimal for post-training pre-trained LLMs. In this paper, we identify that the \texttt{Softmax} operation is a primary bottleneck for LLM inference and discover that it is actually highly redundant during post-training. We propose Softmax \textbf{Uni}fication in \textbf{Att}e\textbf{n}tion (\textbf{UniAttn}), a novel post-training method that unifies Softmax activations across transformer blocks to reduce LLM inference costs. Additionally, UniAttn adopts a linear projection to compensate for the errors induced by Softmax unification. Experiments show that UniAttn matches the performance of standard post-training while significantly reducing inference costs, outperforming existing efficient architectures during post-training. Our code will be available at \url{https://github.com/Bostoncake/UniAttn}. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00443</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00443</id><created>2025-02-01</created><authors><author><keyname>Join</keyname><forenames>Cédric</forenames></author><author><keyname>Delaleau</keyname><forenames>Emmanuel</forenames></author><author><keyname>Fliess</keyname><forenames>Michel</forenames></author></authors><title>Model-Free Predictive Control: Introductory Algebraic Calculations, and   a Comparison with HEOL and ANNs</title><categories>eess.SY cs.AI cs.SY</categories><msc-class>49J99</msc-class><acm-class>I.2.8</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Model predictive control (MPC) is a popular control engineering practice, but requires a sound knowledge of the model. Model-free predictive control (MFPC), a burning issue today, also related to reinforcement learning (RL) in AI, is reformulated here via a linear differential equation with constant coefficients, thanks to a new perspective on optimal control combined with recent advances in the field of model-free control. It is replacing Dynamic Programming, the Hamilton-Jacobi-Bellman equation, and Pontryagin's Maximum Principle. The computing burden is low. The implementation is straightforward. Two nonlinear examples, a chemical reactor and a two tank system, are illustrating our approach. A comparison with the HEOL setting, where some expertise of the process model is needed, shows only a slight superiority of the later. A recent identification of the two tank system via a complex ANN architecture might indicate that a full modeling and the corresponding machine learning mechanism are not always necessary neither in control, nor, more generally, in AI. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00448</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00448</id><created>2025-02-01</created><authors><author><keyname>Li</keyname><forenames>Taiji</forenames></author><author><keyname>Chen</keyname><forenames>Hao</forenames></author><author><keyname>Yu</keyname><forenames>Fei</forenames></author><author><keyname>Zhang</keyname><forenames>Yin</forenames></author></authors><title>HERA: Improving Long Document Summarization using Large Language Models   with Context Packaging and Reordering</title><categories>cs.CL</categories><comments>7 pages, 1 figure</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Despite the rapid growth of context length of large language models (LLMs) , LLMs still perform poorly in long document summarization. An important reason for this is that relevant information about an event is scattered throughout long documents, and the messy narrative order impairs the accurate understanding and utilization of LLMs for long documents. To address these issues, we propose a novel summary generation framework, called HERA. Specifically, we first segment a long document by its semantic structure and retrieve text segments about the same event, and finally reorder them to form the input context. We evaluate our approach on two long document summarization datasets. The experimental results show that HERA outperforms foundation models in ROUGE, BERTScore and faithfulness metrics, while HERA does not require additional fine-tuning and resources. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00451</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00451</id><created>2025-02-01</created><authors><author><keyname>Mandal</keyname><forenames>Aishik</forenames></author><author><keyname>Chakraborty</keyname><forenames>Tanmoy</forenames></author><author><keyname>Gurevych</keyname><forenames>Iryna</forenames></author></authors><title>Towards Privacy-aware Mental Health AI Models: Advances, Challenges, and   Opportunities</title><categories>cs.CL cs.AI</categories><comments>18 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Mental illness is a widespread and debilitating condition with substantial societal and personal costs. Traditional diagnostic and treatment approaches, such as self-reported questionnaires and psychotherapy sessions, often impose significant burdens on both patients and clinicians, limiting accessibility and efficiency. Recent advances in Artificial Intelligence (AI), particularly in Natural Language Processing and multimodal techniques, hold great potential for recognizing and addressing conditions such as depression, anxiety, bipolar disorder, schizophrenia, and post-traumatic stress disorder. However, privacy concerns, including the risk of sensitive data leakage from datasets and trained models, remain a critical barrier to deploying these AI systems in real-world clinical settings. These challenges are amplified in multimodal methods, where personal identifiers such as voice and facial data can be misused. This paper presents a critical and comprehensive study of the privacy challenges associated with developing and deploying AI models for mental health. We further prescribe potential solutions, including data anonymization, synthetic data generation, and privacy-preserving model training, to strengthen privacy safeguards in practical applications. Additionally, we discuss evaluation frameworks to assess the privacy-utility trade-offs in these approaches. By addressing these challenges, our work aims to advance the development of reliable, privacy-aware AI tools to support clinical decision-making and improve mental health outcomes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00452</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00452</id><created>2025-02-01</created><authors><author><keyname>Voet</keyname><forenames>Yannis</forenames></author><author><keyname>Sande</keyname><forenames>Espen</forenames></author><author><keyname>Buffa</keyname><forenames>Annalisa</forenames></author></authors><title>Mass lumping and stabilization for immersogeometric analysis</title><categories>math.NA cs.NA</categories><comments>Submitted manuscript</comments><msc-class>65M15, 65M22, 65M60</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Trimmed (multi-patch) geometries are the state-of-the-art technology in computer-aided design for industrial applications such as automobile crashworthiness. In this context, fast solution techniques extensively rely on explicit time integration schemes in conjunction with mass lumping techniques that substitute the consistent mass with a (usually diagonal) approximation. For smooth isogeometric discretizations, Leidinger [1] first showed that mass lumping removed the dependency of the critical time-step on the size of trimmed elements. This finding has attracted considerable attention but has unfortunately overshadowed another more subtle effect: mass lumping may disastrously impact the accuracy of low frequencies and modes, potentially inducing spurious oscillations in the solution. In this article, we provide compelling evidence for this phenomenon and later propose a stabilization technique based on polynomial extensions that restores a level of accuracy comparable to boundary-fitted discretizations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00455</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00455</id><created>2025-02-01</created><authors><author><keyname>Kong</keyname><forenames>Ray Wai Man</forenames></author><author><keyname>Ning</keyname><forenames>Ding</forenames></author><author><keyname>Kong</keyname><forenames>Theodore Ho Tin</forenames></author></authors><title>Line Balancing in the Modern Garment Industry</title><categories>cs.RO</categories><comments>14 pages, 5 figures, 3 tables, preprint</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This article presents applied research on line balancing within the modern garment industry, focusing on the significant impact of intelligent hanger systems and hanger lines on the stitching process, by Lean Methodology for garment modernization. It explores the application of line balancing in the modern garment industry, focusing on the significant impact of intelligent hanger systems and hanger lines on the stitching process. It aligns with Lean Methodology principles for garment modernization. Without the implementation of line balancing technology, the garment manufacturing process using hanger systems cannot improve output rates. The case study demonstrates that implementing intelligent line balancing in a straightforward practical setup facilitates lean practices combined with a digitalization system and automaton. This approach illustrates how to enhance output and reduce accumulated work in progress. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00456</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00456</id><created>2025-02-01</created><authors><author><keyname>Sikar</keyname><forenames>Daniel</forenames></author><author><keyname>Garcez</keyname><forenames>Artur d'Avila</forenames></author><author><keyname>Weyde</keyname><forenames>Tillman</forenames></author></authors><title>Explorations of the Softmax Space: Knowing When the Neural Network   Doesn't Know...</title><categories>cs.LG cs.CV</categories><comments>9 pages, 5 figures, 1 table. arXiv admin note: substantial text   overlap with arXiv:2407.07821</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Ensuring the reliability and safety of automated decision-making is crucial. This paper proposes a new approach for measuring the reliability of predictions in machine learning models. We analyze how the outputs of a trained neural network change using clustering to measure distances between outputs and class centroids. We propose this distance as a metric to evaluate the confidence of predictions. We assign each prediction to a cluster with centroid representing the mean softmax output for all correct predictions of a given class. We then define a safety threshold for a class as the smallest distance from an incorrect prediction to the given class centroid. We evaluate the approach on the MNIST and CIFAR-10 datasets using a Convolutional Neural Network and a Vision Transformer, respectively. The results show that our approach is consistent across these data sets and network models, and indicate that the proposed metric can offer an efficient way of determining when automated predictions are acceptable and when they should be deferred to human operators. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00459</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00459</id><created>2025-02-01</created><authors><author><keyname>Hyunju</keyname><forenames>Kang</forenames></author><author><keyname>Geonhee</keyname><forenames>Han</forenames></author><author><keyname>Yoonjae</keyname><forenames>Jeong</forenames></author><author><keyname>Hogun</keyname><forenames>Park</forenames></author></authors><title>AudioGenX: Explainability on Text-to-Audio Generative Models</title><categories>cs.SD cs.AI cs.LG eess.AS</categories><comments>14 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Text-to-audio generation models (TAG) have achieved significant advances in generating audio conditioned on text descriptions. However, a critical challenge lies in the lack of transparency regarding how each textual input impacts the generated audio. To address this issue, we introduce AudioGenX, an Explainable AI (XAI) method that provides explanations for text-to-audio generation models by highlighting the importance of input tokens. AudioGenX optimizes an Explainer by leveraging factual and counterfactual objective functions to provide faithful explanations at the audio token level. This method offers a detailed and comprehensive understanding of the relationship between text inputs and audio outputs, enhancing both the explainability and trustworthiness of TAG models. Extensive experiments demonstrate the effectiveness of AudioGenX in producing faithful explanations, benchmarked against existing methods using novel evaluation metrics specifically designed for audio generation tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00461</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00461</id><created>2025-02-01</created><authors><author><keyname>Combe</keyname><forenames>Noémie C.</forenames></author></authors><title>On Multiquantum Bits, Segre Embeddings and Coxeter Chambers</title><categories>quant-ph cs.IT math.AG math.IT math.NT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This work explores the interplay between quantum information theory, algebraic geometry, and number theory, with a particular focus on multiqubit systems, their entanglement structure, and their classification via geometric embeddings. The Segre embedding, a fundamental construction in algebraic geometry, provides an algebraic framework to distinguish separable and entangled states, encoding quantum correlations in projective geometry. We develop a systematic study of qubit moduli spaces, illustrating the geometric structure of entanglement through hypercube constructions and Coxeter chamber decompositions.   We establish a bijection between the Segre embeddings of tensor products of projective spaces and binary words of length $n-1$, structured as an $(n-1)$-dimensional hypercube, where adjacency corresponds to a single Segre operation. This reveals a combinatorial structure underlying the hierarchy of embeddings, with direct implications for quantum error correction schemes. The symmetry of the Segre variety under the Coxeter group of type $A$ allows us to analyze quantum states and errors through the lens of reflection groups, viewing separable states as lying in distinct Coxeter chambers on a Segre variety. The transitive action of the permutation group on these chambers provides a natural method for tracking errors in quantum states and potentially reversing them. Beyond foundational aspects, we highlight relations between Segre varieties and Dixon elliptic curves, drawing connections between entanglement and number theory. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00462</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00462</id><created>2025-02-01</created><authors><author><keyname>Ryoo</keyname><forenames>Kihwan</forenames></author><author><keyname>Lim</keyname><forenames>Hyungtae</forenames></author><author><keyname>Myung</keyname><forenames>Hyun</forenames></author></authors><title>MambaGlue: Fast and Robust Local Feature Matching With Mamba</title><categories>cs.CV cs.RO</categories><comments>Proc. IEEE Int'l Conf. Robotics and Automation (ICRA) 2025</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In recent years, robust matching methods using deep learning-based approaches have been actively studied and improved in computer vision tasks. However, there remains a persistent demand for both robust and fast matching techniques. To address this, we propose a novel Mamba-based local feature matching approach, called MambaGlue, where Mamba is an emerging state-of-the-art architecture rapidly gaining recognition for its superior speed in both training and inference, and promising performance compared with Transformer architectures. In particular, we propose two modules: a) MambaAttention mixer to simultaneously and selectively understand the local and global context through the Mamba-based self-attention structure and b) deep confidence score regressor, which is a multi-layer perceptron (MLP)-based architecture that evaluates a score indicating how confidently matching predictions correspond to the ground-truth correspondences. Consequently, our MambaGlue achieves a balance between robustness and efficiency in real-world applications. As verified on various public datasets, we demonstrate that our MambaGlue yields a substantial performance improvement over baseline approaches while maintaining fast inference speed. Our code will be available on https://github.com/url-kaist/MambaGlue </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00463</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00463</id><created>2025-02-01</created><authors><author><keyname>Liu</keyname><forenames>Zhiyu</forenames></author><author><keyname>Han</keyname><forenames>Zhi</forenames></author><author><keyname>Tang</keyname><forenames>Yandong</forenames></author><author><keyname>Zhang</keyname><forenames>Hai</forenames></author><author><keyname>Tang</keyname><forenames>Shaojie</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author></authors><title>Efficient Over-parameterized Matrix Sensing from Noisy Measurements via   Alternating Preconditioned Gradient Descent</title><categories>cs.LG math.OC stat.ML</categories><comments>18 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the noisy matrix sensing problem in the over-parameterization setting, where the estimated rank $r$ is larger than the true rank $r_\star$. Specifically, our main objective is to recover a matrix $ X_\star \in \mathbb{R}^{n_1 \times n_2} $ with rank $ r_\star $ from noisy measurements using an over-parameterized factorized form $ LR^\top $, where $ L \in \mathbb{R}^{n_1 \times r}, \, R \in \mathbb{R}^{n_2 \times r} $ and $ \min\{n_1, n_2\} \ge r &gt; r_\star $, with the true rank $ r_\star $ being unknown. Recently, preconditioning methods have been proposed to accelerate the convergence of matrix sensing problem compared to vanilla gradient descent, incorporating preconditioning terms $ (L^\top L + \lambda I)^{-1} $ and $ (R^\top R + \lambda I)^{-1} $ into the original gradient. However, these methods require careful tuning of the damping parameter $\lambda$ and are sensitive to initial points and step size. To address these limitations, we propose the alternating preconditioned gradient descent (APGD) algorithm, which alternately updates the two factor matrices, eliminating the need for the damping parameter and enabling faster convergence with larger step sizes. We theoretically prove that APGD achieves near-optimal error convergence at a linear rate, starting from arbitrary random initializations. Through extensive experiments, we validate our theoretical results and demonstrate that APGD outperforms other methods, achieving the fastest convergence rate. Notably, both our theoretical analysis and experimental results illustrate that APGD does not rely on the initialization procedure, making it more practical and versatile. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00464</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00464</id><created>2025-02-01</created><authors><author><keyname>Gimeno-Gómez</keyname><forenames>David</forenames></author><author><keyname>Martínez-Hinarejos</keyname><forenames>Carlos-D.</forenames></author></authors><title>Evaluation of End-to-End Continuous Spanish Lipreading in Different Data   Conditions</title><categories>cs.CV</categories><comments>Accepted in the "Language Resources and Evaluation" journal, Springer   Nature</comments><doi>10.1007/s10579-025-09809-4</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Visual speech recognition remains an open research problem where different challenges must be considered by dispensing with the auditory sense, such as visual ambiguities, the inter-personal variability among speakers, and the complex modeling of silence. Nonetheless, recent remarkable results have been achieved in the field thanks to the availability of large-scale databases and the use of powerful attention mechanisms. Besides, multiple languages apart from English are nowadays a focus of interest. This paper presents noticeable advances in automatic continuous lipreading for Spanish. First, an end-to-end system based on the hybrid CTC/Attention architecture is presented. Experiments are conducted on two corpora of disparate nature, reaching state-of-the-art results that significantly improve the best performance obtained to date for both databases. In addition, a thorough ablation study is carried out, where it is studied how the different components that form the architecture influence the quality of speech recognition. Then, a rigorous error analysis is carried out to investigate the different factors that could affect the learning of the automatic system. Finally, a new Spanish lipreading benchmark is consolidated. Code and trained models are available at https://github.com/david-gimeno/evaluating-end2end-spanish-lipreading. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00465</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00465</id><created>2025-02-01</created><authors><author><keyname>Lyu</keyname><forenames>Shen-Huan</forenames></author><author><keyname>He</keyname><forenames>Yi-Xiao</forenames></author><author><keyname>Wang</keyname><forenames>Yanyan</forenames></author><author><keyname>Qu</keyname><forenames>Zhihao</forenames></author><author><keyname>Tang</keyname><forenames>Bin</forenames></author><author><keyname>Ye</keyname><forenames>Baoliu</forenames></author></authors><title>Enhance Learning Efficiency of Oblique Decision Tree via Feature   Concatenation</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Oblique Decision Tree (ODT) separates the feature space by linear projections, as opposed to the conventional Decision Tree (DT) that forces axis-parallel splits. ODT has been proven to have a stronger representation ability than DT, as it provides a way to create shallower tree structures while still approximating complex decision boundaries. However, its learning efficiency is still insufficient, since the linear projections cannot be transmitted to the child nodes, resulting in a waste of model parameters. In this work, we propose an enhanced ODT method with Feature Concatenation (\texttt{FC-ODT}), which enables in-model feature transformation to transmit the projections along the decision paths. Theoretically, we prove that our method enjoys a faster consistency rate w.r.t. the tree depth, indicating that our method possesses a significant advantage in generalization performance, especially for shallow trees. Experiments show that \texttt{FC-ODT} can outperform the other state-of-the-art decision trees with a limited tree depth. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00466</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00466</id><created>2025-02-01</created><authors><author><keyname>Lee</keyname><forenames>Jia-Hua</forenames></author><author><keyname>Lin</keyname><forenames>Bor-Jiun</forenames></author><author><keyname>Sun</keyname><forenames>Wei-Fang</forenames></author><author><keyname>Lee</keyname><forenames>Chun-Yi</forenames></author></authors><title>Enhancing Memory and Imagination Consistency in Diffusion-based World   Models via Linear-Time Sequence Modeling</title><categories>cs.LG</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  World models are crucial for enabling agents to simulate and plan within environments, yet existing approaches struggle with long-term dependencies and inconsistent predictions. We introduce EDELINE, a novel framework that integrates diffusion models with linear-time state space modelsto enhance memory retention and temporal consistency. EDELINE employs a recurrent embedding module based on Mamba SSMs for processing unbounded sequences, a unified architecture for joint reward and termination prediction, and dynamic loss harmonization to balance multi-task learning. Our results across multiple benchmarks demonstrate EDELINE's superiority and robustness over prior baselines in long-horizon tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00470</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00470</id><created>2025-02-01</created><authors><author><keyname>Wu</keyname><forenames>Runxiong</forenames></author><author><keyname>Liu</keyname><forenames>Dong</forenames></author><author><keyname>Wang</keyname><forenames>Xueqin</forenames></author><author><keyname>Wang</keyname><forenames>Andi</forenames></author></authors><title>Distributed Primal-Dual Algorithms: Unification, Connections, and   Insights</title><categories>math.OC cs.LG stat.ML</categories><comments>15 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study primal-dual algorithms for general empirical risk minimization problems in distributed settings, focusing on two prominent classes of algorithms. The first class is the communication-efficient distributed dual coordinate ascent (CoCoA), derived from the coordinate ascent method for solving the dual problem. The second class is the alternating direction method of multipliers (ADMM), including consensus ADMM, linearized ADMM, and proximal ADMM. We demonstrate that both classes of algorithms can be transformed into a unified update form that involves only primal and dual variables. This discovery reveals key connections between the two classes of algorithms: CoCoA can be interpreted as a special case of proximal ADMM for solving the dual problem, while consensus ADMM is closely related to a proximal ADMM algorithm. This discovery provides the insight that by adjusting the augmented Lagrangian parameter, we can easily enable the ADMM variants to outperform the CoCoA variants. We further explore linearized versions of ADMM and analyze the effects of tuning parameters on these ADMM variants in the distributed setting. Our theoretical findings are supported by extensive simulation studies and real-world data analysis. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00471</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00471</id><created>2025-02-01</created><authors><author><keyname>Chebotarev</keyname><forenames>Pavel</forenames></author></authors><title>Evolution of Society Caused by Collective and Individual Decisions</title><categories>physics.soc-ph cs.GT math.OC</categories><comments>15 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Under the assumptions of the ViSE model, we investigate the welfare and performance of a society consisting of one group (a ``party'') and individualists. In the case of Gaussian proposal generators, the expected capital gains can be expressed in standard functions. The relative effectiveness of individualistic and group strategies of agents, as well as the benefits of the entire society, depend on the level of cooperation, the voting threshold, and the favorability of the environment. We focus on the evolution of society in neutral environments caused by changing its structure and the voting rule in the interests of agents. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00472</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00472</id><created>2025-02-01</created><authors><author><keyname>Chakraborty</keyname><forenames>Dibyajyoti</forenames></author><author><keyname>Mohan</keyname><forenames>Arvind T.</forenames></author><author><keyname>Maulik</keyname><forenames>Romit</forenames></author></authors><title>Binned Spectral Power Loss for Improved Prediction of Chaotic Systems</title><categories>cs.LG math.DS physics.flu-dyn</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Forecasting multiscale chaotic dynamical systems with deep learning remains a formidable challenge due to the spectral bias of neural networks, which hinders the accurate representation of fine-scale structures in long-term predictions. This issue is exacerbated when models are deployed autoregressively, leading to compounding errors and instability. In this work, we introduce a novel approach to mitigate the spectral bias which we call the Binned Spectral Power (BSP) Loss. The BSP loss is a frequency-domain loss function that adaptively weighs errors in predicting both larger and smaller scales of the dataset. Unlike traditional losses that focus on pointwise misfits, our BSP loss explicitly penalizes deviations in the energy distribution across different scales, promoting stable and physically consistent predictions. We demonstrate that the BSP loss mitigates the well-known problem of spectral bias in deep learning. We further validate our approach for the data-driven high-dimensional time-series forecasting of a range of benchmark chaotic systems which are typically intractable due to spectral bias. Our results demonstrate that the BSP loss significantly improves the stability and spectral accuracy of neural forecasting models without requiring architectural modifications. By directly targeting spectral consistency, our approach paves the way for more robust deep learning models for long-term forecasting of chaotic dynamical systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00473</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00473</id><created>2025-02-01</created><authors><author><keyname>Bai</keyname><forenames>Lichen</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author><author><keyname>Xie</keyname><forenames>Zeke</forenames></author></authors><title>Weak-to-Strong Diffusion with Reflection</title><categories>cs.LG cs.CV</categories><comments>20 pages, 19 figures, 14 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of diffusion generative models is to align the learned distribution with the real data distribution through gradient score matching. However, inherent limitations in training data quality, modeling strategies, and architectural design lead to inevitable gap between generated outputs and real data. To reduce this gap, we propose Weak-to-Strong Diffusion (W2SD), a novel framework that utilizes the estimated difference between existing weak and strong models (i.e., weak-to-strong difference) to approximate the gap between an ideal model and a strong model. By employing a reflective operation that alternates between denoising and inversion with weak-to-strong difference, we theoretically understand that W2SD steers latent variables along sampling trajectories toward regions of the real data distribution. W2SD is highly flexible and broadly applicable, enabling diverse improvements through the strategic selection of weak-to-strong model pairs (e.g., DreamShaper vs. SD1.5, good experts vs. bad experts in MoE). Extensive experiments demonstrate that W2SD significantly improves human preference, aesthetic quality, and prompt adherence, achieving SOTA performance across various modalities (e.g., image, video), architectures (e.g., UNet-based, DiT-based, MoE), and benchmarks. For example, Juggernaut-XL with W2SD can improve with the HPSv2 winning rate up to 90% over the original results. Moreover, the performance gains achieved by W2SD markedly outweigh its additional computational overhead, while the cumulative improvements from different weak-to-strong difference further solidify its practical utility and deployability. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00474</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00474</id><created>2025-02-01</created><authors><author><keyname>Becker</keyname><forenames>Timothy James</forenames></author><author><keyname>Gezgin</keyname><forenames>Derin</forenames></author><author><keyname>Wu</keyname><forenames>Jun Yi He</forenames></author><author><keyname>Becker</keyname><forenames>Mary</forenames></author></authors><title>A framework for river connectivity classification using temporal image   processing and attention based neural networks</title><categories>cs.CV cs.LG eess.IV</categories><comments>15 pages, 8 figures</comments><acm-class>I.4.3; I.4.1; I.5.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Measuring the connectivity of water in rivers and streams is essential for effective water resource management. Increased extreme weather events associated with climate change can result in alterations to river and stream connectivity. While traditional stream flow gauges are costly to deploy and limited to large river bodies, trail camera methods are a low-cost and easily deployed alternative to collect hourly data. Image capturing, however requires stream ecologists to manually curate (select and label) tens of thousands of images per year. To improve this workflow, we developed an automated instream trail camera image classification system consisting of three parts: (1) image processing, (2) image augmentation and (3) machine learning. The image preprocessing consists of seven image quality filters, foliage-based luma variance reduction, resizing and bottom-center cropping. Images are balanced using variable amount of generative augmentation using diffusion models and then passed to a machine learning classification model in labeled form. By using the vision transformer architecture and temporal image enhancement in our framework, we are able to increase the 75% base accuracy to 90% for a new unseen site image. We make use of a dataset captured and labeled by staff from the Connecticut Department of Energy and Environmental Protection between 2018-2020. Our results indicate that a combination of temporal image processing and attention-based models are effective at classifying unseen river connectivity images. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00476</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00476</id><created>2025-02-01</created><authors><author><keyname>Perez</keyname><forenames>Beatriz</forenames></author><author><keyname>Minguez</keyname><forenames>Roberto</forenames></author><author><keyname>Guanche</keyname><forenames>Raul</forenames></author></authors><title>Offshore wind farm layout optimization using mathematical programming   techniques</title><categories>cs.CE</categories><journal-ref>Renewable energy 53 (May 2013), 389-399</journal-ref><doi>10.1016/j.renene.2012.12.007</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Offshore wind power is a renewable energy of growing relevance in current electric energy systems, presenting favorable wind conditions in comparison with the sites on land. However, the higher energy yield has to compensate the increment in installation and maintenance costs, thus the importance of optimizing resources. One relevant aspect to increase profitability is the wind farm layout. The aim of this paper is to propose a new method to maximize the expected power production of offshore wind farms by setting the appropriate layout, i.e. minimizing the wake effects. The method uses a sequential procedure for global optimization consisting of two steps: i) an heuristic method to set an initial random layout configuration, and ii) the use of nonlinear mathematical programming techniques for local optimization, which use the random layout as an initial solution. The method takes full advantage of the most up-to-date mathematical programming techniques while performing a global optimization approach, which can be easily parallelized. The performance of the proposed procedure is tested using the German offshore wind farm Alpha Ventus, located in the North Sea, yielding an increment of expected annual power production of 3.52% with respect to the actual configuration. According to current electricity prices in Germany, this constitutes an expected profit increment of almost 1 M per year. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00482</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00482</id><created>2025-02-01</created><authors><author><keyname>Zhao</keyname><forenames>Yiming</forenames></author><author><keyname>De Matteis</keyname><forenames>Tiziano</forenames></author><author><keyname>Bogner</keyname><forenames>Justus</forenames></author></authors><title>How Does Microservice Granularity Impact Energy Consumption and   Performance? A Controlled Experiment</title><categories>cs.SE</categories><comments>Accepted for publication at the International Conference on Software   Architecture 2025 (ICSA'25, see https://conf.researchr.org/home/icsa-2025)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context: Microservice architectures are a widely used software deployment approach, with benefits regarding flexibility and scalability. However, their impact on energy consumption is poorly understood, and often overlooked in favor of performance and other quality attributes (QAs). One understudied concept in this area is microservice granularity, i.e., over how many services the system functionality is distributed.   Objective: We therefore aim to analyze the relationship between microservice granularity and two critical QAs in microservice-based systems: energy consumption and performance.   Method: We conducted a controlled experiment using two open-source microservice-based systems of different scales: the small Pet Clinic system and the large Train Ticket system. For each system, we created three levels of granularity by merging or splitting services (coarse, medium, and fine) and then exposed them to five levels of request frequency.   Results: Our findings revealed that: i) granularity significantly affected both energy consumption and response time, e.g., in the large system, fine granularity consumed on average 461 J more energy (13%) and added 5.2 ms to response time (14%) compared to coarse granularity; ii) higher request loads significantly increased both energy consumption and response times, with moving from 40 to 400 requests / s resulting in 651 J higher energy consumption (23%) and 41.2 ms longer response times (98%); iii) there is a complex relationship between granularity, system scale, energy consumption, and performance that warrants careful consideration in microservice design. We derive generalizable takeaways from our results.   Conclusion: Microservices practitioners should take our findings into account when making granularity-related decisions, especially for large-scale systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00483</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00483</id><created>2025-02-01</created><authors><author><keyname>Yuan</keyname><forenames>Guowu</forenames></author><author><keyname>Liu</keyname><forenames>Shicai</forenames></author></authors><title>Exploration and Practice of Improving Programming Ability for the   Undergraduates Majoring in Computer Science</title><categories>cs.CY cs.SE</categories><comments>10 pages, 5 figures</comments><journal-ref>10.18178/ijiet.2021.11.2.1491</journal-ref><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Programming ability is one of the most important abilities for the undergraduates majoring in computer science. Taking Yunnan University as an example, the necessity and importance of improving the ability of programming is analyzed in this paper. The exploration and practice of improving students' ability of programming are discussed from four aspects: arrangement and reform of programming curriculums, construction of online programming practice innovation platform, certification of programming ability and organization of programming competitions. These reforms have achieved good results in recent years, which can provide reference for the practical teaching reform of computer specialty in relevant universities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00484</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00484</id><created>2025-02-01</created><authors><author><keyname>Gourvès</keyname><forenames>Laurent</forenames></author><author><keyname>Lampis</keyname><forenames>Michael</forenames></author><author><keyname>Melissinos</keyname><forenames>Nikolaos</forenames></author><author><keyname>Pagourtzis</keyname><forenames>Aris</forenames></author></authors><title>Satisfactory Budget Division</title><categories>cs.GT cs.CC cs.DS</categories><comments>Accepted for AAMAS 2025 as an extended abstract</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A divisible budget must be allocated to several projects, and agents are asked for their opinion on how much they would give to each project. We consider that an agent is satisfied by a division of the budget if, for at least a certain predefined number $\tau$ of projects, the part of the budget actually allocated to each project is at least as large as the amount the agent requested. The objective is to find a budget division that ``best satisfies'' the agents. In this context, different problems can be stated and we address the following ones. We study $(i)$ the largest proportion of agents that can be satisfied for any instance, $(ii)$ classes of instances admitting a budget division that satisfies all agents, $(iii)$ the complexity of deciding if, for a given instance, every agent can be satisfied, and finally $(iv)$ the question of finding, for a given instance, the smallest total budget to satisfy all agents. We provide answers to these complementary questions for several natural values of the parameter $\tau$, capturing scenarios where we seek to satisfy for each agent all; almost all; half; or at least one of her requests. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00486</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00486</id><created>2025-02-01</created><authors><author><keyname>Minguez</keyname><forenames>Roberto</forenames></author><author><keyname>Tomas</keyname><forenames>Antonio</forenames></author><author><keyname>Mendez</keyname><forenames>Fernando J.</forenames></author><author><keyname>Medina</keyname><forenames>Raul</forenames></author></authors><title>Mixed extreme wave climate model for reanalysis databases</title><categories>cs.CE</categories><journal-ref>Stochastic Environmental Research and Risk Assessment 27 (4),   757-768, 2013</journal-ref><doi>10.1007/s00477-012-0604-y</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Hindcast or wave reanalysis databases (WRDB) constitute a powerful source with respect to instrumental records in the design of offshore and coastal structures, since they offer important advantages for the statistical characterization of wave climate variables, such as continuous long time records of significant wave heights, mean and peak periods, etc. However, reanalysis data is less accurate than instrumental records, making extreme data analysis derived from WRDB prone to under predict design return period values. This paper proposes a mixed extreme value model to deal with maxima, which takes full advantage of both (i) hindcast or wave reanalysis and (ii) instrumental records, reducing the uncertainty in its predictions. The resulting mixed model consistently merges the information given by both kinds of data sets, and it can be applied to any extreme value analysis distribution, such as generalized extreme value, peaks over threshold or Pareto-Poisson. The methodology is illustrated using both synthetically generated and real data, the latter taken from a given location on the northern Spanish coast. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00488</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00488</id><created>2025-02-01</created><authors><author><keyname>Chen</keyname><forenames>Chuqi</forenames></author><author><keyname>Yang</keyname><forenames>Yahong</forenames></author><author><keyname>Xiang</keyname><forenames>Yang</forenames></author><author><keyname>Hao</keyname><forenames>Wenrui</forenames></author></authors><title>Learn Sharp Interface Solution by Homotopy Dynamics</title><categories>cs.LG cs.NA math.NA</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper explores challenges in training Physics-Informed Neural Networks (PINNs), emphasizing the role of the loss landscape in the training process. We examine difficulties in minimizing the PINN loss function, particularly due to ill-conditioning caused by differential operators in the residual term. We compare gradient-based optimizers Adam, L-BFGS, and their combination \al{}, showing the superiority of \al{}, and introduce a novel second-order optimizer, NysNewton-CG (NNCG), which significantly improves PINN performance. Theoretically, our work elucidates the connection between ill-conditioned differential operators and ill-conditioning in the PINN loss and shows the benefits of combining first- and second-order optimization methods. Our work presents valuable insights and more powerful optimization strategies for training PINNs, which could improve the utility of PINNs for solving difficult partial differential equations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00490</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00490</id><created>2025-02-01</created><authors><author><keyname>Wenshøj</keyname><forenames>Jonathan</forenames></author><author><keyname>Pepin</keyname><forenames>Bob</forenames></author><author><keyname>Selvan</keyname><forenames>Raghavendra</forenames></author></authors><title>Oscillations Make Neural Networks Robust to Quantization</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We challenge the prevailing view that oscillations in Quantization Aware Training (QAT) are merely undesirable artifacts caused by the Straight-Through Estimator (STE). Through theoretical analysis of QAT in linear models, we demonstrate that the gradient of the loss function can be decomposed into two terms: the original full-precision loss and a term that causes quantization oscillations. Based on these insights, we propose a novel regularization method that induces oscillations to improve quantization robustness. Contrary to traditional methods that focuses on minimizing the effects of oscillations, our approach leverages the beneficial aspects of weight oscillations to preserve model performance under quantization. Our empirical results on ResNet-18 and Tiny ViT demonstrate that this counter-intuitive strategy matches QAT accuracy at &gt;= 3-bit weight quantization, while maintaining close to full precision accuracy at bits greater than the target bit. Our work therefore provides a new perspective on model preparation for quantization, particularly for finding weights that are robust to changes in the bit of the quantizer -- an area where current methods struggle to match the accuracy of QAT at specific bits. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00494</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00494</id><created>2025-02-01</created><authors><author><keyname>Zheng</keyname><forenames>Shuyuan</forenames></author><author><keyname>Cai</keyname><forenames>Sudong</forenames></author><author><keyname>Xiao</keyname><forenames>Chuan</forenames></author><author><keyname>Cao</keyname><forenames>Yang</forenames></author><author><keyname>Qin</keyname><forenames>Jainbin</forenames></author><author><keyname>Yoshikawa</keyname><forenames>Masatoshi</forenames></author><author><keyname>Onizuka</keyname><forenames>Makoto</forenames></author></authors><title>Data Overvaluation Attack and Truthful Data Valuation</title><categories>cs.CR cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In collaborative machine learning, data valuation, i.e., evaluating the contribution of each client' data to the machine learning model, has become a critical task for incentivizing and selecting positive data contributions. However, existing studies often assume that clients engage in data valuation truthfully, overlooking the practical motivation for clients to exaggerate their contributions. To unlock this threat, this paper introduces the first data overvaluation attack, enabling strategic clients to have their data significantly overvalued. Furthermore, we propose a truthful data valuation metric, named Truth-Shapley. Truth-Shapley is the unique metric that guarantees some promising axioms for data valuation while ensuring that clients' optimal strategy is to perform truthful data valuation. Our experiments demonstrate the vulnerability of existing data valuation metrics to the data overvaluation attack and validate the robustness and effectiveness of Truth-Shapley. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00495</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00495</id><created>2025-02-01</created><authors><author><keyname>Torkestani</keyname><forenames>Mohammad Saleh</forenames></author><author><keyname>Davis</keyname><forenames>Robert</forenames></author><author><keyname>Sarrafzadeh</keyname><forenames>Abdolhossein</forenames></author></authors><title>Looking into the Future of Health-Care Services: Can Life-Like Agents   Change the Future of Health-Care Services?</title><categories>cs.CY cs.AI</categories><comments>6 pages, 2 figures, 3rd International Conference on Machine Learning   and Computing (ICMLC 2011): February 26-28, 2011, Singapore</comments><journal-ref>3rd International Conference on Machine Learning and Computing   (ICMLC 2011): February 26-28, 2011, Singapore. ISBN: 978-1-4244-9252-7</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Time constraints on doctor patient interaction and restricted access to specialists under the managed care system led to increasingly referring to computers as a medical information source and a self-health-care management tool. However, research show that less than 40% of information seekers indicated that online information helped them to make a decision about their health. Searching multiple web sites that need basic computer skills, lack of interaction and no face to face interaction in most search engines and some social issues, led us to develop a specialized life-like agent that would overcome mentioned problems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00497</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00497</id><created>2025-02-01</created><authors><author><keyname>Jeong</keyname><forenames>Sam</forenames></author><author><keyname>Kim</keyname><forenames>Hae Yong</forenames></author></authors><title>Convolutional Fourier Analysis Network (CFAN): A Unified Time-Frequency   Approach for ECG Classification</title><categories>cs.LG eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning has transformed the classification of biomedical signals such as electrocardiograms (ECGs). Advances in deep learning, particularly convolutional neural networks (CNNs), enable automatic feature extraction, raising the question: Can combining time- and frequency-domain attributes enhance classification accuracy? To explore this, we evaluated three ECG classification tasks: (1) arrhythmia detection, (2) identity recognition, and (3) apnea detection. We initially tested three methods: (i) 2-D spectrogram-based frequency-time classification (SPECT), (ii) time-domain classification using a 1-D CNN (CNN1D), and (iii) frequency-domain classification using a Fourier transform-based CNN (FFT1D). Performance was validated using K-fold cross-validation. Among these, CNN1D (time only) performed best, followed by SPECT (time-frequency) and FFT1D (frequency only). Surprisingly, SPECT, which integrates time- and frequency-domain features, performed worse than CNN1D, suggesting a need for a more effective time and frequency fusion approach. To address this, we tested the recently proposed Fourier Analysis Network (FAN), which combines time- and frequency-domain features. However, FAN performed comparably to CNN1D, excelling in some tasks while underperforming in others. To enhance this approach, we developed the Convolutional Fourier Analysis Network (CFAN), which integrates FAN with CNN. CFAN outperformed all previous methods across all classification tasks. These findings underscore the advantages of combining time- and frequency-domain features, demonstrating CFAN's potential as a powerful and versatile solution for ECG classification and broader biomedical signal analysis </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00498</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00498</id><created>2025-02-01</created><authors><author><keyname>Chen</keyname><forenames>Yuxuan</forenames></author><author><keyname>Zhu</keyname><forenames>Xu</forenames></author><author><keyname>Zhou</keyname><forenames>Hua</forenames></author><author><keyname>Ren</keyname><forenames>Zhuyin</forenames></author></authors><title>MetaOpenFOAM 2.0: Large Language Model Driven Chain of Thought for   Automating CFD Simulation and Post-Processing</title><categories>cs.AI physics.comp-ph</categories><comments>16 pages,11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational Fluid Dynamics (CFD) is widely used in aerospace, energy, and biology to model fluid flow, heat transfer, and chemical reactions. While Large Language Models (LLMs) have transformed various domains, their application in CFD remains limited, particularly for complex tasks like post-processing. To bridge this gap, we introduce MetaOpenFOAM 2.0, which leverages Chain of Thought (COT) decomposition and iterative verification to enhance accessibility for non-expert users through natural language inputs. Tested on a new benchmark covering simulation (fluid flow, heat transfer, combustion) and post-processing (extraction, visualization), MetaOpenFOAM 2.0 achieved an Executability score of 6.3/7 and a pass rate of 86.9%, significantly outperforming MetaOpenFOAM 1.0 (2.1/7, 0%). Additionally, it proved cost-efficient, averaging $0.15 per case. An ablation study confirmed that COT-driven decomposition and iterative refinement substantially improved task performance. Furthermore, scaling laws showed that increasing COT steps enhanced accuracy while raising token usage, aligning with LLM post-training scaling trends. These results highlight the transformative potential of LLMs in automating CFD workflows for industrial and research applications. Code is available at https://github.com/Terry-cyx/MetaOpenFOAM </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00499</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00499</id><created>2025-02-01</created><authors><author><keyname>Shaimov</keyname><forenames>Nikita</forenames></author><author><keyname>Lomazova</keyname><forenames>Irina</forenames></author><author><keyname>Mitsyuk</keyname><forenames>Alexey</forenames></author></authors><title>Discovering Directly-Follows Graph Model for Acyclic Processes</title><categories>cs.AI</categories><comments>24 pages, 15 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Process mining is the common name for a range of methods and approaches aimed at analysing and improving processes. Specifically, methods that aim to derive process models from event logs fall under the category of process discovery. Within the range of processes, acyclic processes form a distinct category. In such processes, previously performed actions are not repeated, forming chains of unique actions. However, due to differences in the order of actions, existing process discovery methods can provide models containing cycles even if a process is acyclic. This paper presents a new process discovery algorithm that allows to discover acyclic DFG models for acyclic processes. A model is discovered by partitioning an event log into parts that provide acyclic DFG models and merging them while avoiding the formation of cycles. The resulting algorithm was tested both on real-life and artificial event logs. Absence of cycles improves model visual clarity and precision, also allowing to apply cycle-sensitive methods or visualisations to the model. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00500</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00500</id><created>2025-02-01</created><authors><author><keyname>Cao</keyname><forenames>Yang</forenames></author><author><keyname>Song</keyname><forenames>Zhao</forenames></author><author><keyname>Yang</keyname><forenames>Chiwun</forenames></author></authors><title>Video Latent Flow Matching: Optimal Polynomial Projections for Video   Interpolation and Extrapolation</title><categories>cs.CV cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper considers an efficient video modeling process called Video Latent Flow Matching (VLFM). Unlike prior works, which randomly sampled latent patches for video generation, our method relies on current strong pre-trained image generation models, modeling a certain caption-guided flow of latent patches that can be decoded to time-dependent video frames. We first speculate multiple images of a video are differentiable with respect to time in some latent space. Based on this conjecture, we introduce the HiPPO framework to approximate the optimal projection for polynomials to generate the probability path. Our approach gains the theoretical benefits of the bounded universal approximation error and timescale robustness. Moreover, VLFM processes the interpolation and extrapolation abilities for video generation with arbitrary frame rates. We conduct experiments on several text-to-video datasets to showcase the effectiveness of our method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00501</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00501</id><created>2025-02-01</created><authors><author><keyname>Yang</keyname><forenames>Tianyu</forenames></author><author><keyname>Noor-E-Alam</keyname><forenames>Md.</forenames></author></authors><title>Optimizing Feature Selection in Causal Inference: A Three-Stage   Computational Framework for Unbiased Estimation</title><categories>stat.ME cs.AI cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Feature selection is an important but challenging task in causal inference for obtaining unbiased estimates of causal quantities. Properly selected features in causal inference not only significantly reduce the time required to implement a matching algorithm but, more importantly, can also reduce the bias and variance when estimating causal quantities. When feature selection techniques are applied in causal inference, the crucial criterion is to select variables that, when used for matching, can achieve an unbiased and robust estimation of causal quantities. Recent research suggests that balancing only on treatment-associated variables introduces bias while balancing on spurious variables increases variance. To address this issue, we propose an enhanced three-stage framework that shows a significant improvement in selecting the desired subset of variables compared to the existing state-of-the-art feature selection framework for causal inference, resulting in lower bias and variance in estimating the causal quantity. We evaluated our proposed framework using a state-of-the-art synthetic data across various settings and observed superior performance within a feasible computation time, ensuring scalability for large-scale datasets. Finally, to demonstrate the applicability of our proposed methodology using large-scale real-world data, we evaluated an important US healthcare policy related to the opioid epidemic crisis: whether opioid use disorder has a causal relationship with suicidal behavior. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00503</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00503</id><created>2025-02-01</created><authors><author><keyname>Zhang</keyname><forenames>Bingwei</forenames></author><author><keyname>Yap</keyname><forenames>Chee</forenames></author></authors><title>A Novel Approach to the Initial Value Problem with a complete validated   algorithm</title><categories>cs.SC cs.NA math.NA</categories><comments>13 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Initial Value Problem (IVP) is concerned with finding solutions to a system of autonomous ordinary differential equations (ODE)   \begin{equation}   \textbf{x}' = \textbf{f}(\textbf{x})   \end{equation} with given initial condition $\textbf{x}(0)\in B_0$ for some box $B_0\subseteq \mathbb{R}^n$. Here $\textbf{f}:\mathbb{R}^n\to\mathbb{R}^n$ and $\textbf{x}:[0,1]\to\mathbb{R}^n$ where $\textbf{f}$ and $\textbf{x}$ are $C^1$-continuous. Let $\texttt{IVP}_\textbf{f}(B_0)$ denote the set of all such solutions $\textbf{x}$. Despite over 40 years of development to design a validated algorithm for the IVP problem, no complete algorithm currently exists.   In this paper, we introduce a novel way to exploit the theory of $\textbf{logarithmic norms}$: we introduce the concept of a $\textbf{radical transform}$ $\pi:\mathbb{R}^n\to\mathbb{R}^n$ to convert the above $(\textbf{x},\textbf{f})$-system into another system $\textbf{y}' = \textbf{g}(\textbf{y})$ so that the $(\textbf{y},\textbf{g})$-space has negative logarithmic norm in any desired small enough neighborhood.   Based on such radical transform steps, we construct a complete validated algorithm for the following $\textbf{End-Enclosure Problem}$:   \begin{equation} INPUT: (\textbf{f}, B_0,\varepsilon), \qquad\qquad OUTPUT: (\underline{B}_0,B_1)   \end{equation} where $B_0\subseteq \mathbb{R}^n$ is a box, $\varepsilon&gt;0$, such that $\underline{B}_0\subseteq B_0$, the diameter of $B_1$ is at most $\varepsilon$, and $B_1$ is an end-enclosure for $\texttt{IVP}(\underline{B}_0)$, i.e., for all $\textbf{x}\in \texttt{IVP}(\underline{B}_0)$, $\textbf{x}(1)\in B_1$.   A preliminary implementation of our algorithm shows promise. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00507</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00507</id><created>2025-02-01</created><authors><author><keyname>Liu</keyname><forenames>Yi</forenames></author></authors><title>A statistically consistent measure of Semantic Variability using   Language Models</title><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  To address the issue of variability in the output generated by a language model, we present a measure of semantic variability that is statistically consistent under mild assumptions. This measure, denoted as semantic spectral entropy, is a easy to implement algorithm that requires just off the shelf language models. We put very few restrictions on the language models and we have shown in a clear simulation studies that such method can generate accurate metric despite randomness that arise from the language models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00510</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00510</id><created>2025-02-01</created><authors><author><keyname>Yang</keyname><forenames>Yingxuan</forenames></author><author><keyname>Huang</keyname><forenames>Bo</forenames></author><author><keyname>Qi</keyname><forenames>Siyuan</forenames></author><author><keyname>Feng</keyname><forenames>Chao</forenames></author><author><keyname>Hu</keyname><forenames>Haoyi</forenames></author><author><keyname>Zhu</keyname><forenames>Yuxuan</forenames></author><author><keyname>Hu</keyname><forenames>Jinbo</forenames></author><author><keyname>Zhao</keyname><forenames>Haoran</forenames></author><author><keyname>He</keyname><forenames>Ziyi</forenames></author><author><keyname>Liu</keyname><forenames>Xiao</forenames></author><author><keyname>Wang</keyname><forenames>Zongyu</forenames></author><author><keyname>Qiu</keyname><forenames>Lin</forenames></author><author><keyname>Cao</keyname><forenames>Xuezhi</forenames></author><author><keyname>Cai</keyname><forenames>Xunliang</forenames></author><author><keyname>Yu</keyname><forenames>Yong</forenames></author><author><keyname>Zhang</keyname><forenames>Weinan</forenames></author></authors><title>Who's the MVP? A Game-Theoretic Evaluation Benchmark for Modular   Attribution in LLM Agents</title><categories>cs.AI cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Model (LLM) agents frameworks often employ modular architectures, incorporating components such as planning, reasoning, action execution, and reflection to tackle complex tasks. However, quantifying the contribution of each module to overall system performance remains a significant challenge, impeding optimization and interpretability. To address this, we introduce CapaBench (Capability-level Assessment Benchmark), an evaluation framework grounded in cooperative game theory's Shapley Value, which systematically measures the marginal impact of individual modules and their interactions within an agent's architecture. By replacing default modules with test variants across all possible combinations, CapaBench provides a principle method for attributing performance contributions. Key contributions include: (1) We are the first to propose a Shapley Value-based methodology for quantifying the contributions of capabilities in LLM agents; (2) Modules with high Shapley Values consistently lead to predictable performance gains when combined, enabling targeted optimization; and (3) We build a multi-round dataset of over 1,000 entries spanning diverse domains and practical task scenarios, enabling comprehensive evaluation of agent capabilities. CapaBench bridges the gap between component-level evaluation and holistic system assessment, providing actionable insights for optimizing modular LLM agents and advancing their deployment in complex, real-world scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00511</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00511</id><created>2025-02-01</created><authors><author><keyname>Zhou</keyname><forenames>Zhi</forenames></author><author><keyname>Yuhao</keyname><forenames>Tan</forenames></author><author><keyname>Li</keyname><forenames>Zenan</forenames></author><author><keyname>Yao</keyname><forenames>Yuan</forenames></author><author><keyname>Guo</keyname><forenames>Lan-Zhe</forenames></author><author><keyname>Ma</keyname><forenames>Xiaoxing</forenames></author><author><keyname>Li</keyname><forenames>Yu-Feng</forenames></author></authors><title>Bridging Internal Probability and Self-Consistency for Effective and   Efficient LLM Reasoning</title><categories>cs.LG cs.AI cs.CL</categories><comments>Under review</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Recent advancements in large language models (LLMs) have demonstrated remarkable reasoning capabilities. However, single-shot inference often yields unreliable results for complex reasoning tasks, leading researchers to explore multiple reasoning paths through methods such as perplexity and self-consistency. In this paper, we present the first theoretical error decomposition analysis of these techniques, breaking down their error into estimation error and model error. Our analysis reveals a fundamental trade-off: perplexity methods suffer from substantial model error due to the absence of a proper consistency function, while self-consistency exhibits high estimation error due to a slow error convergence rate. To overcome these limitations, we propose Reasoning-Pruning Perplexity Consistency (RPC). This approach combines Perplexity Consistency, which seamlessly integrates LLM perplexity with self-consistency, and Reasoning Pruning, which eliminates low-probability reasoning paths to effectively prevent the degeneration of estimation error reduction. Theoretical analysis demonstrates that RPC not only accelerates the convergence rate of estimation error to an exponential level but also holds strong potential for further reducing model error. Extensive empirical evaluations on seven benchmark datasets confirm that RPC can significantly improve reasoning performance, sample efficiency, and confidence reliability. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00513</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00513</id><created>2025-02-01</created><authors><author><keyname>Yamamoto</keyname><forenames>Koya</forenames></author><author><keyname>Kelly</keyname><forenames>Patrick</forenames></author><author><keyname>Majji</keyname><forenames>Manoranjan</forenames></author><author><keyname>Guzman</keyname><forenames>Felipe</forenames></author></authors><title>Covariance Analysis of Attitude and Angular Rate Estimation using   Accelerometers</title><categories>eess.SY cs.SY</categories><comments>Attitude estimation paper, 10 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this work a method for using accelerometers for the determination of angular velocity and acceleration is presented. Minimum sensor requirements and insights into how an array of accelerometers can be configured to maximize estimator performance are considered. The framework presented utilizes linear least squares to estimate functions that are quadratic in angular velocity. Simple methods for determining the sign of the spin axis and the linearized covariance approximation are presented and found to perform quite effectively when compared to results obtained by Monte Carlo. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00519</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00519</id><created>2025-02-01</created><authors><author><keyname>Pai</keyname><forenames>Kunal</forenames></author><author><keyname>Devanbu</keyname><forenames>Premkumar</forenames></author><author><keyname>Ahmed</keyname><forenames>Toufique</forenames></author></authors><title>CoDocBench: A Dataset for Code-Documentation Alignment in Software   Maintenance</title><categories>cs.SE cs.LG</categories><comments>Accepted at the International Conference on Mining Software   Repositories (MSR): Data and Tool Showcase Track - 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  One of the central tasks in software maintenance is being able to understand and develop code changes. Thus, given a natural language description of the desired new operation of a function, an agent (human or AI) might be asked to generate the set of edits to that function to implement the desired new operation; likewise, given a set of edits to a function, an agent might be asked to generate a changed description, of that function's new workings. Thus, there is an incentive to train a neural model for change-related tasks. Motivated by this, we offer a new, "natural", large dataset of coupled changes to code and documentation mined from actual high-quality GitHub projects, where each sample represents a single commit where the code and the associated docstring were changed together. We present the methodology for gathering the dataset, and some sample, challenging (but realistic) tasks where our dataset provides opportunities for both learning and evaluation. We find that current models (specifically Llama-3.1 405B, Mixtral 8$\times$22B) do find these maintenance-related tasks challenging. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00520</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00520</id><created>2025-02-01</created><authors><author><keyname>Han</keyname><forenames>Jiale</forenames></author><author><keyname>Dai</keyname><forenames>Xiaowu</forenames></author><author><keyname>Zhu</keyname><forenames>Yuhua</forenames></author></authors><title>Variance Reduction via Resampling and Experience Replay</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Experience replay is a foundational technique in reinforcement learning that enhances learning stability by storing past experiences in a replay buffer and reusing them during training. Despite its practical success, its theoretical properties remain underexplored. In this paper, we present a theoretical framework that models experience replay using resampled $U$- and $V$-statistics, providing rigorous variance reduction guarantees. We apply this framework to policy evaluation tasks using the Least-Squares Temporal Difference (LSTD) algorithm and a Partial Differential Equation (PDE)-based model-free algorithm, demonstrating significant improvements in stability and efficiency, particularly in data-scarce scenarios. Beyond policy evaluation, we extend the framework to kernel ridge regression, showing that the experience replay-based method reduces the computational cost from the traditional $O(n^3)$ in time to as low as $O(n^2)$ in time while simultaneously reducing variance. Extensive numerical experiments validate our theoretical findings, demonstrating the broad applicability and effectiveness of experience replay in diverse machine learning tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00527</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00527</id><created>2025-02-01</created><authors><author><keyname>Wu</keyname><forenames>Songhao</forenames></author><author><keyname>Lv</keyname><forenames>Ang</forenames></author><author><keyname>Feng</keyname><forenames>Xiao</forenames></author><author><keyname>Zhang</keyname><forenames>Yufei</forenames></author><author><keyname>Zhang</keyname><forenames>Xun</forenames></author><author><keyname>Yin</keyname><forenames>Guojun</forenames></author><author><keyname>Lin</keyname><forenames>Wei</forenames></author><author><keyname>Yan</keyname><forenames>Rui</forenames></author></authors><title>PolarQuant: Leveraging Polar Transformation for Efficient Key Cache   Quantization and Decoding Acceleration</title><categories>cs.LG cs.CL</categories><comments>preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The KV cache in large language models is a dominant factor in memory usage, limiting their broader applicability. Quantizing the cache to lower bit widths is an effective way to reduce computational costs; however, previous methods struggle with quantizing key vectors due to outliers, resulting in excessive overhead. We propose a novel quantization approach called PolarQuant, which efficiently addresses the outlier challenge. We observe that outliers typically appear in only one of two dimensions, which are rotated together by a specific angle when rotary position embeddings are applied. When represented as two-dimensional vectors, these dimensions exhibit well-structured patterns, with radii and angles smoothly distributed in polar coordinates. This alleviates the challenge of outliers on per-channel quantization, making them well-suited for quantization. Thus, PolarQuant divides key vectors into groups of two-dimensional sub-vectors, encoding them as the corresponding quantized radius and the polar angle, rather than quantizing original key vectors directly. PolarQuant achieves the superior efficiency in KV cache quantization and accelerates the decoding process by turning the query-key inner product into a table lookup, all while maintaining the downstream performance of full-precision models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00528</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00528</id><created>2025-02-01</created><authors><author><keyname>Huemann</keyname><forenames>Zachary</forenames></author><author><keyname>Church</keyname><forenames>Samuel</forenames></author><author><keyname>Warner</keyname><forenames>Joshua D.</forenames></author><author><keyname>Tran</keyname><forenames>Daniel</forenames></author><author><keyname>Tie</keyname><forenames>Xin</forenames></author><author><keyname>McMillan</keyname><forenames>Alan B</forenames></author><author><keyname>Hu</keyname><forenames>Junjie</forenames></author><author><keyname>Cho</keyname><forenames>Steve Y.</forenames></author><author><keyname>Lubner</keyname><forenames>Meghan</forenames></author><author><keyname>Bradshaw</keyname><forenames>Tyler J.</forenames></author></authors><title>Vision-Language Modeling in PET/CT for Visual Grounding of Positive   Findings</title><categories>cs.CV cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Vision-language models can connect the text description of an object to its specific location in an image through visual grounding. This has potential applications in enhanced radiology reporting. However, these models require large annotated image-text datasets, which are lacking for PET/CT. We developed an automated pipeline to generate weak labels linking PET/CT report descriptions to their image locations and used it to train a 3D vision-language visual grounding model. Our pipeline finds positive findings in PET/CT reports by identifying mentions of SUVmax and axial slice numbers. From 25,578 PET/CT exams, we extracted 11,356 sentence-label pairs. Using this data, we trained ConTEXTual Net 3D, which integrates text embeddings from a large language model with a 3D nnU-Net via token-level cross-attention. The model's performance was compared against LLMSeg, a 2.5D version of ConTEXTual Net, and two nuclear medicine physicians. The weak-labeling pipeline accurately identified lesion locations in 98% of cases (246/251), with 7.5% requiring boundary adjustments. ConTEXTual Net 3D achieved an F1 score of 0.80, outperforming LLMSeg (F1=0.22) and the 2.5D model (F1=0.53), though it underperformed both physicians (F1=0.94 and 0.91). The model achieved better performance on FDG (F1=0.78) and DCFPyL (F1=0.75) exams, while performance dropped on DOTATE (F1=0.58) and Fluciclovine (F1=0.66). The model performed consistently across lesion sizes but showed reduced accuracy on lesions with low uptake. Our novel weak labeling pipeline accurately produced an annotated dataset of PET/CT image-text pairs, facilitating the development of 3D visual grounding models. ConTEXTual Net 3D significantly outperformed other models but fell short of the performance of nuclear medicine physicians. Our study suggests that even larger datasets may be needed to close this performance gap. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00529</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00529</id><created>2025-02-01</created><authors><author><keyname>Khan</keyname><forenames>Arijit</forenames></author><author><keyname>Ke</keyname><forenames>Xiangyu</forenames></author><author><keyname>Wu</keyname><forenames>Yinghui</forenames></author></authors><title>Graph Data Management and Graph Machine Learning: Synergies and   Opportunities</title><categories>cs.DB</categories><comments>15 pages, 1 figure</comments><journal-ref>ACM SIGMOD Record 2025</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The ubiquity of machine learning, particularly deep learning, applied to graphs is evident in applications ranging from cheminformatics (drug discovery) and bioinformatics (protein interaction prediction) to knowledge graph-based query answering, fraud detection, and social network analysis. Concurrently, graph data management deals with the research and development of effective, efficient, scalable, robust, and user-friendly systems and algorithms for storing, processing, and analyzing vast quantities of heterogeneous and complex graph data. Our survey provides a comprehensive overview of the synergies between graph data management and graph machine learning, illustrating how they intertwine and mutually reinforce each other across the entire spectrum of the graph data science and machine learning pipeline. Specifically, the survey highlights two crucial aspects: (1) How graph data management enhances graph machine learning, including contributions such as improved graph neural network performance through graph data cleaning, scalable graph embedding, efficient graph-based vector data management, robust graph neural networks, user-friendly explainability methods; and (2) how graph machine learning, in turn, aids in graph data management, with a focus on applications like query answering over knowledge graphs and various data science tasks. We discuss pertinent open problems and delineate crucial research directions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00530</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00530</id><created>2025-02-01</created><authors><author><keyname>Fan</keyname><forenames>Xudong</forenames></author><author><keyname>Hackl</keyname><forenames>Jürgen</forenames></author></authors><title>Generic Multimodal Spatially Graph Network for Spatially Embedded   Network Representation Learning</title><categories>cs.LG cs.AI cs.SI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Spatially embedded networks (SENs) represent a special type of complex graph, whose topologies are constrained by the networks' embedded spatial environments. The graph representation of such networks is thereby influenced by the embedded spatial features of both nodes and edges. Accurate network representation of the graph structure and graph features is a fundamental task for various graph-related tasks. In this study, a Generic Multimodal Spatially Graph Convolutional Network (GMu-SGCN) is developed for efficient representation of spatially embedded networks. The developed GMu-SGCN model has the ability to learn the node connection pattern via multimodal node and edge features. In order to evaluate the developed model, a river network dataset and a power network dataset have been used as test beds. The river network represents the naturally developed SENs, whereas the power network represents a man-made network. Both types of networks are heavily constrained by the spatial environments and uncertainties from nature. Comprehensive evaluation analysis shows the developed GMu-SGCN can improve accuracy of the edge existence prediction task by 37.1\% compared to a GraphSAGE model which only considers the node's position feature in a power network test bed. Our model demonstrates the importance of considering the multidimensional spatial feature for spatially embedded network representation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00532</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00532</id><created>2025-02-01</created><authors><author><keyname>Elele</keyname><forenames>Martin Joel Mouk</forenames></author><author><keyname>Pau</keyname><forenames>Danilo</forenames></author><author><keyname>Zhuang</keyname><forenames>Shixin</forenames></author><author><keyname>Facchinetti</keyname><forenames>Tullio</forenames></author></authors><title>Enhancing Field-Oriented Control of Electric Drives with Tiny Neural   Network Optimized for Micro-controllers</title><categories>cs.LG cs.SY eess.SY</categories><comments>This paper has been submitted to the EDGE AI Research Symposium 2025   (https://conf.researchr.org/home/tinyml-symp-2025#Call-for-papers previously   known as tinyML Research Symposium). It was peer reviewed and camera ready   updated accordingly to reviewer's feedback. It will be presented in EDGE AI   FOUNDATION Austin 2025 (https://www.edgeaifoundation.org/events/austin-2024)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The deployment of neural networks on resource-constrained micro-controllers has gained momentum, driving many advancements in Tiny Neural Networks. This paper introduces a tiny feed-forward neural network, TinyFC, integrated into the Field-Oriented Control (FOC) of Permanent Magnet Synchronous Motors (PMSMs). Proportional-Integral (PI) controllers are widely used in FOC for their simplicity, although their limitations in handling nonlinear dynamics hinder precision. To address this issue, a lightweight 1,400 parameters TinyFC was devised to enhance the FOC performance while fitting into the computational and memory constraints of a micro-controller. Advanced optimization techniques, including pruning, hyperparameter tuning, and quantization to 8-bit integers, were applied to reduce the model's footprint while preserving the network effectiveness. Simulation results show the proposed approach significantly reduced overshoot by up to 87.5%, with the pruned model achieving complete overshoot elimination, highlighting the potential of tiny neural networks in real-time motor control applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00533</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00533</id><created>2025-02-01</created><authors><author><keyname>Vargun</keyname><forenames>Duygu</forenames></author><author><keyname>Monteiro</keyname><forenames>Igor O.</forenames></author><author><keyname>Rebholz</keyname><forenames>Leo G.</forenames></author></authors><title>Anderson acceleration of a Picard solver for the Oldroyd-B model of   viscoelastic fluids</title><categories>math.NA cs.NA</categories><comments>4 figures, 2 tables</comments><msc-class>65B99</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study an iterative nonlinear solver for the Oldroyd-B system describing incompressible viscoelastic fluid flow. We establish a range of attributes of the fixed-point-based solver, together with the conditions under which it becomes contractive and examining the smoothness properties of its corresponding fixed-point function. Under these properties, we demonstrate that the solver meets the necessary conditions for recent Anderson acceleration (AA) framework, thereby showing that AA enhances the solver's linear convergence rate. Results from two benchmark tests illustrate how AA improves the solver's ability to converge as the Weissenberg number is increased. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00534</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00534</id><created>2025-02-01</created><authors><author><keyname>Chai</keyname><forenames>Jinhang</forenames></author><author><keyname>Chen</keyname><forenames>Elynn</forenames></author><author><keyname>Yang</keyname><forenames>Lin</forenames></author></authors><title>Transition Transfer $Q$-Learning for Composite Markov Decision Processes</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  To bridge the gap between empirical success and theoretical understanding in transfer reinforcement learning (RL), we study a principled approach with provable performance guarantees. We introduce a novel composite MDP framework where high-dimensional transition dynamics are modeled as the sum of a low-rank component representing shared structure and a sparse component capturing task-specific variations. This relaxes the common assumption of purely low-rank transition models, allowing for more realistic scenarios where tasks share core dynamics but maintain individual variations. We introduce UCB-TQL (Upper Confidence Bound Transfer Q-Learning), designed for transfer RL scenarios where multiple tasks share core linear MDP dynamics but diverge along sparse dimensions. When applying UCB-TQL to a target task after training on a source task with sufficient trajectories, we achieve a regret bound of $\tilde{O}(\sqrt{eH^5N})$ that scales independently of the ambient dimension. Here, $N$ represents the number of trajectories in the target task, while $e$ quantifies the sparse differences between tasks. This result demonstrates substantial improvement over single task RL by effectively leveraging their structural similarities. Our theoretical analysis provides rigorous guarantees for how UCB-TQL simultaneously exploits shared dynamics while adapting to task-specific variations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00535</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00535</id><created>2025-02-01</created><authors><author><keyname>Oro</keyname><forenames>David</forenames></author><author><keyname>Fernández</keyname><forenames>Carles</forenames></author><author><keyname>Martorell</keyname><forenames>Xavier</forenames></author><author><keyname>Hernando</keyname><forenames>Javier</forenames></author></authors><title>Work-Efficient Parallel Non-Maximum Suppression Kernels</title><categories>cs.CV cs.DC</categories><comments>Code: https://github.com/hertasecurity/gpu-nms</comments><acm-class>D.1.3; I.4.8</acm-class><journal-ref>The Computer Journal, Volume 65, Issue 4, April 2022, Pages   773-787</journal-ref><doi>10.1093/comjnl/bxaa108</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In the context of object detection, sliding-window classifiers and single-shot Convolutional Neural Network (CNN) meta-architectures typically yield multiple overlapping candidate windows with similar high scores around the true location of a particular object. Non-Maximum Suppression (NMS) is the process of selecting a single representative candidate within this cluster of detections, so as to obtain a unique detection per object appearing on a given picture. In this paper, we present a highly scalable NMS algorithm for embedded GPU architectures that is designed from scratch to handle workloads featuring thousands of simultaneous detections on a given picture. Our kernels are directly applicable to other sequential NMS algorithms such as FeatureNMS, Soft-NMS or AdaptiveNMS that share the inner workings of the classic greedy NMS method. The obtained performance results show that our parallel NMS algorithm is capable of clustering 1024 simultaneous detected objects per frame in roughly 1 ms on both NVIDIA Tegra X1 and NVIDIA Tegra X2 on-die GPUs, while taking 2 ms on NVIDIA Tegra K1. Furthermore, our proposed parallel greedy NMS algorithm yields a 14x-40x speed up when compared to state-of-the-art NMS methods that require learning a CNN from annotated data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00536</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00536</id><created>2025-02-01</created><authors><author><keyname>Xiao</keyname><forenames>Wenbo</forenames></author><author><keyname>Xu</keyname><forenames>Zhihao</forenames></author><author><keyname>Liang</keyname><forenames>Guiping</forenames></author><author><keyname>Deng</keyname><forenames>Yangjun</forenames></author><author><keyname>Xiao</keyname><forenames>Yi</forenames></author></authors><title>CAD: Confidence-Aware Adaptive Displacement for Semi-Supervised Medical   Image Segmentation</title><categories>cs.CV cs.LG</categories><comments>9 pages, 3 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semi-supervised medical image segmentation aims to leverage minimal expert annotations, yet remains confronted by challenges in maintaining high-quality consistency learning. Excessive perturbations can degrade alignment and hinder precise decision boundaries, especially in regions with uncertain predictions. In this paper, we introduce Confidence-Aware Adaptive Displacement (CAD), a framework that selectively identifies and replaces the largest low-confidence regions with high-confidence patches. By dynamically adjusting both the maximum allowable replacement size and the confidence threshold throughout training, CAD progressively refines the segmentation quality without overwhelming the learning process. Experimental results on public medical datasets demonstrate that CAD effectively enhances segmentation quality, establishing new state-of-the-art accuracy in this field. The source code will be released after the paper is published. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00537</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00537</id><created>2025-02-01</created><authors><author><keyname>Tanjim</keyname><forenames>Md Mehrab</forenames></author><author><keyname>Chen</keyname><forenames>Xiang</forenames></author><author><keyname>Bursztyn</keyname><forenames>Victor S.</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Uttaran</forenames></author><author><keyname>Mai</keyname><forenames>Tung</forenames></author><author><keyname>Muppala</keyname><forenames>Vaishnavi</forenames></author><author><keyname>Maharaj</keyname><forenames>Akash</forenames></author><author><keyname>Mitra</keyname><forenames>Saayan</forenames></author><author><keyname>Koh</keyname><forenames>Eunyee</forenames></author><author><keyname>Li</keyname><forenames>Yunyao</forenames></author><author><keyname>Russell</keyname><forenames>Ken</forenames></author></authors><title>Detecting Ambiguities to Guide Query Rewrite for Robust Conversations in   Enterprise AI Assistants</title><categories>cs.CL</categories><comments>Preprint</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Multi-turn conversations with an Enterprise AI Assistant can be challenging due to conversational dependencies in questions, leading to ambiguities and errors. To address this, we propose an NLU-NLG framework for ambiguity detection and resolution through reformulating query automatically and introduce a new task called "Ambiguity-guided Query Rewrite." To detect ambiguities, we develop a taxonomy based on real user conversational logs and draw insights from it to design rules and extract features for a classifier which yields superior performance in detecting ambiguous queries, outperforming LLM-based baselines. Furthermore, coupling the query rewrite module with our ambiguity detecting classifier shows that this end-to-end framework can effectively mitigate ambiguities without risking unnecessary insertions of unwanted phrases for clear queries, leading to an improvement in the overall performance of the AI Assistant. Due to its significance, this has been deployed in the real world application, namely Adobe Experience Platform AI Assistant. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00540</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00540</id><created>2025-02-01</created><authors><author><keyname>Cerrato</keyname><forenames>Antonio</forenames></author><author><keyname>González</keyname><forenames>José A.</forenames></author><author><keyname>Rodríguez-Temblequer</keyname><forenames>Luis</forenames></author></authors><title>Boundary element formulation of the Mild-Slope Equation for harmonic   water waves propagating over unidirectional variable bathymetries</title><categories>math.NA cs.NA physics.flu-dyn</categories><journal-ref>Engineering Analysis with Boundary Elements, Volume 62, January   2016, Pages 22-34</journal-ref><doi>10.1016/j.enganabound.2015.09.006</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper presents a boundary element formulation for the solution of the Mild-Slope equation in wave propagation problems with variable water depth in one direction. Based on the Green's function approximation proposed by Belibassakis \cite{Belibassakis2000}, a complete fundamental-solution kernel is developed and combined with a boundary element scheme for the solution of water wave propagation problems in closed and open domains where the bathymetry changes arbitrarily and smoothly in a preferential direction. The ability of the proposed formulation to accurately represent wave phenomena like refraction, reflection, diffraction and shoaling, is demonstrated with the solution of some example problems, in which arbitrary geometries and variable seabed profiles with slopes up to 1:3 are considered. The obtained results are also compared with theoretical solutions, showing an excellent agreement that demonstrates its potential. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00543</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00543</id><created>2025-02-01</created><authors><author><keyname>Nazeri</keyname><forenames>Mohammad</forenames></author><author><keyname>Pokhrel</keyname><forenames>Anuj</forenames></author><author><keyname>Card</keyname><forenames>Alexandyr</forenames></author><author><keyname>Datar</keyname><forenames>Aniket</forenames></author><author><keyname>Warnell</keyname><forenames>Garrett</forenames></author><author><keyname>Xiao</keyname><forenames>Xuesu</forenames></author></authors><title>VertiFormer: A Data-Efficient Multi-Task Transformer for Off-Road Robot   Mobility</title><categories>cs.RO cs.CV cs.LG</categories><comments>9 figures, url: https://github.com/mhnazeri/VertiFormer</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sophisticated learning architectures, e.g., Transformers, present a unique opportunity for robots to understand complex vehicle-terrain kinodynamic interactions for off-road mobility. While internet-scale data are available for Natural Language Processing (NLP) and Computer Vision (CV) tasks to train Transformers, real-world mobility data are difficult to acquire with physical robots navigating off-road terrain. Furthermore, training techniques specifically designed to process text and image data in NLP and CV may not apply to robot mobility. In this paper, we propose VertiFormer, a novel data-efficient multi-task Transformer model trained with only one hour of data to address such challenges of applying Transformer architectures for robot mobility on extremely rugged, vertically challenging, off-road terrain. Specifically, VertiFormer employs a new learnable masked modeling and next token prediction paradigm to predict the next pose, action, and terrain patch to enable a variety of off-road mobility tasks simultaneously, e.g., forward and inverse kinodynamics modeling. The non-autoregressive design mitigates computational bottlenecks and error propagation associated with autoregressive models. VertiFormer's unified modality representation also enhances learning of diverse temporal mappings and state representations, which, combined with multiple objective functions, further improves model generalization. Our experiments offer insights into effectively utilizing Transformers for off-road robot mobility with limited data and demonstrate our efficiently trained Transformer can facilitate multiple off-road mobility tasks onboard a physical mobile robot. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00545</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00545</id><created>2025-02-01</created><authors><author><keyname>Tu</keyname><forenames>Xiaotong</forenames></author><author><keyname>Ma</keyname><forenames>Chenyu</forenames></author><author><keyname>Wu</keyname><forenames>Qingyao</forenames></author><author><keyname>Liu</keyname><forenames>Yinhao</forenames></author><author><keyname>Zhang</keyname><forenames>Hongyang</forenames></author></authors><title>Integrating Frequency Guidance into Multi-source Domain Generalization   for Bearing Fault Diagnosis</title><categories>cs.LG cs.AI cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent generalizable fault diagnosis researches have effectively tackled the distributional shift between unseen working conditions. Most of them mainly focus on learning domain-invariant representation through feature-level methods. However, the increasing numbers of unseen domains may lead to domain-invariant features contain instance-level spurious correlations, which impact the previous models' generalizable ability. To address the limitations, we propose the Fourier-based Augmentation Reconstruction Network, namely FARNet.The methods are motivated by the observation that the Fourier phase component and amplitude component preserve different semantic information of the signals, which can be employed in domain augmentation techniques. The network comprises an amplitude spectrum sub-network and a phase spectrum sub-network, sequentially reducing the discrepancy between the source and target domains. To construct a more robust generalized model, we employ a multi-source domain data augmentation strategy in the frequency domain. Specifically, a Frequency-Spatial Interaction Module (FSIM) is introduced to handle global information and local spatial features, promoting representation learning between the two sub-networks. To refine the decision boundary of our model output compared to conventional triplet loss, we propose a manifold triplet loss to contribute to generalization. Through extensive experiments on the CWRU and SJTU datasets, FARNet demonstrates effective performance and achieves superior results compared to current cross-domain approaches on the benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00547</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00547</id><created>2025-02-01</created><authors><author><keyname>Wang</keyname><forenames>Zaitian</forenames></author><author><keyname>He</keyname><forenames>Jian</forenames></author><author><keyname>Liang</keyname><forenames>Yu</forenames></author><author><keyname>Hu</keyname><forenames>Xiyuan</forenames></author><author><keyname>Peng</keyname><forenames>Tianhao</forenames></author><author><keyname>Wang</keyname><forenames>Kaixin</forenames></author><author><keyname>Wang</keyname><forenames>Jiakai</forenames></author><author><keyname>Zhang</keyname><forenames>Chenlong</forenames></author><author><keyname>Zhang</keyname><forenames>Weili</forenames></author><author><keyname>Niu</keyname><forenames>Shuang</forenames></author><author><keyname>Xie</keyname><forenames>Xiaoyang</forenames></author></authors><title>Milmer: a Framework for Multiple Instance Learning based Multimodal   Emotion Recognition</title><categories>cs.CV cs.AI cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emotions play a crucial role in human behavior and decision-making, making emotion recognition a key area of interest in human-computer interaction (HCI). This study addresses the challenges of emotion recognition by integrating facial expression analysis with electroencephalogram (EEG) signals, introducing a novel multimodal framework-Milmer. The proposed framework employs a transformer-based fusion approach to effectively integrate visual and physiological modalities. It consists of an EEG preprocessing module, a facial feature extraction and balancing module, and a cross-modal fusion module. To enhance visual feature extraction, we fine-tune a pre-trained Swin Transformer on emotion-related datasets. Additionally, a cross-attention mechanism is introduced to balance token representation across modalities, ensuring effective feature integration. A key innovation of this work is the adoption of a multiple instance learning (MIL) approach, which extracts meaningful information from multiple facial expression images over time, capturing critical temporal dynamics often overlooked in previous studies. Extensive experiments conducted on the DEAP dataset demonstrate the superiority of the proposed framework, achieving a classification accuracy of 96.72% in the four-class emotion recognition task. Ablation studies further validate the contributions of each module, highlighting the significance of advanced feature extraction and fusion strategies in enhancing emotion recognition performance. Our code are available at https://github.com/liangyubuaa/Milmer. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00550</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00550</id><created>2025-02-01</created><authors><author><keyname>Zheng</keyname><forenames>Haoyang</forenames></author><author><keyname>Lin</keyname><forenames>Guang</forenames></author></authors><title>Muti-Fidelity Prediction and Uncertainty Quantification with Laplace   Neural Operators for Parametric Partial Differential Equations</title><categories>cs.LG cs.NA math.NA physics.comp-ph</categories><comments>30 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Laplace Neural Operators (LNOs) have recently emerged as a promising approach in scientific machine learning due to the ability to learn nonlinear maps between functional spaces. However, this framework often requires substantial amounts of high-fidelity (HF) training data, which is often prohibitively expensive to acquire. To address this, we propose multi-fidelity Laplace Neural Operators (MF-LNOs), which combine a low-fidelity (LF) base model with parallel linear/nonlinear HF correctors and dynamic inter-fidelity weighting. This allows us to exploit correlations between LF and HF datasets and achieve accurate inference of quantities of interest even with sparse HF data. We further incorporate a modified replica exchange stochastic gradient Langevin algorithm, which enables a more effective posterior distribution estimation and uncertainty quantification in model predictions. Extensive validation across four canonical dynamical systems (the Lorenz system, Duffing oscillator, Burgers equation, and Brusselator reaction-diffusion system) demonstrates the framework's effectiveness. The results show significant improvements, with testing losses reduced by 40% to 80% compared to traditional approaches. This validates MF-LNO as a versatile tool for surrogate modeling in parametric PDEs, offering significant improvements in data efficiency and uncertainty-aware prediction. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00552</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00552</id><created>2025-02-01</created><authors><author><keyname>Li</keyname><forenames>Sirui</forenames></author><author><keyname>Bragone</keyname><forenames>Federica</forenames></author><author><keyname>Barreau</keyname><forenames>Matthieu</forenames></author><author><keyname>Laneryd</keyname><forenames>Tor</forenames></author><author><keyname>Morozovska</keyname><forenames>Kateryna</forenames></author></authors><title>Optimal Sensor Placement in Power Transformers Using Physics-Informed   Neural Networks</title><categories>cs.LG cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our work aims at simulating and predicting the temperature conditions inside a power transformer using Physics-Informed Neural Networks (PINNs). The predictions obtained are then used to determine the optimal placement for temperature sensors inside the transformer under the constraint of a limited number of sensors, enabling efficient performance monitoring. The method consists of combining PINNs with Mixed Integer Optimization Programming to obtain the optimal temperature reconstruction inside the transformer. First, we extend our PINN model for the thermal modeling of power transformers to solve the heat diffusion equation from 1D to 2D space. Finally, we construct an optimal sensor placement model inside the transformer that can be applied to problems in 1D and 2D. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00557</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00557</id><created>2025-02-01</created><authors><author><keyname>Bach</keyname><forenames>Francis</forenames></author><author><keyname>Saremi</keyname><forenames>Saeed</forenames></author></authors><title>Sampling Binary Data by Denoising through Score Functions</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Gaussian smoothing combined with a probabilistic framework for denoising via the empirical Bayes formalism, i.e., the Tweedie-Miyasawa formula (TMF), are the two key ingredients in the success of score-based generative models in Euclidean spaces. Smoothing holds the key for easing the problem of learning and sampling in high dimensions, denoising is needed for recovering the original signal, and TMF ties these together via the score function of noisy data. In this work, we extend this paradigm to the problem of learning and sampling the distribution of binary data on the Boolean hypercube by adopting Bernoulli noise, instead of Gaussian noise, as a smoothing device. We first derive a TMF-like expression for the optimal denoiser for the Hamming loss, where a score function naturally appears. Sampling noisy binary data is then achieved using a Langevin-like sampler which we theoretically analyze for different noise levels. At high Bernoulli noise levels sampling becomes easy, akin to log-concave sampling in Euclidean spaces. In addition, we extend the sequential multi-measurement sampling of Saremi et al. (2024) to the binary setting where we can bring the "effective noise" down by sampling multiple noisy measurements at a fixed noise level, without the need for continuous-time stochastic processes. We validate our formalism and theoretical findings by experiments on synthetic data and binarized images. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00558</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00558</id><created>2025-02-01</created><authors><author><keyname>Dolan</keyname><forenames>Sydney</forenames></author><author><keyname>Nayak</keyname><forenames>Siddharth</forenames></author><author><keyname>Aloor</keyname><forenames>Jasmine Jerry</forenames></author><author><keyname>Balakrishnan</keyname><forenames>Hamsa</forenames></author></authors><title>Asynchronous Cooperative Multi-Agent Reinforcement Learning with Limited   Communication</title><categories>cs.MA</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the problem setting in which multiple autonomous agents must cooperatively navigate and perform tasks in an unknown, communication-constrained environment. Traditional multi-agent reinforcement learning (MARL) approaches assume synchronous communications and perform poorly in such environments. We propose AsynCoMARL, an asynchronous MARL approach that uses graph transformers to learn communication protocols from dynamic graphs. AsynCoMARL can accommodate infrequent and asynchronous communications between agents, with edges of the graph only forming when agents communicate with each other. We show that AsynCoMARL achieves similar success and collision rates as leading baselines, despite 26\% fewer messages being passed between agents. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00559</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00559</id><created>2025-02-01</created><authors><author><keyname>Gradowski</keyname><forenames>Tomasz</forenames></author><author><keyname>Buchner</keyname><forenames>Teodor</forenames></author></authors><title>Deep learning model for ECG reconstruction reveals the information   content of ECG leads</title><categories>eess.SP cs.LG</categories><comments>18 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study introduces a deep learning model based on the U-net architecture to reconstruct missing leads in electrocardiograms (ECGs). Using publicly available datasets, the model was trained to regenerate 12-lead ECG data from reduced lead configurations, demonstrating high accuracy in lead reconstruction. The results highlight the ability of the model to quantify the information content of each ECG lead and their inter-lead correlations. This has significant implications for optimizing lead selection in diagnostic scenarios, particularly in settings where full 12-lead ECGs are impractical. Additionally, the study provides insights into the physiological underpinnings of ECG signals and their propagation. The findings pave the way for advancements in telemedicine, portable ECG devices, and personalized cardiac diagnostics by reducing redundancy and enhancing signal interpretation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00560</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00560</id><created>2025-02-01</created><authors><author><keyname>Ghimire</keyname><forenames>Mukesh</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Xu</keyname><forenames>Zhe</forenames></author><author><keyname>Ren</keyname><forenames>Yi</forenames></author></authors><title>A Scalable Solver for 2p0s Differential Games with One-Sided Payoff   Information and Continuous Actions, States, and Time</title><categories>cs.GT</categories><comments>32 pages, 12 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Existing solvers for imperfect-information extensive-form games (IIEFGs) often struggle with scalability in terms of action and state space sizes and the number of time steps. However, many real-world games involve continuous action and state spaces and occur in continuous time, making them differential in nature. This paper addresses the scalability challenges for a representative class of two-player zero-sum (2p0s) differential games where the informed player knows the game type (payoff) while the uninformed one only has a prior belief over the set of possible types. Such games encompass a wide range of attack-defense scenarios, where the defender adapts based on their belief about the attacker's target. We make the following contributions: (1) We show that under the Isaacs' condition, the complexity of computing the Nash equilibrium for these games is not related to the action space size; and (2) we propose a multigrid approach to effectively reduce the cost of these games when many time steps are involved. Code for this work is available at \href{https://github.com/ghimiremukesh/cams/tree/conf_sub}{github}. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00561</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00561</id><created>2025-02-01</created><authors><author><keyname>Wallach</keyname><forenames>Hanna</forenames></author><author><keyname>Desai</keyname><forenames>Meera</forenames></author><author><keyname>Cooper</keyname><forenames>A. Feder</forenames></author><author><keyname>Wang</keyname><forenames>Angelina</forenames></author><author><keyname>Atalla</keyname><forenames>Chad</forenames></author><author><keyname>Barocas</keyname><forenames>Solon</forenames></author><author><keyname>Blodgett</keyname><forenames>Su Lin</forenames></author><author><keyname>Chouldechova</keyname><forenames>Alexandra</forenames></author><author><keyname>Corvi</keyname><forenames>Emily</forenames></author><author><keyname>Dow</keyname><forenames>P. Alex</forenames></author><author><keyname>Garcia-Gathright</keyname><forenames>Jean</forenames></author><author><keyname>Olteanu</keyname><forenames>Alexandra</forenames></author><author><keyname>Pangakis</keyname><forenames>Nicholas</forenames></author><author><keyname>Reed</keyname><forenames>Stefanie</forenames></author><author><keyname>Sheng</keyname><forenames>Emily</forenames></author><author><keyname>Vann</keyname><forenames>Dan</forenames></author><author><keyname>Vaughan</keyname><forenames>Jennifer Wortman</forenames></author><author><keyname>Vogel</keyname><forenames>Matthew</forenames></author><author><keyname>Washington</keyname><forenames>Hannah</forenames></author><author><keyname>Jacobs</keyname><forenames>Abigail Z.</forenames></author></authors><title>Position: Evaluating Generative AI Systems is a Social Science   Measurement Challenge</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The measurement tasks involved in evaluating generative AI (GenAI) systems are especially difficult, leading to what has been described as "a tangle of sloppy tests [and] apples-to-oranges comparisons" (Roose, 2024). In this position paper, we argue that the ML community would benefit from learning from and drawing on the social sciences when developing and using measurement instruments for evaluating GenAI systems. Specifically, our position is that evaluating GenAI systems is a social science measurement challenge. We present a four-level framework, grounded in measurement theory from the social sciences, for measuring concepts related to the capabilities, behaviors, and impacts of GenAI. This framework has two important implications for designing and evaluating evaluations: First, it can broaden the expertise involved in evaluating GenAI systems by enabling stakeholders with different perspectives to participate in conceptual debates. Second, it brings rigor to both conceptual and operational debates by offering a set of lenses for interrogating the validity of measurement instruments and their resulting measurements. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00562</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00562</id><created>2025-02-01</created><authors><author><keyname>Hope</keyname><forenames>Benjamin</forenames></author><author><keyname>Bracey</keyname><forenames>Jayden</forenames></author><author><keyname>Choukir</keyname><forenames>Sahar</forenames></author><author><keyname>Warner</keyname><forenames>Derek</forenames></author></authors><title>Assessment of ChatGPT for Engineering Statics Analysis</title><categories>cs.CE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models (LLMs) such as OpenAI's ChatGPT hold potential for automating engineering analysis, yet their reliability in solving multi-step statics problems remains uncertain. This study evaluates the performance of ChatGPT-4o and ChatGPT-o1-preview on foundational statics tasks, from simple calculations of Newton's second law of motion to beam and truss analyses and compares their results to first-year engineering students on a typical statics exam. To enhance accuracy, we developed a Custom GPT, embedding refined prompts directly into its instructions. This optimized model achieved an 82% score, surpassing the 75% student average, demonstrating the impact of tailored guidance. Despite these improvements, LLMs continued to exhibit errors in nuanced or open-ended problems, such as misidentifying tension and compression in truss members. These findings highlight both the promise and current limitations of AI in structural analysis, emphasizing the need for improved reasoning, multimodal capabilities, and targeted training data for future AI-driven automation in civil and mechanical engineering. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00563</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00563</id><created>2025-02-01</created><authors><author><keyname>Lu</keyname><forenames>Renhao</forenames></author></authors><title>Complex Wavelet Mutual Information Loss: A Multi-Scale Loss Function for   Semantic Segmentation</title><categories>cs.CV eess.IV</categories><comments>11 pages, 6 figures</comments><msc-class>68T07</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent advancements in deep neural networks have significantly enhanced the performance of semantic segmentation. However, class imbalance and instance imbalance remain persistent challenges, where smaller instances and thin boundaries are often overshadowed by larger structures. To address the multiscale nature of segmented objects, various models have incorporated mechanisms such as spatial attention and feature pyramid networks. Despite these advancements, most loss functions are still primarily pixel-wise, while regional and boundary-focused loss functions often incur high computational costs or are restricted to small-scale regions. To address this limitation, we propose complex wavelet mutual information (CWMI) loss, a novel loss function that leverages mutual information from subband images decomposed by a complex steerable pyramid. The complex steerable pyramid captures features across multiple orientations and preserves structural similarity across scales. Meanwhile, mutual information is well-suited for capturing high-dimensional directional features and exhibits greater noise robustness. Extensive experiments on diverse segmentation datasets demonstrate that CWMI loss achieves significant improvements in both pixel-wise accuracy and topological metrics compared to state-of-the-art methods, while introducing minimal computational overhead. The code is available at https://anonymous.4open.science/r/CWMI-83B7/ </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00565</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00565</id><created>2025-02-01</created><authors><author><keyname>Lam</keyname><forenames>Bhan</forenames></author><author><keyname>Fan</keyname><forenames>Peijin Esther Monica</forenames></author><author><keyname>Tay</keyname><forenames>Yih Yann</forenames></author><author><keyname>Poon</keyname><forenames>Woei Bing</forenames></author><author><keyname>Ong</keyname><forenames>Zhen-Ting</forenames></author><author><keyname>Ooi</keyname><forenames>Kenneth</forenames></author><author><keyname>Gan</keyname><forenames>Woon-Seng</forenames></author><author><keyname>Ang</keyname><forenames>Shin Yuh</forenames></author></authors><title>Do neonates hear what we measure? Assessing neonatal ward soundscapes at   the neonates ears</title><categories>eess.AS cs.SD</categories><comments>Accepted manuscript submitted to Building and Environment</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Acoustic guidelines for neonatal intensive care units (NICUs) aim to protect vulnerable neonates from noise-induced physiological harm. However, the lack of recognised international standards for measuring neonatal soundscapes has led to inconsistencies in instrumentation and microphone placement in existing literature, raising concerns about the relevance and effectiveness of these guidelines. This study addresses these gaps through long-term acoustic measurements in an operational NICU and a high-dependency ward. We investigate the influence of microphone positioning, bed placement, and ward layout on the assessment of NICU soundscapes. Beyond traditional A-weighted decibel metrics, this study evaluates C-weighted metrics for low-frequency noise, the occurrence of tonal sounds (e.g., alarms), and transient loud events known to disrupt neonates' sleep. Using linear mixed-effects models with aligned ranks transformation ANOVA (LME-ART-ANOVA), our results reveal significant differences in measured noise levels based on microphone placement, highlighting the importance of capturing sound as perceived directly at the neonate's ears. Additionally, bed position and ward layout significantly impact noise exposure, with a NICU bed position consistently exhibiting the highest sound levels across all (psycho)acoustic metrics. These findings support the adoption of binaural measurements along with the integration of additional (psycho)acoustic metrics, such as tonality and transient event occurrence rates, to reliably characterise the neonatal auditory experience. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00567</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00567</id><created>2025-02-01</created><authors><author><keyname>Johri</keyname><forenames>Aditya</forenames></author><author><keyname>Schleiss</keyname><forenames>Johannes</forenames></author><author><keyname>Ranade</keyname><forenames>Nupoor</forenames></author></authors><title>Lessons for GenAI Literacy From a Field Study of Human-GenAI   Augmentation in the Workplace</title><categories>cs.CY cs.AI</categories><comments>Pre-print, paper accepted at IEEE EDUCON2025</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Generative artificial intelligence (GenAI) is increasingly becoming a part of work practices across the technology industry and being used across a range of industries. This has necessitated the need to better understand how GenAI is being used by professionals in the field so that we can better prepare students for the workforce. An improved understanding of the use of GenAI in practice can help provide guidance on the design of GenAI literacy efforts including how to integrate it within courses and curriculum, what aspects of GenAI to teach, and even how to teach it. This paper presents a field study that compares the use of GenAI across three different functions - product development, software engineering, and digital content creation - to identify how GenAI is currently being used in the industry. This study takes a human augmentation approach with a focus on human cognition and addresses three research questions: how is GenAI augmenting work practices; what knowledge is important and how are workers learning; and what are the implications for training the future workforce. Findings show a wide variance in the use of GenAI and in the level of computing knowledge of users. In some industries GenAI is being used in a highly technical manner with deployment of fine-tuned models across domains. Whereas in others, only off-the-shelf applications are being used for generating content. This means that the need for what to know about GenAI varies, and so does the background knowledge needed to utilize it. For the purposes of teaching and learning, our findings indicated that different levels of GenAI understanding needs to be integrated into courses. From a faculty perspective, the work has implications for training faculty so that they are aware of the advances and how students are possibly, as early adopters, already using GenAI to augment their learning practices. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00568</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00568</id><created>2025-02-01</created><authors><author><keyname>Dey</keyname><forenames>Samiran</forenames></author><author><keyname>Banerji</keyname><forenames>Christopher R. S.</forenames></author><author><keyname>Basuchowdhuri</keyname><forenames>Partha</forenames></author><author><keyname>Saha</keyname><forenames>Sanjoy K.</forenames></author><author><keyname>Parashar</keyname><forenames>Deepak</forenames></author><author><keyname>Chakraborti</keyname><forenames>Tapabrata</forenames></author></authors><title>Generating crossmodal gene expression from cancer histopathology   improves multimodal AI predictions</title><categories>cs.CV cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Emerging research has highlighted that artificial intelligence based multimodal fusion of digital pathology and transcriptomic features can improve cancer diagnosis (grading/subtyping) and prognosis (survival risk) prediction. However, such direct fusion for joint decision is impractical in real clinical settings, where histopathology is still the gold standard for diagnosis and transcriptomic tests are rarely requested, at least in the public healthcare system. With our novel diffusion based crossmodal generative AI model PathoGen, we show that genomic expressions synthesized from digital histopathology jointly predicts cancer grading and patient survival risk with high accuracy (state-of-the-art performance), certainty (through conformal coverage guarantee) and interpretability (through distributed attention maps). PathoGen code is available for open use by the research community through GitHub at https://github.com/Samiran-Dey/PathoGen. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00569</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00569</id><created>2025-02-01</created><authors><author><keyname>Devan</keyname><forenames>Umama</forenames></author><author><keyname>Hingle</keyname><forenames>Ashish</forenames></author><author><keyname>McDonald</keyname><forenames>Nora</forenames></author><author><keyname>Johri</keyname><forenames>Aditya</forenames></author></authors><title>Engineering Educators' Perspectives on the Impact of Generative AI in   Higher Education</title><categories>cs.CY</categories><comments>Pre-print, paper accepted to IEEE EDUCON 2025</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The introduction of generative artificial intelligence (GenAI) has been met with a mix of reactions by higher education institutions, ranging from consternation and resistance to wholehearted acceptance. Previous work has looked at the discourse and policies adopted by universities across the U.S. as well as educators, along with the inclusion of GenAI-related content and topics in higher education. Building on previous research, this study reports findings from a survey of engineering educators on their use of and perspectives toward generative AI. Specifically, we surveyed 98 educators from engineering, computer science, and education who participated in a workshop on GenAI in Engineering Education to learn about their perspectives on using these tools for teaching and research. We asked them about their use of and comfort with GenAI, their overall perspectives on GenAI, the challenges and potential harms of using it for teaching, learning, and research, and examined whether their approach to using and integrating GenAI in their classroom influenced their experiences with GenAI and perceptions of it. Consistent with other research in GenAI education, we found that while the majority of participants were somewhat familiar with GenAI, reported use varied considerably. We found that educators harbored mostly hopeful and positive views about the potential of GenAI. We also found that those who engaged more with their students on the topic of GenAI, tend to be more positive about its contribution to learning, while also being more attuned to its potential abuses. These findings suggest that integrating and engaging with generative AI is essential to foster productive interactions between instructors and students around this technology. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00571</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00571</id><created>2025-02-01</created><authors><author><keyname>Aghagolzadeh</keyname><forenames>Hossein</forenames></author><author><keyname>Ezoji</keyname><forenames>Mehdi</forenames></author></authors><title>Contrastive Forward-Forward: A Training Algorithm of Vision Transformer</title><categories>cs.CV cs.LG</categories><comments>22 pages, 8 figures, under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although backpropagation is widely accepted as a training algorithm for artificial neural networks, researchers are always looking for inspiration from the brain to find ways with potentially better performance. Forward-Forward is a new training algorithm that is more similar to what occurs in the brain, although there is a significant performance gap compared to backpropagation. In the Forward-Forward algorithm, the loss functions are placed after each layer, and the updating of a layer is done using two local forward passes and one local backward pass. Forward-Forward is in its early stages and has been designed and evaluated on simple multi-layer perceptron networks to solve image classification tasks. In this work, we have extended the use of this algorithm to a more complex and modern network, namely the Vision Transformer. Inspired by insights from contrastive learning, we have attempted to revise this algorithm, leading to the introduction of Contrastive Forward-Forward. Experimental results show that our proposed algorithm performs significantly better than the baseline Forward-Forward leading to an increase of up to 10% in accuracy and boosting the convergence speed by 5 to 20 times on Vision Transformer. Furthermore, if we take Cross Entropy as the baseline loss function in backpropagation, it will be demonstrated that the proposed modifications to the baseline Forward-Forward reduce its performance gap compared to backpropagation on Vision Transformer, and even outperforms it in certain conditions, such as inaccurate supervision. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00575</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00575</id><created>2025-02-01</created><authors><author><keyname>Ghanizadegan</keyname><forenames>Khashayar</forenames></author><author><keyname>Hashim</keyname><forenames>Hashim A.</forenames></author></authors><title>DeepUKF-VIN: Adaptively-tuned Deep Unscented Kalman Filter for 3D   Visual-Inertial Navigation based on IMU-Vision-Net</title><categories>cs.RO cs.SY eess.SY</categories><doi>10.1016/j.eswa.2025.126656</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the challenge of estimating the orientation, position, and velocity of a vehicle operating in three-dimensional (3D) space with six degrees of freedom (6-DoF). A Deep Learning-based Adaptation Mechanism (DLAM) is proposed to adaptively tune the noise covariance matrices of Kalman-type filters for the Visual-Inertial Navigation (VIN) problem, leveraging IMU-Vision-Net. Subsequently, an adaptively tuned Deep Learning Unscented Kalman Filter for 3D VIN (DeepUKF-VIN) is introduced to utilize the proposed DLAM, thereby robustly estimating key navigation components, including orientation, position, and linear velocity. The proposed DeepUKF-VIN integrates data from onboard sensors, specifically an inertial measurement unit (IMU) and visual feature points extracted from a camera, and is applicable for GPS-denied navigation. Its quaternion-based design effectively captures navigation nonlinearities and avoids the singularities commonly encountered with Euler-angle-based filters. Implemented in discrete space, the DeepUKF-VIN facilitates practical filter deployment. The filter's performance is evaluated using real-world data collected from an IMU and a stereo camera at low sampling rates. The results demonstrate filter stability and rapid attenuation of estimation errors, highlighting its high estimation accuracy. Furthermore, comparative testing against the standard Unscented Kalman Filter (UKF) in two scenarios consistently shows superior performance across all navigation components, thereby validating the efficacy and robustness of the proposed DeepUKF-VIN. Keywords: Deep Learning, Unscented Kalman Filter, Adaptive tuning, Estimation, Navigation, Unmanned Aerial Vehicle, Sensor-fusion. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00577</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00577</id><created>2025-02-01</created><authors><author><keyname>Oh</keyname><forenames>Changdae</forenames></author><author><keyname>Fang</keyname><forenames>Zhen</forenames></author><author><keyname>Im</keyname><forenames>Shawn</forenames></author><author><keyname>Du</keyname><forenames>Xuefeng</forenames></author><author><keyname>Li</keyname><forenames>Yixuan</forenames></author></authors><title>Understanding Multimodal LLMs Under Distribution Shifts: An   Information-Theoretic Approach</title><categories>cs.AI cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multimodal large language models (MLLMs) have shown promising capabilities but struggle under distribution shifts, where evaluation data differ from instruction tuning distributions. Although previous works have provided empirical evaluations, we argue that establishing a formal framework that can characterize and quantify the risk of MLLMs is necessary to ensure the safe and reliable application of MLLMs in the real world. By taking an information-theoretic perspective, we propose the first theoretical framework that enables the quantification of the maximum risk of MLLMs under distribution shifts. Central to our framework is the introduction of Effective Mutual Information (EMI), a principled metric that quantifies the relevance between input queries and model responses. We derive an upper bound for the EMI difference between in-distribution (ID) and out-of-distribution (OOD) data, connecting it to visual and textual distributional discrepancies. Extensive experiments on real benchmark datasets, spanning 61 shift scenarios empirically validate our theoretical insights. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00580</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00580</id><created>2025-02-01</created><authors><author><keyname>Armstrong</keyname><forenames>Stuart</forenames></author><author><keyname>Franklin</keyname><forenames>Matija</forenames></author><author><keyname>Stevens</keyname><forenames>Connor</forenames></author><author><keyname>Gorman</keyname><forenames>Rebecca</forenames></author></authors><title>Defense Against the Dark Prompts: Mitigating Best-of-N Jailbreaking with   Prompt Evaluation</title><categories>cs.CR cs.AI cs.CL cs.CY</categories><acm-class>I.2.0</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent work showed Best-of-N (BoN) jailbreaking using repeated use of random augmentations (such as capitalization, punctuation, etc) is effective against all major large language models (LLMs). We have found that $100\%$ of the BoN paper's successful jailbreaks (confidence interval $[99.65\%, 100.00\%]$) and $99.8\%$ of successful jailbreaks in our replication (confidence interval $[99.28\%, 99.98\%]$) were blocked with our Defense Against The Dark Prompts (DATDP) method. The DATDP algorithm works by repeatedly utilizing an evaluation LLM to evaluate a prompt for dangerous or manipulative behaviors--unlike some other approaches, DATDP also explicitly looks for jailbreaking attempts--until a robust safety rating is generated. This success persisted even when utilizing smaller LLMs to power the evaluation (Claude and LLaMa-3-8B-instruct proved almost equally capable). These results show that, though language models are sensitive to seemingly innocuous changes to inputs, they seem also capable of successfully evaluating the dangers of these inputs. Versions of DATDP can therefore be added cheaply to generative AI systems to produce an immediate significant increase in safety. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00581</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00581</id><created>2025-02-01</created><authors><author><keyname>Morando</keyname><forenames>Luca</forenames></author><author><keyname>Salunkhe</keyname><forenames>Sanket A.</forenames></author><author><keyname>Bobbili</keyname><forenames>Nishanth</forenames></author><author><keyname>Mao</keyname><forenames>Jeffrey</forenames></author><author><keyname>Masci</keyname><forenames>Luca</forenames></author><author><keyname>Nguyen</keyname><forenames>Hung</forenames></author><author><keyname>de Souza</keyname><forenames>Cristino</forenames></author><author><keyname>Loianno</keyname><forenames>Giuseppe</forenames></author></authors><title>Trajectory Planning and Control for Differentially Flat Fixed-Wing   Aerial Systems</title><categories>cs.RO</categories><comments>Approved at Icra 25</comments><journal-ref>Admitted for Publication at 2025 IEEE International Conference on   Robotics and Autonomous Systems (ICRA 2025)</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Efficient real-time trajectory planning and control for fixed-wing unmanned aerial vehicles is challenging due to their non-holonomic nature, complex dynamics, and the additional uncertainties introduced by unknown aerodynamic effects. In this paper, we present a fast and efficient real-time trajectory planning and control approach for fixed-wing unmanned aerial vehicles, leveraging the differential flatness property of fixed-wing aircraft in coordinated flight conditions to generate dynamically feasible trajectories. The approach provides the ability to continuously replan trajectories, which we show is useful to dynamically account for the curvature constraint as the aircraft advances along its path. Extensive simulations and real-world experiments validate our approach, showcasing its effectiveness in generating trajectories even in challenging conditions for small FW such as wind disturbances. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00582</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00582</id><created>2025-02-01</created><authors><author><keyname>Bayraktar</keyname><forenames>Erhan</forenames></author><author><keyname>Ekren</keyname><forenames>Ibrahim</forenames></author><author><keyname>Zhou</keyname><forenames>Hongyi</forenames></author></authors><title>Uniform-in-time weak propagation of chaos for consensus-based   optimization</title><categories>math.OC cs.LG math.PR</categories><comments>keywords: Consensus-based optimization, Uniform-in-time propagation   of chaos, Weak convergence, Sobolev spaces, Linearized Fokker-Planck   equations</comments><msc-class>35Q89, 37N40, 93D50, 82C31, 90C26</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study the uniform-in-time weak propagation of chaos for the consensus-based optimization (CBO) method on a bounded searching domain. We apply the methodology for studying long-time behaviors of interacting particle systems developed in the work of Delarue and Tse (ArXiv:2104.14973). Our work shows that the weak error has order $O(N^{-1})$ uniformly in time, where $N$ denotes the number of particles. The main strategy behind the proofs are the decomposition of the weak errors using the linearized Fokker-Planck equations and the exponential decay of their Sobolev norms. Consequently, our result leads to the joint convergence of the empirical distribution of the CBO particle system to the Dirac-delta distribution at the global minimizer in population size and running time in Wasserstein-type metrics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00583</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00583</id><created>2025-02-01</created><authors><author><keyname>Choi</keyname><forenames>Anna Seo Gyeong</forenames></author><author><keyname>Park</keyname><forenames>Jonghyeon</forenames></author><author><keyname>Oh</keyname><forenames>Myungwoo</forenames></author></authors><title>Data-Driven Mispronunciation Pattern Discovery for Robust Speech   Recognition</title><categories>cs.CL cs.SD eess.AS</categories><comments>Accepted to ICASSP 2025</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Recent advancements in machine learning have significantly improved speech recognition, but recognizing speech from non-fluent or accented speakers remains a challenge. Previous efforts, relying on rule-based pronunciation patterns, have struggled to fully capture non-native errors. We propose two data-driven approaches using speech corpora to automatically detect mispronunciation patterns. By aligning non-native phones with their native counterparts using attention maps, we achieved a 5.7% improvement in speech recognition on native English datasets and a 12.8% improvement for non-native English speakers, particularly Korean speakers. Our method offers practical advancements for robust Automatic Speech Recognition (ASR) systems particularly for situations where prior linguistic knowledge is not applicable. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00585</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00585</id><created>2025-02-01</created><authors><author><keyname>Zhang</keyname><forenames>Jie</forenames></author><author><keyname>Wang</keyname><forenames>Kuan-Chieh</forenames></author><author><keyname>Chiu</keyname><forenames>Bo-Wei</forenames></author><author><keyname>Sun</keyname><forenames>Min-Te</forenames></author></authors><title>Converting Transformers into DGNNs Form</title><categories>cs.LG cs.CL</categories><comments>21 pages, 3 figures, and 8 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent advances in deep learning have established Transformer architectures as the predominant modeling paradigm. Central to the success of Transformers is the self-attention mechanism, which scores the similarity between query and key matrices to modulate a value matrix. This operation bears striking similarities to digraph convolution, prompting an investigation into whether digraph convolution could serve as an alternative to self-attention. In this study, we formalize this concept by introducing a synthetic unitary digraph convolution based on the digraph Fourier transform. The resulting model, which we term Converter, effectively converts a Transformer into a Directed Graph Neural Network (DGNN) form. We have tested Converter on Long-Range Arena benchmark, long document classification, and DNA sequence-based taxonomy classification. Our experimental results demonstrate that Converter achieves superior performance while maintaining computational efficiency and architectural simplicity, which establishes it as a lightweight yet powerful Transformer variant. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00586</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00586</id><created>2025-02-01</created><authors><author><keyname>Wickramasinghe</keyname><forenames>Nimesha</forenames></author><author><keyname>Shaghaghi</keyname><forenames>Arash</forenames></author><author><keyname>Ferrari</keyname><forenames>Elena</forenames></author><author><keyname>Jha</keyname><forenames>Sanjay</forenames></author></authors><title>Less is More: Simplifying Network Traffic Classification Leveraging RFCs</title><categories>cs.CR cs.NI</categories><comments>Accepted to The Web Conference (WWW) 2025 Short Paper Track</comments><doi>10.1145/3701716.3715492</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The rapid growth of encryption has significantly enhanced privacy and security while posing challenges for network traffic classification. Recent approaches address these challenges by transforming network traffic into text or image formats to leverage deep-learning models originally designed for natural language processing, and computer vision. However, these transformations often contradict network protocol specifications, introduce noisy features, and result in resource-intensive processes. To overcome these limitations, we propose NetMatrix, a minimalistic tabular representation of network traffic that eliminates noisy attributes and focuses on meaningful features leveraging RFCs (Request for Comments) definitions. By combining NetMatrix with a vanilla XGBoost classifier, we implement a lightweight approach, LiM ("Less is More") that achieves classification performance on par with state-of-the-art methods such as ET-BERT and YaTC. Compared to selected baselines, experimental evaluations demonstrate that LiM improves resource consumption by orders of magnitude. Overall, this study underscores the effectiveness of simplicity in traffic representation and machine learning model selection, paving the way towards resource-efficient network traffic classification. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00587</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00587</id><created>2025-02-01</created><authors><author><keyname>Alharbi</keyname><forenames>Ebtisaam</forenames></author><author><keyname>Marcolino</keyname><forenames>Leandro Soriano</forenames></author><author><keyname>Ni</keyname><forenames>Qiang</forenames></author><author><keyname>Gouglidis</keyname><forenames>Antonios</forenames></author></authors><title>Robust Knowledge Distillation in Federated Learning: Counteracting   Backdoor Attacks</title><categories>cs.CR cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Federated Learning (FL) enables collaborative model training across multiple devices while preserving data privacy. However, it remains susceptible to backdoor attacks, where malicious participants can compromise the global model. Existing defence methods are limited by strict assumptions on data heterogeneity (Non-Independent and Identically Distributed data) and the proportion of malicious clients, reducing their practicality and effectiveness. To overcome these limitations, we propose Robust Knowledge Distillation (RKD), a novel defence mechanism that enhances model integrity without relying on restrictive assumptions. RKD integrates clustering and model selection techniques to identify and filter out malicious updates, forming a reliable ensemble of models. It then employs knowledge distillation to transfer the collective insights from this ensemble to a global model. Extensive evaluations demonstrate that RKD effectively mitigates backdoor threats while maintaining high model performance, outperforming current state-of-the-art defence methods across various scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00592</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00592</id><created>2025-02-01</created><authors><author><keyname>Wang</keyname><forenames>Yu</forenames></author><author><keyname>Krotov</keyname><forenames>Dmitry</forenames></author><author><keyname>Hu</keyname><forenames>Yuanzhe</forenames></author><author><keyname>Gao</keyname><forenames>Yifan</forenames></author><author><keyname>Zhou</keyname><forenames>Wangchunshu</forenames></author><author><keyname>McAuley</keyname><forenames>Julian</forenames></author><author><keyname>Gutfreund</keyname><forenames>Dan</forenames></author><author><keyname>Feris</keyname><forenames>Rogerio</forenames></author><author><keyname>He</keyname><forenames>Zexue</forenames></author></authors><title>M+: Extending MemoryLLM with Scalable Long-Term Memory</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Equipping large language models (LLMs) with latent-space memory has attracted increasing attention as they can extend the context window of existing language models. However, retaining information from the distant past remains a challenge. For example, MemoryLLM (Wang et al., 2024a), as a representative work with latent-space memory, compresses past information into hidden states across all layers, forming a memory pool of 1B parameters. While effective for sequence lengths up to 16k tokens, it struggles to retain knowledge beyond 20k tokens. In this work, we address this limitation by introducing M+, a memory-augmented model based on MemoryLLM that significantly enhances long-term information retention. M+ integrates a long-term memory mechanism with a co-trained retriever, dynamically retrieving relevant information during text generation. We evaluate M+ on diverse benchmarks, including long-context understanding and knowledge retention tasks. Experimental results show that M+ significantly outperforms MemoryLLM and recent strong baselines, extending knowledge retention from under 20k to over 160k tokens with similar GPU memory overhead. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00593</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00593</id><created>2025-02-01</created><authors><author><keyname>Bahlous-Boldi</keyname><forenames>Ryan</forenames></author><author><keyname>Faldor</keyname><forenames>Maxence</forenames></author><author><keyname>Grillotti</keyname><forenames>Luca</forenames></author><author><keyname>Janmohamed</keyname><forenames>Hannah</forenames></author><author><keyname>Coiffard</keyname><forenames>Lisa</forenames></author><author><keyname>Spector</keyname><forenames>Lee</forenames></author><author><keyname>Cully</keyname><forenames>Antoine</forenames></author></authors><title>Dominated Novelty Search: Rethinking Local Competition in   Quality-Diversity</title><categories>cs.NE cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Quality-Diversity is a family of evolutionary algorithms that generate diverse, high-performing solutions through local competition principles inspired by natural evolution. While research has focused on improving specific aspects of Quality-Diversity algorithms, surprisingly little attention has been paid to investigating alternative formulations of local competition itself -- the core mechanism distinguishing Quality-Diversity from traditional evolutionary algorithms. Most approaches implement local competition through explicit collection mechanisms like fixed grids or unstructured archives, imposing artificial constraints that require predefined bounds or hard-to-tune parameters. We show that Quality-Diversity methods can be reformulated as Genetic Algorithms where local competition occurs through fitness transformations rather than explicit collection mechanisms. Building on this insight, we introduce Dominated Novelty Search, a Quality-Diversity algorithm that implements local competition through dynamic fitness transformations, eliminating the need for predefined bounds or parameters. Our experiments show that Dominated Novelty Search significantly outperforms existing approaches across standard Quality-Diversity benchmarks, while maintaining its advantage in challenging scenarios like high-dimensional and unsupervised spaces. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00594</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00594</id><created>2025-02-01</created><authors><author><keyname>Kapse</keyname><forenames>Saarthak</forenames></author><author><keyname>Betz</keyname><forenames>Robin</forenames></author><author><keyname>Sivanandan</keyname><forenames>Srinivasan</forenames></author></authors><title>Fast Vision Mamba: Pooling Spatial Dimensions for Accelerated Processing</title><categories>cs.CV cs.AI</categories><comments>20 pages, 15 figures, https://github.com/insitro/FastVim</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  State Space Models (SSMs) with selective scan (Mamba) have been adapted into efficient vision models. Mamba, unlike Vision Transformers, achieves linear complexity for token interactions through a recurrent hidden state process. This sequential processing is enhanced by a parallel scan algorithm, which reduces the computational time of recurrent steps from $L$ sequential steps to $log(L)$ parallel steps with respect to the number of input tokens ($L$). In this work, we propose Fast Vision Mamba (FastVim), that further reduces the computational time of the SSM block by reducing the number of recurrent steps in Vision Mamba models while still retaining model performance. By alternately pooling tokens along image dimensions across Mamba blocks, we obtain a 2$\times$ reduction in the number of parallel steps in SSM block. Our model offers up to $72.5\%$ speedup in inference speed compared to baseline Vision Mamba models on high resolution (2048$\times$2048) images. Our experiments demonstrate state-of-the-art performance with dramatically improved throughput in a range of tasks such as image classification, cell perturbation prediction, segmentation, and object detection. Code is made available at https://github.com/insitro/FastVim </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00595</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00595</id><created>2025-02-01</created><authors><author><keyname>Yu</keyname><forenames>Pengfei</forenames></author><author><keyname>Shen</keyname><forenames>Dongming</forenames></author><author><keyname>Meng</keyname><forenames>Silin</forenames></author><author><keyname>Lee</keyname><forenames>Jaewon</forenames></author><author><keyname>Yin</keyname><forenames>Weisu</forenames></author><author><keyname>Cui</keyname><forenames>Andrea Yaoyun</forenames></author><author><keyname>Xu</keyname><forenames>Zhenlin</forenames></author><author><keyname>Zhu</keyname><forenames>Yi</forenames></author><author><keyname>Shi</keyname><forenames>Xingjian</forenames></author><author><keyname>Li</keyname><forenames>Mu</forenames></author><author><keyname>Smola</keyname><forenames>Alex</forenames></author></authors><title>RPGBENCH: Evaluating Large Language Models as Role-Playing Game Engines</title><categories>cs.CL cs.AI</categories><comments>Submitted to ICML 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present RPGBench, the first benchmark designed to evaluate large language models (LLMs) as text-based role-playing game (RPG) engines. RPGBench comprises two core tasks: Game Creation (GC) and Game Simulation (GS). In GC, an LLM must craft a valid and playable RPG world using a structured event-state representation, ensuring logical coherence and proper termination conditions. In GS, the LLM simulates interactive gameplay across multiple rounds while consistently updating states and enforcing game rules. To comprehensively assess performance, RPGBench integrates objective and subjective evaluation methodologies. Objective measures verify adherence to event mechanics and check variable updates without requiring human intervention. Subjective measures, such as content interestingness, action quality, and role-playing capability, are evaluated via an LLM-as-a-judge framework, where a strong LLM grades each candidate's outputs. Empirical results demonstrate that state-of-the-art LLMs can produce engaging stories but often struggle to implement consistent, verifiable game mechanics, particularly in long or complex scenarios. By combining structured, rule-based assessments with LLM-based judgments, RPGBench provides a new standard for evaluating how well LLMs can balance creativity, coherence, and complexity in text-based RPGs, opening avenues for more immersive and controllable interactive storytelling. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00596</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00596</id><created>2025-02-01</created><authors><author><keyname>Alexeev</keyname><forenames>Boris</forenames></author><author><keyname>Mixon</keyname><forenames>Dustin G.</forenames></author></authors><title>Rewinding the byte trail of the White Whale</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Motivated by a popular code golf challenge, we review some key ideas from information theory and discuss how to efficiently compress a streaming file with an acceptable error rate. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00597</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00597</id><created>2025-02-01</created><authors><author><keyname>Rocher-Gonzalez</keyname><forenames>Jose</forenames></author><author><keyname>Escudero-Sahuquillo</keyname><forenames>Jesus</forenames></author><author><keyname>Garcia</keyname><forenames>Pedro J.</forenames></author><author><keyname>Quiles</keyname><forenames>Francisco J.</forenames></author><author><keyname>Mora</keyname><forenames>Gaspar</forenames></author></authors><title>Towards an Efficient Combination of Adaptive Routing and Queuing Schemes   in Fat-Tree Topologies</title><categories>cs.NI</categories><comments>53 pages</comments><journal-ref>Journal of Parallel and Distributed Computing 2021</journal-ref><doi>10.1016/j.jpdc.2020.07.009</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The interconnection network is a key element in High-Performance Computing (HPC) and Datacenter (DC) systems whose performance depends on several design parameters, such as the topology, the switch architecture, and the routing algorithm. Among the most common topologies in HPC systems, the Fat-Tree offers several shortest-path routes between any pair of end-nodes, which allows multi-path routing schemes to balance traffic flows among the available links, thus reducing congestion probability. However, traffic balance cannot solve by itself some congestion situations that may still degrade network performance. Another approach to reduce congestion is queue-based flow separation, but our previous work shows that multi-path routing may spread congested flows across several queues, thus being counterproductive. In this paper, we propose a set of restrictions to improve alternative routes selection for multi-path routing algorithms in Fat-Tree networks, so that they can be positively combined with queuing schemes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00601</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00601</id><created>2025-02-01</created><authors><author><keyname>Abolfazli</keyname><forenames>Amir</forenames></author><author><keyname>Song</keyname><forenames>Zekun</forenames></author><author><keyname>Anand</keyname><forenames>Avishek</forenames></author><author><keyname>Nejdl</keyname><forenames>Wolfgang</forenames></author></authors><title>Enhancing Offline Reinforcement Learning with Curriculum Learning-Based   Trajectory Valuation</title><categories>cs.LG</categories><comments>Accepted at AAMAS 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The success of deep reinforcement learning (DRL) relies on the availability and quality of training data, often requiring extensive interactions with specific environments. In many real-world scenarios, where data collection is costly and risky, offline reinforcement learning (RL) offers a solution by utilizing data collected by domain experts and searching for a batch-constrained optimal policy. This approach is further augmented by incorporating external data sources, expanding the range and diversity of data collection possibilities. However, existing offline RL methods often struggle with challenges posed by non-matching data from these external sources. In this work, we specifically address the problem of source-target domain mismatch in scenarios involving mixed datasets, characterized by a predominance of source data generated from random or suboptimal policies and a limited amount of target data generated from higher-quality policies. To tackle this problem, we introduce Transition Scoring (TS), a novel method that assigns scores to transitions based on their similarity to the target domain, and propose Curriculum Learning-Based Trajectory Valuation (CLTV), which effectively leverages these transition scores to identify and prioritize high-quality trajectories through a curriculum learning approach. Our extensive experiments across various offline RL methods and MuJoCo environments, complemented by rigorous theoretical analysis, demonstrate that CLTV enhances the overall performance and transferability of policies learned by offline RL algorithms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00602</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00602</id><created>2025-02-01</created><authors><author><keyname>Liu</keyname><forenames>Tianci</forenames></author><author><keyname>Dong</keyname><forenames>Zihan</forenames></author><author><keyname>Zhang</keyname><forenames>Linjun</forenames></author><author><keyname>Wang</keyname><forenames>Haoyu</forenames></author><author><keyname>Gao</keyname><forenames>Jing</forenames></author></authors><title>Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large language models (LLMs) have achieved remarkable performance on various natural language tasks. However, they are trained on static corpora and their knowledge can become outdated quickly in the fast-changing world. This motivates the development of knowledge editing (KE) to update specific knowledge in LLMs without changing unrelated others or compromising their pre-trained capabilities. Previous efforts sought to update a small amount of parameters of a LLM and proved effective for making selective updates. Nonetheless, the edited LLM often exhibits degraded ability to reason about the new knowledge. In this work, we identify a key issue: heterogeneous token overfitting (HTO), where the LLM overfits different tokens in the provided knowledge at varying rates. To tackle this, we propose OVERTONE, a token-level smoothing method that mitigates HTO by adaptively refining the target distribution. Theoretically, OVERTONE offers better parameter updates with negligible computation overhead. It also induces an implicit DPO but does not require preference data pairs. Extensive experiments across four editing methods, two LLMs, and diverse scenarios demonstrate the effectiveness and versatility of our method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00603</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00603</id><created>2025-02-01</created><authors><author><keyname>Zhu</keyname><forenames>Jincao</forenames></author><author><keyname>Van Der Merwe</keyname><forenames>Kobus</forenames></author><author><keyname>Foukas</keyname><forenames>Xenofon</forenames></author><author><keyname>Radunovic</keyname><forenames>Bozidar</forenames></author></authors><title>Hades: Hierarchical Adaptable Decoding for Efficient and Elastic vRAN</title><categories>cs.NI</categories><comments>16 pages</comments><msc-class>ACM C.2.1</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In cellular networks, virtualized Radio Access Networks (vRANs) enable replacing traditional specialized hardware at cell sites with software running on commodity servers distributed across edge and remote clouds. However, some vRAN functions (e.g., forward error correction (FEC) decoding) require excessive edge compute resources due to their intensive computational demands and inefficiencies caused by workload fluctuations. This high demand for computational power significantly drives up the costs associated with edge computing, posing a major challenge for deploying 5G/6G vRAN solutions. To address this challenge, we propose Hades, a hierarchical architecture for vRAN that enables the distribution of uplink FEC decoding processing across edge and remote clouds. Hades refactors the vRAN stack and introduces mechanisms that allow controlling and managing the workload over these hierarchical cloud resources. More specifically, Hades splits the traditional non-stop run-to-completion iterative FEC decoding process into latency-critical early decoding iterations, i.e., related to MAC processing and early pre-parsing for content identification, and completion decoding iterations, i.e., decoding tasks with larger decoding delay budgets for final data bits extraction. This partitioning provides Hades the flexibility to utilize the available midhaul (MH) network for offloading the latency tolerant part of decoding to remote cloud instances, while performing time-sensitive decoding at the edge cloud locations for low-delay processing. Hades controls decoding load distribution between the edge and remote clouds, based on the edge decoding capacity and the offload network bandwidth, thus improving the utilization of edge compute. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00604</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00604</id><created>2025-02-01</created><authors><author><keyname>Wang</keyname><forenames>Sifan</forenames></author><author><keyname>Bhartari</keyname><forenames>Ananyae Kumar</forenames></author><author><keyname>Li</keyname><forenames>Bowen</forenames></author><author><keyname>Perdikaris</keyname><forenames>Paris</forenames></author></authors><title>Gradient Alignment in Physics-informed Neural Networks: A Second-Order   Optimization Perspective</title><categories>cs.LG cs.AI physics.comp-ph</categories><comments>39 pages, 22 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Multi-task learning through composite loss functions is fundamental to modern deep learning, yet optimizing competing objectives remains challenging. We present new theoretical and practical approaches for addressing directional conflicts between loss terms, demonstrating their effectiveness in physics-informed neural networks (PINNs) where such conflicts are particularly challenging to resolve. Through theoretical analysis, we demonstrate how these conflicts limit first-order methods and show that second-order optimization naturally resolves them through implicit gradient alignment. We prove that SOAP, a recently proposed quasi-Newton method, efficiently approximates the Hessian preconditioner, enabling breakthrough performance in PINNs: state-of-the-art results on 10 challenging PDE benchmarks, including the first successful application to turbulent flows with Reynolds numbers up to 10,000, with 2-10x accuracy improvements over existing methods. We also introduce a novel gradient alignment score that generalizes cosine similarity to multiple gradients, providing a practical tool for analyzing optimization dynamics. Our findings establish frameworks for understanding and resolving gradient conflicts, with broad implications for optimization beyond scientific computing. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00605</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00605</id><created>2025-02-01</created><authors><author><keyname>Shariatnasab</keyname><forenames>Mahshad</forenames></author><author><keyname>Rini</keyname><forenames>Stefano</forenames></author><author><keyname>Shirani</keyname><forenames>Farhad</forenames></author><author><keyname>Iyengar</keyname><forenames>S. Sitharama</forenames></author></authors><title>The Query/Hit Model for Sequential Hypothesis Testing</title><categories>cs.IT cs.LG eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work introduces the Query/Hit (Q/H) learning model. The setup consists of two agents. One agent, Alice, has access to a streaming source, while the other, Bob, does not have direct access to the source. Communication occurs through sequential Q/H pairs: Bob sends a sequence of source symbols (queries), and Alice responds with the waiting time until each query appears in the source stream (hits). This model is motivated by scenarios with communication, computation, and privacy constraints that limit real-time access to the source. The error exponent for sequential hypothesis testing under the Q/H model is characterized, and a querying strategy, the Dynamic Scout-Sentinel Algorithm (DSSA), is proposed. The strategy employs a mutual information neural estimator to compute the error exponent associated with each query and to select the query with the highest efficiency. Extensive empirical evaluations on both synthetic and real-world datasets -- including mouse movement trajectories, typesetting patterns, and touch-based user interactions -- are provided to evaluate the performance of the proposed strategy in comparison with baselines, in terms of probability of error, query choice, and time-to-detection. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00607</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00607</id><created>2025-02-01</created><authors><author><keyname>Dughmi</keyname><forenames>Shaddin</forenames></author></authors><title>PAC Learning is just Bipartite Matching (Sort of)</title><categories>cs.LG cs.DS stat.ML</categories><comments>Position paper</comments><acm-class>F.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main goal of this article is to convince you, the reader, that supervised learning in the Probably Approximately Correct (PAC) model is closely related to -- of all things -- bipartite matching! En-route from PAC learning to bipartite matching, I will overview a particular transductive model of learning, and associated one-inclusion graphs, which can be viewed as a generalization of some of the hat puzzles that are popular in recreational mathematics. Whereas this transductive model is far from new, it has recently seen a resurgence of interest as a tool for tackling deep questions in learning theory. A secondary purpose of this article could be as a (biased) tutorial on the connections between the PAC and transductive models of learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00609</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00609</id><created>2025-02-01</created><authors><author><keyname>Carstensen</keyname><forenames>Carsten</forenames></author><author><keyname>Heuer</keyname><forenames>Norbert</forenames></author></authors><title>Normal-normal continuous symmetric stress approximation in   three-dimensional linear elasticity</title><categories>math.NA cs.NA</categories><msc-class>65N30, 74G15, 74S05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a conforming setting for a mixed formulation of linear elasticity with symmetric stress that has normal-normal continuous components across faces of tetrahedral meshes. We provide a stress element for this formulation with 30 degrees of freedom that correspond to standard boundary conditions. The resulting scheme converges quasi-optimally and is locking free. Numerical experiments illustrate the performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00611</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00611</id><created>2025-02-01</created><authors><author><keyname>Keshri</keyname><forenames>Rajat</forenames></author><author><keyname>Zachariah</keyname><forenames>Arun George</forenames></author><author><keyname>Boone</keyname><forenames>Michael</forenames></author></authors><title>Enhancing Code Consistency in AI Research with Large Language Models and   Retrieval-Augmented Generation</title><categories>cs.SE cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Ensuring that code accurately reflects the algorithms and methods described in research papers is critical for maintaining credibility and fostering trust in AI research. This paper presents a novel system designed to verify code implementations against the algorithms and methodologies outlined in corresponding research papers. Our system employs Retrieval-Augmented Generation to extract relevant details from both the research papers and code bases, followed by a structured comparison using Large Language Models. This approach improves the accuracy and comprehensiveness of code implementation verification while contributing to the transparency, explainability, and reproducibility of AI research. By automating the verification process, our system reduces manual effort, enhances research credibility, and ultimately advances the state of the art in code verification. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00612</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00612</id><created>2025-02-01</created><authors><author><keyname>Tian</keyname><forenames>Chang</forenames></author><author><keyname>Xing</keyname><forenames>Mingzhe</forenames></author><author><keyname>Shi</keyname><forenames>Zenglin</forenames></author><author><keyname>Blaschko</keyname><forenames>Matthew B.</forenames></author><author><keyname>Yue</keyname><forenames>Yinliang</forenames></author><author><keyname>Moens</keyname><forenames>Marie-Francine</forenames></author></authors><title>Using Causality for Enhanced Prediction of Web Traffic Time Series</title><categories>cs.LG cs.NI</categories><comments>time series, web service, web traffic, causality</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Predicting web service traffic has significant social value, as it can be applied to various practical scenarios, including but not limited to dynamic resource scaling, load balancing, system anomaly detection, service-level agreement compliance, and fraud detection. Web service traffic is characterized by frequent and drastic fluctuations over time and are influenced by heterogeneous web user behaviors, making accurate prediction a challenging task. Previous research has extensively explored statistical approaches, and neural networks to mine features from preceding service traffic time series for prediction. However, these methods have largely overlooked the causal relationships between services. Drawing inspiration from causality in ecological systems, we empirically recognize the causal relationships between web services. To leverage these relationships for improved web service traffic prediction, we propose an effective neural network module, CCMPlus, designed to extract causal relationship features across services. This module can be seamlessly integrated with existing time series models to consistently enhance the performance of web service traffic predictions. We theoretically justify that the causal correlation matrix generated by the CCMPlus module captures causal relationships among services. Empirical results on real-world datasets from Microsoft Azure, Alibaba Group, and Ant Group confirm that our method surpasses state-of-the-art approaches in Mean Squared Error (MSE) and Mean Absolute Error (MAE) for predicting service traffic time series. These findings highlight the efficacy of leveraging causal relationships for improved predictions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00614</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00614</id><created>2025-02-01</created><authors><author><keyname>Cerrato</keyname><forenames>Antonio</forenames></author><author><keyname>Rodríguez-Tembleque</keyname><forenames>Luis</forenames></author><author><keyname>González</keyname><forenames>José A.</forenames></author><author><keyname>Aliabadi</keyname><forenames>M. H. Ferri</forenames></author></authors><title>A coupled finite and boundary spectral element method for linear   water-wave propagation problems</title><categories>math.NA cs.NA math-ph math.MP</categories><journal-ref>Applied Mathematical Modelling, Volume 48, 2017, Pages 1-20, ISSN   0307-904X</journal-ref><doi>10.1016/j.apm.2017.03.061</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  A coupled boundary spectral element method (BSEM) and spectral element method (SEM) formulation for the propagation of small-amplitude water waves over variable bathymetries is presented in this work. The wave model is based on the mild-slope equation (MSE), which provides a good approximation of the propagation of water waves over irregular bottom surfaces with slopes up to 1:3. In unbounded domains or infinite regions, space can be divided into two different areas: a central region of interest, where an irregular bathymetry is included, and an exterior infinite region with straight and parallel bathymetric lines. The SEM allows us to model the central region, where any variation of the bathymetry can be considered, while the exterior infinite region is modelled by the BSEM which, combined with the fundamental solution presented by Cerrato et al. [A. Cerrato, J. A. Gonz\'alez, L. Rodr\'iguez-Tembleque, Boundary element formulation of the mild-slope equation for harmonic water waves propagating over unidirectional variable bathymetries, Eng. Anal. Boundary Elem. 62 (2016) 22-34.] can include bathymetries with straight and parallel contour lines. This coupled model combines important advantages of both methods; it benefits from the flexibility of the SEM for the interior region and, at the same time, includes the fulfilment of the Sommerfeld's radiation condition for the exterior problem, that is provided by the BSEM. The solution approximation inside the elements is constructed by high order Legendre polynomials associated with Legendre-Gauss-Lobatto quadrature points, providing a spectral convergence for both methods. The proposed formulation has been validated in three different benchmark cases with different shapes of the bottom surface. The solutions exhibit the typical p-convergence of spectral methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00615</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00615</id><created>2025-02-01</created><authors><author><keyname>Hasan</keyname><forenames>Kazi Amit</forenames></author><author><keyname>Yasmin</keyname><forenames>Jerin</forenames></author><author><keyname>Hao</keyname><forenames>Huizi</forenames></author><author><keyname>Tian</keyname><forenames>Yuan</forenames></author><author><keyname>Hassan</keyname><forenames>Safwat</forenames></author><author><keyname>Ding</keyname><forenames>Steven</forenames></author></authors><title>Understanding Abandonment and Slowdown Dynamics in the Maven Ecosystem</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The sustainability of libraries is critical for modern software development, yet many libraries face abandonment, posing significant risks to dependent projects. This study explores the prevalence and patterns of library abandonment in the Maven ecosystem. We investigate abandonment trends over the past decade, revealing that approximately one in four libraries fail to survive beyond their creation year. We also analyze the release activities of libraries, focusing on their lifespan and release speed, and analyze the evolution of these metrics within the lifespan of libraries. We find that while slow release speed and relatively long periods of inactivity are often precursors to abandonment, some abandoned libraries exhibit bursts of high frequent release activity late in their life cycle. Our findings contribute to a new understanding of library abandonment dynamics and offer insights for practitioners to identify and mitigate risks in software ecosystems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00616</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00616</id><created>2025-02-01</created><authors><author><keyname>Rocher-Gonzalez</keyname><forenames>Jose</forenames></author><author><keyname>Escudero-Sahuquillo</keyname><forenames>Jesus</forenames></author><author><keyname>Garcia</keyname><forenames>Pedro J.</forenames></author><author><keyname>Quiles</keyname><forenames>Francisco J.</forenames></author></authors><title>Congestion Management in High-Performance Interconnection Networks Using   Adaptive Routing Notifications</title><categories>cs.NI</categories><comments>34 pages</comments><journal-ref>Journal Of Supercomputing 2023</journal-ref><doi>10.1007/s11227-022-04926</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The interconnection network is a crucial subsystem in High-Performance Computing clusters and Data-centers, guaranteeing high bandwidth and low latency to the applications' communication operations. Unfortunately, congestion situations may spoil network performance unless the network design applies specific countermeasures. Adaptive routing algorithms are a traditional approach to dealing with congestion since they provide traffic flows with alternative routes that bypass congested areas. However, adaptive routing decisions at switches are typically based on local information without a global network traffic perspective, leading to congestion spreading throughout the network beyond the original congested areas. In this paper, we propose a new efficient congestion management strategy that leverages adaptive routing notifications currently available in some interconnect technologies and efficiently isolates the congesting flows in reserved spaces at switch buffers. The experiment results based on simulations of realistic traffic scenarios show that our proposal removes the congestion impact. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00617</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00617</id><created>2025-02-01</created><authors><author><keyname>Lindenmaier</keyname><forenames>Gabriel</forenames></author><author><keyname>Papay</keyname><forenames>Sean</forenames></author><author><keyname>Padó</keyname><forenames>Sebastian</forenames></author></authors><title>Efficient Language Modeling for Low-Resource Settings with Hybrid   RNN-Transformer Architectures</title><categories>cs.CL</categories><comments>PDF has 12 pages total, 7 without references and abstract; 10   individual graphics combined to 3 figures; 5 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Transformer-based language models have recently been at the forefront of active research in text generation. However, these models' advances come at the price of prohibitive training costs, with parameter counts in the billions and compute requirements measured in petaflop/s-decades. In this paper, we investigate transformer-based architectures for improving model performance in a low-data regime by selectively replacing attention layers with feed-forward and quasi-recurrent neural network layers. We test these architectures on the standard Enwik8 and Wikitext-103 corpora. Our results show that our reduced architectures outperform existing models with a comparable number of parameters, and obtain comparable performance to larger models while significantly reducing the number of parameters. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00618</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00618</id><created>2025-02-01</created><authors><author><keyname>He</keyname><forenames>Chiyuan</forenames></author><author><keyname>Qiu</keyname><forenames>Zihuan</forenames></author><author><keyname>Meng</keyname><forenames>Fanman</forenames></author><author><keyname>Xu</keyname><forenames>Linfeng</forenames></author><author><keyname>Wu</keyname><forenames>Qingbo</forenames></author><author><keyname>Li</keyname><forenames>Hongliang</forenames></author></authors><title>DesCLIP: Robust Continual Adaptation via General Attribute Descriptions   for Pretrained Vision-Language Models</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continual adaptation of vision-language models (VLMs) focuses on leveraging cross-modal pretrained knowledge to incrementally adapt for expanding downstream tasks and datasets, while tackling the challenge of knowledge forgetting. Existing research often focuses on connecting visual features with specific class text in downstream tasks, overlooking the latent relationships between general and specialized knowledge. Our findings reveal that forcing models to optimize inappropriate visual-text matches exacerbates forgetting of VLMs. To tackle this issue, we propose DesCLIP, which leverages general attribute (GA) descriptions to guide the understanding of specific class objects, enabling VLMs to establish robust \textit{vision-GA-class} trilateral associations rather than relying solely on \textit{vision-class} connections. Specifically, we introduce a language assistant to generate concrete GA description candidates via proper request prompts. Then, an anchor-based embedding filter is designed to obtain highly relevant GA description embeddings, which are leveraged as the paired text embeddings for visual-textual instance matching, thereby tuning the visual encoder. Correspondingly, the class text embeddings are gradually calibrated to align with these shared GA description embeddings. Extensive experiments demonstrate the advancements and efficacy of our proposed method, with comprehensive empirical evaluations highlighting its superior performance compared to existing pretrained and VLM-based continual learning methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00619</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00619</id><created>2025-02-01</created><authors><author><keyname>Oh</keyname><forenames>Yujin</forenames></author><author><keyname>Jin</keyname><forenames>Pengfei</forenames></author><author><keyname>Park</keyname><forenames>Sangjoon</forenames></author><author><keyname>Kim</keyname><forenames>Sekeun</forenames></author><author><keyname>Yoon</keyname><forenames>Siyeop</forenames></author><author><keyname>Kim</keyname><forenames>Kyungsang</forenames></author><author><keyname>Kim</keyname><forenames>Jin Sung</forenames></author><author><keyname>Li</keyname><forenames>Xiang</forenames></author><author><keyname>Li</keyname><forenames>Quanzheng</forenames></author></authors><title>Distribution-aware Fairness Learning in Medical Image Segmentation From   A Control-Theoretic Perspective</title><categories>eess.IV cs.AI cs.CV</categories><comments>12 pages, 3 figures, 9 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ensuring fairness in medical image segmentation is critical due to biases in imbalanced clinical data acquisition caused by demographic attributes (e.g., age, sex, race) and clinical factors (e.g., disease severity). To address these challenges, we introduce Distribution-aware Mixture of Experts (dMoE), inspired by optimal control theory. We provide a comprehensive analysis of its underlying mechanisms and clarify dMoE's role in adapting to heterogeneous distributions in medical image segmentation. Furthermore, we integrate dMoE into multiple network architectures, demonstrating its broad applicability across diverse medical image analysis tasks. By incorporating demographic and clinical factors, dMoE achieves state-of-the-art performance on two 2D benchmark datasets and a 3D in-house dataset. Our results highlight the effectiveness of dMoE in mitigating biases from imbalanced distributions, offering a promising approach to bridging control theory and medical image segmentation within fairness learning paradigms. The source code will be made available. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00620</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00620</id><created>2025-02-01</created><authors><author><keyname>Xue</keyname><forenames>Yihao</forenames></author><author><keyname>Li</keyname><forenames>Jiping</forenames></author><author><keyname>Mirzasoleiman</keyname><forenames>Baharan</forenames></author></authors><title>Representations Shape Weak-to-Strong Generalization: Theoretical   Insights and Empirical Predictions</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Weak-to-Strong Generalization (W2SG), where a weak model supervises a stronger one, serves as an important analogy for understanding how humans might guide superhuman intelligence in the future. Promising empirical results revealed that a strong model can surpass its weak supervisor. While recent work has offered theoretical insights into this phenomenon, a clear understanding of the interactions between weak and strong models that drive W2SG remains elusive. We investigate W2SG through a theoretical lens and show that it can be characterized using kernels derived from the principal components of weak and strong models' internal representations. These kernels can be used to define a space that, at a high level, captures what the weak model is unable to learn but is learnable by the strong model. The projection of labels onto this space quantifies how much the strong model falls short of its full potential due to weak supervision. This characterization also provides insights into how certain errors in weak supervision can be corrected by the strong model, regardless of overfitting. Our theory has significant practical implications, providing a representation-based metric that predicts W2SG performance trends without requiring labels, as shown in experiments on molecular predictions with transformers and 5 NLP tasks involving 52 LLMs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00622</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00622</id><created>2025-02-01</created><authors><author><keyname>Qi</keyname><forenames>Han</forenames></author><author><keyname>Yin</keyname><forenames>Haocheng</forenames></author><author><keyname>Du</keyname><forenames>Yilun</forenames></author><author><keyname>Yang</keyname><forenames>Heng</forenames></author></authors><title>Strengthening Generative Robot Policies through Predictive World   Modeling</title><categories>cs.RO cs.CV cs.LG</categories><comments>Website: https://computationalrobotics.seas.harvard.edu/GPC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present generative predictive control (GPC), a learning control framework that (i) clones a generative diffusion-based policy from expert demonstrations, (ii) trains a predictive action-conditioned world model from both expert demonstrations and random explorations, and (iii) synthesizes an online planner that ranks and optimizes the action proposals from (i) by looking ahead into the future using the world model from (ii). Crucially, we show that conditional video diffusion allows learning (near) physics-accurate visual world models and enable robust visual foresight. Focusing on planar pushing with rich contact and collision, we show GPC dominates behavior cloning across state-based and vision-based, simulated and real-world experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00626</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00626</id><created>2025-02-01</created><authors><author><keyname>Chang</keyname><forenames>Yue</forenames></author><author><keyname>Liu</keyname><forenames>Mengfei</forenames></author><author><keyname>Wang</keyname><forenames>Zhecheng</forenames></author><author><keyname>Chen</keyname><forenames>Peter Yichen</forenames></author><author><keyname>Grinspun</keyname><forenames>Eitan</forenames></author></authors><title>Lifting the Winding Number: Precise Representation of Complex Cuts in   Subspace Physics Simulations</title><categories>cs.GR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Cutting thin-walled deformable structures is common in daily life, but poses significant challenges for simulation due to the introduced spatial discontinuities. Traditional methods rely on mesh-based domain representations, which require frequent remeshing and refinement to accurately capture evolving discontinuities. These challenges are further compounded in reduced-space simulations, where the basis functions are inherently geometry- and mesh-dependent, making it difficult or even impossible for the basis to represent the diverse family of discontinuities introduced by cuts.   Recent advances in representing basis functions with neural fields offer a promising alternative, leveraging their discretization-agnostic nature to represent deformations across varying geometries. However, the inherent continuity of neural fields is an obstruction to generalization, particularly if discontinuities are encoded in neural network weights.   We present Wind Lifter, a novel neural representation designed to accurately model complex cuts in thin-walled deformable structures. Our approach constructs neural fields that reproduce discontinuities precisely at specified locations, without baking in the position of the cut line. Crucially, our approach does not embed the discontinuity in the neural network's weights, opening avenues to generalization of cut placement.   Our method achieves real-time simulation speeds and supports dynamic updates to cut line geometry during the simulation. Moreover, the explicit representation of discontinuities makes our neural field intuitive to control and edit, offering a significant advantage over traditional neural fields, where discontinuities are embedded within the network's weights, and enabling new applications that rely on general cut placement. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00627</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00627</id><created>2025-02-01</created><authors><author><keyname>Aquino</keyname><forenames>Yan</forenames></author><author><keyname>Bento</keyname><forenames>Pedro</forenames></author><author><keyname>Buzelin</keyname><forenames>Arthur</forenames></author><author><keyname>Dayrell</keyname><forenames>Lucas</forenames></author><author><keyname>Malaquias</keyname><forenames>Samira</forenames></author><author><keyname>Santana</keyname><forenames>Caio</forenames></author><author><keyname>Estanislau</keyname><forenames>Victoria</forenames></author><author><keyname>Dutenhefner</keyname><forenames>Pedro</forenames></author><author><keyname>Evangelista</keyname><forenames>Guilherme H. G.</forenames></author><author><keyname>Porfírio</keyname><forenames>Luisa G.</forenames></author><author><keyname>Grossi</keyname><forenames>Caio Souza</forenames></author><author><keyname>Rigueira</keyname><forenames>Pedro B.</forenames></author><author><keyname>Almeida</keyname><forenames>Virgilio</forenames></author><author><keyname>Pappa</keyname><forenames>Gisele L.</forenames></author><author><keyname>Meira</keyname><forenames>Wagner</forenames><suffix>Jr</suffix></author></authors><title>Discord Unveiled: A Comprehensive Dataset of Public Communication   (2015-2024)</title><categories>cs.SI cs.DB</categories><comments>Submitted to ICWSM 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Discord has evolved from a gaming-focused communication tool into a versatile platform supporting diverse online communities. Despite its large user base and active public servers, academic research on Discord remains limited due to data accessibility challenges. This paper introduces Discord Unveiled: A Comprehensive Dataset of Public Communication (2015-2024), the most extensive Discord public server's data to date. The dataset comprises over 2.05 billion messages from 4.74 million users across 3,167 public servers, representing approximately 10% of servers listed in Discord's Discovery feature. Spanning from Discord's launch in 2015 to the end of 2024, it offers a robust temporal and thematic framework for analyzing decentralized moderation, community governance, information dissemination, and social dynamics. Data was collected through Discord's public API, adhering to ethical guidelines and privacy standards via anonymization techniques. Organized into structured JSON files, the dataset facilitates seamless integration with computational social science methodologies. Preliminary analyses reveal significant trends in user engagement, bot utilization, and linguistic diversity, with English predominating alongside substantial representations of Spanish, French, and Portuguese. Additionally, prevalent community themes such as social, art, music, and memes highlight Discord's expansion beyond its gaming origins. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00629</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00629</id><created>2025-02-01</created><authors><author><keyname>Wu</keyname><forenames>Yuxuan</forenames></author><author><keyname>Nakayama</keyname><forenames>Hideki</forenames></author></authors><title>Advanced Weakly-Supervised Formula Exploration for Neuro-Symbolic   Mathematical Reasoning</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, neuro-symbolic methods have become a popular and powerful approach that augments artificial intelligence systems with the capability to perform abstract, logical, and quantitative deductions with enhanced precision and controllability. Recent studies successfully performed symbolic reasoning by leveraging various machine learning models to explicitly or implicitly predict intermediate labels that provide symbolic instructions. However, these intermediate labels are not always prepared for every task as a part of training data, and pre-trained models, represented by Large Language Models (LLMs), also do not consistently generate valid symbolic instructions with their intrinsic knowledge. On the other hand, existing work developed alternative learning techniques that allow the learning system to autonomously uncover optimal symbolic instructions. Nevertheless, their performance also exhibits limitations when faced with relatively huge search spaces or more challenging reasoning problems. In view of this, in this work, we put forward an advanced practice for neuro-symbolic reasoning systems to explore the intermediate labels with weak supervision from problem inputs and final outputs. Our experiments on the Mathematics dataset illustrated the effectiveness of our proposals from multiple aspects. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00630</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00630</id><created>2025-02-01</created><authors><author><keyname>Xie</keyname><forenames>Bin</forenames></author><author><keyname>Tang</keyname><forenames>Hao</forenames></author><author><keyname>Cai</keyname><forenames>Dawen</forenames></author><author><keyname>Yan</keyname><forenames>Yan</forenames></author><author><keyname>Agam</keyname><forenames>Gady</forenames></author></authors><title>Self-Prompt SAM: Medical Image Segmentation via Automatic Prompt SAM   Adaptation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Segment Anything Model (SAM) has demonstrated impressive zero-shot performance and brought a range of unexplored capabilities to natural image segmentation tasks. However, as a very important branch of image segmentation, the performance of SAM remains uncertain when applied to medical image segmentation due to the significant differences between natural images and medical images. Meanwhile, it is harsh to meet the SAM's requirements of extra prompts provided, such as points or boxes to specify medical regions. In this paper, we propose a novel self-prompt SAM adaptation framework for medical image segmentation, named Self-Prompt-SAM. We design a multi-scale prompt generator combined with the image encoder in SAM to generate auxiliary masks. Then, we use the auxiliary masks to generate bounding boxes as box prompts and use Distance Transform to select the most central points as point prompts. Meanwhile, we design a 3D depth-fused adapter (DfusedAdapter) and inject the DFusedAdapter into each transformer in the image encoder and mask decoder to enable pre-trained 2D SAM models to extract 3D information and adapt to 3D medical images. Extensive experiments demonstrate that our method achieves state-of-the-art performance and outperforms nnUNet by 2.3% on AMOS2022, 1.6% on ACDCand 0.5% on Synapse datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00631</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00631</id><created>2025-02-01</created><authors><author><keyname>Qi</keyname><forenames>Xuyin</forenames></author><author><keyname>Zhang</keyname><forenames>Zeyu</forenames></author><author><keyname>Zheng</keyname><forenames>Huazhan</forenames></author><author><keyname>Chen</keyname><forenames>Mingxi</forenames></author><author><keyname>Kutaiba</keyname><forenames>Numan</forenames></author><author><keyname>Lim</keyname><forenames>Ruth</forenames></author><author><keyname>Chiang</keyname><forenames>Cherie</forenames></author><author><keyname>Tham</keyname><forenames>Zi En</forenames></author><author><keyname>Ren</keyname><forenames>Xuan</forenames></author><author><keyname>Zhang</keyname><forenames>Wenxin</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>Hao</forenames></author><author><keyname>Lv</keyname><forenames>Wenbing</forenames></author><author><keyname>Yao</keyname><forenames>Guangzhen</forenames></author><author><keyname>Han</keyname><forenames>Renda</forenames></author><author><keyname>Wang</keyname><forenames>Kangsheng</forenames></author><author><keyname>Li</keyname><forenames>Mingyuan</forenames></author><author><keyname>Mao</keyname><forenames>Hongtao</forenames></author><author><keyname>Li</keyname><forenames>Yu</forenames></author><author><keyname>Liao</keyname><forenames>Zhibin</forenames></author><author><keyname>Zhao</keyname><forenames>Yang</forenames></author><author><keyname>To</keyname><forenames>Minh-Son</forenames></author></authors><title>MedConv: Convolutions Beat Transformers on Long-Tailed Bone Density   Prediction</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Bone density prediction via CT scans to estimate T-scores is crucial, providing a more precise assessment of bone health compared to traditional methods like X-ray bone density tests, which lack spatial resolution and the ability to detect localized changes. However, CT-based prediction faces two major challenges: the high computational complexity of transformer-based architectures, which limits their deployment in portable and clinical settings, and the imbalanced, long-tailed distribution of real-world hospital data that skews predictions. To address these issues, we introduce MedConv, a convolutional model for bone density prediction that outperforms transformer models with lower computational demands. We also adapt Bal-CE loss and post-hoc logit adjustment to improve class balance. Extensive experiments on our AustinSpine dataset shows that our approach achieves up to 21% improvement in accuracy and 20% in ROC AUC over previous state-of-the-art methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00632</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00632</id><created>2025-02-01</created><authors><author><keyname>Xu</keyname><forenames>Ziyang</forenames></author></authors><title>Patterns and Purposes: A Cross-Journal Analysis of AI Tool Usage in   Academic Writing</title><categories>cs.CY</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study investigates the use of AI tools in academic writing through analysis of AI usage declarations in journals. Using a mixed-methods approach combining content analysis, statistical analysis, and text mining, this research analyzed 168 AI declarations from 8,859 articles across 27 categories. Results show that ChatGPT dominates academic writing assistance (77% usage), with significant differences in tool usage between native and non-native English speakers (p = 0.0483) and between international and non-international teams (p = 0.0012). The study reveals that improving readability (51%) and grammar checking (22%) are the primary purposes of AI tool usage. These findings provide insights for journal policy development and understanding the evolving role of AI in academic writing. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00633</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00633</id><created>2025-02-01</created><authors><author><keyname>Zhang</keyname><forenames>Zuyuan</forenames></author><author><keyname>Lan</keyname><forenames>Tian</forenames></author></authors><title>Lipschitz Lifelong Monte Carlo Tree Search for Mastering Non-Stationary   Tasks</title><categories>cs.AI</categories><comments>6 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Monte Carlo Tree Search (MCTS) has proven highly effective in solving complex planning tasks by balancing exploration and exploitation using Upper Confidence Bound for Trees (UCT). However, existing work have not considered MCTS-based lifelong planning, where an agent faces a non-stationary series of tasks -- e.g., with varying transition probabilities and rewards -- that are drawn sequentially throughout the operational lifetime. This paper presents LiZero for Lipschitz lifelong planning using MCTS. We propose a novel concept of adaptive UCT (aUCT) to transfer knowledge from a source task to the exploration/exploitation of a new task, depending on both the Lipschitz continuity between tasks and the confidence of knowledge in in Monte Carlo action sampling. We analyze LiZero's acceleration factor in terms of improved sampling efficiency and also develop efficient algorithms to compute aUCT in an online fashion by both data-driven and model-based approaches, whose sampling complexity and error bounds are also characterized. Experiment results show that LiZero significantly outperforms existing MCTS and lifelong learning baselines in terms of much faster convergence (3$\sim$4x) to optimal rewards. Our results highlight the potential of LiZero to advance decision-making and planning in dynamic real-world environments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00634</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00634</id><created>2025-02-01</created><authors><author><keyname>Yu</keyname><forenames>Donglei</forenames></author><author><keyname>Zhao</keyname><forenames>Yang</forenames></author><author><keyname>Zhu</keyname><forenames>Jie</forenames></author><author><keyname>Xu</keyname><forenames>Yangyifan</forenames></author><author><keyname>Zhou</keyname><forenames>Yu</forenames></author><author><keyname>Zong</keyname><forenames>Chengqing</forenames></author></authors><title>SimulPL: Aligning Human Preferences in Simultaneous Machine Translation</title><categories>cs.CL cs.AI</categories><comments>Accepted to ICLR 2025. 23 pages,13 figures,11 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneous Machine Translation (SiMT) generates translations while receiving streaming source inputs. This requires the SiMT model to learn a read/write policy, deciding when to translate and when to wait for more source input. Numerous linguistic studies indicate that audiences in SiMT scenarios have distinct preferences, such as accurate translations, simpler syntax, and no unnecessary latency. Aligning SiMT models with these human preferences is crucial to improve their performances. However, this issue still remains unexplored. Additionally, preference optimization for SiMT task is also challenging. Existing methods focus solely on optimizing the generated responses, ignoring human preferences related to latency and the optimization of read/write policy during the preference optimization phase. To address these challenges, we propose Simultaneous Preference Learning (SimulPL), a preference learning framework tailored for the SiMT task. In the SimulPL framework, we categorize SiMT human preferences into five aspects: \textbf{translation quality preference}, \textbf{monotonicity preference}, \textbf{key point preference}, \textbf{simplicity preference}, and \textbf{latency preference}. By leveraging the first four preferences, we construct human preference prompts to efficiently guide GPT-4/4o in generating preference data for the SiMT task. In the preference optimization phase, SimulPL integrates \textbf{latency preference} into the optimization objective and enables SiMT models to improve the read/write policy, thereby aligning with human preferences more effectively. Experimental results indicate that SimulPL exhibits better alignment with human preferences across all latency levels in Zh$\rightarrow$En, De$\rightarrow$En and En$\rightarrow$Zh SiMT tasks. Our data and code will be available at \url{https://github.com/EurekaForNLP/SimulPL}. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00637</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00637</id><created>2025-02-01</created><authors><author><keyname>Wei</keyname><forenames>Mengyi</forenames></author><author><keyname>Jiao</keyname><forenames>Chenjing</forenames></author><author><keyname>Zuo</keyname><forenames>Chenyu</forenames></author><author><keyname>Hurni</keyname><forenames>Lorenz</forenames></author><author><keyname>Meng</keyname><forenames>Liqiu</forenames></author></authors><title>Constructing AI ethics narratives based on real-world data: Human-AI   collaboration in data-driven visual storytelling</title><categories>cs.HC</categories><comments>13 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  AI ethics narratives have the potential to shape the public accurate understanding of AI technologies and promote communication among different stakeholders. However, AI ethics narratives are largely lacking. Existing limited narratives tend to center on works of science fiction or corporate marketing campaigns of large technology companies. Misuse of "socio-technical imaginary" can blur the line between speculation and reality for the public, undermining the responsibility and regulation of technology development. Therefore, constructing authentic AI ethics narratives is an urgent task. The emergence of generative AI offers new possibilities for building narrative systems. This study is dedicated to data-driven visual storytelling about AI ethics relying on the human-AI collaboration. Based on the five key elements of story models, we proposed a conceptual framework for human-AI collaboration, explored the roles of generative AI and humans in the creation of visual stories. We implemented the conceptual framework in a real AI news case. This research leveraged advanced generative AI technologies to provide a reference for constructing genuine AI ethics narratives. Our goal is to promote active public engagement and discussions through authentic AI ethics narratives, thereby contributing to the development of better AI policies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00639</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00639</id><created>2025-02-01</created><authors><author><keyname>Ren</keyname><forenames>Tao</forenames></author><author><keyname>Zhang</keyname><forenames>Zishi</forenames></author><author><keyname>Li</keyname><forenames>Zehao</forenames></author><author><keyname>Jiang</keyname><forenames>Jingyang</forenames></author><author><keyname>Qin</keyname><forenames>Shentao</forenames></author><author><keyname>Li</keyname><forenames>Guanghao</forenames></author><author><keyname>Li</keyname><forenames>Yan</forenames></author><author><keyname>Zheng</keyname><forenames>Yi</forenames></author><author><keyname>Li</keyname><forenames>Xinping</forenames></author><author><keyname>Zhan</keyname><forenames>Min</forenames></author><author><keyname>Peng</keyname><forenames>Yijie</forenames></author></authors><title>Zeroth-order Informed Fine-Tuning for Diffusion Model: A Recursive   Likelihood Ratio Optimizer</title><categories>cs.CV cs.AI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The probabilistic diffusion model (DM), generating content by inferencing through a recursive chain structure, has emerged as a powerful framework for visual generation. After pre-training on enormous unlabeled data, the model needs to be properly aligned to meet requirements for downstream applications. How to efficiently align the foundation DM is a crucial task. Contemporary methods are either based on Reinforcement Learning (RL) or truncated Backpropagation (BP). However, RL and truncated BP suffer from low sample efficiency and biased gradient estimation respectively, resulting in limited improvement or, even worse, complete training failure. To overcome the challenges, we propose the Recursive Likelihood Ratio (RLR) optimizer, a zeroth-order informed fine-tuning paradigm for DM. The zeroth-order gradient estimator enables the computation graph rearrangement within the recursive diffusive chain, making the RLR's gradient estimator an unbiased one with the lower variance than other methods. We provide theoretical guarantees for the performance of the RLR. Extensive experiments are conducted on image and video generation tasks to validate the superiority of the RLR. Furthermore, we propose a novel prompt technique that is natural for the RLR to achieve a synergistic effect. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00640</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00640</id><created>2025-02-01</created><authors><author><keyname>Wu</keyname><forenames>Shirley</forenames></author><author><keyname>Galley</keyname><forenames>Michel</forenames></author><author><keyname>Peng</keyname><forenames>Baolin</forenames></author><author><keyname>Cheng</keyname><forenames>Hao</forenames></author><author><keyname>Li</keyname><forenames>Gavin</forenames></author><author><keyname>Dou</keyname><forenames>Yao</forenames></author><author><keyname>Cai</keyname><forenames>Weixin</forenames></author><author><keyname>Zou</keyname><forenames>James</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author><author><keyname>Gao</keyname><forenames>Jianfeng</forenames></author></authors><title>CollabLLM: From Passive Responders to Active Collaborators</title><categories>cs.AI</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large Language Models are typically trained with next-turn rewards, limiting their ability to optimize for long-term interaction. As a result, they often respond passively to ambiguous or open-ended user requests, failing to help users reach their ultimate intents and leading to inefficient conversations. To address these limitations, we introduce CollabLLM, a novel and general training framework that enhances multiturn human-LLM collaboration. Its key innovation is a collaborative simulation that estimates the long-term contribution of responses using Multiturn-aware Rewards. By reinforcement fine-tuning these rewards, CollabLLM goes beyond responding to user requests, and actively uncovers user intent and offers insightful suggestions-a key step towards more human-centered AI. We also devise a multiturn interaction benchmark with three challenging tasks such as document creation. CollabLLM significantly outperforms our baselines with averages of 18.5% higher task performance and 46.3% improved interactivity by LLM judges. Finally, we conduct a large user study with 201 judges, where CollabLLM increases user satisfaction by 17.6% and reduces user spent time by 10.4%. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00641</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00641</id><created>2025-02-01</created><authors><author><keyname>Xu</keyname><forenames>Borui</forenames></author><author><keyname>Chen</keyname><forenames>Yao</forenames></author><author><keyname>Wen</keyname><forenames>Zeyi</forenames></author><author><keyname>Liu</keyname><forenames>Weiguo</forenames></author><author><keyname>He</keyname><forenames>Bingsheng</forenames></author></authors><title>Evaluating Small Language Models for News Summarization: Implications   and Factors Influencing Performance</title><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing demand for efficient summarization tools in resource-constrained environments highlights the need for effective solutions. While large language models (LLMs) deliver superior summarization quality, their high computational resource requirements limit practical use applications. In contrast, small language models (SLMs) present a more accessible alternative, capable of real-time summarization on edge devices. However, their summarization capabilities and comparative performance against LLMs remain underexplored. This paper addresses this gap by presenting a comprehensive evaluation of 19 SLMs for news summarization across 2,000 news samples, focusing on relevance, coherence, factual consistency, and summary length. Our findings reveal significant variations in SLM performance, with top-performing models such as Phi3-Mini and Llama3.2-3B-Ins achieving results comparable to those of 70B LLMs while generating more concise summaries. Notably, SLMs are better suited for simple prompts, as overly complex prompts may lead to a decline in summary quality. Additionally, our analysis indicates that instruction tuning does not consistently enhance the news summarization capabilities of SLMs. This research not only contributes to the understanding of SLMs but also provides practical insights for researchers seeking efficient summarization solutions that balance performance and resource use. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00645</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00645</id><created>2025-02-01</created><authors><author><keyname>Moradi</keyname><forenames>Parsa</forenames></author><author><keyname>Maddah-Ali</keyname><forenames>Mohammad Ali</forenames></author></authors><title>General Coded Computing in a Probabilistic Straggler Regime</title><categories>cs.DC cs.LG</categories><comments>11 pages, 1 figure</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Coded computing has demonstrated promising results in addressing straggler resiliency in distributed computing systems. However, most coded computing schemes are designed for exact computation, requiring the number of responding servers to exceed a certain recovery threshold. Additionally, these schemes are tailored for highly structured functions. Recently, new coded computing schemes for general computing functions, where exact computation is replaced with approximate computation, have emerged. In these schemes, the availability of additional results corresponds to more accurate estimation of computational tasks. This flexibility introduces new questions that need to be addressed. This paper addresses the practically important scenario in the context of general coded computing, where each server may become a straggler with a probability $p$, independently from others. We theoretically analyze the approximation error of two existing general coded computing schemes: Berrut Approximate Coded Computing (BACC) and Learning Theoretic Coded Computing (LeTCC). Under the probabilistic straggler configuration, we demonstrate that the average approximation error for BACC and LeTCC converge to zero with the rate of at least $\mathcal{O}(\log^3_{\frac{1}{p}}(N)\cdot{N^{-3}})$ and $\mathcal{O}(\log^4_{\frac{1}{p}}(N)\cdot{N^{-2}})$, respectively. This is perhaps surprising, as earlier results does not indicate a convergence when the number of stragglers scales with the total number of servers $N$. However, in this case, despite the average number of stragglers being $Np$, the independence of servers in becoming stragglers allows the approximation error to converge to zero. These theoretical results are validated through experiments on various computing functions, including deep neural networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00646</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00646</id><created>2025-02-01</created><authors><author><keyname>Dong</keyname><forenames>Chang</forenames></author><author><keyname>Sun</keyname><forenames>Zechao</forenames></author><author><keyname>Bai</keyname><forenames>Guangdong</forenames></author><author><keyname>Piao</keyname><forenames>Shuying</forenames></author><author><keyname>Chen</keyname><forenames>Weitong</forenames></author><author><keyname>Zhang</keyname><forenames>Wei Emma</forenames></author></authors><title>TrojanTime: Backdoor Attacks on Time Series Classification</title><categories>cs.CR cs.AI cs.LG</categories><comments>13 pages, 3 figures, 3 tables</comments><acm-class>I.2.0</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Time Series Classification (TSC) is highly vulnerable to backdoor attacks, posing significant security threats. Existing methods primarily focus on data poisoning during the training phase, designing sophisticated triggers to improve stealthiness and attack success rate (ASR). However, in practical scenarios, attackers often face restrictions in accessing training data. Moreover, it is a challenge for the model to maintain generalization ability on clean test data while remaining vulnerable to poisoned inputs when data is inaccessible. To address these challenges, we propose TrojanTime, a novel two-step training algorithm. In the first stage, we generate a pseudo-dataset using an external arbitrary dataset through target adversarial attacks. The clean model is then continually trained on this pseudo-dataset and its poisoned version. To ensure generalization ability, the second stage employs a carefully designed training strategy, combining logits alignment and batch norm freezing. We evaluate TrojanTime using five types of triggers across four TSC architectures in UCR benchmark datasets from diverse domains. The results demonstrate the effectiveness of TrojanTime in executing backdoor attacks while maintaining clean accuracy. Finally, to mitigate this threat, we propose a defensive unlearning strategy that effectively reduces the ASR while preserving clean accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00648</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00648</id><created>2025-02-01</created><authors><author><keyname>Swarup</keyname><forenames>Samarth</forenames></author></authors><title>Agency in the Age of AI</title><categories>cs.AI cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is significant concern about the impact of generative AI on society. Modern AI tools are capable of generating ever more realistic text, images, and videos, and functional code, from minimal prompts. Accompanying this rise in ability and usability, there is increasing alarm about the misuses to which these tools can be put, and the intentional and unintentional harms to individuals and society that may result. In this paper, we argue that \emph{agency} is the appropriate lens to study these harms and benefits, but that doing so will require advancement in the theory of agency, and advancement in how this theory is applied in (agent-based) models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00651</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00651</id><created>2025-02-01</created><authors><author><keyname>Lokare</keyname><forenames>Amit</forenames><affiliation>Vanguard</affiliation></author><author><keyname>Bankar</keyname><forenames>Shripad</forenames><affiliation>Comcast</affiliation></author><author><keyname>Mhaske</keyname><forenames>Padmajeet</forenames><affiliation>JPMC</affiliation></author></authors><title>Integrating Cybersecurity Frameworks into IT Security: A Comprehensive   Analysis of Threat Mitigation Strategies and Adaptive Technologies</title><categories>cs.CR</categories><comments>23 Pages</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  The cybersecurity threat landscape is constantly actively making it imperative to develop sound frameworks to protect the IT structures. Based on this introduction, this paper aims to discuss the application of cybersecurity frameworks into the IT security with focus placed on the role of such frameworks in addressing the changing nature of cybersecurity threats. It explores widely used models, including the NIST Cybersecurity Framework, Zero Trust Architecture, and the ISO/IEC 27001, and how they apply to industries including finance, healthcare and government. The discussion also singles out such technologies as Artificial Intelligence (AI) and Machine Learning (ML) as the core for real-time threat detection and response mechanisms. As these integration challenges demonstrate, the study provides tangible and proven approaches to tackle framework implementation issues such as legitimate security issues, limited availability of funds and resources, and compliance with legal requirements. By capturing current trends and exposures, the findings promote strong, portfolio-based and risk-appropriate security approaches adjusted for organizational goals and capable to prevent advanced cyber threats. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00652</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00652</id><created>2025-02-01</created><authors><author><keyname>Jiang</keyname><forenames>Yi</forenames></author><author><keyname>Ma</keyname><forenames>Oubo</forenames></author><author><keyname>Yang</keyname><forenames>Yong</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author><author><keyname>Ji</keyname><forenames>Shouling</forenames></author></authors><title>Reformulation is All You Need: Addressing Malicious Text Features in   DNNs</title><categories>cs.LG cs.CL cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Human language encompasses a wide range of intricate and diverse implicit features, which attackers can exploit to launch adversarial or backdoor attacks, compromising DNN models for NLP tasks. Existing model-oriented defenses often require substantial computational resources as model size increases, whereas sample-oriented defenses typically focus on specific attack vectors or schemes, rendering them vulnerable to adaptive attacks. We observe that the root cause of both adversarial and backdoor attacks lies in the encoding process of DNN models, where subtle textual features, negligible for human comprehension, are erroneously assigned significant weight by less robust or trojaned models. Based on it we propose a unified and adaptive defense framework that is effective against both adversarial and backdoor attacks. Our approach leverages reformulation modules to address potential malicious features in textual inputs while preserving the original semantic integrity. Extensive experiments demonstrate that our framework outperforms existing sample-oriented defense baselines across a diverse range of malicious textual features. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00653</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00653</id><created>2025-02-01</created><authors><author><keyname>Yin</keyname><forenames>Ziyi</forenames></author><author><keyname>Cao</keyname><forenames>Yuanpu</forenames></author><author><keyname>Liu</keyname><forenames>Han</forenames></author><author><keyname>Wang</keyname><forenames>Ting</forenames></author><author><keyname>Chen</keyname><forenames>Jinghui</forenames></author><author><keyname>Ma</keyname><forenames>Fenhlong</forenames></author></authors><title>Towards Robust Multimodal Large Language Models Against Jailbreak   Attacks</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While multimodal large language models (MLLMs) have achieved remarkable success in recent advancements, their susceptibility to jailbreak attacks has come to light. In such attacks, adversaries exploit carefully crafted prompts to coerce models into generating harmful or undesirable content. Existing defense mechanisms often rely on external inference steps or safety alignment training, both of which are less effective and impractical when facing sophisticated adversarial perturbations in white-box scenarios. To address these challenges and bolster MLLM robustness, we introduce SafeMLLM by adopting an adversarial training framework that alternates between an attack step for generating adversarial noise and a model updating step. At the attack step, SafeMLLM generates adversarial perturbations through a newly proposed contrastive embedding attack (CoE-Attack), which optimizes token embeddings under a contrastive objective. SafeMLLM then updates model parameters to neutralize the perturbation effects while preserving model utility on benign inputs. We evaluate SafeMLLM across six MLLMs and six jailbreak methods spanning multiple modalities. Experimental results show that SafeMLLM effectively defends against diverse attacks, maintaining robust performance and utilities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00654</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00654</id><created>2025-02-01</created><authors><author><keyname>Cha</keyname><forenames>Junuk</forenames></author><author><keyname>Yoon</keyname><forenames>Seongro</forenames></author><author><keyname>Strizhkova</keyname><forenames>Valeriya</forenames></author><author><keyname>Bremond</keyname><forenames>Francois</forenames></author><author><keyname>Baek</keyname><forenames>Seungryul</forenames></author></authors><title>EmoTalkingGaussian: Continuous Emotion-conditioned Talking Head   Synthesis</title><categories>cs.CV</categories><comments>22 pages</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  3D Gaussian splatting-based talking head synthesis has recently gained attention for its ability to render high-fidelity images with real-time inference speed. However, since it is typically trained on only a short video that lacks the diversity in facial emotions, the resultant talking heads struggle to represent a wide range of emotions. To address this issue, we propose a lip-aligned emotional face generator and leverage it to train our EmoTalkingGaussian model. It is able to manipulate facial emotions conditioned on continuous emotion values (i.e., valence and arousal); while retaining synchronization of lip movements with input audio. Additionally, to achieve the accurate lip synchronization for in-the-wild audio, we introduce a self-supervised learning method that leverages a text-to-speech network and a visual-audio synchronization network. We experiment our EmoTalkingGaussian on publicly available videos and have obtained better results than state-of-the-arts in terms of image quality (measured in PSNR, SSIM, LPIPS), emotion expression (measured in V-RMSE, A-RMSE, V-SA, A-SA, Emotion Accuracy), and lip synchronization (measured in LMD, Sync-E, Sync-C), respectively. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00655</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00655</id><created>2025-02-01</created><authors><author><keyname>Liu</keyname><forenames>Qianru</forenames></author><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Xu</keyname><forenames>Yuesheng</forenames></author></authors><title>Parameter Choices for Sparse Multi-Parameter Regularization with the   $\ell_1$ Norm</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a multi-parameter regularization approach using the $\ell_1$ norm, designed to better adapt to complex data structures and problem characteristics while offering enhanced flexibility in promoting sparsity in regularized solutions. As data volumes grow, sparse representations of learned functions become critical for reducing computational costs during function operations. We investigate how the selection of multiple regularization parameters influences the sparsity of regularized solutions. Specifically, we characterize the relationship between these parameters and the sparsity of solutions under transform matrices, enabling the development of an iterative scheme for selecting parameters that achieve prescribed sparsity levels. Special attention is given to scenarios where the fidelity term is non-differentiable, and the transform matrix lacks full row rank. In such cases, the regularized solution, along with two auxiliary vectors arising in the sparsity characterization, are essential components of the multi-parameter selection strategy. To address this, we propose a fixed-point proximity algorithm that simultaneously determines these three vectors. This algorithm, combined with our sparsity characterization, forms the basis of a practical multi-parameter selection strategy. Numerical experiments demonstrate the effectiveness of the proposed approach, yielding regularized solutions with both predetermined sparsity levels and satisfactory approximation accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00657</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00657</id><created>2025-02-01</created><authors><author><keyname>Haldar</keyname><forenames>Rajdeep</forenames></author><author><keyname>Wang</keyname><forenames>Ziyi</forenames></author><author><keyname>Song</keyname><forenames>Qifan</forenames></author><author><keyname>Lin</keyname><forenames>Guang</forenames></author><author><keyname>Xing</keyname><forenames>Yue</forenames></author></authors><title>LLM Safety Alignment is Divergence Estimation in Disguise</title><categories>cs.LG cs.AI cs.CY stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a theoretical framework demonstrating that popular Large Language Model (LLM) alignment methods, including Reinforcement Learning from Human Feedback (RLHF) and alternatives, fundamentally function as divergence estimators between aligned (preferred or safe) and unaligned (less-preferred or harmful) distributions. This explains the separation phenomenon between safe and harmful prompts in the model hidden representation after alignment. Inspired by the theoretical results, we identify that some alignment methods are better than others in terms of separation and, introduce a new method, KLDO, and further demonstrate the implication of our theories. We advocate for compliance-refusal datasets over preference datasets to enhance safety alignment, supported by both theoretical reasoning and empirical evidence. Additionally, to quantify safety separation, we leverage a distance metric in the representation space and statistically validate its efficacy as a statistical significant indicator of LLM resilience against jailbreak attacks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00661</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00661</id><created>2025-02-01</created><authors><author><keyname>Kim</keyname><forenames>Changseung</forenames></author><author><keyname>Bae</keyname><forenames>Geunsik</forenames></author><author><keyname>Shin</keyname><forenames>Woojae</forenames></author><author><keyname>Wang</keyname><forenames>Sen</forenames></author><author><keyname>Oh</keyname><forenames>Hyondong</forenames></author></authors><title>EKF-Based Radar-Inertial Odometry with Online Temporal Calibration</title><categories>cs.RO</categories><comments>9 pages, 4 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate time synchronization between heterogeneous sensors is crucial for ensuring robust state estimation in multi-sensor fusion systems. Sensor delays often cause discrepancies between the actual time when the event was captured and the time of sensor measurement, leading to temporal misalignment (time offset) between sensor measurement streams. In this paper, we propose an extended Kalman filter (EKF)-based radar-inertial odometry (RIO) framework that estimates the time offset online. The radar ego-velocity measurement model, estimated from a single radar scan, is formulated to include the time offset for the update. By leveraging temporal calibration, the proposed RIO enables accurate propagation and measurement updates based on a common time stream. Experiments on multiple datasets demonstrated the accurate time offset estimation of the proposed method and its impact on RIO performance, validating the importance of sensor time synchronization. Our implementation of the EKF-RIO with online temporal calibration is available at https://github.com/spearwin/EKF-RIO-TC. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00662</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00662</id><created>2025-02-01</created><authors><author><keyname>Wang</keyname><forenames>Yimu</forenames></author><author><keyname>Riddell</keyname><forenames>Evelien</forenames></author><author><keyname>Chow</keyname><forenames>Adrian</forenames></author><author><keyname>Sedwards</keyname><forenames>Sean</forenames></author><author><keyname>Czarnecki</keyname><forenames>Krzysztof</forenames></author></authors><title>Mitigating the Modality Gap: Few-Shot Out-of-Distribution Detection with   Multi-modal Prototypes and Image Bias Estimation</title><categories>cs.CV cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Existing vision-language model (VLM)-based methods for out-of-distribution (OOD) detection typically rely on similarity scores between input images and in-distribution (ID) text prototypes. However, the modality gap between image and text often results in high false positive rates, as OOD samples can exhibit high similarity to ID text prototypes. To mitigate the impact of this modality gap, we propose incorporating ID image prototypes along with ID text prototypes. We present theoretical analysis and empirical evidence indicating that this approach enhances VLM-based OOD detection performance without any additional training. To further reduce the gap between image and text, we introduce a novel few-shot tuning framework, SUPREME, comprising biased prompts generation (BPG) and image-text consistency (ITC) modules. BPG enhances image-text fusion and improves generalization by conditioning ID text prototypes on the Gaussian-based estimated image domain bias; ITC reduces the modality gap by minimizing intra- and inter-modal distances. Moreover, inspired by our theoretical and empirical findings, we introduce a novel OOD score $S_{\textit{GMP}}$, leveraging uni- and cross-modal similarities. Finally, we present extensive experiments to demonstrate that SUPREME consistently outperforms existing VLM-based OOD detection methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00663</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00663</id><created>2025-02-01</created><authors><author><keyname>Yang</keyname><forenames>Xiaoran</forenames></author><author><keyname>Yu</keyname><forenames>Shuhan</forenames></author><author><keyname>Xu</keyname><forenames>Wenxi</forenames></author></authors><title>Enhanced Convolutional Neural Networks for Improved Image Classification</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image classification is a fundamental task in computer vision with diverse applications, ranging from autonomous systems to medical imaging. The CIFAR-10 dataset is a widely used benchmark to evaluate the performance of classification models on small-scale, multi-class datasets. Convolutional Neural Networks (CNNs) have demonstrated state-of-the-art results; however, they often suffer from overfitting and suboptimal feature representation when applied to challenging datasets like CIFAR-10. In this paper, we propose an enhanced CNN architecture that integrates deeper convolutional blocks, batch normalization, and dropout regularization to achieve superior performance. The proposed model achieves a test accuracy of 84.95%, outperforming baseline CNN architectures. Through detailed ablation studies, we demonstrate the effectiveness of the enhancements and analyze the hierarchical feature representations. This work highlights the potential of refined CNN architectures for tackling small-scale image classification problems effectively. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00665</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00665</id><created>2025-02-01</created><authors><author><keyname>Ling</keyname><forenames>Fuxi</forenames></author><author><keyname>Liu</keyname><forenames>Hongye</forenames></author><author><keyname>Huang</keyname><forenames>Guoqiang</forenames></author><author><keyname>Li</keyname><forenames>Jing</forenames></author><author><keyname>Wu</keyname><forenames>Hong</forenames></author><author><keyname>Tang</keyname><forenames>Zhihao</forenames></author></authors><title>Cross-Modal Synergies: Unveiling the Potential of Motion-Aware Fusion   Networks in Handling Dynamic and Static ReID Scenarios</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Navigating the complexities of person re-identification (ReID) in varied surveillance scenarios, particularly when occlusions occur, poses significant challenges. We introduce an innovative Motion-Aware Fusion (MOTAR-FUSE) network that utilizes motion cues derived from static imagery to significantly enhance ReID capabilities. This network incorporates a dual-input visual adapter capable of processing both images and videos, thereby facilitating more effective feature extraction. A unique aspect of our approach is the integration of a motion consistency task, which empowers the motion-aware transformer to adeptly capture the dynamics of human motion. This technique substantially improves the recognition of features in scenarios where occlusions are prevalent, thereby advancing the ReID process. Our comprehensive evaluations across multiple ReID benchmarks, including holistic, occluded, and video-based scenarios, demonstrate that our MOTAR-FUSE network achieves superior performance compared to existing approaches. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00666</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00666</id><created>2025-02-01</created><authors><author><keyname>Chen</keyname><forenames>Mingyu</forenames></author><author><keyname>Chen</keyname><forenames>Yiding</forenames></author><author><keyname>Sun</keyname><forenames>Wen</forenames></author><author><keyname>Zhang</keyname><forenames>Xuezhou</forenames></author></authors><title>Avoiding $\mathbf{exp(R_{max})}$ scaling in RLHF through   Preference-based Exploration</title><categories>cs.LG cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal technique for large language model (LLM) alignment. This paper studies the setting of online RLHF and focus on improving sample efficiency. All existing algorithms in online RLHF, whether doing passive exploration or active exploration, suffer from a sample complexity that scales exponentially with the scale of the reward function. This fundamental limitation hinders their effectiveness in scenarios with heavily skewed preferences, e.g. questions with a unique correct solution. To address this, we introduce Self-Exploring Preference-Incentive Online Preference Optimization (SE-POPO), an online RLHF algorithm that for the first time achieves a sample complexity that scales polynomially with the reward scale, answering an open problem raised by Xie et al. (2024).. Theoretically, we demonstrate that the sample complexity of SE-POPO dominates that of existing exploration algorithms. Empirically, our systematic evaluation confirms that SE-POPO is more sample-efficient than both exploratory and non-exploratory baselines, in two primary application scenarios of RLHF as well as on public benchmarks, marking a significant step forward in RLHF algorithm design. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00669</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00669</id><created>2025-02-01</created><authors><author><keyname>Kao</keyname><forenames>Ching-Chia</forenames></author><author><keyname>Yu</keyname><forenames>Chia-Mu</forenames></author><author><keyname>Lu</keyname><forenames>Chun-Shien</forenames></author><author><keyname>Chen</keyname><forenames>Chu-Song</forenames></author></authors><title>Safety Alignment Depth in Large Language Models: A Markov Chain   Perspective</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) are increasingly adopted in high-stakes scenarios, yet their safety mechanisms often remain fragile. Simple jailbreak prompts or even benign fine-tuning can bypass these protocols, underscoring the need to understand where and how they fail. Recent findings suggest that vulnerabilities emerge when alignment is confined to only the initial output tokens. Unfortunately, even with the introduction of deep safety alignment, determining the optimal safety depth remains an unresolved challenge. By leveraging the equivalence between autoregressive language models and Markov chains, this paper offers the first theoretical result on how to identify the ideal depth for safety alignment, and demonstrates how permutation-based data augmentation can tighten these bounds. Crucially, we reveal a fundamental interaction between alignment depth and ensemble width-indicating that broader ensembles can compensate for shallower alignments. These insights provide a theoretical foundation for designing more robust, scalable safety strategies that complement existing alignment approaches, opening new avenues for research into safer, more reliable LLMs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00671</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00671</id><created>2025-02-01</created><authors><author><keyname>Shirmarz</keyname><forenames>Alireza</forenames></author><author><keyname>Verdi</keyname><forenames>Fabio Luciano</forenames></author><author><keyname>Singh</keyname><forenames>Suneet Kumar</forenames></author><author><keyname>Rothenberg</keyname><forenames>Christian Esteve</forenames></author></authors><title>POSMAC: Powering Up In-Network AR/CG Traffic Classification with Online   Learning</title><categories>cs.NI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this demonstration, we showcase POSMAC1, a platform designed to deploy Decision Tree (DT) and Random Forest (RF) models on the NVIDIA DOCA DPU, equipped with an ARM processor, for real-time network traffic classification. Developed specifically for Augmented Reality (AR) and Cloud Gaming (CG) traffic classification, POSMAC streamlines model evaluation, and generalization while optimizing throughput to closely match line rates. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00672</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00672</id><created>2025-02-02</created><authors><author><keyname>Xu</keyname><forenames>Haodi</forenames></author><author><keyname>Fan</keyname><forenames>Joshua</forenames></author><author><keyname>Tao</keyname><forenames>Feng</forenames></author><author><keyname>Jiang</keyname><forenames>Lifen</forenames></author><author><keyname>You</keyname><forenames>Fengqi</forenames></author><author><keyname>Houlton</keyname><forenames>Benjamin Z.</forenames></author><author><keyname>Sun</keyname><forenames>Ying</forenames></author><author><keyname>Gomes</keyname><forenames>Carla P.</forenames></author><author><keyname>Luo</keyname><forenames>Yiqi</forenames></author></authors><title>Biogeochemistry-Informed Neural Network (BINN) for Improving Accuracy of   Model Prediction and Scientific Understanding of Soil Organic Carbon</title><categories>physics.geo-ph cs.AI</categories><comments>41 pages, 8 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Big data and the rapid development of artificial intelligence (AI) provide unprecedented opportunities to enhance our understanding of the global carbon cycle and other biogeochemical processes. However, retrieving mechanistic knowledge from big data remains a challenge. Here, we develop a Biogeochemistry-Informed Neural Network (BINN) that seamlessly integrates a vectorized process-based soil carbon cycle model (i.e., Community Land Model version 5, CLM5) into a neural network (NN) structure to examine mechanisms governing soil organic carbon (SOC) storage from big data. BINN demonstrates high accuracy in retrieving biogeochemical parameter values from synthetic data in a parameter recovery experiment. We use BINN to predict six major processes regulating the soil carbon cycle (or components in process-based models) from 25,925 observed SOC profiles across the conterminous US and compared them with the same processes previously retrieved by a Bayesian inference-based PROcess-guided deep learning and DAta-driven modeling (PRODA) approach (Tao et al. 2020; 2023). The high agreement between the spatial patterns of the retrieved processes using the two approaches with an average correlation coefficient of 0.81 confirms BINN's ability in retrieving mechanistic knowledge from big data. Additionally, the integration of neural networks and process-based models in BINN improves computational efficiency by more than 50 times over PRODA. We conclude that BINN is a transformative tool that harnesses the power of both AI and process-based modeling, facilitation new scientific discoveries while improving interpretability and accuracy of Earth system models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00673</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00673</id><created>2025-02-02</created><authors><author><keyname>Sharmaa</keyname><forenames>Kiran</forenames></author><author><keyname>Khurana</keyname><forenames>Parul</forenames></author></authors><title>Retracted Citations and Self-citations in Retracted Publications: A   Comparative Study of Plagiarism and Fake Peer Review</title><categories>cs.IR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Retracted citations remain a significant concern in academia as they perpetuate misinformation and compromise the integrity of scientific literature despite their invalidation. To analyze the impact of retracted citations, we focused on two retraction categories: plagiarism and fake peer review. The data set was sourced from Scopus and the reasons for the retraction were mapped using the Retraction Watch database. The retraction trend shows a steady average growth in plagiarism cases of 1.2 times, while the fake peer review exhibits a fluctuating pattern with an average growth of 5.5 times. Although fewer papers are retracted in the plagiarism category compared to fake peer reviews, plagiarism-related papers receive 2.5 times more citations. Furthermore, the total number of retracted citations for plagiarized papers is 1.8 times higher than that for fake peer review papers. Within the plagiarism category, 46% of the retracted citations are due to plagiarism, while 53.6% of the retracted citations in the fake peer review category are attributed to the fake peer review. The results also suggest that fake peer review cases are identified and retracted more rapidly than plagiarism cases. Finally, self-citations constitute a small percentage of citations to retracted papers but are notably higher among citations that are later retracted in both the categories. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00674</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00674</id><created>2025-02-02</created><authors><author><keyname>Li</keyname><forenames>Wenzhe</forenames></author><author><keyname>Lin</keyname><forenames>Yong</forenames></author><author><keyname>Xia</keyname><forenames>Mengzhou</forenames></author><author><keyname>Jin</keyname><forenames>Chi</forenames></author></authors><title>Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models   Beneficial?</title><categories>cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Ensembling outputs from diverse sources is a straightforward yet effective approach to boost performance. Mixture-of-Agents (MoA) is one such popular ensemble method that aggregates outputs from multiple different Large Language Models (LLMs). This paper raises the question in the context of language models: is mixing different LLMs truly beneficial? We propose Self-MoA -- an ensemble method that aggregates outputs from only the single top-performing LLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms standard MoA that mixes different LLMs in a large number of scenarios: Self-MoA achieves $6.6\%$ improvement over MoA on the AlpacaEval 2.0 benchmark, and an average of $3.8\%$ improvement across various benchmarks, including MMLU, CRUX, and MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0 directly achieves the new state-of-the-art performance on the leaderboard. To understand the effectiveness of Self-MoA, we systematically investigate the trade-off between diversity and quality of outputs under various MoA settings. We confirm that the MoA performance is rather sensitive to the quality, and mixing different LLMs often lowers the average quality of the models. To complement the study, we identify the scenarios where mixing different LLMs could be helpful. This paper further introduces a sequential version of Self-MoA, that is capable of aggregating a large number of LLM outputs on-the-fly over multiple rounds, and is as effective as aggregating all outputs at once. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00675</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00675</id><created>2025-02-02</created><authors><author><keyname>Deng</keyname><forenames>Minghang</forenames></author><author><keyname>Ramachandran</keyname><forenames>Ashwin</forenames></author><author><keyname>Xu</keyname><forenames>Canwen</forenames></author><author><keyname>Hu</keyname><forenames>Lanxiang</forenames></author><author><keyname>Yao</keyname><forenames>Zhewei</forenames></author><author><keyname>Datta</keyname><forenames>Anupam</forenames></author><author><keyname>Zhang</keyname><forenames>Hao</forenames></author></authors><title>ReFoRCE: A Text-to-SQL Agent with Self-Refinement, Format Restriction,   and Column Exploration</title><categories>cs.CL</categories><comments>13 pages, 1 figure</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text-to-SQL systems have unlocked easier access to critical data insights by enabling natural language queries over structured databases. However, deploying such systems in enterprise environments remains challenging due to factors such as large, complex schemas (&gt; 3000 columns), diverse SQL dialects (e.g., BigQuery, Snowflake) and sophisticated query requirements (e.g., transformation, analytics). Current state-of-the-art performance on the Spider 2.0 dataset -- a benchmark built to mimic such complex environments -- remains limited at 20%. Key limitations include inadequate instruction-following, poor long-context comprehension, weak self-refinement, and insufficient dialect-specific knowledge. To address these gaps, we propose ReFoRCE (Self-Refinement Agent with Format Restriction and Column Exploration) which introduces (1) table compression to mitigate long-context limitations (2) format restriction to ensure accurate answer format, and (3) iterative column exploration for enhanced schema understanding. Additionally, it employs self-refinement pipeline consisting of (1) parallelized workflows with voting mechanisms and (2) a Common Table Expression (CTE) based refinement approach to handle unresolved cases. ReFoRCE achieves state-of-the-art results scoring 26.69 on the Spider 2.0-Snow and scoring 24.50 on the Spider 2.0-Lite tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00676</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00676</id><created>2025-02-02</created><authors><author><keyname>Baterisna</keyname><forenames>Dan Alden</forenames></author><author><keyname>Chang</keyname><forenames>Yi-Jun</forenames></author></authors><title>Optimal local certification on graphs of bounded pathwidth</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present proof labeling schemes for graphs with bounded pathwidth that can decide any graph property expressible in monadic second-order (MSO) logic using $O(\log n)$-bit vertex labels. Examples of such properties include planarity, Hamiltonicity, $k$-colorability, $H$-minor-freeness, admitting a perfect matching, and having a vertex cover of a given size.   Our proof labeling schemes improve upon a recent result by Fraigniaud, Montealegre, Rapaport, and Todinca (Algorithmica 2024), which achieved the same result for graphs of bounded treewidth but required $O(\log^2 n)$-bit labels. Our improved label size $O(\log n)$ is optimal, as it is well-known that any proof labeling scheme that accepts paths and rejects cycles requires labels of size $\Omega(\log n)$.   Our result implies that graphs with pathwidth at most $k$ can be certified using $O(\log n)$-bit labels for any fixed constant $k$. Applying the Excluding Forest Theorem of Robertson and Seymour, we deduce that the class of $F$-minor-free graphs can be certified with $O(\log n)$-bit labels for any fixed forest $F$, thereby providing an affirmative answer to an open question posed by Bousquet, Feuilloley, and Pierron (Journal of Parallel and Distributed Computing 2024). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00677</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00677</id><created>2025-02-02</created><authors><author><keyname>Akhtar</keyname><forenames>Siraaj</forenames></author><author><keyname>Khan</keyname><forenames>Saad</forenames></author><author><keyname>Parkinson</keyname><forenames>Simon</forenames></author></authors><title>LLM-based event log analysis techniques: A survey</title><categories>cs.AI cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Event log analysis is an important task that security professionals undertake. Event logs record key information on activities that occur on computing devices, and due to the substantial number of events generated, they consume a large amount of time and resources to analyse. This demanding and repetitive task is also prone to errors. To address these concerns, researchers have developed automated techniques to improve the event log analysis process. Large Language Models (LLMs) have recently demonstrated the ability to successfully perform a wide range of tasks that individuals would usually partake in, to high standards, and at a pace and degree of complexity that outperform humans. Due to this, researchers are rapidly investigating the use of LLMs for event log analysis. This includes fine-tuning, Retrieval-Augmented Generation (RAG) and in-context learning, which affect performance. These works demonstrate good progress, yet there is a need to understand the developing body of knowledge, identify commonalities between works, and identify key challenges and potential solutions to further developments in this domain. This paper aims to survey LLM-based event log analysis techniques, providing readers with an in-depth overview of the domain, gaps identified in previous research, and concluding with potential avenues to explore in future. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00678</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00678</id><created>2025-02-02</created><authors><author><keyname>Choi</keyname><forenames>Hyeong Kyu</forenames></author><author><keyname>Khanov</keyname><forenames>Maxim</forenames></author><author><keyname>Wei</keyname><forenames>Hongxin</forenames></author><author><keyname>Li</keyname><forenames>Yixuan</forenames></author></authors><title>How Contaminated Is Your Benchmark? Quantifying Dataset Leakage in Large   Language Models with Kernel Divergence</title><categories>cs.LG cs.AI cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Dataset contamination, where evaluation datasets overlap with pre-training corpora, inflates performance metrics and undermines the reliability of model evaluations. Quantifying dataset contamination thus becomes essential to ensure that performance evaluations genuinely reflect a model's ability to generalize to unseen data, rather than relying on memorized examples. To address this problem, we propose Kernel Divergence Score (KDS), a novel method that quantifies dataset contamination by computing the divergence between the kernel similarity matrix of sample embeddings, before and after fine-tuning on the benchmark dataset. Leveraging the insight that fine-tuning affects unseen samples more significantly than seen ones, KDS provides a reliable measure of contamination. Through extensive experiments on controlled contamination scenarios, KDS demonstrates a near-perfect correlation with contamination levels and outperforms existing baselines. Additionally, we perform comprehensive ablation studies to analyze the impact of key design choices, providing deeper insights into the components and effectiveness of KDS. These ablations highlight the importance of leveraging fine-grained kernel-based information and confirm the reliability of the proposed framework across diverse datasets and settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00680</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00680</id><created>2025-02-02</created><authors><author><keyname>Meer</keyname><forenames>Klaus</forenames></author><author><keyname>Wurm</keyname><forenames>Adrian</forenames></author></authors><title>Some structural complexity results for $\exists\mathbb R$</title><categories>cs.CC</categories><comments>16 pages, no figures, submitted to CiE 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The complexity class $\exists\mathbb R$, standing for the complexity of deciding the existential first order theory of the reals as real closed field in the Turing model, has raised considerable interest in recent years. It is well known that NP $ \subseteq \exists\mathbb R\subseteq$ PSPACE. In their compendium, Schaefer, Cardinal, and Miltzow give a comprehensive presentation of results together with a rich collection of open problems. Here, we answer some of them dealing with structural issues of $\exists\mathbb R$ as a complexity class. We show analogues of the classical results of Baker, Gill, and Solovay finding oracles which do and do not separate NP form $\exists\mathbb R$, of Ladner's theorem showing the existence of problems in $\exists\mathbb R \setminus$ NP not being complete for $\exists\mathbb R$ (in case the two classes are different), as well as a characterization of $\exists\mathbb R$ by means of descriptive complexity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00681</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00681</id><created>2025-02-02</created><authors><author><keyname>Lin</keyname><forenames>Qika</forenames></author><author><keyname>Peng</keyname><forenames>Zhen</forenames></author><author><keyname>Shi</keyname><forenames>Kaize</forenames></author><author><keyname>He</keyname><forenames>Kai</forenames></author><author><keyname>Xu</keyname><forenames>Yiming</forenames></author><author><keyname>Cambria</keyname><forenames>Erik</forenames></author><author><keyname>Feng</keyname><forenames>Mengling</forenames></author></authors><title>A Survey of Quantized Graph Representation Learning: Connecting Graph   Structures with Large Language Models</title><categories>cs.LG cs.AI cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have witnessed rapid advances in graph representation learning, with the continuous embedding approach emerging as the dominant paradigm. However, such methods encounter issues regarding parameter efficiency, interpretability, and robustness. Thus, Quantized Graph Representation (QGR) learning has recently gained increasing interest, which represents the graph structure with discrete codes instead of conventional continuous embeddings. Given its analogous representation form to natural language, QGR also possesses the capability to seamlessly integrate graph structures with large language models (LLMs). As this emerging paradigm is still in its infancy yet holds significant promise, we undertake this thorough survey to promote its rapid future prosperity. We first present the background of the general quantization methods and their merits. Moreover, we provide an in-depth demonstration of current QGR studies from the perspectives of quantized strategies, training objectives, distinctive designs, knowledge graph quantization, and applications. We further explore the strategies for code dependence learning and integration with LLMs. At last, we give discussions and conclude future directions, aiming to provide a comprehensive picture of QGR and inspire future research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00682</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00682</id><created>2025-02-02</created><authors><author><keyname>Narechania</keyname><forenames>Arpit</forenames></author><author><keyname>Endert</keyname><forenames>Alex</forenames></author><author><keyname>Sinha</keyname><forenames>Atanu R</forenames></author></authors><title>Guidance Source Matters: How Guidance from AI, Expert, or a Group of   Analysts Impacts Visual Data Preparation and Analysis</title><categories>cs.HC cs.AI</categories><comments>21 pages, 10 figures, 6 figures, to appear in proceedings of ACM IUI   2025</comments><doi>10.1145/3708359.3712166</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The progress in generative AI has fueled AI-powered tools like co-pilots and assistants to provision better guidance, particularly during data analysis. However, research on guidance has not yet examined the perceived efficacy of the source from which guidance is offered and the impact of this source on the user's perception and usage of guidance. We ask whether users perceive all guidance sources as equal, with particular interest in three sources: (i) AI, (ii) human expert, and (iii) a group of human analysts. As a benchmark, we consider a fourth source, (iv) unattributed guidance, where guidance is provided without attribution to any source, enabling isolation of and comparison with the effects of source-specific guidance. We design a five-condition between-subjects study, with one condition for each of the four guidance sources and an additional (v) no-guidance condition, which serves as a baseline to evaluate the influence of any kind of guidance. We situate our study in a custom data preparation and analysis tool wherein we task users to select relevant attributes from an unfamiliar dataset to inform a business report. Depending on the assigned condition, users can request guidance, which the system then provides in the form of attribute suggestions. To ensure internal validity, we control for the quality of guidance across source-conditions. Through several metrics of usage and perception, we statistically test five preregistered hypotheses and report on additional analysis. We find that the source of guidance matters to users, but not in a manner that matches received wisdom. For instance, users utilize guidance differently at various stages of analysis, including expressing varying levels of regret, despite receiving guidance of similar quality. Notably, users in the AI condition reported both higher post-task benefit and regret. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00683</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00683</id><created>2025-02-02</created><authors><author><keyname>Sariyildiz</keyname><forenames>Emre</forenames></author></authors><title>IEEEICM25: "Stability of Digital Robust Motion Control Systems with   Disturbance Observer"</title><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, new stability analysis methods are proposed for digital robust motion control systems implemented using a disturbance observer. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00684</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00684</id><created>2025-02-02</created><authors><author><keyname>Jiang</keyname><forenames>Zeyu</forenames></author><author><keyname>Huang</keyname><forenames>Hai</forenames></author><author><keyname>Zuo</keyname><forenames>Xingquan</forenames></author></authors><title>Compositional Concept-Based Neuron-Level Interpretability for Deep   Reinforcement Learning</title><categories>cs.LG cs.AI</categories><comments>8 pages, 3 figures, IJCAI 2025</comments><acm-class>I.2.6; I.2.1; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep reinforcement learning (DRL), through learning policies or values represented by neural networks, has successfully addressed many complex control problems. However, the neural networks introduced by DRL lack interpretability and transparency. Current DRL interpretability methods largely treat neural networks as black boxes, with few approaches delving into the internal mechanisms of policy/value networks. This limitation undermines trust in both the neural network models that represent policies and the explanations derived from them. In this work, we propose a novel concept-based interpretability method that provides fine-grained explanations of DRL models at the neuron level. Our method formalizes atomic concepts as binary functions over the state space and constructs complex concepts through logical operations. By analyzing the correspondence between neuron activations and concept functions, we establish interpretable explanations for individual neurons in policy/value networks. Experimental results on both continuous control tasks and discrete decision-making environments demonstrate that our method can effectively identify meaningful concepts that align with human understanding while faithfully reflecting the network's decision-making logic. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00685</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00685</id><created>2025-02-02</created><authors><author><keyname>Sariyildiz</keyname><forenames>Emre</forenames></author></authors><title>IEEEICM25: "A High-Performance Disturbance Observer"</title><categories>eess.SY cs.RO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel Disturbance Observer, termed the High-Performance Disturbance Observer, which achieves more accurate disturbance estimation compared to the conventional disturbance observer, thereby delivering significant improvements in robustness and performance for motion control systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00686</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00686</id><created>2025-02-02</created><authors><author><keyname>Park</keyname><forenames>Minhyuk</forenames></author><author><keyname>Feng</keyname><forenames>Daniel Wang</forenames></author><author><keyname>Digra</keyname><forenames>Siya</forenames></author><author><keyname>Vue-Le</keyname><forenames>The-Anh</forenames></author><author><keyname>Anne</keyname><forenames>Lahari</forenames></author><author><keyname>Chacko</keyname><forenames>George</forenames></author><author><keyname>Warnow</keyname><forenames>Tandy</forenames></author></authors><title>Improved Community Detection using Stochastic Block Models</title><categories>cs.SI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Identifying edge-dense communities that are also well-connected is an important aspect of understanding community structure. Prior work has shown that community detection methods can produce poorly connected communities, and some can even produce internally disconnected communities. In this study we evaluate the connectivity of communities obtained using Stochastic Block Models. We find that SBMs produce internally disconnected communities from real-world networks. We present a simple technique, Well-Connected Clusters (WCC), which repeatedly removes small edge cuts until the communities meet a user-specified threshold for well-connectivity. Our study using a large collection of synthetic networks based on clustered real-world networks shows that using WCC as a post-processing tool with SBM community detection typically improves clustering accuracy. WCC is fast enough to use on networks with millions of nodes and is freely available in open source form. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00687</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00687</id><created>2025-02-02</created><authors><author><keyname>Zhao</keyname><forenames>Liang</forenames></author><author><keyname>Shao</keyname><forenames>Kunming</forenames></author><author><keyname>Tian</keyname><forenames>Fengshi</forenames></author><author><keyname>Cheng</keyname><forenames>Tim Kwang-Ting</forenames></author><author><keyname>Tsui</keyname><forenames>Chi-Ying</forenames></author><author><keyname>Zou</keyname><forenames>Yi</forenames></author></authors><title>A Flexible Precision Scaling Deep Neural Network Accelerator with   Efficient Weight Combination</title><categories>cs.AR cs.SY eess.SY</categories><comments>Accepted by 2025 IEEE International Symposium on Circuits and Systems   (ISCAS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deploying mixed-precision neural networks on edge devices is friendly to hardware resources and power consumption. To support fully mixed-precision neural network inference, it is necessary to design flexible hardware accelerators for continuous varying precision operations. However, the previous works have issues on hardware utilization and overhead of reconfigurable logic. In this paper, we propose an efficient accelerator for 2~8-bit precision scaling with serial activation input and parallel weight preloaded. First, we set two loading modes for the weight operands and decompose the weight into the corresponding bitwidths, which extends the weight precision support efficiently. Then, to improve hardware utilization of low-precision operations, we design the architecture that performs bit-serial MAC operation with systolic dataflow, and the partial sums are combined spatially. Furthermore, we designed an efficient carry save adder tree supporting both signed and unsigned number summation across rows. The experiment result shows that the proposed accelerator, synthesized with TSMC 28nm CMOS technology, achieves peak throughput of 4.09TOPS and peak energy efficiency of 68.94TOPS/W at 2/2-bit operations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00688</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00688</id><created>2025-02-02</created><authors><author><keyname>Chen</keyname><forenames>Bo</forenames></author><author><keyname>Gong</keyname><forenames>Chengyue</forenames></author><author><keyname>Li</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Liang</keyname><forenames>Yingyu</forenames></author><author><keyname>Sha</keyname><forenames>Zhizhou</forenames></author><author><keyname>Shi</keyname><forenames>Zhenmei</forenames></author><author><keyname>Song</keyname><forenames>Zhao</forenames></author><author><keyname>Wan</keyname><forenames>Mingda</forenames></author></authors><title>High-Order Matching for One-Step Shortcut Diffusion Models</title><categories>cs.CV cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  One-step shortcut diffusion models [Frans, Hafner, Levine and Abbeel, ICLR 2025] have shown potential in vision generation, but their reliance on first-order trajectory supervision is fundamentally limited. The Shortcut model's simplistic velocity-only approach fails to capture intrinsic manifold geometry, leading to erratic trajectories, poor geometric alignment, and instability-especially in high-curvature regions. These shortcomings stem from its inability to model mid-horizon dependencies or complex distributional features, leaving it ill-equipped for robust generative modeling. In this work, we introduce HOMO (High-Order Matching for One-Step Shortcut Diffusion), a game-changing framework that leverages high-order supervision to revolutionize distribution transportation. By incorporating acceleration, jerk, and beyond, HOMO not only fixes the flaws of the Shortcut model but also achieves unprecedented smoothness, stability, and geometric precision. Theoretically, we prove that HOMO's high-order supervision ensures superior approximation accuracy, outperforming first-order methods. Empirically, HOMO dominates in complex settings, particularly in high-curvature regions where the Shortcut model struggles. Our experiments show that HOMO delivers smoother trajectories and better distributional alignment, setting a new standard for one-step generative models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00689</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00689</id><created>2025-02-02</created><authors><author><keyname>Adnan</keyname><forenames>Bassam</forenames></author><author><keyname>Miryala</keyname><forenames>Sathvika</forenames></author><author><keyname>Sambu</keyname><forenames>Aneesh</forenames></author><author><keyname>Vaidhyanathan</keyname><forenames>Karthik</forenames></author><author><keyname>De Sanctis</keyname><forenames>Martina</forenames></author><author><keyname>Spalazzese</keyname><forenames>Romina</forenames></author></authors><title>Leveraging LLMs for Dynamic IoT Systems Generation through   Mixed-Initiative Interaction</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  IoT systems face significant challenges in adapting to user needs, which are often under-specified and evolve with changing environmental contexts. To address these complexities, users should be able to explore possibilities, while IoT systems must learn and support users in the process of providing proper services, e.g., to serve novel experiences. The IoT-Together paradigm aims to meet this demand through the Mixed-Initiative Interaction (MII) paradigm that facilitates a collaborative synergy between users and IoT systems, enabling the co-creation of intelligent and adaptive solutions that are precisely aligned with user-defined goals. This work advances IoT-Together by integrating Large Language Models (LLMs) into its architecture. Our approach enables intelligent goal interpretation through a multi-pass dialogue framework and dynamic service generation at runtime according to user needs. To demonstrate the efficacy of our methodology, we design and implement the system in the context of a smart city tourism case study. We evaluate the system's performance using agent-based simulation and user studies. Results indicate efficient and accurate service identification and high adaptation quality. The empirical evidence indicates that the integration of Large Language Models (LLMs) into IoT architectures can significantly enhance the architectural adaptability of the system while ensuring real-world usability. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00690</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00690</id><created>2025-02-02</created><authors><author><keyname>Cao</keyname><forenames>Yuefan</forenames></author><author><keyname>Li</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Liang</keyname><forenames>Yingyu</forenames></author><author><keyname>Sha</keyname><forenames>Zhizhou</forenames></author><author><keyname>Shi</keyname><forenames>Zhenmei</forenames></author><author><keyname>Song</keyname><forenames>Zhao</forenames></author><author><keyname>Zhang</keyname><forenames>Jiahao</forenames></author></authors><title>Dissecting Submission Limit in Desk-Rejections: A Mathematical Analysis   of Fairness in AI Conference Policies</title><categories>cs.LG cs.AI cs.CY cs.DL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As AI research surges in both impact and volume, conferences have imposed submission limits to maintain paper quality and alleviate organizational pressure. In this work, we examine the fairness of desk-rejection systems under submission limits and reveal that existing practices can result in substantial inequities. Specifically, we formally define the paper submission limit problem and identify a critical dilemma: when the number of authors exceeds three, it becomes impossible to reject papers solely based on excessive submissions without negatively impacting innocent authors. Thus, this issue may unfairly affect early-career researchers, as their submissions may be penalized due to co-authors with significantly higher submission counts, while senior researchers with numerous papers face minimal consequences. To address this, we propose an optimization-based fairness-aware desk-rejection mechanism and formally define two fairness metrics: individual fairness and group fairness. We prove that optimizing individual fairness is NP-hard, whereas group fairness can be efficiently optimized via linear programming. Through case studies, we demonstrate that our proposed system ensures greater equity than existing methods, including those used in CVPR 2025, offering a more socially just approach to managing excessive submissions in AI conferences. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00691</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00691</id><created>2025-02-02</created><authors><author><keyname>Wang</keyname><forenames>Haozhe</forenames></author><author><keyname>Li</keyname><forenames>Long</forenames></author><author><keyname>Qu</keyname><forenames>Chao</forenames></author><author><keyname>Zhu</keyname><forenames>Fengming</forenames></author><author><keyname>Xu</keyname><forenames>Weidi</forenames></author><author><keyname>Chu</keyname><forenames>Wei</forenames></author><author><keyname>Lin</keyname><forenames>Fangzhen</forenames></author></authors><title>Learning Autonomous Code Integration for Math Language Models</title><categories>cs.AI cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent research on tool integration for math Large Language Models (LLMs) aims to combine complementary strengths of chain-of-thought (CoT) reasoning and code execution. However, we discover a critical limitation: current tool-integrated math LLMs rely on externally dictated instructions to decide whether to use CoT or code, lacking the autonomy to choose the most appropriate method independently. This prompts us to study \emph{Autonomous Code integration} for math LLMs, which enables models to \emph{independently} develop their own methodology-selection strategy in the absence of reliable supervision. To address this challenge, we propose an innovative Expectation-Maximization (EM) formulation that refines the model's decision-making through the exploration of its capabilities. This framework alternates between (a) computing a reference strategy that improves the model's belief over its capabilities through self-exploration, and (b) updating the model based on the refined belief. We further enhance this framework with an efficient implementation, incorporating a novel data synthesis strategy and off-policy reinforcement learning. Extensive experiments demonstrate that our approach, using only a public query set, significantly boosts the performance of existing math LLMs, raising accuracy by nearly 20\% to 65.28\% on the challenging MATH benchmark, while reducing code executions by up to 65\% . </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00693</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00693</id><created>2025-02-02</created><authors><author><keyname>Ke</keyname><forenames>Yekun</forenames></author><author><keyname>Liang</keyname><forenames>Yingyu</forenames></author><author><keyname>Sha</keyname><forenames>Zhizhou</forenames></author><author><keyname>Shi</keyname><forenames>Zhenmei</forenames></author><author><keyname>Song</keyname><forenames>Zhao</forenames></author></authors><title>DPBloomfilter: Securing Bloom Filters with Differential Privacy</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Bloom filter is a simple yet space-efficient probabilistic data structure that supports membership queries for dramatically large datasets. It is widely utilized and implemented across various industrial scenarios, often handling massive datasets that include sensitive user information necessitating privacy preservation. To address the challenge of maintaining privacy within the Bloom filter, we have developed the DPBloomfilter. This innovation integrates the classical differential privacy mechanism, specifically the Random Response technique, into the Bloom filter, offering robust privacy guarantees under the same running complexity as the standard Bloom filter. Through rigorous simulation experiments, we have demonstrated that our DPBloomfilter algorithm maintains high utility while ensuring privacy protections. To the best of our knowledge, this is the first work to provide differential privacy guarantees for the Bloom filter for membership query problems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00694</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00694</id><created>2025-02-02</created><authors><author><keyname>Barkan</keyname><forenames>Ella</forenames></author><author><keyname>Siddiqui</keyname><forenames>Ibrahim</forenames></author><author><keyname>Cheng</keyname><forenames>Kevin J.</forenames></author><author><keyname>Golts</keyname><forenames>Alex</forenames></author><author><keyname>Shoshan</keyname><forenames>Yoel</forenames></author><author><keyname>Weber</keyname><forenames>Jeffrey K.</forenames></author><author><keyname>Mota</keyname><forenames>Yailin Campos</forenames></author><author><keyname>Ozery-Flato</keyname><forenames>Michal</forenames></author><author><keyname>Sautto</keyname><forenames>Giuseppe A.</forenames></author></authors><title>Leveraging Large Language Models to Predict Antibody Biological Activity   Against Influenza A Hemagglutinin</title><categories>cs.LG cs.AI q-bio.QM</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Monoclonal antibodies (mAbs) represent one of the most prevalent FDA-approved modalities for treating autoimmune diseases, infectious diseases, and cancers. However, discovery and development of therapeutic antibodies remains a time-consuming and expensive process. Recent advancements in machine learning (ML) and artificial intelligence (AI) have shown significant promise in revolutionizing antibody discovery and optimization. In particular, models that predict antibody biological activity enable in-silico evaluation of binding and functional properties; such models can prioritize antibodies with the highest likelihoods of success in costly and time-intensive laboratory testing procedures. We here explore an AI model for predicting the binding and receptor blocking activity of antibodies against influenza A hemagglutinin (HA) antigens. Our present model is developed with the MAMMAL framework for biologics discovery to predict antibody-antigen interactions using only sequence information. To evaluate the model's performance, we tested it under various data split conditions to mimic real-world scenarios.   Our models achieved an AUROC $\geq$ 0.91 for predicting the activity of existing antibodies against seen HAs and an AUROC of 0.9 for unseen HAs. For novel antibody activity prediction, the AUROC was 0.73, which further declined to 0.63-0.66 under stringent constraints on similarity to existing antibodies. These results demonstrate the potential of AI foundation models to transform antibody design by reducing dependence on extensive laboratory testing and enabling more efficient prioritization of antibody candidates. Moreover, our findings emphasize the critical importance of diverse and comprehensive antibody datasets to improve the generalization of prediction models, particularly for novel antibody development. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00695</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00695</id><created>2025-02-02</created><authors><author><keyname>Wu</keyname><forenames>Linglong</forenames></author><author><keyname>Shan</keyname><forenames>Xuhao</forenames></author><author><keyname>Ge</keyname><forenames>Ruiquan</forenames></author><author><keyname>Liang</keyname><forenames>Ruoyu</forenames></author><author><keyname>Zhang</keyname><forenames>Chi</forenames></author><author><keyname>Li</keyname><forenames>Yonghong</forenames></author><author><keyname>Elazab</keyname><forenames>Ahmed</forenames></author><author><keyname>Luo</keyname><forenames>Huoling</forenames></author><author><keyname>Liu</keyname><forenames>Yunbi</forenames></author><author><keyname>Wang</keyname><forenames>Changmiao</forenames></author></authors><title>TMI-CLNet: Triple-Modal Interaction Network for Chronic Liver Disease   Prognosis From Imaging, Clinical, and Radiomic Data Fusion</title><categories>cs.CV cs.AI</categories><comments>6 pages, 3 figures, accepted by IEEE ISBI 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Chronic liver disease represents a significant health challenge worldwide and accurate prognostic evaluations are essential for personalized treatment plans. Recent evidence suggests that integrating multimodal data, such as computed tomography imaging, radiomic features, and clinical information, can provide more comprehensive prognostic information. However, modalities have an inherent heterogeneity, and incorporating additional modalities may exacerbate the challenges of heterogeneous data fusion. Moreover, existing multimodal fusion methods often struggle to adapt to richer medical modalities, making it difficult to capture inter-modal relationships. To overcome these limitations, We present the Triple-Modal Interaction Chronic Liver Network (TMI-CLNet). Specifically, we develop an Intra-Modality Aggregation module and a Triple-Modal Cross-Attention Fusion module, which are designed to eliminate intra-modality redundancy and extract cross-modal information, respectively. Furthermore, we design a Triple-Modal Feature Fusion loss function to align feature representations across modalities. Extensive experiments on the liver prognosis dataset demonstrate that our approach significantly outperforms existing state-of-the-art unimodal models and other multi-modal techniques. Our code is available at https://github.com/Mysterwll/liver.git. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00698</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00698</id><created>2025-02-02</created><authors><author><keyname>Cai</keyname><forenames>Huanqia</forenames></author><author><keyname>Yang</keyname><forenames>Yijun</forenames></author><author><keyname>Hu</keyname><forenames>Winston</forenames></author></authors><title>MM-IQ: Benchmarking Human-Like Abstraction and Reasoning in Multimodal   Models</title><categories>cs.AI cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IQ testing has served as a foundational methodology for evaluating human cognitive capabilities, deliberately decoupling assessment from linguistic background, language proficiency, or domain-specific knowledge to isolate core competencies in abstraction and reasoning. Yet, artificial intelligence research currently lacks systematic benchmarks to quantify these critical cognitive dimensions in multimodal systems. To address this critical gap, we propose MM-IQ, a comprehensive evaluation framework comprising 2,710 meticulously curated test items spanning 8 distinct reasoning paradigms.   Through systematic evaluation of leading open-source and proprietary multimodal models, our benchmark reveals striking limitations: even state-of-the-art architectures achieve only marginally superior performance to random chance (27.49% vs. 25% baseline accuracy). This substantial performance chasm highlights the inadequacy of current multimodal systems in approximating fundamental human reasoning capacities, underscoring the need for paradigm-shifting advancements to bridge this cognitive divide. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00699</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00699</id><created>2025-02-02</created><authors><author><keyname>Guo</keyname><forenames>Yulu</forenames></author><author><keyname>Zhang</keyname><forenames>Tongjia</forenames></author><author><keyname>Sun</keyname><forenames>Shu</forenames></author><author><keyname>Tao</keyname><forenames>Meixia</forenames></author><author><keyname>Gao</keyname><forenames>Ruifeng</forenames></author></authors><title>Measurement and Analysis of Scattering From Building Surfaces at   Millimeter-Wave Frequency</title><categories>eess.SP cs.ET</categories><comments>6 pages, 7 figures. 2025 IEEE Wireless Communications and Networking   Conference Workshops (WCNC Wkshps), Milan, Italy, 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In future air-to-ground integrated networks, the scattering effects from ground-based scatterers, such as buildings, cannot be neglected in millimeter-wave and higher frequency bands, and have a significant impact on channel characteristics. However, current scattering measurement studies primarily focus on single incident angles within the incident plane, leading to insufficient characterization of scattering properties. In this paper, we present scattering measurements conducted at 28 GHz on various real-world building surfaces with multiple incident angles and three-dimensional (3D) receiving angles. The measured data are analyzed in conjunction with parameterized scattering models in ray tracing and numerical simulations. Results indicate that for millimeter-wave channel modeling near building surfaces, it is crucial to account not only for surface materials but also for the scattering properties of the building surfaces with respect to the incident angle and receiving positions in 3D space. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00700</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00700</id><created>2025-02-02</created><authors><author><keyname>Chen</keyname><forenames>Yunuo</forenames></author><author><keyname>Li</keyname><forenames>Qian</forenames></author><author><keyname>He</keyname><forenames>Bing</forenames></author><author><keyname>Feng</keyname><forenames>Donghui</forenames></author><author><keyname>Wu</keyname><forenames>Ronghua</forenames></author><author><keyname>Wang</keyname><forenames>Qi</forenames></author><author><keyname>Song</keyname><forenames>Li</forenames></author><author><keyname>Lu</keyname><forenames>Guo</forenames></author><author><keyname>Zhang</keyname><forenames>Wenjun</forenames></author></authors><title>S2CFormer: Reorienting Learned Image Compression from Spatial   Interaction to Channel Aggregation</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transformers have achieved significant success in learned image compression (LIC), with Swin Transformers emerging as the mainstream choice for nonlinear transforms. A common belief is that their sophisticated spatial operations contribute most to their efficacy. However, the crucial role of the feed-forward network (FFN) based Channel Aggregation module within the transformer architecture has been largely overlooked, and the over-design of spatial operations leads to a suboptimal trade-off between decoding latency and R-D performance. In this paper, we reevaluate the key factors behind the competence of transformers in LIC. By replacing spatial operations with identity mapping, we are surprised to find that channel operations alone can approach the R-D performance of the leading methods. This solid lower bound of performance emphasizes that the presence of channel aggregation is more essential for the LIC model to achieve competitive performance, while the previously complex spatial interactions are partly redundant. Based on this insight, we initiate the "S2CFormer" paradigm, a general architecture that reorients the focus of LIC from Spatial Interaction to Channel Aggregation. We present two instantiations of the S2CFormer: S2C-Conv, and S2C-Attention. Each one incorporates a simple operator for spatial interaction and serves as nonlinear transform blocks for our LIC models. Both models demonstrate state-of-the-art (SOTA) R-D performance and significantly faster decoding speed. These results also motivate further exploration of advanced FFN structures to enhance the R-D performance while maintaining model efficiency. With these foundations, we introduce S2C-Hybrid, an enhanced LIC model that combines the strengths of different S2CFormer instantiations. This model outperforms all the existing methods on several datasets, setting a new benchmark for efficient and high-performance LIC. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00702</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00702</id><created>2025-02-02</created><authors><author><keyname>Lyu</keyname><forenames>Sheng</forenames></author><author><keyname>Huang</keyname><forenames>Ruiming</forenames></author><author><keyname>Ji</keyname><forenames>Sijie</forenames></author><author><keyname>Rehman</keyname><forenames>Yasar Abbas Ur</forenames></author><author><keyname>Ma</keyname><forenames>Lan</forenames></author><author><keyname>Wu</keyname><forenames>Chenshu</forenames></author></authors><title>CardioLive: Empowering Video Streaming with Online Cardiac Monitoring</title><categories>cs.HC cs.NI cs.SD eess.AS eess.IV</categories><comments>Preprint</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Online Cardiac Monitoring (OCM) emerges as a compelling enhancement for the next-generation video streaming platforms. It enables various applications including remote health, online affective computing, and deepfake detection. Yet the physiological information encapsulated in the video streams has been long neglected. In this paper, we present the design and implementation of CardioLive, the first online cardiac monitoring system in video streaming platforms. We leverage the naturally co-existed video and audio streams and devise CardioNet, the first audio-visual network to learn the cardiac series. It incorporates multiple unique designs to extract temporal and spectral features, ensuring robust performance under realistic video streaming conditions. To enable the Service-On-Demand online cardiac monitoring, we implement CardioLive as a plug-and-play middleware service and develop systematic solutions to practical issues including changing FPS and unsynchronized streams. Extensive experiments have been done to demonstrate the effectiveness of our system. We achieve a Mean Square Error (MAE) of 1.79 BPM error, outperforming the video-only and audio-only solutions by 69.2% and 81.2%, respectively. Our CardioLive service achieves average throughputs of 115.97 and 98.16 FPS when implemented in Zoom and YouTube. We believe our work opens up new applications for video stream systems. We will release the code soon. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00703</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00703</id><created>2025-02-02</created><authors><author><keyname>Irigoyen</keyname><forenames>Marcos</forenames></author><author><keyname>Santana</keyname><forenames>Carla</forenames></author><author><keyname>Araújo</keyname><forenames>Ramon C. F</forenames></author><author><keyname>Xavier-de-Souza</keyname><forenames>Samuel</forenames></author></authors><title>DeLIAP e DeLIAJ: Interfaces de biblioteca de Dependabilidade para Python   e Julia</title><categories>cs.DC</categories><comments>6 pages, in Portuguese. First online version, adapted from the SPCEEC   submitted version. English translation pending</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The evergrowing computational complexity of High Performance Computing applications is often met with an horizontal scalling of computing systems. Colaterally, each added node risks being a single point of failure to parallel programs, increasing the demand for fault tolerant techniques to be applied, specially at software level. Under such conditions, the fault tolerance library DeLIA was developed in C/C++ with error detection and recovery features. We propose, then, to extend the library's capabilities to Python and Julia through the wrappers DeLIAP and DeLIAJ in order to lower the barrier to entry for implementing fault-tolerant solutions in these languages, which both lack alternatives to the library. To validate the efficiency of the wrappers, an application of the Julia wrapper in the 4D Full waveform inversion method was analyzed, quantitatively assessing the introduced overhead through runtime comparisons, while an implementation report is provided to address applicability. The added computational cost reflected on a median overhead of 1.4%, while limitations in the original parallel computing module used in the application rendered local-scope data checkpointing unfeasible. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00705</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00705</id><created>2025-02-02</created><authors><author><keyname>Cisneros-Velarde</keyname><forenames>Pedro</forenames></author><author><keyname>Shrimali</keyname><forenames>Bhavesh</forenames></author><author><keyname>Banerjee</keyname><forenames>Arindam</forenames></author></authors><title>Optimization for Neural Operators can Benefit from Width</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural Operators that directly learn mappings between function spaces, such as Deep Operator Networks (DONs) and Fourier Neural Operators (FNOs), have received considerable attention. Despite the universal approximation guarantees for DONs and FNOs, there is currently no optimization convergence guarantee for learning such networks using gradient descent (GD). In this paper, we address this open problem by presenting a unified framework for optimization based on GD and applying it to establish convergence guarantees for both DONs and FNOs. In particular, we show that the losses associated with both of these neural operators satisfy two conditions -- restricted strong convexity (RSC) and smoothness -- that guarantee a decrease on their loss values due to GD. Remarkably, these two conditions are satisfied for each neural operator due to different reasons associated with the architectural differences of the respective models. One takeaway that emerges from the theory is that wider networks should lead to better optimization convergence for both DONs and FNOs. We present empirical results on canonical operator learning problems to support our theoretical results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00706</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00706</id><created>2025-02-02</created><authors><author><keyname>Nikolic</keyname><forenames>Ivica</forenames></author><author><keyname>Baluta</keyname><forenames>Teodora</forenames></author><author><keyname>Saxena</keyname><forenames>Prateek</forenames></author></authors><title>Model Provenance Testing for Large Language Models</title><categories>cs.CR cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models are increasingly customized through fine-tuning and other adaptations, creating challenges in enforcing licensing terms and managing downstream impacts. Tracking model origins is crucial both for protecting intellectual property and for identifying derived models when biases or vulnerabilities are discovered in foundation models. We address this challenge by developing a framework for testing model provenance: Whether one model is derived from another. Our approach is based on the key observation that real-world model derivations preserve significant similarities in model outputs that can be detected through statistical analysis. Using only black-box access to models, we employ multiple hypothesis testing to compare model similarities against a baseline established by unrelated models. On two comprehensive real-world benchmarks spanning models from 30M to 4B parameters and comprising over 600 models, our tester achieves 90-95% precision and 80-90% recall in identifying derived models. These results demonstrate the viability of systematic provenance verification in production environments even when only API access is available. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00708</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00708</id><created>2025-02-02</created><authors><author><keyname>Li</keyname><forenames>Qixuan</forenames></author><author><keyname>Wang</keyname><forenames>Chao</forenames></author><author><keyname>He</keyname><forenames>Zongjin</forenames></author><author><keyname>Peng</keyname><forenames>Yan</forenames></author></authors><title>PhiP-G: Physics-Guided Text-to-3D Compositional Scene Generation</title><categories>cs.CV cs.AI</categories><comments>13 pages.8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text-to-3D asset generation has achieved significant optimization under the supervision of 2D diffusion priors. However, when dealing with compositional scenes, existing methods encounter several challenges: 1). failure to ensure that composite scene layouts comply with physical laws; 2). difficulty in accurately capturing the assets and relationships described in complex scene descriptions; 3). limited autonomous asset generation capabilities among layout approaches leveraging large language models (LLMs). To avoid these compromises, we propose a novel framework for compositional scene generation, PhiP-G, which seamlessly integrates generation techniques with layout guidance based on a world model. Leveraging LLM-based agents, PhiP-G analyzes the complex scene description to generate a scene graph, and integrating a multimodal 2D generation agent and a 3D Gaussian generation method for targeted assets creation. For the stage of layout, PhiP-G employs a physical pool with adhesion capabilities and a visual supervision agent, forming a world model for layout prediction and planning. Extensive experiments demonstrate that PhiP-G significantly enhances the generation quality and physical rationality of the compositional scenes. Notably, PhiP-G attains state-of-the-art (SOTA) performance in CLIP scores, achieves parity with the leading methods in generation quality as measured by the T$^3$Bench, and improves efficiency by 24x. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00709</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00709</id><created>2025-02-02</created><authors><author><keyname>Jin</keyname><forenames>Can</forenames></author><author><keyname>Peng</keyname><forenames>Hongwu</forenames></author><author><keyname>Zhang</keyname><forenames>Anxiang</forenames></author><author><keyname>Chen</keyname><forenames>Nuo</forenames></author><author><keyname>Zhao</keyname><forenames>Jiahui</forenames></author><author><keyname>Xie</keyname><forenames>Xi</forenames></author><author><keyname>Li</keyname><forenames>Kuangzheng</forenames></author><author><keyname>Feng</keyname><forenames>Shuya</forenames></author><author><keyname>Zhong</keyname><forenames>Kai</forenames></author><author><keyname>Ding</keyname><forenames>Caiwen</forenames></author><author><keyname>Metaxas</keyname><forenames>Dimitris N.</forenames></author></authors><title>RankFlow: A Multi-Role Collaborative Reranking Workflow Utilizing Large   Language Models</title><categories>cs.IR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In an Information Retrieval (IR) system, reranking plays a critical role by sorting candidate passages according to their relevance to a specific query. This process demands a nuanced understanding of the variations among passages linked to the query. In this work, we introduce RankFlow, a multi-role reranking workflow that leverages the capabilities of Large Language Models (LLMs) and role specializations to improve reranking performance. RankFlow enlists LLMs to fulfill four distinct roles: the query Rewriter, the pseudo Answerer, the passage Summarizer, and the Reranker. This orchestrated approach enables RankFlow to: (1) accurately interpret queries, (2) draw upon LLMs' extensive pre-existing knowledge, (3) distill passages into concise versions, and (4) assess passages in a comprehensive manner, resulting in notably better reranking results. Our experimental results reveal that RankFlow outperforms existing leading approaches on widely recognized IR benchmarks, such as TREC-DL, BEIR, and NovelEval. Additionally, we investigate the individual contributions of each role in RankFlow. Code is available at https://github.com/jincan333/RankFlow. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00711</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00711</id><created>2025-02-02</created><authors><author><keyname>Zhang</keyname><forenames>Chunbai</forenames></author><author><keyname>Wang</keyname><forenames>Chao</forenames></author><author><keyname>Zhou</keyname><forenames>Yang</forenames></author><author><keyname>Peng</keyname><forenames>Yan</forenames></author></authors><title>VIKSER: Visual Knowledge-Driven Self-Reinforcing Reasoning Framework</title><categories>cs.CV cs.AI</categories><comments>17 pages,12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual reasoning refers to the task of solving questions about visual information. Current visual reasoning methods typically employ pre-trained vision-language model (VLM) strategies or deep neural network approaches. However, existing efforts are constrained by limited reasoning interpretability, while hindering by the phenomenon of underspecification in the question text. Additionally, the absence of fine-grained visual knowledge limits the precise understanding of subject behavior in visual reasoning tasks. To address these issues, we propose VIKSER (Visual Knowledge-Driven Self-Reinforcing Reasoning Framework). Specifically, VIKSER, trained using knowledge distilled from large language models, extracts fine-grained visual knowledge with the assistance of visual relationship detection techniques. Subsequently, VIKSER utilizes fine-grained visual knowledge to paraphrase the question with underspecification. Additionally, we design a novel prompting method called Chain-of-Evidence (CoE), which leverages the power of ``evidence for reasoning'' to endow VIKSER with interpretable reasoning capabilities. Meanwhile, the integration of self-reflection technology empowers VIKSER with the ability to learn and improve from its mistakes. Experiments conducted on widely used datasets demonstrate that VIKSER achieves new state-of-the-art (SOTA) results in relevant tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00712</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00712</id><created>2025-02-02</created><authors><author><keyname>Sang</keyname><forenames>Shengtian</forenames></author><author><keyname>Jahanandish</keyname><forenames>Hassan</forenames></author><author><keyname>Li</keyname><forenames>Cynthia Xinran</forenames></author><author><keyname>Bhattachary</keyname><forenames>Indrani</forenames></author><author><keyname>Lee</keyname><forenames>Jeong Hoon</forenames></author><author><keyname>Zhang</keyname><forenames>Lichun</forenames></author><author><keyname>Vesal</keyname><forenames>Sulaiman</forenames></author><author><keyname>Ghanouni</keyname><forenames>Pejman</forenames></author><author><keyname>Fan</keyname><forenames>Richard</forenames></author><author><keyname>Sonn</keyname><forenames>Geoffrey A.</forenames></author><author><keyname>Rusu</keyname><forenames>Mirabela</forenames></author></authors><title>Registration-Enhanced Segmentation Method for Prostate Cancer in   Ultrasound Images</title><categories>eess.IV cs.AI cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prostate cancer is a major cause of cancer-related deaths in men, where early detection greatly improves survival rates. Although MRI-TRUS fusion biopsy offers superior accuracy by combining MRI's detailed visualization with TRUS's real-time guidance, it is a complex and time-intensive procedure that relies heavily on manual annotations, leading to potential errors. To address these challenges, we propose a fully automatic MRI-TRUS fusion-based segmentation method that identifies prostate tumors directly in TRUS images without requiring manual annotations. Unlike traditional multimodal fusion approaches that rely on naive data concatenation, our method integrates a registration-segmentation framework to align and leverage spatial information between MRI and TRUS modalities. This alignment enhances segmentation accuracy and reduces reliance on manual effort. Our approach was validated on a dataset of 1,747 patients from Stanford Hospital, achieving an average Dice coefficient of 0.212, outperforming TRUS-only (0.117) and naive MRI-TRUS fusion (0.132) methods, with significant improvements (p $&lt;$ 0.01). This framework demonstrates the potential for reducing the complexity of prostate cancer diagnosis and provides a flexible architecture applicable to other multimodal medical imaging tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00714</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00714</id><created>2025-02-02</created><authors><author><keyname>Li</keyname><forenames>Jiahao</forenames></author><author><keyname>Tong</keyname><forenames>Dezhong</forenames></author><author><keyname>Hao</keyname><forenames>Zhuonan</forenames></author><author><keyname>Zhu</keyname><forenames>Yinbo</forenames></author><author><keyname>Wu</keyname><forenames>Hengan</forenames></author><author><keyname>Liu</keyname><forenames>Mingchao</forenames></author><author><keyname>Huang</keyname><forenames>Weicheng</forenames></author></authors><title>Harnessing Discrete Differential Geometry: A Virtual Playground for the   Bilayer Soft Robotics</title><categories>cs.RO cond-mat.soft</categories><comments>15 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Soft robots have garnered significant attention due to their promising applications across various domains. A hallmark of these systems is their bilayer structure, where strain mismatch caused by differential expansion between layers induces complex deformations. Despite progress in theoretical modeling and numerical simulation, accurately capturing their dynamic behavior, especially during environmental interactions, remains challenging. This study presents a novel simulation environment based on the Discrete Elastic Rod (DER) model to address the challenge. By leveraging discrete differential geometry (DDG), the DER approach offers superior convergence compared to conventional methods like Finite Element Method (FEM), particularly in handling contact interactions -- an essential aspect of soft robot dynamics in real-world scenarios. Our simulation framework incorporates key features of bilayer structures, including stretching, bending, twisting, and inter-layer coupling. This enables the exploration of a wide range of dynamic behaviors for bilayer soft robots, such as gripping, crawling, jumping, and swimming. The insights gained from this work provide a robust foundation for the design and control of advanced bilayer soft robotic systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00715</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00715</id><created>2025-02-02</created><authors><author><keyname>Barker</keyname><forenames>Ryan</forenames></author><author><keyname>Dorcheh</keyname><forenames>Alireza Ebrahimi</forenames></author><author><keyname>Seyfi</keyname><forenames>Tolunay</forenames></author><author><keyname>Afghah</keyname><forenames>Fatemeh</forenames></author></authors><title>REAL: Reinforcement Learning-Enabled xApps for Experimental Closed-Loop   Optimization in O-RAN with OSC RIC and srsRAN</title><categories>cs.NI</categories><comments>6 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Open Radio Access Network (O-RAN) offers an open, programmable architecture for next-generation wireless networks, enabling advanced control through AI-based applications on the near-Real-Time RAN Intelligent Controller (near-RT RIC). However, fully integrated, real-time demonstrations of closed-loop optimization in O-RAN remain scarce. In this paper, we present a complete framework that combines the O-RAN Software Community RIC (OSC RIC) with srsRAN for near-real-time network slicing using Reinforcement Learning (RL). Our system orchestrates resources across diverse slice types (eMBB, URLLC, mMTC) for up to 12 UEs. We incorporate GNU Radio blocks for channel modeling, including Free-Space Path Loss (FSPL), single-tap multipath, AWGN, and Doppler effects, to emulate an urban mobility scenario. Experimental results show that our RL-based xApps dynamically adapt resource allocation and maintain QoS under varying traffic demands, highlighting both the feasibility and challenges of end-to-end AI-driven optimization in a lightweight O-RAN testbed. Our findings establish a baseline for real-time RL-based slicing in a disaggregated 5G framework and underscore the need for further enhancements to support fully simulated PHY digital twins without reliance on commercial software. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00716</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00716</id><created>2025-02-02</created><authors><author><keyname>Teimuri</keyname><forenames>Mohammad T.</forenames></author><author><keyname>Dehghanian</keyname><forenames>Zahra</forenames></author><author><keyname>Aminian</keyname><forenames>Gholamali</forenames></author><author><keyname>Rabiee</keyname><forenames>Hamid R.</forenames></author></authors><title>UPL: Uncertainty-aware Pseudo-labeling for Imbalance Transductive Node   Classification</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph-structured datasets often suffer from class imbalance, which complicates node classification tasks. In this work, we address this issue by first providing an upper bound on population risk for imbalanced transductive node classification. We then propose a simple and novel algorithm, Uncertainty-aware Pseudo-labeling (UPL). Our approach leverages pseudo-labels assigned to unlabeled nodes to mitigate the adverse effects of imbalance on classification accuracy. Furthermore, the UPL algorithm enhances the accuracy of pseudo-labeling by reducing training noise of pseudo-labels through a novel uncertainty-aware approach. We comprehensively evaluate the UPL algorithm across various benchmark datasets, demonstrating its superior performance compared to existing state-of-the-art methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00717</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00717</id><created>2025-02-02</created><authors><author><keyname>Wang</keyname><forenames>Chao</forenames></author><author><keyname>Yang</keyname><forenames>Jianming</forenames></author><author><keyname>Zhou</keyname><forenames>Yang</forenames></author></authors><title>MINT: Mitigating Hallucinations in Large Vision-Language Models via   Token Reduction</title><categories>cs.CV</categories><comments>8 pages, 5 figures, 4 tables</comments><acm-class>I.2.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hallucination has been a long-standing and inevitable problem that hinders the application of Large Vision-Language Models (LVLMs) in domains that require high reliability. Various methods focus on improvement depending on data annotations or training strategies, yet place less emphasis on LLM's inherent problems. To fill this gap, we delve into the attention mechanism of the decoding process in the LVLM. Intriguingly, our investigation uncovers the prevalent attention redundancy within the hierarchical architecture of the LVLM, manifesting as overextended image processing in deep layers and an overabundance of non-essential image tokens. Stemming from the observation, we thus propose MINT, a novel training-free decoding strategy, MItigating hallucinations via tokeN reducTion. Specifically, we dynamically intensify the LVLM's local perception capability by masking its attention to irrelevant image tokens. In addition, we use contrastive decoding that pushes the model to focus more on those key image regions. Our full method aims to guide the model in concentrating more on key visual elements during generation. Extensive experimental results on several popular public benchmarks show that our approach achieves a 4% improvement in mitigating hallucinations caused by distracted perception compared to original models. Meanwhile, our approach is demonstrated to make the model perceive 5% more visual points even though we reduce a suite of image tokens. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00718</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00718</id><created>2025-02-02</created><authors><author><keyname>Gupta</keyname><forenames>Isha</forenames></author><author><keyname>Khachaturov</keyname><forenames>David</forenames></author><author><keyname>Mullins</keyname><forenames>Robert</forenames></author></authors><title>"I am bad": Interpreting Stealthy, Universal and Robust Audio Jailbreaks   in Audio-Language Models</title><categories>cs.LG cs.SD eess.AS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The rise of multimodal large language models has introduced innovative human-machine interaction paradigms but also significant challenges in machine learning safety. Audio-Language Models (ALMs) are especially relevant due to the intuitive nature of spoken communication, yet little is known about their failure modes. This paper explores audio jailbreaks targeting ALMs, focusing on their ability to bypass alignment mechanisms. We construct adversarial perturbations that generalize across prompts, tasks, and even base audio samples, demonstrating the first universal jailbreaks in the audio modality, and show that these remain effective in simulated real-world conditions. Beyond demonstrating attack feasibility, we analyze how ALMs interpret these audio adversarial examples and reveal them to encode imperceptible first-person toxic speech - suggesting that the most effective perturbations for eliciting toxic outputs specifically embed linguistic features within the audio signal. These results have important implications for understanding the interactions between different modalities in multimodal models, and offer actionable insights for enhancing defenses against adversarial audio attacks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00719</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00719</id><created>2025-02-02</created><authors><author><keyname>Sakurai</keyname><forenames>Kosuke</forenames></author><author><keyname>Shimizu</keyname><forenames>Ryotaro</forenames></author><author><keyname>Goto</keyname><forenames>Masayuki</forenames></author></authors><title>Vision and Language Reference Prompt into SAM for Few-shot Segmentation</title><categories>cs.CV</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Segment Anything Model (SAM) represents a large-scale segmentation model that enables powerful zero-shot capabilities with flexible prompts. While SAM can segment any object in zero-shot, it requires user-provided prompts for each target image and does not attach any label information to masks. Few-shot segmentation models addressed these issues by inputting annotated reference images as prompts to SAM and can segment specific objects in target images without user-provided prompts. Previous SAM-based few-shot segmentation models only use annotated reference images as prompts, resulting in limited accuracy due to a lack of reference information. In this paper, we propose a novel few-shot segmentation model, Vision and Language reference Prompt into SAM (VLP-SAM), that utilizes the visual information of the reference images and the semantic information of the text labels by inputting not only images but also language as reference information. In particular, VLP-SAM is a simple and scalable structure with minimal learnable parameters, which inputs prompt embeddings with vision-language information into SAM using a multimodal vision-language model. To demonstrate the effectiveness of VLP-SAM, we conducted experiments on the PASCAL-5i and COCO-20i datasets, and achieved high performance in the few-shot segmentation task, outperforming the previous state-of-the-art model by a large margin (6.3% and 9.5% in mIoU, respectively). Furthermore, VLP-SAM demonstrates its generality in unseen objects that are not included in the training data. Our code is available at https://github.com/kosukesakurai1/VLP-SAM. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00722</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00722</id><created>2025-02-02</created><authors><author><keyname>Jiang</keyname><forenames>Youhe</forenames></author><author><keyname>Fu</keyname><forenames>Fangcheng</forenames></author><author><keyname>Yao</keyname><forenames>Xiaozhe</forenames></author><author><keyname>He</keyname><forenames>Guoliang</forenames></author><author><keyname>Miao</keyname><forenames>Xupeng</forenames></author><author><keyname>Klimovic</keyname><forenames>Ana</forenames></author><author><keyname>Cui</keyname><forenames>Bin</forenames></author><author><keyname>Yuan</keyname><forenames>Binhang</forenames></author><author><keyname>Yoneki</keyname><forenames>Eiko</forenames></author></authors><title>Demystifying Cost-Efficiency in LLM Serving over Heterogeneous GPUs</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advancements in Large Language Models (LLMs) have led to increasingly diverse requests, accompanied with varying resource (compute and memory) demands to serve them. However, this in turn degrades the cost-efficiency of LLM serving as common practices primarily rely on homogeneous GPU resources. In response to this problem, this work conducts a thorough study about serving LLMs over heterogeneous GPU resources on cloud platforms. The rationale is that different GPU types exhibit distinct compute and memory characteristics, aligning well with the divergent resource demands of diverse requests. Particularly, through comprehensive benchmarking, we discover that the cost-efficiency of LLM serving can be substantially optimized by meticulously determining GPU composition, deployment configurations, and workload assignments. Subsequently, we design a scheduling algorithm via mixed-integer linear programming, aiming at deducing the most cost-efficient serving plan under the constraints of price budget and real-time GPU availability. Remarkably, our approach effectively outperforms homogeneous and heterogeneous baselines under a wide array of scenarios, covering diverse workload traces, varying GPU availablilities, and multi-model serving. This casts new light on more accessible and efficient LLM serving over heterogeneous cloud resources. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00724</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00724</id><created>2025-02-02</created><authors><author><keyname>Habi</keyname><forenames>Hai Victor</forenames></author><author><keyname>Messer</keyname><forenames>Hagit</forenames></author><author><keyname>Bresler</keyname><forenames>Yoram</forenames></author></authors><title>Learned Bayesian Cram\'er-Rao Bound for Unknown Measurement Models Using   Score Neural Networks</title><categories>eess.SP cs.AI cs.LG stat.ML</categories><comments>28 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bayesian Cram\'er-Rao bound (BCRB) is a crucial tool in signal processing for assessing the fundamental limitations of any estimation problem as well as benchmarking within a Bayesian frameworks. However, the BCRB cannot be computed without full knowledge of the prior and the measurement distributions. In this work, we propose a fully learned Bayesian Cram\'er-Rao bound (LBCRB) that learns both the prior and the measurement distributions. Specifically, we suggest two approaches to obtain the LBCRB: the Posterior Approach and the Measurement-Prior Approach. The Posterior Approach provides a simple method to obtain the LBCRB, whereas the Measurement-Prior Approach enables us to incorporate domain knowledge to improve the sample complexity and {interpretability}. To achieve this, we introduce a Physics-encoded score neural network which enables us to easily incorporate such domain knowledge into a neural network. We {study the learning} errors of the two suggested approaches theoretically, and validate them numerically. We demonstrate the two approaches on several signal processing examples, including a linear measurement problem with unknown mixing and Gaussian noise covariance matrices, frequency estimation, and quantized measurement. In addition, we test our approach on a nonlinear signal processing problem of frequency estimation with real-world underwater ambient noise. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00725</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00725</id><created>2025-02-02</created><authors><author><keyname>Shi</keyname><forenames>Dingyuan</forenames></author><author><keyname>Zhang</keyname><forenames>Lulu</forenames></author><author><keyname>Tong</keyname><forenames>Yongxin</forenames></author><author><keyname>Xu</keyname><forenames>Ke</forenames></author></authors><title>Understanding and Mitigating the High Computational Cost in Path Data   Diffusion</title><categories>cs.LG</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advancements in mobility services, navigation systems, and smart transportation technologies have made it possible to collect large amounts of path data. Modeling the distribution of this path data, known as the Path Generation (PG) problem, is crucial for understanding urban mobility patterns and developing intelligent transportation systems. Recent studies have explored using diffusion models to address the PG problem due to their ability to capture multimodal distributions and support conditional generation. A recent work devises a diffusion process explicitly in graph space and achieves state-of-the-art performance. However, this method suffers a high computation cost in terms of both time and memory, which prohibits its application. In this paper, we analyze this method both theoretically and experimentally and find that the main culprit of its high computation cost is its explicit design of the diffusion process in graph space. To improve efficiency, we devise a Latent-space Path Diffusion (LPD) model, which operates in latent space instead of graph space. Our LPD significantly reduces both time and memory costs by up to 82.8% and 83.1%, respectively. Despite these reductions, our approach does not suffer from performance degradation. It outperforms the state-of-the-art method in most scenarios by 24.5%~34.0%. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00726</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00726</id><created>2025-02-02</created><authors><author><keyname>Poupart</keyname><forenames>Yoann</forenames></author><author><keyname>Beynier</keyname><forenames>Aurélie</forenames></author><author><keyname>Maudet</keyname><forenames>Nicolas</forenames></author></authors><title>Perspectives for Direct Interpretability in Multi-Agent Deep   Reinforcement Learning</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multi-Agent Deep Reinforcement Learning (MADRL) was proven efficient in solving complex problems in robotics or games, yet most of the trained models are hard to interpret. While learning intrinsically interpretable models remains a prominent approach, its scalability and flexibility are limited in handling complex tasks or multi-agent dynamics. This paper advocates for direct interpretability, generating post hoc explanations directly from trained models, as a versatile and scalable alternative, offering insights into agents' behaviour, emergent phenomena, and biases without altering models' architectures. We explore modern methods, including relevance backpropagation, knowledge edition, model steering, activation patching, sparse autoencoders and circuit discovery, to highlight their applicability to single-agent, multi-agent, and training process challenges. By addressing MADRL interpretability, we propose directions aiming to advance active topics such as team identification, swarm coordination and sample efficiency. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00728</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00728</id><created>2025-02-02</created><authors><author><keyname>Kong</keyname><forenames>Mingze</forenames></author><author><keyname>Wang</keyname><forenames>Zhiyong</forenames></author><author><keyname>Shu</keyname><forenames>Yao</forenames></author><author><keyname>Dai</keyname><forenames>Zhongxiang</forenames></author></authors><title>Meta-Prompt Optimization for LLM-Based Sequential Decision Making</title><categories>cs.LG</categories><comments>Preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large language models (LLMs) have recently been employed as agents to solve sequential decision-making tasks such as Bayesian optimization and multi-armed bandits (MAB). These works usually adopt an LLM for sequential action selection by providing it with a fixed, manually designed meta-prompt. However, numerous previous works have found that the prompt has a significant impact on the performance of the LLM, which calls for a method to automatically optimize the meta-prompt for LLM-based agents. Unfortunately, the non-stationarity in the reward observations during LLM-based sequential decision-making makes meta-prompt optimization highly challenging. To address this challenge, we draw inspirations from adversarial bandit algorithms, which are inherently capable of handling non-stationary reward observations. Building on this foundation, we propose our EXPonential-weight algorithm for prompt Optimization} (EXPO) to automatically optimize the task description and meta-instruction in the meta-prompt for LLM-based agents. We also extend EXPO to additionally optimize the exemplars (i.e., history of interactions) in the meta-prompt to further enhance the performance, hence introducing our EXPO-ES algorithm. We use extensive experiments to show that our algorithms significantly improve the performance of LLM-based sequential decision-making. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00729</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00729</id><created>2025-02-02</created><authors><author><keyname>Taitler</keyname><forenames>Boaz</forenames></author><author><keyname>Ben-Porat</keyname><forenames>Omer</forenames></author></authors><title>Selective Response Strategies for GenAI</title><categories>cs.AI cs.GT cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rise of Generative AI (GenAI) has significantly impacted human-based forums like Stack Overflow, which are essential for generating high-quality data. This creates a negative feedback loop, hindering the development of GenAI systems, which rely on such data to provide accurate responses. In this paper, we provide a possible remedy: A novel strategy we call selective response. Selective response implies that GenAI could strategically provide inaccurate (or conservative) responses to queries involving emerging topics and novel technologies, thereby driving users to use human-based forums like Stack Overflow. We show that selective response can potentially have a compounding effect on the data generation process, increasing both GenAI's revenue and user welfare in the long term. From an algorithmic perspective, we propose an approximately optimal approach to maximize GenAI's revenue under social welfare constraints. From a regulatory perspective, we derive sufficient and necessary conditions for selective response to improve welfare improvements. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00730</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00730</id><created>2025-02-02</created><authors><author><keyname>Li</keyname><forenames>Yang</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Feng</keyname><forenames>Tianzhi</forenames></author><author><keyname>Li</keyname><forenames>Fu</forenames></author><author><keyname>Wu</keyname><forenames>Chennan</forenames></author><author><keyname>Fu</keyname><forenames>Boxun</forenames></author><author><keyname>Zhao</keyname><forenames>Zhifu</forenames></author><author><keyname>Wang</keyname><forenames>Xiaotian</forenames></author><author><keyname>Shi</keyname><forenames>Guangming</forenames></author></authors><title>Spatio-Temporal Progressive Attention Model for EEG Classification in   Rapid Serial Visual Presentation Task</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As a type of multi-dimensional sequential data, the spatial and temporal dependencies of electroencephalogram (EEG) signals should be further investigated. Thus, in this paper, we propose a novel spatial-temporal progressive attention model (STPAM) to improve EEG classification in rapid serial visual presentation (RSVP) tasks. STPAM first adopts three distinct spatial experts to learn the spatial topological information of brain regions progressively, which is used to minimize the interference of irrelevant brain regions. Concretely, the former expert filters out EEG electrodes in the relative brain regions to be used as prior knowledge for the next expert, ensuring that the subsequent experts gradually focus their attention on information from significant EEG electrodes. This process strengthens the effect of the important brain regions. Then, based on the above-obtained feature sequence with spatial information, three temporal experts are adopted to capture the temporal dependence by progressively assigning attention to the crucial EEG slices. Except for the above EEG classification method, in this paper, we build a novel Infrared RSVP EEG Dataset (IRED) which is based on dim infrared images with small targets for the first time, and conduct extensive experiments on it. The results show that our STPAM can achieve better performance than all the compared methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00734</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00734</id><created>2025-02-02</created><authors><author><keyname>Chu</keyname><forenames>Yun</forenames></author><author><keyname>Wang</keyname><forenames>Qiuhao</forenames></author><author><keyname>Zhou</keyname><forenames>Enze</forenames></author><author><keyname>Fu</keyname><forenames>Ling</forenames></author><author><keyname>Liu</keyname><forenames>Qian</forenames></author><author><keyname>Zheng</keyname><forenames>Gang</forenames></author></authors><title>CycleGuardian: A Framework for Automatic RespiratorySound classification   Based on Improved Deep clustering and Contrastive Learning</title><categories>cs.SD cs.AI eess.AS</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Auscultation plays a pivotal role in early respiratory and pulmonary disease diagnosis. Despite the emergence of deep learning-based methods for automatic respiratory sound classification post-Covid-19, limited datasets impede performance enhancement. Distinguishing between normal and abnormal respiratory sounds poses challenges due to the coexistence of normal respiratory components and noise components in both types. Moreover, different abnormal respiratory sounds exhibit similar anomalous features, hindering their differentiation. Besides, existing state-of-the-art models suffer from excessive parameter size, impeding deployment on resource-constrained mobile platforms. To address these issues, we design a lightweight network CycleGuardian and propose a framework based on an improved deep clustering and contrastive learning. We first generate a hybrid spectrogram for feature diversity and grouping spectrograms to facilitating intermittent abnormal sound capture.Then, CycleGuardian integrates a deep clustering module with a similarity-constrained clustering component to improve the ability to capture abnormal features and a contrastive learning module with group mixing for enhanced abnormal feature discernment. Multi-objective optimization enhances overall performance during training. In experiments we use the ICBHI2017 dataset, following the official split method and without any pre-trained weights, our method achieves Sp: 82.06 $\%$, Se: 44.47$\%$, and Score: 63.26$\%$ with a network model size of 38M, comparing to the current model, our method leads by nearly 7$\%$, achieving the current best performances. Additionally, we deploy the network on Android devices, showcasing a comprehensive intelligent respiratory sound auscultation system. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00735</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00735</id><created>2025-02-02</created><authors><author><keyname>Chiu</keyname><forenames>Chun Wai</forenames></author><author><keyname>Huang</keyname><forenames>Linghan</forenames></author><author><keyname>Li</keyname><forenames>Bo</forenames></author><author><keyname>Chen</keyname><forenames>Huaming</forenames></author></authors><title>From Compliance to Exploitation: Jailbreak Prompt Attacks on Multimodal   LLMs</title><categories>cs.CR cs.AI cs.SE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) have seen widespread applications across various domains due to their growing ability to process diverse types of input data, including text, audio, image and video. While LLMs have demonstrated outstanding performance in understanding and generating contexts for different scenarios, they are vulnerable to prompt-based attacks, which are mostly via text input. In this paper, we introduce the first voice-based jailbreak attack against multimodal LLMs, termed as Flanking Attack, which can process different types of input simultaneously towards the multimodal LLMs. Our work is motivated by recent advancements in monolingual voice-driven large language models, which have introduced new attack surfaces beyond traditional text-based vulnerabilities for LLMs. To investigate these risks, we examine the frontier multimodal LLMs, which can be accessed via different types of inputs such as audio input, focusing on how adversarial prompts can bypass its defense mechanisms. We propose a novel strategy, in which the disallowed prompt is flanked by benign, narrative-driven prompts. It is integrated in the Flanking Attack which attempts to humanizes the interaction context and execute the attack through a fictional setting. To better evaluate the attack performance, we present a semi-automated self-assessment framework for policy violation detection. We demonstrate that Flank Attack is capable of manipulating state-of-the-art LLMs into generating misaligned and forbidden outputs, which achieves an average attack success rate ranging from 0.67 to 0.93 across seven forbidden scenarios. These findings highlight both the potency of prompt-based obfuscation in voice-enabled contexts and the limitations of current LLMs' moderation safeguards and the urgent need for advanced defense strategies to address the challenges posed by evolving, context-rich attacks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00737</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00737</id><created>2025-02-02</created><authors><author><keyname>Le</keyname><forenames>Tam</forenames></author><author><keyname>Nguyen</keyname><forenames>Truyen</forenames></author><author><keyname>Hino</keyname><forenames>Hideitsu</forenames></author><author><keyname>Fukumizu</keyname><forenames>Kenji</forenames></author></authors><title>Scalable Sobolev IPM for Probability Measures on a Graph</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We investigate the Sobolev IPM problem for probability measures supported on a graph metric space. Sobolev IPM is an important instance of integral probability metrics (IPM), and is obtained by constraining a critic function within a unit ball defined by the Sobolev norm. In particular, it has been used to compare probability measures and is crucial for several theoretical works in machine learning. However, to our knowledge, there are no efficient algorithmic approaches to compute Sobolev IPM effectively, which hinders its practical applications. In this work, we establish a relation between Sobolev norm and weighted $L^p$-norm, and leverage it to propose a \emph{novel regularization} for Sobolev IPM. By exploiting the graph structure, we demonstrate that the regularized Sobolev IPM provides a \emph{closed-form} expression for fast computation. This advancement addresses long-standing computational challenges, and paves the way to apply Sobolev IPM for practical applications, even in large-scale settings. Additionally, the regularized Sobolev IPM is negative definite. Utilizing this property, we design positive-definite kernels upon the regularized Sobolev IPM, and provide preliminary evidences of their advantages on document classification and topological data analysis for measures on a graph. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00739</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00739</id><created>2025-02-02</created><authors><author><keyname>Le</keyname><forenames>Tam</forenames></author><author><keyname>Nguyen</keyname><forenames>Truyen</forenames></author><author><keyname>Hino</keyname><forenames>Hideitsu</forenames></author><author><keyname>Fukumizu</keyname><forenames>Kenji</forenames></author></authors><title>Orlicz-Sobolev Transport for Unbalanced Measures on a Graph</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Moving beyond $L^p$ geometric structure, Orlicz-Wasserstein (OW) leverages a specific class of convex functions for Orlicz geometric structure. While OW remarkably helps to advance certain machine learning approaches, it has a high computational complexity due to its two-level optimization formula. Recently, Le et al. (2024) exploits graph structure to propose generalized Sobolev transport (GST), i.e., a scalable variant for OW. However, GST assumes that input measures have the same mass. Unlike optimal transport (OT), it is nontrivial to incorporate a mass constraint to extend GST for measures on a graph, possibly having different total mass. In this work, we propose to take a step back by considering the entropy partial transport (EPT) for nonnegative measures on a graph. By leveraging Caffarelli &amp; McCann (2010)'s observations, EPT can be reformulated as a standard complete OT between two corresponding balanced measures. Consequently, we develop a novel EPT with Orlicz geometric structure, namely Orlicz-EPT, for unbalanced measures on a graph. Especially, by exploiting the dual EPT formulation and geometric structures of the graph-based Orlicz-Sobolev space, we derive a novel regularization to propose Orlicz-Sobolev transport (OST). The resulting distance can be efficiently computed by simply solving a univariate optimization problem, unlike the high-computational two-level optimization problem for Orlicz-EPT. Additionally, we derive geometric structures for the OST and draw its relations to other transport distances. We empirically show that OST is several-order faster than Orlicz-EPT. We further illustrate preliminary evidences on the advantages of OST for document classification, and several tasks in topological data analysis. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00744</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00744</id><created>2025-02-02</created><authors><author><keyname>Franssen</keyname><forenames>Christian</forenames></author><author><keyname>Jiang</keyname><forenames>Jinyang</forenames></author><author><keyname>Peng</keyname><forenames>Yijie</forenames></author><author><keyname>Heidergott</keyname><forenames>Bernd</forenames></author></authors><title>CoNNect: A Swiss-Army-Knife Regularizer for Pruning of Neural Networks</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pruning encompasses a range of techniques aimed at increasing the sparsity of neural networks (NNs). These techniques can generally be framed as minimizing a loss function subject to an $L_0$-norm constraint. This paper introduces CoNNect, a novel differentiable regularizer for sparse NN training that ensures connectivity between input and output layers. CoNNect integrates with established pruning strategies and supports both structured and unstructured pruning. We proof that CoNNect approximates $L_0$-regularization, guaranteeing maximally connected network structures while avoiding issues like layer collapse. Numerical experiments demonstrate that CoNNect improves classical pruning strategies and enhances state-of-the-art one-shot pruners, such as DepGraph and LLM-pruner. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00745</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00745</id><created>2025-02-02</created><authors><author><keyname>Bajpai</keyname><forenames>Divya Jyoti</forenames></author><author><keyname>Hanawal</keyname><forenames>Manjesh Kumar</forenames></author></authors><title>BEEM: Boosting Performance of Early Exit DNNs using Multi-Exit   Classifiers as Experts</title><categories>cs.LG cs.CL cs.CV</categories><comments>Published at International Conference on Learning Representations   (ICLR) 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Early Exit (EE) techniques have emerged as a means to reduce inference latency in Deep Neural Networks (DNNs). The latency improvement and accuracy in these techniques crucially depend on the criteria used to make exit decisions. We propose a new decision criterion where exit classifiers are treated as experts BEEM and aggregate their confidence scores. The confidence scores are aggregated only if neighbouring experts are consistent in prediction as the samples pass through them, thus capturing their ensemble effect. A sample exits when the aggregated confidence value exceeds a threshold. The threshold is set using the error rates of the intermediate exits aiming to surpass the performance of conventional DNN inference. Experimental results on the COCO dataset for Image captioning and GLUE datasets for various language tasks demonstrate that our method enhances the performance of state-of-the-art EE methods, achieving improvements in speed-up by a factor 1.5x to 2.1x. When compared to the final layer, its accuracy is comparable in harder Image Captioning and improves in the easier language tasks. The source code for this work is publicly available at https://github.com/Div290/BEEM1/tree/main </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00747</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00747</id><created>2025-02-02</created><authors><author><keyname>Ohashi</keyname><forenames>Atsumoto</forenames></author><author><keyname>Higashinaka</keyname><forenames>Ryuichiro</forenames></author></authors><title>Universal Post-Processing Networks for Joint Optimization of Modules in   Task-Oriented Dialogue Systems</title><categories>cs.CL cs.AI</categories><comments>Accepted by AAAI 2025 Main Technical Track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Post-processing networks (PPNs) are components that modify the outputs of arbitrary modules in task-oriented dialogue systems and are optimized using reinforcement learning (RL) to improve the overall task completion capability of the system. However, previous PPN-based approaches have been limited to handling only a subset of modules within a system, which poses a significant limitation in improving the system performance. In this study, we propose a joint optimization method for post-processing the outputs of all modules using universal post-processing networks (UniPPNs), which are language-model-based networks that can modify the outputs of arbitrary modules in a system as a sequence-transformation task. Moreover, our RL algorithm, which employs a module-level Markov decision process, enables fine-grained value and advantage estimation for each module, thereby stabilizing joint learning for post-processing the outputs of all modules. Through both simulation-based and human evaluation experiments using the MultiWOZ dataset, we demonstrated that UniPPN outperforms conventional PPNs in the task completion capability of task-oriented dialogue systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00749</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00749</id><created>2025-02-02</created><authors><author><keyname>Ziegler</keyname><forenames>Andreas</forenames></author><author><keyname>Gossard</keyname><forenames>Thomas</forenames></author><author><keyname>Glover</keyname><forenames>Arren</forenames></author><author><keyname>Zell</keyname><forenames>Andreas</forenames></author></authors><title>An Event-Based Perception Pipeline for a Table Tennis Robot</title><categories>cs.RO cs.CV</categories><comments>This work has been submitted to the IEEE for possible publication</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Table tennis robots gained traction over the last years and have become a popular research challenge for control and perception algorithms. Fast and accurate ball detection is crucial for enabling a robotic arm to rally the ball back successfully. So far, most table tennis robots use conventional, frame-based cameras for the perception pipeline. However, frame-based cameras suffer from motion blur if the frame rate is not high enough for fast-moving objects. Event-based cameras, on the other hand, do not have this drawback since pixels report changes in intensity asynchronously and independently, leading to an event stream with a temporal resolution on the order of us. To the best of our knowledge, we present the first real-time perception pipeline for a table tennis robot that uses only event-based cameras. We show that compared to a frame-based pipeline, event-based perception pipelines have an update rate which is an order of magnitude higher. This is beneficial for the estimation and prediction of the ball's position, velocity, and spin, resulting in lower mean errors and uncertainties. These improvements are an advantage for the robot control, which has to be fast, given the short time a table tennis ball is flying until the robot has to hit back. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00750</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00750</id><created>2025-02-02</created><authors><author><keyname>Tener</keyname><forenames>Felix</forenames></author><author><keyname>Lanir</keyname><forenames>Joel</forenames></author></authors><title>Guiding, not Driving: Design and Evaluation of a Command-Based User   Interface for Teleoperation of Autonomous Vehicles</title><categories>cs.HC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Autonomous vehicles (AVs) are rapidly evolving as an innovative mode of transportation. However, the consensus in both industry and academia is that AVs cannot independently resolve all traffic scenarios. Consequently, the need for remote human assistance becomes clear. To enable the widespread integration of AVs on public roadways, it is imperative to develop novel models for remote operation. One such model is tele-assistance, which promotes delegating low-level maneuvers to automation through high-level directives. Our study investigates the design and evaluation of a new command-based tele-assistance user interface for the teleoperation of AVs. First, by integrating various control paradigms and interaction concepts, we created a simulation-based, high-fidelity interactive prototype consisting of 175 screens. Next, we conducted a comprehensive usability study with 14 expert teleoperators to assess the acceptance and usability of the system. Finally, we formulated high-level insights and guidelines for designing command-based user interfaces for the remote operation of AVs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00752</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00752</id><created>2025-02-02</created><authors><author><keyname>Delvecchio</keyname><forenames>Giovanni Pio</forenames></author><author><keyname>Nguyen</keyname><forenames>Huy Hong</forenames></author><author><keyname>Echizen</keyname><forenames>Isao</forenames></author></authors><title>Zero-Shot Warning Generation for Misinformative Multimodal Content</title><categories>cs.AI cs.CL cs.IR</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The widespread prevalence of misinformation poses significant societal concerns. Out-of-context misinformation, where authentic images are paired with false text, is particularly deceptive and easily misleads audiences. Most existing detection methods primarily evaluate image-text consistency but often lack sufficient explanations, which are essential for effectively debunking misinformation. We present a model that detects multimodal misinformation through cross-modality consistency checks, requiring minimal training time. Additionally, we propose a lightweight model that achieves competitive performance using only one-third of the parameters. We also introduce a dual-purpose zero-shot learning task for generating contextualized warnings, enabling automated debunking and enhancing user comprehension. Qualitative and human evaluations of the generated warnings highlight both the potential and limitations of our approach. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00753</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00753</id><created>2025-02-02</created><authors><author><keyname>Yu</keyname><forenames>Dingzhi</forenames></author><author><keyname>Jiang</keyname><forenames>Wei</forenames></author><author><keyname>Wan</keyname><forenames>Yuanyu</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author></authors><title>Mirror Descent Under Generalized Smoothness</title><categories>math.OC cs.LG</categories><comments>59 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smoothness is crucial for attaining fast rates in first-order optimization. However, many optimization problems in modern machine learning involve non-smooth objectives. Recent studies relax the smoothness assumption by allowing the Lipschitz constant of the gradient to grow with respect to the gradient norm, which accommodates a broad range of objectives in practice. Despite this progress, existing generalizations of smoothness are restricted to Euclidean geometry with $\ell_2$-norm and only have theoretical guarantees for optimization in the Euclidean space. In this paper, we address this limitation by introducing a new $\ell*$-smoothness concept that measures the norm of Hessian in terms of a general norm and its dual, and establish convergence for mirror-descent-type algorithms, matching the rates under the classic smoothness. Notably, we propose a generalized self-bounding property that facilitates bounding the gradients via controlling suboptimality gaps, serving as a principal component for convergence analysis. Beyond deterministic optimization, we establish an anytime convergence for stochastic mirror descent based on a new bounded noise condition that encompasses the widely adopted bounded or affine noise assumptions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00754</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00754</id><created>2025-02-02</created><authors><author><keyname>Zhu</keyname><forenames>Aiqing</forenames></author><author><keyname>Pan</keyname><forenames>Yuting</forenames></author><author><keyname>Li</keyname><forenames>Qianxiao</forenames></author></authors><title>Continuity-Preserving Convolutional Autoencoders for Learning Continuous   Latent Dynamical Models from Images</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous dynamical systems are cornerstones of many scientific and engineering disciplines. While machine learning offers powerful tools to model these systems from trajectory data, challenges arise when these trajectories are captured as images, resulting in pixel-level observations that are discrete in nature. Consequently, a naive application of a convolutional autoencoder can result in latent coordinates that are discontinuous in time. To resolve this, we propose continuity-preserving convolutional autoencoders (CpAEs) to learn continuous latent states and their corresponding continuous latent dynamical models from discrete image frames. We present a mathematical formulation for learning dynamics from image frames, which illustrates issues with previous approaches and motivates our methodology based on promoting the continuity of convolution filters, thereby preserving the continuity of the latent states. This approach enables CpAEs to produce latent states that evolve continuously with the underlying dynamics, leading to more accurate latent dynamical models. Extensive experiments across various scenarios demonstrate the effectiveness of CpAEs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00757</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00757</id><created>2025-02-02</created><authors><author><keyname>Rosser</keyname><forenames>J</forenames></author><author><keyname>Foerster</keyname><forenames>Jakob Nicolaus</forenames></author></authors><title>AgentBreeder: Mitigating the AI Safety Impact of Multi-Agent Scaffolds</title><categories>cs.CR cs.AI cs.NE</categories><msc-class>68T42, 68T50</msc-class><acm-class>I.2.11</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Scaffolding Large Language Models (LLMs) into multi-agent systems often improves performance on complex tasks, but the safety impact of such scaffolds has not been as thoroughly explored. In this paper, we introduce AGENTBREEDER a framework for multi-objective evolutionary search over scaffolds. Our REDAGENTBREEDER evolves scaffolds towards jailbreaking the base LLM while achieving high task success, while BLUEAGENTBREEDER instead aims to combine safety with task reward. We evaluate the systems discovered by the different instances of AGENTBREEDER and popular baselines using widely recognized reasoning, mathematics, and safety benchmarks. Our work highlights and mitigates the safety risks due to multi-agent scaffolding. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00758</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00758</id><created>2025-02-02</created><authors><author><keyname>Mangrum</keyname><forenames>Michael</forenames></author><author><keyname>Pemberton</keyname><forenames>Jonathan</forenames></author><author><keyname>Wetherby</keyname><forenames>Benedict</forenames></author><author><keyname>Montague</keyname><forenames>Philip</forenames></author></authors><title>Structural Latency Perturbation in Large Language Models Through   Recursive State Induction</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Computational efficiency has remained a critical consideration in scaling high-capacity language models, with inference latency and resource consumption presenting significant constraints on real-time applications. The study has introduced a structured latency perturbation mechanism that modifies computational pathways through recursive state induction, enabling dynamic suppression of redundant activations while preserving generative fidelity. A formal mathematical framework has been established to describe recursive perturbations, ensuring that modifications remain adaptive rather than statically imposed. Experiments have demonstrated that applying recursive state adjustments reduces inference latency across varying sequence lengths, with longer text generations benefiting from cumulative efficiency improvements. Comparative evaluations against structured pruning and quantization have indicated that latency gains can be achieved without compromising token retention or memory utilization. The analysis of computational overhead has suggested that selectively suppressing redundant activations contributes to improved power efficiency, particularly in scenarios requiring extended text generation. An assessment of linguistic stability has shown that token-level consistency remains largely intact under controlled perturbation thresholds, reinforcing the viability of structural latency modifications as an alternative to weight-centric optimization techniques. The results have supported the hypothesis that recursive state induction offers an effective method for reducing computational complexity without requiring architectural modifications or external augmentation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00760</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00760</id><created>2025-02-02</created><authors><author><keyname>Suhail</keyname><forenames>Pirzada</forenames></author><author><keyname>Sethi</keyname><forenames>Amit</forenames></author></authors><title>Privacy Preserving Properties of Vision Classifiers</title><categories>cs.LG cs.CR cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Vision classifiers are often trained on proprietary datasets containing sensitive information, yet the models themselves are frequently shared openly under the privacy-preserving assumption. Although these models are assumed to protect sensitive information in their training data, the extent to which this assumption holds for different architectures remains unexplored. This assumption is challenged by inversion attacks which attempt to reconstruct training data from model weights, exposing significant privacy vulnerabilities. In this study, we systematically evaluate the privacy-preserving properties of vision classifiers across diverse architectures, including Multi-Layer Perceptrons (MLPs), Convolutional Neural Networks (CNNs), and Vision Transformers (ViTs). Using network inversion-based reconstruction techniques, we assess the extent to which these architectures memorize and reveal training data, quantifying the relative ease of reconstruction across models. Our analysis highlights how architectural differences, such as input representation, feature extraction mechanisms, and weight structures, influence privacy risks. By comparing these architectures, we identify which are more resilient to inversion attacks and examine the trade-offs between model performance and privacy preservation, contributing to the development of secure and privacy-respecting machine learning models for sensitive applications. Our findings provide actionable insights into the design of secure and privacy-aware machine learning systems, emphasizing the importance of evaluating architectural decisions in sensitive applications involving proprietary or personal data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00761</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00761</id><created>2025-02-02</created><authors><author><keyname>Xu</keyname><forenames>Liangyu</forenames></author><author><keyname>Zhang</keyname><forenames>Xuemiao</forenames></author><author><keyname>Duan</keyname><forenames>Feiyu</forenames></author><author><keyname>Wang</keyname><forenames>Sirui</forenames></author><author><keyname>Wang</keyname><forenames>Jingang</forenames></author><author><keyname>Cai</keyname><forenames>Xunliang</forenames></author></authors><title>FIRE: Flexible Integration of Data Quality Ratings for Effective   Pre-Training</title><categories>cs.CL</categories><comments>19 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Selecting high-quality data can significantly improve the pre-training efficiency of large language models (LLMs). Existing methods often rely on heuristic techniques and single quality signals, limiting their ability to comprehensively evaluate data quality. In this work, we propose FIRE, a flexible and scalable framework for integrating multiple data quality raters, which allows for a comprehensive assessment of data quality across various dimensions. FIRE aligns multiple quality signals into a unified space, and integrates diverse data quality raters to provide a comprehensive quality signal for each data point. Further, we introduce a progressive data selection scheme based on FIRE that iteratively refines the selection of high-quality data points, balancing computational complexity with the refinement of orthogonality. Experiments on the SlimPajama dataset reveal that FIRE consistently outperforms other selection methods and significantly enhances the pre-trained model across a wide range of downstream tasks, with a 2.9\% average performance boost and reducing the FLOPs necessary to achieve a certain performance level by more than half. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00762</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00762</id><created>2025-02-02</created><authors><author><keyname>Moshtaghpour</keyname><forenames>Amirafshar</forenames></author><author><keyname>Kirkland</keyname><forenames>Angus I.</forenames></author></authors><title>On Overlap Ratio in Defocused Electron Ptychography</title><categories>eess.SP cs.IR physics.app-ph physics.med-ph</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Four-dimensional Scanning Transmission Electron Microscopy (4D STEM) with data acquired using a defocused electron probe is a promising tool for characterising complex biological specimens and materials through a phase retrieval process known as Electron Ptychography (EP). The efficacy of 4D STEM acquisition and the resulting quality of EP reconstruction depends on the overlap ratio of adjacent illuminated areas. This paper demonstrates how the overlap ratio impacts the data redundancy and the quality of the EP reconstruction. We define two quantities as a function of the overlap ratio that are independent of both the object and the EP algorithm. Subsequently, we evaluate an EP algorithm for varying overlap ratios using simulated 4D STEM datasets. Notably, a 40% or greater overlap ratio yields stable, high-quality reconstructions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00763</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00763</id><created>2025-02-02</created><authors><author><keyname>Sheshadri</keyname><forenames>Srividya</forenames></author><author><keyname>Radhakrishnan</keyname><forenames>Unnikrishnan</forenames></author><author><keyname>Padmavilochanan</keyname><forenames>Aswathi</forenames></author><author><keyname>Coley</keyname><forenames>Christopher</forenames></author><author><keyname>Bhavani</keyname><forenames>Rao R.</forenames></author></authors><title>Generative AI for Analyzing Participatory Rural Appraisal Data: An   Exploratory Case Study in Gender Research</title><categories>cs.CY</categories><comments>6 pages, 2 figures, International Conference on Gender and Technology   2025</comments><acm-class>J.4; K.4; I.4</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This study explores the novel application of Generative Artificial Intelligence (GenAI) in analyzing unstructured visual data generated through Participatory Rural Appraisal (PRA), specifically focusing on women's empowerment research in rural communities. Using the "Ideal Village" PRA activity as a case study, we evaluate three state-of-the-art Large Language Models (LLMs) - GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro - in their ability to interpret hand-drawn artifacts containing multilingual content from various Indian states. Through comparative analysis, we assess the models' performance across critical dimensions including visual interpretation, language translation, and data classification. Our findings reveal significant challenges in AI's current capabilities to process such unstructured data, particularly in handling multilingual content, maintaining contextual accuracy, and avoiding hallucinations. While the models showed promise in basic visual interpretation, they struggled with nuanced cultural contexts and consistent classification of empowerment-related elements. This study contributes to both AI and gender research by highlighting the potential and limitations of AI in analyzing participatory research data, while emphasizing the need for human oversight and improved contextual understanding. Our findings suggest future directions for developing more inclusive AI models that can better serve community-based participatory research, particularly in gender studies and rural development contexts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00765</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00765</id><created>2025-02-02</created><authors><author><keyname>Li</keyname><forenames>Jiate</forenames></author><author><keyname>Wang</keyname><forenames>Binghui</forenames></author></authors><title>AGNNCert: Defending Graph Neural Networks against Arbitrary   Perturbations with Deterministic Certification</title><categories>cs.CR</categories><comments>Accepted by Usenix Security 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Graph neural networks (GNNs) achieve the state-of-the-art on graph-relevant tasks such as node and graph classification. However, recent works show GNNs are vulnerable to adversarial perturbations include the perturbation on edges, nodes, and node features, the three components forming a graph. Empirical defenses against such attacks are soon broken by adaptive ones. While certified defenses offer robustness guarantees, they face several limitations: 1) almost all restrict the adversary's capability to only one type of perturbation, which is impractical; 2) all are designed for a particular GNN task, which limits their applicability; and 3) the robustness guarantees of all methods except one are not 100% accurate.   We address all these limitations by developing AGNNCert, the first certified defense for GNNs against arbitrary (edge, node, and node feature) perturbations with deterministic robustness guarantees, and applicable to the two most common node and graph classification tasks. AGNNCert also encompass existing certified defenses as special cases. Extensive evaluations on multiple benchmark node/graph classification datasets and two real-world graph datasets, and multiple GNNs validate the effectiveness of AGNNCert to provably defend against arbitrary perturbations. AGNNCert also shows its superiority over the state-of-the-art certified defenses against the individual edge perturbation and node perturbation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00767</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00767</id><created>2025-02-02</created><authors><author><keyname>Li</keyname><forenames>Xiayang</forenames></author><author><keyname>Zhang</keyname><forenames>Shihua</forenames></author></authors><title>Learning-Based TSP-Solvers Tend to Be Overly Greedy</title><categories>cs.LG cs.AI cs.DS</categories><comments>19 pages, 6 figures</comments><msc-class>90C27</msc-class><acm-class>I.2.0; I.2.6</acm-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Deep learning has shown significant potential in solving combinatorial optimization problems such as the Euclidean traveling salesman problem (TSP). However, most training and test instances for existing TSP algorithms are generated randomly from specific distributions like uniform distribution. This has led to a lack of analysis and understanding of the performance of deep learning algorithms in out-of-distribution (OOD) generalization scenarios, which has a close relationship with the worst-case performance in the combinatorial optimization field. For data-driven algorithms, the statistical properties of randomly generated datasets are critical. This study constructs a statistical measure called nearest-neighbor density to verify the asymptotic properties of randomly generated datasets and reveal the greedy behavior of learning-based solvers, i.e., always choosing the nearest neighbor nodes to construct the solution path. Based on this statistical measure, we develop interpretable data augmentation methods that rely on distribution shifts or instance perturbations and validate that the performance of the learning-based solvers degenerates much on such augmented data. Moreover, fine-tuning learning-based solvers with augmented data further enhances their generalization abilities. In short, we decipher the limitations of learning-based TSP solvers tending to be overly greedy, which may have profound implications for AI-empowered combinatorial optimization solvers. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00775</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00775</id><created>2025-02-02</created><authors><author><keyname>Maranjyan</keyname><forenames>Artavazd</forenames></author><author><keyname>Saad</keyname><forenames>El Mehdi</forenames></author><author><keyname>Richtárik</keyname><forenames>Peter</forenames></author><author><keyname>Orabona</keyname><forenames>Francesco</forenames></author></authors><title>ATA: Adaptive Task Allocation for Efficient Resource Management in   Distributed Machine Learning</title><categories>cs.LG cs.DC math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Asynchronous methods are fundamental for parallelizing computations in distributed machine learning. They aim to accelerate training by fully utilizing all available resources. However, their greedy approach can lead to inefficiencies using more computation than required, especially when computation times vary across devices. If the computation times were known in advance, training could be fast and resource-efficient by assigning more tasks to faster workers. The challenge lies in achieving this optimal allocation without prior knowledge of the computation time distributions. In this paper, we propose ATA (Adaptive Task Allocation), a method that adapts to heterogeneous and random distributions of worker computation times. Through rigorous theoretical analysis, we show that ATA identifies the optimal task allocation and performs comparably to methods with prior knowledge of computation times. Experimental results further demonstrate that ATA is resource-efficient, significantly reducing costs compared to the greedy approach, which can be arbitrarily expensive depending on the number of workers. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00778</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00778</id><created>2025-02-02</created><authors><author><keyname>Nishikawa</keyname><forenames>Hiroaki</forenames></author></authors><title>An Efficient Implementation of Edge-Based Discretization without Forming   Dual Control Volumes</title><categories>math.NA cs.NA math.DG physics.comp-ph</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper shows that lumped directed-area vectors at edges and dual control volumes required to implement the edge-based discretization can be computed without explicitly defining the dual control volume around each node for triangular and tetrahedral grids. It is a simpler implementation because there is no need to form a dual control volume by connecting edge-midpoints, face centroids, and element centroids, and also reduces the time for computing lumped directed-area vectors for a given grid, especially for tetrahedral grids. The speed-up achieved by the proposed algorithm may not be large enough to greatly impact the overall simulation time, but the proposed algorithm is expected to serve as a major stepping stone towards extending the edge-based discretization to four dimensions and beyond (e.g., space-time simulations). Efficient algorithms for computing lumped directed-area vectors and dual volumes without forming dual volumes are presented, and their implementations are described and compared with traditional algorithms in terms of complexity as well as actual computing time for a given grid. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00779</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00779</id><created>2025-02-02</created><authors><author><keyname>Jeon</keyname><forenames>Eun Som</forenames></author><author><keyname>Choi</keyname><forenames>Hongjun</forenames></author><author><keyname>Buman</keyname><forenames>Matthew P.</forenames></author><author><keyname>Turaga</keyname><forenames>Pavan</forenames></author></authors><title>Role of Mixup in Topological Persistence Based Knowledge Distillation   for Wearable Sensor Data</title><categories>cs.LG cs.AI eess.SP</categories><comments>IEEE Sensors Journal (2024)</comments><doi>10.1109/JSEN.2024.3517653</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The analysis of wearable sensor data has enabled many successes in several applications. To represent the high-sampling rate time-series with sufficient detail, the use of topological data analysis (TDA) has been considered, and it is found that TDA can complement other time-series features. Nonetheless, due to the large time consumption and high computational resource requirements of extracting topological features through TDA, it is difficult to deploy topological knowledge in various applications. To tackle this problem, knowledge distillation (KD) can be adopted, which is a technique facilitating model compression and transfer learning to generate a smaller model by transferring knowledge from a larger network. By leveraging multiple teachers in KD, both time-series and topological features can be transferred, and finally, a superior student using only time-series data is distilled. On the other hand, mixup has been popularly used as a robust data augmentation technique to enhance model performance during training. Mixup and KD employ similar learning strategies. In KD, the student model learns from the smoothed distribution generated by the teacher model, while mixup creates smoothed labels by blending two labels. Hence, this common smoothness serves as the connecting link that establishes a connection between these two methods. In this paper, we analyze the role of mixup in KD with time-series as well as topological persistence, employing multiple teachers. We present a comprehensive analysis of various methods in KD and mixup on wearable sensor data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00780</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00780</id><created>2025-02-02</created><authors><author><keyname>Babuc</keyname><forenames>Diogen</forenames></author></authors><title>Constructing Fundamentals for the Theory of Proportions and Symbolic   Allusions Applied Interdisciplinarily</title><categories>cs.IT math.IT q-bio.NC</categories><comments>7 pages, 1 figure, interdisciplinarity</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Theory of Proportions and Symbolic Allusions applied Interdisciplinary (TPASAI) is a framework that integrates mathematics, linguistics, psychology, and game theory to uncover hidden patterns and proportions in reality. Its central idea is that numerical encoding of symbols, dates, and language can reveal recurring structures and connections that reflect universal principles. By applying fractal analysis, the theory identifies patterns across different scales, offering a unifying perspective on the structure of the world. One key aspect of TPASAI is symbolic analysis, which allows for the reinterpretation of traumatic experiences in psychotherapy. For example, assigning numerical values to elements like fingers, dates, or words can help individuals uncover meaningful associations between personal experiences and collective symbols. This approach encourages cognitive flexibility and provides a therapeutic avenue for recontextualizing emotions. The theory also incorporates principles of game theory, which frame reality as a system of symbolic "codes" governed by rules that can be understood and strategically used. This perspective is especially useful for psychological conditions like obsessive-compulsive disorder (OCD), enabling patients to approach their obsessions as decipherable patterns rather than rigid constraints. TPASAI has practical applications in psychology, education, and technology. In education, it aids in teaching mathematical and linguistic concepts by exploring connections between symbolic representations and real-world events. In technology, the methodology can be employed in ciphering and natural language processing. The innovation of TPASAI lies in its ability to merge the structured rigor of mathematics with the interpretative flexibility of symbolic analysis, offering a deeper understanding of events and relationships. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00782</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00782</id><created>2025-02-02</created><authors><author><keyname>Wang</keyname><forenames>Yizheng</forenames></author><author><keyname>Bai</keyname><forenames>Jinshuai</forenames></author><author><keyname>Eshaghi</keyname><forenames>Mohammad Sadegh</forenames></author><author><keyname>Anitescu</keyname><forenames>Cosmin</forenames></author><author><keyname>Zhuang</keyname><forenames>Xiaoying</forenames></author><author><keyname>Rabczuk</keyname><forenames>Timon</forenames></author><author><keyname>Liu</keyname><forenames>Yinghua</forenames></author></authors><title>Transfer Learning in Physics-Informed Neural Networks: Full Fine-Tuning,   Lightweight Fine-Tuning, and Low-Rank Adaptation</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  AI for PDEs has garnered significant attention, particularly Physics-Informed Neural Networks (PINNs). However, PINNs are typically limited to solving specific problems, and any changes in problem conditions necessitate retraining. Therefore, we explore the generalization capability of transfer learning in the strong and energy form of PINNs across different boundary conditions, materials, and geometries. The transfer learning methods we employ include full finetuning, lightweight finetuning, and Low-Rank Adaptation (LoRA). The results demonstrate that full finetuning and LoRA can significantly improve convergence speed while providing a slight enhancement in accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00783</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00783</id><created>2025-02-02</created><authors><author><keyname>Yu</keyname><forenames>Zhenyu</forenames></author><author><keyname>Wang</keyname><forenames>Jinnian</forenames></author></authors><title>A method for estimating forest carbon storage distribution density via   artificial intelligence generated content model</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forest is the most significant land-based carbon storage mechanism. The forest carbon sink can effectively decrease the atmospheric CO2 concentration and mitigate climate change. Remote sensing estimation not only ensures high accuracy of data, but also enables large-scale area observation. Optical images provide the possibility for long-term monitoring, which is a potential issue in the future carbon storage estimation research. We chose Huize County, Qujing City, Yunnan Province, China as the study area, took GF-1 WFV satellite image as the data, introduced the KD-VGG module to extract the initial features, and proposed the improved implicit diffusion model (IIDM). The results showed that: (1) The VGG-19 module after knowledge distillation can realize the initial feature extraction, reduce the inference time and improve the accuracy in the case of reducing the number of model parameters. (2) The Attention + MLP module was added for feature fusion to obtain the relationship between global and local features and realized the restoration of high-fidelity images in the continuous scale range. (3) The IIDM model proposed in this paper had the highest estimation accuracy, with RMSE of 28.68, which was 13.16 higher than that of the regression model, about 31.45%. In the estimation of carbon storage, the generative model can extract deeper features, and its performance was significantly better than other models. It demonstrated the feasibility of artificial intelligence-generated content (AIGC) in the field of quantitative remote sensing and provided valuable insights for the study of carbon neutralization effect. By combining the actual characteristics of the forest, the regional carbon storage estimation with a resolution of 16-meter was utilized to provide a significant theoretical basis for the formulation of forest carbon sink regulation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00784</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00784</id><created>2025-02-02</created><authors><author><keyname>Yu</keyname><forenames>Zhenyu</forenames></author><author><keyname>Wang</keyname><forenames>Jinnian</forenames></author></authors><title>Estimating forest carbon stocks from high-resolution remote sensing   imagery by reducing domain shift with style transfer</title><categories>cs.CV eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forests function as crucial carbon reservoirs on land, and their carbon sinks can efficiently reduce atmospheric CO2 concentrations and mitigate climate change. Currently, the overall trend for monitoring and assessing forest carbon stocks is to integrate ground monitoring sample data with satellite remote sensing imagery. This style of analysis facilitates large-scale observation. However, these techniques require improvement in accuracy. We used GF-1 WFV and Landsat TM images to analyze Huize County, Qujing City, Yunnan Province in China. Using the style transfer method, we introduced Swin Transformer to extract global features through attention mechanisms, converting the carbon stock estimation into an image translation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00785</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00785</id><created>2025-02-02</created><authors><author><keyname>AlShekh</keyname><forenames>Raghad H.</forenames></author><author><keyname>Ali</keyname><forenames>Qutaiba I.</forenames></author></authors><title>Forecasting Global Network Traffic Trends: The Role of Virtual Reality</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Virtual Reality (VR) technology demands real-time data transmission to deliver an immersive and interactive user experience. This study investigates the implementation of UDP Ethernet communication in VR systems, focusing on its impact on network performance. Experiments were conducted to analyze how factors such as cable length, data rate, and packet processing rate (PPR) influence system performance. A series of tests were performed, and the results were visualized through detailed graphs. The findings reveal how variations in these parameters affect communication speed and stability, providing insights for optimizing VR system design. By leveraging the high-speed, low-overhead advantages of UDP Ethernet, this study may contribute understanding of network performance in VR applications, offering practical guidance for developers and engineers in creating responsive and efficient VR environments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00787</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00787</id><created>2025-02-02</created><authors><author><keyname>Malallah</keyname><forenames>Omar M</forenames></author><author><keyname>Ali</keyname><forenames>Qutaiba I.</forenames></author></authors><title>Mathematical Modeling for Network Upgrades in Internet Service Provider   Infrastructure</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The ongoing growth of the need for superior Internet services creates great pressure on the ISPs as to the accurate estimation of network upgrade need. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00789</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00789</id><created>2025-02-02</created><authors><author><keyname>Ali</keyname><forenames>Amer T.</forenames></author><author><keyname>Ali</keyname><forenames>Qutaiba I.</forenames></author></authors><title>Improving SDN Performance Using Network Coding: A Quantitative Analysis</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Software Defined Networking or SDN is an architectural approach to managing the network where the control and forwarding are different planes that are controlled through an application interface. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00791</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00791</id><created>2025-02-02</created><authors><author><keyname>Xing</keyname><forenames>Ling</forenames></author><author><keyname>Wang</keyname><forenames>Alex Jinpeng</forenames></author><author><keyname>Yan</keyname><forenames>Rui</forenames></author><author><keyname>Tang</keyname><forenames>Jinhui</forenames></author></authors><title>Vision-centric Token Compression in Large Language Model</title><categories>cs.CL cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large Language Models (LLMs) have revolutionized natural language processing, excelling in handling longer sequences. However, the inefficiency and redundancy in processing extended in-context tokens remain a challenge. Many attempts to address this rely on compressing tokens with smaller text encoders, yet we question whether text encoders are truly indispensable. Our journey leads to an unexpected discovery-a much smaller vision encoder, applied directly to sequences of text tokens, can rival text encoders on text tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small text understanding benchmarks, VIST leads to comparable results with 16% fewer FLOPs and 50% less memory usage. We further uncover significant token redundancy and devise a frequency-based masking strategy to guide the focus of the visual encoder toward the most critical tokens. Interestingly, we observe the trained visual encoder performs like a summarizer, selectively ignoring less important words such as prepositions and conjunctions. This approach delivers remarkable results, outperforming traditional text encoder-based methods by 5.7% on average over benchmarks like TriviaQA, NQ, PopQA, TREF, SST2, and SST5, setting a new standard for token efficiency in LLMs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00792</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00792</id><created>2025-02-02</created><authors><author><keyname>Cai</keyname><forenames>Leng</forenames></author><author><keyname>He</keyname><forenames>Junxuan</forenames></author><author><keyname>Li</keyname><forenames>Yikai</forenames></author><author><keyname>Liang</keyname><forenames>Junjie</forenames></author><author><keyname>Lin</keyname><forenames>Yuanping</forenames></author><author><keyname>Quan</keyname><forenames>Ziming</forenames></author><author><keyname>Zeng</keyname><forenames>Yawen</forenames></author><author><keyname>Xu</keyname><forenames>Jin</forenames></author></authors><title>RTBAgent: A LLM-based Agent System for Real-Time Bidding</title><categories>cs.AI</categories><comments>Accepted by WWW 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-Time Bidding (RTB) enables advertisers to place competitive bids on impression opportunities instantaneously, striving for cost-effectiveness in a highly competitive landscape. Although RTB has widely benefited from the utilization of technologies such as deep learning and reinforcement learning, the reliability of related methods often encounters challenges due to the discrepancies between online and offline environments and the rapid fluctuations of online bidding. To handle these challenges, RTBAgent is proposed as the first RTB agent system based on large language models (LLMs), which synchronizes real competitive advertising bidding environments and obtains bidding prices through an integrated decision-making process. Specifically, obtaining reasoning ability through LLMs, RTBAgent is further tailored to be more professional for RTB via involved auxiliary modules, i.e., click-through rate estimation model, expert strategy knowledge, and daily reflection. In addition, we propose a two-step decision-making process and multi-memory retrieval mechanism, which enables RTBAgent to review historical decisions and transaction records and subsequently make decisions more adaptive to market changes in real-time bidding. Empirical testing with real advertising datasets demonstrates that RTBAgent significantly enhances profitability. The RTBAgent code will be publicly accessible at: https://github.com/CaiLeng/RTBAgent. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00795</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00795</id><created>2025-02-02</created><authors><author><keyname>Feng</keyname><forenames>Wingho</forenames></author><author><keyname>Li</keyname><forenames>Quanwang</forenames></author><author><keyname>Wang</keyname><forenames>Chen</forenames></author><author><keyname>Fan</keyname><forenames>Jian-sheng</forenames></author></authors><title>Data Fusion for Full-Range Response Reconstruction via Diffusion Models</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurately capturing the full-range response of structures is crucial in structural health monitoring (SHM) for ensuring safety and operational integrity. However, limited sensor deployment due to cost, accessibility, or scale often hinders comprehensive monitoring. This paper presents a novel data fusion framework utilizing diffusion models to reconstruct the full-range structural response from sparse and heterogeneous sensor measurements. We incorporate Diffusion Posterior Sampling (DPS) into the reconstruction framework, using sensor measurements as probabilistic constraints to guide the sampling process. A lightweight neural network serves as the surrogate forward model within the DPS algorithm, which maps full-range structural responses to local sensor data. This approach enables flexibility in sensor configurations while reducing computational costs. The proposed framework is validated on a steel plate shear wall exhibiting nonlinear responses. Comparative experiments are conducted with three forward models. Among these, the neural network surrogate model achieves a desirable reconstruction accuracy, with a weighted mean absolute percentage error (WMAPE) as low as 1.57%, while also demonstrating superior adaptability and computational efficiency. Additional experiments explore the impact of sensor placement strategies and noise levels. Results show that even under sparse measurements or high noise conditions, the WMAPE remains capped at 15%, demonstrating the robustness in challenging scenarios. The proposed framework shows new possibilities for probabilistic modeling and decision-making in SHM, offering a novel data fusion approach for full-range monitoring of structures. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00796</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00796</id><created>2025-02-02</created><authors><author><keyname>Levy</keyname><forenames>Matan</forenames></author><author><keyname>Ben-Ari</keyname><forenames>Rami</forenames></author><author><keyname>Samuel</keyname><forenames>Dvir</forenames></author><author><keyname>Darshan</keyname><forenames>Nir</forenames></author><author><keyname>Lischinski</keyname><forenames>Dani</forenames></author></authors><title>Task-Specific Adaptation with Restricted Model Access</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The emergence of foundational models has greatly improved performance across various downstream tasks, with fine-tuning often yielding even better results. However, existing fine-tuning approaches typically require access to model weights and layers, leading to challenges such as managing multiple model copies or inference pipelines, inefficiencies in edge device optimization, and concerns over proprietary rights, privacy, and exposure to unsafe model variants. In this paper, we address these challenges by exploring "Gray-box" fine-tuning approaches, where the model's architecture and weights remain hidden, allowing only gradient propagation. We introduce a novel yet simple and effective framework that adapts to new tasks using two lightweight learnable modules at the model's input and output. Additionally, we present a less restrictive variant that offers more entry points into the model, balancing performance with model exposure. We evaluate our approaches across several backbones on benchmarks such as text-image alignment, text-video alignment, and sketch-image alignment. Results show that our Gray-box approaches are competitive with full-access fine-tuning methods, despite having limited access to the model. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00798</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00798</id><created>2025-02-02</created><authors><author><keyname>Gu</keyname><forenames>Qiangqiang</forenames></author><author><keyname>Pandey</keyname><forenames>Shishir Kumar</forenames></author></authors><title>Deep Neural Network for Phonon-Assisted Optical Spectra in   Semiconductors</title><categories>cond-mat.mtrl-sci cs.LG</categories><comments>5 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Phonon-assisted optical absorption in semiconductors is crucial for understanding and optimizing optoelectronic devices, yet its accurate simulation remains a significant challenge in computational materials science. We present an efficient approach that combines deep learning tight-binding (TB) and potential models to efficiently calculate the phonon-assisted optical absorption in semiconductors with $ab$ $initio$ accuracy. Our strategy enables efficient sampling of atomic configurations through molecular dynamics and rapid computation of electronic structure and optical properties from the TB models. We demonstrate its efficacy by calculating the temperature-dependent optical absorption spectra and band gap renormalization of Si and GaAs due to electron-phonon coupling over a temperature range of 100-400 K. Our results show excellent agreement with experimental data, capturing both indirect and direct absorption processes, including subtle features like the Urbach tail. This approach offers a powerful tool for studying complex materials with high accuracy and efficiency, paving the way for high-throughput screening of optoelectronic materials. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00800</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00800</id><created>2025-02-02</created><authors><author><keyname>Yang</keyname><forenames>Mengping</forenames></author><author><keyname>Wang</keyname><forenames>Zhe</forenames></author><author><keyname>Chi</keyname><forenames>Ziqiu</forenames></author><author><keyname>Li</keyname><forenames>Dongdong</forenames></author><author><keyname>Du</keyname><forenames>Wenli</forenames></author></authors><title>Adversarial Semantic Augmentation for Training Generative Adversarial   Networks under Limited Data</title><categories>cs.CV eess.IV</categories><comments>This work was completed in 2022 and submitted to an IEEE journal for   potential publication</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Generative adversarial networks (GANs) have made remarkable achievements in synthesizing images in recent years. Typically, training GANs requires massive data, and the performance of GANs deteriorates significantly when training data is limited. To improve the synthesis performance of GANs in low-data regimes, existing approaches use various data augmentation techniques to enlarge the training sets. However, it is identified that these augmentation techniques may leak or even alter the data distribution. To remedy this, we propose an adversarial semantic augmentation (ASA) technique to enlarge the training data at the semantic level instead of the image level. Concretely, considering semantic features usually encode informative information of images, we estimate the covariance matrices of semantic features for both real and generated images to find meaningful transformation directions. Such directions translate original features to another semantic representation, e.g., changing the backgrounds or expressions of the human face dataset. Moreover, we derive an upper bound of the expected adversarial loss. By optimizing the upper bound, our semantic augmentation is implicitly achieved. Such design avoids redundant sampling of the augmented features and introduces negligible computation overhead, making our approach computation efficient. Extensive experiments on both few-shot and large-scale datasets demonstrate that our method consistently improve the synthesis quality under various data regimes, and further visualized and analytic results suggesting satisfactory versatility of our proposed method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00801</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00801</id><created>2025-02-02</created><authors><author><keyname>Huang</keyname><forenames>Zhiwei</forenames></author><author><keyname>Li</keyname><forenames>Jiaqi</forenames></author><author><keyname>Zhong</keyname><forenames>Ping</forenames></author><author><keyname>Fan</keyname><forenames>Rui</forenames></author></authors><title>Environment-Driven Online LiDAR-Camera Extrinsic Calibration</title><categories>cs.CV cs.AI cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LiDAR-camera extrinsic calibration (LCEC) is the core for data fusion in computer vision. Existing methods typically rely on customized calibration targets or fixed scene types, lacking the flexibility to handle variations in sensor data and environmental contexts. This paper introduces EdO-LCEC, the first environment-driven, online calibration approach that achieves human-like adaptability. Inspired by the human perceptual system, EdO-LCEC incorporates a generalizable scene discriminator to actively interpret environmental conditions, creating multiple virtual cameras that capture detailed spatial and textural information. To overcome cross-modal feature matching challenges between LiDAR and camera, we propose dual-path correspondence matching (DPCM), which leverages both structural and textural consistency to achieve reliable 3D-2D correspondences. Our approach formulates the calibration process as a spatial-temporal joint optimization problem, utilizing global constraints from multiple views and scenes to improve accuracy, particularly in sparse or partially overlapping sensor views. Extensive experiments on real-world datasets demonstrate that EdO-LCEC achieves state-of-the-art performance, providing reliable and precise calibration across diverse, challenging environments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00802</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00802</id><created>2025-02-02</created><authors><author><keyname>Falzari</keyname><forenames>Massimiliano</forenames></author><author><keyname>Sabatelli</keyname><forenames>Matthia</forenames></author></authors><title>Fisher-Guided Selective Forgetting: Mitigating The Primacy Bias in Deep   Reinforcement Learning</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deep Reinforcement Learning (DRL) systems often tend to overfit to early experiences, a phenomenon known as the primacy bias (PB). This bias can severely hinder learning efficiency and final performance, particularly in complex environments. This paper presents a comprehensive investigation of PB through the lens of the Fisher Information Matrix (FIM). We develop a framework characterizing PB through distinct patterns in the FIM trace, identifying critical memorization and reorganization phases during learning. Building on this understanding, we propose Fisher-Guided Selective Forgetting (FGSF), a novel method that leverages the geometric structure of the parameter space to selectively modify network weights, preventing early experiences from dominating the learning process. Empirical results across DeepMind Control Suite (DMC) environments show that FGSF consistently outperforms baselines, particularly in complex tasks. We analyze the different impacts of PB on actor and critic networks, the role of replay ratios in exacerbating the effect, and the effectiveness of even simple noise injection methods. Our findings provide a deeper understanding of PB and practical mitigation strategies, offering a FIM-based geometric perspective for advancing DRL. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00803</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00803</id><created>2025-02-02</created><authors><author><keyname>Wu</keyname><forenames>Haixu</forenames></author><author><keyname>Ma</keyname><forenames>Yuezhou</forenames></author><author><keyname>Zhou</keyname><forenames>Hang</forenames></author><author><keyname>Weng</keyname><forenames>Huikun</forenames></author><author><keyname>Wang</keyname><forenames>Jianmin</forenames></author><author><keyname>Long</keyname><forenames>Mingsheng</forenames></author></authors><title>ProPINN: Demystifying Propagation Failures in Physics-Informed Neural   Networks</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Physics-informed neural networks (PINNs) have earned high expectations in solving partial differential equations (PDEs), but their optimization usually faces thorny challenges due to the unique derivative-dependent loss function. By analyzing the loss distribution, previous research observed the propagation failure phenomenon of PINNs, intuitively described as the correct supervision for model outputs cannot ``propagate'' from initial states or boundaries to the interior domain. Going beyond intuitive understanding, this paper provides the first formal and in-depth study of propagation failure and its root cause. Based on a detailed comparison with classical finite element methods, we ascribe the failure to the conventional single-point-processing architecture of PINNs and further prove that propagation failure is essentially caused by the lower gradient correlation of PINN models on nearby collocation points. Compared to superficial loss maps, this new perspective provides a more precise quantitative criterion to identify where and why PINN fails. The theoretical finding also inspires us to present a new PINN architecture, named ProPINN, which can effectively unite the gradient of region points for better propagation. ProPINN can reliably resolve PINN failure modes and significantly surpass advanced Transformer-based models with 46% relative promotion. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00806</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00806</id><created>2025-02-02</created><authors><author><keyname>He</keyname><forenames>Yufei</forenames></author><author><keyname>Sui</keyname><forenames>Yuan</forenames></author><author><keyname>He</keyname><forenames>Xiaoxin</forenames></author><author><keyname>Liu</keyname><forenames>Yue</forenames></author><author><keyname>Sun</keyname><forenames>Yifei</forenames></author><author><keyname>Hooi</keyname><forenames>Bryan</forenames></author></authors><title>UniGraph2: Learning a Unified Embedding Space to Bind Multimodal Graphs</title><categories>cs.LG</categories><comments>WWW 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Existing foundation models, such as CLIP, aim to learn a unified embedding space for multimodal data, enabling a wide range of downstream web-based applications like search, recommendation, and content classification. However, these models often overlook the inherent graph structures in multimodal datasets, where entities and their relationships are crucial. Multimodal graphs (MMGs) represent such graphs where each node is associated with features from different modalities, while the edges capture the relationships between these entities. On the other hand, existing graph foundation models primarily focus on text-attributed graphs (TAGs) and are not designed to handle the complexities of MMGs. To address these limitations, we propose UniGraph2, a novel cross-domain graph foundation model that enables general representation learning on MMGs, providing a unified embedding space. UniGraph2 employs modality-specific encoders alongside a graph neural network (GNN) to learn a unified low-dimensional embedding space that captures both the multimodal information and the underlying graph structure. We propose a new cross-domain multi-graph pre-training algorithm at scale to ensure effective transfer learning across diverse graph domains and modalities. Additionally, we adopt a Mixture of Experts (MoE) component to align features from different domains and modalities, ensuring coherent and robust embeddings that unify the information across modalities. Extensive experiments on a variety of multimodal graph tasks demonstrate that UniGraph2 significantly outperforms state-of-the-art models in tasks such as representation learning, transfer learning, and multimodal generative tasks, offering a scalable and flexible solution for learning on MMGs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00808</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00808</id><created>2025-02-02</created><authors><author><keyname>Wu</keyname><forenames>Yixin</forenames></author><author><keyname>Yang</keyname><forenames>Ziqing</forenames></author><author><keyname>Shen</keyname><forenames>Yun</forenames></author><author><keyname>Backes</keyname><forenames>Michael</forenames></author><author><keyname>Zhang</keyname><forenames>Yang</forenames></author></authors><title>Synthetic Artifact Auditing: Tracing LLM-Generated Synthetic Data Usage   in Downstream Applications</title><categories>cs.LG cs.CR cs.CY</categories><comments>To Appear in the 34th USENIX Security Symposium, August 13-15, 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models (LLMs) have facilitated the generation of high-quality, cost-effective synthetic data for developing downstream models and conducting statistical analyses in various domains. However, the increased reliance on synthetic data may pose potential negative impacts. Numerous studies have demonstrated that LLM-generated synthetic data can perpetuate and even amplify societal biases and stereotypes, and produce erroneous outputs known as ``hallucinations'' that deviate from factual knowledge. In this paper, we aim to audit artifacts, such as classifiers, generators, or statistical plots, to identify those trained on or derived from synthetic data and raise user awareness, thereby reducing unexpected consequences and risks in downstream applications. To this end, we take the first step to introduce synthetic artifact auditing to assess whether a given artifact is derived from LLM-generated synthetic data. We then propose an auditing framework with three methods including metric-based auditing, tuning-based auditing, and classification-based auditing. These methods operate without requiring the artifact owner to disclose proprietary training details. We evaluate our auditing framework on three text classification tasks, two text summarization tasks, and two data visualization tasks across three training scenarios. Our evaluation demonstrates the effectiveness of all proposed auditing methods across all these tasks. For instance, black-box metric-based auditing can achieve an average accuracy of $0.868 \pm 0.071$ for auditing classifiers and $0.880 \pm 0.052$ for auditing generators using only 200 random queries across three scenarios. We hope our research will enhance model transparency and regulatory compliance, ensuring the ethical and responsible use of synthetic data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00814</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00814</id><created>2025-02-02</created><authors><author><keyname>Cai</keyname><forenames>Jianfeng</forenames></author><author><keyname>Zhu</keyname><forenames>Jinhua</forenames></author><author><keyname>Sun</keyname><forenames>Ruopei</forenames></author><author><keyname>Wang</keyname><forenames>Yue</forenames></author><author><keyname>Li</keyname><forenames>Li</forenames></author><author><keyname>Zhou</keyname><forenames>Wengang</forenames></author><author><keyname>Li</keyname><forenames>Houqiang</forenames></author></authors><title>Disentangling Length Bias In Preference Learning Via   Response-Conditioned Modeling</title><categories>cs.LG cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Reinforcement Learning from Human Feedback (RLHF) has achieved considerable success in aligning large language models (LLMs) by modeling human preferences with a learnable reward model and employing a reinforcement learning algorithm to maximize the reward model's scores. However, these reward models are susceptible to exploitation through various superficial confounding factors, with length bias emerging as a particularly significant concern. Moreover, while the pronounced impact of length bias on preference modeling suggests that LLMs possess an inherent sensitivity to length perception, our preliminary investigations reveal that fine-tuned LLMs consistently struggle to adhere to explicit length instructions. To address these two limitations, we propose a novel framework wherein the reward model explicitly differentiates between human semantic preferences and response length requirements. Specifically, we introduce a Response-conditioned Bradley-Terry (Rc-BT) model that enhances the reward model's capability in length bias mitigating and length instruction following, through training on our augmented dataset. Furthermore, we propose the Rc-DPO algorithm to leverage the Rc-BT model for direct policy optimization (DPO) of LLMs, simultaneously mitigating length bias and promoting adherence to length instructions. Extensive evaluations demonstrate that our approach substantially improves both preference modeling and length instruction compliance, with its effectiveness validated across various foundational models and preference datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00816</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00816</id><created>2025-02-02</created><authors><author><keyname>Liu</keyname><forenames>Yong</forenames></author><author><keyname>Qin</keyname><forenames>Guo</forenames></author><author><keyname>Shi</keyname><forenames>Zhiyuan</forenames></author><author><keyname>Chen</keyname><forenames>Zhi</forenames></author><author><keyname>Yang</keyname><forenames>Caiyin</forenames></author><author><keyname>Huang</keyname><forenames>Xiangdong</forenames></author><author><keyname>Wang</keyname><forenames>Jianmin</forenames></author><author><keyname>Long</keyname><forenames>Mingsheng</forenames></author></authors><title>Sundial: A Family of Highly Capable Time Series Foundation Models</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We introduce Sundial, a family of native, flexible, and scalable time series foundation models. To predict the next-patch's distribution, we propose a TimeFlow Loss based on flow-matching, which facilitates native pre-training of Transformers on time series without discrete tokenization. Conditioned on arbitrary-length time series, our model is pre-trained without specifying any prior distribution and can generate multiple probable predictions, achieving flexibility in representation learning beyond using parametric densities. Towards time series foundation models, we leverage minimal but crucial adaptations of Transformers and curate TimeBench with 1 trillion time points, comprising mostly real-world datasets and synthetic data. By mitigating mode collapse through TimeFlow Loss, we pre-train a family of Sundial models on TimeBench, which exhibit unprecedented model capacity and generalization performance on zero-shot forecasting. In addition to presenting good scaling behavior, Sundial achieves new state-of-the-art on both point forecasting and probabilistic forecasting benchmarks. We believe that Sundial's pioneering generative paradigm will facilitate a wide variety of forecasting scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00817</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00817</id><created>2025-02-02</created><authors><author><keyname>Lin</keyname><forenames>Zheng-Lin</forenames></author><author><keyname>Shih</keyname><forenames>Yu-Fei</forenames></author><author><keyname>Hsieh</keyname><forenames>Shu-Kai</forenames></author></authors><title>Probing Large Language Models in Reasoning and Translating Complex   Linguistic Puzzles</title><categories>cs.CL</categories><comments>8 pages, 8 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper investigates the utilization of Large Language Models (LLMs) for solving complex linguistic puzzles, a domain requiring advanced reasoning and adept translation capabilities akin to human cognitive processes. We explore specific prompting techniques designed to enhance ability of LLMs to reason and elucidate their decision-making pathways, with a focus on Input-Output Prompting (IO), Chain-of-Thought Prompting (CoT), and Solo Performance Prompting (SPP). Utilizing datasets from the Puzzling Machine Competition and various Linguistics Olympiads, we employ a comprehensive set of metrics to assess the performance of GPT-4 0603, a prominent LLM, across these prompting methods. Our findings illuminate the potential of LLMs in linguistic reasoning and complex translation tasks, highlighting their capabilities and identifying limitations in the context of linguistic puzzles. This research contributes significantly to the broader field of Natural Language Processing (NLP) by providing insights into the optimization of LLM applications for improved reasoning and translation accuracy, thereby enriching the ongoing dialogue in NLP advancements. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00818</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00818</id><created>2025-02-02</created><authors><author><keyname>Wu</keyname><forenames>Junxi</forenames></author><author><keyname>Hu</keyname><forenames>Dongjian</forenames></author><author><keyname>Bao</keyname><forenames>Yajie</forenames></author><author><keyname>Xia</keyname><forenames>Shu-Tao</forenames></author><author><keyname>Zou</keyname><forenames>Changliang</forenames></author></authors><title>Error-quantified Conformal Inference for Time Series</title><categories>stat.ML cs.LG</categories><comments>ICLR 2025 camera version</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Uncertainty quantification in time series prediction is challenging due to the temporal dependence and distribution shift on sequential data. Conformal inference provides a pivotal and flexible instrument for assessing the uncertainty of machine learning models through prediction sets. Recently, a series of online conformal inference methods updated thresholds of prediction sets by performing online gradient descent on a sequence of quantile loss functions. A drawback of such methods is that they only use the information of revealed non-conformity scores via miscoverage indicators but ignore error quantification, namely the distance between the non-conformity score and the current threshold. To accurately leverage the dynamic of miscoverage error, we propose \textit{Error-quantified Conformal Inference} (ECI) by smoothing the quantile loss function. ECI introduces a continuous and adaptive feedback scale with the miscoverage error, rather than simple binary feedback in existing methods. We establish a long-term coverage guarantee for ECI under arbitrary dependence and distribution shift. The extensive experimental results show that ECI can achieve valid miscoverage control and output tighter prediction sets than other baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00820</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00820</id><created>2025-02-02</created><authors><author><keyname>Montazeran</keyname><forenames>Behrooz</forenames></author><author><keyname>Köthe</keyname><forenames>Ullrich</forenames></author></authors><title>OOD Detection with immature Models</title><categories>cs.LG cs.CV</categories><comments>17 pages, 2 Tables, 9 Figures</comments><msc-class>53A45</msc-class><acm-class>I.4.7; I.4.9</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Likelihood-based deep generative models (DGMs) have gained significant attention for their ability to approximate the distributions of high-dimensional data. However, these models lack a performance guarantee in assigning higher likelihood values to in-distribution (ID) inputs, data the models are trained on, compared to out-of-distribution (OOD) inputs. This counter-intuitive behaviour is particularly pronounced when ID inputs are more complex than OOD data points. One potential approach to address this challenge involves leveraging the gradient of a data point with respect to the parameters of the DGMs. A recent OOD detection framework proposed estimating the joint density of layer-wise gradient norms for a given data point as a model-agnostic method, demonstrating superior performance compared to the Typicality Test across likelihood-based DGMs and image dataset pairs. In particular, most existing methods presuppose access to fully converged models, the training of which is both time-intensive and computationally demanding. In this work, we demonstrate that using immature models,stopped at early stages of training, can mostly achieve equivalent or even superior results on this downstream task compared to mature models capable of generating high-quality samples that closely resemble ID data. This novel finding enhances our understanding of how DGMs learn the distribution of ID data and highlights the potential of leveraging partially trained models for downstream tasks. Furthermore, we offer a possible explanation for this unexpected behaviour through the concept of support overlap. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00823</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00823</id><created>2025-02-02</created><authors><author><keyname>Meyer</keyname><forenames>Maxime</forenames></author><author><keyname>Adhikary</keyname><forenames>Soumik</forenames></author><author><keyname>Guo</keyname><forenames>Naixu</forenames></author><author><keyname>Rebentrost</keyname><forenames>Patrick</forenames></author></authors><title>Online Learning of Pure States is as Hard as Mixed States</title><categories>quant-ph cs.LG</categories><comments>21 pages, 5 figures</comments><msc-class>81P18 (Primary) 68T05, 62L10 (Secondary)</msc-class><acm-class>I.2.6</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Quantum state tomography, the task of learning an unknown quantum state, is a fundamental problem in quantum information. In standard settings, the complexity of this problem depends significantly on the type of quantum state that one is trying to learn, with pure states being substantially easier to learn than general mixed states. A natural question is whether this separation holds for any quantum state learning setting. In this work, we consider the online learning framework and prove the surprising result that learning pure states in this setting is as hard as learning mixed states. More specifically, we show that both classes share almost the same sequential fat-shattering dimension, leading to identical regret scaling under the $L_1$-loss. We also generalize previous results on full quantum state tomography in the online setting to learning only partially the density matrix, using smooth analysis. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00826</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00826</id><created>2025-02-02</created><authors><author><keyname>Perry</keyname><forenames>Julian</forenames></author><author><keyname>Sanders</keyname><forenames>Frank</forenames></author><author><keyname>Scott</keyname><forenames>Carter</forenames></author></authors><title>Weak Supervision Dynamic KL-Weighted Diffusion Models Guided by Large   Language Models</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we presents a novel method for improving text-to-image generation by combining Large Language Models (LLMs) with diffusion models, a hybrid approach aimed at achieving both higher quality and efficiency in image synthesis from text descriptions. Our approach introduces a new dynamic KL-weighting strategy to optimize the diffusion process, along with incorporating semantic understanding from pre-trained LLMs to guide the generation process. The proposed method significantly improves both the visual quality and alignment of generated images with text descriptions, addressing challenges such as computational inefficiency, instability in training, and robustness to textual variability. We evaluate our method on the COCO dataset and demonstrate its superior performance over traditional GAN-based models, both quantitatively and qualitatively. Extensive experiments, including ablation studies and human evaluations, confirm that our method outperforms existing approaches in terms of image realism, relevance to the input text, and overall aesthetic quality. Our approach also shows promise in scalability to other multimodal tasks, making it a versatile solution for a wide range of generative applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00828</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00828</id><created>2025-02-02</created><authors><author><keyname>Hwang</keyname><forenames>Yoontae</forenames></author><author><keyname>Kong</keyname><forenames>Yaxuan</forenames></author><author><keyname>Zohren</keyname><forenames>Stefan</forenames></author><author><keyname>Lee</keyname><forenames>Yongjae</forenames></author></authors><title>Decision-informed Neural Networks with Large Language Model Integration   for Portfolio Optimization</title><categories>q-fin.PM cs.AI q-fin.CP</categories><comments>Submitted paper</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper addresses the critical disconnect between prediction and decision quality in portfolio optimization by integrating Large Language Models (LLMs) with decision-focused learning. We demonstrate both theoretically and empirically that minimizing the prediction error alone leads to suboptimal portfolio decisions. We aim to exploit the representational power of LLMs for investment decisions. An attention mechanism processes asset relationships, temporal dependencies, and macro variables, which are then directly integrated into a portfolio optimization layer. This enables the model to capture complex market dynamics and align predictions with the decision objectives. Extensive experiments on S\&amp;P100 and DOW30 datasets show that our model consistently outperforms state-of-the-art deep learning models. In addition, gradient-based analyses show that our model prioritizes the assets most crucial to decision making, thus mitigating the effects of prediction errors on portfolio performance. These findings underscore the value of integrating decision objectives into predictions for more robust and context-aware portfolio management. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00829</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00829</id><created>2025-02-02</created><authors><author><keyname>Wu</keyname><forenames>Xixi</forenames></author><author><keyname>Shen</keyname><forenames>Yifei</forenames></author><author><keyname>Ge</keyname><forenames>Fangzhou</forenames></author><author><keyname>Shan</keyname><forenames>Caihua</forenames></author><author><keyname>Jiao</keyname><forenames>Yizhu</forenames></author><author><keyname>Sun</keyname><forenames>Xiangguo</forenames></author><author><keyname>Cheng</keyname><forenames>Hong</forenames></author></authors><title>A Comprehensive Analysis on LLM-based Node Classification Algorithms</title><categories>cs.LG cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Node classification is a fundamental task in graph analysis, with broad applications across various fields. Recent breakthroughs in Large Language Models (LLMs) have enabled LLM-based approaches for this task. Although many studies demonstrate the impressive performance of LLM-based methods, the lack of clear design guidelines may hinder their practical application. In this work, we aim to establish such guidelines through a fair and systematic comparison of these algorithms. As a first step, we developed LLMNodeBed, a comprehensive codebase and testbed for node classification using LLMs. It includes ten datasets, eight LLM-based algorithms, and three learning paradigms, and is designed for easy extension with new methods and datasets. Subsequently, we conducted extensive experiments, training and evaluating over 2,200 models, to determine the key settings (e.g., learning paradigms and homophily) and components (e.g., model size) that affect performance. Our findings uncover eight insights, e.g., (1) LLM-based methods can significantly outperform traditional methods in a semi-supervised setting, while the advantage is marginal in a supervised setting; (2) Graph Foundation Models can beat open-source LLMs but still fall short of strong LLMs like GPT-4o in a zero-shot setting. We hope that the release of LLMNodeBed, along with our insights, will facilitate reproducible research and inspire future studies in this field. Codes and datasets are released at \href{https://llmnodebed.github.io/}{https://llmnodebed.github.io/}. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00831</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00831</id><created>2025-02-02</created><authors><author><keyname>Scherer</keyname><forenames>Maike</forenames></author><author><keyname>Brand</keyname><forenames>Lukas</forenames></author><author><keyname>Wolf</keyname><forenames>Louis</forenames></author><author><keyname>Dieck</keyname><forenames>Teena tom</forenames></author><author><keyname>Schäfer</keyname><forenames>Maximilian</forenames></author><author><keyname>Lotter</keyname><forenames>Sebastian</forenames></author><author><keyname>Burkovski</keyname><forenames>Andreas</forenames></author><author><keyname>Sticht</keyname><forenames>Heinrich</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author><author><keyname>Castiglione</keyname><forenames>Kathrin</forenames></author></authors><title>Closed-Loop Long-Term Experimental Molecular Communication System</title><categories>cs.ET</categories><comments>30 pages single column, 7 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a fluid-based experimental molecular communication (MC) testbed which uses media modulation. Motivated by the natural human cardiovascular system, the testbed operates in a closed-loop tube system. The proposed system is designed to be biocompatible, resource-efficient, and controllable from outside the tube. As signaling molecule, the testbed employs the green fluorescent protein variant "Dreiklang" (GFPD). GFPDs can be reversibly switched via light of different wavelengths between a bright fluorescent state and a less fluorescent state. GFPDs in solution are filled into the testbed prior to the start of information transmission and remain there for an entire experiment. For information transmission, an optical transmitter (TX) and an optical eraser (EX), which are located outside the tube, are used to write and erase the information encoded in the state of the GFPDs, respectively. At the receiver (RX), the state of the GFPDs is read out by fluorescence detection. In our testbed, due to the closed-loop setup, we observe new forms of inter-symbol interferences (ISI), which do not occur in short experiments and open-loop systems. For the testbed, we developed a communication scheme, which includes blind transmission start detection, symbol-by-symbol synchronization, and adaptive threshold detection. We comprehensively analyze our MC experiments using different performance metrics. Moreover, we experimentally demonstrate the error-free transmission of 5370 bit at a data rate of 36 $\textrm{bit}\, \textrm{min}^{\boldsymbol{-1}}$ using 8-ary modulation and the error-free binary transmission of around 90000 bit at a data rate of 12 $\textrm{bit}\, \textrm{min}^{\boldsymbol{-1}}$. For the latter experiment, data was transmitted for a period of 125 hours. All signals recorded and parts of the evaluation code are publicly available on Zenodo and Github, respectively. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00832</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00832</id><created>2025-02-02</created><authors><author><keyname>Long</keyname><forenames>Robert</forenames></author><author><keyname>Gonzalez</keyname><forenames>Eric</forenames></author><author><keyname>Fuller</keyname><forenames>Harrison</forenames></author></authors><title>Generalization of Medical Large Language Models through Cross-Domain   Weak Supervision</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advancement of large language models (LLMs) has opened new frontiers in natural language processing, particularly in specialized domains like healthcare. In this paper, we propose the Incremental Curriculum-Based Fine-Tuning (ICFT) framework to enhance the generative capabilities of medical large language models (MLLMs). ICFT combines curriculum-based learning, dual-stage memory coordination, and parameter-efficient fine-tuning to enable a progressive transition from general linguistic knowledge to strong domain-specific expertise. Experimental results across diverse medical NLP tasks, including question answering, preference classification, and response generation, demonstrate that ICFT consistently outperforms state-of-the-art baselines, achieving improvements in both accuracy and efficiency. Further analysis reveals the framework's ability to generalize to unseen data, reduce errors, and deliver diverse, contextually relevant medical responses. These findings establish ICFT as a robust and scalable solution for adapting LLMs to the medical domain, offering practical benefits for real-world healthcare applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00833</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00833</id><created>2025-02-02</created><authors><author><keyname>P</keyname><forenames>Akhshan</forenames></author><author><keyname>Sanjay</keyname><forenames>Taneti</forenames></author><author><keyname>S</keyname><forenames>Chandrakala</forenames></author></authors><title>Cross multiscale vision transformer for deep fake detection</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The proliferation of deep fake technology poses significant challenges to digital media authenticity, necessitating robust detection mechanisms. This project evaluates deep fake detection using the SP Cup's 2025 deep fake detection challenge dataset. We focused on exploring various deep learning models for detecting deep fake content, utilizing traditional deep learning techniques alongside newer architectures. Our approach involved training a series of models and rigorously assessing their performance using metrics such as accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00834</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00834</id><created>2025-02-02</created><authors><author><keyname>Hou</keyname><forenames>Zhichao</forenames></author><author><keyname>Gao</keyname><forenames>Weizhi</forenames></author><author><keyname>Krim</keyname><forenames>Hamid</forenames></author><author><keyname>Liu</keyname><forenames>Xiaorui</forenames></author></authors><title>Boosting Adversarial Robustness and Generalization with Structural Prior</title><categories>cs.LG cs.CR cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work investigates a novel approach to boost adversarial robustness and generalization by incorporating structural prior into the design of deep learning models. Specifically, our study surprisingly reveals that existing dictionary learning-inspired convolutional neural networks (CNNs) provide a false sense of security against adversarial attacks. To address this, we propose Elastic Dictionary Learning Networks (EDLNets), a novel ResNet architecture that significantly enhances adversarial robustness and generalization. This novel and effective approach is supported by a theoretical robustness analysis using influence functions. Moreover, extensive and reliable experiments demonstrate consistent and significant performance improvement on open robustness leaderboards such as RobustBench, surpassing state-of-the-art baselines. To the best of our knowledge, this is the first work to discover and validate that structural prior can reliably enhance deep learning robustness under strong adaptive attacks, unveiling a promising direction for future research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00835</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00835</id><created>2025-02-02</created><authors><author><keyname>Yuan</keyname><forenames>Yuanchen</forenames></author><author><keyname>Cheng</keyname><forenames>Jin</forenames></author><author><keyname>Urpí</keyname><forenames>Núria Armengol</forenames></author><author><keyname>Coros</keyname><forenames>Stelian</forenames></author></authors><title>CAIMAN: Causal Action Influence Detection for Sample Efficient   Loco-manipulation</title><categories>cs.RO cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Enabling legged robots to perform non-prehensile loco-manipulation with large and heavy objects is crucial for enhancing their versatility. However, this is a challenging task, often requiring sophisticated planning strategies or extensive task-specific reward shaping, especially in unstructured scenarios with obstacles. In this work, we present CAIMAN, a novel framework for learning loco-manipulation that relies solely on sparse task rewards. We leverage causal action influence to detect states where the robot is in control over other entities in the environment, and use this measure as an intrinsically motivated objective to enable sample-efficient learning. We employ a hierarchical control strategy, combining a low-level locomotion policy with a high-level policy that prioritizes task-relevant velocity commands. Through simulated and real-world experiments, including object manipulation with obstacles, we demonstrate the framework's superior sample efficiency, adaptability to diverse environments, and successful transfer to hardware without fine-tuning. The proposed approach paves the way for scalable, robust, and autonomous loco-manipulation in real-world applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00837</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00837</id><created>2025-02-02</created><authors><author><keyname>Mohammadi</keyname><forenames>Hadi</forenames></author><author><keyname>Bagheri</keyname><forenames>Ayoub</forenames></author><author><keyname>Giachanou</keyname><forenames>Anastasia</forenames></author><author><keyname>Oberski</keyname><forenames>Daniel L.</forenames></author></authors><title>Explainability in Practice: A Survey of Explainable NLP Across Various   Domains</title><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Natural Language Processing (NLP) has become a cornerstone in many critical sectors, including healthcare, finance, and customer relationship management. This is especially true with the development and use of advanced models such as GPT-based architectures and BERT, which are widely used in decision-making processes. However, the black-box nature of these advanced NLP models has created an urgent need for transparency and explainability. This review explores explainable NLP (XNLP) with a focus on its practical deployment and real-world applications, examining its implementation and the challenges faced in domain-specific contexts. The paper underscores the importance of explainability in NLP and provides a comprehensive perspective on how XNLP can be designed to meet the unique demands of various sectors, from healthcare's need for clear insights to finance's emphasis on fraud detection and risk assessment. Additionally, this review aims to bridge the knowledge gap in XNLP literature by offering a domain-specific exploration and discussing underrepresented areas such as real-world applicability, metric evaluation, and the role of human interaction in model assessment. The paper concludes by suggesting future research directions that could enhance the understanding and broader application of XNLP. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00840</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00840</id><created>2025-02-02</created><authors><author><keyname>Zhang</keyname><forenames>Jiawen</forenames></author><author><keyname>Chen</keyname><forenames>Kejia</forenames></author><author><keyname>He</keyname><forenames>Lipeng</forenames></author><author><keyname>Lou</keyname><forenames>Jian</forenames></author><author><keyname>Li</keyname><forenames>Dan</forenames></author><author><keyname>Feng</keyname><forenames>Zunlei</forenames></author><author><keyname>Song</keyname><forenames>Mingli</forenames></author><author><keyname>Liu</keyname><forenames>Jian</forenames></author><author><keyname>Ren</keyname><forenames>Kui</forenames></author><author><keyname>Yang</keyname><forenames>Xiaohu</forenames></author></authors><title>Activation Approximations Can Incur Safety Vulnerabilities Even in   Aligned LLMs: Comprehensive Analysis and Defense</title><categories>cs.CR cs.AI</categories><comments>19 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) have showcased remarkable capabilities across various domains. Accompanying the evolving capabilities and expanding deployment scenarios of LLMs, their deployment challenges escalate due to their sheer scale and the advanced yet complex activation designs prevalent in notable model series, such as Llama, Gemma, and Mistral. These challenges have become particularly pronounced in resource-constrained deployment scenarios, where mitigating inference efficiency bottlenecks is imperative. Among various recent efforts, activation approximation has emerged as a promising avenue for pursuing inference efficiency, sometimes considered indispensable in applications such as private inference. Despite achieving substantial speedups with minimal impact on utility, even appearing sound and practical for real-world deployment, the safety implications of activation approximations remain unclear. In this work, we fill this critical gap in LLM safety by conducting the first systematic safety evaluation of activation approximations. Our safety vetting spans seven sota techniques across three popular categories, revealing consistent safety degradation across ten safety-aligned LLMs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00841</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00841</id><created>2025-02-02</created><authors><author><keyname>Bampis</keyname><forenames>Evripidis</forenames></author><author><keyname>Escoffier</keyname><forenames>Bruno</forenames></author><author><keyname>Fotakis</keyname><forenames>Dimitris</forenames></author><author><keyname>Patsilinakos</keyname><forenames>Panagiotis</forenames></author><author><keyname>Xefteris</keyname><forenames>Michalis</forenames></author></authors><title>Polynomial Time Learning-Augmented Algorithms for NP-hard Permutation   Problems</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider a learning-augmented framework for NP-hard permutation problems. The algorithm has access to predictions telling, given a pair $u,v$ of elements, whether $u$ is before $v$ or not in an optimal solution. Building on the work of Braverman and Mossel (SODA 2008), we show that for a class of optimization problems including scheduling, network design and other graph permutation problems, these predictions allow to solve them in polynomial time with high probability, provided that predictions are true with probability at least $1/2+\epsilon$. Moreover, this can be achieved with a parsimonious access to the predictions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00843</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00843</id><created>2025-02-02</created><authors><author><keyname>Lin</keyname><forenames>Yuxin</forenames></author><author><keyname>Qi</keyname><forenames>Mengshi</forenames></author><author><keyname>Liu</keyname><forenames>Liang</forenames></author><author><keyname>Ma</keyname><forenames>Huadong</forenames></author></authors><title>VLM-Assisted Continual learning for Visual Question Answering in   Self-Driving</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel approach for solving the Visual Question Answering (VQA) task in autonomous driving by integrating Vision-Language Models (VLMs) with continual learning. In autonomous driving, VQA plays a vital role in enabling the system to understand and reason about its surroundings. However, traditional models often struggle with catastrophic forgetting when sequentially exposed to new driving tasks, such as perception, prediction, and planning, each requiring different forms of knowledge. To address this challenge, we present a novel continual learning framework that combines VLMs with selective memory replay and knowledge distillation, reinforced by task-specific projection layer regularization. The knowledge distillation allows a previously trained model to act as a "teacher" to guide the model through subsequent tasks, minimizing forgetting. Meanwhile, task-specific projection layers calculate the loss based on the divergence of feature representations, ensuring continuity in learning and reducing the shift between tasks. Evaluated on the DriveLM dataset, our framework shows substantial performance improvements, with gains ranging from 21.40% to 32.28% across various metrics. These results highlight the effectiveness of combining continual learning with VLMs in enhancing the resilience and reliability of VQA systems in autonomous driving. We will release our source code. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00846</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00846</id><created>2025-02-02</created><authors><author><keyname>Mildner</keyname><forenames>Terje</forenames></author><author><keyname>Hamelijnck</keyname><forenames>Oliver</forenames></author><author><keyname>Giampouras</keyname><forenames>Paris</forenames></author><author><keyname>Damoulas</keyname><forenames>Theodoros</forenames></author></authors><title>Federated Generalised Variational Inference: A Robust Probabilistic   Federated Learning Framework</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce FedGVI, a probabilistic Federated Learning (FL) framework that is provably robust to both prior and likelihood misspecification. FedGVI addresses limitations in both frequentist and Bayesian FL by providing unbiased predictions under model misspecification, with calibrated uncertainty quantification. Our approach generalises previous FL approaches, specifically Partitioned Variational Inference (Ashman et al., 2022), by allowing robust and conjugate updates, decreasing computational complexity at the clients. We offer theoretical analysis in terms of fixed-point convergence, optimality of the cavity distribution, and provable robustness. Additionally, we empirically demonstrate the effectiveness of FedGVI in terms of improved robustness and predictive performance on multiple synthetic and real world classification data sets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00847</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00847</id><created>2025-02-02</created><authors><author><keyname>Zhang</keyname><forenames>Jiawen</forenames></author><author><keyname>Chen</keyname><forenames>Kejia</forenames></author><author><keyname>Feng</keyname><forenames>Zunlei</forenames></author><author><keyname>Lou</keyname><forenames>Jian</forenames></author><author><keyname>Song</keyname><forenames>Mingli</forenames></author><author><keyname>Liu</keyname><forenames>Jian</forenames></author><author><keyname>Yang</keyname><forenames>Xiaohu</forenames></author></authors><title>SecPE: Secure Prompt Ensembling for Private and Robust Large Language   Models</title><categories>cs.CR cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  With the growing popularity of LLMs among the general public users, privacy-preserving and adversarial robustness have become two pressing demands for LLM-based services, which have largely been pursued separately but rarely jointly. In this paper, to the best of our knowledge, we are among the first attempts towards robust and private LLM inference by tightly integrating two disconnected fields: private inference and prompt ensembling. The former protects users' privacy by encrypting inference data transmitted and processed by LLMs, while the latter enhances adversarial robustness by yielding an aggregated output from multiple prompted LLM responses. Although widely recognized as effective individually, private inference for prompt ensembling together entails new challenges that render the naive combination of existing techniques inefficient. To overcome the hurdles, we propose SecPE, which designs efficient fully homomorphic encryption (FHE) counterparts for the core algorithmic building blocks of prompt ensembling. We conduct extensive experiments on 8 tasks to evaluate the accuracy, robustness, and efficiency of SecPE. The results show that SecPE maintains high clean accuracy and offers better robustness at the expense of merely $2.5\%$ efficiency overhead compared to baseline private inference methods, indicating a satisfactory ``accuracy-robustness-efficiency'' tradeoff. For the efficiency of the encrypted Argmax operation that incurs major slowdown for prompt ensembling, SecPE is 35.4x faster than the state-of-the-art peers, which can be of independent interest beyond this work. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00848</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00848</id><created>2025-02-02</created><authors><author><keyname>Lyu</keyname><forenames>Yuanhuiyi</forenames></author><author><keyname>Zheng</keyname><forenames>Xu</forenames></author><author><keyname>Jiang</keyname><forenames>Lutao</forenames></author><author><keyname>Yan</keyname><forenames>Yibo</forenames></author><author><keyname>Zou</keyname><forenames>Xin</forenames></author><author><keyname>Zhou</keyname><forenames>Huiyu</forenames></author><author><keyname>Zhang</keyname><forenames>Linfeng</forenames></author><author><keyname>Hu</keyname><forenames>Xuming</forenames></author></authors><title>RealRAG: Retrieval-augmented Realistic Image Generation via   Self-reflective Contrastive Learning</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent text-to-image generative models, e.g., Stable Diffusion V3 and Flux, have achieved notable progress. However, these models are strongly restricted to their limited knowledge, a.k.a., their own fixed parameters, that are trained with closed datasets. This leads to significant hallucinations or distortions when facing fine-grained and unseen novel real-world objects, e.g., the appearance of the Tesla Cybertruck. To this end, we present the first real-object-based retrieval-augmented generation framework (RealRAG), which augments fine-grained and unseen novel object generation by learning and retrieving real-world images to overcome the knowledge gaps of generative models. Specifically, to integrate missing memory for unseen novel object generation, we train a reflective retriever by self-reflective contrastive learning, which injects the generator's knowledge into the sef-reflective negatives, ensuring that the retrieved augmented images compensate for the model's missing knowledge. Furthermore, the real-object-based framework integrates fine-grained visual knowledge for the generative models, tackling the distortion problem and improving the realism for fine-grained object generation. Our Real-RAG is superior in its modular application to all types of state-of-the-art text-to-image generative models and also delivers remarkable performance boosts with all of them, such as a gain of 16.18% FID score with the auto-regressive model on the Stanford Car benchmark. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00850</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00850</id><created>2025-02-02</created><authors><author><keyname>Zhou</keyname><forenames>Chi</forenames></author><author><keyname>Luo</keyname><forenames>Wang</forenames></author><author><keyname>Li</keyname><forenames>Haoran</forenames></author><author><keyname>Han</keyname><forenames>Congying</forenames></author><author><keyname>Guo</keyname><forenames>Tiande</forenames></author><author><keyname>Zhang</keyname><forenames>Zicheng</forenames></author></authors><title>Dual Alignment Maximin Optimization for Offline Model-based RL</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Offline reinforcement learning agents face significant deployment challenges due to the synthetic-to-real distribution mismatch. While most prior research has focused on improving the fidelity of synthetic sampling and incorporating off-policy mechanisms, the directly integrated paradigm often fails to ensure consistent policy behavior in biased models and underlying environmental dynamics, which inherently arise from discrepancies between behavior and learning policies. In this paper, we first shift the focus from model reliability to policy discrepancies while optimizing for expected returns, and then self-consistently incorporate synthetic data, deriving a novel actor-critic paradigm, Dual Alignment Maximin Optimization (DAMO). It is a unified framework to ensure both model-environment policy consistency and synthetic and offline data compatibility. The inner minimization performs dual conservative value estimation, aligning policies and trajectories to avoid out-of-distribution states and actions, while the outer maximization ensures that policy improvements remain consistent with inner value estimates. Empirical evaluations demonstrate that DAMO effectively ensures model and policy alignments, achieving competitive performance across diverse benchmark tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00853</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00853</id><created>2025-02-02</created><authors><author><keyname>Tong</keyname><forenames>Wai</forenames></author><author><keyname>Li</keyname><forenames>Haobo</forenames></author><author><keyname>Xia</keyname><forenames>Meng</forenames></author><author><keyname>Kam-Kwai</keyname><forenames>Wong</forenames></author><author><keyname>Pong</keyname><forenames>Ting-Chuen</forenames></author><author><keyname>Qu</keyname><forenames>Huamin</forenames></author><author><keyname>Yang</keyname><forenames>Yalong</forenames></author></authors><title>Exploring Spatial Hybrid User Interface for Visual Sensemaking</title><categories>cs.HC</categories><comments>Accepted by IEEE TVCG</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We built a spatial hybrid system that combines a personal computer (PC) and virtual reality (VR) for visual sensemaking, addressing limitations in both environments. Although VR offers immense potential for interactive data visualization (e.g., large display space and spatial navigation), it can also present challenges such as imprecise interactions and user fatigue. At the same time, a PC offers precise and familiar interactions but has limited display space and interaction modality. Therefore, we iteratively designed a spatial hybrid system (PC+VR) to complement these two environments by enabling seamless switching between PC and VR environments. To evaluate the system's effectiveness and user experience, we compared it to using a single computing environment (i.e., PC-only and VR-only). Our study results (N=18) showed that spatial PC+VR could combine the benefits of both devices to outperform user preference for VR-only without a negative impact on performance from device switching overhead. Finally, we discussed future design implications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00855</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00855</id><created>2025-02-02</created><authors><author><keyname>Zhang</keyname><forenames>Jianyu</forenames></author><author><keyname>Zhao</keyname><forenames>Yongwang</forenames></author><author><keyname>Zhang</keyname><forenames>Long</forenames></author><author><keyname>Hu</keyname><forenames>Jilin</forenames></author><author><keyname>Luan</keyname><forenames>Xiaokun</forenames></author><author><keyname>Xu</keyname><forenames>Zhiwei</forenames></author><author><keyname>Yang</keyname><forenames>Feng</forenames></author></authors><title>Psychometric-Based Evaluation for Theorem Proving with Large Language   Models</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large language models (LLMs) for formal theorem proving have become a prominent research focus. At present, the proving ability of these LLMs is mainly evaluated through proof pass rates on datasets such as miniF2F. However, this evaluation method overlooks the varying importance of theorems. As a result, it fails to highlight the real performance disparities between LLMs and leads to high evaluation costs. This study proposes a psychometric-based evaluation method for theorem proving with LLMs, comprising two main components: Dataset Annotation and Adaptive Evaluation. First, we propose a metric calculation method to annotate the dataset with difficulty and discrimination metrics. Specifically, we annotate each theorem in the miniF2F dataset and grade them into varying difficulty levels according to the performance of LLMs, resulting in an enhanced dataset: miniF2F-Graded. Experimental results show that the difficulty grading in miniF2F-Graded better reflects the theorem difficulty perceived by LLMs. Secondly, we design an adaptive evaluation method to dynamically select the most suitable theorems for testing based on the annotated metrics and the real-time performance of LLMs. We apply this method to evaluate 10 LLMs. The results show that our method finely highlights the performance disparities between LLMs. It also reduces evaluation costs by using only 23% of the theorems in the dataset. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00857</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00857</id><created>2025-02-02</created><authors><author><keyname>Mozafari</keyname><forenames>Jamshid</forenames></author><author><keyname>Piryani</keyname><forenames>Bhawna</forenames></author><author><keyname>Abdallah</keyname><forenames>Abdelrahman</forenames></author><author><keyname>Jatowt</keyname><forenames>Adam</forenames></author></authors><title>HintEval: A Comprehensive Framework for Hint Generation and Evaluation   for Questions</title><categories>cs.CL cs.IR</categories><comments>Submitted to SIGIR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) are transforming how people find information, and many users turn nowadays to chatbots to obtain answers to their questions. Despite the instant access to abundant information that LLMs offer, it is still important to promote critical thinking and problem-solving skills. Automatic hint generation is a new task that aims to support humans in answering questions by themselves by creating hints that guide users toward answers without directly revealing them. In this context, hint evaluation focuses on measuring the quality of hints, helping to improve the hint generation approaches. However, resources for hint research are currently spanning different formats and datasets, while the evaluation tools are missing or incompatible, making it hard for researchers to compare and test their models. To overcome these challenges, we introduce HintEval, a Python library that makes it easy to access diverse datasets and provides multiple approaches to generate and evaluate hints. HintEval aggregates the scattered resources into a single toolkit that supports a range of research goals and enables a clear, multi-faceted, and reliable evaluation. The proposed library also includes detailed online documentation, helping users quickly explore its features and get started. By reducing barriers to entry and encouraging consistent evaluation practices, HintEval offers a major step forward for facilitating hint generation and analysis research within the NLP/IR community. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00858</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00858</id><created>2025-02-02</created><authors><author><keyname>Xu</keyname><forenames>Manjie</forenames></author><author><keyname>Yang</keyname><forenames>Xinyi</forenames></author><author><keyname>Liang</keyname><forenames>Wei</forenames></author><author><keyname>Zhang</keyname><forenames>Chi</forenames></author><author><keyname>Zhu</keyname><forenames>Yixin</forenames></author></authors><title>Learning to Plan with Personalized Preferences</title><categories>cs.AI cs.HC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Effective integration of AI agents into daily life requires them to understand and adapt to individual human preferences, particularly in collaborative roles. Although recent studies on embodied intelligence have advanced significantly, they typically adopt generalized approaches that overlook personal preferences in planning. We address this limitation by developing agents that not only learn preferences from few demonstrations but also learn to adapt their planning strategies based on these preferences. Our research leverages the observation that preferences, though implicitly expressed through minimal demonstrations, can generalize across diverse planning scenarios. To systematically evaluate this hypothesis, we introduce Preference-based Planning (PbP) benchmark, an embodied benchmark featuring hundreds of diverse preferences spanning from atomic actions to complex sequences. Our evaluation of SOTA methods reveals that while symbol-based approaches show promise in scalability, significant challenges remain in learning to generate and execute plans that satisfy personalized preferences. We further demonstrate that incorporating learned preferences as intermediate representations in planning significantly improves the agent's ability to construct personalized plans. These findings establish preferences as a valuable abstraction layer for adaptive planning, opening new directions for research in preference-guided plan generation and execution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00859</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00859</id><created>2025-02-02</created><authors><author><keyname>Huang</keyname><forenames>Yongqiang</forenames></author><author><keyname>Shao</keyname><forenames>Zerui</forenames></author><author><keyname>Yang</keyname><forenames>Ziyuan</forenames></author><author><keyname>Lu</keyname><forenames>Zexin</forenames></author><author><keyname>Zhang</keyname><forenames>Yi</forenames></author></authors><title>FedRIR: Rethinking Information Representation in Federated Learning</title><categories>cs.LG cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile and Web-of-Things (WoT) devices at the network edge generate vast amounts of data for machine learning applications, yet privacy concerns hinder centralized model training. Federated Learning (FL) allows clients (devices) to collaboratively train a shared model coordinated by a central server without transfer private data, but inherent statistical heterogeneity among clients presents challenges, often leading to a dilemma between clients' needs for personalized local models and the server's goal of building a generalized global model. Existing FL methods typically prioritize either global generalization or local personalization, resulting in a trade-off between these two objectives and limiting the full potential of diverse client data. To address this challenge, we propose a novel framework that simultaneously enhances global generalization and local personalization by Rethinking Information Representation in the Federated learning process (FedRIR). Specifically, we introduce Masked Client-Specific Learning (MCSL), which isolates and extracts fine-grained client-specific features tailored to each client's unique data characteristics, thereby enhancing personalization. Concurrently, the Information Distillation Module (IDM) refines the global shared features by filtering out redundant client-specific information, resulting in a purer and more robust global representation that enhances generalization. By integrating the refined global features with the isolated client-specific features, we construct enriched representations that effectively capture both global patterns and local nuances, thereby improving the performance of downstream tasks on the client. The code is available at https://github.com/Deep-Imaging-Group/FedRIR. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00861</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00861</id><created>2025-02-02</created><authors><author><keyname>Silva</keyname><forenames>Paulo Cesar Souza</forenames></author><author><keyname>Pellanda</keyname><forenames>Paulo Cesar</forenames></author><author><keyname>Oliveira</keyname><forenames>Tiago Roux</forenames></author></authors><title>Multivariable Stochastic Newton-Based Extremum Seeking with Delays</title><categories>math.OC cs.SY eess.SY</categories><comments>28 pages, 13 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents a Newton-based stochastic extremum-seeking control method for real-time optimization in multi-input systems with distinct input delays. It combines predictor-based feedback and Hessian inverse estimation via stochastic perturbations to enable delay compensation with user-defined convergence rates. The method ensures exponential stability and convergence near the unknown extremum, even under long delays. It extends to multi-input, single-output systems with cross-coupled channels. Stability is analyzed using backstepping and infinite-dimensional averaging. Numerical simulations demonstrate its effectiveness in handling time-delayed channels, showcasing both the challenges and benefits of real-time optimization in distributed parameter settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00865</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00865</id><created>2025-02-02</created><authors><author><keyname>Loeffler</keyname><forenames>Christoffer</forenames></author><author><keyname>Freile</keyname><forenames>Andrea Martínez</forenames></author><author><keyname>Pizarro</keyname><forenames>Tomás Rey</forenames></author></authors><title>Predicting potentially unfair clauses in Chilean terms of services with   natural language processing</title><categories>cs.CL cs.AI cs.CY cs.LG</categories><comments>37 pages, 2 figures, under review</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This study addresses the growing concern of information asymmetry in consumer contracts, exacerbated by the proliferation of online services with complex Terms of Service that are rarely even read. Even though research on automatic analysis methods is conducted, the problem is aggravated by the general focus on English-language Machine Learning approaches and on major jurisdictions, such as the European Union. We introduce a new methodology and a substantial dataset addressing this gap. We propose a novel annotation scheme with four categories and a total of 20 classes, and apply it on 50 online Terms of Service used in Chile. Our evaluation of transformer-based models highlights how factors like language- and/or domain-specific pre-training, few-shot sample size, and model architecture affect the detection and classification of potentially abusive clauses. Results show a large variability in performance for the different tasks and models, with the highest macro-F1 scores for the detection task ranging from 79% to 89% and micro-F1 scores up to 96%, while macro-F1 scores for the classification task range from 60% to 70% and micro-F1 scores from 64% to 80%. Notably, this is the first Spanish-language multi-label classification dataset for legal clauses, applying Chilean law and offering a comprehensive evaluation of Spanish-language models in the legal domain. Our work lays the ground for future research in method development for rarely considered legal analysis and potentially leads to practical applications to support consumers in Chile and Latin America as a whole. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00869</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00869</id><created>2025-02-02</created><authors><author><keyname>Morsali</keyname><forenames>Alireza</forenames></author><author><keyname>Vaez</keyname><forenames>MohammadJavad</forenames></author><author><keyname>Soltani</keyname><forenames>Hossein</forenames></author><author><keyname>Kazerouni</keyname><forenames>Amirhossein</forenames></author><author><keyname>Taati</keyname><forenames>Babak</forenames></author><author><keyname>Mohammad-Noori</keyname><forenames>Morteza</forenames></author></authors><title>STAF: Sinusoidal Trainable Activation Functions for Implicit Neural   Representation</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Implicit Neural Representations (INRs) have emerged as a powerful framework for modeling continuous signals. The spectral bias of ReLU-based networks is a well-established limitation, restricting their ability to capture fine-grained details in target signals. While previous works have attempted to mitigate this issue through frequency-based encodings or architectural modifications, these approaches often introduce additional complexity and do not fully address the underlying challenge of learning high-frequency components efficiently. We introduce Sinusoidal Trainable Activation Functions (STAF), designed to directly tackle this limitation by enabling networks to adaptively learn and represent complex signals with higher precision and efficiency. STAF inherently modulates its frequency components, allowing for self-adaptive spectral learning. This capability significantly improves convergence speed and expressivity, making STAF highly effective for both signal representations and inverse problems. Through extensive evaluations, we demonstrate that STAF outperforms state-of-the-art (SOTA) methods in accuracy and reconstruction fidelity with superior Peak Signal-to-Noise Ratio (PSNR). These results establish STAF as a robust solution for overcoming spectral bias and the capacity-convergence gap, making it valuable for computer graphics and related fields. Our codebase is publicly accessible on the https://github.com/AlirezaMorsali/STAF. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00870</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00870</id><created>2025-02-02</created><authors><author><keyname>Jiang</keyname><forenames>Wenzheng</forenames></author><author><keyname>Wang</keyname><forenames>Ji</forenames></author><author><keyname>Zhang</keyname><forenames>Xiongtao</forenames></author><author><keyname>Bao</keyname><forenames>Weidong</forenames></author><author><keyname>Tan</keyname><forenames>Cheston</forenames></author><author><keyname>Fan</keyname><forenames>Flint Xiaofeng</forenames></author></authors><title>FedHPD: Heterogeneous Federated Reinforcement Learning via Policy   Distillation</title><categories>cs.LG cs.AI cs.MA</categories><comments>This preprint presents the full version of the Extended Abstract   accepted by AAMAS 2025, including all the proofs and experiments</comments><acm-class>I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Federated Reinforcement Learning (FedRL) improves sample efficiency while preserving privacy; however, most existing studies assume homogeneous agents, limiting its applicability in real-world scenarios. This paper investigates FedRL in black-box settings with heterogeneous agents, where each agent employs distinct policy networks and training configurations without disclosing their internal details. Knowledge Distillation (KD) is a promising method for facilitating knowledge sharing among heterogeneous models, but it faces challenges related to the scarcity of public datasets and limitations in knowledge representation when applied to FedRL. To address these challenges, we propose Federated Heterogeneous Policy Distillation (FedHPD), which solves the problem of heterogeneous FedRL by utilizing action probability distributions as a medium for knowledge sharing. We provide a theoretical analysis of FedHPD's convergence under standard assumptions. Extensive experiments corroborate that FedHPD shows significant improvements across various reinforcement learning benchmark tasks, further validating our theoretical findings. Moreover, additional experiments demonstrate that FedHPD operates effectively without the need for an elaborate selection of public datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00871</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00871</id><created>2025-02-02</created><authors><author><keyname>Sieradzki</keyname><forenames>Szymon</forenames></author><author><keyname>Mańdziuk</keyname><forenames>Jacek</forenames></author></authors><title>Modified Adaptive Tree-Structured Parzen Estimator for Hyperparameter   Optimization</title><categories>cs.LG</categories><comments>21 pages, 10 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we review hyperparameter optimization methods for machine learning models, with a particular focus on the Adaptive Tree-Structured Parzen Estimator (ATPE) algorithm. We propose several modifications to ATPE and assess their efficacy on a diverse set of standard benchmark functions. Experimental results demonstrate that the proposed modifications significantly improve the effectiveness of ATPE hyperparameter optimization on selected benchmarks, a finding that holds practical relevance for their application in real-world machine learning / optimization tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00872</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00872</id><created>2025-02-02</created><authors><author><keyname>Dwary</keyname><forenames>Tithi</forenames></author><author><keyname>Mozhui</keyname><forenames>Khyodeno</forenames></author><author><keyname>Krishna</keyname><forenames>K. V.</forenames></author></authors><title>Representation Number of Word-Representable Split Graphs</title><categories>math.CO cs.DM</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A split graph is a graph whose vertex set can be partitioned into a clique and an independent set. The word-representability of split graphs was studied in a series of papers in the literature, and the class of word-representable split graphs was characterized through semi-transitive orientation. Nonetheless, the representation number of this class of graphs is still not known. In general, determining the representation number of a word-representable graph is an NP-complete problem. In this work, through an algorithmic procedure, we show that the representation number of the class of word-representable split graphs is at most three. Further, we characterize the class of word-representable split graphs as well as the class of split comparability graphs which have representation number exactly three. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00873</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00873</id><created>2025-02-02</created><authors><author><keyname>Kantamneni</keyname><forenames>Subhash</forenames></author><author><keyname>Tegmark</keyname><forenames>Max</forenames></author></authors><title>Language Models Use Trigonometry to Do Addition</title><categories>cs.AI cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Mathematical reasoning is an increasingly important indicator of large language model (LLM) capabilities, yet we lack understanding of how LLMs process even simple mathematical tasks. To address this, we reverse engineer how three mid-sized LLMs compute addition. We first discover that numbers are represented in these LLMs as a generalized helix, which is strongly causally implicated for the tasks of addition and subtraction, and is also causally relevant for integer division, multiplication, and modular arithmetic. We then propose that LLMs compute addition by manipulating this generalized helix using the "Clock" algorithm: to solve $a+b$, the helices for $a$ and $b$ are manipulated to produce the $a+b$ answer helix which is then read out to model logits. We model influential MLP outputs, attention head outputs, and even individual neuron preactivations with these helices and verify our understanding with causal interventions. By demonstrating that LLMs represent numbers on a helix and manipulate this helix to perform addition, we present the first representation-level explanation of an LLM's mathematical capability. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00874</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00874</id><created>2025-02-02</created><authors><author><keyname>Yang</keyname><forenames>Jing</forenames></author></authors><title>Paper Copilot: The Artificial Intelligence and Machine Learning   Community Should Adopt a More Transparent and Regulated Peer Review Process</title><categories>cs.DL cs.AI cs.CV cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid growth of submissions to top-tier Artificial Intelligence (AI) and Machine Learning (ML) conferences has prompted many venues to transition from closed to open review platforms. Some have fully embraced open peer reviews, allowing public visibility throughout the process, while others adopt hybrid approaches, such as releasing reviews only after final decisions or keeping reviews private despite using open peer review systems. In this work, we analyze the strengths and limitations of these models, highlighting the growing community interest in transparent peer review. To support this discussion, we examine insights from Paper Copilot, a website launched two years ago to aggregate and analyze AI / ML conference data while engaging a global audience. The site has attracted over 200,000 early-career researchers, particularly those aged 18-34 from 177 countries, many of whom are actively engaged in the peer review process. Drawing on our findings, this position paper advocates for a more transparent, open, and well-regulated peer review aiming to foster greater community involvement and propel advancements in the field. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00879</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00879</id><created>2025-02-02</created><authors><author><keyname>Rmus</keyname><forenames>Milena</forenames></author><author><keyname>Jagadish</keyname><forenames>Akshay K.</forenames></author><author><keyname>Mathony</keyname><forenames>Marvin</forenames></author><author><keyname>Ludwig</keyname><forenames>Tobias</forenames></author><author><keyname>Schulz</keyname><forenames>Eric</forenames></author></authors><title>Towards Automation of Cognitive Modeling using Large Language Models</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Computational cognitive models, which formalize theories of cognition, enable researchers to quantify cognitive processes and arbitrate between competing theories by fitting models to behavioral data. Traditionally, these models are handcrafted, which requires significant domain knowledge, coding expertise, and time investment. Previous work has demonstrated that Large Language Models (LLMs) are adept at pattern recognition in-context, solving complex problems, and generating executable code. In this work, we leverage these abilities to explore the potential of LLMs in automating the generation of cognitive models based on behavioral data. We evaluated the LLM in two different tasks: model identification (relating data to a source model), and model generation (generating the underlying cognitive model). We performed these tasks across two cognitive domains - decision making and learning. In the case of data simulated from canonical cognitive models, we found that the LLM successfully identified and generated the ground truth model. In the case of human data, where behavioral noise and lack of knowledge of the true underlying process pose significant challenges, the LLM generated models that are identical or close to the winning model from cognitive science literature. Our findings suggest that LLMs can have a transformative impact on cognitive modeling. With this project, we aim to contribute to an ongoing effort of automating scientific discovery in cognitive science. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00880</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00880</id><created>2025-02-02</created><authors><author><keyname>Giovannelli</keyname><forenames>Alexander</forenames></author><author><keyname>Pavanatto</keyname><forenames>Leonardo</forenames></author><author><keyname>Davari</keyname><forenames>Shakiba</forenames></author><author><keyname>Miao</keyname><forenames>Haichao</forenames></author><author><keyname>Chheang</keyname><forenames>Vuthea</forenames></author><author><keyname>Giera</keyname><forenames>Brian</forenames></author><author><keyname>Bremer</keyname><forenames>Peer-Timo</forenames></author><author><keyname>Bowman</keyname><forenames>Doug</forenames></author></authors><title>Investigating the Influence of Playback Interactivity during Guided   Tours for Asynchronous Collaboration in Virtual Reality</title><categories>cs.HC</categories><comments>11 pages, 4 figures, 2 tables, to be published in 2025 IEEE   conference on virtual reality and 3D user interfaces (VR)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative virtual environments allow workers to contribute to team projects across space and time. While much research has closely examined the problem of working in different spaces at the same time, few have investigated the best practices for collaborating in those spaces at different times aside from textual and auditory annotations. We designed a system that allows experts to record a tour inside a virtual inspection space, preserving knowledge and providing later observers with insights through a 3D playback of the expert's inspection. We also created several interactions to ensure that observers are tracking the tour and remaining engaged. We conducted a user study to evaluate the influence of these interactions on an observing user's information recall and user experience. Findings indicate that independent viewpoint control during a tour enhances the user experience compared to fully passive playback and that additional interactivity can improve auditory and spatial recall of key information conveyed during the tour. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00881</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00881</id><created>2025-02-02</created><authors><author><keyname>Fok</keyname><forenames>Raymond</forenames></author><author><keyname>Siu</keyname><forenames>Alexa</forenames></author><author><keyname>Weld</keyname><forenames>Daniel S.</forenames></author></authors><title>Toward Living Narrative Reviews: An Empirical Study of the Processes and   Challenges in Updating Survey Articles in Computing Research</title><categories>cs.HC cs.DL</categories><comments>10 pages; To appear at CHI 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Surveying prior literature to establish a foundation for new knowledge is essential for scholarly progress. However, survey articles are resource-intensive and challenging to create, and can quickly become outdated as new research is published, risking information staleness and inaccuracy. Keeping survey articles current with the latest evidence is therefore desirable, though there is a limited understanding of why, when, and how these surveys should be updated. Toward this end, through a series of in-depth retrospective interviews with 11 researchers, we present an empirical examination of the work practices in authoring and updating survey articles in computing research. We find that while computing researchers acknowledge the value in maintaining an updated survey, continuous updating remains unmanageable and misaligned with academic incentives. Our findings suggest key leverage points within current workflows that present opportunities for enabling technologies to facilitate more efficient and effective updates. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00882</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00882</id><created>2025-02-02</created><authors><author><keyname>Goldshlager</keyname><forenames>Gil</forenames></author><author><keyname>Hu</keyname><forenames>Jiang</forenames></author><author><keyname>Lin</keyname><forenames>Lin</forenames></author></authors><title>Worth Their Weight: Randomized and Regularized Block Kaczmarz Algorithms   without Preprocessing</title><categories>cs.LG cs.NA math.NA math.OC stat.ML</categories><comments>25 pages, 8 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Due to the ever growing amounts of data leveraged for machine learning and scientific computing, it is increasingly important to develop algorithms that sample only a small portion of the data at a time. In the case of linear least-squares, the randomized block Kaczmarz method (RBK) is an appealing example of such an algorithm, but its convergence is only understood under sampling distributions that require potentially prohibitively expensive preprocessing steps. To address this limitation, we analyze RBK when the data is sampled uniformly, showing that its iterates converge in a Monte Carlo sense to a $\textit{weighted}$ least-squares solution. Unfortunately, for general problems the condition number of the weight matrix and the variance of the iterates can become arbitrarily large. We resolve these issues by incorporating regularization into the RBK iterations. Numerical experiments, including examples arising from natural gradient optimization, suggest that the regularized algorithm, ReBlocK, outperforms minibatch stochastic gradient descent for realistic problems that exhibit fast singular value decay. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00883</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00883</id><created>2025-02-02</created><authors><author><keyname>Xiao</keyname><forenames>Teng</forenames></author><author><keyname>Yuan</keyname><forenames>Yige</forenames></author><author><keyname>Chen</keyname><forenames>Zhengyu</forenames></author><author><keyname>Li</keyname><forenames>Mingxiao</forenames></author><author><keyname>Liang</keyname><forenames>Shangsong</forenames></author><author><keyname>Ren</keyname><forenames>Zhaochun</forenames></author><author><keyname>Honavar</keyname><forenames>Vasant G</forenames></author></authors><title>SimPER: A Minimalist Approach to Preference Alignment without   Hyperparameters</title><categories>cs.LG cs.CL</categories><comments>ICLR 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing preference optimization objectives for language model alignment require additional hyperparameters that must be extensively tuned to achieve optimal performance, increasing both the complexity and time required for fine-tuning large language models. In this paper, we propose a simple yet effective hyperparameter-free preference optimization algorithm for alignment.We observe that promising performance can be achieved simply by optimizing inverse perplexity, which is calculated as the inverse of the exponentiated average log-likelihood of the chosen and rejected responses in the preference dataset. The resulting simple learning objective, SimPER, is easy to implement and eliminates the need for expensive hyperparameter tuning and a reference model, making it both computationally and memory efficient. Extensive experiments on widely used real-world benchmarks, including MT-Bench, AlpacaEval 2, and 10 key benchmarks of the Open LLM Leaderboard with 5 base models, demonstrate that SimPER consistently and significantly outperforms existing approaches-even without any hyperparameters or a reference model . For example, despite its simplicity, SimPER outperforms state-of-the-art methods by up to 5.7 points on AlpacaEval 2 and achieves the highest average ranking across 10 benchmarks on the Open LLM Leaderboard. The source code for SimPER is publicly available at: https://github.com/tengxiao1/SimPER. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00885</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00885</id><created>2025-02-02</created><authors><author><keyname>Dang</keyname><forenames>Thanh</forenames></author><author><keyname>Barsbey</keyname><forenames>Melih</forenames></author><author><keyname>Sonet</keyname><forenames>A K M Rokonuzzaman</forenames></author><author><keyname>Gurbuzbalaban</keyname><forenames>Mert</forenames></author><author><keyname>Simsekli</keyname><forenames>Umut</forenames></author><author><keyname>Zhu</keyname><forenames>Lingjiong</forenames></author></authors><title>Algorithmic Stability of Stochastic Gradient Descent with Momentum under   Heavy-Tailed Noise</title><categories>stat.ML cs.LG math.OC math.PR</categories><comments>64 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the generalization properties of optimization algorithms under heavy-tailed noise has gained growing attention. However, the existing theoretical results mainly focus on stochastic gradient descent (SGD) and the analysis of heavy-tailed optimizers beyond SGD is still missing. In this work, we establish generalization bounds for SGD with momentum (SGDm) under heavy-tailed gradient noise. We first consider the continuous-time limit of SGDm, i.e., a Levy-driven stochastic differential equation (SDE), and establish quantitative Wasserstein algorithmic stability bounds for a class of potentially non-convex loss functions. Our bounds reveal a remarkable observation: For quadratic loss functions, we show that SGDm admits a worse generalization bound in the presence of heavy-tailed noise, indicating that the interaction of momentum and heavy tails can be harmful for generalization. We then extend our analysis to discrete-time and develop a uniform-in-time discretization error bound, which, to our knowledge, is the first result of its kind for SDEs with degenerate noise. This result shows that, with appropriately chosen step-sizes, the discrete dynamics retain the generalization properties of the limiting SDE. We illustrate our theory on both synthetic quadratic problems and neural networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00888</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00888</id><created>2025-02-02</created><authors><author><keyname>Giovannelli</keyname><forenames>Alexander</forenames></author><author><keyname>Murphy</keyname><forenames>Fionn</forenames></author><author><keyname>Davis</keyname><forenames>Trey</forenames></author><author><keyname>Lee</keyname><forenames>Chaerin</forenames></author><author><keyname>Abulikemu</keyname><forenames>Rehema</forenames></author><author><keyname>Gallagher</keyname><forenames>Matthew</forenames></author><author><keyname>Sharma</keyname><forenames>Sahil</forenames></author><author><keyname>Lisle</keyname><forenames>Lee</forenames></author><author><keyname>Bowman</keyname><forenames>Doug</forenames></author></authors><title>Planet Purifiers: A Collaborative Immersive Experience Proposing New   Modifications to HOMER and Fishing Reel Interaction Techniques</title><categories>cs.HC</categories><comments>2 pages, 2 figures, to be published in 2025 IEEE Conference on   Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents our solution to the 2025 3DUI Contest challenge. We aimed to develop a collaborative, immersive experience that raises awareness about trash pollution in natural landscapes while enhancing traditional interaction techniques in virtual environments. To achieve these objectives, we created an engaging multiplayer game where one user collects harmful pollutants while the other user provides medication to impacted wildlife using enhancements to traditional interaction techniques: HOMER and Fishing Reel. We enhanced HOMER to use a cone volume to reduce the precise aiming required by a selection raycast to provide a more efficient means to collect pollutants at large distances, coined as FLOW-MATCH. To improve the animal feed distribution to wildlife far away from the user with Fishing Reel, we created RAWR-XD, an asymmetric bi-manual technique to more conveniently adjust the reeling speed using the non-selecting wrist rotation of the user. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00892</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00892</id><created>2025-02-02</created><authors><author><keyname>Gabbay</keyname><forenames>Murdoch J.</forenames></author><author><keyname>Zanolini</keyname><forenames>Luca</forenames></author></authors><title>A declarative approach to specifying distributed algorithms using   three-valued modal logic</title><categories>cs.LO math.LO</categories><msc-class>03B45, 03B50, 68Q60</msc-class><acm-class>F.4.1; F.3.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present Coalition Logic, a three-valued modal fixed-point logic designed for declaratively specifying and reasoning about distributed algorithms, such as the Paxos consensus algorithm.   Our methodology represents a distributed algorithm as a logical theory, enabling correctness properties to be derived directly within the framework -- or revealing logical errors in the algorithm's design when they exist.   Coalition Logic adopts a declarative approach, specifying the overall logic of computation without prescribing control flow. Notably, message-passing is not explicitly modeled, distinguishing our framework from approaches like TLA+. This abstraction emphasises the logical essence of distributed algorithms, offering a novel perspective on their specification and reasoning.   We define the syntax and semantics of Coalition Logic, explore its theoretical properties, and demonstrate its applicability through a detailed treatment of the Paxos consensus algorithm. By presenting Paxos as a logical theory and deriving its standard correctness properties, we showcase the framework's capacity to handle non-trivial distributed systems.   We envision Coalition Logic as a versatile tool for specifying and reasoning about distributed algorithms. The Paxos example highlights the framework's ability to capture intricate details, offering a new lens through which distributed algorithms can be specified, studied, and checked. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00893</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00893</id><created>2025-02-02</created><authors><author><keyname>Shi</keyname><forenames>Haochen</forenames></author><author><keyname>Wang</keyname><forenames>Weizhuo</forenames></author><author><keyname>Song</keyname><forenames>Shuran</forenames></author><author><keyname>Liu</keyname><forenames>C. Karen</forenames></author></authors><title>ToddlerBot: Open-Source ML-Compatible Humanoid Platform for   Loco-Manipulation</title><categories>cs.RO</categories><comments>Project website: https://toddlerbot.github.io/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning-based robotics research driven by data demands a new approach to robot hardware design-one that serves as both a platform for policy execution and a tool for embodied data collection to train policies. We introduce ToddlerBot, a low-cost, open-source humanoid robot platform designed for scalable policy learning and research in robotics and AI. ToddlerBot enables seamless acquisition of high-quality simulation and real-world data. The plug-and-play zero-point calibration and transferable motor system identification ensure a high-fidelity digital twin, enabling zero-shot policy transfer from simulation to the real world. A user-friendly teleoperation interface facilitates streamlined real-world data collection for learning motor skills from human demonstrations. Utilizing its data collection ability and anthropomorphic design, ToddlerBot is an ideal platform to perform whole-body loco-manipulation. Additionally, ToddlerBot's compact size (0.56m, 3.4kg) ensures safe operation in real-world environments. Reproducibility is achieved with an entirely 3D-printed, open-source design and commercially available components, keeping the total cost under 6,000 USD. Comprehensive documentation allows assembly and maintenance with basic technical expertise, as validated by a successful independent replication of the system. We demonstrate ToddlerBot's capabilities through arm span, payload, endurance tests, loco-manipulation tasks, and a collaborative long-horizon scenario where two robots tidy a toy session together. By advancing ML-compatibility, capability, and reproducibility, ToddlerBot provides a robust platform for scalable learning and dynamic policy execution in robotics research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00894</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00894</id><created>2025-02-02</created><authors><author><keyname>Asgari</keyname><forenames>Ehsaneddin</forenames></author><author><keyname>Kheir</keyname><forenames>Yassine El</forenames></author><author><keyname>Javaheri</keyname><forenames>Mohammad Ali Sadraei</forenames></author></authors><title>MorphBPE: A Morpho-Aware Tokenizer Bridging Linguistic Complexity for   Efficient LLM Training Across Morphologies</title><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Tokenization is fundamental to Natural Language Processing (NLP), directly impacting model efficiency and linguistic fidelity. While Byte Pair Encoding (BPE) is widely used in Large Language Models (LLMs), it often disregards morpheme boundaries, leading to suboptimal segmentation, particularly in morphologically rich languages. We introduce MorphBPE, a morphology-aware extension of BPE that integrates linguistic structure into subword tokenization while preserving statistical efficiency. Additionally, we propose two morphology-based evaluation metrics: (i) Morphological Consistency F1-Score, which quantifies the consistency between morpheme sharing and token sharing, contributing to LLM training convergence, and (ii) Morphological Edit Distance, which measures alignment between morphemes and tokens concerning interpretability. Experiments on English, Russian, Hungarian, and Arabic across 300M and 1B parameter LLMs demonstrate that MorphBPE consistently reduces cross-entropy loss, accelerates convergence, and improves morphological alignment scores. Fully compatible with existing LLM pipelines, MorphBPE requires minimal modifications for integration. The MorphBPE codebase and tokenizer playground will be available at: https://github.com/llm-lab-org/MorphBPE and https://tokenizer.llm-lab.org </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00896</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00896</id><created>2025-02-02</created><authors><author><keyname>Jin</keyname><forenames>Can</forenames></author><author><keyname>Li</keyname><forenames>Ying</forenames></author><author><keyname>Zhao</keyname><forenames>Mingyu</forenames></author><author><keyname>Zhao</keyname><forenames>Shiyu</forenames></author><author><keyname>Wang</keyname><forenames>Zhenting</forenames></author><author><keyname>He</keyname><forenames>Xiaoxiao</forenames></author><author><keyname>Han</keyname><forenames>Ligong</forenames></author><author><keyname>Che</keyname><forenames>Tong</forenames></author><author><keyname>Metaxas</keyname><forenames>Dimitris N.</forenames></author></authors><title>LoR-VP: Low-Rank Visual Prompting for Efficient Vision Model Adaptation</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Visual prompting has gained popularity as a method for adapting pre-trained models to specific tasks, particularly in the realm of parameter-efficient tuning. However, existing visual prompting techniques often pad the prompt parameters around the image, limiting the interaction between the visual prompts and the original image to a small set of patches while neglecting the inductive bias present in shared information across different patches. In this study, we conduct a thorough preliminary investigation to identify and address these limitations. We propose a novel visual prompt design, introducing Low-Rank matrix multiplication for Visual Prompting (LoR-VP), which enables shared and patch-specific information across rows and columns of image pixels. Extensive experiments across seven network architectures and four datasets demonstrate significant improvements in both performance and efficiency compared to state-of-the-art visual prompting methods, achieving up to 6 times faster training times, utilizing 18 times fewer visual prompt parameters, and delivering a 3.1% improvement in performance. The code is available as https://github.com/jincan333/LoR-VP. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00897</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00897</id><created>2025-02-02</created><authors><author><keyname>Cheng</keyname><forenames>Shijun</forenames></author><author><keyname>Alkhalifah</keyname><forenames>Tariq</forenames></author></authors><title>Multi-frequency wavefield solutions for variable velocity models using   meta-learning enhanced low-rank physics-informed neural network</title><categories>cs.LG physics.geo-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physics-informed neural networks (PINNs) face significant challenges in modeling multi-frequency wavefields in complex velocity models due to their slow convergence, difficulty in representing high-frequency details, and lack of generalization to varying frequencies and velocity scenarios. To address these issues, we propose Meta-LRPINN, a novel framework that combines low-rank parameterization using singular value decomposition (SVD) with meta-learning and frequency embedding. Specifically, we decompose the weights of PINN's hidden layers using SVD and introduce an innovative frequency embedding hypernetwork (FEH) that links input frequencies with the singular values, enabling efficient and frequency-adaptive wavefield representation. Meta-learning is employed to provide robust initialization, improving optimization stability and reducing training time. Additionally, we implement adaptive rank reduction and FEH pruning during the meta-testing phase to further enhance efficiency. Numerical experiments, which are presented on multi-frequency scattered wavefields for different velocity models, demonstrate that Meta-LRPINN achieves much fast convergence speed and much high accuracy compared to baseline methods such as Meta-PINN and vanilla PINN. Also, the proposed framework shows strong generalization to out-of-distribution frequencies while maintaining computational efficiency. These results highlight the potential of our Meta-LRPINN for scalable and adaptable seismic wavefield modeling. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00899</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00899</id><created>2025-02-02</created><authors><author><keyname>Makni</keyname><forenames>Mehdi</forenames></author><author><keyname>Behdin</keyname><forenames>Kayhan</forenames></author><author><keyname>Xu</keyname><forenames>Zheng</forenames></author><author><keyname>Ponomareva</keyname><forenames>Natalia</forenames></author><author><keyname>Mazumder</keyname><forenames>Rahul</forenames></author></authors><title>HASSLE-free: A unified Framework for Sparse plus Low-Rank Matrix   Decomposition for LLMs</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The impressive capabilities of large foundation models come at a cost of substantial computing resources to serve them. Compressing these pre-trained models is of practical interest as it can democratize deploying them to the machine learning community at large by lowering the costs associated with inference. A promising compression scheme is to decompose foundation models' dense weights into a sum of sparse plus low-rank matrices. In this paper, we design a unified framework coined HASSLE-free for (semi-structured) sparse plus low-rank matrix decomposition of foundation models. Our framework introduces the local layer-wise reconstruction error objective for this decomposition, we demonstrate that prior work solves a relaxation of this optimization problem; and we provide efficient and scalable methods to minimize the exact introduced optimization problem. HASSLE-free substantially outperforms state-of-the-art methods in terms of the introduced objective and a wide range of LLM evaluation benchmarks. For the Llama3-8B model with a 2:4 sparsity component plus a 64-rank component decomposition, a compression scheme for which recent work shows important inference acceleration on GPUs, HASSLE-free reduces the test perplexity by 12% for the WikiText-2 dataset and reduces the gap (compared to the dense model) of the average of eight popular zero-shot tasks by 15% compared to existing methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00901</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00901</id><created>2025-02-02</created><authors><author><keyname>Troiani</keyname><forenames>Emanuele</forenames></author><author><keyname>Cui</keyname><forenames>Hugo</forenames></author><author><keyname>Dandi</keyname><forenames>Yatin</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Zdeborová</keyname><forenames>Lenka</forenames></author></authors><title>Fundamental limits of learning in sequence multi-index models and deep   attention networks: High-dimensional asymptotics and sharp thresholds</title><categories>cs.LG cond-mat.dis-nn</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this manuscript, we study the learning of deep attention neural networks, defined as the composition of multiple self-attention layers, with tied and low-rank weights. We first establish a mapping of such models to sequence multi-index models, a generalization of the widely studied multi-index model to sequential covariates, for which we establish a number of general results. In the context of Bayesian-optimal learning, in the limit of large dimension $D$ and commensurably large number of samples $N$, we derive a sharp asymptotic characterization of the optimal performance as well as the performance of the best-known polynomial-time algorithm for this setting --namely approximate message-passing--, and characterize sharp thresholds on the minimal sample complexity required for better-than-random prediction performance. Our analysis uncovers, in particular, how the different layers are learned sequentially. Finally, we discuss how this sequential learning can also be observed in a realistic setup. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00902</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00902</id><created>2025-02-02</created><authors><author><keyname>Wolter</keyname><forenames>Moritz</forenames></author><author><keyname>Veeramacheneni</keyname><forenames>Lokesh</forenames></author></authors><title>Position: More Rigorous Software Engineering Would Improve   Reproducibility in Machine Learning Research</title><categories>cs.SE cs.LG</categories><comments>Source code available at   https://github.com/BonnBytes/position_we_need_more_tests_in_ml</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Experimental verification and falsification of scholarly work are part of the scientific method's core. To improve the Machine Learning (ML)-communities' ability to verify results from prior work, we argue for more robust software engineering. We estimate the adoption of common engineering best practices by examining repository links from all recently accepted International Conference on Machine Learning (ICML), International Conference on Learning Representations (ICLR) and Neural Information Processing Systems (NeurIPS) papers as well as ICML papers over time. Based on the results, we recommend how we, as a community, can improve reproducibility in ML-research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00903</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00903</id><created>2025-02-02</created><authors><author><keyname>Kang</keyname><forenames>Taewoo</forenames></author><author><keyname>Thorson</keyname><forenames>Kjerstin</forenames></author><author><keyname>Peng</keyname><forenames>Tai-Quan</forenames></author><author><keyname>Hiaeshutter-Rice</keyname><forenames>Dan</forenames></author><author><keyname>Lee</keyname><forenames>Sanguk</forenames></author><author><keyname>Soroka</keyname><forenames>Stuart</forenames></author></authors><title>Embracing Dialectic Intersubjectivity: Coordination of Different   Perspectives in Content Analysis with LLM Persona Simulation</title><categories>cs.CL cs.AI cs.CY cs.SI</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This study attempts to advancing content analysis methodology from consensus-oriented to coordination-oriented practices, thereby embracing diverse coding outputs and exploring the dynamics among differential perspectives. As an exploratory investigation of this approach, we evaluate six GPT-4o configurations to analyze sentiment in Fox News and MSNBC transcripts on Biden and Trump during the 2020 U.S. presidential campaign, examining patterns across these models. By assessing each model's alignment with ideological perspectives, we explore how partisan selective processing could be identified in LLM-Assisted Content Analysis (LACA). Findings reveal that partisan persona LLMs exhibit stronger ideological biases when processing politically congruent content. Additionally, intercoder reliability is higher among same-partisan personas compared to cross-partisan pairs. This approach enhances the nuanced understanding of LLM outputs and advances the integrity of AI-driven social science research, enabling simulations of real-world implications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00908</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00908</id><created>2025-02-02</created><authors><author><keyname>Lane</keyname><forenames>Logan</forenames></author><author><keyname>Thomas</keyname><forenames>Jerald</forenames></author><author><keyname>Giovannelli</keyname><forenames>Alexander</forenames></author><author><keyname>Tahmid</keyname><forenames>Ibrahim</forenames></author><author><keyname>Bowman</keyname><forenames>Doug</forenames></author></authors><title>Exploring the Effects of Level of Control in the Initialization of   Shared Whiteboarding Sessions in Collaborative Augmented Reality</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Augmented Reality (AR) collaboration can benefit from a shared 2D surface, such as a whiteboard. However, many features of each collaborators physical environment must be considered in order to determine the best placement and shape of the shared surface. We explored the effects of three methods for beginning a collaborative whiteboarding session with varying levels of user control: MANUAL, DISCRETE CHOICE, and AUTOMATIC by conducting a simulated AR study within Virtual Reality (VR). In the MANUAL method, users draw their own surfaces directly in the environment until they agree on the placement; in the DISCRETE CHOICE method, the system provides three options for whiteboard size and location; and in the AUTOMATIC method, the system automatically creates a whiteboard that fits within each collaborators environment. We evaluate these three conditions in a study in which two collaborators used each method to begin collaboration sessions. After establishing a session, the users worked together to complete an affinity diagramming task using the shared whiteboard. We found that the majority of participants preferred to have direct control during the initialization of a new collaboration session, despite the additional workload induced by the Manual method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00911</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00911</id><created>2025-02-02</created><authors><author><keyname>Hawkins</keyname><forenames>Richard</forenames></author></authors><title>Developing Compelling Safety Cases</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper describes a method for creating compelling safety cases. The method seeks to help improve safety case practice in order to address the weaknesses identified in current practice, in particular confirmation bias, after-the-fact assurance and safety cases as a paperwork exercise. Rather than creating new notations and tools to address these issues, we contend that it is improvements in the safety case process that will make the most significant improvement to safety case practice. Our method builds upon established approaches and best practice to create an approach that will ensure safety cases are risk-focused, seek to identify ways in which the system may not be safe (rather than just assuming it is), drive safe design and operation of the system (influencing the system itself rather than just documenting what's there), are used to support decisions made throughout the life of the system, including system operation and change, and encourage developers and operators to think about and understand why their system is safe (and when it isn't). A simple example of an infusion pump system is used to illustrate how the new method is applied in practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00915</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00915</id><created>2025-02-02</created><authors><author><keyname>Yardim</keyname><forenames>Batuhan</forenames></author><author><keyname>Cayci</keyname><forenames>Semih</forenames></author><author><keyname>He</keyname><forenames>Niao</forenames></author></authors><title>A Variational Inequality Approach to Independent Learning in Static   Mean-Field Games</title><categories>math.OC cs.GT</categories><comments>53 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Competitive games involving thousands or even millions of players are prevalent in real-world contexts, such as transportation, communications, and computer networks. However, learning in these large-scale multi-agent environments presents a grand challenge, often referred to as the "curse of many agents". In this paper, we formalize and analyze the Static Mean-Field Game (SMFG) under both full and bandit feedback, offering a generic framework for modeling large population interactions while enabling independent learning.   We first establish close connections between SMFG and variational inequality (VI), showing that SMFG can be framed as a VI problem in the infinite agent limit. Building on the VI perspective, we propose independent learning and exploration algorithms that efficiently converge to approximate Nash equilibria, when dealing with a finite number of agents. Theoretically, we provide explicit finite sample complexity guarantees for independent learning across various feedback models in repeated play scenarios, assuming (strongly-)monotone payoffs. Numerically, we validate our results through both simulations and real-world applications in city traffic and network access management. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00916</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00916</id><created>2025-02-02</created><authors><author><keyname>Heiman</keyname><forenames>Alice</forenames></author></authors><title>The Accuracy, Robustness, and Readability of LLM-Generated   Sustainability-Related Word Definitions</title><categories>cs.CL</categories><comments>NLP4Ecology Workshop 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A common language with standardized definitions is crucial for effective climate discussions. However, concerns exist about LLMs misrepresenting climate terms. We compared 300 official IPCC glossary definitions with those generated by GPT-4o-mini, Llama3.1 8B, and Mistral 7B, analyzing adherence, robustness, and readability using SBERT sentence embeddings. The LLMs scored an average adherence of $0.57-0.59 \pm 0.15$, and their definitions proved harder to read than the originals. Model-generated definitions vary mainly among words with multiple or ambiguous definitions, showing the potential to highlight terms that need standardization. The results show how LLMs could support environmental discourse while emphasizing the need to align model outputs with established terminology for clarity and consistency. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00918</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00918</id><created>2025-02-02</created><authors><author><keyname>Sabin-Miller</keyname><forenames>David</forenames></author><author><keyname>Abrams</keyname><forenames>Daniel M.</forenames></author></authors><title>Equilibrium Moment Analysis of It\^o SDEs</title><categories>math.DS cs.NA math-ph math.MP math.NA</categories><comments>Main text: 4 pages, 1 figure. Supplemental Material: 3 pages, 4   figures. arXiv admin note: substantial text overlap with arXiv:2210.03781</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic differential equations have proved to be a valuable governing framework for many real-world systems which exhibit ``noise'' or randomness in their evolution. One quality of interest in such systems is the shape of their equilibrium probability distribution, if such a thing exists. In some cases a straightforward integral equation may yield this steady-state distribution, but in other cases the equilibrium distribution exists and yet that integral equation diverges. Here we establish a new equilibrium-analysis technique based on the logic of finite-timestep simulation which allows us to glean information about the equilibrium regardless -- in particular, a relationship between the raw moments of the equilibrium distribution. We utilize this technique to extract information about one such equilibrium resistant to direct definition. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00919</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00919</id><created>2025-02-02</created><authors><author><keyname>Zhang</keyname><forenames>Stephen</forenames></author><author><keyname>Khan</keyname><forenames>Mustafa</forenames></author><author><keyname>Papyan</keyname><forenames>Vardan</forenames></author></authors><title>Attention Sinks and Outlier Features: A 'Catch, Tag, and Release'   Mechanism for Embeddings</title><categories>cs.CL cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two prominent features of large language models (LLMs) is the presence of large-norm (outlier) features and the tendency for tokens to attend very strongly to a select few tokens. Despite often having no semantic relevance, these select tokens, called attention sinks, along with the large outlier features, have proven important for model performance, compression, and streaming. Consequently, investigating the roles of these phenomena within models and exploring how they might manifest in the model parameters has become an area of active interest. Through an empirical investigation, we demonstrate that attention sinks utilize outlier features to: catch a sequence of tokens, tag the captured tokens by applying a common perturbation, and then release the tokens back into the residual stream, where the tagged tokens are eventually retrieved. We prove that simple tasks, like averaging, necessitate the 'catch, tag, release' mechanism hence explaining why it would arise organically in modern LLMs. Our experiments also show that the creation of attention sinks can be completely captured in the model parameters using low-rank matrices, which has important implications for model compression and substantiates the success of recent approaches that incorporate a low-rank term to offset performance degradation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00920</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00920</id><created>2025-02-02</created><authors><author><keyname>Müller</keyname><forenames>Henning</forenames></author><author><keyname>Faust</keyname><forenames>Erik</forenames></author><author><keyname>Schlüter</keyname><forenames>Alexander</forenames></author><author><keyname>Müller</keyname><forenames>Ralf</forenames></author></authors><title>Extending the Lattice Boltzmann Method to Non-linear Solid Mechanics</title><categories>cs.CE cs.NA math.NA</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This work outlines a Lattice Boltzmann Method (LBM) for geometrically and constitutively nonlinear solid mechanics to simulate large deformations under dynamic loading conditions. The method utilizes the moment chain approach, where the nonlinear constitutive law is incorporated via a forcing term. Stress and deformation measures are expressed in the reference configuration. Finite difference schemes are employed for gradient and divergence computations, and Neumann- and Dirichlet-type boundary conditions are introduced.   Numerical studies are performed to assess the proposed method and illustrate its capabilities. Benchmark tests for weakly dynamic uniaxial tension and simple shear across a range of Poisson's ratios demonstrate the feasibility of the scheme and serve as validation of the implementation. Furthermore, a dynamic test case involving the propagation of bending waves in a cantilever beam highlights the potential of the method to model complex dynamic phenomena. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00921</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00921</id><created>2025-02-02</created><authors><author><keyname>Li</keyname><forenames>Marvin</forenames></author><author><keyname>Karan</keyname><forenames>Aayush</forenames></author><author><keyname>Chen</keyname><forenames>Sitan</forenames></author></authors><title>Blink of an eye: a simple theory for feature localization in generative   models</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models (LLMs) can exhibit undesirable and unexpected behavior in the blink of an eye. In a recent Anthropic demo, Claude switched from coding to Googling pictures of Yellowstone, and these sudden shifts in behavior have also been observed in reasoning patterns and jailbreaks. This phenomenon is not unique to autoregressive models: in diffusion models, key features of the final output are decided in narrow ``critical windows'' of the generation process. In this work we develop a simple, unifying theory to explain this phenomenon. We show that it emerges generically as the generation process localizes to a sub-population of the distribution it models. While critical windows have been studied at length in diffusion models, existing theory heavily relies on strong distributional assumptions and the particulars of Gaussian diffusion. In contrast to existing work our theory (1) applies to autoregressive and diffusion models; (2) makes no distributional assumptions; (3) quantitatively improves previous bounds even when specialized to diffusions; and (4) requires basic tools and no stochastic calculus or statistical physics-based machinery. We also identify an intriguing connection to the all-or-nothing phenomenon from statistical inference. Finally, we validate our predictions empirically for LLMs and find that critical windows often coincide with failures in problem solving for various math and reasoning benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00922</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00922</id><created>2025-02-02</created><authors><author><keyname>Yubeaton</keyname><forenames>Patrick</forenames></author><author><keyname>Mahmoud</keyname><forenames>Tareq</forenames></author><author><keyname>Naga</keyname><forenames>Shehab</forenames></author><author><keyname>Taheri</keyname><forenames>Pooria</forenames></author><author><keyname>Xia</keyname><forenames>Tianhua</forenames></author><author><keyname>George</keyname><forenames>Arun</forenames></author><author><keyname>Khalil</keyname><forenames>Yasmein</forenames></author><author><keyname>Zhang</keyname><forenames>Sai Qian</forenames></author><author><keyname>Joshi</keyname><forenames>Siddharth</forenames></author><author><keyname>Hegde</keyname><forenames>Chinmay</forenames></author><author><keyname>Garg</keyname><forenames>Siddharth</forenames></author></authors><title>Huff-LLM: End-to-End Lossless Compression for Efficient LLM Inference</title><categories>cs.LG cs.AR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As they become more capable, large language models (LLMs) have continued to rapidly increase in size. This has exacerbated the difficulty in running state of the art LLMs on small, edge devices. Standard techniques advocate solving this problem through lossy compression techniques such as quantization or pruning. However, such compression techniques are lossy, and have been shown to change model behavior in unpredictable manners. We propose Huff-LLM, an \emph{end-to-end, lossless} model compression method that lets users store LLM weights in compressed format \emph{everywhere} -- cloud, disk, main memory, and even in on-chip memory/buffers. This allows us to not only load larger models in main memory, but also reduces bandwidth required to load weights on chip, and makes more efficient use of on-chip weight buffers. In addition to the memory savings achieved via compression, we also show latency and energy efficiency improvements when performing inference with the compressed model. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00926</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00926</id><created>2025-02-02</created><authors><author><keyname>Allison</keyname><forenames>Katherine</forenames></author><author><keyname>Kelly</keyname><forenames>Jonathan</forenames></author><author><keyname>Hatton</keyname><forenames>Benjamin</forenames></author></authors><title>Structured Pneumatic Fingerpads for Actively Tunable Grip Friction</title><categories>cs.RO</categories><comments>Submitted to IEEE International Conference on Soft Robotics   (RoboSoft). 7 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grip surfaces with tunable friction can actively modify contact conditions, enabling transitions between higher- and lower-friction states for grasp adjustment. Friction can be increased to grip securely and then decreased to gently release (e.g., for handovers) or manipulate in-hand. Recent friction-tuning surface designs using soft pneumatic chambers show good control over grip friction; however, most require complex fabrication processes and/or custom gripper hardware. We present a practical structured fingerpad design for friction tuning that uses less than \$1 USD of materials, takes only seconds to repair, and is easily adapted to existing grippers. Our design uses surface morphology changes to tune friction. The fingerpad is actuated by pressurizing its internal chambers, thereby deflecting its flexible grip surface out from or into these chambers. We characterize the friction-tuning capabilities of our design by measuring the shear force required to pull an object from a gripper equipped with two independently actuated fingerpads. Our results show that varying actuation pressure and timing changes the magnitude of friction forces on a gripped object by up to a factor of 2.8. We demonstrate additional features including macro-scale interlocking behaviour and pressure-based object detection. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00928</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00928</id><created>2025-02-02</created><authors><author><keyname>Karimi-Bidhendi</keyname><forenames>Saeed</forenames></author><author><keyname>Geraci</keyname><forenames>Giovanni</forenames></author><author><keyname>Jafarkhani</keyname><forenames>Hamid</forenames></author></authors><title>Mathematical Cell Deployment Optimization for Capacity and Coverage of   Ground and UAV Users</title><categories>cs.IT eess.SP math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general mathematical framework for optimizing cell deployment and antenna configuration in wireless networks, inspired by quantization theory. Unlike traditional methods, our framework supports networks with deterministically located nodes, enabling modeling and optimization under controlled deployment scenarios. We demonstrate our framework through two applications: joint fine-tuning of antenna parameters across base stations (BSs) to optimize network coverage, capacity, and load balancing, and the strategic deployment of new BSs, including the optimization of their locations and antenna settings. These optimizations are conducted for a heterogeneous 3D user population, comprising ground users (GUEs) and uncrewed aerial vehicles (UAVs) along aerial corridors. Our case studies highlight the framework's versatility in optimizing performance metrics such as the coverage-capacity trade-off and capacity per region. Our results confirm that optimizing the placement and orientation of additional BSs consistently outperforms approaches focused solely on antenna adjustments, regardless of GUE distribution. Furthermore, joint optimization for both GUEs and UAVs significantly enhances UAV service without severely affecting GUE performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00930</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00930</id><created>2025-02-02</created><authors><author><keyname>Rodrigues</keyname><forenames>Victor Hugo Pereira</forenames></author><author><keyname>Oliveira</keyname><forenames>Tiago Roux</forenames></author><author><keyname>Krstic</keyname><forenames>Miroslav</forenames></author><author><keyname>Tabuada</keyname><forenames>Paulo</forenames></author></authors><title>Event-Triggered Newton-Based Extremum Seeking Control</title><categories>math.OC cs.SY eess.SY</categories><comments>9 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper proposes the incorporation of static event-triggered control in the actuation path of Newton-based extremum seeking and its comparison with the earlier gradient version. As in the continuous methods, the convergence rate of the gradient approach depends on the unknown Hessian of the nonlinear map to be optimized, whereas the proposed event-triggered Newton-based extremum seeking eliminates this dependence, becoming user-assignable. This is achieved by means of a dynamic estimator for the Hessian's inverse, implemented as a Riccati equation filter. Lyapunov stability and averaging theory for discontinuous systems are applied to analyze the closed-loop system. Local exponential practical stability is guaranteed to a small neighborhood of the extremum point of scalar and static maps. Numerical simulations illustrate the advantages of the proposed approach over the previous gradient method, including improved convergence speed, followed by a reduction in the amplitude and updating frequency of the control signals. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00931</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00931</id><created>2025-02-02</created><authors><author><keyname>Du</keyname><forenames>Yi</forenames></author><author><keyname>Fu</keyname><forenames>Taimeng</forenames></author><author><keyname>Chen</keyname><forenames>Zhuoqun</forenames></author><author><keyname>Li</keyname><forenames>Bowen</forenames></author><author><keyname>Su</keyname><forenames>Shaoshu</forenames></author><author><keyname>Zhao</keyname><forenames>Zhipeng</forenames></author><author><keyname>Wang</keyname><forenames>Chen</forenames></author></authors><title>VL-Nav: Real-time Vision-Language Navigation with Spatial Reasoning</title><categories>cs.RO cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vision-language navigation in unknown environments is crucial for mobile robots. In scenarios such as household assistance and rescue, mobile robots need to understand a human command, such as "find a person wearing black". We present a novel vision-language navigation (VL-Nav) system that integrates efficient spatial reasoning on low-power robots. Unlike prior methods that rely on a single image-level feature similarity to guide a robot, we introduce the heuristic-vision-language (HVL) spatial reasoning for goal point selection. It combines pixel-wise vision-language features and heuristic exploration to enable efficient navigation to human-instructed instances in various environments robustly. We deploy VL-Nav on a four-wheel mobile robot and conduct comprehensive navigation tasks in various environments of different scales and semantic complexities, indoors and outdoors. Remarkably, VL-Nav operates at a real-time frequency of 30 Hz with a Jetson Orin NX, highlighting its ability to conduct efficient vision-language navigation. Experimental results show that VL-Nav achieves an overall success rate of 86.3%, outperforming previous methods by 44.15%. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00935</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00935</id><created>2025-02-02</created><authors><author><keyname>Nakamura</keyname><forenames>Kensuke</forenames></author><author><keyname>Peters</keyname><forenames>Lasse</forenames></author><author><keyname>Bajcsy</keyname><forenames>Andrea</forenames></author></authors><title>Generalizing Safety Beyond Collision-Avoidance via Latent-Space   Reachability Analysis</title><categories>cs.RO cs.LG</categories><comments>6 figures, 6 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Hamilton-Jacobi (HJ) reachability is a rigorous mathematical framework that enables robots to simultaneously detect unsafe states and generate actions that prevent future failures. While in theory, HJ reachability can synthesize safe controllers for nonlinear systems and nonconvex constraints, in practice, it has been limited to hand-engineered collision-avoidance constraints modeled via low-dimensional state-space representations and first-principles dynamics. In this work, our goal is to generalize safe robot controllers to prevent failures that are hard -- if not impossible -- to write down by hand, but can be intuitively identified from high-dimensional observations: for example, spilling the contents of a bag. We propose Latent Safety Filters, a latent-space generalization of HJ reachability that tractably operates directly on raw observation data (e.g., RGB images) by performing safety analysis in the latent embedding space of a generative world model. This transforms nuanced constraint specification to a classification problem in latent space and enables reasoning about dynamical consequences that are hard to simulate. In simulation and hardware experiments, we use Latent Safety Filters to safeguard arbitrary policies (from generative policies to direct teleoperation) from complex safety hazards, like preventing a Franka Research 3 manipulator from spilling the contents of a bag or toppling cluttered objects. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00937</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00937</id><created>2025-02-02</created><authors><author><keyname>Qiu</keyname><forenames>Haoran</forenames></author><author><keyname>Biswas</keyname><forenames>Anish</forenames></author><author><keyname>Zhao</keyname><forenames>Zihan</forenames></author><author><keyname>Mohan</keyname><forenames>Jayashree</forenames></author><author><keyname>Khare</keyname><forenames>Alind</forenames></author><author><keyname>Choukse</keyname><forenames>Esha</forenames></author><author><keyname>Goiri</keyname><forenames>Íñigo</forenames></author><author><keyname>Zhang</keyname><forenames>Zeyu</forenames></author><author><keyname>Shen</keyname><forenames>Haiying</forenames></author><author><keyname>Bansal</keyname><forenames>Chetan</forenames></author><author><keyname>Ramjee</keyname><forenames>Ramachandran</forenames></author><author><keyname>Fonseca</keyname><forenames>Rodrigo</forenames></author></authors><title>Towards Efficient Large Multimodal Model Serving</title><categories>cs.DC cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Recent advances in generative AI have led to large multi-modal models (LMMs) capable of simultaneously processing inputs of various modalities such as text, images, video, and audio. While these models demonstrate impressive capabilities, efficiently serving them in production environments poses significant challenges due to their complex architectures and heterogeneous resource requirements.   We present the first comprehensive systems analysis of two prominent LMM architectures, decoder-only and cross-attention, on six representative open-source models. We investigate their multi-stage inference pipelines and resource utilization patterns that lead to unique systems design implications. We also present an in-depth analysis of production LMM inference traces, uncovering unique workload characteristics, including variable, heavy-tailed request distributions, diverse modal combinations, and bursty traffic patterns.   Our key findings reveal that different LMM inference stages exhibit highly heterogeneous performance characteristics and resource demands, while concurrent requests across modalities lead to significant performance interference. To address these challenges, we propose a decoupled serving architecture that enables independent resource allocation and adaptive scaling for each stage. We further propose optimizations such as stage colocation to maximize throughput and resource utilization while meeting the latency objectives. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00939</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00939</id><created>2025-02-02</created><authors><author><keyname>Flores</keyname><forenames>Erick Andrew Bustamante</forenames></author><author><keyname>Olivera</keyname><forenames>Harley Vera</forenames></author><author><keyname>Valencia</keyname><forenames>Ivan Cesar Medrano</forenames></author><author><keyname>Cubas</keyname><forenames>Carlos Fernando Montoya</forenames></author></authors><title>Fruit Fly Classification (Diptera: Tephritidae) in Images, Applying   Transfer Learning</title><categories>cs.CV cs.AI</categories><comments>15 pages and 19 figures</comments><msc-class>68T10</msc-class><acm-class>I.2.10</acm-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This study develops a transfer learning model for the automated classification of two species of fruit flies, Anastrepha fraterculus and Ceratitis capitata, in a controlled laboratory environment. The research addresses the need to optimize identification and classification, which are currently performed manually by experts, being affected by human factors and facing time challenges. The methodological process of this study includes the capture of high-quality images using a mobile phone camera and a stereo microscope, followed by segmentation to reduce size and focus on relevant morphological areas. The images were carefully labeled and preprocessed to ensure the quality and consistency of the dataset used to train the pre-trained convolutional neural network models VGG16, VGG19, and Inception-v3. The results were evaluated using the F1-score, achieving 82% for VGG16 and VGG19, while Inception-v3 reached an F1-score of 93%. Inception-v3's reliability was verified through model testing in uncontrolled environments, with positive results, complemented by the Grad-CAM technique, demonstrating its ability to capture essential morphological features. These findings indicate that Inception-v3 is an effective and replicable approach for classifying Anastrepha fraterculus and Ceratitis capitata, with potential for implementation in automated monitoring systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00940</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00940</id><created>2025-02-02</created><authors><author><keyname>Fernandez-Bes</keyname><forenames>Jesus</forenames></author><author><keyname>Cid-Sueiro</keyname><forenames>Jesus</forenames></author><author><keyname>Marques</keyname><forenames>Antonio G.</forenames></author></authors><title>An MDP Model for Censoring in Harvesting Sensors: Optimal and   Approximated Solutions</title><categories>eess.SY cs.AI cs.SY</categories><journal-ref>IEEE Journal on Selected Areas in Communications ( Volume: 33,   Issue: 8, August 2015)</journal-ref><doi>10.1109/JSAC.2015.2391792</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we propose a novel censoring policy for energy-efficient transmissions in energy-harvesting sensors. The problem is formulated as an infinite-horizon Markov Decision Process (MDP). The objective to be optimized is the expected sum of the importance (utility) of all transmitted messages. Assuming that such importance can be evaluated at the transmitting node, we show that, under certain conditions on the battery model, the optimal censoring policy is a threshold function on the importance value. Specifically, messages are transmitted only if their importance is above a threshold whose value depends on the battery level. Exploiting this property, we propose a model-based stochastic scheme that approximates the optimal solution, with less computational complexity and faster convergence speed than a conventional Q-learning algorithm. Numerical experiments in single-hop and multi-hop networks confirm the analytical advantages of the proposed scheme. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00941</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00941</id><created>2025-02-02</created><authors><author><keyname>Pavanatto</keyname><forenames>Leonardo</forenames></author><author><keyname>Giovannelli</keyname><forenames>Alexander</forenames></author><author><keyname>Giera</keyname><forenames>Brian</forenames></author><author><keyname>Bremer</keyname><forenames>Peer-Timo</forenames></author><author><keyname>Miao</keyname><forenames>Haichao</forenames></author><author><keyname>Bowman</keyname><forenames>Doug</forenames></author></authors><title>Exploring Multiscale Navigation of Homogeneous and Dense Objects with   Progressive Refinement in Virtual Reality</title><categories>cs.HC</categories><comments>10 pages, 5 figures, 1 table, to be published in 2025 IEEE conference   on virtual reality and 3D user interfaces (VR)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Locating small features in a large, dense object in virtual reality (VR) poses a significant interaction challenge. While existing multiscale techniques support transitions between various levels of scale, they are not focused on handling dense, homogeneous objects with hidden features. We propose a novel approach that applies the concept of progressive refinement to VR navigation, enabling focused inspections. We conducted a user study where we varied two independent variables in our design, navigation style (STRUCTURED vs. UNSTRUCTURED) and display mode (SELECTION vs. EVERYTHING), to better understand their effects on efficiency and awareness during multiscale navigation. Our results showed that unstructured navigation can be faster than structured and that displaying only the selection can be faster than displaying the entire object. However, using an everything display mode can support better location awareness and object understanding. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00943</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00943</id><created>2025-02-02</created><authors><author><keyname>Wong</keyname><forenames>Cliff</forenames></author><author><keyname>Preston</keyname><forenames>Sam</forenames></author><author><keyname>Liu</keyname><forenames>Qianchu</forenames></author><author><keyname>Gero</keyname><forenames>Zelalem</forenames></author><author><keyname>Bagga</keyname><forenames>Jass</forenames></author><author><keyname>Zhang</keyname><forenames>Sheng</forenames></author><author><keyname>Jain</keyname><forenames>Shrey</forenames></author><author><keyname>Zhao</keyname><forenames>Theodore</forenames></author><author><keyname>Gu</keyname><forenames>Yu</forenames></author><author><keyname>Xu</keyname><forenames>Yanbo</forenames></author><author><keyname>Kiblawi</keyname><forenames>Sid</forenames></author><author><keyname>Weerasinghe</keyname><forenames>Roshanthi</forenames></author><author><keyname>Leidner</keyname><forenames>Rom</forenames></author><author><keyname>Young</keyname><forenames>Kristina</forenames></author><author><keyname>Piening</keyname><forenames>Brian</forenames></author><author><keyname>Bifulco</keyname><forenames>Carlo</forenames></author><author><keyname>Naumann</keyname><forenames>Tristan</forenames></author><author><keyname>Wei</keyname><forenames>Mu</forenames></author><author><keyname>Poon</keyname><forenames>Hoifung</forenames></author></authors><title>Universal Abstraction: Harnessing Frontier Models to Structure   Real-World Data at Scale</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vast majority of real-world patient information resides in unstructured clinical text, and the process of medical abstraction seeks to extract and normalize structured information from this unstructured input. However, traditional medical abstraction methods can require significant manual efforts that can include crafting rules or annotating training labels, limiting scalability. In this paper, we propose UniMedAbstractor (UMA), a zero-shot medical abstraction framework leveraging Large Language Models (LLMs) through a modular and customizable prompt template. We refer to our approach as universal abstraction as it can quickly scale to new attributes through its universal prompt template without curating attribute-specific training labels or rules. We evaluate UMA for oncology applications, focusing on fifteen key attributes representing the cancer patient journey, from short-context attributes (e.g., performance status, treatment) to complex long-context attributes requiring longitudinal reasoning (e.g., tumor site, histology, TNM staging). Experiments on real-world data show UMA's strong performance and generalizability. Compared to supervised and heuristic baselines, UMA with GPT-4o achieves on average an absolute 2-point F1/accuracy improvement for both short-context and long-context attribute abstraction. For pathologic T staging, UMA even outperforms the supervised model by 20 points in accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00944</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00944</id><created>2025-02-02</created><authors><author><keyname>Speckhard</keyname><forenames>Daniel</forenames></author><author><keyname>Bechtel</keyname><forenames>Tim</forenames></author><author><keyname>Kehl</keyname><forenames>Sebastian</forenames></author><author><keyname>Godwin</keyname><forenames>Jonathan</forenames></author><author><keyname>Draxl</keyname><forenames>Claudia</forenames></author></authors><title>Analysis of static and dynamic batching algorithms for graph neural   networks</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Graph neural networks (GNN) have shown promising results for several domains such as materials science, chemistry, and the social sciences. GNN models often contain millions of parameters, and like other neural network (NN) models, are often fed only a fraction of the graphs that make up the training dataset in batches to update model parameters. The effect of batching algorithms on training time and model performance has been thoroughly explored for NNs but not yet for GNNs. We analyze two different batching algorithms for graph based models, namely static and dynamic batching. We use the Jraph library built on JAX to perform our experiments, where we compare the two batching methods for two datasets, the QM9 dataset of small molecules and the AFLOW materials database. Our experiments show that significant training time savings can be found from changing the batching algorithm, but the fastest algorithm depends on the data, model, batch size and number of training steps run. Experiments show no significant difference in model learning between the algorithms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00947</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00947</id><created>2025-02-02</created><authors><author><keyname>Vishwanath</keyname><forenames>Siddharth</forenames></author><author><keyname>Arias-Castro</keyname><forenames>Ery</forenames></author></authors><title>Minimax Optimality of Classical Scaling Under General Noise Conditions</title><categories>math.ST cs.LG stat.ML stat.TH</categories><comments>45 pages, 4 figures</comments><msc-class>62R07, 94A16, 62G05, 62C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish the consistency of classical scaling under a broad class of noise models, encompassing many commonly studied cases in literature. Our approach requires only finite fourth moments of the noise, significantly weakening standard assumptions. We derive convergence rates for classical scaling and establish matching minimax lower bounds, demonstrating that classical scaling achieves minimax optimality in recovering the true configuration even when the input dissimilarities are corrupted by noise. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00949</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00949</id><created>2025-02-02</created><authors><author><keyname>Di Gianantonio</keyname><forenames>Pietro</forenames></author><author><keyname>Edalat</keyname><forenames>Abbas</forenames></author></authors><title>A domain-theoretic framework for conditional probability and Bayesian   updating in programming</title><categories>cs.LO cs.PL</categories><comments>17 pages</comments><msc-class>03B70</msc-class><acm-class>F.3.2</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a domain-theoretic framework for probabilistic programming that provides a constructive definition of conditional probability and addresses computability challenges previously identified in the literature. We introduce a novel approach based on an observable notion of events that enables computability. We examine two methods for computing conditional probabilities -- one using conditional density functions and another using trace sampling with rejection -- and prove they yield consistent results within our framework. We implement these ideas in a simple probabilistic functional language with primitives for sampling and evaluation, providing both operational and denotational semantics and proving their consistency. Our work provides a rigorous foundation for implementing conditional probability in probabilistic programming languages. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00950</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00950</id><created>2025-02-02</created><authors><author><keyname>Jafari</keyname><forenames>Farzane</forenames></author></authors><title>Fast Bitrate Identification using Overlapped LCS</title><categories>cs.MM cs.CR</categories><comments>9 pages</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Audio data are widely exchanged over telecommunications networks. Due to the limitations of network resources, these data are typically compressed before transmission. Various methods are available for compressing audio data. To access such audio information, it is first necessary to identify the codec used for compression. One of the most effective approaches for audio codec identification involves analyzing the content of received packets. In these methods, statistical features extracted from the packets are utilized to determine the codec employed. This paper proposes a novel method for audio codec classification based on features derived from the overlapped longest common sub-string and sub-sequence (LCS). The simulation results, which achieved an accuracy of 97\% for 8 KB packets, demonstrate the superiority of the proposed method over conventional approaches. This method divides each 8 KB packet into fifteen 1 KB packets with a 50\% overlap. The results indicate that this division has no significant impact on the simulation outcomes, while significantly speeding up the feature extraction, being \(8 \times\) faster than the traditional method for extracting LCS features. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00951</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00951</id><created>2025-02-02</created><authors><author><keyname>Dragan</keyname><forenames>Feodor F.</forenames></author></authors><title>Graph parameters that are coarsely equivalent to tree-length</title><categories>math.CO cs.DS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Two graph parameters are said to be coarsely equivalent if they are within constant factors from each other for every graph $G$. Recently, several graph parameters were shown to be coarsely equivalent to tree-length. Recall that the length of a tree-decomposition ${\cal T}(G)$ of a graph $G$ is the largest diameter of a bag in ${\cal T}(G)$, and the tree-length of $G$ is the minimum of the length, over all tree-decompositions of $G$. We present simpler and sometimes with better bounds proofs for those known in literature results and further extend this list of graph parameters coarsely equivalent to tree-length. Among other new results, we show that the tree-length of a graph $G$ is small if and only if for every bramble ${\cal F}$ (or every Helly family of connected subgraphs ${\cal F}$, or every Helly family of paths ${\cal F}$) of $G$, there is a disk in $G$ with small radius that intercepts all members of ${\cal F}$. Furthermore, the tree-length of a graph $G$ is small if and only if $G$ can be embedded with a small additive distortion to an unweighted tree with the same vertex set as in $G$ (not involving any Steiner points). Additionally, we introduce a new natural "bridging`` property for cycles, which generalizes a known property of cycles in chordal graphs, and show that it also coarsely defines the tree-length. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00952</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00952</id><created>2025-02-02</created><authors><author><keyname>Zhao</keyname><forenames>Dora</forenames></author><author><keyname>Yang</keyname><forenames>Diyi</forenames></author><author><keyname>Bernstein</keyname><forenames>Michael S.</forenames></author></authors><title>Mapping the Spiral of Silence: Surveying Unspoken Opinions in Online   Communities</title><categories>cs.SI cs.CY cs.HC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We often treat social media as a lens onto society. How might that lens be distorting the actual popularity of political and social viewpoints? In this paper, we examine the difference between the viewpoints publicly posted in a community and the privately surveyed viewpoints of community members, contributing a measurement of a theory called the "spiral of silence." This theory observes that people are less likely to voice their opinion when they believe they are in the minority--leading to a spiral where minority opinions are less likely to be shared, so they appear even further in the minority, and become even less likely to be shared. We surveyed active members of politically oriented Reddit communities to gauge their willingness to post on contentious topics, yielding 627 responses from 108 participants about 11 topics and 33 subreddits. We find that 72.6% of participants who perceive themselves in the minority remain silent, and are only half as likely to post their viewpoint compared to those who believe their opinion is in the majority. Communities perceived as being more inclusive reduce the magnitude of this effect. These results emphasize how far out of step the opinions we see online may be with the population they purport to represent. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00953</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00953</id><created>2025-02-02</created><authors><author><keyname>Rodríguez-Tembleque</keyname><forenames>Luis</forenames></author><author><keyname>González</keyname><forenames>José A.</forenames></author><author><keyname>Cerrato</keyname><forenames>Antonio</forenames></author></authors><title>Partitioned solution strategies for coupled BEM-FEM acoustic   fluid-structure interaction problems</title><categories>math.NA cs.NA</categories><journal-ref>Computers &amp; Structures, Volume 152, 2015, Pages 45-58, ISSN   0045-7949</journal-ref><doi>10.1016/j.compstruc.2015.02.018</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper investigates two FEM-BEM coupling formulations for acoustic fluid-structure interaction (FSI) problems, using the Finite Element Method (FEM) to model the structure and the Boundary Element Method (BEM) to represent a linear acoustic fluid. The coupling methods described interconnect fluid and structure using classical or localized Lagrange multipliers, allowing the connection of non-matching interfaces. First coupling technique is the well known mortar method, that uses classical multipliers and is compared with a new formulation of the method of localized Lagrange multipliers (LLM) for FSI applications with non-matching interfaces. The proposed non-overlapping domain decomposition technique uses a classical non-symmetrical acoustic BEM formulation for the fluid, although a symmetric Galerkin BEM formulation could be used as well. A comparison between the localized methodology and the mortar method in highly non conforming interface meshes is presented. Furthermore, the methodology proposes an iterative preconditioned and projected bi-conjugate gradient solver which presents very good scalability properties in the solution of this kind of problems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00954</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00954</id><created>2025-02-02</created><authors><author><keyname>Mao</keyname><forenames>Ye</forenames></author><author><keyname>Luo</keyname><forenames>Weixun</forenames></author><author><keyname>Jing</keyname><forenames>Junpeng</forenames></author><author><keyname>Qiu</keyname><forenames>Anlan</forenames></author><author><keyname>Mikolajczyk</keyname><forenames>Krystian</forenames></author></authors><title>Hypo3D: Exploring Hypothetical Reasoning in 3D</title><categories>cs.CV</categories><comments>19 pages, 15 figures, 9 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rise of vision-language foundation models marks an advancement in bridging the gap between human and machine capabilities in 3D scene reasoning. Existing 3D reasoning benchmarks assume real-time scene accessibility, which is impractical due to the high cost of frequent scene updates. To this end, we introduce Hypothetical 3D Reasoning, namely Hypo3D, a benchmark designed to evaluate models' ability to reason without access to real-time scene data. Models need to imagine the scene state based on a provided change description before reasoning. Hypo3D is formulated as a 3D Visual Question Answering (VQA) benchmark, comprising 7,727 context changes across 700 indoor scenes, resulting in 14,885 question-answer pairs. An anchor-based world frame is established for all scenes, ensuring consistent reference to a global frame for directional terms in context changes and QAs. Extensive experiments show that state-of-the-art foundation models struggle to reason in hypothetically changed scenes. This reveals a substantial performance gap compared to humans, particularly in scenarios involving movement changes and directional reasoning. Even when the context change is irrelevant to the question, models often incorrectly adjust their answers. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00955</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00955</id><created>2025-02-02</created><authors><author><keyname>Shi</keyname><forenames>Wentao</forenames></author><author><keyname>Yu</keyname><forenames>Zichun</forenames></author><author><keyname>Feng</keyname><forenames>Fuli</forenames></author><author><keyname>He</keyname><forenames>Xiangnan</forenames></author><author><keyname>Xiong</keyname><forenames>Chenyan</forenames></author></authors><title>Efficient Multi-Agent System Training with Data Influence-Oriented Tree   Search</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Monte Carlo Tree Search (MCTS) based methods provide promising approaches for generating synthetic data to enhance the self-training of Large Language Model (LLM) based multi-agent systems (MAS). These methods leverage Q-values to estimate individual agent contributions. However, relying solely on Q-values to identify informative data may misalign with the data synthesis objective, as the focus should be on selecting data that best enhances model training. To address this discrepancy, we propose Data Influence-oriented Tree Search (DITS), a novel framework that incorporates influence scores to guide both tree search and data selection. By leveraging influence scores, we effectively identify the most impactful data for system improvement, thereby enhancing model performance. Furthermore, we derive influence score estimation methods tailored for non-differentiable metrics, significantly reducing computational overhead by utilizing inference computations. Extensive experiments on eight multi-agent datasets demonstrate the robustness and effectiveness of the proposed methods. Notably, our findings reveal that allocating more inference resources to estimate influence scores, rather than Q-values, during data synthesis can more effectively and efficiently enhance model training. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00960</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00960</id><created>2025-02-02</created><authors><author><keyname>Yang</keyname><forenames>Mingyu</forenames></author><author><keyname>Lu</keyname><forenames>Jitong</forenames></author><author><keyname>Kim</keyname><forenames>Hun-Seok</forenames></author></authors><title>SAM-guided Pseudo Label Enhancement for Multi-modal 3D Semantic   Segmentation</title><categories>cs.CV</categories><comments>ICRA 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-modal 3D semantic segmentation is vital for applications such as autonomous driving and virtual reality (VR). To effectively deploy these models in real-world scenarios, it is essential to employ cross-domain adaptation techniques that bridge the gap between training data and real-world data. Recently, self-training with pseudo-labels has emerged as a predominant method for cross-domain adaptation in multi-modal 3D semantic segmentation. However, generating reliable pseudo-labels necessitates stringent constraints, which often result in sparse pseudo-labels after pruning. This sparsity can potentially hinder performance improvement during the adaptation process. We propose an image-guided pseudo-label enhancement approach that leverages the complementary 2D prior knowledge from the Segment Anything Model (SAM) to introduce more reliable pseudo-labels, thereby boosting domain adaptation performance. Specifically, given a 3D point cloud and the SAM masks from its paired image data, we collect all 3D points covered by each SAM mask that potentially belong to the same object. Then our method refines the pseudo-labels within each SAM mask in two steps. First, we determine the class label for each mask using majority voting and employ various constraints to filter out unreliable mask labels. Next, we introduce Geometry-Aware Progressive Propagation (GAPP) which propagates the mask label to all 3D points within the SAM mask while avoiding outliers caused by 2D-3D misalignment. Experiments conducted across multiple datasets and domain adaptation scenarios demonstrate that our proposed method significantly increases the quantity of high-quality pseudo-labels and enhances the adaptation performance over baseline methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00961</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00961</id><created>2025-02-02</created><authors><author><keyname>Kemp</keyname><forenames>Matthew</forenames></author><author><keyname>Kalutarage</keyname><forenames>Harsha</forenames></author><author><keyname>Al-Kadri</keyname><forenames>M. Omar</forenames></author></authors><title>AI-Powered Spearphishing Cyber Attacks: Fact or Fiction?</title><categories>cs.CR cs.CY</categories><comments>11 pages, 10 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Due to society's continuing technological advance, the capabilities of machine learning-based artificial intelligence systems continue to expand and influence a wider degree of topics. Alongside this expansion of technology, there is a growing number of individuals willing to misuse these systems to defraud and mislead others. Deepfake technology, a set of deep learning algorithms that are capable of replacing the likeness or voice of one individual with another with alarming accuracy, is one of these technologies. This paper investigates the threat posed by malicious use of this technology, particularly in the form of spearphishing attacks. It uses deepfake technology to create spearphishing-like attack scenarios and validate them against average individuals. Experimental results show that 66% of participants failed to identify AI created audio as fake while 43% failed to identify such videos as fake, confirming the growing fear of threats posed by the use of these technologies by cybercriminals. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00963</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00963</id><created>2025-02-02</created><authors><author><keyname>Soroco</keyname><forenames>Mauricio</forenames></author><author><keyname>Song</keyname><forenames>Jialin</forenames></author><author><keyname>Xia</keyname><forenames>Mengzhou</forenames></author><author><keyname>Emond</keyname><forenames>Kye</forenames></author><author><keyname>Sun</keyname><forenames>Weiran</forenames></author><author><keyname>Chen</keyname><forenames>Wuyang</forenames></author></authors><title>PDE-Controller: LLMs for Autoformalization and Reasoning of PDEs</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While recent AI-for-math has made strides in pure mathematics, areas of applied mathematics, particularly PDEs, remain underexplored despite their significant real-world applications. We present PDE-Controller, a framework that enables large language models (LLMs) to control systems governed by partial differential equations (PDEs). Our approach enables LLMs to transform informal natural language instructions into formal specifications, and then execute reasoning and planning steps to improve the utility of PDE control. We build a holistic solution comprising datasets (both human-written cases and 2 million synthetic samples), math-reasoning models, and novel evaluation metrics, all of which require significant effort. Our PDE-Controller significantly outperforms prompting the latest open-source and GPT models in reasoning, autoformalization, and program synthesis, achieving up to a 62% improvement in utility gain for PDE control. By bridging the gap between language generation and PDE systems, we demonstrate the potential of LLMs in addressing complex scientific and engineering challenges. We will release all data, model checkpoints, and code at https://pde-controller.github.io/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00964</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00964</id><created>2025-02-02</created><authors><author><keyname>Padigela</keyname><forenames>Harshith</forenames></author><author><keyname>Shah</keyname><forenames>Chintan</forenames></author><author><keyname>Juyal</keyname><forenames>Dinkar</forenames></author></authors><title>ML-Dev-Bench: Comparative Analysis of AI Agents on ML development   workflows</title><categories>cs.SE cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In this report, we present ML-Dev-Bench, a benchmark aimed at testing agentic capabilities on applied Machine Learning development tasks. While existing benchmarks focus on isolated coding tasks or Kaggle-style competitions, ML-Dev-Bench tests agents' ability to handle the full complexity of ML development workflows. The benchmark assesses performance across critical aspects including dataset handling, model training, improving existing models, debugging, and API integration with popular ML tools. We evaluate three agents -- ReAct, Openhands, and AIDE -- on a diverse set of 25 tasks, providing insights into their strengths and limitations in handling practical ML development challenges. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00965</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00965</id><created>2025-02-02</created><authors><author><keyname>Wang</keyname><forenames>Xinze</forenames></author><author><keyname>Chen</keyname><forenames>Chen</forenames></author><author><keyname>Yang</keyname><forenames>Yinfei</forenames></author><author><keyname>Chen</keyname><forenames>Hong-You</forenames></author><author><keyname>Zhang</keyname><forenames>Bowen</forenames></author><author><keyname>Pal</keyname><forenames>Aditya</forenames></author><author><keyname>Zhu</keyname><forenames>Xiangxin</forenames></author><author><keyname>Du</keyname><forenames>Xianzhi</forenames></author></authors><title>CLIP-UP: A Simple and Efficient Mixture-of-Experts CLIP Training Recipe   with Sparse Upcycling</title><categories>cs.CV cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Mixture-of-Experts (MoE) models are crucial for scaling model capacity while controlling inference costs. While integrating MoE into multimodal models like CLIP improves performance, training these models is notoriously challenging and expensive. We propose CLIP-Upcycling (CLIP-UP), an efficient alternative training strategy that converts a pre-trained dense CLIP model into a sparse MoE architecture. Through extensive experimentation with various settings and auxiliary losses, we demonstrate that CLIP-UP significantly reduces training complexity and cost. Remarkably, our sparse CLIP B/16 model, trained with CLIP-UP, outperforms its dense counterpart by 7.2% and 6.6% on COCO and Flickr30k text-to-image Recall@1 benchmarks respectively. It even surpasses the larger CLIP L/14 model on this task while using only 30% of the inference FLOPs. We further demonstrate the generalizability of our training recipe across different scales, establishing sparse upcycling as a practical and scalable approach for building efficient, high-performance CLIP models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00966</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00966</id><created>2025-02-02</created><authors><author><keyname>Pu</keyname><forenames>Isabella</forenames></author><author><keyname>Snyder</keyname><forenames>Jeff</forenames></author><author><keyname>Leonard</keyname><forenames>Naomi Ehrich</forenames></author></authors><title>The Beatbots: A Musician-Informed Multi-Robot Percussion Quartet</title><categories>cs.RO cs.HC</categories><comments>Copyright protected by IEEE/ACM, 10 pages, 4 figures, 1 table, in   proceedings of 20th IEEE/ACM International Conference on Human-Robot   Interaction (HRI 2025)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Artistic creation is often seen as a uniquely human endeavor, yet robots bring distinct advantages to music-making, such as precise tempo control, unpredictable rhythmic complexities, and the ability to coordinate intricate human and robot performances. While many robotic music systems aim to mimic human musicianship, our work emphasizes the unique strengths of robots, resulting in a novel multi-robot performance instrument called the Beatbots, capable of producing music that is challenging for humans to replicate using current methods. The Beatbots were designed using an ``informed prototyping'' process, incorporating feedback from three musicians throughout development. We evaluated the Beatbots through a live public performance, surveying participants (N=28) to understand how they perceived and interacted with the robotic performance. Results show that participants valued the playfulness of the experience, the aesthetics of the robot system, and the unconventional robot-generated music. Expert musicians and non-expert roboticists demonstrated especially positive mindset shifts during the performance, although participants across all demographics had favorable responses. We propose design principles to guide the development of future robotic music systems and identify key robotic music affordances that our musician consultants considered particularly important for robotic music performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00968</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00968</id><created>2025-02-02</created><authors><author><keyname>Singh</keyname><forenames>Anuj</forenames></author><author><keyname>Mukherjee</keyname><forenames>Sayak</forenames></author><author><keyname>Beirami</keyname><forenames>Ahmad</forenames></author><author><keyname>Jamali-Rad</keyname><forenames>Hadi</forenames></author></authors><title>CoDe: Blockwise Control for Denoising Diffusion Models</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aligning diffusion models to downstream tasks often requires finetuning new models or gradient-based guidance at inference time to enable sampling from the reward-tilted posterior. In this work, we explore a simple inference-time gradient-free guidance approach, called controlled denoising (CoDe), that circumvents the need for differentiable guidance functions and model finetuning. CoDe is a blockwise sampling method applied during intermediate denoising steps, allowing for alignment with downstream rewards. Our experiments demonstrate that, despite its simplicity, CoDe offers a favorable trade-off between reward alignment, prompt instruction following, and inference cost, achieving a competitive performance against the state-of-the-art baselines. Our code is available at: https://github.com/anujinho/code. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00969</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00969</id><created>2025-02-02</created><authors><author><keyname>Li</keyname><forenames>Xiangci</forenames></author><author><keyname>Chen</keyname><forenames>Zhiyu</forenames></author><author><keyname>Choi</keyname><forenames>Jason Ingyu</forenames></author><author><keyname>Vedula</keyname><forenames>Nikhita</forenames></author><author><keyname>Fetahu</keyname><forenames>Besnik</forenames></author><author><keyname>Rokhlenko</keyname><forenames>Oleg</forenames></author><author><keyname>Malmasi</keyname><forenames>Shervin</forenames></author></authors><title>Wizard of Shopping: Target-Oriented E-commerce Dialogue Generation with   Decision Tree Branching</title><categories>cs.CL</categories><comments>Accepted by SIGDIAL 2024 but withdrawn</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The goal of conversational product search (CPS) is to develop an intelligent, chat-based shopping assistant that can directly interact with customers to understand shopping intents, ask clarification questions, and find relevant products. However, training such assistants is hindered mainly due to the lack of reliable and large-scale datasets. Prior human-annotated CPS datasets are extremely small in size and lack integration with real-world product search systems. We propose a novel approach, TRACER, which leverages large language models (LLMs) to generate realistic and natural conversations for different shopping domains. TRACER's novelty lies in grounding the generation to dialogue plans, which are product search trajectories predicted from a decision tree model, that guarantees relevant product discovery in the shortest number of search conditions. We also release the first target-oriented CPS dataset Wizard of Shopping (WoS), containing highly natural and coherent conversations (3.6k) from three shopping domains. Finally, we demonstrate the quality and effectiveness of WoS via human evaluations and downstream tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00972</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00972</id><created>2025-02-02</created><authors><author><keyname>Hong</keyname><forenames>Yicong</forenames></author><author><keyname>Mai</keyname><forenames>Long</forenames></author><author><keyname>Yao</keyname><forenames>Yuan</forenames></author><author><keyname>Liu</keyname><forenames>Feng</forenames></author></authors><title>Pushing the Boundaries of State Space Models for Image and Video   Generation</title><categories>cs.CV cs.LG</categories><comments>21 pages, paper under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While Transformers have become the dominant architecture for visual generation, linear attention models, such as the state-space models (SSM), are increasingly recognized for their efficiency in processing long visual sequences. However, the essential efficiency of these models comes from formulating a limited recurrent state, enforcing causality among tokens that are prone to inconsistent modeling of N-dimensional visual data, leaving questions on their capacity to generate long non-causal sequences. In this paper, we explore the boundary of SSM on image and video generation by building the largest-scale diffusion SSM-Transformer hybrid model to date (5B parameters) based on the sub-quadratic bi-directional Hydra and self-attention, and generate up to 2K images and 360p 8 seconds (16 FPS) videos. Our results demonstrate that the model can produce faithful results aligned with complex text prompts and temporal consistent videos with high dynamics, suggesting the great potential of using SSMs for visual generation tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00973</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00973</id><created>2025-02-02</created><authors><author><keyname>Nguyen</keyname><forenames>Minh Ngoc</forenames></author><author><keyname>Le-Duc</keyname><forenames>Khai</forenames></author><author><keyname>Pham</keyname><forenames>Tan-Hanh</forenames></author><author><keyname>Nguyen</keyname><forenames>Trang</forenames></author><author><keyname>Luu</keyname><forenames>Quang Minh</forenames></author><author><keyname>Tran</keyname><forenames>Ba Kien</forenames></author><author><keyname>Hy</keyname><forenames>Truong-Son</forenames></author><author><keyname>Dremin</keyname><forenames>Viktor</forenames></author><author><keyname>Sokolovsky</keyname><forenames>Sergei</forenames></author><author><keyname>Rafailov</keyname><forenames>Edik</forenames></author></authors><title>A Wearable Device Dataset for Mental Health Assessment Using Laser   Doppler Flowmetry and Fluorescence Spectroscopy Sensors</title><categories>cs.LG eess.SP</categories><comments>Preprint, 55 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this study, we introduce a novel method to predict mental health by building machine learning models for a non-invasive wearable device equipped with Laser Doppler Flowmetry (LDF) and Fluorescence Spectroscopy (FS) sensors. Besides, we present the corresponding dataset to predict mental health, e.g. depression, anxiety, and stress levels via the DAS-21 questionnaire. To our best knowledge, this is the world's largest and the most generalized dataset ever collected for both LDF and FS studies. The device captures cutaneous blood microcirculation parameters, and wavelet analysis of the LDF signal extracts key rhythmic oscillations. The dataset, collected from 132 volunteers aged 18-94 from 19 countries, explores relationships between physiological features, demographics, lifestyle habits, and health conditions. We employed a variety of machine learning methods to classify stress detection, in which LightGBM is identified as the most effective model for stress detection, achieving a ROC AUC of 0.7168 and a PR AUC of 0.8852. In addition, we also incorporated Explainable Artificial Intelligence (XAI) techniques into our analysis to investigate deeper insights into the model's predictions. Our results suggest that females, younger individuals and those with a higher Body Mass Index (BMI) or heart rate have a greater likelihood of experiencing mental health conditions like stress and anxiety. All related code and data are published online: https://github.com/leduckhai/Wearable_LDF-FS. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00975</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00975</id><created>2025-02-02</created><authors><author><keyname>Rahman</keyname><forenames>Md. Abdur</forenames></author></authors><title>Detection of Distributed Denial of Service Attacks based on Machine   Learning Algorithms</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Distributed Denial of Service (DDoS) attacks make the challenges to provide the services of the data resources to the web clients. In this paper, we concern to study and apply different Machine Learning (ML) techniques to separate the DDoS attack instances from benign instances. Our experimental results show that forward and backward data bytes of our dataset are observed more similar for DDoS attacks compared to the data bytes for benign attempts. This paper uses different machine learning techniques for the detection of the attacks efficiently in order to make sure the offered services from web servers available. This results from the proposed approach suggest that 97.1% of DDoS attacks are successfully detected by the Support Vector Machine (SVM). These accuracies are better while comparing to the several existing machine learning approaches. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00976</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00976</id><created>2025-02-02</created><authors><author><keyname>Tang</keyname><forenames>Wentao</forenames></author></authors><title>Learning the Integral Quadratic Constraints on Plant-Model Mismatch</title><categories>eess.SY cs.SY</categories><comments>6 pages, 5 figures; submitted to the 9th IEEE Conference on Control   Technology and Applications (CCTA 2025)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While a characterization of plant-model mismatch is necessary for robust control, the mismatch usually can not be described accurately due to the lack of knowledge about the plant model or the complexity of nonlinear plants. Hence, this paper considers this problem in a data-driven way, where the mismatch is captured by parametric forms of integral quadratic constraints (IQCs) and the parameters contained in the IQC equalities are learned from sampled trajectories from the plant. To this end, a one-class support vector machine (OC-SVM) formulation is proposed, and its generalization performance is analyzed based on the statistical learning theory. The proposed approach is demonstrated by a single-input-single-output time delay mismatch and a nonlinear two-phase reactor with a linear nominal model, showing accurate recovery of frequency-domain uncertainties. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00977</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00977</id><created>2025-02-02</created><authors><author><keyname>Ou</keyname><forenames>Litu</forenames></author><author><keyname>Lapata</keyname><forenames>Mirella</forenames></author></authors><title>Context-Aware Hierarchical Merging for Long Document Summarization</title><categories>cs.CL</categories><comments>30 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Hierarchical Merging is a technique commonly used to summarize very long texts ($&gt;$100K tokens) by breaking down the input into smaller sections, summarizing those sections individually, and then merging or combining those summaries into a final coherent summary. Although it helps address the limitations of large language models (LLMs) with fixed input length constraints, the recursive merging process can amplify LLM hallucinations, increasing the risk of factual inaccuracies. In this paper, we seek to mitigate hallucinations by enriching hierarchical merging with context from the source document. Specifically, we propose different approaches to contextual augmentation ranging from \emph{replacing} intermediate summaries with relevant input context, to \emph{refining} them while using the context as supporting evidence, and \emph{aligning} them implicitly (via citations) to the input. Experimental results on datasets representing legal and narrative domains show that contextual augmentation consistently outperforms zero-shot and hierarchical merging baselines for the Llama 3.1 model family. Our analysis further reveals that refinement methods tend to perform best when paired with extractive summarization for identifying relevant input. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00980</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00980</id><created>2025-02-02</created><authors><author><keyname>Cho</keyname><forenames>So-Yoon</forenames></author><author><keyname>Lee</keyname><forenames>Sungchul</forenames></author><author><keyname>Kim</keyname><forenames>Hyun-Gyoon</forenames></author></authors><title>Forecasting VIX using interpretable Kolmogorov-Arnold networks</title><categories>cs.LG cs.AI cs.CE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents the use of Kolmogorov-Arnold Networks (KANs) for forecasting the CBOE Volatility Index (VIX). Unlike traditional MLP-based neural networks that are often criticized for their black-box nature, KAN offers an interpretable approach via learnable spline-based activation functions and symbolification. Based on a parsimonious architecture with symbolic functions, KAN expresses a forecast of the VIX as a closed-form in terms of explanatory variables, and provide interpretable insights into key characteristics of the VIX, including mean reversion and the leverage effect. Through in-depth empirical analysis across multiple datasets and periods, we show that KANs achieve competitive forecasting performance while requiring significantly fewer parameters compared to MLP-based neural network models. Our findings demonstrate the capacity and potential of KAN as an interpretable financial time-series forecasting method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00983</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00983</id><created>2025-02-02</created><authors><author><keyname>Zhang</keyname><forenames>Zhengzhe</forenames></author><author><keyname>Meng</keyname><forenames>Wenjia</forenames></author><author><keyname>Sun</keyname><forenames>Haoliang</forenames></author><author><keyname>Pan</keyname><forenames>Gang</forenames></author></authors><title>CausalCOMRL: Context-Based Offline Meta-Reinforcement Learning with   Causal Representation</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Context-based offline meta-reinforcement learning (OMRL) methods have achieved appealing success by leveraging pre-collected offline datasets to develop task representations that guide policy learning. However, current context-based OMRL methods often introduce spurious correlations, where task components are incorrectly correlated due to confounders. These correlations can degrade policy performance when the confounders in the test task differ from those in the training task. To address this problem, we propose CausalCOMRL, a context-based OMRL method that integrates causal representation learning. This approach uncovers causal relationships among the task components and incorporates the causal relationships into task representations, enhancing the generalizability of RL agents. We further improve the distinction of task representations from different tasks by using mutual information optimization and contrastive learning. Utilizing these causal task representations, we employ SAC to optimize policies on meta-RL benchmarks. Experimental results show that CausalCOMRL achieves better performance than other methods on most benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00987</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00987</id><created>2025-02-02</created><authors><author><keyname>Albert</keyname><forenames>Paul</forenames></author><author><keyname>Zhang</keyname><forenames>Frederic Z.</forenames></author><author><keyname>Saratchandran</keyname><forenames>Hemanth</forenames></author><author><keyname>Rodriguez-Opazo</keyname><forenames>Cristian</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author><author><keyname>Abbasnejad</keyname><forenames>Ehsan</forenames></author></authors><title>RandLoRA: Full-rank parameter-efficient fine-tuning of large models</title><categories>cs.CL cs.AI cs.CV</categories><comments>To appear at the International Conference on Learning Representations   (ICLR) 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Low-Rank Adaptation (LoRA) and its variants have shown impressive results in reducing the number of trainable parameters and memory requirements of large transformer networks while maintaining fine-tuning performance. However, the low-rank nature of the weight update inherently limits the representation power of fine-tuned models, potentially compromising performance on complex tasks. This raises a critical question: when a performance gap between LoRA and standard fine-tuning is observed, is it due to the reduced number of trainable parameters or the rank deficiency? This paper aims to answer this question by introducing RandLoRA, a parameter-efficient method that performs full-rank updates using a learned linear combinations of low-rank, non-trainable random matrices. Our method limits the number of trainable parameters by restricting optimization to diagonal scaling matrices applied to the fixed random matrices. This allows us to effectively overcome the low-rank limitations while maintaining parameter and memory efficiency during training. Through extensive experimentation across vision, language, and vision-language benchmarks, we systematically evaluate the limitations of LoRA and existing random basis methods. Our findings reveal that full-rank updates are beneficial across vision and language tasks individually, and even more so for vision-language tasks, where RandLoRA significantly reduces -- and sometimes eliminates -- the performance gap between standard fine-tuning and LoRA, demonstrating its efficacy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00988</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00988</id><created>2025-02-02</created><authors><author><keyname>Goswami</keyname><forenames>Kanika</forenames></author><author><keyname>Mathur</keyname><forenames>Puneet</forenames></author><author><keyname>Rossi</keyname><forenames>Ryan</forenames></author><author><keyname>Dernoncourt</keyname><forenames>Franck</forenames></author></authors><title>PlotGen: Multi-Agent LLM-based Scientific Data Visualization via   Multimodal Feedback</title><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Scientific data visualization is pivotal for transforming raw data into comprehensible visual representations, enabling pattern recognition, forecasting, and the presentation of data-driven insights. However, novice users often face difficulties due to the complexity of selecting appropriate tools and mastering visualization techniques. Large Language Models (LLMs) have recently demonstrated potential in assisting code generation, though they struggle with accuracy and require iterative debugging. In this paper, we propose PlotGen, a novel multi-agent framework aimed at automating the creation of precise scientific visualizations. PlotGen orchestrates multiple LLM-based agents, including a Query Planning Agent that breaks down complex user requests into executable steps, a Code Generation Agent that converts pseudocode into executable Python code, and three retrieval feedback agents - a Numeric Feedback Agent, a Lexical Feedback Agent, and a Visual Feedback Agent - that leverage multimodal LLMs to iteratively refine the data accuracy, textual labels, and visual correctness of generated plots via self-reflection. Extensive experiments show that PlotGen outperforms strong baselines, achieving a 4-6 percent improvement on the MatPlotBench dataset, leading to enhanced user trust in LLM-generated visualizations and improved novice productivity due to a reduction in debugging time needed for plot errors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00989</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00989</id><created>2025-02-02</created><authors><author><keyname>Goswami</keyname><forenames>Kanika</forenames></author><author><keyname>Mathur</keyname><forenames>Puneet</forenames></author><author><keyname>Rossi</keyname><forenames>Ryan</forenames></author><author><keyname>Dernoncourt</keyname><forenames>Franck</forenames></author></authors><title>ChartCitor: Multi-Agent Framework for Fine-Grained Chart Visual   Attribution</title><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) can perform chart question-answering tasks but often generate unverified hallucinated responses. Existing answer attribution methods struggle to ground responses in source charts due to limited visual-semantic context, complex visual-text alignment requirements, and difficulties in bounding box prediction across complex layouts. We present ChartCitor, a multi-agent framework that provides fine-grained bounding box citations by identifying supporting evidence within chart images. The system orchestrates LLM agents to perform chart-to-table extraction, answer reformulation, table augmentation, evidence retrieval through pre-filtering and re-ranking, and table-to-chart mapping. ChartCitor outperforms existing baselines across different chart types. Qualitative user studies show that ChartCitor helps increase user trust in Generative AI by providing enhanced explainability for LLM-assisted chart QA and enables professionals to be more productive. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00991</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00991</id><created>2025-02-02</created><authors><author><keyname>Zhuang</keyname><forenames>Qiyu</forenames></author><author><keyname>Lu</keyname><forenames>Wei</forenames></author><author><keyname>Liu</keyname><forenames>Shuang</forenames></author><author><keyname>Chen</keyname><forenames>Yuxing</forenames></author><author><keyname>Shi</keyname><forenames>Xinyue</forenames></author><author><keyname>Zhao</keyname><forenames>Zhanhao</forenames></author><author><keyname>Sun</keyname><forenames>Yipeng</forenames></author><author><keyname>Pan</keyname><forenames>Anqun</forenames></author><author><keyname>Du</keyname><forenames>Xiaoyong</forenames></author></authors><title>TxnSails: Achieving Serializable Transaction Scheduling with   Self-Adaptive Isolation Level Selection</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Achieving the serializable isolation level, regarded as the gold standard for transaction processing, is costly. Recent studies reveal that adjusting specific query patterns within a workload can still achieve serializability even at lower isolation levels. Nevertheless, these studies typically overlook the trade-off between the performance advantages of lower isolation levels and the overhead required to maintain serializability, potentially leading to suboptimal isolation level choices that fail to maximize performance. In this paper, we present TxnSails, a middle-tier solution designed to achieve serializable scheduling with self-adaptive isolation level selection. First, TxnSails incorporates a unified concurrency control algorithm that achieves serializability at lower isolation levels with minimal additional overhead. Second, TxnSails employs a deep learning method to characterize the trade-off between the performance benefits and overhead associated with lower isolation levels, thus predicting the optimal isolation level. Finally, TxnSails implements a cross-isolation validation mechanism to ensure serializability during real-time isolation level transitions. Extensive experiments demonstrate that TxnSails outperforms state-of-the-art solutions by up to 26.7x and PostgreSQL's serializable isolation level by up to 4.8x. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00992</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00992</id><created>2025-02-02</created><authors><author><keyname>Zhou</keyname><forenames>Dongliang</forenames></author><author><keyname>Zhang</keyname><forenames>Haijun</forenames></author><author><keyname>Ma</keyname><forenames>Jianghong</forenames></author><author><keyname>Fan</keyname><forenames>Jicong</forenames></author><author><keyname>Zhang</keyname><forenames>Zhao</forenames></author></authors><title>FCBoost-Net: A Generative Network for Synthesizing Multiple Collocated   Outfits via Fashion Compatibility Boosting</title><categories>cs.CV cs.MM</categories><comments>This paper has been accepted for presentation at ACM Multimedia 2023</comments><doi>10.1145/3581783.3612036</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Outfit generation is a challenging task in the field of fashion technology, in which the aim is to create a collocated set of fashion items that complement a given set of items. Previous studies in this area have been limited to generating a unique set of fashion items based on a given set of items, without providing additional options to users. This lack of a diverse range of choices necessitates the development of a more versatile framework. However, when the task of generating collocated and diversified outfits is approached with multimodal image-to-image translation methods, it poses a challenging problem in terms of non-aligned image translation, which is hard to address with existing methods. In this research, we present FCBoost-Net, a new framework for outfit generation that leverages the power of pre-trained generative models to produce multiple collocated and diversified outfits. Initially, FCBoost-Net randomly synthesizes multiple sets of fashion items, and the compatibility of the synthesized sets is then improved in several rounds using a novel fashion compatibility booster. This approach was inspired by boosting algorithms and allows the performance to be gradually improved in multiple steps. Empirical evidence indicates that the proposed strategy can improve the fashion compatibility of randomly synthesized fashion items as well as maintain their diversity. Extensive experiments confirm the effectiveness of our proposed framework with respect to visual authenticity, diversity, and fashion compatibility. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00996</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00996</id><created>2025-02-02</created><authors><author><keyname>Zhou</keyname><forenames>Ben</forenames></author><author><keyname>Jain</keyname><forenames>Sarthak</forenames></author><author><keyname>Zhang</keyname><forenames>Yi</forenames></author><author><keyname>Ning</keyname><forenames>Qiang</forenames></author><author><keyname>Wang</keyname><forenames>Shuai</forenames></author><author><keyname>Benajiba</keyname><forenames>Yassine</forenames></author><author><keyname>Roth</keyname><forenames>Dan</forenames></author></authors><title>Self-supervised Analogical Learning using Language Models</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models have been shown to suffer from reasoning inconsistency issues. That is, they fail more in situations unfamiliar to the training data, even though exact or very similar reasoning paths exist in more common cases that they can successfully solve. Such observations motivate us to propose methods that encourage models to understand the high-level and abstract reasoning processes during training instead of only the final answer. This way, models can transfer the exact solution to similar cases, regardless of their relevance to the pre-training data distribution. In this work, we propose SAL, a self-supervised analogical learning framework. SAL mimics the human analogy process and trains models to explicitly transfer high-quality symbolic solutions from cases that they know how to solve to other rare cases in which they tend to fail more. We show that the resulting models after SAL learning outperform base language models on a wide range of reasoning benchmarks, such as StrategyQA, GSM8K, and HotpotQA, by 2% to 20%. At the same time, we show that our model is more generalizable and controllable through analytical studies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00997</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00997</id><created>2025-02-02</created><authors><author><keyname>Zhou</keyname><forenames>Yuhang</forenames></author><author><keyname>Karamanolakis</keyname><forenames>Giannis</forenames></author><author><keyname>Soto</keyname><forenames>Victor</forenames></author><author><keyname>Rumshisky</keyname><forenames>Anna</forenames></author><author><keyname>Kulkarni</keyname><forenames>Mayank</forenames></author><author><keyname>Huang</keyname><forenames>Furong</forenames></author><author><keyname>Ai</keyname><forenames>Wei</forenames></author><author><keyname>Lu</keyname><forenames>Jianhua</forenames></author></authors><title>MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs</title><categories>cs.CL cs.AI</categories><comments>Accepted by NAACL 2024 Main</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. However, the effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures. State-of-the-art MoE merging methods only work with homogeneous model architectures and rely on simple unweighted averaging to merge expert layers, which does not address parameter interference and requires extensive fine-tuning of the merged MoE to restore performance. To address these limitations, this paper introduces new MoE merging techniques, including strategies to mitigate parameter interference, routing heuristics to reduce the need for MoE fine-tuning, and a novel method for merging experts with different architectures. Extensive experiments across multiple domains demonstrate the effectiveness of our proposed methods, reducing fine-tuning costs, improving performance over state-of-the-art methods, and expanding the applicability of MoE merging. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01000</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01000</id><created>2025-02-02</created><authors><author><keyname>Yang</keyname><forenames>Jingyun</forenames></author><author><keyname>Zhang</keyname><forenames>Guoqing</forenames></author><author><keyname>Wang</keyname><forenames>Jingge</forenames></author><author><keyname>Li</keyname><forenames>Yang</forenames></author></authors><title>Adapting Foundation Models for Few-Shot Medical Image Segmentation:   Actively and Sequentially</title><categories>cs.CV</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Recent advances in foundation models have brought promising results in computer vision, including medical image segmentation. Fine-tuning foundation models on specific low-resource medical tasks has become a standard practice. However, ensuring reliable and robust model adaptation when the target task has a large domain gap and few annotated samples remains a challenge. Previous few-shot domain adaptation (FSDA) methods seek to bridge the distribution gap between source and target domains by utilizing auxiliary data. The selection and scheduling of auxiliaries are often based on heuristics, which can easily cause negative transfer. In this work, we propose an Active and Sequential domain AdaPtation (ASAP) framework for dynamic auxiliary dataset selection in FSDA. We formulate FSDA as a multi-armed bandit problem and derive an efficient reward function to prioritize training on auxiliary datasets that align closely with the target task, through a single-round fine-tuning. Empirical validation on diverse medical segmentation datasets demonstrates that our method achieves favorable segmentation performance, significantly outperforming the state-of-the-art FSDA methods, achieving an average gain of 27.75% on MRI and 7.52% on CT datasets in Dice score. Code is available at the git repository: https://github.com/techicoco/ASAP. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01001</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01001</id><created>2025-02-02</created><authors><author><keyname>Cheng</keyname><forenames>Yukun</forenames></author><author><keyname>Deng</keyname><forenames>Xiaotie</forenames></author><author><keyname>Ma</keyname><forenames>Yunxuan</forenames></author></authors><title>Networked Digital Public Goods Games with Heterogeneous Players and   Convex Costs</title><categories>cs.GT econ.TH</categories><comments>20 pages, 1 figure</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the digital age, resources such as open-source software and publicly accessible databases form a crucial category of digital public goods, providing extensive benefits for Internet. This paper investigates networked public goods games involving heterogeneous players and convex costs, focusing on the characterization of Nash Equilibrium (NE). In these games, each player can choose her effort level, representing her contributions to public goods. Network structures are employed to model the interactions among participants. Each player's utility consists of a concave value component, influenced by the collective efforts of all players, and a convex cost component, determined solely by the individual's own effort. To the best of our knowledge, this study is the first to explore the networked public goods game with convex costs.   Our research begins by examining welfare solutions aimed at maximizing social welfare and ensuring the convergence of pseudo-gradient ascent dynamics. We establish the presence of NE in this model and provide an in-depth analysis of the conditions under which NE is unique. We also delve into comparative statics, an essential tool in economics, to evaluate how slight modifications in the model--interpreted as monetary redistribution--affect player utilities. In addition, we analyze a particular scenario with a predefined game structure, illustrating the practical relevance of our theoretical insights. Overall, our research enhances the broader understanding of strategic interactions and structural dynamics in networked public goods games, with significant implications for policy design in internet economic and social networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01002</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01002</id><created>2025-02-02</created><authors><author><keyname>Zhang</keyname><forenames>Wenfei</forenames></author><author><keyname>Zhao</keyname><forenames>Ruipeng</forenames></author><author><keyname>Yao</keyname><forenames>Yongxiang</forenames></author><author><keyname>Wan</keyname><forenames>Yi</forenames></author><author><keyname>Wu</keyname><forenames>Peihao</forenames></author><author><keyname>Li</keyname><forenames>Jiayuan</forenames></author><author><keyname>Li</keyname><forenames>Yansheng</forenames></author><author><keyname>Zhang</keyname><forenames>Yongjun</forenames></author></authors><title>Multi-Resolution SAR and Optical Remote Sensing Image Registration   Methods: A Review, Datasets, and Future Perspectives</title><categories>cs.CV</categories><comments>48 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synthetic Aperture Radar (SAR) and optical image registration is essential for remote sensing data fusion, with applications in military reconnaissance, environmental monitoring, and disaster management. However, challenges arise from differences in imaging mechanisms, geometric distortions, and radiometric properties between SAR and optical images. As image resolution increases, fine SAR textures become more significant, leading to alignment issues and 3D spatial discrepancies. Two major gaps exist: the lack of a publicly available multi-resolution, multi-scene registration dataset and the absence of systematic analysis of current methods. To address this, the MultiResSAR dataset was created, containing over 10k pairs of multi-source, multi-resolution, and multi-scene SAR and optical images. Sixteen state-of-the-art algorithms were tested. Results show no algorithm achieves 100% success, and performance decreases as resolution increases, with most failing on sub-meter data. XoFTR performs best among deep learning methods (40.58%), while RIFT performs best among traditional methods (66.51%). Future research should focus on noise suppression, 3D geometric fusion, cross-view transformation modeling, and deep learning optimization for robust registration of high-resolution SAR and optical images. The dataset is available at https://github.com/betterlll/Multi-Resolution-SAR-dataset-. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01004</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01004</id><created>2025-02-02</created><authors><author><keyname>Chen</keyname><forenames>Jianqiu</forenames></author><author><keyname>Zhou</keyname><forenames>Zikun</forenames></author><author><keyname>Li</keyname><forenames>Xin</forenames></author><author><keyname>Zheng</keyname><forenames>Ye</forenames></author><author><keyname>Bao</keyname><forenames>Tianpeng</forenames></author><author><keyname>He</keyname><forenames>Zhenyu</forenames></author></authors><title>ZeroBP: Learning Position-Aware Correspondence for Zero-shot 6D Pose   Estimation in Bin-Picking</title><categories>cs.CV</categories><comments>ICRA 2025</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Bin-picking is a practical and challenging robotic manipulation task, where accurate 6D pose estimation plays a pivotal role. The workpieces in bin-picking are typically textureless and randomly stacked in a bin, which poses a significant challenge to 6D pose estimation. Existing solutions are typically learning-based methods, which require object-specific training. Their efficiency of practical deployment for novel workpieces is highly limited by data collection and model retraining. Zero-shot 6D pose estimation is a potential approach to address the issue of deployment efficiency. Nevertheless, existing zero-shot 6D pose estimation methods are designed to leverage feature matching to establish point-to-point correspondences for pose estimation, which is less effective for workpieces with textureless appearances and ambiguous local regions. In this paper, we propose ZeroBP, a zero-shot pose estimation framework designed specifically for the bin-picking task. ZeroBP learns Position-Aware Correspondence (PAC) between the scene instance and its CAD model, leveraging both local features and global positions to resolve the mismatch issue caused by ambiguous regions with similar shapes and appearances. Extensive experiments on the ROBI dataset demonstrate that ZeroBP outperforms state-of-the-art zero-shot pose estimation methods, achieving an improvement of 9.1% in average recall of correct poses. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01009</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01009</id><created>2025-02-02</created><authors><author><keyname>Pan</keyname><forenames>Lishuo</forenames></author><author><keyname>Catellani</keyname><forenames>Mattia</forenames></author><author><keyname>Sabattini</keyname><forenames>Lorenzo</forenames></author><author><keyname>Ayanian</keyname><forenames>Nora</forenames></author></authors><title>Robust Trajectory Generation and Control for Quadrotor Motion Planning   with Field-of-View Control Barrier Certification</title><categories>cs.RO</categories><comments>13 pages, 10 figures, submitted to RSS 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Many approaches to multi-robot coordination are susceptible to failure due to communication loss and uncertainty in estimation. We present a real-time communication-free distributed algorithm for navigating robots to their desired goals certified by control barrier functions, that model and control the onboard sensing behavior to keep neighbors in the limited field of view for position estimation. The approach is robust to temporary tracking loss and directly synthesizes control in real time to stabilize visual contact through control Lyapunov-barrier functions. The main contributions of this paper are a continuous-time robust trajectory generation and control method certified by control barrier functions for distributed multi-robot systems and a discrete optimization procedure, namely, MPC-CBF, to approximate the certified controller. In addition, we propose a linear surrogate of high-order control barrier function constraints and use sequential quadratic programming to solve MPC-CBF efficiently. We demonstrate results in simulation with 10 robots and physical experiments with 2 custom-built UAVs. To the best of our knowledge, this work is the first of its kind to generate a robust continuous-time trajectory and controller concurrently, certified by control barrier functions utilizing piecewise splines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01012</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01012</id><created>2025-02-02</created><authors><author><keyname>Zhu</keyname><forenames>Haonan</forenames></author><author><keyname>Silva</keyname><forenames>Mary</forenames></author><author><keyname>Cadena</keyname><forenames>Jose</forenames></author><author><keyname>Soper</keyname><forenames>Braden</forenames></author><author><keyname>Lisicki</keyname><forenames>Michał</forenames></author><author><keyname>Peetoom</keyname><forenames>Braian</forenames></author><author><keyname>Baranzini</keyname><forenames>Sergio E.</forenames></author><author><keyname>Sundaram</keyname><forenames>Shivshankar</forenames></author><author><keyname>Ray</keyname><forenames>Priyadip</forenames></author><author><keyname>Drocco</keyname><forenames>Jeff</forenames></author></authors><title>Deep Active Learning based Experimental Design to Uncover Synergistic   Genetic Interactions for Host Targeted Therapeutics</title><categories>cs.LG q-bio.QM stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent technological advances have introduced new high-throughput methods for studying host-virus interactions, but testing synergistic interactions between host gene pairs during infection remains relatively slow and labor intensive. Identification of multiple gene knockdowns that effectively inhibit viral replication requires a search over the combinatorial space of all possible target gene pairs and is infeasible via brute-force experiments. Although active learning methods for sequential experimental design have shown promise, existing approaches have generally been restricted to single-gene knockdowns or small-scale double knockdown datasets. In this study, we present an integrated Deep Active Learning (DeepAL) framework that incorporates information from a biological knowledge graph (SPOKE, the Scalable Precision Medicine Open Knowledge Engine) to efficiently search the configuration space of a large dataset of all pairwise knockdowns of 356 human genes in HIV infection. Through graph representation learning, the framework is able to generate task-specific representations of genes while also balancing the exploration-exploitation trade-off to pinpoint highly effective double-knockdown pairs. We additionally present an ensemble method for uncertainty quantification and an interpretation of the gene pairs selected by our algorithm via pathway analysis. To our knowledge, this is the first work to show promising results on double-gene knockdown experimental data of appreciable scale (356 by 356 matrix). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01013</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01013</id><created>2025-02-02</created><authors><author><keyname>Buban</keyname><forenames>James</forenames></author><author><keyname>Zhang</keyname><forenames>Hongyang</forenames></author><author><keyname>Angione</keyname><forenames>Claudio</forenames></author><author><keyname>Yang</keyname><forenames>Harry</forenames></author><author><keyname>Farhan</keyname><forenames>Ahmad</forenames></author><author><keyname>Sultanov</keyname><forenames>Seyfal</forenames></author><author><keyname>Du</keyname><forenames>Michael</forenames></author><author><keyname>Ma</keyname><forenames>Xuran</forenames></author><author><keyname>Wang</keyname><forenames>Zihao</forenames></author><author><keyname>Zhao</keyname><forenames>Yue</forenames></author><author><keyname>Owlia</keyname><forenames>Arria</forenames></author><author><keyname>Johnston</keyname><forenames>Fielding</forenames></author><author><keyname>Colangelo</keyname><forenames>Patrick</forenames></author></authors><title>Encrypted Large Model Inference: The Equivariant Encryption Paradigm</title><categories>cs.CR cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large scale deep learning model, such as modern language models and diffusion architectures, have revolutionized applications ranging from natural language processing to computer vision. However, their deployment in distributed or decentralized environments raises significant privacy concerns, as sensitive data may be exposed during inference. Traditional techniques like secure multi-party computation, homomorphic encryption, and differential privacy offer partial remedies but often incur substantial computational overhead, latency penalties, or limited compatibility with non-linear network operations. In this work, we introduce Equivariant Encryption (EE), a novel paradigm designed to enable secure, "blind" inference on encrypted data with near zero performance overhead. Unlike fully homomorphic approaches that encrypt the entire computational graph, EE selectively obfuscates critical internal representations within neural network layers while preserving the exact functionality of both linear and a prescribed set of non-linear operations. This targeted encryption ensures that raw inputs, intermediate activations, and outputs remain confidential, even when processed on untrusted infrastructure. We detail the theoretical foundations of EE, compare its performance and integration complexity against conventional privacy preserving techniques, and demonstrate its applicability across a range of architectures, from convolutional networks to large language models. Furthermore, our work provides a comprehensive threat analysis, outlining potential attack vectors and baseline strategies, and benchmarks EE against standard inference pipelines in decentralized settings. The results confirm that EE maintains high fidelity and throughput, effectively bridging the gap between robust data confidentiality and the stringent efficiency requirements of modern, large scale model inference. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01014</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01014</id><created>2025-02-02</created><authors><author><keyname>Shu</keyname><forenames>Yao</forenames></author><author><keyname>Zhang</keyname><forenames>Qixin</forenames></author><author><keyname>He</keyname><forenames>Kun</forenames></author><author><keyname>Dai</keyname><forenames>Zhongxiang</forenames></author></authors><title>Refining Adaptive Zeroth-Order Optimization at Ease</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, zeroth-order (ZO) optimization plays an essential role in scenarios where gradient information is inaccessible or unaffordable, such as black-box systems and resource-constrained environments. While existing adaptive methods such as ZO-AdaMM have shown promise, they are fundamentally limited by their underutilization of moment information during optimization, usually resulting in underperforming convergence. To overcome these limitations, this paper introduces Refined Adaptive Zeroth-Order Optimization (R-AdaZO). Specifically, we first show the untapped variance reduction effect of first moment estimate on ZO gradient estimation, which improves the accuracy and stability of ZO updates. We then refine the second moment estimate based on these variance-reduced gradient estimates to better capture the geometry of the optimization landscape, enabling a more effective scaling of ZO updates. We present rigorous theoretical analysis to show (I) the first analysis to the variance reduction of first moment estimate in ZO optimization, (II) the improved second moment estimates with a more accurate approximation of its variance-free ideal, (III) the first variance-aware convergence framework for adaptive ZO methods, which may be of independent interest, and (IV) the faster convergence of R-AdaZO than existing baselines like ZO-AdaMM. Our extensive experiments, including synthetic problems, black-box adversarial attack, and memory-efficient fine-tuning of large language models (LLMs), further verify the superior convergence of R-AdaZO, indicating that R-AdaZO offers an improved solution for real-world ZO optimization challenges. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01015</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01015</id><created>2025-02-02</created><authors><author><keyname>Zeng</keyname><forenames>Siqi</forenames></author><author><keyname>He</keyname><forenames>Yifei</forenames></author><author><keyname>You</keyname><forenames>Weiqiu</forenames></author><author><keyname>Hao</keyname><forenames>Yifan</forenames></author><author><keyname>Tsai</keyname><forenames>Yao-Hung Hubert</forenames></author><author><keyname>Yamada</keyname><forenames>Makoto</forenames></author><author><keyname>Zhao</keyname><forenames>Han</forenames></author></authors><title>Efficient Model Editing with Task Vector Bases: A Theoretical Framework   and Scalable Approach</title><categories>cs.LG</categories><comments>25 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Task vectors, which are derived from the difference between pre-trained and fine-tuned model weights, enable flexible task adaptation and model merging through arithmetic operations such as addition and negation. However, existing approaches often rely on heuristics with limited theoretical support, often leading to performance gaps comparing to direct task fine tuning. Meanwhile, although it is easy to manipulate saved task vectors with arithmetic for different purposes, such compositional flexibility demands high memory usage, especially when dealing with a huge number of tasks, limiting scalability. This work addresses these issues with a theoretically grounded framework that explains task vector arithmetic and introduces the task vector bases framework. Building upon existing task arithmetic literature, our method significantly reduces the memory cost for downstream arithmetic with little effort, while achieving competitive performance and maintaining compositional advantage, providing a practical solution for large-scale task arithmetic. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01020</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01020</id><created>2025-02-02</created><authors><author><keyname>Basak</keyname><forenames>Setu Kumar</forenames></author><author><keyname>Pardeshi</keyname><forenames>Tanmay</forenames></author><author><keyname>Reaves</keyname><forenames>Bradley</forenames></author><author><keyname>Williams</keyname><forenames>Laurie</forenames></author></authors><title>RiskHarvester: A Risk-based Tool to Prioritize Secret Removal Efforts in   Software Artifacts</title><categories>cs.CR cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since 2020, GitGuardian has been detecting checked-in hard-coded secrets in GitHub repositories. During 2020-2023, GitGuardian has observed an upward annual trend and a four-fold increase in hard-coded secrets, with 12.8 million exposed in 2023. However, removing all the secrets from software artifacts is not feasible due to time constraints and technical challenges. Additionally, the security risks of the secrets are not equal, protecting assets ranging from obsolete databases to sensitive medical data. Thus, secret removal should be prioritized by security risk reduction, which existing secret detection tools do not support. The goal of this research is to aid software practitioners in prioritizing secrets removal efforts through our security risk-based tool. We present RiskHarvester, a risk-based tool to compute a security risk score based on the value of the asset and ease of attack on a database. We calculated the value of asset by identifying the sensitive data categories present in a database from the database keywords in the source code. We utilized data flow analysis, SQL, and ORM parsing to identify the database keywords. To calculate the ease of attack, we utilized passive network analysis to retrieve the database host information. To evaluate RiskHarvester, we curated RiskBench, a benchmark of 1,791 database secret-asset pairs with sensitive data categories and host information manually retrieved from 188 GitHub repositories. RiskHarvester demonstrates precision of (95%) and recall (90%) in detecting database keywords for the value of asset and precision of (96%) and recall (94%) in detecting valid hosts for ease of attack. Finally, we conducted a survey (52 respondents) to understand whether developers prioritize secret removal based on security risk score. We found that 86% of the developers prioritized the secrets for removal with descending security risk scores. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01023</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01023</id><created>2025-02-02</created><authors><author><keyname>Kim</keyname><forenames>Taechang</forenames></author><author><keyname>Ji</keyname><forenames>Sooyeon</forenames></author><author><keyname>Min</keyname><forenames>Kyeongseon</forenames></author><author><keyname>Kim</keyname><forenames>Minjun</forenames></author><author><keyname>Youn</keyname><forenames>Jonghyo</forenames></author><author><keyname>Oh</keyname><forenames>Chungseok</forenames></author><author><keyname>Kim</keyname><forenames>Jiye</forenames></author><author><keyname>Lee</keyname><forenames>Jongho</forenames></author></authors><title>Vessel segmentation for X-separation</title><categories>cs.CV q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $\chi$-separation is an advanced quantitative susceptibility mapping (QSM) method that is designed to generate paramagnetic ($\chi_{para}$) and diamagnetic ($|\chi_{dia}|$) susceptibility maps, reflecting the distribution of iron and myelin in the brain. However, vessels have shown artifacts, interfering with the accurate quantification of iron and myelin in applications. To address this challenge, a new vessel segmentation method for $\chi$-separation is developed. The method comprises three steps: 1) Seed generation from $\textit{R}_2^*$ and the product of $\chi_{para}$ and $|\chi_{dia}|$ maps; 2) Region growing, guided by vessel geometry, creating a vessel mask; 3) Refinement of the vessel mask by excluding non-vessel structures. The performance of the method was compared to conventional vessel segmentation methods both qualitatively and quantitatively. To demonstrate the utility of the method, it was tested in two applications: quantitative evaluation of a neural network-based $\chi$-separation reconstruction method ($\chi$-sepnet-$\textit{R}_2^*$) and population-averaged region of interest (ROI) analysis. The proposed method demonstrates superior performance to the conventional vessel segmentation methods, effectively excluding the non-vessel structures, achieving the highest Dice score coefficient. For the applications, applying vessel masks report notable improvements for the quantitative evaluation of $\chi$-sepnet-$\textit{R}_2^*$ and statistically significant differences in population-averaged ROI analysis. These applications suggest excluding vessels when analyzing the $\chi$-separation maps provide more accurate evaluations. The proposed method has the potential to facilitate various applications, offering reliable analysis through the generation of a high-quality vessel mask. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01025</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01025</id><created>2025-02-02</created><authors><author><keyname>Xie</keyname><forenames>Roy</forenames></author><author><keyname>Wang</keyname><forenames>Junlin</forenames></author><author><keyname>Rosu</keyname><forenames>Paul</forenames></author><author><keyname>Deng</keyname><forenames>Chunyuan</forenames></author><author><keyname>Sun</keyname><forenames>Bolun</forenames></author><author><keyname>Lin</keyname><forenames>Zihao</forenames></author><author><keyname>Dhingra</keyname><forenames>Bhuwan</forenames></author></authors><title>Knowing When to Stop: Dynamic Context Cutoff for Large Language Models</title><categories>cs.CL</categories><comments>Project Website: https://royxie.com/when-to-stop-project/</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Large language models (LLMs) process entire input contexts indiscriminately, which is inefficient in cases where the information required to answer a query is localized within the context. We present dynamic context cutoff, a human-inspired method enabling LLMs to self-terminate processing upon acquiring sufficient task-relevant information. Through analysis of model internals, we discover that specific attention heads inherently encode "sufficiency signals" - detectable through lightweight classifiers - that predict when critical information has been processed. This reveals a new efficiency paradigm: models' internal understanding naturally dictates processing needs rather than external compression heuristics. Comprehensive experiments across six QA datasets (up to 40K tokens) with three model families (LLaMA/Qwen/Mistral, 1B0-70B) demonstrate 1.33x average token reduction while improving accuracy by 1.3%. Furthermore, our method demonstrates better performance with the same rate of token reduction compared to other context efficiency methods. Additionally, we observe an emergent scaling phenomenon: while smaller models require require probing for sufficiency detection, larger models exhibit intrinsic self-assessment capabilities through prompting. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01027</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01027</id><created>2025-02-02</created><authors><author><keyname>Montreuil</keyname><forenames>Yannis</forenames></author><author><keyname>Carlier</keyname><forenames>Axel</forenames></author><author><keyname>Ng</keyname><forenames>Lai Xing</forenames></author><author><keyname>Ooi</keyname><forenames>Wei Tsang</forenames></author></authors><title>Adversarial Robustness in Two-Stage Learning-to-Defer: Algorithms and   Guarantees</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Learning-to-Defer (L2D) facilitates optimal task allocation between AI systems and decision-makers. Despite its potential, we show that current two-stage L2D frameworks are highly vulnerable to adversarial attacks, which can misdirect queries or overwhelm decision agents, significantly degrading system performance. This paper conducts the first comprehensive analysis of adversarial robustness in two-stage L2D frameworks. We introduce two novel attack strategies -- untargeted and targeted -- that exploit inherent structural vulnerabilities in these systems. To mitigate these threats, we propose SARD, a robust, convex, deferral algorithm rooted in Bayes and $(\mathcal{R},\mathcal{G})$-consistency. Our approach guarantees optimal task allocation under adversarial perturbations for all surrogates in the cross-entropy family. Extensive experiments on classification, regression, and multi-task benchmarks validate the robustness of SARD. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01029</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01029</id><created>2025-02-02</created><authors><author><keyname>Ma</keyname><forenames>Jiangqin</forenames></author><author><keyname>Mahmoudinia</keyname><forenames>Erfan</forenames></author></authors><title>Comprehensive Modeling Approaches for Forecasting Bitcoin Transaction   Fees: A Comparative Study</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Transaction fee prediction in Bitcoin's ecosystem represents a crucial challenge affecting both user costs and miner revenue optimization. This study presents a systematic evaluation of six predictive models for forecasting Bitcoin transaction fees across a 24-hour horizon (144 blocks): SARIMAX, Prophet, Time2Vec, Time2Vec with Attention, a Hybrid model combining SARIMAX with Gradient Boosting, and the Temporal Fusion Transformer (TFT). Our approach integrates comprehensive feature engineering spanning mempool metrics, network parameters, and historical fee patterns to capture the multifaceted dynamics of fee behavior.   Through rigorous 5-fold cross-validation and independent testing, our analysis reveals that traditional statistical approaches outperform more complex deep learning architectures. The SARIMAX model achieves superior accuracy on the independent test set, while Prophet demonstrates strong performance during cross-validation. Notably, sophisticated deep learning models like Time2Vec and TFT show comparatively lower predictive power despite their architectural complexity. This performance disparity likely stems from the relatively constrained training dataset of 91 days, suggesting that deep learning models may achieve enhanced results with extended historical data.   These findings offer significant practical implications for cryptocurrency stakeholders, providing empirically-validated guidance for fee-sensitive decision making while illuminating critical considerations in model selection based on data constraints. The study establishes a foundation for advanced fee prediction while highlighting the current advantages of traditional statistical methods in this domain. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01031</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01031</id><created>2025-02-02</created><authors><author><keyname>Lee</keyname><forenames>Junghun</forenames></author><author><keyname>Kim</keyname><forenames>Hyunju</forenames></author><author><keyname>Bu</keyname><forenames>Fanchen</forenames></author><author><keyname>Ko</keyname><forenames>Jihoon</forenames></author><author><keyname>Shin</keyname><forenames>Kijung</forenames></author></authors><title>DiffIM: Differentiable Influence Minimization with Surrogate Modeling   and Continuous Relaxation</title><categories>cs.LG cs.SI</categories><comments>Accepted to AAAI'25</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In social networks, people influence each other through social links, which can be represented as propagation among nodes in graphs. Influence minimization (IMIN) is the problem of manipulating the structures of an input graph (e.g., removing edges) to reduce the propagation among nodes. IMIN can represent time-critical real-world applications, such as rumor blocking, but IMIN is theoretically difficult and computationally expensive. Moreover, the discrete nature of IMIN hinders the usage of powerful machine learning techniques, which requires differentiable computation. In this work, we propose DiffIM, a novel method for IMIN with two differentiable schemes for acceleration: (1) surrogate modeling for efficient influence estimation, which avoids time-consuming simulations (e.g., Monte Carlo), and (2) the continuous relaxation of decisions, which avoids the evaluation of individual discrete decisions (e.g., removing an edge). We further propose a third accelerating scheme, gradient-driven selection, that chooses edges instantly based on gradients without optimization (spec., gradient descent iterations) on each test instance. Through extensive experiments on real-world graphs, we show that each proposed scheme significantly improves speed with little (or even no) IMIN performance degradation. Our method is Pareto-optimal (i.e., no baseline is faster and more effective than it) and typically several orders of magnitude (spec., up to 15,160X) faster than the most effective baseline while being more effective. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01032</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01032</id><created>2025-02-02</created><authors><author><keyname>Belrose</keyname><forenames>Nora</forenames></author><author><keyname>Rigg</keyname><forenames>Alice</forenames></author></authors><title>Converting MLPs into Polynomials in Closed Form</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent work has shown that purely quadratic functions can replace MLPs in transformers with no significant loss in performance, while enabling new methods of interpretability based on linear algebra. In this work, we theoretically derive closed-form least-squares optimal approximations of feedforward networks (multilayer perceptrons and gated linear units) using polynomial functions of arbitrary degree. When the $R^2$ is high, this allows us to interpret MLPs and GLUs by visualizing the eigendecomposition of the coefficients of their linear and quadratic approximants. We also show that these approximants can be used to create SVD-based adversarial examples. By tracing the $R^2$ of linear and quadratic approximants across training time, we find new evidence that networks start out simple, and get progressively more complex. Even at the end of training, however, our quadratic approximants explain over 95% of the variance in network outputs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01033</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01033</id><created>2025-02-02</created><authors><author><keyname>Liu</keyname><forenames>Zequan</forenames></author><author><keyname>Zhao</keyname><forenames>Yi</forenames></author><author><keyname>Tan</keyname><forenames>Ming</forenames></author><author><keyname>Zhu</keyname><forenames>Wei</forenames></author><author><keyname>Tian</keyname><forenames>Aaron Xuxiang</forenames></author></authors><title>PARA: Parameter-Efficient Fine-tuning with Prompt Aware Representation   Adjustment</title><categories>cs.CL</categories><comments>accepted by ACL-2024</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the realm of parameter-efficient fine-tuning (PEFT) methods, while options like LoRA are available, there is a persistent demand in the industry for a PEFT approach that excels in both efficiency and performance within the context of single-backbone multi-tenant applications. This paper introduces a new and straightforward PEFT technique, termed \underline{P}rompt \underline{A}ware \underline{R}epresentation \underline{A}djustment (PARA). The core of our proposal is to integrate a lightweight vector generator within each Transformer layer. This generator produces vectors that are responsive to input prompts, thereby adjusting the hidden representations accordingly. Our extensive experimentation across diverse tasks has yielded promising results. Firstly, the PARA method has been shown to surpass current PEFT benchmarks in terms of performance, despite having a similar number of adjustable parameters. Secondly, it has proven to be more efficient than LoRA in the single-backbone multi-tenant scenario, highlighting its significant potential for industrial adoption. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01034</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01034</id><created>2025-02-02</created><authors><author><keyname>Quinn</keyname><forenames>Patrick</forenames></author><author><keyname>Nehma</keyname><forenames>George</forenames></author><author><keyname>Tiwari</keyname><forenames>Madhur</forenames></author></authors><title>End-to-End Imitation Learning for Optimal Asteroid Proximity Operations</title><categories>cs.RO cs.LG</categories><comments>7 pages, 8 figures. Submitted to the 2025 IEEE Aerospace Conference</comments><acm-class>I.2.9</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Controlling spacecraft near asteroids in deep space comes with many challenges. The delays involved necessitate heavy usage of limited onboard computation resources while fuel efficiency remains a priority to support the long loiter times needed for gathering data. Additionally, the difficulty of state determination due to the lack of traditional reference systems requires a guidance, navigation, and control (GNC) pipeline that ideally is both computationally and fuel-efficient, and that incorporates a robust state determination system. In this paper, we propose an end-to-end algorithm utilizing neural networks to generate near-optimal control commands from raw sensor data, as well as a hybrid model predictive control (MPC) guided imitation learning controller delivering improvements in computational efficiency over a traditional MPC controller. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01035</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01035</id><created>2025-02-02</created><authors><author><keyname>Xiao</keyname><forenames>Jiuhong</forenames></author><author><keyname>Loianno</keyname><forenames>Giuseppe</forenames></author></authors><title>UASTHN: Uncertainty-Aware Deep Homography Estimation for UAV   Satellite-Thermal Geo-localization</title><categories>cs.RO cs.CV</categories><comments>7 pages, 6 figures, accepted at ICRA 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Geo-localization is an essential component of Unmanned Aerial Vehicle (UAV) navigation systems to ensure precise absolute self-localization in outdoor environments. To address the challenges of GPS signal interruptions or low illumination, Thermal Geo-localization (TG) employs aerial thermal imagery to align with reference satellite maps to accurately determine the UAV's location. However, existing TG methods lack uncertainty measurement in their outputs, compromising system robustness in the presence of textureless or corrupted thermal images, self-similar or outdated satellite maps, geometric noises, or thermal images exceeding satellite maps. To overcome these limitations, this paper presents \textit{UASTHN}, a novel approach for Uncertainty Estimation (UE) in Deep Homography Estimation (DHE) tasks for TG applications. Specifically, we introduce a novel Crop-based Test-Time Augmentation (CropTTA) strategy, which leverages the homography consensus of cropped image views to effectively measure data uncertainty. This approach is complemented by Deep Ensembles (DE) employed for model uncertainty, offering comparable performance with improved efficiency and seamless integration with any DHE model. Extensive experiments across multiple DHE models demonstrate the effectiveness and efficiency of CropTTA in TG applications. Analysis of detected failure cases underscores the improved reliability of CropTTA under challenging conditions. Finally, we demonstrate the capability of combining CropTTA and DE for a comprehensive assessment of both data and model uncertainty. Our research provides profound insights into the broader intersection of localization and uncertainty estimation. The code and data is publicly available. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01036</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01036</id><created>2025-02-02</created><authors><author><keyname>Fujimoto</keyname><forenames>Takumi</forenames></author><author><keyname>Nishi</keyname><forenames>Hiroaki</forenames></author></authors><title>eagle: early approximated gradient based learning rate estimator</title><categories>cs.LG cs.AI</categories><comments>43pages, 24figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose EAGLE update rule, a novel optimization method that accelerates loss convergence during the early stages of training by leveraging both current and previous step parameter and gradient values. The update algorithm estimates optimal parameters by computing the changes in parameters and gradients between consecutive training steps and leveraging the local curvature of the loss landscape derived from these changes. However, this update rule has potential instability, and to address that, we introduce an adaptive switching mechanism that dynamically selects between Adam and EAGLE update rules to enhance training stability. Experiments on standard benchmark datasets demonstrate that EAGLE optimizer, which combines this novel update rule with the switching mechanism achieves rapid training loss convergence with fewer epochs, compared to conventional optimization methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01039</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01039</id><created>2025-02-02</created><authors><author><keyname>Austin-Gabriel</keyname><forenames>Blessing</forenames></author><author><keyname>Varde</keyname><forenames>Aparna S.</forenames></author><author><keyname>Liu</keyname><forenames>Hao</forenames></author></authors><title>Geoinformatics-Guided Machine Learning for Power Plant Classification</title><categories>cs.LG</categories><acm-class>I.2.6; I.2.m; J.2</acm-class><journal-ref>AAAI 2025 Conference Bridge Program</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper proposes an approach in the area of Knowledge-Guided Machine Learning (KGML) via a novel integrated framework comprising CNN (Convolutional Neural Networks) and ViT (Vision Transformers) along with GIS (Geographic Information Systems) to enhance power plant classification in the context of energy management. Knowledge from geoinformatics derived through Spatial Masks (SM) in GIS is infused into an architecture of CNN and ViT, in this proposed KGML approach. It is found to provide much better performance compared to the baseline of CNN and ViT only in the classification of multiple types of power plants from real satellite imagery, hence emphasizing the vital role of the geoinformatics-guided approach. This work makes a contribution to the main theme of KGML that can be beneficial in many AI systems today. It makes broader impacts on AI in Smart Cities, and Environmental Computing. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01041</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01041</id><created>2025-02-02</created><authors><author><keyname>Jeong</keyname><forenames>Mingi</forenames></author><author><keyname>Molinaro</keyname><forenames>Cristian</forenames></author><author><keyname>Deb</keyname><forenames>Tonmoay</forenames></author><author><keyname>Zhang</keyname><forenames>Youzhi</forenames></author><author><keyname>Pugliese</keyname><forenames>Andrea</forenames></author><author><keyname>Santos</keyname><forenames>Eugene</forenames><suffix>Jr.</suffix></author><author><keyname>Subrahmanian</keyname><forenames>VS</forenames></author><author><keyname>Li</keyname><forenames>Alberto Quattrini</forenames></author></authors><title>Multi-Object Active Search and Tracking by Multiple Agents in Untrusted,   Dynamically Changing Environments</title><categories>cs.RO</categories><comments>Submitted to Autonomous Robots</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper addresses the problem of both actively searching and tracking multiple unknown dynamic objects in a known environment with multiple cooperative autonomous agents with partial observability. The tracking of a target ends when the uncertainty is below a threshold. Current methods typically assume homogeneous agents without access to external information and utilize short-horizon target predictive models. Such assumptions limit real-world applications. We propose a fully integrated pipeline where the main contributions are: (1) a time-varying weighted belief representation capable of handling knowledge that changes over time, which includes external reports of varying levels of trustworthiness in addition to the agents; (2) the integration of a Long Short Term Memory-based trajectory prediction within the optimization framework for long-horizon decision-making, which reasons in time-configuration space, thus increasing responsiveness; and (3) a comprehensive system that accounts for multiple agents and enables information-driven optimization. When communication is available, our strategy consolidates exploration results collected asynchronously by agents and external sources into a headquarters, who can allocate each agent to maximize the overall team's utility, using all available information. We tested our approach extensively in simulations against baselines, and in robustness and ablation studies. In addition, we performed experiments in a 3D physics based engine robot simulator to test the applicability in the real world, as well as with real-world trajectories obtained from an oceanography computational fluid dynamics simulator. Results show the effectiveness of our method, which achieves mission completion times 1.3 to 3.2 times faster in finding all targets, even under the most challenging scenarios where the number of targets is 5 times greater than that of the agents. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01042</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01042</id><created>2025-02-02</created><authors><author><keyname>Han</keyname><forenames>Peixuan</forenames></author><author><keyname>Qian</keyname><forenames>Cheng</forenames></author><author><keyname>Chen</keyname><forenames>Xiusi</forenames></author><author><keyname>Zhang</keyname><forenames>Yuji</forenames></author><author><keyname>Zhang</keyname><forenames>Denghui</forenames></author><author><keyname>Ji</keyname><forenames>Heng</forenames></author></authors><title>Internal Activation as the Polar Star for Steering Unsafe LLM Behavior</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks but also pose significant risks due to their potential to generate harmful content. Although existing safety mechanisms can improve model safety, they often lead to overly cautious behavior and fail to fully utilize LLMs' internal cognitive processes. Drawing inspiration from cognitive science, where humans rely on reflective reasoning (System 2 thinking) to regulate language and behavior, we empirically demonstrate that LLMs also possess a similar capacity for internal assessment and regulation, which can be actively detected.   Building on this insight, we introduce SafeSwitch, a framework that dynamically regulates unsafe outputs by monitoring and utilizing the model's internal states. Our empirical results show that SafeSwitch reduces harmful outputs by over 80% on safety benchmarks while maintaining strong utility. Compared to traditional safety alignment methods, SafeSwitch delivers more informative and context-aware refusals, demonstrates resilience to unseen queries, and achieves these benefits while only tuning less than 6% of the original parameters. These features make SafeSwitch a promising approach for implementing nuanced safety controls in LLMs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01043</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01043</id><created>2025-02-02</created><authors><author><keyname>Altahhan</keyname><forenames>Muhammad Ramzy</forenames></author><author><keyname>Munday</keyname><forenames>Lynn</forenames></author><author><keyname>Azmy</keyname><forenames>Yousry</forenames></author></authors><title>Multiphysics Continuous Shape Optimization of the TAP Reactor Components</title><categories>cs.CE</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The Transatomic Power (TAP) reactor has an unusual design for a molten salt reactor technology, building upon the foundation laid by the Molten Salt Reactor Experiment (MSRE). This design introduces three key modifications to enhance efficiency and compactness: a revised fuel salt composition, an alternative moderator material, and moderator pins surrounded by the molten salt fuel. Unlike traditional solid-fueled reactors that rely on excess positive reactivity at the beginning of life, the TAP concept employs a dynamic approach. The core's design, featuring a cylindrical geometry with square assemblies of moderator rods surrounded by flowing fuel salt, provides flexibility in adjusting the moderator-to-fuel ratio during operation - using movable moderator rods - further adding criticality control capability in addition to the control rods system. Shape optimization of the core can play a crucial role in enhancing performance and efficiency. By applying multiphysics continuous shape optimization techniques to key components, such as the unit cells of the TAP reactor or its moderator assemblies, we can fine-tune the reactor's geometry to achieve optimal performance in key physics like neutronics and thermal hydraulics. We explore this aspect using the optimization module in the Multiphysics Object Oriented Simulation Environment (MOOSE) framework which allows for multiphysics continuous shape optimization. The results reported here illustrate the benefits of applying continuous shape optimization in the design of nuclear reactor components and can help in extending the TAP reactor's performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01044</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01044</id><created>2025-02-02</created><authors><author><keyname>Sung</keyname><forenames>Kijin</forenames></author><author><keyname>Hoshino</keyname><forenames>Kenta</forenames></author><author><keyname>Honda</keyname><forenames>Akihiko</forenames></author><author><keyname>Shima</keyname><forenames>Takeya</forenames></author><author><keyname>Ohtsuka</keyname><forenames>Toshiyuki</forenames></author></authors><title>Nonlinear receding-horizon differential game for drone racing along a   three-dimensional path</title><categories>eess.SY cs.SY</categories><comments>16 pages, 10 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Drone racing involves high-speed navigation of three-dimensional paths, posing a substantial challenge in control engineering. This study presents a game-theoretic control framework, the nonlinear receding-horizon differential game (NRHDG), designed for competitive drone racing. NRHDG enhances robustness in adversarial settings by predicting and countering an opponent's worst-case behavior in real time. It extends standard nonlinear model predictive control (NMPC), which otherwise assumes a fixed opponent model. First, we develop a novel path-following formulation based on projection point dynamics, eliminating the need for costly distance minimization. Second, we propose a potential function that allows each drone to switch between overtaking and obstructing maneuvers based on real-time race situations. Third, we establish a new performance metric to evaluate NRHDG with NMPC under race scenarios. Simulation results demonstrate that NRHDG outperforms NMPC in terms of both overtaking efficiency and obstructing capabilities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01045</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01045</id><created>2025-02-02</created><authors><author><keyname>Wang</keyname><forenames>Zilong</forenames></author><author><keyname>Dou</keyname><forenames>Zhiyang</forenames></author><author><keyname>Liu</keyname><forenames>Yuan</forenames></author><author><keyname>Lin</keyname><forenames>Cheng</forenames></author><author><keyname>Dong</keyname><forenames>Xiao</forenames></author><author><keyname>Guo</keyname><forenames>Yunhui</forenames></author><author><keyname>Zhang</keyname><forenames>Chenxu</forenames></author><author><keyname>Li</keyname><forenames>Xin</forenames></author><author><keyname>Wang</keyname><forenames>Wenping</forenames></author><author><keyname>Guo</keyname><forenames>Xiaohu</forenames></author></authors><title>WonderHuman: Hallucinating Unseen Parts in Dynamic 3D Human   Reconstruction</title><categories>cs.CV cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present WonderHuman to reconstruct dynamic human avatars from a monocular video for high-fidelity novel view synthesis. Previous dynamic human avatar reconstruction methods typically require the input video to have full coverage of the observed human body. However, in daily practice, one typically has access to limited viewpoints, such as monocular front-view videos, making it a cumbersome task for previous methods to reconstruct the unseen parts of the human avatar. To tackle the issue, we present WonderHuman, which leverages 2D generative diffusion model priors to achieve high-quality, photorealistic reconstructions of dynamic human avatars from monocular videos, including accurate rendering of unseen body parts. Our approach introduces a Dual-Space Optimization technique, applying Score Distillation Sampling (SDS) in both canonical and observation spaces to ensure visual consistency and enhance realism in dynamic human reconstruction. Additionally, we present a View Selection strategy and Pose Feature Injection to enforce the consistency between SDS predictions and observed data, ensuring pose-dependent effects and higher fidelity in the reconstructed avatar. In the experiments, our method achieves SOTA performance in producing photorealistic renderings from the given monocular video, particularly for those challenging unseen parts. The project page and source code can be found at https://wyiguanw.github.io/WonderHuman/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01046</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01046</id><created>2025-02-02</created><authors><author><keyname>Ye</keyname><forenames>Jiaxin</forenames></author><author><keyname>Cao</keyname><forenames>Boyuan</forenames></author><author><keyname>Shan</keyname><forenames>Hongming</forenames></author></authors><title>Emotional Face-to-Speech</title><categories>cs.SD cs.CV eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How much can we infer about an emotional voice solely from an expressive face? This intriguing question holds great potential for applications such as virtual character dubbing and aiding individuals with expressive language disorders. Existing face-to-speech methods offer great promise in capturing identity characteristics but struggle to generate diverse vocal styles with emotional expression. In this paper, we explore a new task, termed emotional face-to-speech, aiming to synthesize emotional speech directly from expressive facial cues. To that end, we introduce DEmoFace, a novel generative framework that leverages a discrete diffusion transformer (DiT) with curriculum learning, built upon a multi-level neural audio codec. Specifically, we propose multimodal DiT blocks to dynamically align text and speech while tailoring vocal styles based on facial emotion and identity. To enhance training efficiency and generation quality, we further introduce a coarse-to-fine curriculum learning algorithm for multi-level token processing. In addition, we develop an enhanced predictor-free guidance to handle diverse conditioning scenarios, enabling multi-conditional generation and disentangling complex attributes effectively. Extensive experimental results demonstrate that DEmoFace generates more natural and consistent speech compared to baselines, even surpassing speech-driven methods. Demos are shown at https://demoface-ai.github.io/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01048</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01048</id><created>2025-02-02</created><authors><author><keyname>Fel</keyname><forenames>Thomas</forenames></author></authors><title>Sparks of Explainability: Recent Advancements in Explaining Large Vision   Models</title><categories>cs.CV cs.AI</categories><comments>Doctoral thesis</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This thesis explores advanced approaches to improve explainability in computer vision by analyzing and modeling the features exploited by deep neural networks. Initially, it evaluates attribution methods, notably saliency maps, by introducing a metric based on algorithmic stability and an approach utilizing Sobol indices, which, through quasi-Monte Carlo sequences, allows a significant reduction in computation time. In addition, the EVA method offers a first formulation of attribution with formal guarantees via verified perturbation analysis.   Experimental results indicate that in complex scenarios these methods do not provide sufficient understanding, particularly because they identify only "where" the model focuses without clarifying "what" it perceives. Two hypotheses are therefore examined: aligning models with human reasoning -- through the introduction of a training routine that integrates the imitation of human explanations and optimization within the space of 1-Lipschitz functions -- and adopting a conceptual explainability approach.   The CRAFT method is proposed to automate the extraction of the concepts used by the model and to assess their importance, complemented by MACO, which enables their visualization. These works converge towards a unified framework, illustrated by an interactive demonstration applied to the 1000 ImageNet classes in a ResNet model. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01049</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01049</id><created>2025-02-02</created><authors><author><keyname>Odiathevar</keyname><forenames>Murugaraj</forenames></author><author><keyname>Yup</keyname><forenames>Kim Chung</forenames></author></authors><title>Simulating Application Behavior for Network Monitoring and Security</title><categories>cs.NI stat.AP</categories><comments>4 pages, 6 figures, source code, github</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Existing network simulations often rely on simplistic models that send packets at random intervals, failing to capture the critical role of application-level behaviour. This paper presents a statistical approach that extracts and models application behaviour using probability density functions to generate realistic network simulations. By convolving learned application patterns, the framework produces dynamic, scalable traffic representations that closely mimic real-world networks. The method enables rigorous testing of network monitoring tools and anomaly detection systems by dynamically adjusting application behaviour. It is lightweight, capable of running multiple emulated applications on a single machine, and scalable for analysing large networks where real data collection is impractical. To encourage adoption and further testing, the full code is provided as open-source, allowing researchers and practitioners to replicate and extend the framework for diverse network environments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01050</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01050</id><created>2025-02-02</created><authors><author><keyname>Zhang</keyname><forenames>Haoxiang</forenames><affiliation>Allen</affiliation></author><author><keyname>Liu</keyname><forenames>Yurong</forenames><affiliation>Allen</affiliation></author><author><keyname>Wei-Lun</keyname><affiliation>Allen</affiliation></author><author><keyname>Hung</keyname></author><author><keyname>Santos</keyname><forenames>Aécio</forenames></author><author><keyname>Freire</keyname><forenames>Juliana</forenames></author></authors><title>AutoDDG: Automated Dataset Description Generation using Large Language   Models</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of datasets across open data portals and enterprise data lakes presents an opportunity for deriving data-driven insights. However, widely-used dataset search systems rely on keyword searches over dataset metadata, including descriptions, to facilitate discovery. When these descriptions are incomplete, missing, or inconsistent with dataset contents, findability is severely hindered. In this paper, we address the problem of automatic dataset description generation: how to generate informative descriptions that enhance dataset discovery and support relevance assessment. We introduce AutoDDG, a framework for automated dataset description generation tailored for tabular data. To derive descriptions that are comprehensive, accurate, readable and concise, AutoDDG adopts a data-driven approach to summarize the contents of a dataset, and leverages LLMs to both enrich the summaries with semantic information and to derive human-readable descriptions. An important challenge for this problem is how to evaluate the effectiveness of methods for data description generation and the quality of the descriptions. We propose a multi-pronged evaluation strategy that: (1) measures the improvement in dataset retrieval within a dataset search engine, (2) compares generated descriptions to existing ones (when available), and (3) evaluates intrinsic quality metrics such as readability, faithfulness to the data, and conciseness. Additionally, we introduce two new benchmarks to support this evaluation. Our experimental results, using these benchmarks, demonstrate that AutoDDG generates high-quality, accurate descriptions and significantly improves dataset retrieval performance across diverse use cases. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01051</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01051</id><created>2025-02-02</created><authors><author><keyname>Zhang</keyname><forenames>Tao</forenames></author><author><keyname>Da</keyname><forenames>Cheng</forenames></author><author><keyname>Ding</keyname><forenames>Kun</forenames></author><author><keyname>Jin</keyname><forenames>Kun</forenames></author><author><keyname>Li</keyname><forenames>Yan</forenames></author><author><keyname>Gao</keyname><forenames>Tingting</forenames></author><author><keyname>Zhang</keyname><forenames>Di</forenames></author><author><keyname>Xiang</keyname><forenames>Shiming</forenames></author><author><keyname>Pan</keyname><forenames>Chunhong</forenames></author></authors><title>Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level   Preference Optimization</title><categories>cs.CV</categories><comments>20 pages, 14 tables, 15 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Preference optimization for diffusion models aims to align them with human preferences for images. Previous methods typically leverage Vision-Language Models (VLMs) as pixel-level reward models to approximate human preferences. However, when used for step-level preference optimization, these models face challenges in handling noisy images of different timesteps and require complex transformations into pixel space. In this work, we demonstrate that diffusion models are inherently well-suited for step-level reward modeling in the latent space, as they can naturally extract features from noisy latent images. Accordingly, we propose the Latent Reward Model (LRM), which repurposes components of diffusion models to predict preferences of latent images at various timesteps. Building on LRM, we introduce Latent Preference Optimization (LPO), a method designed for step-level preference optimization directly in the latent space. Experimental results indicate that LPO not only significantly enhances performance in aligning diffusion models with general, aesthetic, and text-image alignment preferences, but also achieves 2.5-28$\times$ training speedup compared to existing preference optimization methods. Our code will be available at https://github.com/casiatao/LPO. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01053</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01053</id><created>2025-02-02</created><authors><author><keyname>T</keyname><forenames>Rehannara Beegum</forenames></author><author><keyname>Idris</keyname><forenames>Mohd Yamani Idna</forenames></author><author><keyname>Ayub</keyname><forenames>Mohamad Nizam Bin</forenames></author><author><keyname>Shehadeh</keyname><forenames>Hisham A</forenames></author><author><keyname>Ali</keyname><forenames>Usman</forenames></author></authors><title>Hybrid Firefly Algorithm and Sperm Swarm Optimization Algorithm using   Newton-Raphson Method (HFASSON) and its application in CR-VANET</title><categories>cs.NE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper proposes a new hybrid algorithm, combining FA, SSO, and the N-R method to accelerate convergence towards global optima, named the Hybrid Firefly Algorithm and Sperm Swarm Optimization with Newton-Raphson (HFASSON). The performance of HFASSON is evaluated using 23 benchmark functions from the CEC 2017 suite, tested in 30, 50, and 100 dimensions. A statistical comparison is performed to assess the effectiveness of HFASSON against FA, SSO, HFASSO, and five hybrid algorithms: Water Cycle Moth Flame Optimization (WCMFO), Hybrid Particle Swarm Optimization and Genetic Algorithm (HPSOGA), Hybrid Sperm Swarm Optimization and Gravitational Search Algorithm (HSSOGSA), Grey Wolf and Cuckoo Search Algorithm (GWOCS), and Hybrid Firefly Genetic Algorithm (FAGA). Results from the Friedman rank test show the superior performance of HFASSON. Additionally, HFASSON is applied to Cognitive Radio Vehicular Ad-hoc Networks (CR-VANET), outperforming basic CR-VANET in spectrum utilization. These findings demonstrate HFASSON's efficiency in wireless network applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01055</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01055</id><created>2025-02-03</created><authors><author><keyname>Li</keyname><forenames>Yulin</forenames></author><author><keyname>Han</keyname><forenames>Haoyu</forenames></author><author><keyname>Kang</keyname><forenames>Shucheng</forenames></author><author><keyname>Ma</keyname><forenames>Jun</forenames></author><author><keyname>Yang</keyname><forenames>Heng</forenames></author></authors><title>On the Surprising Robustness of Sequential Convex Optimization for   Contact-Implicit Motion Planning</title><categories>math.OC cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contact-implicit motion planning-embedding contact sequencing as implicit complementarity constraints-holds the promise of leveraging continuous optimization to discover new contact patterns online. Nevertheless, the resulting optimization, being an instance of Mathematical Programming with Complementary Constraints, fails the classical constraint qualifications that are crucial for the convergence of popular numerical solvers. We present robust contact-implicit motion planning with sequential convex programming (CRISP), a solver that departs from the usual primal-dual algorithmic framework but instead only focuses on the primal problem. CRISP solves a convex quadratic program with an adaptive trust region radius at each iteration, and its convergence is evaluated by a merit function using weighted penalty. We (i) provide sufficient conditions on CRISP's convergence to first-order stationary points of the merit function; (ii) release a high-performance C++ implementation of CRISP with a generic nonlinear programming interface; and (iii) demonstrate CRISP's surprising robustness in solving contact-implicit planning with naive initialization. In fact, CRISP solves several contact-implicit problems with all-zero initialization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01056</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01056</id><created>2025-02-03</created><authors><author><keyname>Wang</keyname><forenames>Chao</forenames></author><author><keyname>Zhou</keyname><forenames>Xuancheng</forenames></author><author><keyname>Fu</keyname><forenames>Weiwei</forenames></author><author><keyname>Zhou</keyname><forenames>Yang</forenames></author></authors><title>Mitigating Hallucinations in Large Vision-Language Models with Internal   Fact-based Contrastive Decoding</title><categories>cs.CV cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large Visual Language Models (LVLMs) integrate visual and linguistic modalities, exhibiting exceptional performance across various multimodal tasks. Nevertheless, LVLMs remain vulnerable to the issue of object hallucinations. Previous efforts to mitigate this issue focus on supervised fine-tuning (SFT) or incorporating external knowledge, both of which entail significant costs related to training and the acquisition of external data. To address these challenges, we propose a novel model-agnostic approach termed Internal Fact-based Contrastive Decoding (IFCD), designed to mitigate and suppress hallucinations during the inference process of LVLMs by exploiting the LVLMs' own hallucinations. IFCD is grounded in experimental observations that alterations to the LVLMs' internal representations tend to amplify hallucinations caused by language bias. By contrasting disturbed distribution, IFCD calibrates the LVLMs' output and effectively removes the hallucinatory logits from the final predictions. Experimental results validate that IFCD significantly alleviates both object-level and attribute-level hallucinations while achieving an average 9% accuracy improvement on POPE and 8% accuracy improvement on MME object hallucinations subset compared with direct decoding, respectively. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01057</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01057</id><created>2025-02-03</created><authors><author><keyname>Li</keyname><forenames>Bo</forenames></author><author><keyname>Zeng</keyname><forenames>Qi</forenames></author><author><keyname>Warfield</keyname><forenames>Simon K.</forenames></author><author><keyname>Karimi</keyname><forenames>Davood</forenames></author></authors><title>FetDTIAlign: A Deep Learning Framework for Affine and Deformable   Registration of Fetal Brain dMRI</title><categories>eess.IV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diffusion MRI (dMRI) provides unique insights into fetal brain microstructure in utero. Longitudinal and cross-sectional fetal dMRI studies can reveal crucial neurodevelopmental changes but require precise spatial alignment across scans and subjects. This is challenging due to low data quality, rapid brain development, and limited anatomical landmarks. Existing registration methods, designed for high-quality adult data, struggle with these complexities. To address this, we introduce FetDTIAlign, a deep learning approach for fetal brain dMRI registration, enabling accurate affine and deformable alignment. FetDTIAlign features a dual-encoder architecture and iterative feature-based inference, reducing the impact of noise and low resolution. It optimizes network configurations and domain-specific features at each registration stage, enhancing both robustness and accuracy. We validated FetDTIAlign on data from 23 to 36 weeks gestation, covering 60 white matter tracts. It consistently outperformed two classical optimization-based methods and a deep learning pipeline, achieving superior anatomical correspondence. Further validation on external data from the Developing Human Connectome Project confirmed its generalizability across acquisition protocols. Our results demonstrate the feasibility of deep learning for fetal brain dMRI registration, providing a more accurate and reliable alternative to classical techniques. By enabling precise cross-subject and tract-specific analyses, FetDTIAlign supports new discoveries in early brain development. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01059</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01059</id><created>2025-02-03</created><authors><author><keyname>Yoon</keyname><forenames>Seungri</forenames></author><author><keyname>Jeon</keyname><forenames>Woosang</forenames></author><author><keyname>Choi</keyname><forenames>Sanghyeok</forenames></author><author><keyname>Kim</keyname><forenames>Taehyeong</forenames></author><author><keyname>Ahn</keyname><forenames>Tae In</forenames></author></authors><title>Knowledge Synthesis of Photosynthesis Research Using a Large Language   Model</title><categories>cs.CL cs.AI</categories><comments>17 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of biological data analysis tools and large language models (LLMs) has opened up new possibilities for utilizing AI in plant science research, with the potential to contribute significantly to knowledge integration and research gap identification. Nonetheless, current LLMs struggle to handle complex biological data and theoretical models in photosynthesis research and often fail to provide accurate scientific contexts. Therefore, this study proposed a photosynthesis research assistant (PRAG) based on OpenAI's GPT-4o with retrieval-augmented generation (RAG) techniques and prompt optimization. Vector databases and an automated feedback loop were used in the prompt optimization process to enhance the accuracy and relevance of the responses to photosynthesis-related queries. PRAG showed an average improvement of 8.7% across five metrics related to scientific writing, with a 25.4% increase in source transparency. Additionally, its scientific depth and domain coverage were comparable to those of photosynthesis research papers. A knowledge graph was used to structure PRAG's responses with papers within and outside the database, which allowed PRAG to match key entities with 63% and 39.5% of the database and test papers, respectively. PRAG can be applied for photosynthesis research and broader plant science domains, paving the way for more in-depth data analysis and predictive capabilities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01060</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01060</id><created>2025-02-03</created><authors><author><keyname>Ranga</keyname><forenames>Sriram</forenames></author><author><keyname>Chattopadhyay</keyname><forenames>Nandish</forenames></author><author><keyname>Chattopadhyay</keyname><forenames>Anupam</forenames></author></authors><title>Learning Nonlinearity of Boolean Functions: An Experimentation with   Neural Networks</title><categories>cs.LG cs.AI cs.CR</categories><comments>To be published in International conference on Artificial   Intelligence and Sustainable Computing, AISC 2024</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper investigates the learnability of the nonlinearity property of Boolean functions using neural networks. We train encoder style deep neural networks to learn to predict the nonlinearity of Boolean functions from examples of functions in the form of a truth table and their corresponding nonlinearity values. We report empirical results to show that deep neural networks are able to learn to predict the property for functions in 4 and 5 variables with an accuracy above 95%. While these results are positive and a disciplined analysis is being presented for the first time in this regard, we should also underline the statutory warning that it seems quite challenging to extend the idea to higher number of variables, and it is also not clear whether one can get advantage in terms of time and space complexity over the existing combinatorial algorithms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01061</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01061</id><created>2025-02-03</created><authors><author><keyname>Lin</keyname><forenames>Gaojie</forenames></author><author><keyname>Jiang</keyname><forenames>Jianwen</forenames></author><author><keyname>Yang</keyname><forenames>Jiaqi</forenames></author><author><keyname>Zheng</keyname><forenames>Zerong</forenames></author><author><keyname>Liang</keyname><forenames>Chao</forenames></author></authors><title>OmniHuman-1: Rethinking the Scaling-Up of One-Stage Conditioned Human   Animation Models</title><categories>cs.CV</categories><comments>https://omnihuman-lab.github.io/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  End-to-end human animation, such as audio-driven talking human generation, has undergone notable advancements in the recent few years. However, existing methods still struggle to scale up as large general video generation models, limiting their potential in real applications. In this paper, we propose OmniHuman, a Diffusion Transformer-based framework that scales up data by mixing motion-related conditions into the training phase. To this end, we introduce two training principles for these mixed conditions, along with the corresponding model architecture and inference strategy. These designs enable OmniHuman to fully leverage data-driven motion generation, ultimately achieving highly realistic human video generation. More importantly, OmniHuman supports various portrait contents (face close-up, portrait, half-body, full-body), supports both talking and singing, handles human-object interactions and challenging body poses, and accommodates different image styles. Compared to existing end-to-end audio-driven methods, OmniHuman not only produces more realistic videos, but also offers greater flexibility in inputs. It also supports multiple driving modalities (audio-driven, video-driven and combined driving signals). Video samples are provided on the ttfamily project page (https://omnihuman-lab.github.io) </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01066</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01066</id><created>2025-02-03</created><authors><author><keyname>Zhang</keyname><forenames>Yuan</forenames></author><author><keyname>Zhong</keyname><forenames>Kuncai</forenames></author><author><keyname>Zhang</keyname><forenames>Jiliang</forenames></author></authors><title>DH-TRNG: A Dynamic Hybrid TRNG with Ultra-High Throughput and   Area-Energy Efficiency</title><categories>cs.CR</categories><doi>10.1145/3649329.3656236</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a vital security primitive, the true random number generator (TRNG) is a mandatory component to build roots of trust for any encryption system. However, existing TRNGs suffer from bottlenecks of low throughput and high area-energy consumption. In this work, we propose DH-TRNG, a dynamic hybrid TRNG circuitry architecture with ultra-high throughput and area-energy efficiency. Our DH-TRNG exhibits portability to distinct process FPGAs and passes both NIST and AIS-31 tests without any post-processing. The experiments show it incurs only 8 slices with the highest throughput of 670Mbps and 620Mbps on Xilinx Virtex-6 and Artix-7, respectively. Compared to the state-of-the-art TRNGs, our proposed design has the highest Throughput/SlicesPower with a 2.63 times increase. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01067</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01067</id><created>2025-02-03</created><authors><author><keyname>Karpov</keyname><forenames>Nikolai</forenames></author><author><keyname>Wang</keyname><forenames>Chen</forenames></author></authors><title>Nearly Tight Bounds for Exploration in Streaming Multi-armed Bandits   with Known Optimality Gap</title><categories>cs.LG cs.DS</categories><comments>AAAI 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the sample-memory-pass trade-offs for pure exploration in multi-pass streaming multi-armed bandits (MABs) with the *a priori* knowledge of the optimality gap $\Delta_{[2]}$. Here, and throughout, the optimality gap $\Delta_{[i]}$ is defined as the mean reward gap between the best and the $i$-th best arms. A recent line of results by Jin, Huang, Tang, and Xiao [ICML'21] and Assadi and Wang [COLT'24] have shown that if there is no known $\Delta_{[2]}$, a pass complexity of $\Theta(\log(1/\Delta_{[2]}))$ (up to $\log\log(1/\Delta_{[2]})$ terms) is necessary and sufficient to obtain the *worst-case optimal* sample complexity of $O(n/\Delta^{2}_{[2]})$ with a single-arm memory. However, our understanding of multi-pass algorithms with known $\Delta_{[2]}$ is still limited. Here, the key open problem is how many passes are required to achieve the complexity, i.e., $O( \sum_{i=2}^{n}1/\Delta^2_{[i]})$ arm pulls, with a sublinear memory size.   In this work, we show that the ``right answer'' for the question is $\Theta(\log{n})$ passes (up to $\log\log{n}$ terms). We first present a lower bound, showing that any algorithm that finds the best arm with slightly sublinear memory -- a memory of $o({n}/{\text{polylog}({n})})$ arms -- and $O(\sum_{i=2}^{n}{1}/{\Delta^{2}_{[i]}}\cdot \log{(n)})$ arm pulls has to make $\Omega(\frac{\log{n}}{\log\log{n}})$ passes over the stream. We then show a nearly-matching algorithm that assuming the knowledge of $\Delta_{[2]}$, finds the best arm with $O( \sum_{i=2}^{n}1/\Delta^2_{[i]} \cdot \log{n})$ arm pulls and a *single arm* memory. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01068</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01068</id><created>2025-02-03</created><authors><author><keyname>Jo</keyname><forenames>Dongwon</forenames></author><author><keyname>Song</keyname><forenames>Jiwon</forenames></author><author><keyname>Kim</keyname><forenames>Yulhwa</forenames></author><author><keyname>Kim</keyname><forenames>Jae-Joon</forenames></author></authors><title>FastKV: KV Cache Compression for Fast Long-Context Processing with   Token-Selective Propagation</title><categories>cs.LG cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While large language models (LLMs) excel at handling long-context sequences, they require substantial key-value (KV) caches to store contextual information, which can heavily burden computational efficiency and memory usage. Previous efforts to compress these KV caches primarily focused on reducing memory demands but were limited in enhancing latency. To address this issue, we introduce FastKV, a KV cache compression method designed to enhance latency for long-context sequences. To enhance processing speeds while maintaining accuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that retains the full context information in the initial layers of LLMs and selectively propagates only a portion of this information in deeper layers even in the prefill stage. Additionally, FastKV incorporates grouped-query attention (GQA)-aware KV cache compression to exploit the advantages of GQA in both memory and computational efficiency. Our experimental results show that FastKV achieves 2.00$\times$ and 1.40$\times$ improvements in time-to-first-token (TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art KV cache compression method. Moreover, FastKV successfully maintains accuracy on long-context benchmarks at levels comparable to the baselines. Our code is available at https://github.com/dongwonjo/FastKV. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01070</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01070</id><created>2025-02-03</created><authors><author><keyname>Kim</keyname><forenames>Jiwoo</forenames></author><author><keyname>Lee</keyname><forenames>Joonhyung</forenames></author><author><keyname>Park</keyname><forenames>Gunho</forenames></author><author><keyname>Kim</keyname><forenames>Byeongwook</forenames></author><author><keyname>Kwon</keyname><forenames>Se Jung</forenames></author><author><keyname>Lee</keyname><forenames>Dongsoo</forenames></author><author><keyname>Lee</keyname><forenames>Youngjoo</forenames></author></authors><title>An Investigation of FP8 Across Accelerators for LLM Inference</title><categories>cs.LG cs.PF</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The introduction of 8-bit floating-point (FP8) computation units in modern AI accelerators has generated significant interest in FP8-based large language model (LLM) inference. Unlike 16-bit floating-point formats, FP8 in deep learning requires a shared scaling factor. Additionally, while E4M3 and E5M2 are well-defined at the individual value level, their scaling and accumulation methods remain unspecified and vary across hardware and software implementations. As a result, FP8 behaves more like a quantization format than a standard numeric representation. In this work, we provide the first comprehensive analysis of FP8 computation and acceleration on two AI accelerators: the NVIDIA H100 and Intel Gaudi 2. Our findings highlight that the Gaudi 2, by leveraging FP8, achieves higher throughput-to-power efficiency during LLM inference, offering valuable insights into the practical implications of FP8 adoption for datacenter-scale LLM serving. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01071</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01071</id><created>2025-02-03</created><authors><author><keyname>Samson</keyname><forenames>Marie</forenames></author><author><keyname>Muraccioli</keyname><forenames>Bastien</forenames></author><author><keyname>Kanehiro</keyname><forenames>Fumio</forenames></author></authors><title>Scalable, Training-Free Visual Language Robotics: A Modular Multi-Model   Framework for Consumer-Grade GPUs</title><categories>cs.RO</categories><journal-ref>2025 IEEE/SICE International Symposium on System Integration</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The integration of language instructions with robotic control, particularly through Vision Language Action (VLA) models, has shown significant potential. However, these systems are often hindered by high computational costs, the need for extensive retraining, and limited scalability, making them less accessible for widespread use.   In this paper, we introduce SVLR (Scalable Visual Language Robotics), an open-source, modular framework that operates without the need for retraining, providing a scalable solution for robotic control. SVLR leverages a combination of lightweight, open-source AI models including the Vision-Language Model (VLM) Mini-InternVL, zero-shot image segmentation model CLIPSeg, Large Language Model Phi-3, and sentence similarity model all-MiniLM to process visual and language inputs. These models work together to identify objects in an unknown environment, use them as parameters for task execution, and generate a sequence of actions in response to natural language instructions. A key strength of SVLR is its scalability. The framework allows for easy integration of new robotic tasks and robots by simply adding text descriptions and task definitions, without the need for retraining. This modularity ensures that SVLR can continuously adapt to the latest advancements in AI technologies and support a wide range of robots and tasks.   SVLR operates effectively on an NVIDIA RTX 2070 (mobile) GPU, demonstrating promising performance in executing pick-and-place tasks. While these initial results are encouraging, further evaluation across a broader set of tasks and comparisons with existing VLA models are needed to assess SVLR's generalization capabilities and performance in more complex scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01074</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01074</id><created>2025-02-03</created><authors><author><keyname>Hu</keyname><forenames>Chengxin</forenames></author><author><keyname>Li</keyname><forenames>Hao</forenames></author><author><keyname>Yuan</keyname><forenames>Yihe</forenames></author><author><keyname>Song</keyname><forenames>Zezheng</forenames></author><author><keyname>Wang</keyname><forenames>Haixin</forenames></author></authors><title>Omni-Mol: Exploring Universal Convergent Space for Omni-Molecular Tasks</title><categories>cs.LG</categories><comments>29 pages, 13 figures, 7 tables, paper under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building generalist models has recently demonstrated remarkable capabilities in diverse scientific domains. Within the realm of molecular learning, several studies have explored unifying diverse tasks across diverse domains. However, negative conflicts and interference between molecules and knowledge from different domain may have a worse impact in threefold. First, conflicting molecular representations can lead to optimization difficulties for the models. Second, mixing and scaling up training data across diverse tasks is inherently challenging. Third, the computational cost of refined pretraining is prohibitively high. To address these limitations, this paper presents Omni-Mol, a scalable and unified LLM-based framework for direct instruction tuning. Omni-Mol builds on three key components to tackles conflicts: (1) a unified encoding mechanism for any task input; (2) an active-learning-driven data selection strategy that significantly reduces dataset size; (3) a novel design of the adaptive gradient stabilization module and anchor-and-reconcile MoE framework that ensures stable convergence. Experimentally, Omni-Mol achieves state-of-the-art performance across 15 molecular tasks, demonstrates the presence of scaling laws in the molecular domain, and is supported by extensive ablation studies and analyses validating the effectiveness of its design. The code and weights of the powerful AI-driven chemistry generalist are open-sourced at: https://anonymous.4open.science/r/Omni-Mol-8EDB. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01076</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01076</id><created>2025-02-03</created><authors><author><keyname>Fang</keyname><forenames>Sheng</forenames></author><author><keyname>Liu</keyname><forenames>Yong-Jin</forenames></author><author><keyname>Yao</keyname><forenames>Wei</forenames></author><author><keyname>Yu</keyname><forenames>Chengming</forenames></author><author><keyname>Zhang</keyname><forenames>Jin</forenames></author></authors><title>qNBO: quasi-Newton Meets Bilevel Optimization</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bilevel optimization, addressing challenges in hierarchical learning tasks, has gained significant interest in machine learning. The practical implementation of the gradient descent method to bilevel optimization encounters computational hurdles, notably the computation of the exact lower-level solution and the inverse Hessian of the lower-level objective. Although these two aspects are inherently connected, existing methods typically handle them separately by solving the lower-level problem and a linear system for the inverse Hessian-vector product. In this paper, we introduce a general framework to address these computational challenges in a coordinated manner. Specifically, we leverage quasi-Newton algorithms to accelerate the resolution of the lower-level problem while efficiently approximating the inverse Hessian-vector product. Furthermore, by exploiting the superlinear convergence properties of BFGS, we establish the non-asymptotic convergence analysis of the BFGS adaptation within our framework. Numerical experiments demonstrate the comparable or superior performance of the proposed algorithms in real-world learning tasks, including hyperparameter optimization, data hyper-cleaning, and few-shot meta-learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01078</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01078</id><created>2025-02-03</created><authors><author><keyname>Li</keyname><forenames>Qi</forenames></author><author><keyname>Yuan</keyname><forenames>Jinhong</forenames></author><author><keyname>Qiu</keyname><forenames>Min</forenames></author></authors><title>Parallel Coding for Orthogonal Delay-Doppler Division Multiplexing</title><categories>cs.IT eess.SP math.IT</categories><comments>12 pages, 12 figures, accepted by IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel parallel coding transmission strategy and an iterative detection and decoding receiver signal processing technique for orthogonal delay-Doppler division multiplexing (ODDM) modulation. Specifically, the proposed approach employs a parallel channel encoding (PCE) scheme that consists of multiple short-length codewords for each delay-Doppler multicarrier (DDMC) symbol. Building upon such a PCE transmission framework, we then introduce an iterative detection and decoding algorithm incorporating a successive decoding feedback (SDF) technique, which enables instant information exchange between the detector and decoder for each DDMC symbol. To characterize the error performance of the proposed scheme, we perform density evolution analysis considering the finite blocklength effects. Our analysis results, coupled with extensive simulations, demonstrate that the proposed PCE scheme with the SDF algorithm not only showcases a better overall performance but also requires much less decoding complexity to implement, compared to the conventional benchmark scheme that relies on a single long channel code for coding the entire ODDM frame. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01080</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01080</id><created>2025-02-03</created><authors><author><keyname>Zhou</keyname><forenames>Dongliang</forenames></author><author><keyname>Zhang</keyname><forenames>Haijun</forenames></author><author><keyname>Ma</keyname><forenames>Jianghong</forenames></author><author><keyname>Shi</keyname><forenames>Jianyang</forenames></author></authors><title>BC-GAN: A Generative Adversarial Network for Synthesizing a Batch of   Collocated Clothing</title><categories>cs.CV cs.MM</categories><comments>This paper was accepted by IEEE TCSVT</comments><doi>10.1109/TCSVT.2023.3318216</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collocated clothing synthesis using generative networks has become an emerging topic in the field of fashion intelligence, as it has significant potential economic value to increase revenue in the fashion industry. In previous studies, several works have attempted to synthesize visually-collocated clothing based on a given clothing item using generative adversarial networks (GANs) with promising results. These works, however, can only accomplish the synthesis of one collocated clothing item each time. Nevertheless, users may require different clothing items to meet their multiple choices due to their personal tastes and different dressing scenarios. To address this limitation, we introduce a novel batch clothing generation framework, named BC-GAN, which is able to synthesize multiple visually-collocated clothing images simultaneously. In particular, to further improve the fashion compatibility of synthetic results, BC-GAN proposes a new fashion compatibility discriminator in a contrastive learning perspective by fully exploiting the collocation relationship among all clothing items. Our model was examined in a large-scale dataset with compatible outfits constructed by ourselves. Extensive experiment results confirmed the effectiveness of our proposed BC-GAN in comparison to state-of-the-art methods in terms of diversity, visual authenticity, and fashion compatibility. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01081</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01081</id><created>2025-02-03</created><authors><author><keyname>Toh</keyname><forenames>Vernon Y. H.</forenames></author><author><keyname>Chia</keyname><forenames>Yew Ken</forenames></author><author><keyname>Ghosal</keyname><forenames>Deepanway</forenames></author><author><keyname>Poria</keyname><forenames>Soujanya</forenames></author></authors><title>The Jumping Reasoning Curve? Tracking the Evolution of Reasoning   Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles</title><categories>cs.CV cs.AI cs.CL</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The releases of OpenAI's o1 and o3 mark a significant paradigm shift in Large Language Models towards advanced reasoning capabilities. Notably, o3 outperformed humans in novel problem-solving and skill acquisition on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, this benchmark is limited to symbolic patterns, whereas humans often perceive and reason about multimodal scenarios involving both vision and language data. Thus, there is an urgent need to investigate advanced reasoning capabilities in multimodal tasks. To this end, we track the evolution of the GPT-[n] and o-[n] series models on challenging multimodal puzzles, requiring fine-grained visual perception with abstract or algorithmic reasoning. The superior performance of o1 comes at nearly 750 times the computational cost of GPT-4o, raising concerns about its efficiency. Our results reveal a clear upward trend in reasoning capabilities across model iterations, with notable performance jumps across GPT-series models and subsequently to o1. Nonetheless, we observe that the o1 model still struggles with simple multimodal puzzles requiring abstract reasoning. Furthermore, its performance in algorithmic puzzles remains poor. We plan to continuously track new models in the series and update our results in this paper accordingly. All resources used in this evaluation are openly available https://github.com/declare-lab/LLM-PuzzleTest. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01083</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01083</id><created>2025-02-03</created><authors><author><keyname>Cheng</keyname><forenames>Jiali</forenames></author><author><keyname>Amiri</keyname><forenames>Hadi</forenames></author></authors><title>Tool Unlearning for Tool-Augmented LLMs</title><categories>cs.LG cs.AI cs.CL</categories><comments>https://clu-uml.github.io/MU-Bench-Project-Page/</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Tool-augmented large language models (LLMs) are often trained on datasets of query-response pairs, which embed the ability to use tools or APIs directly into the parametric knowledge of LLMs. Tool-augmented LLMs need the ability to forget learned tools due to security vulnerabilities, privacy regulations, or tool deprecations. However, ``tool unlearning'' has not been investigated in unlearning literature. We introduce this novel task, which requires addressing distinct challenges compared to traditional unlearning: knowledge removal rather than forgetting individual samples, the high cost of optimizing LLMs, and the need for principled evaluation metrics. To bridge these gaps, we propose ToolDelete, the first approach for unlearning tools from tool-augmented LLMs. It implements three key properties to address the above challenges for effective tool unlearning and introduces a new membership inference attack (MIA) model for effective evaluation. Extensive experiments on multiple tool learning datasets and tool-augmented LLMs show that ToolDelete effectively unlearns randomly selected tools, while preserving the LLM's knowledge on non-deleted tools and maintaining performance on general tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01084</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01084</id><created>2025-02-03</created><authors><author><keyname>Lin</keyname><forenames>Weiwei</forenames></author><author><keyname>He</keyname><forenames>Chenghan</forenames></author></authors><title>Continuous Autoregressive Modeling with Stochastic Monotonic Alignment   for Speech Synthesis</title><categories>cs.LG cs.SD eess.AS</categories><comments>ICLR 2025</comments><journal-ref>ICLR 2025</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a novel autoregressive modeling approach for speech synthesis, combining a variational autoencoder (VAE) with a multi-modal latent space and an autoregressive model that uses Gaussian Mixture Models (GMM) as the conditional probability distribution. Unlike previous methods that rely on residual vector quantization, our model leverages continuous speech representations from the VAE's latent space, greatly simplifying the training and inference pipelines. We also introduce a stochastic monotonic alignment mechanism to enforce strict monotonic alignments. Our approach significantly outperforms the state-of-the-art autoregressive model VALL-E in both subjective and objective evaluations, achieving these results with only 10.3\% of VALL-E's parameters. This demonstrates the potential of continuous speech language models as a more efficient alternative to existing quantization-based speech language models. Sample audio can be found at https://tinyurl.com/gmm-lm-tts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01085</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01085</id><created>2025-02-03</created><authors><author><keyname>Huang</keyname><forenames>Xuhan</forenames></author><author><keyname>Hu</keyname><forenames>Yan</forenames></author><author><keyname>Li</keyname><forenames>Zhiyan</forenames></author><author><keyname>Wang</keyname><forenames>Zhiyong</forenames></author><author><keyname>Wang</keyname><forenames>Benyou</forenames></author><author><keyname>Dai</keyname><forenames>Zhongxiang</forenames></author></authors><title>Federated Linear Dueling Bandits</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contextual linear dueling bandits have recently garnered significant attention due to their widespread applications in important domains such as recommender systems and large language models. Classical dueling bandit algorithms are typically only applicable to a single agent. However, many applications of dueling bandits involve multiple agents who wish to collaborate for improved performance yet are unwilling to share their data. This motivates us to draw inspirations from federated learning, which involves multiple agents aiming to collaboratively train their neural networks via gradient descent (GD) without sharing their raw data. Previous works have developed federated linear bandit algorithms which rely on closed-form updates of the bandit parameters (e.g., the linear function parameter) to achieve collaboration. However, in linear dueling bandits, the linear function parameter lacks a closed-form expression and its estimation requires minimizing a loss function. This renders these previous methods inapplicable. In this work, we overcome this challenge through an innovative and principled combination of online gradient descent (for minimizing the loss function to estimate the linear function parameters) and federated learning, hence introducing the first federated linear dueling bandit algorithms. Through rigorous theoretical analysis, we prove that our algorithms enjoy a sub-linear upper bound on its cumulative regret. We also use empirical experiments to demonstrate the effectiveness of our algorithms and the practical benefit of collaboration. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01089</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01089</id><created>2025-02-03</created><authors><author><keyname>Dev</keyname><forenames>Kapal</forenames></author><author><keyname>Khowaja</keyname><forenames>Sunder Ali</forenames></author><author><keyname>Zeydan</keyname><forenames>Engin</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author></authors><title>Advanced Architectures Integrated with Agentic AI for Next-Generation   Wireless Networks</title><categories>cs.NI cs.AI</categories><comments>6 Pages</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper investigates a range of cutting-edge technologies and architectural innovations aimed at simplifying network operations, reducing operational expenditure (OpEx), and enabling the deployment of new service models. The focus is on (i) Proposing novel, more efficient 6G architectures, with both Control and User planes enabling the seamless expansion of services, while addressing long-term 6G network evolution. (ii) Exploring advanced techniques for constrained artificial intelligence (AI) operations, particularly the design of AI agents for real-time learning, optimizing energy consumption, and the allocation of computational resources. (iii) Identifying technologies and architectures that support the orchestration of backend services using serverless computing models across multiple domains, particularly for vertical industries. (iv) Introducing optically-based, ultra-high-speed, low-latency network architectures, with fast optical switching and real-time control, replacing conventional electronic switching to reduce power consumption by an order of magnitude. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01090</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01090</id><created>2025-02-03</created><authors><author><keyname>Chen</keyname><forenames>Jiali</forenames></author><author><keyname>Hei</keyname><forenames>Xusen</forenames></author><author><keyname>Xue</keyname><forenames>Yuqi</forenames></author><author><keyname>Wu</keyname><forenames>Zihan</forenames></author><author><keyname>Xie</keyname><forenames>Jiayuan</forenames></author><author><keyname>Cai</keyname><forenames>Yi</forenames></author></authors><title>Classic4Children: Adapting Chinese Literary Classics for Children with   Large Language Model</title><categories>cs.CL cs.AI</categories><comments>Accepted at NAACL 2025 Findings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chinese literary classics hold significant cultural and educational value, offering deep insights into morality, history, and human nature. These works often include classical Chinese and complex narratives, making them difficult for children to read. To bridge this gap, we introduce a child-friendly literary adaptation (CLA) task to adapt the Chinese literary classic into engaging and accessible text for children. However, recent large language models (LLMs) overlook children's reading preferences (\ie, vivid character portrayals, concise narrative structures, and appropriate readability), which poses challenges in CLA. In this paper, we propose a method called InstructChild, which augments the LLM with these preferences for adaptation. Specifically, we first obtain the characters' personalities and narrative structure as additional information for fine-grained instruction tuning. Then, we devise a readability metric as the reward to align the LLM with the children's reading level. Finally, a lookahead decoding strategy is applied to improve the readability of the generated text during inference. To support the evaluation of CLA task, we construct the Classic4Children dataset, which comprises both the original and child-friendly versions of the Four Great Classical Novels of Chinese literature. Experimental results show that our InstructChild significantly improves automatic and human evaluation performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01091</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01091</id><created>2025-02-03</created><authors><author><keyname>Ariai</keyname><forenames>Farid</forenames></author><author><keyname>Mahmoudi</keyname><forenames>Maryam Tayefeh</forenames></author><author><keyname>Moeini</keyname><forenames>Ali</forenames></author></authors><title>Enhancing Aspect-based Sentiment Analysis with ParsBERT in Persian   Language</title><categories>cs.CL cs.AI</categories><journal-ref>Journal of AI and Data Mining, 2024, 12(1): 1-14</journal-ref><doi>10.22044/jadm.2023.13666.2482</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the era of pervasive internet use and the dominance of social networks, researchers face significant challenges in Persian text mining including the scarcity of adequate datasets in Persian and the inefficiency of existing language models. This paper specifically tackles these challenges, aiming to amplify the efficiency of language models tailored to the Persian language. Focusing on enhancing the effectiveness of sentiment analysis, our approach employs an aspect-based methodology utilizing the ParsBERT model, augmented with a relevant lexicon. The study centers on sentiment analysis of user opinions extracted from the Persian website 'Digikala.' The experimental results not only highlight the proposed method's superior semantic capabilities but also showcase its efficiency gains with an accuracy of 88.2% and an F1 score of 61.7. The importance of enhancing language models in this context lies in their pivotal role in extracting nuanced sentiments from user-generated content, ultimately advancing the field of sentiment analysis in Persian text mining by increasing efficiency and accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01092</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01092</id><created>2025-02-03</created><authors><author><keyname>Kim</keyname><forenames>Dabin</forenames></author><author><keyname>Jang</keyname><forenames>Inkyu</forenames></author><author><keyname>Han</keyname><forenames>Youngsoo</forenames></author><author><keyname>Hwang</keyname><forenames>Sunwoo</forenames></author><author><keyname>Kim</keyname><forenames>H. Jin</forenames></author></authors><title>Enhancing Feature Tracking Reliability for Visual Navigation using   Real-Time Safety Filter</title><categories>cs.RO cs.CV cs.SY eess.SY</categories><comments>7 pages, 6 figures, Accepted to 2025 IEEE International Conference on   Robotics &amp; Automation (ICRA 2025)</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Vision sensors are extensively used for localizing a robot's pose, particularly in environments where global localization tools such as GPS or motion capture systems are unavailable. In many visual navigation systems, localization is achieved by detecting and tracking visual features or landmarks, which provide information about the sensor's relative pose. For reliable feature tracking and accurate pose estimation, it is crucial to maintain visibility of a sufficient number of features. This requirement can sometimes conflict with the robot's overall task objective. In this paper, we approach it as a constrained control problem. By leveraging the invariance properties of visibility constraints within the robot's kinematic model, we propose a real-time safety filter based on quadratic programming. This filter takes a reference velocity command as input and produces a modified velocity that minimally deviates from the reference while ensuring the information score from the currently visible features remains above a user-specified threshold. Numerical simulations demonstrate that the proposed safety filter preserves the invariance condition and ensures the visibility of more features than the required minimum. We also validated its real-world performance by integrating it into a visual simultaneous localization and mapping (SLAM) algorithm, where it maintained high estimation quality in challenging environments, outperforming a simple tracking controller. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01094</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01094</id><created>2025-02-03</created><authors><author><keyname>Samari</keyname><forenames>Behrad</forenames></author><author><keyname>Nejati</keyname><forenames>Amy</forenames></author><author><keyname>Lavaei</keyname><forenames>Abolfazl</forenames></author></authors><title>Model Order Reduction from Data with Certification</title><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model order reduction (MOR) involves offering low-dimensional models that effectively approximate the behavior of complex high-order systems. Due to potential model complexities and computational costs, designing controllers for high-dimensional systems with complex behaviors can be challenging, rendering MOR a practical alternative to achieve results that closely resemble those of the original complex systems. To construct such effective reduced-order models (ROMs), existing literature generally necessitates precise knowledge of original systems, which is often unavailable in real-world scenarios. This paper introduces a data-driven scheme to construct ROMs of dynamical systems with unknown mathematical models. Our methodology leverages data and establishes similarity relations between output trajectories of unknown systems and their data-driven ROMs via the notion of simulation functions (SFs), capable of formally quantifying their closeness. To achieve this, under a rank condition readily fulfillable using data, we collect only two input-state trajectories from unknown systems to construct both ROMs and SFs, while offering correctness guarantees. We demonstrate that the proposed ROMs derived from data can be leveraged for controller synthesis endeavors while effectively ensuring high-level logic properties over unknown dynamical models. We showcase our data-driven findings across a range of benchmark scenarios involving various unknown physical systems, demonstrating the enforcement of diverse complex properties. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01098</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01098</id><created>2025-02-03</created><authors><author><keyname>Irigireddy</keyname><forenames>Bharath</forenames></author><author><keyname>Bandaru</keyname><forenames>Varaprasad</forenames></author></authors><title>SatFlow: Generative model based framework for producing High Resolution   Gap Free Remote Sensing Imagery</title><categories>cs.CV cs.LG</categories><acm-class>J.2.5, I.4.5</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Frequent, high-resolution remote sensing imagery is crucial for agricultural and environmental monitoring. Satellites from the Landsat collection offer detailed imagery at 30m resolution but with lower temporal frequency, whereas missions like MODIS and VIIRS provide daily coverage at coarser resolutions. Clouds and cloud shadows contaminate about 55\% of the optical remote sensing observations, posing additional challenges. To address these challenges, we present SatFlow, a generative model-based framework that fuses low-resolution MODIS imagery and Landsat observations to produce frequent, high-resolution, gap-free surface reflectance imagery. Our model, trained via Conditional Flow Matching, demonstrates better performance in generating imagery with preserved structural and spectral integrity. Cloud imputation is treated as an image inpainting task, where the model reconstructs cloud-contaminated pixels and fills gaps caused by scan lines during inference by leveraging the learned generative processes. Experimental results demonstrate the capability of our approach in reliably imputing cloud-covered regions. This capability is crucial for downstream applications such as crop phenology tracking, environmental change detection etc., </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01100</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01100</id><created>2025-02-03</created><authors><author><keyname>Lin</keyname><forenames>Bill Yuchen</forenames></author><author><keyname>Bras</keyname><forenames>Ronan Le</forenames></author><author><keyname>Richardson</keyname><forenames>Kyle</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashish</forenames></author><author><keyname>Poovendran</keyname><forenames>Radha</forenames></author><author><keyname>Clark</keyname><forenames>Peter</forenames></author><author><keyname>Choi</keyname><forenames>Yejin</forenames></author></authors><title>ZebraLogic: On the Scaling Limits of LLMs for Logical Reasoning</title><categories>cs.AI cs.CL cs.LG</categories><comments>Website: https://huggingface.co/spaces/WildEval/ZebraLogic</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We investigate the logical reasoning capabilities of large language models (LLMs) and their scalability in complex non-monotonic reasoning. To this end, we introduce ZebraLogic, a comprehensive evaluation framework for assessing LLM reasoning performance on logic grid puzzles derived from constraint satisfaction problems (CSPs). ZebraLogic enables the generation of puzzles with controllable and quantifiable complexity, facilitating a systematic study of the scaling limits of models such as Llama, o1 models, and DeepSeek-R1. By encompassing a broad range of search space complexities and diverse logical constraints, ZebraLogic provides a structured environment to evaluate reasoning under increasing difficulty.   Our results reveal a significant decline in accuracy as problem complexity grows -- a phenomenon we term the curse of complexity. This limitation persists even with larger models and increased inference-time computation, suggesting inherent constraints in current LLM reasoning capabilities. Additionally, we explore strategies to enhance logical reasoning, including Best-of-N sampling, backtracking mechanisms, and self-verification prompts. Our findings offer critical insights into the scalability of LLM reasoning, highlight fundamental limitations, and outline potential directions for improvement. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01101</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01101</id><created>2025-02-03</created><authors><author><keyname>Jiang</keyname><forenames>Lifan</forenames></author><author><keyname>Chen</keyname><forenames>Shuang</forenames></author><author><keyname>Wu</keyname><forenames>Boxi</forenames></author><author><keyname>Guan</keyname><forenames>Xiaotong</forenames></author><author><keyname>Zhang</keyname><forenames>Jiahui</forenames></author></authors><title>VidSketch: Hand-drawn Sketch-Driven Video Generation with Diffusion   Control</title><categories>cs.CV cs.AI</categories><comments>17pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the advancement of generative artificial intelligence, previous studies have achieved the task of generating aesthetic images from hand-drawn sketches, fulfilling the public's needs for drawing. However, these methods are limited to static images and lack the ability to control video animation generation using hand-drawn sketches. To address this gap, we propose VidSketch, the first method capable of generating high-quality video animations directly from any number of hand-drawn sketches and simple text prompts, bridging the divide between ordinary users and professional artists. Specifically, our method introduces a Level-Based Sketch Control Strategy to automatically adjust the guidance strength of sketches during the generation process, accommodating users with varying drawing skills. Furthermore, a TempSpatial Attention mechanism is designed to enhance the spatiotemporal consistency of generated video animations, significantly improving the coherence across frames. You can find more detailed cases on our official website. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01102</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01102</id><created>2025-02-03</created><authors><author><keyname>Bezzam</keyname><forenames>Eric</forenames></author><author><keyname>Perron</keyname><forenames>Yohann</forenames></author><author><keyname>Vetterli</keyname><forenames>Martin</forenames></author></authors><title>Towards Robust and Generalizable Lensless Imaging with Modular Learned   Reconstruction</title><categories>eess.IV cs.CV</categories><comments>16 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Lensless cameras disregard the conventional design that imaging should mimic the human eye. This is done by replacing the lens with a thin mask, and moving image formation to the digital post-processing. State-of-the-art lensless imaging techniques use learned approaches that combine physical modeling and neural networks. However, these approaches make simplifying modeling assumptions for ease of calibration and computation. Moreover, the generalizability of learned approaches to lensless measurements of new masks has not been studied. To this end, we utilize a modular learned reconstruction in which a key component is a pre-processor prior to image recovery. We theoretically demonstrate the pre-processor's necessity for standard image recovery techniques (Wiener filtering and iterative algorithms), and through extensive experiments show its effectiveness for multiple lensless imaging approaches and across datasets of different mask types (amplitude and phase). We also perform the first generalization benchmark across mask types to evaluate how well reconstructions trained with one system generalize to others. Our modular reconstruction enables us to use pre-trained components and transfer learning on new systems to cut down weeks of tedious measurements and training. As part of our work, we open-source four datasets, and software for measuring datasets and for training our modular reconstruction. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01105</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01105</id><created>2025-02-03</created><authors><author><keyname>Song</keyname><forenames>Yiren</forenames></author><author><keyname>Chen</keyname><forenames>Danze</forenames></author><author><keyname>Shou</keyname><forenames>Mike Zheng</forenames></author></authors><title>LayerTracer: Cognitive-Aligned Layered SVG Synthesis via Diffusion   Transformer</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Generating cognitive-aligned layered SVGs remains challenging due to existing methods' tendencies toward either oversimplified single-layer outputs or optimization-induced shape redundancies. We propose LayerTracer, a diffusion transformer based framework that bridges this gap by learning designers' layered SVG creation processes from a novel dataset of sequential design operations. Our approach operates in two phases: First, a text-conditioned DiT generates multi-phase rasterized construction blueprints that simulate human design workflows. Second, layer-wise vectorization with path deduplication produces clean, editable SVGs. For image vectorization, we introduce a conditional diffusion mechanism that encodes reference images into latent tokens, guiding hierarchical reconstruction while preserving structural integrity. Extensive experiments demonstrate LayerTracer's superior performance against optimization-based and neural baselines in both generation quality and editability, effectively aligning AI-generated vectors with professional design cognition. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01106</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01106</id><created>2025-02-03</created><authors><author><keyname>Shirani</keyname><forenames>Sadegh</forenames></author><author><keyname>Luo</keyname><forenames>Yuwei</forenames></author><author><keyname>Overman</keyname><forenames>William</forenames></author><author><keyname>Xiong</keyname><forenames>Ruoxuan</forenames></author><author><keyname>Bayati</keyname><forenames>Mohsen</forenames></author></authors><title>Can We Validate Counterfactual Estimations in the Presence of General   Network Interference?</title><categories>cs.LG econ.EM stat.ME stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In experimental settings with network interference, a unit's treatment can influence outcomes of other units, challenging both causal effect estimation and its validation. Classic validation approaches fail as outcomes are only observable under one treatment scenario and exhibit complex correlation patterns due to interference. To address these challenges, we introduce a new framework enabling cross-validation for counterfactual estimation. At its core is our distribution-preserving network bootstrap method -- a theoretically-grounded approach inspired by approximate message passing. This method creates multiple subpopulations while preserving the underlying distribution of network effects. We extend recent causal message-passing developments by incorporating heterogeneous unit-level characteristics and varying local interactions, ensuring reliable finite-sample performance through non-asymptotic analysis. We also develop and publicly release a comprehensive benchmark toolbox with diverse experimental environments, from networks of interacting AI agents to opinion formation in real-world communities and ride-sharing applications. These environments provide known ground truth values while maintaining realistic complexities, enabling systematic examination of causal inference methods. Extensive evaluation across these environments demonstrates our method's robustness to diverse forms of network interference. Our work provides researchers with both a practical estimation framework and a standardized platform for testing future methodological developments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01107</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01107</id><created>2025-02-03</created><authors><author><keyname>Wang</keyname><forenames>Jingyuan</forenames></author><author><keyname>Lin</keyname><forenames>Yujing</forenames></author><author><keyname>Li</keyname><forenames>Yudong</forenames></author></authors><title>GTG: Generalizable Trajectory Generation Model for Urban Mobility</title><categories>cs.LG</categories><comments>12 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trajectory data mining is crucial for smart city management. However, collecting large-scale trajectory datasets is challenging due to factors such as commercial conflicts and privacy regulations. Therefore, we urgently need trajectory generation techniques to address this issue. Existing trajectory generation methods rely on the global road network structure of cities. When the road network structure changes, these methods are often not transferable to other cities. In fact, there exist invariant mobility patterns between different cities: 1) People prefer paths with the minimal travel cost; 2) The travel cost of roads has an invariant relationship with the topological features of the road network. Based on the above insight, this paper proposes a Generalizable Trajectory Generation model (GTG). The model consists of three parts: 1) Extracting city-invariant road representation based on Space Syntax method; 2) Cross-city travel cost prediction through disentangled adversarial training; 3) Travel preference learning by shortest path search and preference update. By learning invariant movement patterns, the model is capable of generating trajectories in new cities. Experiments on three datasets demonstrates that our model significantly outperforms existing models in terms of generalization ability. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01108</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01108</id><created>2025-02-03</created><authors><author><keyname>Saha</keyname><forenames>Mithun</forenames></author><author><keyname>Xu</keyname><forenames>Maxwell A.</forenames></author><author><keyname>Mao</keyname><forenames>Wanting</forenames></author><author><keyname>Neupane</keyname><forenames>Sameer</forenames></author><author><keyname>Rehg</keyname><forenames>James M.</forenames></author><author><keyname>Kumar</keyname><forenames>Santosh</forenames></author></authors><title>Pulse-PPG: An Open-Source Field-Trained PPG Foundation Model for   Wearable Applications Across Lab and Field Settings</title><categories>cs.LG cs.AI eess.SP</categories><comments>The first two listed authors contributed equally to this research</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Photoplethysmography (PPG)-based foundation models are gaining traction due to the widespread use of PPG in biosignal monitoring and their potential to generalize across diverse health applications. In this paper, we introduce Pulse-PPG, the first open-source PPG foundation model trained exclusively on raw PPG data collected over a 100-day field study with 120 participants. Existing PPG foundation models are either open-source but trained on clinical data or closed-source, limiting their applicability in real-world settings. We evaluate Pulse-PPG across multiple datasets and downstream tasks, comparing its performance against a state-of-the-art foundation model trained on clinical data. Our results demonstrate that Pulse-PPG, trained on uncurated field data, exhibits superior generalization across clinical and mobile health applications in both lab and field settings. This suggests that exposure to real-world variability enables the model to learn fine-grained representations, making it more adaptable across tasks. Furthermore, pre-training on field data surprisingly outperforms its pre-training on clinical data in many tasks, reinforcing the importance of training on real-world, diverse datasets. To encourage further advancements in robust foundation models leveraging field data, we plan to release Pulse-PPG, providing researchers with a powerful resource for developing more generalizable PPG-based models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01110</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01110</id><created>2025-02-03</created><authors><author><keyname>Carlet</keyname><forenames>Claude</forenames></author><author><keyname>Sarkar</keyname><forenames>Palash</forenames></author></authors><title>The Nonlinear Filter Model of Stream Cipher Redivivus</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The nonlinear filter model is an old and well understood approach to the design of secure stream ciphers. Extensive research over several decades has shown how to attack stream ciphers based on this model and has identified the security properties required of the Boolean function used as the filtering function to resist such attacks. This led to the problem of constructing Boolean functions which provide adequate security \textit{and} at the same time are efficient to implement. Unfortunately, over the last two decades no good solutions to this problem appeared in the literature. The lack of good solutions has effectively led to nonlinear filter model becoming more or less obsolete. This is a big loss to the cryptographic design toolkit, since the great advantages of the nonlinear filter model are its simplicity, well understood security and the potential to provide low cost solutions for hardware oriented stream ciphers. In this paper, we revive the nonlinear filter model by constructing appropriate Boolean functions which provide required security and are also efficient to implement. We put forward concrete suggestions of stream ciphers which are $\kappa$-bit secure against known types of attacks for $\kappa=80,128,160,192,224$ and $256$. For the $80$-bit, $128$-bit, and the $256$-bit security levels, the circuits for the corresponding stream ciphers require about 1743.5, 2771.5, and 5607.5 NAND gates respectively. For the $80$-bit and the $128$-bit security levels, the gate count estimates compare quite well to the famous ciphers Trivium and Grain-128a respectively, while for the $256$-bit security level, we do not know of any other stream cipher design which has such a low gate count. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01111</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01111</id><created>2025-02-03</created><authors><author><keyname>Cheng</keyname><forenames>Shijun</forenames></author><author><keyname>Harsuko</keyname><forenames>Randy</forenames></author><author><keyname>Alkhalifah</keyname><forenames>Tariq</forenames></author></authors><title>A generative foundation model for an all-in-one seismic processing   framework</title><categories>physics.geo-ph cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Seismic data often face challenges in their utilization due to noise contamination, incomplete acquisition, and limited low-frequency information, which hinder accurate subsurface imaging and interpretation. Traditional processing methods rely heavily on task-specific designs to address these challenges and fail to account for the variability of data. To address these limitations, we present a generative seismic foundation model (GSFM), a unified framework based on generative diffusion models (GDMs), designed to tackle multi-task seismic processing challenges, including denoising, backscattered noise attenuation, interpolation, and low-frequency extrapolation. GSFM leverages a pre-training stage on synthetic data to capture the features of clean, complete, and broadband seismic data distributions and applies an iterative fine-tuning strategy to adapt the model to field data. By adopting a target-oriented diffusion process prediction, GSFM improves computational efficiency without compromising accuracy. Synthetic data tests demonstrate GSFM surpasses benchmarks with equivalent architectures in all tasks and achieves performance comparable to traditional pre-training strategies, even after their fine-tuning. Also, field data tests suggest that our iterative fine-tuning approach addresses the generalization limitations of conventional pre-training and fine-tuning paradigms, delivering significantly enhanced performance across diverse tasks. Furthermore, GSFM's inherent probabilistic nature enables effective uncertainty quantification, offering valuable insights into the reliability of processing results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01113</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01113</id><created>2025-02-03</created><authors><author><keyname>Luo</keyname><forenames>Linhao</forenames></author><author><keyname>Zhao</keyname><forenames>Zicheng</forenames></author><author><keyname>Haffari</keyname><forenames>Gholamreza</forenames></author><author><keyname>Phung</keyname><forenames>Dinh</forenames></author><author><keyname>Gong</keyname><forenames>Chen</forenames></author><author><keyname>Pan</keyname><forenames>Shirui</forenames></author></authors><title>GFM-RAG: Graph Foundation Model for Retrieval Augmented Generation</title><categories>cs.IR cs.AI cs.CL</categories><comments>19 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Retrieval-augmented generation (RAG) has proven effective in integrating knowledge into large language models (LLMs). However, conventional RAGs struggle to capture complex relationships between pieces of knowledge, limiting their performance in intricate reasoning that requires integrating knowledge from multiple sources. Recently, graph-enhanced retrieval augmented generation (GraphRAG) builds graph structure to explicitly model these relationships, enabling more effective and efficient retrievers. Nevertheless, its performance is still hindered by the noise and incompleteness within the graph structure. To address this, we introduce GFM-RAG, a novel graph foundation model (GFM) for retrieval augmented generation. GFM-RAG is powered by an innovative graph neural network that reasons over graph structure to capture complex query-knowledge relationships. The GFM with 8M parameters undergoes a two-stage training process on large-scale datasets, comprising 60 knowledge graphs with over 14M triples and 700k documents. This results in impressive performance and generalizability for GFM-RAG, making it the first graph foundation model applicable to unseen datasets for retrieval without any fine-tuning required. Extensive experiments on three multi-hop QA datasets and seven domain-specific RAG datasets demonstrate that GFM-RAG achieves state-of-the-art performance while maintaining efficiency and alignment with neural scaling laws, highlighting its potential for further improvement. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01116</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01116</id><created>2025-02-03</created><authors><author><keyname>Li</keyname><forenames>Guanlin</forenames></author><author><keyname>Chen</keyname><forenames>Kangjie</forenames></author><author><keyname>Guo</keyname><forenames>Shangwei</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author><author><keyname>Qiu</keyname><forenames>Han</forenames></author><author><keyname>Zhang</keyname><forenames>Chao</forenames></author><author><keyname>Wang</keyname><forenames>Guoyin</forenames></author><author><keyname>Zhang</keyname><forenames>Tianwei</forenames></author><author><keyname>Li</keyname><forenames>Jiwei</forenames></author></authors><title>Picky LLMs and Unreliable RMs: An Empirical Study on Safety Alignment   after Instruction Tuning</title><categories>cs.AI cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models (LLMs) have emerged as powerful tools for addressing a wide range of general inquiries and tasks. Despite this, fine-tuning aligned LLMs on smaller, domain-specific datasets, critical to adapting them to specialized tasks, can inadvertently degrade their safety alignment, even when the datasets are benign. This phenomenon makes models more susceptible to providing inappropriate responses. In this study, we systematically examine the factors contributing to safety alignment degradation in benign fine-tuning scenarios. Our analysis identifies three critical factors affecting aligned LLMs: answer structure, identity calibration, and role-play. Additionally, we evaluate the reliability of state-of-the-art reward models (RMs), which are often used to guide alignment processes. Our findings reveal that these RMs frequently fail to accurately reflect human preferences regarding safety, underscoring their limitations in practical applications. By uncovering these challenges, our work highlights the complexities of maintaining safety alignment during fine-tuning and offers guidance to help developers balance utility and safety in LLMs. Datasets and fine-tuning code used in our experiments can be found in https://github.com/GuanlinLee/llm_instruction_tuning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01117</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01117</id><created>2025-02-03</created><authors><author><keyname>Guan</keyname><forenames>Yunchuan</forenames></author><author><keyname>Liu</keyname><forenames>Yu</forenames></author><author><keyname>Zhou</keyname><forenames>Ke</forenames></author><author><keyname>Shen</keyname><forenames>Zhiqi</forenames></author><author><keyname>Belongie</keyname><forenames>Serge</forenames></author><author><keyname>Hwang</keyname><forenames>Jenq-Neng</forenames></author><author><keyname>Li</keyname><forenames>Lei</forenames></author></authors><title>Learning to Learn Weight Generation via Trajectory Diffusion</title><categories>cs.LG cs.AI cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diffusion-based algorithms have emerged as promising techniques for weight generation, particularly in scenarios like multi-task learning that require frequent weight updates. However, existing solutions suffer from limited cross-task transferability. In addition, they only utilize optimal weights as training samples, ignoring the value of other weights in the optimization process. To address these issues, we propose Lt-Di, which integrates the diffusion algorithm with meta-learning to generate weights for unseen tasks. Furthermore, we extend the vanilla diffusion algorithm into a trajectory diffusion algorithm to utilize other weights along the optimization trajectory. Trajectory diffusion decomposes the entire diffusion chain into multiple shorter ones, improving training and inference efficiency. We analyze the convergence properties of the weight generation paradigm and improve convergence efficiency without additional time overhead. Our experiments demonstrate Lt-Di's higher accuracy while reducing computational overhead across various tasks, including zero-shot and few-shot learning, multi-domain generalization, and large-scale language model fine-tuning.Our code is released at https://github.com/tuantuange/Lt-Di. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01118</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01118</id><created>2025-02-03</created><authors><author><keyname>Sun</keyname><forenames>Jiahang</forenames></author><author><keyname>Wang</keyname><forenames>Zhiyong</forenames></author><author><keyname>Yang</keyname><forenames>Runhan</forenames></author><author><keyname>Xiao</keyname><forenames>Chenjun</forenames></author><author><keyname>Lui</keyname><forenames>John C. S.</forenames></author><author><keyname>Dai</keyname><forenames>Zhongxiang</forenames></author></authors><title>Large Language Model-Enhanced Multi-Armed Bandits</title><categories>cs.LG cs.AI</categories><comments>Preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large language models (LLMs) have been adopted to solve sequential decision-making tasks such as multi-armed bandits (MAB), in which an LLM is directly instructed to select the arms to pull in every iteration. However, this paradigm of direct arm selection using LLMs has been shown to be suboptimal in many MAB tasks. Therefore, we propose an alternative approach which combines the strengths of classical MAB and LLMs. Specifically, we adopt a classical MAB algorithm as the high-level framework and leverage the strong in-context learning capability of LLMs to perform the sub-task of reward prediction. Firstly, we incorporate the LLM-based reward predictor into the classical Thompson sampling (TS) algorithm and adopt a decaying schedule for the LLM temperature to ensure a transition from exploration to exploitation. Next, we incorporate the LLM-based reward predictor (with a temperature of 0) into a regression oracle-based MAB algorithm equipped with an explicit exploration mechanism. We also extend our TS-based algorithm to dueling bandits where only the preference feedback between pairs of arms is available, which requires non-trivial algorithmic modifications. We conduct empirical evaluations using both synthetic MAB tasks and experiments designed using real-world text datasets, in which the results show that our algorithms consistently outperform previous baseline methods based on direct arm selection. Interestingly, we also demonstrate that in challenging tasks where the arms lack semantic meanings that can be exploited by the LLM, our approach achieves considerably better performance than LLM-based direct arm selection. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01120</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01120</id><created>2025-02-03</created><authors><author><keyname>Krauthgamer</keyname><forenames>Robert</forenames></author><author><keyname>Petruschka</keyname><forenames>Nir</forenames></author></authors><title>Lipschitz Decompositions of Finite $\ell_{p}$ Metrics</title><categories>cs.CG math.MG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lipschitz decomposition is a useful tool in the design of efficient algorithms involving metric spaces. While many bounds are known for different families of finite metrics, the optimal parameters for $n$-point subsets of $\ell_p$, for $p &gt; 2$, remained open, see e.g. [Naor, SODA 2017]. We make significant progress on this question and establish the bound $\beta=O(\log^{1-1/p} n)$. Building on prior work, we demonstrate applications of this result to two problems, high-dimensional geometric spanners and distance labeling schemes. In addition, we sharpen a related decomposition bound for $1&lt;p&lt;2$, due to Filtser and Neiman [Algorithmica 2022]. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01122</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01122</id><created>2025-02-03</created><authors><author><keyname>Kanatsoulis</keyname><forenames>Charilaos I.</forenames></author><author><keyname>Choi</keyname><forenames>Evelyn</forenames></author><author><keyname>Jegelka</keyname><forenames>Stephanie</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>Learning Efficient Positional Encodings with Graph Neural Networks</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Positional encodings (PEs) are essential for effective graph representation learning because they provide position awareness in inherently position-agnostic transformer architectures and increase the expressive capacity of Graph Neural Networks (GNNs). However, designing powerful and efficient PEs for graphs poses significant challenges due to the absence of canonical node ordering and the scale of the graph. {In this work, we identify four key properties that graph PEs should satisfy}: stability, expressive power, scalability, and genericness. We find that existing eigenvector-based PE methods often fall short of jointly satisfying these criteria. To address this gap, we introduce PEARL, a novel framework of learnable PEs for graphs. Our primary insight is that message-passing GNNs function as nonlinear mappings of eigenvectors, enabling the design of GNN architectures for generating powerful and efficient PEs. A crucial challenge lies in initializing node attributes in a manner that is both expressive and permutation equivariant. We tackle this by initializing GNNs with random node inputs or standard basis vectors, thereby unlocking the expressive power of message-passing operations, while employing statistical pooling functions to maintain permutation equivariance. Our analysis demonstrates that PEARL approximates equivariant functions of eigenvectors with linear complexity, while rigorously establishing its stability and high expressive power. Experimental evaluations show that PEARL outperforms lightweight versions of eigenvector-based PEs and achieves comparable performance to full eigenvector-based PEs, but with one or two orders of magnitude lower complexity. Our code is available at https://github.com/ehejin/Pearl-PE. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01126</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01126</id><created>2025-02-03</created><authors><author><keyname>Shrivastava</keyname><forenames>Vaishnavi</forenames></author><author><keyname>Kumar</keyname><forenames>Ananya</forenames></author><author><keyname>Liang</keyname><forenames>Percy</forenames></author></authors><title>Language Models Prefer What They Know: Relative Confidence Estimation   via Confidence Preferences</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Language models (LMs) should provide reliable confidence estimates to help users detect mistakes in their outputs and defer to human experts when necessary. Asking a language model to assess its confidence ("Score your confidence from 0-1.") is a natural way of evaluating its uncertainty. However, models struggle to provide absolute assessments of confidence (i.e. judging confidence in answering a question independent of other questions) and the coarse-grained scores they produce are not useful for evaluating the correctness of their answers. We propose relative confidence estimation, where we match up questions against each other and ask the model to make relative judgments of confidence ("Which question are you more confident in answering correctly?"). Treating each question as a "player" in a series of matchups against other questions and the model's preferences as match outcomes, we can use rank aggregation methods like Elo rating and Bradley-Terry to translate the model's confidence preferences into confidence scores. We evaluate relative confidence estimation against absolute confidence estimation and self-consistency confidence methods on five state-of-the-art LMs -- GPT-4, GPT-4o, Gemini 1.5 Pro, Claude 3.5 Sonnet, and Llama 3.1 405B -- across 14 challenging STEM, social science, and commonsense reasoning question answering tasks. Our results demonstrate that relative confidence estimation consistently provides more reliable confidence scores than absolute confidence estimation, with average gains of 3.5% in selective classification AUC over direct absolute confidence estimation methods and 1.7% over self-consistency approaches across all models and datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01127</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01127</id><created>2025-02-03</created><authors><author><keyname>Wu</keyname><forenames>Young</forenames></author><author><keyname>Zhu</keyname><forenames>Yancheng</forenames></author><author><keyname>Cai</keyname><forenames>Jin-Yi</forenames></author><author><keyname>Zhu</keyname><forenames>Xiaojin</forenames></author></authors><title>The Battling Influencers Game: Nash Equilibria Structure of a Potential   Game and Implications to Value Alignment</title><categories>cs.GT cs.AI</categories><comments>9 pages, 8 figures, submitted to ICML</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  When multiple influencers attempt to compete for a receiver's attention, their influencing strategies must account for the presence of one another. We introduce the Battling Influencers Game (BIG), a multi-player simultaneous-move general-sum game, to provide a game-theoretic characterization of this social phenomenon. We prove that BIG is a potential game, that it has either one or an infinite number of pure Nash equilibria (NEs), and these pure NEs can be found by convex optimization. Interestingly, we also prove that at any pure NE, all (except at most one) influencers must exaggerate their actions to the maximum extent. In other words, it is rational for the influencers to be non-truthful and extreme because they anticipate other influencers to cancel out part of their influence. We discuss the implications of BIG to value alignment. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01128</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01128</id><created>2025-02-03</created><authors><author><keyname>Carlson</keyname><forenames>Fredrik Bagge</forenames></author><author><keyname>Tapscott</keyname><forenames>Cody</forenames></author><author><keyname>Baraldi</keyname><forenames>Gabriel</forenames></author><author><keyname>Rackauckas</keyname><forenames>Chris</forenames></author></authors><title>C-code generation considered unnecessary: go directly to binary, do not   pass C. Compilation of Julia code for deployment in model-based engineering</title><categories>eess.SY cs.SY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Since time immemorial an old adage has always seemed to ring true: you cannot use a high-level productive programming language like Python or R for real-time control and embedded-systems programming, you must rewrite your program in C. We present a counterexample to this mantra by demonstrating how recent compiler developments in the Julia programming language allow users of Julia and the equation-based modeling language ModelingToolkit to compile and deploy binaries for real-time model-based estimation and control. Contrary to the approach taken by a majority of modeling and simulation tools, we do not generate C code, and instead demonstrate how we may use the native Julia code-generation pipeline through LLVM to compile architecture-specific binaries from high-level code. This approach avoids many of the restrictions typically placed on high-level languages to enable C-code generation. As case studies, we include a nonlinear state estimator derived from an equation-based model which is compiled into a program that performs state estimation for deployment onto a Raspberry Pi, as well as a PID controller library implemented in Julia and compiled into a shared library callable from a C program. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01129</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01129</id><created>2025-02-03</created><authors><author><keyname>Malhotra</keyname><forenames>Shubham</forenames></author></authors><title>Deep Reinforcement Learning for Dynamic Resource Allocation in Wireless   Networks</title><categories>cs.DC cs.AI cs.ET cs.LG</categories><comments>6 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report investigates the application of deep reinforcement learning (DRL) algorithms for dynamic resource allocation in wireless communication systems. An environment that includes a base station, multiple antennas, and user equipment is created. Using the RLlib library, various DRL algorithms such as Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) are then applied. These algorithms are compared based on their ability to optimize resource allocation, focusing on the impact of different learning rates and scheduling policies. The findings demonstrate that the choice of algorithm and learning rate significantly influences system performance, with DRL providing more efficient resource allocation compared to traditional methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01130</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01130</id><created>2025-02-03</created><authors><author><keyname>Rafiei</keyname><forenames>Amin</forenames></author><author><keyname>MacLachlan</keyname><forenames>Scott</forenames></author></authors><title>Improved monolithic multigrid methods for high-order Taylor-Hood   discretizations</title><categories>math.NA cs.NA</categories><msc-class>65N30, 65N55, 65F10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerical simulation of incompressible fluid flows has been an active topic of research in Scientific Computing for many years, with many contributions to both discretizations and linear and nonlinear solvers. In this work, we propose an improved relaxation scheme for higher-order Taylor-Hood discretizations of the incompressible Stokes and Navier-Stokes equations, demonstrating its efficiency within monolithic multigrid preconditioners for the linear(ized) equations. The key to this improvement is an improved patch construction for Vanka-style relaxation introducing, for the first time, overlap in the pressure degrees of freedom within the patches. Numerical results demonstrate significant improvement in both multigrid iterations and time-to-solution for the linear Stokes case, on both triangular and quadrilateral meshes. For the nonlinear Navier-Stokes case, we show similar improvements, including in the number of nonlinear iterations needed in an inexact Newton method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01131</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01131</id><created>2025-02-03</created><authors><author><keyname>Munoz</keyname><forenames>Daniel</forenames></author></authors><title>Simple Linear Neuron Boosting</title><categories>cs.LG stat.ML</categories><comments>12 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Given a differentiable network architecture and loss function, we revisit optimizing the network's neurons in function space using Boosted Backpropagation (Grubb &amp; Bagnell, 2010), in contrast to optimizing in parameter space. From this perspective, we reduce descent in the space of linear functions that optimizes the network's backpropagated-errors to a preconditioned gradient descent algorithm. We show that this preconditioned update rule is equivalent to reparameterizing the network to whiten each neuron's features, with the benefit that the normalization occurs outside of inference. In practice, we use this equivalence to construct an online estimator for approximating the preconditioner and we propose an online, matrix-free learning algorithm with adaptive step sizes. The algorithm is applicable whenever autodifferentiation is available, including convolutional networks and transformers, and it is simple to implement for both the local and distributed training settings. We demonstrate fast convergence both in terms of epochs and wall clock time on a variety of tasks and networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01137</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01137</id><created>2025-02-03</created><authors><author><keyname>Malhotra</keyname><forenames>Shubham</forenames></author></authors><title>Self-Organizing Interaction Spaces: A Framework for Engineering   Pervasive Applications in Mobile and Distributed Environments</title><categories>cs.DC cs.AI cs.LG cs.SE</categories><comments>9 pages, 3 listings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid adoption of pervasive and mobile computing has led to an unprecedented rate of data production and consumption by mobile applications at the network edge. These applications often require interactions such as data exchange, behavior coordination, and collaboration, which are typically mediated by cloud servers. While cloud computing has been effective for distributed systems, challenges like latency, cost, and intermittent connectivity persist. With the advent of 5G technology, features like location-awareness and device-to-device (D2D) communication enable a more distributed and adaptive architecture. This paper introduces Self-Organizing Interaction Spaces (SOIS), a novel framework for engineering pervasive applications. SOIS leverages the dynamic and heterogeneous nature of mobile nodes, allowing them to form adaptive organizational structures based on their individual and social contexts. The framework provides two key abstractions for modeling and programming pervasive applications using an organizational mindset and mechanisms for adapting dynamic organizational structures. Case examples and performance evaluations of a simulated mobile crowd-sensing application demonstrate the feasibility and benefits of SOIS. Results highlight its potential to enhance efficiency and reduce reliance on traditional cloud models, paving the way for innovative solutions in mobile and distributed environments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01141</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01141</id><created>2025-02-03</created><authors><author><keyname>Chen</keyname><forenames>Qian</forenames></author><author><keyname>Rinderle-Ma</keyname><forenames>Stefanie</forenames></author><author><keyname>Wen</keyname><forenames>Lijie</forenames></author></authors><title>Beyond Yes or No: Predictive Compliance Monitoring Approaches for   Quantifying the Magnitude of Compliance Violations</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most existing process compliance monitoring approaches detect compliance violations in an ex post manner. Only predicate prediction focuses on predicting them. However, predicate prediction provides a binary yes/no notion of compliance, lacking the ability to measure to which extent an ongoing process instance deviates from the desired state as specified in constraints. Here, being able to quantify the magnitude of violation would provide organizations with deeper insights into their operational performance, enabling informed decision making to reduce or mitigate the risk of non-compliance. Thus, we propose two predictive compliance monitoring approaches to close this research gap. The first approach reformulates the binary classification problem as a hybrid task that considers both classification and regression, while the second employs a multi-task learning method to explicitly predict the compliance status and the magnitude of violation for deviant cases simultaneously. In this work, we focus on temporal constraints as they are significant in almost any application domain, e.g., health care. The evaluation on synthetic and real-world event logs demonstrates that our approaches are capable of quantifying the magnitude of violations while maintaining comparable performance for compliance predictions achieved by state-of-the-art approaches. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01142</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01142</id><created>2025-02-03</created><authors><author><keyname>Guan</keyname><forenames>Xinyan</forenames></author><author><keyname>Zeng</keyname><forenames>Jiali</forenames></author><author><keyname>Meng</keyname><forenames>Fandong</forenames></author><author><keyname>Xin</keyname><forenames>Chunlei</forenames></author><author><keyname>Lu</keyname><forenames>Yaojie</forenames></author><author><keyname>Lin</keyname><forenames>Hongyu</forenames></author><author><keyname>Han</keyname><forenames>Xianpei</forenames></author><author><keyname>Sun</keyname><forenames>Le</forenames></author><author><keyname>Zhou</keyname><forenames>Jie</forenames></author></authors><title>DeepRAG: Thinking to Retrieval Step by Step for Large Language Models</title><categories>cs.AI cs.CL cs.IR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) have shown remarkable potential in reasoning while they still suffer from severe factual hallucinations due to timeliness, accuracy, and coverage of parametric knowledge. Meanwhile, integrating reasoning with retrieval-augmented generation (RAG) remains challenging due to ineffective task decomposition and redundant retrieval, which can introduce noise and degrade response quality. In this paper, we propose DeepRAG, a framework that models retrieval-augmented reasoning as a Markov Decision Process (MDP), enabling strategic and adaptive retrieval. By iteratively decomposing queries, DeepRAG dynamically determines whether to retrieve external knowledge or rely on parametric reasoning at each step. Experiments show that DeepRAG improves retrieval efficiency while improving answer accuracy by 21.99%, demonstrating its effectiveness in optimizing retrieval-augmented reasoning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01143</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01143</id><created>2025-02-03</created><authors><author><keyname>He</keyname><forenames>Tairan</forenames></author><author><keyname>Gao</keyname><forenames>Jiawei</forenames></author><author><keyname>Xiao</keyname><forenames>Wenli</forenames></author><author><keyname>Zhang</keyname><forenames>Yuanhang</forenames></author><author><keyname>Wang</keyname><forenames>Zi</forenames></author><author><keyname>Wang</keyname><forenames>Jiashun</forenames></author><author><keyname>Luo</keyname><forenames>Zhengyi</forenames></author><author><keyname>He</keyname><forenames>Guanqi</forenames></author><author><keyname>Sobanbab</keyname><forenames>Nikhil</forenames></author><author><keyname>Pan</keyname><forenames>Chaoyi</forenames></author><author><keyname>Yi</keyname><forenames>Zeji</forenames></author><author><keyname>Qu</keyname><forenames>Guannan</forenames></author><author><keyname>Kitani</keyname><forenames>Kris</forenames></author><author><keyname>Hodgins</keyname><forenames>Jessica</forenames></author><author><keyname>Fan</keyname><forenames>Linxi "Jim"</forenames></author><author><keyname>Zhu</keyname><forenames>Yuke</forenames></author><author><keyname>Liu</keyname><forenames>Changliu</forenames></author><author><keyname>Shi</keyname><forenames>Guanya</forenames></author></authors><title>ASAP: Aligning Simulation and Real-World Physics for Learning Agile   Humanoid Whole-Body Skills</title><categories>cs.RO cs.AI cs.LG cs.SY eess.SY</categories><comments>Project website: https://agile.human2humanoid.com/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humanoid robots hold the potential for unparalleled versatility in performing human-like, whole-body skills. However, achieving agile and coordinated whole-body motions remains a significant challenge due to the dynamics mismatch between simulation and the real world. Existing approaches, such as system identification (SysID) and domain randomization (DR) methods, often rely on labor-intensive parameter tuning or result in overly conservative policies that sacrifice agility. In this paper, we present ASAP (Aligning Simulation and Real-World Physics), a two-stage framework designed to tackle the dynamics mismatch and enable agile humanoid whole-body skills. In the first stage, we pre-train motion tracking policies in simulation using retargeted human motion data. In the second stage, we deploy the policies in the real world and collect real-world data to train a delta (residual) action model that compensates for the dynamics mismatch. Then, ASAP fine-tunes pre-trained policies with the delta action model integrated into the simulator to align effectively with real-world dynamics. We evaluate ASAP across three transfer scenarios: IsaacGym to IsaacSim, IsaacGym to Genesis, and IsaacGym to the real-world Unitree G1 humanoid robot. Our approach significantly improves agility and whole-body coordination across various dynamic motions, reducing tracking error compared to SysID, DR, and delta dynamics learning baselines. ASAP enables highly agile motions that were previously difficult to achieve, demonstrating the potential of delta action learning in bridging simulation and real-world dynamics. These results suggest a promising sim-to-real direction for developing more expressive and agile humanoids. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01145</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01145</id><created>2025-02-03</created><authors><author><keyname>Issaid</keyname><forenames>Chaouki Ben</forenames></author><author><keyname>Vepakomma</keyname><forenames>Praneeth</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author></authors><title>Tackling Feature and Sample Heterogeneity in Decentralized Multi-Task   Learning: A Sheaf-Theoretic Approach</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Federated multi-task learning (FMTL) aims to simultaneously learn multiple related tasks across clients without sharing sensitive raw data. However, in the decentralized setting, existing FMTL frameworks are limited in their ability to capture complex task relationships and handle feature and sample heterogeneity across clients. To address these challenges, we introduce a novel sheaf-theoretic-based approach for FMTL. By representing client relationships using cellular sheaves, our framework can flexibly model interactions between heterogeneous client models. We formulate the sheaf-based FMTL optimization problem using sheaf Laplacian regularization and propose the Sheaf-FMTL algorithm to solve it. We show that the proposed framework provides a unified view encompassing many existing federated learning (FL) and FMTL approaches. Furthermore, we prove that our proposed algorithm, Sheaf-FMTL, achieves a sublinear convergence rate in line with state-of-the-art decentralized FMTL algorithms. Extensive experiments demonstrate that Sheaf-FMTL exhibits communication savings by sending significantly fewer bits compared to decentralized FMTL baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01146</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01146</id><created>2025-02-03</created><authors><author><keyname>Du</keyname><forenames>Yuxuan</forenames></author><author><keyname>Wang</keyname><forenames>Xinbiao</forenames></author><author><keyname>Guo</keyname><forenames>Naixu</forenames></author><author><keyname>Yu</keyname><forenames>Zhan</forenames></author><author><keyname>Qian</keyname><forenames>Yang</forenames></author><author><keyname>Zhang</keyname><forenames>Kaining</forenames></author><author><keyname>Hsieh</keyname><forenames>Min-Hsiu</forenames></author><author><keyname>Rebentrost</keyname><forenames>Patrick</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author></authors><title>Quantum Machine Learning: A Hands-on Tutorial for Machine Learning   Practitioners and Researchers</title><categories>quant-ph cs.AI cs.LG</categories><comments>260 pages; Comments are welcome</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This tutorial intends to introduce readers with a background in AI to quantum machine learning (QML) -- a rapidly evolving field that seeks to leverage the power of quantum computers to reshape the landscape of machine learning. For self-consistency, this tutorial covers foundational principles, representative QML algorithms, their potential applications, and critical aspects such as trainability, generalization, and computational complexity. In addition, practical code demonstrations are provided in https://qml-tutorial.github.io/ to illustrate real-world implementations and facilitate hands-on learning. Together, these elements offer readers a comprehensive overview of the latest advancements in QML. By bridging the gap between classical machine learning and quantum computing, this tutorial serves as a valuable resource for those looking to engage with QML and explore the forefront of AI in the quantum era. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01148</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01148</id><created>2025-02-03</created><authors><author><keyname>Huang</keyname><forenames>Xiajie</forenames></author><author><keyname>Wang</keyname><forenames>Fei</forenames></author><author><keyname>Han</keyname><forenames>Weimin</forenames></author><author><keyname>Ling</keyname><forenames>Min</forenames></author></authors><title>A Discontinuous Galerkin Method for H(curl)-Elliptic Hemivariational   Inequalities</title><categories>math.NA cs.NA math.AP</categories><comments>28 pages, 3 figures</comments><msc-class>65N30 (Primary), 35Q61, 49J40, 49J52 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a Discontinuous Galerkin (DG) method for solving H(curl)-elliptic hemivariational inequalities. By selecting an appropriate numerical flux, we construct an Interior Penalty Discontinuous Galerkin (IPDG) scheme. A comprehensive numerical analysis of the IPDG method is conducted, addressing key aspects such as consistency, boundedness, stability, and the existence, uniqueness, uniform boundedness of the numerical solutions. Building on these properties, we establish a priori error estimates, demonstrating the optimal convergence order of the numerical solutions under suitable solution regularity assumptions. Finally, a numerical example is presented to illustrate the theoretically predicted convergence order and to show the effectiveness of the proposed method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01152</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01152</id><created>2025-02-03</created><authors><author><keyname>Zhou</keyname><forenames>Nanjun</forenames></author><author><keyname>Lin</keyname><forenames>Weilin</forenames></author><author><keyname>Liu</keyname><forenames>Li</forenames></author></authors><title>Gradient Norm-based Fine-Tuning for Backdoor Defense in Automatic Speech   Recognition</title><categories>cs.SD cs.LG eess.AS</categories><comments>5 pages, 5 figures. This work has been accpeted by ICASSP 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Backdoor attacks have posed a significant threat to the security of deep neural networks (DNNs). Despite considerable strides in developing defenses against backdoor attacks in the visual domain, the specialized defenses for the audio domain remain empty. Furthermore, the defenses adapted from the visual to audio domain demonstrate limited effectiveness. To fill this gap, we propose Gradient Norm-based FineTuning (GN-FT), a novel defense strategy against the attacks in the audio domain, based on the observation from the corresponding backdoored models. Specifically, we first empirically find that the backdoored neurons exhibit greater gradient values compared to other neurons, while clean neurons stay the lowest. On this basis, we fine-tune the backdoored model by incorporating the gradient norm regularization, aiming to weaken and reduce the backdoored neurons. We further approximate the loss computation for lower implementation costs. Extensive experiments on two speech recognition datasets across five models demonstrate the superior performance of our proposed method. To the best of our knowledge, this work is the first specialized and effective defense against backdoor attacks in the audio domain. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01154</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01154</id><created>2025-02-03</created><authors><author><keyname>Hsu</keyname><forenames>Yu-Ling</forenames></author><author><keyname>Su</keyname><forenames>Hsuan</forenames></author><author><keyname>Chen</keyname><forenames>Shang-Tse</forenames></author></authors><title>Jailbreaking with Universal Multi-Prompts</title><categories>cs.CL cs.AI cs.CR cs.LG</categories><comments>Accepted by NAACL Findings 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models (LLMs) have seen rapid development in recent years, revolutionizing various applications and significantly enhancing convenience and productivity. However, alongside their impressive capabilities, ethical concerns and new types of attacks, such as jailbreaking, have emerged. While most prompting techniques focus on optimizing adversarial inputs for individual cases, resulting in higher computational costs when dealing with large datasets. Less research has addressed the more general setting of training a universal attacker that can transfer to unseen tasks. In this paper, we introduce JUMP, a prompt-based method designed to jailbreak LLMs using universal multi-prompts. We also adapt our approach for defense, which we term DUMP. Experimental results demonstrate that our method for optimizing universal multi-prompts outperforms existing techniques. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01156</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01156</id><created>2025-02-03</created><authors><author><keyname>Houache</keyname><forenames>Samy</forenames><affiliation>IMB</affiliation></author><author><keyname>Aujol</keyname><forenames>Jean François</forenames><affiliation>UB, IMB</affiliation></author><author><keyname>Traonmilin</keyname><forenames>Yann</forenames><affiliation>IMB</affiliation></author></authors><title>On the impact of the parametrization of deep convolutional neural   networks on post-training quantization</title><categories>cs.IT math.IT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces novel theoretical approximation bounds for the output of quantized neural networks, with a focus on convolutional neural networks (CNN). By considering layerwise parametrization and focusing on the quantization of weights, we provide bounds that gain several orders of magnitude compared to state-of-the art results on classical deep convolutional neural netorks such as MobileNetV2 or ResNets. These gains are achieved by improving the behaviour of the approximation bounds with respect to the depth parameter, which has the most impact on the approximation error induced by quantization. To complement our theoretical result, we provide a numerical exploration of our bounds on Mo-bileNetV2 and ResNets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01157</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01157</id><created>2025-02-03</created><authors><author><keyname>Govindarajan</keyname><forenames>Shrisudhan</forenames></author><author><keyname>Rebain</keyname><forenames>Daniel</forenames></author><author><keyname>Yi</keyname><forenames>Kwang Moo</forenames></author><author><keyname>Tagliasacchi</keyname><forenames>Andrea</forenames></author></authors><title>Radiant Foam: Real-Time Differentiable Ray Tracing</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on differentiable scene representations is consistently moving towards more efficient, real-time models. Recently, this has led to the popularization of splatting methods, which eschew the traditional ray-based rendering of radiance fields in favor of rasterization. This has yielded a significant improvement in rendering speeds due to the efficiency of rasterization algorithms and hardware, but has come at a cost: the approximations that make rasterization efficient also make implementation of light transport phenomena like reflection and refraction much more difficult. We propose a novel scene representation which avoids these approximations, but keeps the efficiency and reconstruction quality of splatting by leveraging a decades-old efficient volumetric mesh ray tracing algorithm which has been largely overlooked in recent computer vision research. The resulting model, which we name Radiant Foam, achieves rendering speed and quality comparable to Gaussian Splatting, without the constraints of rasterization. Unlike ray traced Gaussian models that use hardware ray tracing acceleration, our method requires no special hardware or APIs beyond the standard features of a programmable GPU. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01158</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01158</id><created>2025-02-03</created><authors><author><keyname>Guerra-Manzanares</keyname><forenames>Alejandro</forenames></author><author><keyname>Shamout</keyname><forenames>Farah E.</forenames></author></authors><title>MIND: Modality-Informed Knowledge Distillation Framework for Multimodal   Clinical Prediction Tasks</title><categories>cs.LG cs.CV</categories><comments>Published in Transactions on Machine Learning Research (01/2025),   https://openreview.net/forum?id=BhOJreYmur&amp;noteId=ymnAhncuez</comments><journal-ref>Transactions on Machine Learning Research (TMLR), 01/2025</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multimodal fusion leverages information across modalities to learn better feature representations with the goal of improving performance in fusion-based tasks. However, multimodal datasets, especially in medical settings, are typically smaller than their unimodal counterparts, which can impede the performance of multimodal models. Additionally, the increase in the number of modalities is often associated with an overall increase in the size of the multimodal network, which may be undesirable in medical use cases. Utilizing smaller unimodal encoders may lead to sub-optimal performance, particularly when dealing with high-dimensional clinical data. In this paper, we propose the Modality-INformed knowledge Distillation (MIND) framework, a multimodal model compression approach based on knowledge distillation that transfers knowledge from ensembles of pre-trained deep neural networks of varying sizes into a smaller multimodal student. The teacher models consist of unimodal networks, allowing the student to learn from diverse representations. MIND employs multi-head joint fusion models, as opposed to single-head models, enabling the use of unimodal encoders in the case of unimodal samples without requiring imputation or masking of absent modalities. As a result, MIND generates an optimized multimodal model, enhancing both multimodal and unimodal representations. It can also be leveraged to balance multimodal learning during training. We evaluate MIND on binary and multilabel clinical prediction tasks using time series data and chest X-ray images. Additionally, we assess the generalizability of the MIND framework on three non-medical multimodal multiclass datasets. Experimental results demonstrate that MIND enhances the performance of the smaller multimodal network across all five tasks, as well as various fusion methods and multimodal architectures, compared to state-of-the-art baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01159</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01159</id><created>2025-02-03</created><authors><author><keyname>Li</keyname><forenames>Chenyue</forenames></author><author><keyname>Deng</keyname><forenames>Wen</forenames></author><author><keyname>Lu</keyname><forenames>Mengqian</forenames></author><author><keyname>Yuan</keyname><forenames>Binhang</forenames></author></authors><title>AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model   for Atmospheric Science</title><categories>cs.LG cs.AI</categories><comments>16 pages, 3 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid advancements in large language models (LLMs), particularly in their reasoning capabilities, hold transformative potential for addressing complex challenges in atmospheric science. However, leveraging LLMs effectively in this domain requires a robust and comprehensive evaluation benchmark. To address this need, we present AtmosSci-Bench, a novel benchmark designed to systematically assess LLM performance across five core categories of atmospheric science problems: hydrology, atmospheric dynamics, atmospheric physics, geophysics, and physical oceanography. We employ a template-based question generation framework, enabling scalable and diverse multiple-choice questions curated from graduate-level atmospheric science problems. We conduct a comprehensive evaluation of representative LLMs, categorized into four groups: instruction-tuned models, advanced reasoning models, math-augmented models, and domain-specific climate models. Our analysis provides some interesting insights into the reasoning and problem-solving capabilities of LLMs in atmospheric science. We believe AtmosSci-Bench can serve as a critical step toward advancing LLM applications in climate service by offering a standard and rigorous evaluation framework. Our source codes are currently available at https://github.com/Relaxed-System-Lab/AtmosSci-Bench. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01160</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01160</id><created>2025-02-03</created><authors><author><keyname>Lai</keyname><forenames>Yong</forenames></author><author><keyname>Tong</keyname><forenames>Haolong</forenames></author><author><keyname>Xu</keyname><forenames>Zhenghang</forenames></author><author><keyname>Yin</keyname><forenames>Minghao</forenames></author></authors><title>Scalable Precise Computation of Shannon Entropy</title><categories>cs.AI cs.IT math.IT</categories><comments>9 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantitative information flow analyses (QIF) are a class of techniques for measuring the amount of confidential information leaked by a program to its public outputs.   Shannon entropy is an important method to quantify the amount of leakage in QIF.   This paper focuses on the programs modeled in Boolean constraints and optimizes the two stages of the Shannon entropy computation to implement a scalable precise tool PSE.   In the first stage, we design a knowledge compilation language called \ADDAND that combines Algebraic Decision Diagrams and conjunctive decomposition.   \ADDAND avoids enumerating possible outputs of a program and supports tractable entropy computation.   In the second stage, we optimize the model counting queries that are used to compute the probabilities of outputs.   We compare PSE with the state-of-the-art probably approximately correct tool EntropyEstimation, which was shown to significantly outperform the existing precise tools.   The experimental results demonstrate that PSE solved 55 more benchmarks compared to EntropyEstimation in a total of 441. For 98% of the benchmarks that both PSE and EntropyEstimation solved, PSE is at least $10\times$ as efficient as EntropyEstimation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01163</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01163</id><created>2025-02-03</created><authors><author><keyname>Emmerich</keyname><forenames>Michael</forenames></author></authors><title>Minimum Riesz s-Energy Subset Selection in Ordered Point Sets via   Dynamic Programming</title><categories>cs.DS</categories><comments>10 pages, 5 figures, conference or other essential info</comments><acm-class>F.2.2</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a dynamic programming algorithm for selecting a representative subset of size $k$ of points from a given set with $n$ points such that the Riesz s-energy is minimized. Whereas in general dimensions the problem is NP hard, in the one-dimensional case, the natural ordering of the data points allows for an efficient recursion. This approach is then extended to problems related to two-dimensional Pareto front representations arising in biobjective optimization problems. The proposed methods guarantee an optimal solution under the assumption of sorted (or non-dominated) input, and the overall time complexity is shown to be $O(n^2 k)$. Alongside the theory, we offer computational examples and an open-source Python implementation of the algorithms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01167</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01167</id><created>2025-02-03</created><authors><author><keyname>Sliwowski</keyname><forenames>Daniel</forenames></author><author><keyname>Lee</keyname><forenames>Dongheui</forenames></author></authors><title>ConditionNET: Learning Preconditions and Effects for Execution   Monitoring</title><categories>cs.RO cs.LG</categories><comments>9 pages, 5 figures, 3 tables</comments><journal-ref>in IEEE Robotics and Automation Letters, vol. 10, no. 2, pp.   1337-1344, Feb. 2025</journal-ref><doi>10.1109/LRA.2024.3520916</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The introduction of robots into everyday scenarios necessitates algorithms capable of monitoring the execution of tasks. In this paper, we propose ConditionNET, an approach for learning the preconditions and effects of actions in a fully data-driven manner. We develop an efficient vision-language model and introduce additional optimization objectives during training to optimize for consistent feature representations. ConditionNET explicitly models the dependencies between actions, preconditions, and effects, leading to improved performance. We evaluate our model on two robotic datasets, one of which we collected for this paper, containing 406 successful and 138 failed teleoperated demonstrations of a Franka Emika Panda robot performing tasks like pouring and cleaning the counter. We show in our experiments that ConditionNET outperforms all baselines on both anomaly detection and phase prediction tasks. Furthermore, we implement an action monitoring system on a real robot to demonstrate the practical applicability of the learned preconditions and effects. Our results highlight the potential of ConditionNET for enhancing the reliability and adaptability of robots in real-world environments. The data is available on the project website: https://dsliwowski1.github.io/ConditionNET_page. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01170</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01170</id><created>2025-02-03</created><authors><author><keyname>Kou</keyname><forenames>Zhiqiang</forenames></author><author><keyname>Qin</keyname><forenames>Si</forenames></author><author><keyname>Wang</keyname><forenames>Hailin</forenames></author><author><keyname>Xie</keyname><forenames>Mingkun</forenames></author><author><keyname>Chen</keyname><forenames>Shuo</forenames></author><author><keyname>Jia</keyname><forenames>Yuheng</forenames></author><author><keyname>Liu</keyname><forenames>Tongliang</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author><author><keyname>Geng</keyname><forenames>Xin</forenames></author></authors><title>Label Distribution Learning with Biased Annotations by Learning   Multi-Label Representation</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multi-label learning (MLL) has gained attention for its ability to represent real-world data. Label Distribution Learning (LDL), an extension of MLL to learning from label distributions, faces challenges in collecting accurate label distributions. To address the issue of biased annotations, based on the low-rank assumption, existing works recover true distributions from biased observations by exploring the label correlations. However, recent evidence shows that the label distribution tends to be full-rank, and naive apply of low-rank approximation on biased observation leads to inaccurate recovery and performance degradation. In this paper, we address the LDL with biased annotations problem from a novel perspective, where we first degenerate the soft label distribution into a hard multi-hot label and then recover the true label information for each instance. This idea stems from an insight that assigning hard multi-hot labels is often easier than assigning a soft label distribution, and it shows stronger immunity to noise disturbances, leading to smaller label bias. Moreover, assuming that the multi-label space for predicting label distributions is low-rank offers a more reasonable approach to capturing label correlations. Theoretical analysis and experiments confirm the effectiveness and robustness of our method on real-world datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01171</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01171</id><created>2025-02-03</created><authors><author><keyname>Luo</keyname><forenames>Erpai</forenames></author><author><keyname>Wei</keyname><forenames>Xinran</forenames></author><author><keyname>Huang</keyname><forenames>Lin</forenames></author><author><keyname>Li</keyname><forenames>Yunyang</forenames></author><author><keyname>Yang</keyname><forenames>Han</forenames></author><author><keyname>Wang</keyname><forenames>Zun</forenames></author><author><keyname>Liu</keyname><forenames>Chang</forenames></author><author><keyname>Xia</keyname><forenames>Zaishuo</forenames></author><author><keyname>Zhang</keyname><forenames>Jia</forenames></author><author><keyname>Shao</keyname><forenames>Bin</forenames></author></authors><title>Efficient and Scalable Density Functional Theory Hamiltonian Prediction   through Adaptive Sparsity</title><categories>cs.LG physics.comp-ph</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Hamiltonian matrix prediction is pivotal in computational chemistry, serving as the foundation for determining a wide range of molecular properties. While SE(3) equivariant graph neural networks have achieved remarkable success in this domain, their substantial computational cost-driven by high-order tensor product (TP) operations-restricts their scalability to large molecular systems with extensive basis sets. To address this challenge, we introduce SPHNet, an efficient and scalable equivariant network that incorporates adaptive sparsity into Hamiltonian prediction. SPHNet employs two innovative sparse gates to selectively constrain non-critical interaction combinations, significantly reducing tensor product computations while maintaining accuracy. To optimize the sparse representation, we develop a Three-phase Sparsity Scheduler, ensuring stable convergence and achieving high performance at sparsity rates of up to 70 percent. Extensive evaluations on QH9 and PubchemQH datasets demonstrate that SPHNet achieves state-of-the-art accuracy while providing up to a 7x speedup over existing models. Beyond Hamiltonian prediction, the proposed sparsification techniques also hold significant potential for improving the efficiency and scalability of other SE(3) equivariant networks, further broadening their applicability and impact. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01172</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01172</id><created>2025-02-03</created><authors><author><keyname>Lakemann</keyname><forenames>Tim Felix</forenames></author><author><keyname>Licea</keyname><forenames>Daniel Bonilla</forenames></author><author><keyname>Walter</keyname><forenames>Viktor</forenames></author><author><keyname>Báča</keyname><forenames>Tomáš</forenames></author><author><keyname>Saska</keyname><forenames>Martin</forenames></author></authors><title>Towards Agile Swarming in Real World: Onboard Relative Localization with   Fast Tracking of Active Blinking Markers</title><categories>cs.RO cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel onboard tracking approach enabling vision-based relative localization and communication using Active blinking Marker Tracking (AMT) is introduced in this article. Active blinking markers on multi-robot team members improve the robustness of relative localization for aerial vehicles in tightly coupled swarms during real-world deployments, while also serving as a resilient communication channel. Traditional tracking algorithms struggle to track fast moving blinking markers due to their intermittent appearance in the camera frames. AMT addresses this by using weighted polynomial regression to predict the future appearance of active blinking markers while accounting for uncertainty in the prediction. In outdoor experiments, the AMT approach outperformed state-of-the-art methods in tracking density, accuracy, and complexity. The experimental validation of this novel tracking approach for relative localization involved testing motion patterns motivated by our research on agile multi-robot deployment. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01177</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01177</id><created>2025-02-03</created><authors><author><keyname>Blöcker</keyname><forenames>Christopher</forenames></author><author><keyname>Rosvall</keyname><forenames>Martin</forenames></author><author><keyname>Scholtes</keyname><forenames>Ingo</forenames></author><author><keyname>West</keyname><forenames>Jevin D.</forenames></author></authors><title>Insights from Network Science can advance Deep Graph Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep graph learning and network science both analyze graphs but approach similar problems from different perspectives. Whereas network science focuses on models and measures that reveal the organizational principles of complex systems with explicit assumptions, deep graph learning focuses on flexible and generalizable models that learn patterns in graph data in an automated fashion. Despite these differences, both fields share the same goal: to better model and understand patterns in graph-structured data. Early efforts to integrate methods, models, and measures from network science and deep graph learning indicate significant untapped potential. In this position, we explore opportunities at their intersection. We discuss open challenges in deep graph learning, including data augmentation, improved evaluation practices, higher-order models, and pooling methods. Likewise, we highlight challenges in network science, including scaling to massive graphs, integrating continuous gradient-based optimization, and developing standardized benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01179</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01179</id><created>2025-02-03</created><authors><author><keyname>Lai</keyname><forenames>Wen</forenames></author><author><keyname>Fraser</keyname><forenames>Alexander</forenames></author><author><keyname>Titov</keyname><forenames>Ivan</forenames></author></authors><title>Joint Localization and Activation Editing for Low-Resource Fine-Tuning</title><categories>cs.CL cs.AI cs.LG</categories><comments>The code for the method is released at   https://github.com/wenlai-lavine/jola</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, are commonly used to adapt LLMs. However, the effectiveness of standard PEFT methods is limited in low-resource scenarios with only a few hundred examples. Recent advances in interpretability research have inspired the emergence of activation editing techniques, which modify the activations of specific model components. These methods, due to their extremely small parameter counts, show promise for small datasets. However, their performance is highly dependent on identifying the correct modules to edit and often lacks stability across different datasets. In this paper, we propose Joint Localization and Activation Editing (JoLA), a method that jointly learns (1) which heads in the Transformer to edit (2) whether the intervention should be additive, multiplicative, or both and (3) the intervention parameters themselves - the vectors applied as additive offsets or multiplicative scalings to the head output. Through evaluations on three benchmarks spanning commonsense reasoning, natural language understanding, and natural language generation, we demonstrate that JoLA consistently outperforms existing methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01180</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01180</id><created>2025-02-03</created><authors><author><keyname>Gurpegui</keyname><forenames>Alba</forenames></author><author><keyname>Tegling</keyname><forenames>Emma</forenames></author><author><keyname>Rantzer</keyname><forenames>Anders</forenames></author></authors><title>A Minimax Optimal Controller for Positive Systems</title><categories>math.OC cs.SY eess.SY</categories><comments>Presented at the 26th International Symposium on Mathematical Theory   of Networks and Systems (Cambridge, UK)</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We present an explicit solution to the discrete-time Bellman equation for minimax optimal control of positive systems under unconstrained disturbances. The primary contribution of our result relies on deducing a bound for the disturbance penalty, which characterizes the existence of a finite solution to the problem class. Moreover, this constraint on the disturbance penalty reveals that, in scenarios where a solution is feasible, the problem converges to its equivalent minimization problem in the absence of disturbances. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01181</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01181</id><created>2025-02-03</created><authors><author><keyname>Wu</keyname><forenames>Zhiliang</forenames></author><author><keyname>Chen</keyname><forenames>Kerui</forenames></author><author><keyname>Li</keyname><forenames>Kun</forenames></author><author><keyname>Fan</keyname><forenames>Hehe</forenames></author><author><keyname>Yang</keyname><forenames>Yi</forenames></author></authors><title>BVINet: Unlocking Blind Video Inpainting with Zero Annotations</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Video inpainting aims to fill in corrupted regions of the video with plausible contents. Existing methods generally assume that the locations of corrupted regions are known, focusing primarily on the "how to inpaint". This reliance necessitates manual annotation of the corrupted regions using binary masks to indicate "whereto inpaint". However, the annotation of these masks is labor-intensive and expensive, limiting the practicality of current methods. In this paper, we expect to relax this assumption by defining a new blind video inpainting setting, enabling the networks to learn the mapping from corrupted video to inpainted result directly, eliminating the need of corrupted region annotations. Specifically, we propose an end-to-end blind video inpainting network (BVINet) to address both "where to inpaint" and "how to inpaint" simultaneously. On the one hand, BVINet can predict the masks of corrupted regions by detecting semantic-discontinuous regions of the frame and utilizing temporal consistency prior of the video. On the other hand, the predicted masks are incorporated into the BVINet, allowing it to capture valid context information from uncorrupted regions to fill in corrupted ones. Besides, we introduce a consistency loss to regularize the training parameters of BVINet. In this way, mask prediction and video completion mutually constrain each other, thereby maximizing the overall performance of the trained model. Furthermore, we customize a dataset consisting of synthetic corrupted videos, real-world corrupted videos, and their corresponding completed videos. This dataset serves as a valuable resource for advancing blind video inpainting research. Extensive experimental results demonstrate the effectiveness and superiority of our method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01182</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01182</id><created>2025-02-03</created><authors><author><keyname>Oh</keyname><forenames>Seokjin</forenames></author><author><keyname>Noh</keyname><forenames>Keonwoong</forenames></author><author><keyname>Jung</keyname><forenames>Woohwan</forenames></author></authors><title>A Single Model Ensemble Framework for Neural Machine Translation using   Pivot Translation</title><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Despite the significant advances in neural machine translation, performance remains subpar for low-resource language pairs. Ensembling multiple systems is a widely adopted technique to enhance performance, often accomplished by combining probability distributions. However, the previous approaches face the challenge of high computational costs for training multiple models. Furthermore, for black-box models, averaging token-level probabilities at each decoding step is not feasible. To address the problems of multi-model ensemble methods, we present a pivot-based single model ensemble. The proposed strategy consists of two steps: pivot-based candidate generation and post-hoc aggregation. In the first step, we generate candidates through pivot translation. This can be achieved with only a single model and facilitates knowledge transfer from high-resource pivot languages, resulting in candidates that are not only diverse but also more accurate. Next, in the aggregation step, we select k high-quality candidates from the generated candidates and merge them to generate a final translation that outperforms the existing candidates. Our experimental results show that our method produces translations of superior quality by leveraging candidates from pivot translation to capture the subtle nuances of the source sentence. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01183</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01183</id><created>2025-02-03</created><authors><author><keyname>Guo</keyname><forenames>Qianyu</forenames></author><author><keyname>Wu</keyname><forenames>Jingrong</forenames></author><author><keyname>Wu</keyname><forenames>Tianxing</forenames></author><author><keyname>Wang</keyname><forenames>Haofen</forenames></author><author><keyname>Ge</keyname><forenames>Weifeng</forenames></author><author><keyname>Zhang</keyname><forenames>Wenqiang</forenames></author></authors><title>Enhancing Environmental Robustness in Few-shot Learning via Conditional   Representation Learning</title><categories>cs.CV</categories><comments>15 pages, 8 figures, Accepted by IEEE Transactions on Image   Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Few-shot learning (FSL) has recently been extensively utilized to overcome the scarcity of training data in domain-specific visual recognition. In real-world scenarios, environmental factors such as complex backgrounds, varying lighting conditions, long-distance shooting, and moving targets often cause test images to exhibit numerous incomplete targets or noise disruptions. However, current research on evaluation datasets and methodologies has largely ignored the concept of "environmental robustness", which refers to maintaining consistent performance in complex and diverse physical environments. This neglect has led to a notable decline in the performance of FSL models during practical testing compared to their training performance. To bridge this gap, we introduce a new real-world multi-domain few-shot learning (RD-FSL) benchmark, which includes four domains and six evaluation datasets. The test images in this benchmark feature various challenging elements, such as camouflaged objects, small targets, and blurriness. Our evaluation experiments reveal that existing methods struggle to utilize training images effectively to generate accurate feature representations for challenging test images. To address this problem, we propose a novel conditional representation learning network (CRLNet) that integrates the interactions between training and testing images as conditional information in their respective representation processes. The main goal is to reduce intra-class variance or enhance inter-class variance at the feature representation level. Finally, comparative experiments reveal that CRLNet surpasses the current state-of-the-art methods, achieving performance improvements ranging from 6.83% to 16.98% across diverse settings and backbones. The source code and dataset are available at https://github.com/guoqianyu-alberta/Conditional-Representation-Learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01184</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01184</id><created>2025-02-03</created><authors><author><keyname>Samanta</keyname><forenames>Ankur</forenames></author><author><keyname>Gupta</keyname><forenames>Rohan</forenames></author><author><keyname>Misra</keyname><forenames>Aditi</forenames></author><author><keyname>Clarke</keyname><forenames>Christian McIntosh</forenames></author><author><keyname>Rajadas</keyname><forenames>Jayakumar</forenames></author></authors><title>FragmentNet: Adaptive Graph Fragmentation for Graph-to-Sequence   Molecular Representation Learning</title><categories>cs.LG cs.AI physics.chem-ph q-bio.QM</categories><comments>22 pages, 13 figures, 5 tables</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Molecular property prediction uses molecular structure to infer chemical properties. Chemically interpretable representations that capture meaningful intramolecular interactions enhance the usability and effectiveness of these predictions. However, existing methods often rely on atom-based or rule-based fragment tokenization, which can be chemically suboptimal and lack scalability. We introduce FragmentNet, a graph-to-sequence foundation model with an adaptive, learned tokenizer that decomposes molecular graphs into chemically valid fragments while preserving structural connectivity. FragmentNet integrates VQVAE-GCN for hierarchical fragment embeddings, spatial positional encodings for graph serialization, global molecular descriptors, and a transformer. Pre-trained with Masked Fragment Modeling and fine-tuned on MoleculeNet tasks, FragmentNet outperforms models with similarly scaled architectures and datasets while rivaling larger state-of-the-art models requiring significantly more resources. This novel framework enables adaptive decomposition, serialization, and reconstruction of molecular graphs, facilitating fragment-based editing and visualization of property trends in learned embeddings - a powerful tool for molecular design and optimization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01185</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01185</id><created>2025-02-03</created><authors><author><keyname>Mishaly</keyname><forenames>Yehuda</forenames></author><author><keyname>Wolf</keyname><forenames>Lior</forenames></author><author><keyname>Nachmani</keyname><forenames>Eliya</forenames></author></authors><title>Deep Active Speech Cancellation with Multi-Band Mamba Network</title><categories>cs.SD cs.AI cs.LG eess.AS eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel deep learning network for Active Speech Cancellation (ASC), advancing beyond Active Noise Cancellation (ANC) methods by effectively canceling both noise and speech signals. The proposed Multi-Band Mamba architecture segments input audio into distinct frequency bands, enabling precise anti-signal generation and improved phase alignment across frequencies. Additionally, we introduce an optimization-driven loss function that provides near-optimal supervisory signals for anti-signal generation. Experimental results demonstrate substantial performance gains, achieving up to 7.2dB improvement in ANC scenarios and 6.2dB in ASC, significantly outperforming existing methods. Audio samples are available at https://mishalydev.github.io/DeepASC-Demo </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01186</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01186</id><created>2025-02-03</created><authors><author><keyname>Labedan</keyname><forenames>Patrice</forenames></author><author><keyname>Drougard</keyname><forenames>Nicolas</forenames></author><author><keyname>Berezin</keyname><forenames>Alexandre</forenames></author><author><keyname>Sun</keyname><forenames>Guowei</forenames></author><author><keyname>Dieulafait</keyname><forenames>Francis</forenames></author></authors><title>A High-Accuracy SSIM-based Scoring System for Coin Die Link   Identification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analyses of ancient coins, and especially the identification of those struck with the same die, provides invaluable information for archaeologists and historians. Nowadays, these die links are identified manually, which makes the process laborious, if not impossible when big treasures are discovered as the number of comparisons is too large. This study introduces advances that promise to streamline and enhance archaeological coin analysis. Our contributions include: 1) First publicly accessible labeled dataset of coin pictures (329 images) for die link detection, facilitating method benchmarking; 2) Novel SSIM-based scoring method for rapid and accurate discrimination of coin pairs, outperforming current techniques used in this research field; 3) Evaluation of clustering techniques using our score, demonstrating near-perfect die link identification. We provide datasets, to foster future research and the development of even more powerful tools for archaeology, and more particularly for numismatics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01187</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01187</id><created>2025-02-03</created><authors><author><keyname>Li</keyname><forenames>Hao</forenames></author><author><keyname>Huang</keyname><forenames>Di</forenames></author><author><keyname>Wang</keyname><forenames>Ziyu</forenames></author><author><keyname>Rahmani</keyname><forenames>Amir M.</forenames></author></authors><title>Skewed Memorization in Large Language Models: Quantification and   Decomposition</title><categories>cs.AI cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Memorization in Large Language Models (LLMs) poses privacy and security risks, as models may unintentionally reproduce sensitive or copyrighted data. Existing analyses focus on average-case scenarios, often neglecting the highly skewed distribution of memorization. This paper examines memorization in LLM supervised fine-tuning (SFT), exploring its relationships with training duration, dataset size, and inter-sample similarity. By analyzing memorization probabilities over sequence lengths, we link this skewness to the token generation process, offering insights for estimating memorization and comparing it to established metrics. Through theoretical analysis and empirical evaluation, we provide a comprehensive understanding of memorization behaviors and propose strategies to detect and mitigate risks, contributing to more privacy-preserving LLMs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01188</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01188</id><created>2025-02-03</created><authors><author><keyname>Zahid</keyname><forenames>Anam</forenames></author><author><keyname>Ali</keyname><forenames>Abdur Rehman</forenames></author><author><keyname>Raza</keyname><forenames>Shaina</forenames></author><author><keyname>Shahnawaz</keyname><forenames>Rai</forenames></author><author><keyname>Kamiran</keyname><forenames>Faisal</forenames></author><author><keyname>Karim</keyname><forenames>Asim</forenames></author></authors><title>FairUDT: Fairness-aware Uplift Decision Trees</title><categories>cs.LG stat.ML</categories><comments>Published in Knowledge-based Systems (2025)</comments><journal-ref>Knowledge-based Systems 311 (2025) 113068</journal-ref><doi>10.1016/j.knosys.2025.113068</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Training data used for developing machine learning classifiers can exhibit biases against specific protected attributes. Such biases typically originate from historical discrimination or certain underlying patterns that disproportionately under-represent minority groups, such as those identified by their gender, religion, or race. In this paper, we propose a novel approach, FairUDT, a fairness-aware Uplift-based Decision Tree for discrimination identification. FairUDT demonstrates how the integration of uplift modeling with decision trees can be adapted to include fair splitting criteria. Additionally, we introduce a modified leaf relabeling approach for removing discrimination. We divide our dataset into favored and deprived groups based on a binary sensitive attribute, with the favored dataset serving as the treatment group and the deprived dataset as the control group. By applying FairUDT and our leaf relabeling approach to preprocess three benchmark datasets, we achieve an acceptable accuracy-discrimination tradeoff. We also show that FairUDT is inherently interpretable and can be utilized in discrimination detection tasks. The code for this project is available https://github.com/ara-25/FairUDT </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01189</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01189</id><created>2025-02-03</created><authors><author><keyname>Ohayon</keyname><forenames>Guy</forenames></author><author><keyname>Manor</keyname><forenames>Hila</forenames></author><author><keyname>Michaeli</keyname><forenames>Tomer</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author></authors><title>Compressed Image Generation with Denoising Diffusion Codebook Models</title><categories>eess.IV cs.AI cs.CV cs.IT eess.SP math.IT</categories><comments>Code and demo are available at https://ddcm-2025.github.io/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel generative approach based on Denoising Diffusion Models (DDMs), which produces high-quality image samples along with their losslessly compressed bit-stream representations. This is obtained by replacing the standard Gaussian noise sampling in the reverse diffusion with a selection of noise samples from pre-defined codebooks of fixed iid Gaussian vectors. Surprisingly, we find that our method, termed Denoising Diffusion Codebook Model (DDCM), retains sample quality and diversity of standard DDMs, even for extremely small codebooks. We leverage DDCM and pick the noises from the codebooks that best match a given image, converting our generative model into a highly effective lossy image codec achieving state-of-the-art perceptual image compression results. More generally, by setting other noise selections rules, we extend our compression method to any conditional image generation task (e.g., image restoration), where the generated images are produced jointly with their condensed bit-stream representations. Our work is accompanied by a mathematical interpretation of the proposed compressed conditional generation schemes, establishing a connection with score-based approximations of posterior samplers for the tasks considered. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01190</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01190</id><created>2025-02-03</created><authors><author><keyname>Eum</keyname><forenames>Seungho</forenames></author><author><keyname>Cho</keyname><forenames>Ihjoon</forenames></author><author><keyname>Kim</keyname><forenames>Junghyeon</forenames></author></authors><title>Dance recalibration for dance coherency with recurrent convolution block</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  With the recent advancements in generative AI such as GAN, Diffusion, and VAE, the use of generative AI for dance generation has seen significant progress and received considerable interest. In this study, We propose R-Lodge, an enhanced version of Lodge. R-Lodge incorporates Recurrent Sequential Representation Learning named Dance Recalibration to original coarse-to-fine long dance generation model. R-Lodge utilizes Dance Recalibration method using $N$ Dance Recalibration Block to address the lack of consistency in the coarse dance representation of the Lodge model. By utilizing this method, each generated dance motion incorporates a bit of information from the previous dance motions. We evaluate R-Lodge on FineDance dataset and the results show that R-Lodge enhances the consistency of the whole generated dance motions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01191</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01191</id><created>2025-02-03</created><authors><author><keyname>Cai</keyname><forenames>Yuxuan</forenames></author><author><keyname>Wang</keyname><forenames>Xiyu</forenames></author><author><keyname>Tsutsui</keyname><forenames>Satoshi</forenames></author><author><keyname>Pang</keyname><forenames>Winnie</forenames></author><author><keyname>Wen</keyname><forenames>Bihan</forenames></author></authors><title>Towards Robust and Reliable Concept Representations:   Reliability-Enhanced Concept Embedding Model</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Concept Bottleneck Models (CBMs) aim to enhance interpretability by predicting human-understandable concepts as intermediates for decision-making. However, these models often face challenges in ensuring reliable concept representations, which can propagate to downstream tasks and undermine robustness, especially under distribution shifts. Two inherent issues contribute to concept unreliability: sensitivity to concept-irrelevant features (e.g., background variations) and lack of semantic consistency for the same concept across different samples. To address these limitations, we propose the Reliability-Enhanced Concept Embedding Model (RECEM), which introduces a two-fold strategy: Concept-Level Disentanglement to separate irrelevant features from concept-relevant information and a Concept Mixup mechanism to ensure semantic alignment across samples. These mechanisms work together to improve concept reliability, enabling the model to focus on meaningful object attributes and generate faithful concept representations. Experimental results demonstrate that RECEM consistently outperforms existing baselines across multiple datasets, showing superior performance under background and domain shifts. These findings highlight the effectiveness of disentanglement and alignment strategies in enhancing both reliability and robustness in CBMs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01193</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01193</id><created>2025-02-03</created><authors><author><keyname>Kouam</keyname><forenames>Anne Josiane</forenames><affiliation>TRiBE</affiliation></author><author><keyname>Viana</keyname><forenames>Aline Carneiro</forenames><affiliation>TRiBE</affiliation></author><author><keyname>Martins</keyname><forenames>Philippe</forenames><affiliation>INFRES</affiliation></author><author><keyname>Adjih</keyname><forenames>Cedric</forenames><affiliation>TRiBE</affiliation></author><author><keyname>Tchana</keyname><forenames>Alain</forenames><affiliation>Grenoble INP</affiliation></author></authors><title>SigN: SIMBox Activity Detection Through Latency Anomalies at the   Cellular Edge</title><categories>cs.NI</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite their widespread adoption, cellular networks face growing vulnerabilities due to their inherent complexity and the integration of advanced technologies. One of the major threats in this landscape is Voice over IP (VoIP) to GSM gateways, known as SIMBox devices. These devices use multiple SIM cards to route VoIP traffic through cellular networks, enabling international bypass fraud with losses of up to $3.11 billion annually. Beyond financial impact, SIMBox activity degrades network performance, threatens national security, and facilitates eavesdropping on communications. Existing detection methods for SIMBox activity are hindered by evolving fraud techniques and implementation complexities, limiting their practical adoption in operator networks.This paper addresses the limitations of current detection methods by introducing SigN , a novel approach to identifying SIMBox activity at the cellular edge. The proposed method focuses on detecting remote SIM card association, a technique used by SIMBox appliances to mimic human mobility patterns. The method detects latency anomalies between SIMBox and standard devices by analyzing cellular signaling during network attachment. Extensive indoor and outdoor experiments demonstrate that SIMBox devices generate significantly higher attachment latencies, particularly during the authentication phase, where latency is up to 23 times greater than that of standard devices. We attribute part of this overhead to immutable factors such as LTE authentication standards and Internet-based communication protocols. Therefore, our approach offers a robust, scalable, and practical solution to mitigate SIMBox activity risks at the network edge. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01194</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01194</id><created>2025-02-03</created><authors><author><keyname>Tonglet</keyname><forenames>Jonathan</forenames></author><author><keyname>Thiem</keyname><forenames>Gabriel</forenames></author><author><keyname>Gurevych</keyname><forenames>Iryna</forenames></author></authors><title>COVE: COntext and VEracity prediction for out-of-context images</title><categories>cs.CL</categories><comments>Camera-ready version accepted to NAACL 2025 Main Conference</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Images taken out of their context are the most prevalent form of multimodal misinformation. Debunking them requires (1) providing the true context of the image and (2) checking the veracity of the image's caption. However, existing automated fact-checking methods fail to tackle both objectives explicitly. In this work, we introduce COVE, a new method that predicts first the true COntext of the image and then uses it to predict the VEracity of the caption. COVE beats the SOTA context prediction model on all context items, often by more than five percentage points. It is competitive with the best veracity prediction models on synthetic data and outperforms them on real-world data, showing that it is beneficial to combine the two tasks sequentially. Finally, we conduct a human study that reveals that the predicted context is a reusable and interpretable artifact to verify new out-of-context captions for the same image. Our code and data are made available. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01197</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01197</id><created>2025-02-03</created><authors><author><keyname>Ang</keyname><forenames>Elijah H. W.</forenames></author><author><keyname>De Wagter</keyname><forenames>Christophe</forenames></author><author><keyname>de Croon</keyname><forenames>Guido C. H. E.</forenames></author></authors><title>Multi-objective Evolution of Drone Morphology</title><categories>cs.RO</categories><comments>7 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of multicopter drones has remained almost the same since its inception. While conventional designs, such as the quadcopter, work well in many cases, they may not be optimal in specific environments or missions. This paper revisits rotary drone design by exploring which body morphologies are optimal for different objectives and constraints. Specifically, an evolutionary algorithm is used to produce optimal drone morphologies for three objectives: (1) high thrust-to-weight ratio, (2) high maneuverability, and (3) small size. To generate a range of optimal drones with performance trade-offs between them, the non-dominated sorting genetic algorithm II, or NSGA-II is used. A randomly sampled population of 600 is evolved over 2000 generations. The NSGA-II algorithm evolved drone bodies that outperform a standard 5-inch 220 mm wheelbase quadcopter in at least one of the three objectives. The three extrema in the Pareto front show improvement of 487.8%, 23.5% and 4.8% in maneuverability, thrust-to-weight ratio and size, respectively. The improvement in maneuverability can be attributed to the tilt angles of the propellers, while the increase in thrust-to-weight ratio is primarily due to the higher number of propellers. The quadcopter is located on the Pareto front for the three objectives. However, our results also show that other designs can be better depending on the objectives. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01199</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01199</id><created>2025-02-03</created><authors><author><keyname>Huang</keyname><forenames>Haiduo</forenames></author><author><keyname>Liu</keyname><forenames>Zhenhua</forenames></author><author><keyname>Xia</keyname><forenames>Tian</forenames></author><author><keyname>zhao</keyname><forenames>Wenzhe</forenames></author><author><keyname>Ren</keyname><forenames>Pengju</forenames></author></authors><title>Nearly Lossless Adaptive Bit Switching</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model quantization is widely applied for compressing and accelerating deep neural networks (DNNs). However, conventional Quantization-Aware Training (QAT) focuses on training DNNs with uniform bit-width. The bit-width settings vary across different hardware and transmission demands, which induces considerable training and storage costs. Hence, the scheme of one-shot joint training multiple precisions is proposed to address this issue. Previous works either store a larger FP32 model to switch between different precision models for higher accuracy or store a smaller INT8 model but compromise accuracy due to using shared quantization parameters. In this paper, we introduce the Double Rounding quantization method, which fully utilizes the quantized representation range to accomplish nearly lossless bit-switching while reducing storage by using the highest integer precision instead of full precision. Furthermore, we observe a competitive interference among different precisions during one-shot joint training, primarily due to inconsistent gradients of quantization scales during backward propagation. To tackle this problem, we propose an Adaptive Learning Rate Scaling (ALRS) technique that dynamically adapts learning rates for various precisions to optimize the training process. Additionally, we extend our Double Rounding to one-shot mixed precision training and develop a Hessian-Aware Stochastic Bit-switching (HASB) strategy. Experimental results on the ImageNet-1K classification demonstrate that our methods have enough advantages to state-of-the-art one-shot joint QAT in both multi-precision and mixed-precision. We also validate the feasibility of our method on detection and segmentation tasks, as well as on LLMs task. Our codes are available at https://github.com/haiduo/Double-Rounding. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01201</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01201</id><created>2025-02-03</created><authors><author><keyname>Li</keyname><forenames>Yiyue</forenames></author><author><keyname>Zhang</keyname><forenames>Shaoting</forenames></author><author><keyname>Li</keyname><forenames>Kang</forenames></author><author><keyname>Lao</keyname><forenames>Qicheng</forenames></author></authors><title>One-to-Normal: Anomaly Personalization for Few-shot Anomaly Detection</title><categories>cs.CV</categories><comments>In The Thirty-eighth Annual Conference on Neural Information   Processing Systems (NeurIPS2024)</comments><msc-class>68T45</msc-class><acm-class>I.2.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional Anomaly Detection (AD) methods have predominantly relied on unsupervised learning from extensive normal data. Recent AD methods have evolved with the advent of large pre-trained vision-language models, enhancing few-shot anomaly detection capabilities. However, these latest AD methods still exhibit limitations in accuracy improvement. One contributing factor is their direct comparison of a query image's features with those of few-shot normal images. This direct comparison often leads to a loss of precision and complicates the extension of these techniques to more complex domains--an area that remains underexplored in a more refined and comprehensive manner. To address these limitations, we introduce the anomaly personalization method, which performs a personalized one-to-normal transformation of query images using an anomaly-free customized generation model, ensuring close alignment with the normal manifold. Moreover, to further enhance the stability and robustness of prediction results, we propose a triplet contrastive anomaly inference strategy, which incorporates a comprehensive comparison between the query and generated anomaly-free data pool and prompt information. Extensive evaluations across eleven datasets in three domains demonstrate our model's effectiveness compared to the latest AD methods. Additionally, our method has been proven to transfer flexibly to other AD methods, with the generated image data effectively improving the performance of other AD methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01203</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01203</id><created>2025-02-03</created><authors><author><keyname>Aminian</keyname><forenames>Gholamali</forenames></author><author><keyname>Asadi</keyname><forenames>Amir R.</forenames></author><author><keyname>Shenfeld</keyname><forenames>Idan</forenames></author><author><keyname>Mroueh</keyname><forenames>Youssef</forenames></author></authors><title>Theoretical Analysis of KL-regularized RLHF with Multiple Reference   Models</title><categories>cs.LG stat.ML</categories><comments>Under review</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent methods for aligning large language models (LLMs) with human feedback predominantly rely on a single reference model, which limits diversity, model overfitting, and underutilizes the wide range of available pre-trained models. Incorporating multiple reference models has the potential to address these limitations by broadening perspectives, reducing bias, and leveraging the strengths of diverse open-source LLMs. However, integrating multiple reference models into reinforcement learning with human feedback (RLHF) frameworks poses significant theoretical challenges, particularly in reverse KL-regularization, where achieving exact solutions has remained an open problem. This paper presents the first \emph{exact solution} to the multiple reference model problem in reverse KL-regularized RLHF. We introduce a comprehensive theoretical framework that includes rigorous statistical analysis and provides sample complexity guarantees. Additionally, we extend our analysis to forward KL-regularized RLHF, offering new insights into sample complexity requirements in multiple reference scenarios. Our contributions lay the foundation for more advanced and adaptable LLM alignment techniques, enabling the effective use of multiple reference models. This work paves the way for developing alignment frameworks that are both theoretically sound and better suited to the challenges of modern AI ecosystems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01204</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01204</id><created>2025-02-03</created><authors><author><keyname>Ait-Bachir</keyname><forenames>Romuald</forenames><affiliation>ODYSSEY, IMT Atlantique - MEE, Lab-STICC\_OSE</affiliation></author><author><keyname>Granero-Belinchon</keyname><forenames>Carlos</forenames><affiliation>ODYSSEY, IMT Atlantique - MEE, Lab-STICC\_OSE</affiliation></author><author><keyname>Michel</keyname><forenames>Aurélie</forenames><affiliation>CESBIO, CNES</affiliation></author><author><keyname>Michel</keyname><forenames>Julien</forenames><affiliation>CESBIO, CNES</affiliation></author><author><keyname>Briottet</keyname><forenames>Xavier</forenames><affiliation>Lab-STICC\_OSE, IMT Atlantique - MEE, ODYSSEY</affiliation></author><author><keyname>Drumetz</keyname><forenames>Lucas</forenames><affiliation>Lab-STICC\_OSE, IMT Atlantique - MEE, ODYSSEY</affiliation></author></authors><title>Land Surface Temperature Super-Resolution with a Scale-Invariance-Free   Neural Approach: Application to MODIS</title><categories>cs.LG cs.CV</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the trade-off between the temporal and spatial resolution of thermal spaceborne sensors, super-resolution methods have been developed to provide fine-scale Land SurfaceTemperature (LST) maps. Most of them are trained at low resolution but applied at fine resolution, and so they require a scale-invariance hypothesis that is not always adapted. Themain contribution of this work is the introduction of a Scale-Invariance-Free approach for training Neural Network (NN) models, and the implementation of two NN models, calledScale-Invariance-Free Convolutional Neural Network for Super-Resolution (SIF-CNN-SR) for the super-resolution of MODIS LST products. The Scale-Invariance-Free approach consists ontraining the models in order to provide LST maps at high spatial resolution that recover the initial LST when they are degraded at low resolution and that contain fine-scale texturesinformed by the high resolution NDVI. The second contribution of this work is the release of a test database with ASTER LST images concomitant with MODIS ones that can be usedfor evaluation of super-resolution algorithms. We compare the two proposed models, SIF-CNN-SR1 and SIF-CNN-SR2, with four state-of-the-art methods, Bicubic, DMS, ATPRK, Tsharp,and a CNN sharing the same architecture as SIF-CNN-SR but trained under the scale-invariance hypothesis. We show that SIF-CNN-SR1 outperforms the state-of-the-art methods and the other two CNN models as evaluated with LPIPS and Fourier space metrics focusing on the analysis of textures. These results and the available ASTER-MODIS database for evaluation are promising for future studies on super-resolution of LST. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01205</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01205</id><created>2025-02-03</created><authors><author><keyname>Kanerva</keyname><forenames>Jenna</forenames></author><author><keyname>Ledins</keyname><forenames>Cassandra</forenames></author><author><keyname>Käpyaho</keyname><forenames>Siiri</forenames></author><author><keyname>Ginter</keyname><forenames>Filip</forenames></author></authors><title>OCR Error Post-Correction with LLMs in Historical Documents: No Free   Lunches</title><categories>cs.CL</categories><comments>To be published in RESOURCEFUL 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Optical Character Recognition (OCR) systems often introduce errors when transcribing historical documents, leaving room for post-correction to improve text quality. This study evaluates the use of open-weight LLMs for OCR error correction in historical English and Finnish datasets. We explore various strategies, including parameter optimization, quantization, segment length effects, and text continuation methods. Our results demonstrate that while modern LLMs show promise in reducing character error rates (CER) in English, a practically useful performance for Finnish was not reached. Our findings highlight the potential and limitations of LLMs in scaling OCR post-correction for large historical corpora. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01206</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01206</id><created>2025-02-03</created><authors><author><keyname>Zhao</keyname><forenames>Xinlong</forenames></author><author><keyname>Sun</keyname><forenames>Jiande</forenames></author><author><keyname>Zhang</keyname><forenames>Jia</forenames></author><author><keyname>Hou</keyname><forenames>Sujuan</forenames></author><author><keyname>Li</keyname><forenames>Shuai</forenames></author><author><keyname>Liu</keyname><forenames>Tong</forenames></author><author><keyname>Liu</keyname><forenames>Ke</forenames></author></authors><title>PerfSeer: An Efficient and Accurate Deep Learning Models Performance   Predictor</title><categories>cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting the performance of deep learning (DL) models, such as execution time and resource utilization, is crucial for Neural Architecture Search (NAS), DL cluster schedulers, and other technologies that advance deep learning. The representation of a model is the foundation for its performance prediction. However, existing methods cannot comprehensively represent diverse model configurations, resulting in unsatisfactory accuracy. To address this, we represent a model as a graph that includes the topology, along with the node, edge, and global features, all of which are crucial for effectively capturing the performance of the model. Based on this representation, we propose PerfSeer, a novel predictor that uses a Graph Neural Network (GNN)-based performance prediction model, SeerNet. SeerNet fully leverages the topology and various features, while incorporating optimizations such as Synergistic Max-Mean aggregation (SynMM) and Global-Node Perspective Boost (GNPB) to capture the critical performance information more effectively, enabling it to predict the performance of models accurately. Furthermore, SeerNet can be extended to SeerNet-Multi by using Project Conflicting Gradients (PCGrad), enabling efficient simultaneous prediction of multiple performance metrics without significantly affecting accuracy. We constructed a dataset containing performance metrics for 53k+ model configurations, including execution time, memory usage, and Streaming Multiprocessor (SM) utilization during both training and inference. The evaluation results show that PerfSeer outperforms nn-Meter, Brp-NAS, and DIPPM. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01207</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01207</id><created>2025-02-03</created><authors><author><keyname>Homburger</keyname><forenames>Hannes</forenames></author><author><keyname>Wirtensohn</keyname><forenames>Stefan</forenames></author><author><keyname>Hoher</keyname><forenames>Patrick</forenames></author><author><keyname>Baur</keyname><forenames>Tim</forenames></author><author><keyname>Griesser</keyname><forenames>Dennis</forenames></author><author><keyname>Diehl</keyname><forenames>Moritz</forenames></author><author><keyname>Reuter</keyname><forenames>Johannes</forenames></author></authors><title>Solgenia -- A Test Vessel Toward Energy-Efficient Autonomous Water Taxi   Applications</title><categories>cs.RO cs.SY eess.SY</categories><comments>17 pages, 22 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous surface vessels are a promising building block of the future's transport sector and are investigated by research groups worldwide. This paper presents a comprehensive and systematic overview of the autonomous research vessel Solgenia including the latest investigations and recently presented methods that contributed to the fields of autonomous systems, applied numerical optimization, nonlinear model predictive control, multi-extended-object-tracking, computer vision, and collision avoidance. These are considered to be the main components of autonomous water taxi applications. Autonomous water taxis have the potential to transform the traffic in cities close to the water into a more efficient, sustainable, and flexible future state. Regarding this transformation, the test platform Solgenia offers an opportunity to gain new insights by investigating novel methods in real-world experiments. An established test platform will strongly reduce the effort required for real-world experiments in the future. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01208</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01208</id><created>2025-02-03</created><authors><author><keyname>Ji</keyname><forenames>Xiaotong</forenames></author><author><keyname>Ramesh</keyname><forenames>Shyam Sundhar</forenames></author><author><keyname>Zimmer</keyname><forenames>Matthieu</forenames></author><author><keyname>Bogunovic</keyname><forenames>Ilija</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Ammar</keyname><forenames>Haitham Bou</forenames></author></authors><title>Almost Surely Safe Alignment of Large Language Models at Inference-Time</title><categories>cs.LG cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Even highly capable large language models (LLMs) can produce biased or unsafe responses, and alignment techniques, such as RLHF, aimed at mitigating this issue, are expensive and prone to overfitting as they retrain the LLM. This paper introduces a novel inference-time alignment approach that ensures LLMs generate safe responses almost surely, i.e., with a probability approaching one. We achieve this by framing the safe generation of inference-time responses as a constrained Markov decision process within the LLM's latent space. Crucially, we augment a safety state that tracks the evolution of safety constraints and enables us to demonstrate formal safety guarantees upon solving the MDP in the latent space. Building on this foundation, we propose InferenceGuard, a practical implementation that safely aligns LLMs without modifying the model weights. Empirically, we demonstrate InferenceGuard effectively balances safety and task performance, outperforming existing inference-time alignment methods in generating safe and aligned responses. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01210</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01210</id><created>2025-02-03</created><authors><author><keyname>Kirkham</keyname><forenames>Sam</forenames></author><author><keyname>Strycharczuk</keyname><forenames>Patrycja</forenames></author><author><keyname>Davies</keyname><forenames>Rob</forenames></author><author><keyname>Welburn</keyname><forenames>Danielle</forenames></author></authors><title>Modelling change in neural dynamics during phonetic accommodation</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Short-term phonetic accommodation is a fundamental driver behind accent change, but how does real-time input from another speaker's voice shape the speech planning representations of an interlocutor? We advance a computational model of change in phonetic representations during phonetic accommodation, grounded in dynamic neural field equations for movement planning and memory dynamics. We test the model's ability to capture empirical patterns from an experimental study where speakers shadowed a model talker with a different accent from their own. The experimental data shows vowel-specific degrees of convergence during shadowing, followed by return to baseline (or minor divergence) post-shadowing. The model can reproduce these phenomena by modulating the magnitude of inhibitory memory dynamics, which may reflect resistance to accommodation due to phonological and/or sociolinguistic pressures. We discuss the implications of these results for the relation between short-term phonetic accommodation and longer-term patterns of sound change. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01211</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01211</id><created>2025-02-03</created><authors><author><keyname>Bothmann</keyname><forenames>Ludwig</forenames></author><author><keyname>Boustani</keyname><forenames>Philip A.</forenames></author><author><keyname>Alvarez</keyname><forenames>Jose M.</forenames></author><author><keyname>Casalicchio</keyname><forenames>Giuseppe</forenames></author><author><keyname>Bischl</keyname><forenames>Bernd</forenames></author><author><keyname>Dandl</keyname><forenames>Susanne</forenames></author></authors><title>Privilege Scores</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bias-transforming methods of fairness-aware machine learning aim to correct a non-neutral status quo with respect to a protected attribute (PA). Current methods, however, lack an explicit formulation of what drives non-neutrality. We introduce privilege scores (PS) to measure PA-related privilege by comparing the model predictions in the real world with those in a fair world in which the influence of the PA is removed. At the individual level, PS can identify individuals who qualify for affirmative action; at the global level, PS can inform bias-transforming policies. After presenting estimation methods for PS, we propose privilege score contributions (PSCs), an interpretation method that attributes the origin of privilege to mediating features and direct effects. We provide confidence intervals for both PS and PSCs. Experiments on simulated and real-world data demonstrate the broad applicability of our methods and provide novel insights into gender and racial privilege in mortgage and college admissions applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01214</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01214</id><created>2025-02-03</created><authors><author><keyname>Maglione-Mathey</keyname><forenames>German</forenames></author><author><keyname>Escudero-Sahuquillo</keyname><forenames>Jesus</forenames></author><author><keyname>Garcia</keyname><forenames>Pedro Javier</forenames></author><author><keyname>Quiles</keyname><forenames>Francisco J.</forenames></author><author><keyname>Zahavi</keyname><forenames>Eitan</forenames></author></authors><title>Leveraging InfiniBand Controller to Configure Deadlock-Free Routing   Engines for Dragonflies</title><categories>cs.NI cs.DC</categories><journal-ref>Journal of Parallel and Distributed Computing (2021)</journal-ref><doi>10.1016/j.jpdc.2020.07.010</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Dragonfly topology is currently one of the most popular network topologies in high-performance parallel systems. The interconnection networks of many of these systems are built from components based on the InfiniBand specification. However, due to some constraints in this specification, the available versions of the InfiniBand network controller (OpenSM) do not include routing engines based on some popular deadlock-free routing algorithms proposed theoretically for Dragonflies, such as the one proposed by Kim and Dally based on Virtual-Channel shifting. In this paper we propose a straightforward method to integrate this routing algorithm in OpenSM as a routing engine, explaining in detail the configuration required to support it. We also provide experiment results, obtained both from a real InfiniBand-based cluster and from simulation, to validate the new routing engine and to compare its performance and requirements against other routing engines currently available in OpenSM. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01215</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01215</id><created>2025-02-03</created><authors><author><keyname>Chen</keyname><forenames>Jiehua</forenames></author><author><keyname>Schlotter</keyname><forenames>Ildikó</forenames></author></authors><title>Control in Stable Marriage and Stable Roommates: Complexity and   Algorithms</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study control problems in the context of matching under preferences: We examine how a central authority, called the controller, can manipulate an instance of the Stable Marriage or Stable Roommates problems in order to achieve certain goals. We investigate the computational complexity of the emerging problems, and provide both efficient algorithms and intractability results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01216</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01216</id><created>2025-02-03</created><authors><author><keyname>Liu</keyname><forenames>Tongkun</forenames></author><author><keyname>Li</keyname><forenames>Bing</forenames></author><author><keyname>Jin</keyname><forenames>Xiao</forenames></author><author><keyname>Shi</keyname><forenames>Yupeng</forenames></author><author><keyname>Li</keyname><forenames>Qiuying</forenames></author><author><keyname>Wei</keyname><forenames>Xiang</forenames></author></authors><title>Exploring Few-Shot Defect Segmentation in General Industrial Scenarios   with Metric Learning and Vision Foundation Models</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Industrial defect segmentation is critical for manufacturing quality control. Due to the scarcity of training defect samples, few-shot semantic segmentation (FSS) holds significant value in this field. However, existing studies mostly apply FSS to tackle defects on simple textures, without considering more diverse scenarios. This paper aims to address this gap by exploring FSS in broader industrial products with various defect types. To this end, we contribute a new real-world dataset and reorganize some existing datasets to build a more comprehensive few-shot defect segmentation (FDS) benchmark. On this benchmark, we thoroughly investigate metric learning-based FSS methods, including those based on meta-learning and those based on Vision Foundation Models (VFMs). We observe that existing meta-learning-based methods are generally not well-suited for this task, while VFMs hold great potential. We further systematically study the applicability of various VFMs in this task, involving two paradigms: feature matching and the use of Segment Anything (SAM) models. We propose a novel efficient FDS method based on feature matching. Meanwhile, we find that SAM2 is particularly effective for addressing FDS through its video track mode. The contributed dataset and code will be available at: https://github.com/liutongkun/GFDS. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01218</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01218</id><created>2025-02-03</created><authors><author><keyname>Zhang</keyname><forenames>Zhizhen</forenames></author><author><keyname>Zhu</keyname><forenames>Lei</forenames></author><author><keyname>Fang</keyname><forenames>Zhen</forenames></author><author><keyname>Huang</keyname><forenames>Zi</forenames></author><author><keyname>Luo</keyname><forenames>Yadan</forenames></author></authors><title>Provable Ordering and Continuity in Vision-Language Pretraining for   Generalizable Embodied Agents</title><categories>cs.RO cs.AI cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pre-training vision-language representations on human action videos has emerged as a promising approach to reduce reliance on large-scale expert demonstrations for training embodied agents. However, prior methods often employ time contrastive learning based on goal-reaching heuristics, progressively aligning language instructions from the initial to the final frame. This overemphasis on future frames can result in erroneous vision-language associations, as actions may terminate early or include irrelevant moments in the end. To address this issue, we propose Action Temporal Coherence Learning (AcTOL) to learn ordered and continuous vision-language representations without rigid goal-based constraint. AcTOL treats a video as a continuous trajectory where it (1) contrasts semantic differences between frames to reflect their natural ordering, and (2) imposes a local Brownian bridge constraint to ensure smooth transitions across intermediate frames. Extensive imitation learning experiments across varying numbers of demonstrations show that the pretrained features significantly enhance downstream manipulation tasks by up to 49% with high robustness to different linguistic styles of instructions, offering a viable pathway toward generalized embodied agents. The source code is included in the supplementary material for reference. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01220</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01220</id><created>2025-02-03</created><authors><author><keyname>Khodja</keyname><forenames>Hichem Ammar</forenames></author><author><keyname>Béchet</keyname><forenames>Frédéric</forenames></author><author><keyname>Brabant</keyname><forenames>Quentin</forenames></author><author><keyname>Nasr</keyname><forenames>Alexis</forenames></author><author><keyname>Lecorvé</keyname><forenames>Gwénolé</forenames></author></authors><title>On the Robustness of Temporal Factual Knowledge in Language Models</title><categories>cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper explores the temporal robustness of language models (LMs) in handling factual knowledge. While LMs can often complete simple factual statements, their ability to manage temporal facts (those valid only within specific timeframes) remains uncertain. We design a controlled experiment to test the robustness of temporal factual knowledge inside LMs, which we use to evaluate several pretrained and instruction-tuned models using prompts on popular Wikidata facts, assessing their performance across different temporal granularities (Day, Month, and Year). Our findings indicate that even very large state-of-the-art models, such as Llama-3.1-70B, vastly lack robust knowledge of temporal facts. In addition, they are incapable of generalizing their knowledge from one granularity to another. These results highlight the inherent limitations of using LMs as temporal knowledge bases. The source code and data to reproduce our experiments will be released. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01221</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01221</id><created>2025-02-03</created><authors><author><keyname>Lai</keyname><forenames>Anthony Cheuk Tung</forenames></author><author><keyname>Ke</keyname><forenames>Ping Fan</forenames></author><author><keyname>Ho</keyname><forenames>Alan</forenames></author></authors><title>Ransomware IR Model: Proactive Threat Intelligence-Based Incident   Response Strategy</title><categories>cs.CR</categories><comments>10 pages, 1 figure, 2 tables, case study</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ransomware impact different organizations for years, it causes huge monetary, reputation loss and operation impact. Other than typical data encryption by ransomware, attackers can request ransom from the victim organizations via data extortion, otherwise, attackers will publish stolen data publicly in their ransomware dashboard forum and data-sharing platforms. However, there is no clear and proven published incident response strategy to satisfy different business priorities and objectives under ransomware attack in detail. In this paper, we quote one of our representative front-line ransomware incident response experiences for Company X. Organization and incident responder can reference our established model strategy and implement proactive threat intelligence-based incident response architecture if one is under ransomware attack, which helps to respond the incident more effectively and speedy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01225</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01225</id><created>2025-02-03</created><authors><author><keyname>Xu</keyname><forenames>Zhiyuan</forenames></author><author><keyname>Gardiner</keyname><forenames>Joseph</forenames></author><author><keyname>Belguith</keyname><forenames>Sana</forenames></author></authors><title>The dark deep side of DeepSeek: Fine-tuning attacks against the safety   alignment of CoT-enabled models</title><categories>cs.CR cs.AI</categories><comments>12 Pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models are typically trained on vast amounts of data during the pre-training phase, which may include some potentially harmful information. Fine-tuning attacks can exploit this by prompting the model to reveal such behaviours, leading to the generation of harmful content. In this paper, we focus on investigating the performance of the Chain of Thought based reasoning model, DeepSeek, when subjected to fine-tuning attacks. Specifically, we explore how fine-tuning manipulates the model's output, exacerbating the harmfulness of its responses while examining the interaction between the Chain of Thought reasoning and adversarial inputs. Through this study, we aim to shed light on the vulnerability of Chain of Thought enabled models to fine-tuning attacks and the implications for their safety and ethical deployment. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01226</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01226</id><created>2025-02-03</created><authors><author><keyname>Sandberg</keyname><forenames>Jack</forenames></author><author><keyname>Chehreghani</keyname><forenames>Morteza Haghir</forenames></author></authors><title>Efficient Prior Selection in Gaussian Process Bandits with Thompson   Sampling</title><categories>cs.LG stat.ML</categories><comments>16 pages, 12 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Gaussian process (GP) bandits provide a powerful framework for solving blackbox optimization of unknown functions. The characteristics of the unknown function depends heavily on the assumed GP prior. Most work in the literature assume that this prior is known but in practice this seldom holds. Instead, practitioners often rely on maximum likelihood estimation to select the hyperparameters of the prior - which lacks theoretical guarantees. In this work, we propose two algorithms for joint prior selection and regret minimization in GP bandits based on GP Thompson sampling (GP-TS): Prior-Elimination GP-TS (PE-GP-TS) and HyperPrior GP-TS (HP-GP-TS). We theoretically analyze the algorithms and establish upper bounds for their respective regret. In addition, we demonstrate the effectiveness of our algorithms compared to the alternatives through experiments with synthetic and real-world data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01227</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01227</id><created>2025-02-03</created><authors><author><keyname>Gąsieniec</keyname><forenames>Leszek</forenames></author><author><keyname>Grodzicki</keyname><forenames>Tytus</forenames></author><author><keyname>Stachowiak</keyname><forenames>Grzegorz</forenames></author></authors><title>Near-State and State-Optimal Self-Stabilising Leader Election Population   Protocols</title><categories>cs.DC</categories><acm-class>F.2.2</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We investigate leader election problem via ranking within self-stabilising population protocols. In this scenario, the agent's state space comprises $n$ rank states and $x$ extra states. The initial configuration of $n$ agents consists of arbitrary arrangements of rank and extra states, with the objective of self-ranking. Specifically, each agent is tasked with stabilising in a unique rank state silently, implying that after stabilisation, each agent remains in its designated state indefinitely.   In this paper, we present several new self-stabilising ranking protocols, greatly enriching our comprehension of these intricate problems. All protocols ensure self-stabilisation time with high probability (whp), defined as $1-n^{-\eta},$ for a constant $\eta&gt;0.$ We delve into three scenarios, from which we derive stable (always correct), either state-optimal or almost state-optimal, silent ranking protocols that self-stabilise within a time frame of $o(n^2)$ whp, including:   - Utilising a novel concept of an agent trap, we derive a state-optimal ranking protocol that achieves self-stabilisation in time $O(min(kn^{3/2},n^2\log^2 n)),$ for any $k$-distant starting configuration.   - Furthermore, we show that the incorporation of a single extra state ($x=1$) ensures a ranking protocol that self-stabilises in time $O(n^{7/4}\log^2 n)=o(n^2)$, regardless of the initial configuration.   - Lastly, we show that extra $x=O(\log n)$ states admit self-stabilising ranking with the best currently known stabilisation time $O(n\log n)$, when whp and $x=O(\log n)$ guarantees are imposed. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01228</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01228</id><created>2025-02-03</created><authors><author><keyname>Caroleo</keyname><forenames>Giammarco</forenames></author><author><keyname>Albini</keyname><forenames>Alessandro</forenames></author><author><keyname>Maiolino</keyname><forenames>Perla</forenames></author></authors><title>Soft Robot Localization Using Distributed Miniaturized Time-of-Flight   Sensors</title><categories>cs.RO</categories><comments>The manuscript has been accepted as a contributed paper for the 8th   IEEE-RAS International Conference on Soft Robotics (RoboSoft 2025)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thanks to their compliance and adaptability, soft robots can be deployed to perform tasks in constrained or complex environments. In these scenarios, spatial awareness of the surroundings and the ability to localize the robot within the environment represent key aspects. While state-of-the-art localization techniques are well-explored in autonomous vehicles and walking robots, they rely on data retrieved with lidar or depth sensors which are bulky and thus difficult to integrate into small soft robots. Recent developments in miniaturized Time of Flight (ToF) sensors show promise as a small and lightweight alternative to bulky sensors. These sensors can be potentially distributed on the soft robot body, providing multi-point depth data of the surroundings. However, the small spatial resolution and the noisy measurements pose a challenge to the success of state-of-the-art localization algorithms, which are generally applied to much denser and more reliable measurements. In this paper, we enforce distributed VL53L5CX ToF sensors, mount them on the tip of a soft robot, and investigate their usage for self-localization tasks. Experimental results show that the soft robot can effectively be localized with respect to a known map, with an error comparable to the uncertainty on the measures provided by the miniaturized ToF sensors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01229</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01229</id><created>2025-02-03</created><authors><author><keyname>Heinrich</keyname><forenames>Roman</forenames></author><author><keyname>Luthra</keyname><forenames>Manisha</forenames></author><author><keyname>Wehrstein</keyname><forenames>Johannes</forenames></author><author><keyname>Kornmayer</keyname><forenames>Harald</forenames></author><author><keyname>Binnig</keyname><forenames>Carsten</forenames></author></authors><title>How Good are Learned Cost Models, Really? Insights from Query   Optimization Tasks</title><categories>cs.DB</categories><comments>The paper has been accepted by SIGMOD 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally, query optimizers rely on cost models to choose the best execution plan from several candidates, making precise cost estimates critical for efficient query execution. In recent years, cost models based on machine learning have been proposed to overcome the weaknesses of traditional cost models. While these models have been shown to provide better prediction accuracy, only limited efforts have been made to investigate how well Learned Cost Models (LCMs) actually perform in query optimization and how they affect overall query performance. In this paper, we address this by a systematic study evaluating LCMs on three of the core query optimization tasks: join ordering, access path selection, and physical operator selection. In our study, we compare seven state-of-the-art LCMs to a traditional cost model and, surprisingly, find that the traditional model often still outperforms LCMs in these tasks. We conclude by highlighting major takeaways and recommendations to guide future research toward making LCMs more effective for query optimization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01231</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01231</id><created>2025-02-03</created><authors><author><keyname>Yoganathan</keyname><forenames>V.</forenames></author><author><keyname>Osburg</keyname><forenames>V. -S.</forenames></author><author><keyname>Colladon</keyname><forenames>A. Fronzetti</forenames></author><author><keyname>Charles</keyname><forenames>V.</forenames></author><author><keyname>Toporowski</keyname><forenames>W.</forenames></author></authors><title>Societal Attitudes Toward Service Robots: Adore, Abhor, Ignore, or   Unsure?</title><categories>cs.RO</categories><comments>in press</comments><acm-class>I.2.9</acm-class><journal-ref>Journal of Service Research (2024)</journal-ref><doi>10.1177/10946705241295841</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Societal or population-level attitudes are aggregated patterns of different individual attitudes, representing collective general predispositions. As service robots become ubiquitous, understanding attitudes towards them at the population (vs. individual) level enables firms to expand robot services to a broad (vs. niche) market. Targeting population-level attitudes would benefit service firms because: (1) they are more persistent, thus, stronger predictors of behavioral patterns and (2) this approach is less reliant on personal data, whereas individualized services are vulnerable to AI-related privacy risks. As for service theory, ignoring broad unobserved differences in attitudes produces biased conclusions, and our systematic review of previous research highlights a poor understanding of potential heterogeneity in attitudes toward service robots. We present five diverse studies (S1-S5), utilizing multinational and "real world" data (Ntotal = 89,541; years: 2012-2024). Results reveal a stable structure comprising four distinct attitude profiles (S1-S5): positive ("adore"), negative ("abhor"), indifferent ("ignore"), and ambivalent ("unsure"). The psychological need for interacting with service staff, and for autonomy and relatedness in technology use, function as attitude profile antecedents (S2). Importantly, the attitude profiles predict differences in post-interaction discomfort and anxiety (S3), satisfaction ratings and service evaluations (S4), and perceived sociability and uncanniness based on a robot's humanlikeness (S5). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01232</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01232</id><created>2025-02-03</created><authors><author><keyname>Cropper</keyname><forenames>Andrew</forenames></author><author><keyname>Cerna</keyname><forenames>David M.</forenames></author></authors><title>Efficient rule induction by ignoring pointless rules</title><categories>cs.AI</categories><comments>Under review for a conference</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The goal of inductive logic programming (ILP) is to find a set of logical rules that generalises training examples and background knowledge. We introduce an ILP approach that identifies pointless rules. A rule is pointless if it contains a redundant literal or cannot discriminate against negative examples. We show that ignoring pointless rules allows an ILP system to soundly prune the hypothesis space. Our experiments on multiple domains, including visual reasoning and game playing, show that our approach can reduce learning times by 99% whilst maintaining predictive accuracies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01235</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01235</id><created>2025-02-03</created><authors><author><keyname>Zhang</keyname><forenames>Yuanhe</forenames></author><author><keyname>Liu</keyname><forenames>Fanghui</forenames></author><author><keyname>Chen</keyname><forenames>Yudong</forenames></author></authors><title>One-step full gradient suffices for low-rank fine-tuning, provably and   efficiently</title><categories>stat.ML cs.AI cs.LG</categories><comments>86 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper studies how to improve the performance of Low-Rank Adaption (LoRA) as guided by our theoretical analysis. Our first set of theoretical results show that for random initialization and linear models, \textit{i)} LoRA will align to the certain singular subspace of one-step gradient of full fine-tuning; \textit{ii)} preconditioners improve convergence in the high-rank case. These insights motivate us to focus on preconditioned LoRA using a specific spectral initialization strategy for aligning with certain subspaces. For both linear and nonlinear models, we prove that alignment and generalization guarantees can be directly achieved at initialization, and the subsequent linear convergence can be also built. Our analysis leads to the \emph{LoRA-One} algorithm (using \emph{One}-step gradient and preconditioning), a theoretically grounded algorithm that achieves significant empirical improvement over vanilla LoRA and its variants on several benchmarks. Our theoretical analysis, based on decoupling the learning dynamics and characterizing how spectral initialization contributes to feature learning, may be of independent interest for understanding matrix sensing and deep learning theory. The source code can be found in the https://github.com/YuanheZ/LoRA-One. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01236</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01236</id><created>2025-02-03</created><authors><author><keyname>Li</keyname><forenames>Xiang Lisa</forenames></author><author><keyname>Chowdhury</keyname><forenames>Neil</forenames></author><author><keyname>Johnson</keyname><forenames>Daniel D.</forenames></author><author><keyname>Hashimoto</keyname><forenames>Tatsunori</forenames></author><author><keyname>Liang</keyname><forenames>Percy</forenames></author><author><keyname>Schwettmann</keyname><forenames>Sarah</forenames></author><author><keyname>Steinhardt</keyname><forenames>Jacob</forenames></author></authors><title>Eliciting Language Model Behaviors with Investigator Agents</title><categories>cs.LG cs.AI cs.CL</categories><comments>20 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Language models exhibit complex, diverse behaviors when prompted with free-form text, making it difficult to characterize the space of possible outputs. We study the problem of behavior elicitation, where the goal is to search for prompts that induce specific target behaviors (e.g., hallucinations or harmful responses) from a target language model. To navigate the exponentially large space of possible prompts, we train investigator models to map randomly-chosen target behaviors to a diverse distribution of outputs that elicit them, similar to amortized Bayesian inference. We do this through supervised fine-tuning, reinforcement learning via DPO, and a novel Frank-Wolfe training objective to iteratively discover diverse prompting strategies. Our investigator models surface a variety of effective and human-interpretable prompts leading to jailbreaks, hallucinations, and open-ended aberrant behaviors, obtaining a 100% attack success rate on a subset of AdvBench (Harmful Behaviors) and an 85% hallucination rate. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01237</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01237</id><created>2025-02-03</created><authors><author><keyname>Gorbatovski</keyname><forenames>Alexey</forenames></author><author><keyname>Shaposhnikov</keyname><forenames>Boris</forenames></author><author><keyname>Sinii</keyname><forenames>Viacheslav</forenames></author><author><keyname>Malakhov</keyname><forenames>Alexey</forenames></author><author><keyname>Gavrilov</keyname><forenames>Daniil</forenames></author></authors><title>The Differences Between Direct Alignment Algorithms are a Blur</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Direct Alignment Algorithms (DAAs) simplify language model alignment by replacing reinforcement learning (RL) and reward modeling (RM) in Reinforcement Learning from Human Feedback (RLHF) with direct policy optimization. DAAs can be classified by their ranking losses (pairwise vs. pointwise), by the rewards used in those losses (e.g., likelihood ratios of policy and reference policy, or odds ratios), or by whether a Supervised Fine-Tuning (SFT) phase is required (two-stage vs. one-stage). We first show that one-stage methods underperform two-stage methods. To address this, we incorporate an explicit SFT phase and introduce the $\beta$ parameter, controlling the strength of preference optimization, into single-stage ORPO and ASFT. These modifications improve their performance in Alpaca Eval 2 by +$3.46$ (ORPO) and +$8.27$ (ASFT), matching two-stage methods like DPO. Further analysis reveals that the key factor is whether the approach uses pairwise or pointwise objectives, rather than the specific implicit reward or loss function. These results highlight the importance of careful evaluation to avoid premature claims of performance gains or overall superiority in alignment algorithms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01240</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01240</id><created>2025-02-03</created><authors><author><keyname>Reimann</keyname><forenames>Lennart M.</forenames></author><author><keyname>Rezunov</keyname><forenames>Evgenii</forenames></author><author><keyname>Germek</keyname><forenames>Dominik</forenames></author><author><keyname>Collini</keyname><forenames>Luca</forenames></author><author><keyname>Pilato</keyname><forenames>Christian</forenames></author><author><keyname>Karri</keyname><forenames>Ramesh</forenames></author><author><keyname>Leupers</keyname><forenames>Rainer</forenames></author></authors><title>The Impact of Logic Locking on Confidentiality: An Automated Evaluation</title><categories>cs.CR cs.AR</categories><comments>8 pages, accepted at 26th International Symposium on Quality   Electronic Design (ISQED'25)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Logic locking secures hardware designs in untrusted foundries by incorporating key-driven gates to obscure the original blueprint. While this method safeguards the integrated circuit from malicious alterations during fabrication, its influence on data confidentiality during runtime has been ignored. In this study, we employ path sensitization to formally examine the impact of logic locking on confidentiality. By applying three representative logic locking mechanisms on open-source cryptographic benchmarks, we utilize an automatic test pattern generation framework to evaluate the effect of locking on cryptographic encryption keys and sensitive data signals. Our analysis reveals that logic locking can inadvertently cause sensitive data leakage when incorrect logic locking keys are used. We show that a single malicious logic locking key can expose over 70% of an encryption key. If an adversary gains control over other inputs, the entire encryption key can be compromised. This research uncovers a significant security vulnerability in logic locking and emphasizes the need for comprehensive security assessments that extend beyond key-recovery attacks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01241</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01241</id><created>2025-02-03</created><authors><author><keyname>Yang</keyname><forenames>Ziqing</forenames></author><author><keyname>Wu</keyname><forenames>Yixin</forenames></author><author><keyname>Wen</keyname><forenames>Rui</forenames></author><author><keyname>Backes</keyname><forenames>Michael</forenames></author><author><keyname>Zhang</keyname><forenames>Yang</forenames></author></authors><title>Peering Behind the Shield: Guardrail Identification in Large Language   Models</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Human-AI conversations have gained increasing attention since the era of large language models. Consequently, more techniques, such as input/output guardrails and safety alignment, are proposed to prevent potential misuse of such Human-AI conversations. However, the ability to identify these guardrails has significant implications, both for adversarial exploitation and for auditing purposes by red team operators. In this work, we propose a novel method, AP-Test, which identifies the presence of a candidate guardrail by leveraging guardrail-specific adversarial prompts to query the AI agent. Extensive experiments of four candidate guardrails under diverse scenarios showcase the effectiveness of our method. The ablation study further illustrates the importance of the components we designed, such as the loss terms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01242</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01242</id><created>2025-02-03</created><authors><author><keyname>Dacre</keyname><forenames>Bailey</forenames></author><author><keyname>Bessone</keyname><forenames>Nicolas</forenames></author><author><keyname>Preti</keyname><forenames>Matteo Lo</forenames></author><author><keyname>Cafiso</keyname><forenames>Diana</forenames></author><author><keyname>Moreno</keyname><forenames>Rodrigo</forenames></author><author><keyname>Faíña</keyname><forenames>Andrés</forenames></author><author><keyname>Beccai</keyname><forenames>Lucia</forenames></author></authors><title>Neural Cellular Automata for Decentralized Sensing using a Soft   Inductive Sensor Array for Distributed Manipulator Systems</title><categories>cs.RO cs.LG</categories><comments>8 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Distributed Manipulator Systems (DMS), decentralization is a highly desirable property as it promotes robustness and facilitates scalability by distributing computational burden and eliminating singular points of failure. However, current DMS typically utilize a centralized approach to sensing, such as single-camera computer vision systems. This centralization poses a risk to system reliability and offers a significant limiting factor to system size. In this work, we introduce a decentralized approach for sensing and in a Distributed Manipulator Systems using Neural Cellular Automata (NCA). Demonstrating a decentralized sensing in a hardware implementation, we present a novel inductive sensor board designed for distributed sensing and evaluate its ability to estimate global object properties, such as the geometric center, through local interactions and computations. Experiments demonstrate that NCA-based sensing networks accurately estimate object position at 0.24 times the inter sensor distance. They maintain resilience under sensor faults and noise, and scale seamlessly across varying network sizes. These findings underscore the potential of local, decentralized computations to enable scalable, fault-tolerant, and noise-resilient object property estimation in DMS </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01243</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01243</id><created>2025-02-03</created><authors><author><keyname>Zhou</keyname><forenames>Chengfeng</forenames></author><author><keyname>Wang</keyname><forenames>Ji</forenames></author><author><keyname>Qin</keyname><forenames>Juanjuan</forenames></author><author><keyname>Wang</keyname><forenames>Yining</forenames></author><author><keyname>Sun</keyname><forenames>Ling</forenames></author><author><keyname>Dai</keyname><forenames>Weiwei</forenames></author></authors><title>OphthBench: A Comprehensive Benchmark for Evaluating Large Language   Models in Chinese Ophthalmology</title><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models (LLMs) have shown significant promise across various medical applications, with ophthalmology being a notable area of focus. Many ophthalmic tasks have shown substantial improvement through the integration of LLMs. However, before these models can be widely adopted in clinical practice, evaluating their capabilities and identifying their limitations is crucial. To address this research gap and support the real-world application of LLMs, we introduce the OphthBench, a specialized benchmark designed to assess LLM performance within the context of Chinese ophthalmic practices. This benchmark systematically divides a typical ophthalmic clinical workflow into five key scenarios: Education, Triage, Diagnosis, Treatment, and Prognosis. For each scenario, we developed multiple tasks featuring diverse question types, resulting in a comprehensive benchmark comprising 9 tasks and 591 questions. This comprehensive framework allows for a thorough assessment of LLMs' capabilities and provides insights into their practical application in Chinese ophthalmology. Using this benchmark, we conducted extensive experiments and analyzed the results from 39 popular LLMs. Our evaluation highlights the current gap between LLM development and its practical utility in clinical settings, providing a clear direction for future advancements. By bridging this gap, we aim to unlock the potential of LLMs and advance their development in ophthalmology. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01247</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01247</id><created>2025-02-03</created><authors><author><keyname>Khalfaoui-Hassani</keyname><forenames>Ismail</forenames></author><author><keyname>Kesselheim</keyname><forenames>Stefan</forenames></author></authors><title>Learnable polynomial, trigonometric, and tropical activations</title><categories>cs.LG cs.AI cs.CL cs.CV math.AG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper investigates scalable neural networks with learnable activation functions based on orthogonal function bases and tropical polynomials, targeting ImageNet-1K classification and next token prediction on OpenWebText. Traditional activations, such as ReLU, are static. In contrast, learnable activations enable the network to adapt dynamically during training. However, stability issues, such as vanishing or exploding gradients, arise with improper variance management in deeper networks. To remedy this, we propose an initialization scheme that single-handedly preserves unitary variance in transformers and convolutional networks, ensuring stable gradient flow even in deep architectures. Extensive experiments demonstrate that networks with Hermite, Fourier, and Tropical-based learnable activations significantly improve over GPT-2 and ConvNeXt networks in terms of accuracy and perplexity in train and test, highlighting the viability of learnable activations in large-scale tasks. The activation functions developed here are the subject of a library coded entirely in pure PyTorch: torchortho, available at https://github.com/K-H-Ismail/torchortho. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01248</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01248</id><created>2025-02-03</created><authors><author><keyname>Wirthl</keyname><forenames>Barbara</forenames></author><author><keyname>Decuzzi</keyname><forenames>Paolo</forenames></author><author><keyname>Schrefler</keyname><forenames>Bernhard A.</forenames></author><author><keyname>Wall</keyname><forenames>Wolfgang A.</forenames></author></authors><title>Computational modelling of cancer nanomedicine: Integrating hyperthermia   treatment into a multiphase porous-media tumour model</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heat-based cancer treatment, so-called hyperthermia, can be used to destroy tumour cells directly or to make them more susceptible to chemotherapy or radiation therapy. To apply heat locally, iron oxide nanoparticles are injected into the bloodstream and accumulate at the tumour site, where they generate heat when exposed to an alternating magnetic field. However, the temperature must be precisely controlled to achieve therapeutic benefits while avoiding damage to healthy tissue. We therefore present a computational model for nanoparticle-mediated hyperthermia treatment fully integrated into a multiphase porous-media model of the tumour and its microenvironment. We study how the temperature depends on the amount of nanoparticles accumulated in the tumour area and the specific absorption rate of the nanoparticles. Our results show that host tissue surrounding the tumour is also exposed to considerable doses of heat due to the high thermal conductivity of the tissue, which may cause pain or even unnecessary irreversible damage. Further, we include a lumped and a discrete model for the cooling effect of blood perfusion. Using a discrete model of a realistic microvasculature reveals that the small capillaries do not have a significant cooling effect during hyperthermia treatment and that the commonly used lumped model based on Pennes' bioheat equation overestimates the effect: within the specific conditions analysed, the difference between lumped and discrete approaches is approximatively 0.75{\deg}C, which could influence the therapeutic intervention outcome. Such a comprehensive computational model, as presented here, can provide insights into the optimal treatment parameters for nanoparticle-mediated hyperthermia and can be used to design more efficient treatment strategies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01250</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01250</id><created>2025-02-03</created><authors><author><keyname>Zhou</keyname><forenames>Haokun</forenames></author></authors><title>Beyond Win Rates: A Clustering-Based Approach to Character Balance   Analysis in Team-Based Games</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Character diversity in competitive games, while enriching gameplay, often introduces balance challenges that can negatively impact player experience and strategic depth. Traditional balance assessments rely on aggregate metrics like win rates and pick rates, which offer limited insight into the intricate dynamics of team-based games and nuanced character roles. This paper proposes a novel clustering-based methodology to analyze character balance, leveraging in-game data from Valorant to account for team composition influences and reveal latent character roles. By applying hierarchical agglomerative clustering with Jensen-Shannon Divergence to professional match data from the Valorant Champions Tour 2022, our approach identifies distinct clusters of agents exhibiting similar co-occurrence patterns within team compositions. This method not only complements existing quantitative metrics but also provides a more holistic and interpretable perspective on character synergies and potential imbalances, offering game developers a valuable tool for informed and context-aware balance adjustments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01252</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01252</id><created>2025-02-03</created><authors><author><keyname>Li</keyname><forenames>Yuheng</forenames></author><author><keyname>Wang</keyname><forenames>Panpan</forenames></author><author><keyname>Chen</keyname><forenames>Haipeng</forenames></author></authors><title>Can Reinforcement Learning Solve Asymmetric Combinatorial-Continuous   Zero-Sum Games?</title><categories>cs.GT</categories><comments>28 pages, 2 figures, 10 tables</comments><journal-ref>Published in ICLR 2025</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There have been extensive studies on learning in zero-sum games, focusing on the analysis of the existence and algorithmic convergence of Nash equilibrium (NE). Existing studies mainly focus on symmetric games where the strategy spaces of the players are of the same type and size. For the few studies that do consider asymmetric games, they are mostly restricted to matrix games. In this paper, we define and study a new practical class of asymmetric games called two-player Asymmetric Combinatorial-Continuous zEro-Sum (ACCES) games, featuring a combinatorial action space for one player and an infinite compact space for the other. Such ACCES games have broad implications in the real world, particularly in combinatorial optimization problems (COPs) where one player optimizes a solution in a combinatorial space, and the opponent plays against it in an infinite (continuous) compact space (e.g., a nature player deciding epistemic parameters of the environmental model). Our first key contribution is to prove the existence of NE for two-player ACCES games, using the idea of essentially finite game approximation. Building on the theoretical insights and double oracle (DO)-based solutions to complex zero-sum games, our second contribution is to design the novel algorithm, Combinatorial Continuous DO (CCDO), to solve ACCES games, and prove the convergence of the proposed algorithm. Considering the NP-hardness of most COPs and recent advancements in reinforcement learning (RL)-based solutions to COPs, our third contribution is to propose a practical algorithm to solve NE in the real world, CCDORL (based on CCDO), and provide the novel convergence analysis in the ACCES game. Experimental results across diverse instances of COPs demonstrate the empirical effectiveness of our algorithms. The code of this work is available at https://github.com/wmd3i/CCDO-RL. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01253</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01253</id><created>2025-02-03</created><authors><author><keyname>Seneviratne</keyname><forenames>Oshani</forenames></author><author><keyname>Capuzzo</keyname><forenames>Brendan</forenames></author><author><keyname>Van Woensel</keyname><forenames>William</forenames></author></authors><title>Explainability-Driven Quality Assessment for Rule-Based Systems</title><categories>cs.AI cs.LO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper introduces an explanation framework designed to enhance the quality of rules in knowledge-based reasoning systems based on dataset-driven insights. The traditional method for rule induction from data typically requires labor-intensive labeling and data-driven learning. This framework provides an alternative and instead allows for the data-driven refinement of existing rules: it generates explanations of rule inferences and leverages human interpretation to refine rules. It leverages four complementary explanation types: trace-based, contextual, contrastive, and counterfactual, providing diverse perspectives for debugging, validating, and ultimately refining rules. By embedding explainability into the reasoning architecture, the framework enables knowledge engineers to address inconsistencies, optimize thresholds, and ensure fairness, transparency, and interpretability in decision-making processes. Its practicality is demonstrated through a use case in finance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01256</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01256</id><created>2025-02-03</created><authors><author><keyname>S</keyname><forenames>Rajashekhar V</forenames></author><author><keyname>Prabhakar</keyname><forenames>Gowdham</forenames></author></authors><title>Soft is Safe: Human-Robot Interaction for Soft Robots</title><categories>cs.RO</categories><comments>53 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the presence of robots increasing in the society, the need for interacting with robots is becoming necessary. The field of Human-Robot Interaction (HRI) has emerged important since more repetitive and tiresome jobs are being done by robots. In the recent times, the field of soft robotics has seen a boom in the field of research and commercialization. The Industry 5.0 focuses on human robot collaboration which also spurs the field of soft robotics. However the HRI for soft robotics is still in the nascent stage. In this work we review and then discuss how HRI is done for soft robots. We first discuss the control, design, materials and manufacturing of soft robots. This will provide an understanding of what is being interacted with. Then we discuss about the various input and output modalities that are used in HRI. The applications where the HRI for soft robots are found in the literature are discussed in detail. Then the limitations of HRI for soft robots and various research opportunities that exist in this field are discussed in detail. It is concluded that there is a huge scope for development for HRI for soft robots. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01262</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01262</id><created>2025-02-03</created><authors><author><keyname>Park</keyname><forenames>Eun-Sol</forenames></author><author><keyname>Park</keyname><forenames>MiSo</forenames></author><author><keyname>Park</keyname><forenames>Seung</forenames></author><author><keyname>Shin</keyname><forenames>Yong-Goo</forenames></author></authors><title>FSPGD: Rethinking Black-box Attacks on Semantic Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transferability, the ability of adversarial examples crafted for one model to deceive other models, is crucial for black-box attacks. Despite advancements in attack methods for semantic segmentation, transferability remains limited, reducing their effectiveness in real-world applications. To address this, we introduce the Feature Similarity Projected Gradient Descent (FSPGD) attack, a novel black-box approach that enhances both attack performance and transferability. Unlike conventional segmentation attacks that rely on output predictions for gradient calculation, FSPGD computes gradients from intermediate layer features. Specifically, our method introduces a loss function that targets local information by comparing features between clean images and adversarial examples, while also disrupting contextual information by accounting for spatial relationships between objects. Experiments on Pascal VOC 2012 and Cityscapes datasets demonstrate that FSPGD achieves superior transferability and attack performance, establishing a new state-of-the-art benchmark. Code is available at https://github.com/KU-AIVS/FSPGD. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01264</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01264</id><created>2025-02-03</created><authors><author><keyname>Wang</keyname><forenames>Jia-Qi</forenames></author><author><keyname>He</keyname><forenames>Rong-Qiang</forenames></author><author><keyname>Lu</keyname><forenames>Zhong-Yi</forenames></author></authors><title>Generalized Lanczos method for systematic optimization of neural-network   quantum states</title><categories>cond-mat.str-el cs.LG physics.comp-ph</categories><comments>11 pages, 7 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, artificial intelligence for science has made significant inroads into various fields of natural science research. In the field of quantum many-body computation, researchers have developed numerous ground state solvers based on neural-network quantum states (NQSs), achieving ground state energies with accuracy comparable to or surpassing traditional methods such as variational Monte Carlo methods, density matrix renormalization group, and quantum Monte Carlo methods. Here, we combine supervised learning, reinforcement learning, and the Lanczos method to develop a systematic approach to improving the NQSs of many-body systems, which we refer to as the NQS Lanczos method. The algorithm mainly consists of two parts: the supervised learning part and the reinforcement learning part. Through supervised learning, the Lanczos states are represented by the NQSs. Through reinforcement learning, the NQSs are further optimized. We analyze the reasons for the underfitting problem and demonstrate how the NQS Lanczos method systematically improves the energy in the highly frustrated regime of the two-dimensional Heisenberg $J_1$-$J_2$ model. Compared to the existing method that combines the Lanczos method with the restricted Boltzmann machine, the primary advantage of the NQS Lanczos method is its linearly increasing computational cost. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01265</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01265</id><created>2025-02-03</created><authors><author><keyname>Bshouty</keyname><forenames>Nader H.</forenames></author></authors><title>On Exact Learning of $d$-Monotone Functions</title><categories>cs.LG cs.DS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we study the learnability of the Boolean class of $d$-monotone functions $f:{\cal X}\to\{0,1\}$ from membership and equivalence queries, where $({\cal X},\le)$ is a finite lattice. We show that the class of $d$-monotone functions that are represented in the form $f=F(g_1,g_2,\ldots,g_d)$, where $F$ is any Boolean function $F:\{0,1\}^d\to\{0,1\}$ and $g_1,\ldots,g_d:{\cal X}\to \{0,1\}$ are any monotone functions, is learnable in time $\sigma({\cal X})\cdot (size(f)/d+1)^{d}$ where $\sigma({\cal X})$ is the maximum sum of the number of immediate predecessors in a chain from the largest element to the smallest element in the lattice ${\cal X}$ and $size(f)=size(g_1)+\cdots+size(g_d)$, where $size(g_i)$ is the number of minimal elements in $g_i^{-1}(1)$.   For the Boolean function $f:\{0,1\}^n\to\{0,1\}$, the class of $d$-monotone functions that are represented in the form $f=F(g_1,g_2,\ldots,g_d)$, where $F$ is any Boolean function and $g_1,\ldots,g_d$ are any monotone DNF, is learnable in time $O(n^2)\cdot (size(f)/d+1)^{d}$ where $size(f)=size(g_1)+\cdots+size(g_d)$.   In particular, this class is learnable in polynomial time when $d$ is constant. Additionally, this class is learnable in polynomial time when $size(g_i)$ is constant for all $i$ and $d=O(\log n)$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01267</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01267</id><created>2025-02-03</created><authors><author><keyname>Alvarez</keyname><forenames>Jose M.</forenames></author><author><keyname>Ruggieri</keyname><forenames>Salvatore</forenames></author></authors><title>Counterfactual Situation Testing: From Single to Multidimensional   Discrimination</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present counterfactual situation testing (CST), a causal data mining framework for detecting individual discrimination in a dataset of classifier decisions. CST answers the question "what would have been the model outcome had the individual, or complainant, been of a different protected status?" It extends the legally-grounded situation testing (ST) of Thanh et al. (2011) by operationalizing the notion of fairness given the difference via counterfactual reasoning. ST finds for each complainant similar protected and non-protected instances in the dataset; constructs, respectively, a control and test group; and compares the groups such that a difference in outcomes implies a potential case of individual discrimination. CST, instead, avoids this idealized comparison by establishing the test group on the complainant's generated counterfactual, which reflects how the protected attribute when changed influences other seemingly neutral attributes of the complainant. Under CST we test for discrimination for each complainant by comparing similar individuals within each group but dissimilar individuals across groups. We consider single (e.g., gender) and multidimensional (e.g., gender and race) discrimination testing. For multidimensional discrimination we study multiple and intersectional discrimination and, as feared by legal scholars, find evidence that the former fails to account for the latter kind. Using a k-nearest neighbor implementation, we showcase CST on synthetic and real data. Experimental results show that CST uncovers a higher number of cases than ST, even when the model is counterfactually fair. In fact, CST extends counterfactual fairness (CF) of Kusner et al. (2017) by equipping CF with confidence intervals. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01268</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01268</id><created>2025-02-03</created><authors><author><keyname>Eldeeb</keyname><forenames>Eslam</forenames></author><author><keyname>Alves</keyname><forenames>Hirley</forenames></author></authors><title>Resilient UAV Trajectory Planning via Few-Shot Meta-Offline   Reinforcement Learning</title><categories>cs.RO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reinforcement learning (RL) has been a promising essence in future 5G-beyond and 6G systems. Its main advantage lies in its robust model-free decision-making in complex and large-dimension wireless environments. However, most existing RL frameworks rely on online interaction with the environment, which might not be feasible due to safety and cost concerns. Another problem with online RL is the lack of scalability of the designed algorithm with dynamic or new environments. This work proposes a novel, resilient, few-shot meta-offline RL algorithm combining offline RL using conservative Q-learning (CQL) and meta-learning using model-agnostic meta-learning (MAML). The proposed algorithm can train RL models using static offline datasets without any online interaction with the environments. In addition, with the aid of MAML, the proposed model can be scaled up to new unseen environments. We showcase the proposed algorithm for optimizing an unmanned aerial vehicle (UAV) 's trajectory and scheduling policy to minimize the age-of-information (AoI) and transmission power of limited-power devices. Numerical results show that the proposed few-shot meta-offline RL algorithm converges faster than baseline schemes, such as deep Q-networks and CQL. In addition, it is the only algorithm that can achieve optimal joint AoI and transmission power using an offline dataset with few shots of data points and is resilient to network failures due to unprecedented environmental changes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01269</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01269</id><created>2025-02-03</created><authors><author><keyname>Ziyi</keyname><forenames>Chen</forenames></author><author><keyname>Jia-wen</keyname><forenames>Gu</forenames></author></authors><title>Exploratory Utility Maximization Problem with Tsallis Entropy</title><categories>cs.LG q-fin.MF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study expected utility maximization problem with constant relative risk aversion utility function in a complete market under the reinforcement learning framework. To induce exploration, we introduce the Tsallis entropy regularizer, which generalizes the commonly used Shannon entropy. Unlike the classical Merton's problem, which is always well-posed and admits closed-form solutions, we find that the utility maximization exploratory problem is ill-posed in certain cases, due to over-exploration. With a carefully selected primary temperature function, we investigate two specific examples, for which we fully characterize their well-posedness and provide semi-closed-form solutions. It is interesting to find that one example has the well-known Gaussian distribution as the optimal strategy, while the other features the rare Wigner semicircle distribution, which is equivalent to a scaled Beta distribution. The means of the two optimal exploratory policies coincide with that of the classical counterpart. In addition, we examine the convergence of the value function and optimal exploratory strategy as the exploration vanishes. Finally, we design a reinforcement learning algorithm and conduct numerical experiments to demonstrate the advantages of reinforcement learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01270</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01270</id><created>2025-02-03</created><authors><author><keyname>Pimparkhede</keyname><forenames>Sameer</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Pushpak</forenames></author></authors><title>Main Predicate and Their Arguments as Explanation Signals For Intent   Classification</title><categories>cs.CL</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Intent classification is crucial for conversational agents (chatbots), and deep learning models perform well in this area. However, little research has been done on the explainability of intent classification due to the absence of suitable benchmark data. Human annotation of explanation signals in text samples is time-consuming and costly. However, from inspection of data on intent classification, we see that, more often than not, the main verb denotes the action, and the direct object indicates the domain of conversation, serving as explanation signals for intent. This observation enables us to hypothesize that the main predicate in the text utterances, along with the arguments of the main predicate, can serve as explanation signals. Leveraging this, we introduce a new technique to automatically augment text samples from intent classification datasets with word-level explanations. We mark main predicates (primarily verbs) and their arguments (dependency relations) as explanation signals in benchmark intent classification datasets ATIS and SNIPS, creating a unique 21k-instance dataset for explainability. Further, we experiment with deep learning and language models. We observe that models that work well for classification do not perform well in explainability metrics like plausibility and faithfulness. We also observe that guiding models to focus on explanation signals from our dataset during training improves the plausibility Token F1 score by 3-4%, improving the model's reasoning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01272</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01272</id><created>2025-02-03</created><authors><author><keyname>Liu</keyname><forenames>Chang</forenames></author><author><keyname>Huang</keyname><forenames>Hai</forenames></author><author><keyname>Xing</keyname><forenames>Yujie</forenames></author><author><keyname>Zuo</keyname><forenames>Xingquan</forenames></author></authors><title>Boosting Graph Robustness Against Backdoor Attacks: An Over-Similarity   Perspective</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph Neural Networks (GNNs) have achieved notable success in tasks such as social and transportation networks. However, recent studies have highlighted the vulnerability of GNNs to backdoor attacks, raising significant concerns about their reliability in real-world applications. Despite initial efforts to defend against specific graph backdoor attacks, existing defense methods face two main challenges: either the inability to establish a clear distinction between triggers and clean nodes, resulting in the removal of many clean nodes, or the failure to eliminate the impact of triggers, making it challenging to restore the target nodes to their pre-attack state. Through empirical analysis of various existing graph backdoor attacks, we observe that the triggers generated by these methods exhibit over-similarity in both features and structure. Based on this observation, we propose a novel graph backdoor defense method SimGuard. We first utilizes a similarity-based metric to detect triggers and then employs contrastive learning to train a backdoor detector that generates embeddings capable of separating triggers from clean nodes, thereby improving detection efficiency. Extensive experiments conducted on real-world datasets demonstrate that our proposed method effectively defends against various graph backdoor attacks while preserving performance on clean nodes. The code will be released upon acceptance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01273</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01273</id><created>2025-02-03</created><authors><author><keyname>Naman</keyname><forenames>Agrawal</forenames></author><author><keyname>Shariffdeen</keyname><forenames>Ridwan</forenames></author><author><keyname>Wang</keyname><forenames>Guanlin</forenames></author><author><keyname>Rasnayaka</keyname><forenames>Sanka</forenames></author><author><keyname>Iyer</keyname><forenames>Ganesh Neelakanta</forenames></author></authors><title>Analysis of Student-LLM Interaction in a Software Engineering Project</title><categories>cs.SE cs.AI</categories><comments>8 pages</comments><acm-class>D.2.3</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) are becoming increasingly competent across various domains, educators are showing a growing interest in integrating these LLMs into the learning process. Especially in software engineering, LLMs have demonstrated qualitatively better capabilities in code summarization, code generation, and debugging. Despite various research on LLMs for software engineering tasks in practice, limited research captures the benefits of LLMs for pedagogical advancements and their impact on the student learning process. To this extent, we analyze 126 undergraduate students' interaction with an AI assistant during a 13-week semester to understand the benefits of AI for software engineering learning. We analyze the conversations, code generated, code utilized, and the human intervention levels to integrate the code into the code base.   Our findings suggest that students prefer ChatGPT over CoPilot. Our analysis also finds that ChatGPT generates responses with lower computational complexity compared to CoPilot. Furthermore, conversational-based interaction helps improve the quality of the code generated compared to auto-generated code. Early adoption of LLMs in software engineering is crucial to remain competitive in the rapidly developing landscape. Hence, the next generation of software engineers must acquire the necessary skills to interact with AI to improve productivity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01275</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01275</id><created>2025-02-03</created><authors><author><keyname>Ayanara</keyname><forenames>Dominica</forenames></author><author><keyname>Hillingworth</keyname><forenames>Atticus</forenames></author><author><keyname>Casselbury</keyname><forenames>Jonathan</forenames></author><author><keyname>Montague</keyname><forenames>Dominic</forenames></author></authors><title>Spectral Entanglement Fingerprinting: A Novel Framework for Ransomware   Detection Using Cross-Frequency Anomalous Waveform Signatures</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Malicious encryption techniques continue to evolve, bypassing conventional detection mechanisms that rely on static signatures or predefined behavioral rules. Spectral analysis presents an alternative approach that transforms system activity data into the frequency domain, enabling the identification of anomalous waveform signatures that are difficult to obfuscate through traditional evasion techniques. The proposed Spectral Entanglement Fingerprinting (SEF) framework leverages power spectral densities, coherence functions, and entropy-based metrics to extract hidden patterns indicative of unauthorized encryption activities. Detection accuracy evaluations demonstrate that frequency-domain transformations achieve superior performance in distinguishing malicious from benign processes, particularly in the presence of polymorphic and metamorphic modifications. Comparative analyses with established methods reveal that frequency-based detection minimizes false positive and false negative rates, ensuring operational efficiency without excessive computational overhead. Experimental results indicate that entropy variations in encrypted data streams provide meaningful classification insights, allowing the differentiation of distinct ransomware families based on spectral characteristics alone. The latency assessment confirms that SEF operates within a time window that enables proactive intervention, mitigating encryption-induced damage before data integrity is compromised. Scalability evaluations suggest that the framework remains effective even under concurrent execution of multiple ransomware instances, supporting its suitability for high-throughput environments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01276</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01276</id><created>2025-02-03</created><authors><author><keyname>Wever</keyname><forenames>Marcel</forenames></author><author><keyname>Muschalik</keyname><forenames>Maximilian</forenames></author><author><keyname>Fumagalli</keyname><forenames>Fabian</forenames></author><author><keyname>Lindauer</keyname><forenames>Marius</forenames></author></authors><title>HyperSHAP: Shapley Values and Interactions for Hyperparameter Importance</title><categories>cs.LG cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Hyperparameter optimization (HPO) is a crucial step in achieving strong predictive performance. However, the impact of individual hyperparameters on model generalization is highly context-dependent, prohibiting a one-size-fits-all solution and requiring opaque automated machine learning (AutoML) systems to find optimal configurations. The black-box nature of most AutoML systems undermines user trust and discourages adoption. To address this, we propose a game-theoretic explainability framework for HPO that is based on Shapley values and interactions. Our approach provides an additive decomposition of a performance measure across hyperparameters, enabling local and global explanations of hyperparameter importance and interactions. The framework, named HyperSHAP, offers insights into ablations, the tunability of learning algorithms, and optimizer behavior across different hyperparameter spaces. We evaluate HyperSHAP on various HPO benchmarks by analyzing the interaction structure of the HPO problem. Our results show that while higher-order interactions exist, most performance improvements can be explained by focusing on lower-order representations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01277</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01277</id><created>2025-02-03</created><authors><author><keyname>Nguyen</keyname><forenames>Thanh-Tung</forenames></author><author><keyname>Liebe</keyname><forenames>Lucas</forenames></author><author><keyname>Tau</keyname><forenames>Nhat-Quang</forenames></author><author><keyname>Wu</keyname><forenames>Yuheng</forenames></author><author><keyname>Cheng</keyname><forenames>Jinghan</forenames></author><author><keyname>Lee</keyname><forenames>Dongman</forenames></author></authors><title>OCTOPINF: Workload-Aware Inference Serving for Edge Video Analytics</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Edge Video Analytics (EVA) has gained significant attention as a major application of pervasive computing, enabling real-time visual processing. EVA pipelines, composed of deep neural networks (DNNs), typically demand efficient inference serving under stringent latency requirements, which is challenging due to the dynamic Edge environments (e.g., workload variability and network instability). Moreover, EVA pipelines also face significant resource contention caused by resource (e.g., GPU) constraints at the Edge. In this paper, we introduce OCTOPINF, a novel resource-efficient and workload-aware inference serving system designed for real-time EVA. OCTOPINF tackles the unique challenges of dynamic edge environments through fine-grained resource allocation, adaptive batching, and workload balancing between edge devices and servers. Furthermore, we propose a spatiotemporal scheduling algorithm that optimizes the co-location of inference tasks on GPUs, improving performance and ensuring service-level objectives (SLOs) compliance. Extensive evaluations on a real-world testbed demonstrate the effectiveness of our approach. It achieves an effective throughput increase of up to 10x compared to the baselines and shows better robustness in challenging scenarios. OCTOPINF can be used for any DNN-based EVA inference task with minimal adaptation and is available at https://github.com/tungngreen/PipelineScheduler. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01278</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01278</id><created>2025-02-03</created><authors><author><keyname>Nayak</keyname><forenames>Nancy</forenames></author><author><keyname>Leung</keyname><forenames>Kin K.</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>DRL-based Dolph-Tschebyscheff Beamforming in Downlink Transmission for   Mobile Users</title><categories>eess.SP cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the emergence of AI technologies in next-generation communication systems, machine learning plays a pivotal role due to its ability to address high-dimensional, non-stationary optimization problems within dynamic environments while maintaining computational efficiency. One such application is directional beamforming, achieved through learning-based blind beamforming techniques that utilize already existing radio frequency (RF) fingerprints of the user equipment obtained from the base stations and eliminate the need for additional hardware or channel and angle estimations. However, as the number of users and antenna dimensions increase, thereby expanding the problem's complexity, the learning process becomes increasingly challenging, and the performance of the learning-based method cannot match that of the optimal solution. In such a scenario, we propose a deep reinforcement learning-based blind beamforming technique using a learnable Dolph-Tschebyscheff antenna array that can change its beam pattern to accommodate mobile users. Our simulation results show that the proposed method can support data rates very close to the best possible values. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01280</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01280</id><created>2025-02-03</created><authors><author><keyname>Xing</keyname><forenames>Zheng</forenames></author><author><keyname>Zhao</keyname><forenames>Weibing</forenames></author></authors><title>Trajectory Map-Matching in Urban Road Networks Based on RSS Measurements</title><categories>eess.SY cs.SY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper proposes an RSS-based approach to reconstruct vehicle trajectories within a road network, enforcing signal propagation rules and vehicle mobility constraints to mitigate the impact of RSS noise and sparsity. The key challenge lies in leveraging latent spatiotemporal correlations within RSS data while navigating complex road networks. To address this, we develop a Hidden Markov Model (HMM)-based RSS embedding (HRE) technique that employs alternating optimization to infer vehicle trajectories from RSS measurements. This model captures spatiotemporal dependencies while a road graph ensures network compliance. Additionally, we introduce a maximum speed-constrained rough trajectory estimation (MSR) method to guide the optimization process, enabling rapid convergence to a favorable local solution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01281</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01281</id><created>2025-02-03</created><authors><author><keyname>Toikka</keyname><forenames>Henrik</forenames></author><author><keyname>Alamikkotervo</keyname><forenames>Eerik</forenames></author><author><keyname>Ojala</keyname><forenames>Risto</forenames></author></authors><title>Label Correction for Road Segmentation Using Road-side Cameras</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Reliable road segmentation in all weather conditions is critical for intelligent transportation applications, autonomous vehicles and advanced driver's assistance systems. For robust performance, all weather conditions should be included in the training data of deep learning-based perception models. However, collecting and annotating such a dataset requires extensive resources. In this paper, existing roadside camera infrastructure is utilized for collecting road data in varying weather conditions automatically. Additionally, a novel semi-automatic annotation method for roadside cameras is proposed. For each camera, only one frame is labeled manually and then the label is transferred to other frames of that camera feed. The small camera movements between frames are compensated using frequency domain image registration. The proposed method is validated with roadside camera data collected from 927 cameras across Finland over 4 month time period during winter. Training on the semi-automatically labeled data boosted the segmentation performance of several deep learning segmentation models. Testing was carried out on two different datasets to evaluate the robustness of the resulting models. These datasets were an in-domain roadside camera dataset and out-of-domain dataset captured with a vehicle on-board camera. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01282</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01282</id><created>2025-02-03</created><authors><author><keyname>Ámon</keyname><forenames>Attila Miklós</forenames></author><author><keyname>Fenech</keyname><forenames>Kristian</forenames></author><author><keyname>Kovács</keyname><forenames>Péter</forenames></author><author><keyname>Dózsa</keyname><forenames>Tamás</forenames></author></authors><title>Rational Gaussian wavelets and corresponding model driven neural   networks</title><categories>stat.ML cs.AI cs.LG</categories><comments>Submitted to IEEE Transactions on Signal Processing, 2024 (under   review)</comments><msc-class>65D15</msc-class><acm-class>G.1.2</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we consider the continuous wavelet transform using Gaussian wavelets multiplied by an appropriate rational term. The zeros and poles of this rational modifier act as free parameters and their choice highly influences the shape of the mother wavelet. This allows the proposed construction to approximate signals with complex morphology using only a few wavelet coefficients. We show that the proposed rational Gaussian wavelets are admissible and provide numerical approximations of the wavelet coefficients using variable projection operators. In addition, we show how the proposed variable projection based rational Gaussian wavelet transform can be used in neural networks to obtain a highly interpretable feature learning layer. We demonstrate the effectiveness of the proposed scheme through a biomedical application, namely, the detection of ventricular ectopic beats (VEBs) in real ECG measurements. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01286</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01286</id><created>2025-02-03</created><authors><author><keyname>Marušić</keyname><forenames>Davor</forenames></author><author><keyname>Popović</keyname><forenames>Siniša</forenames></author><author><keyname>Kalafatić</keyname><forenames>Zoran</forenames></author></authors><title>Template Matching in Images using Segmented Normalized Cross-Correlation</title><categories>cs.CV</categories><comments>14 pages, 2 tables, 3 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, a new variant of an algorithm for normalized cross-correlation (NCC) is proposed in the context of template matching in images. The proposed algorithm is based on the precomputation of a template image approximation, enabling more efficient calculation of approximate NCC with the source image than using the original template for exact NCC calculation. The approximate template is precomputed from the template image by a split-and-merge approach, resulting in a decomposition to axis-aligned rectangular segments, whose sizes depend on per-segment pixel intensity variance. In the approximate template, each segment is assigned the mean grayscale value of the corresponding pixels from the original template. The proposed algorithm achieves superior computational performance with negligible NCC approximation errors compared to the well-known Fast Fourier Transform (FFT)-based NCC algorithm, when applied on less visually complex and/or smaller template images. In other cases, the proposed algorithm can maintain either computational performance or NCC approximation error within the range of the FFT-based algorithm, but not both. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01289</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01289</id><created>2025-02-03</created><authors><author><keyname>Tastan</keyname><forenames>Nurbek</forenames></author><author><keyname>Nandakumar</keyname><forenames>Karthik</forenames></author></authors><title>A Framework for Double-Blind Federated Adaptation of Foundation Models</title><categories>cs.LG cs.CR cs.CV cs.DC</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The availability of foundational models (FMs) pre-trained on large-scale data has advanced the state-of-the-art in many computer vision tasks. While FMs have demonstrated good zero-shot performance on many image classification tasks, there is often scope for performance improvement by adapting the FM to the downstream task. However, the data that is required for this adaptation typically exists in silos across multiple entities (data owners) and cannot be collated at a central location due to regulations and privacy concerns. At the same time, a learning service provider (LSP) who owns the FM cannot share the model with the data owners due to proprietary reasons. In some cases, the data owners may not even have the resources to store such large FMs. Hence, there is a need for algorithms to adapt the FM in a double-blind federated manner, i.e., the data owners do not know the FM or each other's data, and the LSP does not see the data for the downstream tasks. In this work, we propose a framework for double-blind federated adaptation of FMs using fully homomorphic encryption (FHE). The proposed framework first decomposes the FM into a sequence of FHE-friendly blocks through knowledge distillation. The resulting FHE-friendly model is adapted for the downstream task via low-rank parallel adapters that can be learned without backpropagation through the FM. Since the proposed framework requires the LSP to share intermediate representations with the data owners, we design a privacy-preserving permutation scheme to prevent the data owners from learning the FM through model extraction attacks. Finally, a secure aggregation protocol is employed for federated learning of the low-rank parallel adapters. Empirical results on four datasets demonstrate the practical feasibility of the proposed framework. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01290</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01290</id><created>2025-02-03</created><authors><author><keyname>Torrinhas</keyname><forenames>João</forenames></author><author><keyname>Luís</keyname><forenames>Miguel</forenames></author><author><keyname>Dias</keyname><forenames>Duarte</forenames></author><author><keyname>Rito</keyname><forenames>Pedro</forenames></author><author><keyname>Sargento</keyname><forenames>Susana</forenames></author></authors><title>MPTCP in Single Radio Access Networks: a Paradox or an Opportunity?</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper addresses the use of MultiPath Transmission Control Protocol (MPTCP) in a single Radio Access Technology (RAT) network. Different from other studies where multiple network access technologies are explored by the MPTCP, in this work we assess and evaluate the capability of MPTCP to operate over a single radio access network environment. With a vehicular network as use case, we show how the IEEE 802.11p interface is shared among the multiple logical links created between the On-Board Unit (OBU) and the several Road Side Units (RSUs) in its range, supporting the different MPTCP subflows. The results, obtained through experimentation with real vehicular networking hardware, show that MPTCP allows for seamless handovers, ensuring continuous, stable and efficient communication in highly mobile environments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01293</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01293</id><created>2025-02-03</created><authors><author><keyname>Piccinini</keyname><forenames>Lorenzo</forenames></author><author><keyname>Simoncini</keyname><forenames>Valeria</forenames></author></authors><title>TT-LSQR For Tensor Least Squares Problems and Application to Data Mining   *</title><categories>math.NA cs.NA</categories><comments>21 pages, 10 figures, 6 tables, 1 algorithm</comments><msc-class>65F45, 65F55, 15A23</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We are interested in the numerical solution of the tensor least squares problem \[   \min_{\mathcal{X}} \| \mathcal{F} - \sum_{i =1}^{\ell} \mathcal{X} \times_1 A_1^{(i)} \times_2 A_2^{(i)} \cdots \times_d A_d^{(i)} \|_F, \] where $\mathcal{X}\in\mathbb{R}^{m_1 \times m_2 \times \cdots \times m_d}$, $\mathcal{F}\in\mathbb{R}^{n_1\times n_2 \times \cdots \times n_d}$ are tensors with $d$ dimensions, and the coefficients $A_j^{(i)}$ are tall matrices of conforming dimensions. We first describe a tensor implementation of the classical LSQR method by Paige and Saunders, using the tensor-train representation as key ingredient. We also show how to incorporate sketching to lower the computational cost of dealing with the tall matrices $A_j^{(i)}$. We then use this methodology to address a problem in information retrieval, the classification of a new query document among already categorized documents, according to given keywords. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01295</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01295</id><created>2025-02-03</created><authors><author><keyname>Ahmetaj</keyname><forenames>S.</forenames></author><author><keyname>Boneva</keyname><forenames>I.</forenames></author><author><keyname>Hidders</keyname><forenames>J.</forenames></author><author><keyname>Hose</keyname><forenames>K.</forenames></author><author><keyname>Jakubowski</keyname><forenames>M.</forenames></author><author><keyname>Labra-Gayo</keyname><forenames>J. E.</forenames></author><author><keyname>Martens</keyname><forenames>W.</forenames></author><author><keyname>Mogavero</keyname><forenames>F.</forenames></author><author><keyname>Murlak</keyname><forenames>F.</forenames></author><author><keyname>Okulmus</keyname><forenames>C.</forenames></author><author><keyname>Polleres</keyname><forenames>A.</forenames></author><author><keyname>Savkovic</keyname><forenames>O.</forenames></author><author><keyname>Simkus</keyname><forenames>M.</forenames></author><author><keyname>Tomaszuk</keyname><forenames>D.</forenames></author></authors><title>Common Foundations for SHACL, ShEx, and PG-Schema</title><categories>cs.DB cs.AI</categories><comments>To be published at WWW 2025</comments><acm-class>I.2.4</acm-class><doi>10.1145/3696410.3714694</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Graphs have emerged as an important foundation for a variety of applications, including capturing and reasoning over factual knowledge, semantic data integration, social networks, and providing factual knowledge for machine learning algorithms. To formalise certain properties of the data and to ensure data quality, there is a need to describe the schema of such graphs. Because of the breadth of applications and availability of different data models, such as RDF and property graphs, both the Semantic Web and the database community have independently developed graph schema languages: SHACL, ShEx, and PG-Schema. Each language has its unique approach to defining constraints and validating graph data, leaving potential users in the dark about their commonalities and differences. In this paper, we provide formal, concise definitions of the core components of each of these schema languages. We employ a uniform framework to facilitate a comprehensive comparison between the languages and identify a common set of functionalities, shedding light on both overlapping and distinctive features of the three languages. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01296</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01296</id><created>2025-02-03</created><authors><author><keyname>Xie</keyname><forenames>HongXin</forenames></author><author><keyname>Sun</keyname><forenames>JianDe</forenames></author><author><keyname>Shao</keyname><forenames>Yi</forenames></author><author><keyname>Li</keyname><forenames>Shuai</forenames></author><author><keyname>Hou</keyname><forenames>Sujuan</forenames></author><author><keyname>Sun</keyname><forenames>YuLong</forenames></author><author><keyname>Liu</keyname><forenames>Yuxiang</forenames></author></authors><title>Molecular Odor Prediction with Harmonic Modulated Feature Mapping and   Chemically-Informed Loss</title><categories>cs.LG q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Molecular odor prediction has great potential across diverse fields such as chemistry, pharmaceuticals, and environmental science, enabling the rapid design of new materials and enhancing environmental monitoring. However, current methods face two main challenges: First, existing models struggle with non-smooth objective functions and the complexity of mixed feature dimensions; Second, datasets suffer from severe label imbalance, which hampers model training, particularly in learning minority class labels. To address these issues, we introduce a novel feature mapping method and a molecular ensemble optimization loss function. By incorporating feature importance learning and frequency modulation, our model adaptively adjusts the contribution of each feature, efficiently capturing the intricate relationship between molecular structures and odor descriptors. Our feature mapping preserves feature independence while enhancing the model's efficiency in utilizing molecular features through frequency modulation. Furthermore, the proposed loss function dynamically adjusts label weights, improves structural consistency, and strengthens label correlations, effectively addressing data imbalance and label co-occurrence challenges. Experimental results show that our method significantly can improves the accuracy of molecular odor prediction across various deep learning models, demonstrating its promising potential in molecular structure representation and chemoinformatics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01297</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01297</id><created>2025-02-03</created><authors><author><keyname>Zhai</keyname><forenames>Shangjin</forenames></author><author><keyname>Wang</keyname><forenames>Nan</forenames></author><author><keyname>Wang</keyname><forenames>Xiaomeng</forenames></author><author><keyname>Chen</keyname><forenames>Danpeng</forenames></author><author><keyname>Xie</keyname><forenames>Weijian</forenames></author><author><keyname>Bao</keyname><forenames>Hujun</forenames></author><author><keyname>Zhang</keyname><forenames>Guofeng</forenames></author></authors><title>XR-VIO: High-precision Visual Inertial Odometry with Fast Initialization   for XR Applications</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach to Visual Inertial Odometry (VIO), focusing on the initialization and feature matching modules. Existing methods for initialization often suffer from either poor stability in visual Structure from Motion (SfM) or fragility in solving a huge number of parameters simultaneously. To address these challenges, we propose a new pipeline for visual inertial initialization that robustly handles various complex scenarios. By tightly coupling gyroscope measurements, we enhance the robustness and accuracy of visual SfM. Our method demonstrates stable performance even with only four image frames, yielding competitive results. In terms of feature matching, we introduce a hybrid method that combines optical flow and descriptor-based matching. By leveraging the robustness of continuous optical flow tracking and the accuracy of descriptor matching, our approach achieves efficient, accurate, and robust tracking results. Through evaluation on multiple benchmarks, our method demonstrates state-of-the-art performance in terms of accuracy and success rate. Additionally, a video demonstration on mobile devices showcases the practical applicability of our approach in the field of Augmented Reality/Virtual Reality (AR/VR). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01298</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01298</id><created>2025-02-03</created><authors><author><keyname>Arazzi</keyname><forenames>Marco</forenames></author><author><keyname>Ligari</keyname><forenames>Davide</forenames></author><author><keyname>Nicolazzo</keyname><forenames>Serena</forenames></author><author><keyname>Nocera</keyname><forenames>Antonino</forenames></author></authors><title>Augmented Knowledge Graph Querying leveraging LLMs</title><categories>cs.IR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Adopting Knowledge Graphs (KGs) as a structured, semantic-oriented, data representation model has significantly improved data integration, reasoning, and querying capabilities across different domains. This is especially true in modern scenarios such as Industry 5.0, in which the integration of data produced by humans, smart devices, and production processes plays a crucial role. However, the management, retrieval, and visualization of data from a KG using formal query languages can be difficult for non-expert users due to their technical complexity, thus limiting their usage inside industrial environments. For this reason, we introduce SparqLLM, a framework that utilizes a Retrieval-Augmented Generation (RAG) solution, to enhance the querying of Knowledge Graphs (KGs). SparqLLM executes the Extract, Transform, and Load (ETL) pipeline to construct KGs from raw data. It also features a natural language interface powered by Large Language Models (LLMs) to enable automatic SPARQL query generation. By integrating template-based methods as retrieved-context for the LLM, SparqLLM enhances query reliability and reduces semantic errors, ensuring more accurate and efficient KG interactions. Moreover, to improve usability, the system incorporates a dynamic visualization dashboard that adapts to the structure of the retrieved data, presenting the query results in an intuitive format. Rigorous experimental evaluations demonstrate that SparqLLM achieves high query accuracy, improved robustness, and user-friendly interaction with KGs, establishing it as a scalable solution to access semantic data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01299</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01299</id><created>2025-02-03</created><authors><author><keyname>Wu</keyname><forenames>Hanlin</forenames></author><author><keyname>Rao</keyname><forenames>Xiaohui</forenames></author><author><keyname>Cai</keyname><forenames>Zhenguang G.</forenames></author></authors><title>Probabilistic adaptation of language comprehension for individual   speakers: Evidence from neural oscillations</title><categories>q-bio.NC cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Listeners adapt language comprehension based on their mental representations of speakers, but how these representations are dynamically updated remains unclear. We investigated whether listeners probabilistically adapt their comprehension based on the likelihood of speakers producing stereotype-incongruent utterances. Our findings reveal two potential mechanisms: a speaker-general mechanism that adjusts overall expectations about speaker-content relationships, and a speaker-specific mechanism that updates individual speaker models. In two EEG experiments, participants heard speakers make stereotype-congruent or incongruent utterances, with incongruency base rate manipulated between blocks. In Experiment 1, speaker incongruency modulated both high-beta (21-30 Hz) and theta (4-6 Hz) oscillations: incongruent utterances decreased oscillatory power in low base rate condition but increased it in high base rate condition. The theta effect varied with listeners' openness trait: less open participants showed theta increases to speaker-incongruencies, suggesting maintenance of speaker-specific information, while more open participants showed theta decreases, indicating flexible model updating. In Experiment 2, we dissociated base rate from the target speaker by manipulating the overall base rate using an alternative non-target speaker. Only the high-beta effect persisted, showing power decrease for speaker-incongruencies in low base rate condition but no effect in high base rate condition. The high-beta oscillations might reflect the speaker-general adjustment, while theta oscillations may index the speaker-specific model updating. These findings provide evidence for how language processing is shaped by social cognition in real time. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01303</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01303</id><created>2025-02-03</created><authors><author><keyname>Huang</keyname><forenames>Haiduo</forenames></author><author><keyname>Xia</keyname><forenames>Tian</forenames></author><author><keyname>zhao</keyname><forenames>Wenzhe</forenames></author><author><keyname>Ren</keyname><forenames>Pengju</forenames></author></authors><title>Partial Channel Network: Compute Fewer, Perform Better</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing a module or mechanism that enables a network to maintain low parameters and FLOPs without sacrificing accuracy and throughput remains a challenge. To address this challenge and exploit the redundancy within feature map channels, we propose a new solution: partial channel mechanism (PCM). Specifically, through the split operation, the feature map channels are divided into different parts, with each part corresponding to different operations, such as convolution, attention, pooling, and identity mapping. Based on this assumption, we introduce a novel partial attention convolution (PATConv) that can efficiently combine convolution with visual attention. Our exploration indicates that the PATConv can completely replace both the regular convolution and the regular visual attention while reducing model parameters and FLOPs. Moreover, PATConv can derive three new types of blocks: Partial Channel-Attention block (PAT_ch), Partial Spatial-Attention block (PAT_sp), and Partial Self-Attention block (PAT_sf). In addition, we propose a novel dynamic partial convolution (DPConv) that can adaptively learn the proportion of split channels in different layers to achieve better trade-offs. Building on PATConv and DPConv, we propose a new hybrid network family, named PartialNet, which achieves superior top-1 accuracy and inference speed compared to some SOTA models on ImageNet-1K classification and excels in both detection and segmentation on the COCO dataset. Our code is available at https://github.com/haiduo/PartialNet. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01304</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01304</id><created>2025-02-03</created><authors><author><keyname>Vu</keyname><forenames>Minh Nhat</forenames></author><author><keyname>Wachter</keyname><forenames>Alexander</forenames></author><author><keyname>Ebmer</keyname><forenames>Gerald</forenames></author><author><keyname>Ecker</keyname><forenames>Marc-Philip</forenames></author><author><keyname>Glück</keyname><forenames>Tobias</forenames></author><author><keyname>Nguyen</keyname><forenames>Anh</forenames></author><author><keyname>Kemmetmueller</keyname><forenames>Wolfgang</forenames></author><author><keyname>Kugi</keyname><forenames>Andreas</forenames></author></authors><title>Towards Autonomous Wood-Log Grasping with a Forestry Crane: Simulator   and Benchmarking</title><categories>cs.RO</categories><comments>7 pages. Accepted to ICRA 2025</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Forestry machines operated in forest production environments face challenges when performing manipulation tasks, especially regarding the complicated dynamics of underactuated crane systems and the heavy weight of logs to be grasped. This study investigates the feasibility of using reinforcement learning for forestry crane manipulators in grasping and lifting heavy wood logs autonomously. We first build a simulator using Mujoco physics engine to create realistic scenarios, including modeling a forestry crane with 8 degrees of freedom from CAD data and wood logs of different sizes. We further implement a velocity controller for autonomous log grasping with deep reinforcement learning using a curriculum strategy. Utilizing our new simulator, the proposed control strategy exhibits a success rate of 96% when grasping logs of different diameters and under random initial configurations of the forestry crane. In addition, reward functions and reinforcement learning baselines are implemented to provide an open-source benchmark for the community in large-scale manipulation tasks. A video with several demonstrations can be seen at https://www.acin.tuwien.ac.at/en/d18a/ </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01306</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01306</id><created>2025-02-03</created><authors><author><keyname>Leschanowsky</keyname><forenames>Anna</forenames></author><author><keyname>Salamatjoo</keyname><forenames>Farnaz</forenames></author><author><keyname>Kolagar</keyname><forenames>Zahra</forenames></author><author><keyname>Popp</keyname><forenames>Birgit</forenames></author></authors><title>Expert-Generated Privacy Q&amp;A Dataset for Conversational AI and User   Study Insights</title><categories>cs.HC</categories><comments>Submitted to CHI'25</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Conversational assistants process personal data and must comply with data protection regulations that require providers to be transparent with users about how their data is handled. Transparency, in a legal sense, demands preciseness, comprehensibility and accessibility, yet existing solutions fail to meet these requirements. To address this, we introduce a new human-expert-generated dataset for Privacy Question-Answering (Q&amp;A), developed through an iterative process involving legal professionals and conversational designers. We evaluate this dataset through linguistic analysis and a user study, comparing it to privacy policy excerpts and state-of-the-art responses from Amazon Alexa. Our findings show that the proposed answers improve usability and clarity compared to existing solutions while achieving legal preciseness, thereby enhancing the accessibility of data processing information for Conversational AI and Natural Language Processing applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01307</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01307</id><created>2025-02-03</created><authors><author><keyname>Müller</keyname><forenames>Henrik</forenames></author><author><keyname>Kudenko</keyname><forenames>Daniel</forenames></author></authors><title>Improving the Effectiveness of Potential-Based Reward Shaping in   Reinforcement Learning</title><categories>cs.LG</categories><comments>9 pages, 4 figures, accepted as extended abstract at AAMAS 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Potential-based reward shaping is commonly used to incorporate prior knowledge of how to solve the task into reinforcement learning because it can formally guarantee policy invariance. As such, the optimal policy and the ordering of policies by their returns are not altered by potential-based reward shaping. In this work, we highlight the dependence of effective potential-based reward shaping on the initial Q-values and external rewards, which determine the agent's ability to exploit the shaping rewards to guide its exploration and achieve increased sample efficiency. We formally derive how a simple linear shift of the potential function can be used to improve the effectiveness of reward shaping without changing the encoded preferences in the potential function, and without having to adjust the initial Q-values, which can be challenging and undesirable in deep reinforcement learning. We show the theoretical limitations of continuous potential functions for correctly assigning positive and negative reward shaping values. We verify our theoretical findings empirically on Gridworld domains with sparse and uninformative reward functions, as well as on the Cart Pole and Mountain Car environments, where we demonstrate the application of our results in deep reinforcement learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01309</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01309</id><created>2025-02-03</created><authors><author><keyname>Menneer</keyname><forenames>Rupert</forenames></author><author><keyname>Margadji</keyname><forenames>Christos</forenames></author><author><keyname>Pattinson</keyname><forenames>Sebastian W.</forenames></author></authors><title>Heterogeneous Image GNN: Graph-Conditioned Diffusion for Image Synthesis</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce a novel method for conditioning diffusion-based image synthesis models with heterogeneous graph data. Existing approaches typically incorporate conditioning variables directly into model architectures, either through cross-attention layers that attend to text latents or image concatenation that spatially restrict generation. However, these methods struggle to handle complex scenarios involving diverse, relational conditioning variables, which are more naturally represented as unstructured graphs. This paper presents Heterogeneous Image Graphs (HIG), a novel representation that models conditioning variables and target images as two interconnected graphs, enabling efficient handling of variable-length conditioning inputs and their relationships. We also propose a magnitude-preserving GNN that integrates the HIG into the existing EDM2 diffusion model using a ControlNet approach. Our approach improves upon the SOTA on a variety of conditioning inputs for the COCO-stuff and Visual Genome datasets, and showcases the ability to condition on graph attributes and relationships represented by edges in the HIG. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01310</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01310</id><created>2025-02-03</created><authors><author><keyname>Tarasov</keyname><forenames>Roman</forenames></author><author><keyname>Mokrov</keyname><forenames>Petr</forenames></author><author><keyname>Gazdieva</keyname><forenames>Milena</forenames></author><author><keyname>Burnaev</keyname><forenames>Evgeny</forenames></author><author><keyname>Korotin</keyname><forenames>Alexander</forenames></author></authors><title>A Statistical Learning Perspective on Semi-dual Adversarial Neural   Optimal Transport Solvers</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural network based Optimal Transport (OT) is a recent and fruitful direction in the generative modeling community. It finds its applications in various fields such as domain translation, image super-resolution, computational biology and others. Among the existing approaches to OT, of considerable interest are adversarial minimax solvers based on semi-dual formulations of OT problems. While promising, these methods lack theoretical investigation from a statistical learning perspective. Our work fills this gap by establishing upper bounds on the generalization error of an approximate OT map recovered by the minimax quadratic OT solver. Importantly, the bounds we derive depend solely on some standard statistical and mathematical properties of the considered functional classes (neural networks). While our analysis focuses on the quadratic OT, we believe that similar bounds could be derived for more general OT formulations, paving the promising direction for future research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01311</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01311</id><created>2025-02-03</created><authors><author><keyname>Ghosh</keyname><forenames>Nimisha</forenames></author><author><keyname>Dutta</keyname><forenames>Pratik</forenames></author><author><keyname>Santoni</keyname><forenames>Daniele</forenames></author></authors><title>TFBS-Finder: Deep Learning-based Model with DNABERT and Convolutional   Networks to Predict Transcription Factor Binding Sites</title><categories>cs.LG cs.AI q-bio.GN</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Transcription factors are proteins that regulate the expression of genes by binding to specific genomic regions known as Transcription Factor Binding Sites (TFBSs), typically located in the promoter regions of those genes. Accurate prediction of these binding sites is essential for understanding the complex gene regulatory networks underlying various cellular functions. In this regard, many deep learning models have been developed for such prediction, but there is still scope of improvement. In this work, we have developed a deep learning model which uses pre-trained DNABERT, a Convolutional Neural Network (CNN) module, a Modified Convolutional Block Attention Module (MCBAM), a Multi-Scale Convolutions with Attention (MSCA) module and an output module. The pre-trained DNABERT is used for sequence embedding, thereby capturing the long-term dependencies in the DNA sequences while the CNN, MCBAM and MSCA modules are useful in extracting higher-order local features. TFBS-Finder is trained and tested on 165 ENCODE ChIP-seq datasets. We have also performed ablation studies as well as cross-cell line validations and comparisons with other models. The experimental results show the superiority of the proposed method in predicting TFBSs compared to the existing methodologies. The codes and the relevant datasets are publicly available at https://github.com/NimishaGhosh/TFBS-Finder/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01312</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01312</id><created>2025-02-03</created><authors><author><keyname>Lin</keyname><forenames>Xiao</forenames></author><author><keyname>Peng</keyname><forenames>Yun</forenames></author><author><keyname>Wang</keyname><forenames>Liuyi</forenames></author><author><keyname>Zhong</keyname><forenames>Xianyou</forenames></author><author><keyname>Zhu</keyname><forenames>Minghao</forenames></author><author><keyname>Yang</keyname><forenames>Jingwei</forenames></author><author><keyname>Liu</keyname><forenames>Chengju</forenames></author><author><keyname>Chen</keyname><forenames>Qijun</forenames></author></authors><title>CleanPose: Category-Level Object Pose Estimation via Causal Learning and   Knowledge Distillation</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Category-level object pose estimation aims to recover the rotation, translation and size of unseen instances within predefined categories. In this task, deep neural network-based methods have demonstrated remarkable performance. However, previous studies show they suffer from spurious correlations raised by "unclean" confounders in models, hindering their performance on novel instances with significant variations. To address this issue, we propose CleanPose, a novel approach integrating causal learning and knowledge distillation to enhance category-level pose estimation. To mitigate the negative effect of unobserved confounders, we develop a causal inference module based on front-door adjustment, which promotes unbiased estimation by reducing potential spurious correlations. Additionally, to further improve generalization ability, we devise a residual-based knowledge distillation method that has proven effective in providing comprehensive category information guidance. Extensive experiments across multiple benchmarks (REAL275, CAMERA25 and HouseCat6D) hightlight the superiority of proposed CleanPose over state-of-the-art methods. Code will be released. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01313</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01313</id><created>2025-02-03</created><authors><author><keyname>Geary</keyname><forenames>Jack</forenames></author><author><keyname>Gouk</keyname><forenames>Henry</forenames></author></authors><title>Strategic Classification with Randomised Classifiers</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the problem of strategic classification, where a learner must build a model to classify agents based on features that have been strategically modified. Previous work in this area has concentrated on the case when the learner is restricted to deterministic classifiers. In contrast, we perform a theoretical analysis of an extension to this setting that allows the learner to produce a randomised classifier. We show that, under certain conditions, the optimal randomised classifier can achieve better accuracy than the optimal deterministic classifier, but under no conditions can it be worse. When a finite set of training data is available, we show that the excess risk of Strategic Empirical Risk Minimisation over the class of randomised classifiers is bounded in a similar manner as the deterministic case. In both the deterministic and randomised cases, the risk of the classifier produced by the learner converges to that of the corresponding optimal classifier as the volume of available training data grows. Moreover, this convergence happens at the same rate as in the i.i.d. case. Our findings are compared with previous theoretical work analysing the problem of strategic classification. We conclude that randomisation has the potential to alleviate some issues that could be faced in practice without introducing any substantial downsides. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01315</identifier><datestamp>2025-02-04</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01315</id><created>2025-02-03</created><authors><author><keyname>Dabiri</keyname><forenames>Azita</forenames></author><author><keyname>Önür</keyname><forenames>Giray</forenames></author><author><keyname>Gros</keyname><forenames>Sebastien</forenames></author><author><keyname>De Schutter</keyname><forenames>Bart</forenames></author></authors><title>Optimization-based Coordination of Traffic Lights and Automated Vehicles   at Intersections</title><categories>math.OC cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper tackles the challenge of coordinating traffic lights and automated vehicles at signalized intersections, formulated as a constrained finite-horizon optimal control problem. The problem falls into the category of mixed-integer nonlinear programming, posing challenges for solving large instances. To address this, we introduce a decomposition approach consisting of an upper-level problem for traffic light timing allocation and a set of lower-level problems that generate appropriate commands for automated vehicles in each intersection movement. By leveraging solutions from the lower-level problems and employing parametric optimization techniques, we solve the upper-level problem using a standard sequential quadratic programming approach. The paper concludes by presenting an illustrative numerical example that highlights the effectiveness of our algorithm compared to scenarios where no coordination between traffic lights and vehicles exists. </abstract></arXiv></metadata></record>
