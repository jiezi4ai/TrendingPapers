<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:1401.1137</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>1401.1137</id><created>2014-01-06</created><updated>2015-03-27</updated><authors><author><keyname>Caron</keyname><forenames>François</forenames></author><author><keyname>Fox</keyname><forenames>Emily B.</forenames></author></authors><title>Sparse graphs using exchangeable random measures</title><categories>stat.ME cs.SI math.ST stat.ML stat.TH</categories><comments>New title. Extended version</comments><journal-ref>Journal of the Royal Statistical Society Series B: Statistical   Methodology, Volume 79, Issue 5, November 2017, Pages 1295-1366</journal-ref><doi>10.1111/rssb.12233</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical network modeling has focused on representing the graph as a discrete structure, namely the adjacency matrix, and considering the exchangeability of this array. In such cases, the Aldous-Hoover representation theorem (Aldous, 1981;Hoover, 1979} applies and informs us that the graph is necessarily either dense or empty. In this paper, we instead consider representing the graph as a measure on $\mathbb{R}_+^2$. For the associated definition of exchangeability in this continuous space, we rely on the Kallenberg representation theorem (Kallenberg, 2005). We show that for certain choices of such exchangeable random measures underlying our graph construction, our network process is sparse with power-law degree distribution. In particular, we build on the framework of completely random measures (CRMs) and use the theory associated with such processes to derive important network properties, such as an urn representation for our analysis and network simulation. Our theoretical results are explored empirically and compared to common network models. We then present a Hamiltonian Monte Carlo algorithm for efficient exploration of the posterior distribution and demonstrate that we are able to recover graphs ranging from dense to sparse--and perform associated tests--based on our flexible CRM-based formulation. We explore network properties in a range of real datasets, including Facebook social circles, a political blogosphere, protein networks, citation networks, and world wide web networks, including networks with hundreds of thousands of nodes and millions of edges. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:1602.02114</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>1602.02114</id><created>2016-02-05</created><updated>2017-08-23</updated><authors><author><keyname>Todeschini</keyname><forenames>Adrien</forenames></author><author><keyname>Miscouridou</keyname><forenames>Xenia</forenames></author><author><keyname>Caron</keyname><forenames>François</forenames></author></authors><title>Exchangeable Random Measures for Sparse and Modular Graphs with   Overlapping Communities</title><categories>stat.ME cs.SI physics.soc-ph stat.ML</categories><journal-ref>Journal of the Royal Statistical Society Series B: Statistical   Methodology, Volume 82, Issue 2, April 2020, Pages 487-520</journal-ref><doi>10.1111/rssb.12363</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel statistical model for sparse networks with overlapping community structure. The model is based on representing the graph as an exchangeable point process, and naturally generalizes existing probabilistic models with overlapping block-structure to the sparse regime. Our construction builds on vectors of completely random measures, and has interpretable parameters, each node being assigned a vector representing its level of affiliation to some latent communities. We develop methods for simulating this class of random graphs, as well as to perform posterior inference. We show that the proposed approach can recover interpretable structure from two real-world networks and can handle graphs with thousands of nodes and tens of thousands of edges. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:1902.04714</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>1902.04714</id><created>2019-02-12</created><updated>2019-07-09</updated><authors><author><keyname>Ayed</keyname><forenames>Fadhel</forenames></author><author><keyname>Lee</keyname><forenames>Juho</forenames></author><author><keyname>Caron</keyname><forenames>François</forenames></author></authors><title>Beyond the Chinese Restaurant and Pitman-Yor processes: Statistical   Models with Double Power-law Behavior</title><categories>stat.ML cs.LG</categories><journal-ref>Proceedings of the 36th International Conference on Machine   Learning, PMLR 97:395-404, 2019</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian nonparametric approaches, in particular the Pitman-Yor process and the associated two-parameter Chinese Restaurant process, have been successfully used in applications where the data exhibit a power-law behavior. Examples include natural language processing, natural images or networks. There is also growing empirical evidence that some datasets exhibit a two-regime power-law behavior: one regime for small frequencies, and a second regime, with a different exponent, for high frequencies. In this paper, we introduce a class of completely random measures which are doubly regularly-varying. Contrary to the Pitman-Yor process, we show that when completely random measures in this class are normalized to obtain random probability measures and associated random partitions, such partitions exhibit a double power-law behavior. We discuss in particular three models within this class: the beta prime process (Broderick et al. (2015, 2018), a novel process called generalized BFRY process, and a mixture construction. We derive efficient Markov chain Monte Carlo algorithms to estimate the parameters of these models. Finally, we show that the proposed models provide a better fit than the Pitman-Yor process on various datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:1905.09059</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>1905.09059</id><created>2019-05-22</created><authors><author><keyname>Rathasamuth</keyname><forenames>Wanthanee</forenames></author><author><keyname>Pasupa</keyname><forenames>Kitsuchart</forenames></author><author><keyname>Tongsima</keyname><forenames>Sissades</forenames></author></authors><title>Selection of a Minimal Number of Significant Porcine SNPs by an   Information Gain and Genetic Algorithm Hybrid Model</title><categories>q-bio.QM cs.NE stat.AP</categories><comments>16 pages, 9 figures, preprint submitted to Malaysian Journal of   Computer Science</comments><journal-ref>Malaysian Journal of Computer Science, SI2, 79--95 (2019)</journal-ref><doi>10.22452/mjcs.sp2019no2.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A panel of large number of common Single Nucleotide Polymorphisms (SNPs) distributed across an entire porcine genome has been widely used to represent genetic variability of pig. With the advent of SNP-array technology, a genome-wide genetic profile of a specimen can be easily observed. Among the large number of such variations, there exist a much smaller subset of the SNP panel that could equally be used to correctly identify the corresponding breed. This work presents a SNP selection heuristic that can still be used effectively in the breed classification process. The proposed feature selection was done by the approach of combining a filter method and a wrapper method--information gain method and genetic algorithm--plus a feature frequency selection step, while classification was done by support vector machine. The approach was able to reduce the number of significant SNPs to 0.86 % of the total number of SNPs in a swine dataset and provided a high classification accuracy of 94.80 %. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:1905.10733</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>1905.10733</id><created>2019-05-26</created><authors><author><keyname>Lee</keyname><forenames>Juho</forenames></author><author><keyname>Miscouridou</keyname><forenames>Xenia</forenames></author><author><keyname>Caron</keyname><forenames>François</forenames></author></authors><title>A unified construction for series representations and finite   approximations of completely random measures</title><categories>math.ST cs.LG stat.ML stat.TH</categories><journal-ref>Bernoulli 29(3): 2142-2166, 2023</journal-ref><doi>10.3150/22-BEJ1536</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Infinite-activity completely random measures (CRMs) have become important building blocks of complex Bayesian nonparametric models. They have been successfully used in various applications such as clustering, density estimation, latent feature models, survival analysis or network science. Popular infinite-activity CRMs include the (generalized) gamma process and the (stable) beta process. However, except in some specific cases, exact simulation or scalable inference with these models is challenging and finite-dimensional approximations are often considered. In this work, we propose a general and unified framework to derive both series representations and finite-dimensional approximations of CRMs. Our framework can be seen as an extension of constructions based on size-biased sampling of Poisson point process [Perman1992]. It includes as special cases several known series representations as well as novel ones. In particular, we show that one can get novel series representations for the generalized gamma process and the stable beta process. We also provide some analysis of the truncation error. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2001.03329</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2001.03329</id><created>2020-01-10</created><authors><author><keyname>Pasupa</keyname><forenames>Kitsuchart</forenames></author><author><keyname>Vatathanavaro</keyname><forenames>Supawit</forenames></author><author><keyname>Tungjitnob</keyname><forenames>Suchat</forenames></author></authors><title>Convolutional Neural Networks based Focal Loss for Class Imbalance   Problem: A Case Study of Canine Red Blood Cells Morphology Classification</title><categories>eess.IV cs.CV</categories><journal-ref>Journal of Ambient Intelligence and Humanized Computing, 14,   15259--15275 (2023)</journal-ref><doi>10.1007/s12652-020-01773-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Morphologies of red blood cells are normally interpreted by a pathologist. It is time-consuming and laborious. Furthermore, a misclassified red blood cell morphology will lead to false disease diagnosis and improper treatment. Thus, a decent pathologist must truly be an expert in classifying red blood cell morphology. In the past decade, many approaches have been proposed for classifying human red blood cell morphology. However, those approaches have not addressed the class imbalance problem in classification. A class imbalance problem---a problem where the numbers of samples in classes are very different---is one of the problems that can lead to a biased model towards the majority class. Due to the rarity of every type of abnormal blood cell morphology, the data from the collection process are usually imbalanced. In this study, we aimed to solve this problem specifically for classification of dog red blood cell morphology by using a Convolutional Neural Network (CNN)---a well-known deep learning technique---in conjunction with a focal loss function, adept at handling class imbalance problem. The proposed technique was conducted on a well-designed framework: two different CNNs were used to verify the effectiveness of the focal loss function and the optimal hyper-parameters were determined by 5-fold cross-validation. The experimental results show that both CNNs models augmented with the focal loss function achieved higher $F_{1}$-scores, compared to the models augmented with a conventional cross-entropy loss function that does not address class imbalance problem. In other words, the focal loss function truly enabled the CNNs models to be less biased towards the majority class than the cross-entropy did in the classification task of imbalanced dog red blood cell data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2005.06411</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2005.06411</id><created>2020-05-13</created><updated>2025-02-05</updated><authors><author><keyname>Murawski</keyname><forenames>Andrzej S.</forenames></author><author><keyname>Ramsay</keyname><forenames>Steven J.</forenames></author><author><keyname>Tzevelekos</keyname><forenames>Nikos</forenames></author></authors><title>Bisimilarity in fresh-register automata</title><categories>cs.LO cs.FL</categories><proxy>LMCS</proxy><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Register automata are a basic model of computation over infinite alphabets. Fresh-register automata extend register automata with the capability to generate fresh symbols in order to model computational scenarios involving name creation. This paper investigates the complexity of the bisimilarity problem for classes of register and fresh-register automata. We examine all main disciplines that have appeared in the literature: general register assignments; assignments where duplicate register values are disallowed; and assignments without duplicates in which registers cannot be empty. In the general case, we show that the problem is EXPTIME-complete. However, the absence of duplicate values in registers enables us to identify inherent symmetries inside the associated bisimulation relations, which can be used to establish a polynomial bound on the depth of Attacker-winning strategies. Furthermore, they enable a highly succinct representation of the corresponding bisimulations. By exploiting results from group theory and computational group theory, we can then show solvability in PSPACE and NP respectively for the latter two register disciplines. In each case, we find that freshness does not affect the complexity class of the problem. The results allow us to close a complexity gap for language equivalence of deterministic register automata. We show that deterministic language inequivalence for the no-duplicates fragment is NP-complete, which disproves an old conjecture of Sakamoto. Finally, we discover that, unlike in the finite-alphabet case, the addition of pushdown store makes bisimilarity undecidable, even in the case of visibly pushdown storage. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2008.12019</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2008.12019</id><created>2020-08-27</created><updated>2025-02-04</updated><authors><author><keyname>Arhancet</keyname><forenames>Cédric</forenames></author></authors><title>Quantum information theory and Fourier multipliers on quantum groups</title><categories>math.OA cs.IT math-ph math.FA math.IT math.MP quant-ph</categories><comments>136 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we compute the exact values of the minimum output entropy and the completely bounded minimal entropy of very large classes of quantum channels acting on matrix algebras $\mathrm{M}_n$. Our new and simple approach relies on the theory of locally compact quantum groups and our results use a new and precise description of bounded Fourier multipliers from $\mathrm{L}^1(\mathbb{G})$ into $\mathrm{L}^p(\mathbb{G})$ for $1 &lt; p \leq \infty$ where $\mathbb{G}$ is a co-amenable locally compact quantum group and on the automatic completely boundedness of these multipliers that this description entails. Indeed, our approach even allows to use convolution operators on quantum hypergroups. This enable us to connect equally the topic of computation of entropies and capacities to subfactor planar algebras. We also give a upper bound of the classical capacity of each considered quantum channel which is already sharp in the commutative case. Quite surprisingly, we observe by direct computations that some Fourier multipliers identifies to direct sums of classical examples of quantum channels (as dephasing channel or depolarizing channels). Indeed, we show that the study of unital qubit channels can be seen as a part of the theory of Fourier multipliers on the von Neumann algebra of the quaternion group $\mathbb{Q}_8$. Unexpectedly, we also connect ergodic actions of (quantum) groups to this topic of computation, allowing some transference to other channels. We also connect the Quantum Harmonic analysis of Werner. Finally, we investigate entangling breaking and $\mathrm{PPT}$ Fourier multipliers and we characterize conditional expectations which are entangling breaking. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2010.08891</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2010.08891</id><created>2020-10-17</created><updated>2025-02-04</updated><authors><author><keyname>Shrestha</keyname><forenames>Aayam</forenames></author><author><keyname>Lee</keyname><forenames>Stefan</forenames></author><author><keyname>Tadepalli</keyname><forenames>Prasad</forenames></author><author><keyname>Fern</keyname><forenames>Alan</forenames></author></authors><title>DeepAveragers: Offline Reinforcement Learning by Solving Derived   Non-Parametric MDPs</title><categories>cs.LG cs.AI stat.ML</categories><comments>Camera Ready ICLR 2021</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an approach to offline reinforcement learning (RL) based on optimally solving finitely-represented MDPs derived from a static dataset of experience. This approach can be applied on top of any learned representation and has the potential to easily support multiple solution objectives as well as zero-shot adjustment to changing environments and goals. Our main contribution is to introduce the Deep Averagers with Costs MDP (DAC-MDP) and to investigate its solutions for offline RL. DAC-MDPs are a non-parametric model that can leverage deep representations and account for limited data by introducing costs for exploiting under-represented parts of the model. In theory, we show conditions that allow for lower-bounding the performance of DAC-MDP solutions. We also investigate the empirical behavior in a number of environments, including those with image-based observations. Overall, the experiments demonstrate that the framework can work in practice and scale to large complex offline RL problems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2108.03140</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2108.03140</id><created>2021-08-06</created><authors><author><keyname>Kudisthalert</keyname><forenames>Wasu</forenames></author><author><keyname>Pasupa</keyname><forenames>Kitsuchart</forenames></author><author><keyname>Morales</keyname><forenames>Aythami</forenames></author><author><keyname>Fierrez</keyname><forenames>Julian</forenames></author></authors><title>SELM: Siamese Extreme Learning Machine with Application to Face   Biometrics</title><categories>cs.CV cs.LG</categories><comments>15 pages, 11 figures</comments><journal-ref>Neural Computing and Applications, 34, 12143--12157 (2022)</journal-ref><doi>10.1007/s00521-022-07100-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extreme Learning Machine is a powerful classification method very competitive existing classification methods. It is extremely fast at training. Nevertheless, it cannot perform face verification tasks properly because face verification tasks require comparison of facial images of two individuals at the same time and decide whether the two faces identify the same person. The structure of Extreme Leaning Machine was not designed to feed two input data streams simultaneously, thus, in 2-input scenarios Extreme Learning Machine methods are normally applied using concatenated inputs. However, this setup consumes two times more computational resources and it is not optimized for recognition tasks where learning a separable distance metric is critical. For these reasons, we propose and develop a Siamese Extreme Learning Machine (SELM). SELM was designed to be fed with two data streams in parallel simultaneously. It utilizes a dual-stream Siamese condition in the extra Siamese layer to transform the data before passing it along to the hidden layer. Moreover, we propose a Gender-Ethnicity-Dependent triplet feature exclusively trained on a variety of specific demographic groups. This feature enables learning and extracting of useful facial features of each group. Experiments were conducted to evaluate and compare the performances of SELM, Extreme Learning Machine, and DCNN. The experimental results showed that the proposed feature was able to perform correct classification at 97.87% accuracy and 99.45% AUC. They also showed that using SELM in conjunction with the proposed feature provided 98.31% accuracy and 99.72% AUC. They outperformed the well-known DCNN and Extreme Leaning Machine methods by a wide margin. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2108.08158</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2108.08158</id><created>2021-08-18</created><updated>2025-02-05</updated><authors><author><keyname>Okamoto</keyname><forenames>Hideaki</forenames></author><author><keyname>Cap</keyname><forenames>Quan Huu</forenames></author><author><keyname>Nomura</keyname><forenames>Takakiyo</forenames></author><author><keyname>Nabeshima</keyname><forenames>Kazuhito</forenames></author><author><keyname>Hashimoto</keyname><forenames>Jun</forenames></author><author><keyname>Iyatomi</keyname><forenames>Hitoshi</forenames></author></authors><title>Practical X-ray Gastric Cancer Diagnostic Support Using Refined   Stochastic Data Augmentation and Hard Boundary Box Training</title><categories>eess.IV cs.CV</categories><comments>20 pages, 6 figures</comments><journal-ref>Artificial Intelligence in Medicine, 103075, 2025</journal-ref><doi>10.1016/j.artmed.2025.103075</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Endoscopy is widely used to diagnose gastric cancer and has a high diagnostic performance, but it must be performed by a physician, which limits the number of people who can be diagnosed. In contrast, gastric X-rays can be taken by radiographers, thus allowing a much larger number of patients to undergo imaging. However, the diagnosis of X-ray images relies heavily on the expertise and experience of physicians, and few machine learning methods have been developed to assist in this process. We propose a novel and practical gastric cancer diagnostic support system for gastric X-ray images that will enable more people to be screened. The system is based on a general deep learning-based object detection model and incorporates two novel techniques: refined probabilistic stomach image augmentation (R-sGAIA) and hard boundary box training (HBBT). R-sGAIA enhances the probabilistic gastric fold region and provides more learning patterns for cancer detection models. HBBT is an efficient training method that improves model performance by allowing the use of unannotated negative (i.e., healthy control) samples, which are typically unusable in conventional detection models. The proposed system achieved a sensitivity (SE) for gastric cancer of 90.2\%, higher than that of an expert (85.5%). Under these conditions, two out of five candidate boxes identified by the system were cancerous (precision = 42.5%), with an image processing speed of 0.51 seconds per image. The system also outperformed methods using the same object detection model and state-of-the-art data augmentation by showing a 5.9-point improvement in the F1 score. In summary, this system efficiently identifies areas for radiologists to examine within a practical time frame, thus significantly reducing their workload. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2109.14357</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2109.14357</id><created>2021-09-29</created><authors><author><keyname>Poncelet</keyname><forenames>Jakob</forenames></author><author><keyname>Van hamme</keyname><forenames>Hugo</forenames></author></authors><title>Comparison of Self-Supervised Speech Pre-Training Methods on Flemish   Dutch</title><categories>eess.AS cs.SD</categories><comments>To be published in the 2021 IEEE Automatic Speech Recognition and   Understanding Workshop (ASRU 2021)</comments><journal-ref>2021 IEEE Automatic Speech Recognition and Understanding Workshop   (ASRU), pp. 169-176</journal-ref><doi>10.1109/ASRU51503.2021.9688061</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent research in speech processing exhibits a growing interest in unsupervised and self-supervised representation learning from unlabelled data to alleviate the need for large amounts of annotated data. We investigate several popular pre-training methods and apply them to Flemish Dutch. We compare off-the-shelf English pre-trained models to models trained on an increasing amount of Flemish data. We find that the most important factors for positive transfer to downstream speech recognition tasks include a substantial amount of data and a matching pre-training domain. Ideally, we also finetune on an annotated subset in the target language. All pre-trained models improve linear phone separability in Flemish, but not all methods improve Automatic Speech Recognition. We experience superior performance with wav2vec 2.0 and we obtain a 30% WER improvement by finetuning the multilingually pre-trained XLSR-53 model on Flemish Dutch, after integration into an HMM-DNN acoustic model. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2110.07000</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2110.07000</id><created>2021-10-13</created><updated>2025-02-05</updated><authors><author><keyname>Lan</keyname><forenames>Leon</forenames></author><author><keyname>Zocca</keyname><forenames>Alessandro</forenames></author></authors><title>Mixed-integer linear programming approaches for tree partitioning of   power networks</title><categories>math.OC cs.SY eess.SY</categories><comments>10 pages, 4 figures</comments><doi>10.1109/TCNS.2025.3538472</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In transmission networks, power flows and network topology are deeply intertwined due to power flow physics. Recent literature shows that a specific more hierarchical network structure can effectively inhibit the propagation of line failures across the entire system. In particular, a novel approach named tree partitioning has been proposed, which seeks to bolster the robustness of power networks through strategic alterations in network topology, accomplished via targeted line switching actions. Several tree partitioning problem formulations have been proposed by considering different objectives, among which power flow disruption and network congestion. Furthermore, various heuristic methods based on a two-stage and recursive approach have been proposed. The present work provides a general framework for tree partitioning problems based on mixed-integer linear programming (MILP). In particular, we present a novel MILP formulation to optimally solve tree partitioning problems and also propose a two-stage heuristic based on MILP. We perform extensive numerical experiments to solve two tree partitioning problem variants, demonstrating the excellent performance of our solution methods. Lastly, through exhaustive cascading failure simulations, we compare the effectiveness of various tree partitioning strategies and show that, on average, they can achieve a substantial reduction in lost load compared to the original topologies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2110.13103</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2110.13103</id><created>2021-10-25</created><updated>2025-02-05</updated><authors><author><keyname>Chehreghani</keyname><forenames>Morteza Haghir</forenames></author></authors><title>Shift of Pairwise Similarities for Data Clustering</title><categories>cs.LG cs.AI</categories><comments>Machine Learning (2022). An extension of the following work: Morteza   Haghir Chehreghani, "Clustering by Shift", IEEE International Conference on   Data Mining (ICDM), pp. 793-798, 2017</comments><doi>10.1007/s10994-022-06189-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several clustering methods (e.g., Normalized Cut and Ratio Cut) divide the Min Cut cost function by a cluster dependent factor (e.g., the size or the degree of the clusters), in order to yield a more balanced partitioning. We, instead, investigate adding such regularizations to the original cost function. We first consider the case where the regularization term is the sum of the squared size of the clusters, and then generalize it to adaptive regularization of the pairwise similarities. This leads to shifting (adaptively) the pairwise similarities which might make some of them negative. We then study the connection of this method to Correlation Clustering and then propose an efficient local search optimization algorithm with fast theoretical convergence rate to solve the new clustering problem. In the following, we investigate the shift of pairwise similarities on some common clustering methods, and finally, we demonstrate the superior performance of the method by extensive experiments on different datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2111.03861</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2111.03861</id><created>2021-11-06</created><authors><author><keyname>Awais</keyname><forenames>Ch Muhammad</forenames></author><author><keyname>Bekkouch</keyname><forenames>Imad Eddine Ibrahim</forenames></author></authors><title>What augmentations are sensitive to hyper-parameters and why?</title><categories>cs.CV cs.AI cs.LG</categories><comments>10 pages, 17 figures</comments><doi>10.1007/978-3-031-10461-9_31</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply augmentations to our dataset to enhance the quality of our predictions and make our final models more resilient to noisy data and domain drifts. Yet the question remains, how are these augmentations going to perform with different hyper-parameters? In this study we evaluate the sensitivity of augmentations with regards to the model's hyper parameters along with their consistency and influence by performing a Local Surrogate (LIME) interpretation on the impact of hyper-parameters when different augmentations are applied to a machine learning model. We have utilized Linear regression coefficients for weighing each augmentation. Our research has proved that there are some augmentations which are highly sensitive to hyper-parameters and others which are more resilient and reliable. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2204.05923</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2204.05923</id><created>2022-04-12</created><updated>2025-02-05</updated><authors><author><keyname>Engquist</keyname><forenames>Björn</forenames></author><author><keyname>Ren</keyname><forenames>Kui</forenames></author><author><keyname>Yang</keyname><forenames>Yunan</forenames></author></authors><title>An Algebraically Converging Stochastic Gradient Descent Algorithm for   Global Optimization</title><categories>math.OC cs.LG</categories><comments>35 pages, 9 figures</comments><msc-class>90C26, 90C15, 65K05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new gradient descent algorithm with added stochastic terms for finding the global optimizers of nonconvex optimization problems. A key component in the algorithm is the adaptive tuning of the randomness based on the value of the objective function. In the language of simulated annealing, the temperature is state-dependent. With this, we prove the global convergence of the algorithm with an algebraic rate both in probability and in the parameter space. This is a significant improvement over the classical rate from using a more straightforward control of the noise term. The convergence proof is based on the actual discrete setup of the algorithm, not just its continuous limit as often done in the literature. We also present several numerical examples to demonstrate the efficiency and robustness of the algorithm for reasonably complex objective functions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2205.08187</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2205.08187</id><created>2022-05-17</created><updated>2023-09-11</updated><authors><author><keyname>Lee</keyname><forenames>Hoil</forenames></author><author><keyname>Ayed</keyname><forenames>Fadhel</forenames></author><author><keyname>Jung</keyname><forenames>Paul</forenames></author><author><keyname>Lee</keyname><forenames>Juho</forenames></author><author><keyname>Yang</keyname><forenames>Hongseok</forenames></author><author><keyname>Caron</keyname><forenames>François</forenames></author></authors><title>Deep neural networks with dependent weights: Gaussian Process mixture   limit, heavy tails, sparsity and compressibility</title><categories>stat.ML cs.LG math.PR math.ST stat.TH</categories><comments>96 pages, 15 figures, 9 tables</comments><msc-class>68T07 (Primary), 62M45, 60F99 (Secondary)</msc-class><journal-ref>Journal Of Machine Learning Research, 24(289):1-78, 2023</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article studies the infinite-width limit of deep feedforward neural networks whose weights are dependent, and modelled via a mixture of Gaussian distributions. Each hidden node of the network is assigned a nonnegative random variable that controls the variance of the outgoing weights of that node. We make minimal assumptions on these per-node random variables: they are iid and their sum, in each layer, converges to some finite random variable in the infinite-width limit. Under this model, we show that each layer of the infinite-width neural network can be characterised by two simple quantities: a non-negative scalar parameter and a L\'evy measure on the positive reals. If the scalar parameters are strictly positive and the L\'evy measures are trivial at all hidden layers, then one recovers the classical Gaussian process (GP) limit, obtained with iid Gaussian weights. More interestingly, if the L\'evy measure of at least one layer is non-trivial, we obtain a mixture of Gaussian processes (MoGP) in the large-width limit. The behaviour of the neural network in this regime is very different from the GP regime. One obtains correlated outputs, with non-Gaussian distributions, possibly with heavy tails. Additionally, we show that, in this regime, the weights are compressible, and some nodes have asymptotically non-negligible contributions, therefore representing important hidden features. Many sparsity-promoting neural network models can be recast as special cases of our approach, and we discuss their infinite-width limits; we also present an asymptotic analysis of the pruning error. We illustrate some of the benefits of the MoGP regime over the GP regime in terms of representation learning and compressibility on simulated, MNIST and Fashion MNIST datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2205.09639</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2205.09639</id><created>2022-05-19</created><updated>2025-02-05</updated><authors><author><keyname>Neufeld</keyname><forenames>Ariel</forenames></author><author><keyname>Wu</keyname><forenames>Sizhou</forenames></author></authors><title>Multilevel Picard approximation algorithm for semilinear partial   integro-differential equations and its complexity analysis</title><categories>math.NA cs.NA math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a multilevel Picard approximation algorithm for semilinear parabolic partial integro-differential equations (PIDEs). We prove that the numerical approximation scheme converges to the unique viscosity solution of the PIDE under consideration. To that end, we derive a Feynman-Kac representation for the unique viscosity solution of the semilinear PIDE, extending the classical Feynman-Kac representation for linear PIDEs. Furthermore, we show that the algorithm does not suffer from the curse of dimensionality, i.e. the computational complexity of the algorithm is bounded polynomially in the dimension $d$ and the reciprocal of the prescribed accuracy $\varepsilon$. We also provide a numerical example in up to 10'000 dimensions to demonstrate its applicability. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2206.07010</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2206.07010</id><created>2022-06-14</created><authors><author><keyname>Sellami</keyname><forenames>Khaled</forenames><affiliation>Laval University</affiliation></author><author><keyname>Saied</keyname><forenames>Mohamed Aymen</forenames><affiliation>Laval University</affiliation></author><author><keyname>Ouni</keyname><forenames>Ali</forenames><affiliation>ETS Montreal</affiliation><affiliation>University of Quebec</affiliation></author></authors><title>A Hierarchical-DBSCAN Method for Extracting Microservices from   Monolithic Applications</title><categories>cs.SE</categories><doi>10.1145/3530019.35300</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The microservices architectural style offers many advantages such as scalability, reusability and ease of maintainability. As such microservices has become a common architectural choice when developing new applications. Hence, to benefit from these advantages, monolithic applications need to be redesigned in order to migrate to a microservice based architecture. Due to the inherent complexity and high costs related to this process, it is crucial to automate this task. In this paper, we propose a method that can identify potential microservices from a given monolithic application. Our method takes as input the source code of the source application in order to measure the similarities and dependencies between all of the classes in the system using their interactions and the domain terminology employed within the code. These similarity values are then used with a variant of a density-based clustering algorithm to generate a hierarchical structure of the recommended microservices while identifying potential outlier classes. We provide an empirical evaluation of our approach through different experimental settings including a comparison with existing human-designed microservices and a comparison with 5 baselines. The results show that our method succeeds in generating microservices that are overall more cohesive and that have fewer interactions in-between them with up to 0.9 of precision score when compared to human-designed microservices. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2207.14352</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2207.14352</id><created>2022-07-28</created><updated>2025-02-04</updated><authors><author><keyname>Wang</keyname><forenames>Yuxiang</forenames></author><author><keyname>Zhang</keyname><forenames>You</forenames></author><author><keyname>Duan</keyname><forenames>Zhiyao</forenames></author><author><keyname>Bocko</keyname><forenames>Mark</forenames></author></authors><title>Predicting Global HRTFs From Scanned Head Geometry Using Deep Learning   and Compact Representations</title><categories>eess.AS cs.SD</categories><comments>10 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the growing field of virtual auditory display, personalized head-related transfer functions (HRTFs) play a vital role in establishing an accurate sound image for mixed and augmented reality applications. In this work, we propose an HRTF personalization method employing convolutional neural networks (CNN) to predict a subject HRTFs for all directions from their scanned head geometry. To ease the training of the CNN models, we propose novel pre-processing methods for both the head scans and HRTF data to achieve compact representations. For the head scan, we use truncated spherical cap harmonic (SCH) coefficients to represent the pinna area, which is important in the acoustic scattering process. For the HRTF data, we use truncated spherical harmonic (SH) coefficients to represent the HRTF magnitudes and onsets. One CNN model is trained to predict the SH coefficients of the HRTF magnitudes from the SCH coefficients of the scanned ear geometry and other anthropometric measurements of the head. The other CNN model is trained to predict SH coefficients of the HRTF onsets from only the anthropometric measurements of the ear, head, and torso. Combining the magnitude and onset predictions, our method is able to predict the complete and global HRTF data. A leave-one-out validation with the log-spectral distortion (LSD) metric is used for objective evaluation. The results show a decent LSD level at both spatial \&amp; temporal dimensions compared to the ground-truth HRTFs and a lower LSD than the boundary element method (BEM) simulation of HRTFs that the database provides. The localization simulation results with an auditory model are also consistent with the objective evaluation metrics, showing the localization responses with our predicted HRTFs are significantly better than with the BEM-calculated ones. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2208.07853</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2208.07853</id><created>2022-08-16</created><updated>2025-02-05</updated><authors><author><keyname>Neto</keyname><forenames>Jeova Farias Sales Rocha</forenames></author></authors><title>Estimating Appearance Models for Image Segmentation via Tensor   Factorization</title><categories>cs.CV stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Image Segmentation is one of the core tasks in Computer Vision and solving it often depends on modeling the image appearance data via the color distributions of each it its constituent regions. Whereas many segmentation algorithms handle the appearance models dependence using alternation or implicit methods, we propose here a new approach to directly estimate them from the image without prior information on the underlying segmentation. Our method uses local high order color statistics from the image as an input to tensor factorization-based estimator for latent variable models. This approach is able to estimate models in multiregion images and automatically output the regions proportions without prior user interaction, overcoming the drawbacks from a prior attempt to this problem. We also demonstrate the performance of our proposed method in many challenging synthetic and real imaging scenarios and show that it leads to an efficient segmentation algorithm. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2209.09404</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2209.09404</id><created>2022-09-19</created><updated>2025-02-05</updated><authors><author><keyname>Chan</keyname><forenames>Timothy C. Y.</forenames></author><author><keyname>Lin</keyname><forenames>Bo</forenames></author><author><keyname>Saxe</keyname><forenames>Shoshanna</forenames></author></authors><title>Machine Learning-Augmented Optimization of Large Bilevel and Two-stage   Stochastic Programs: Application to Cycling Network Design</title><categories>math.OC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide range of decision problems can be formulated as bilevel programs with independent followers, which as a special case include two-stage stochastic programs. These problems are notoriously difficult to solve especially when a large number of followers present. Motivated by a real-world cycling infrastructure planning application, we present a general approach to solving such problems. We propose an optimization model that explicitly considers a sampled subset of followers and exploits a machine learning model to estimate the objective values of unsampled followers. We prove bounds on the optimality gap of the generated leader decision as measured by the original objective function that considers the full follower set. We then develop follower sampling algorithms to tighten the bounds and a representation learning approach to learn follower features, which are used as inputs to the embedded machine learning model. Through numerical studies, we show that our approach generates leader decisions of higher quality compared to baselines. Finally, in collaboration with the City of Toronto, we perform a real-world case study in Toronto where we solve a cycling network design problem with over one million followers. Compared to the current practice, our approach improves Toronto's cycling accessibility by 19.2%, equivalent to $18M in potential cost savings. Our approach is being used to inform the cycling infrastructure planning in Toronto and outperforms the current practice by a large margin. It can be generalized to any decision problems that are formulated as bilevel programs with independent followers. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2210.07869</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2210.07869</id><created>2022-10-14</created><updated>2025-02-05</updated><authors><author><keyname>Lichter</keyname><forenames>Moritz</forenames></author></authors><title>Witnessed Symmetric Choice and Interpretations in Fixed-Point Logic with   Counting</title><categories>cs.LO math.LO</categories><comments>46 pages, 5 figures, [v2], [v3], [v4] Corrected minor mistakes, added   figures, and some smaller improvements</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  At the core of the quest for a logic for PTime is a mismatch between algorithms making arbitrary choices and isomorphism-invariant logics. One approach to overcome this problem is witnessed symmetric choice. It allows for choices from definable orbits which are certified by definable witnessing automorphisms.   We consider the extension of fixed-point logic with counting (IFPC) with witnessed symmetric choice (IFPC+WSC) and a further extension with an interpretation operator (IFPC+WSC+I). The latter operator evaluates a subformula in the structure defined by an interpretation. This structure possibly has other automorphisms exploitable by the WSC-operator. For similar extensions of pure fixed-point logic (IFP) it is known that IFP+WSCI simulates counting which IFP+WSC fails to do. For IFPC+WSC it is unknown whether the interpretation operator increases expressiveness and thus allows studying the relation between WSC and interpretations beyond counting.   We separate IFPC+WSC from IFPC+WSCI by showing that IFPC+WSC is not closed under FO-interpretations. By the same argument, we answer an open question of Dawar and Richerby regarding non-witnessed symmetric choice in IFP. Additionally, we prove that nesting WSC-operators increases the expressiveness using the so-called CFI graphs. We show that if IFPC+WSC+I canonizes a particular class of base graphs, then it also canonizes the corresponding CFI graphs. This differs from various other logics, where CFI graphs provide difficult instances. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2301.11842</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2301.11842</id><created>2023-01-27</created><updated>2025-02-05</updated><authors><author><keyname>Gratzer</keyname><forenames>Daniel</forenames></author></authors><title>Normalization for multimodal type theory</title><categories>cs.LO</categories><comments>Extended version of the LICS paper by the same name (doi:   10.1145/3531130.3532398, technical report: arxiv:2106.01414)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We prove normalization for MTT, a general multimodal dependent type theory capable of expressing modal type theories for guarded recursion, internalized parametricity, and various other prototypical modal situations. We prove that deciding type checking and conversion in MTT can be reduced to deciding the equality of modalities in the underlying modal situation, immediately yielding a type checking algorithm for all instantiations of MTT in the literature.   This proof uses a generalization of synthetic Tait computability -- an abstract approach to gluing proofs -- to account for modalities. This extension is based on MTT itself, so that this proof also constitutes a significant case study of MTT. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2302.10184</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2302.10184</id><created>2023-02-04</created><updated>2025-02-05</updated><authors><author><keyname>Huang</keyname><forenames>Zhongzhan</forenames></author><author><keyname>Liang</keyname><forenames>Mingfu</forenames></author><author><keyname>Zhong</keyname><forenames>Shanshan</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author></authors><title>AttNS: Attention-Inspired Numerical Solving For Limited Data Scenarios</title><categories>cs.LG cs.AI cs.NA math.NA</categories><comments>Accepted by ICML'24</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the attention-inspired numerical solver (AttNS), a concise method that helps the generalization and robustness issues faced by the AI-Hybrid numerical solver in solving differential equations due to limited data. AttNS is inspired by the effectiveness of attention modules in Residual Neural Networks (ResNet) in enhancing model generalization and robustness for conventional deep learning tasks. Drawing from the dynamical system perspective of ResNet, we seamlessly incorporate attention mechanisms into the design of numerical methods tailored for the characteristics of solving differential equations. Our results on benchmarks, ranging from high-dimensional problems to chaotic systems, showcases AttNS consistently enhancing various numerical solvers without any intricate model crafting. Finally, we analyze AttNS experimentally and theoretically, demonstrating its ability to achieve strong generalization and robustness while ensuring the convergence of the solver. This includes requiring less data compared to other advanced methods to achieve comparable generalization errors and better prevention of numerical explosion issues when solving differential equations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2303.11293</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2303.11293</id><created>2023-03-13</created><updated>2025-02-05</updated><authors><author><keyname>Pran</keyname><forenames>Rakib Hassan</forenames></author></authors><title>Advancing Network Securing Strategies with Network Algorithms for   Integrated Air Defense System (IADS) Missile Batteries</title><categories>cs.SI stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recently, the Integrated Air Defense System (IADS) has become vital for the defense system as the military defense system is vital for national security. Placing Integrated Air Defense System batteries among locations to protect locations assets is a crucial problem because optimal solutions are needed for interceptor missiles to intercept attacker missiles for maximizing protection of assets across locations or places. In this research, the procedures of using network algorithms along with developing several network algorithms are going to be demonstrated to develop a model for sequential development of seven network securing strategies of placing Surface to Air Missile (SAM) batteries to maximize the protection of assets across locations (based on given asset values) by generating optimal solutions through computation to destroy maximum attacker missiles by using minimum interceptor missiles with given intercept probability. This network securing strategies can be implemented not only for Integrated Air Defense System (IADS) planning but also Counter Air (CA) planning as Integrated Air Defense System (IADS) is conducted with defensive counter air supported by attack operations in offensive counter air. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2304.05708</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2304.05708</id><created>2023-04-12</created><updated>2023-10-10</updated><authors><author><keyname>Chen</keyname><forenames>Liang</forenames></author><author><keyname>Chen</keyname><forenames>Yaru</forenames></author><author><keyname>Li</keyname><forenames>Qiuqi</forenames></author><author><keyname>Zhang</keyname><forenames>Zhiwen</forenames></author></authors><title>Stochastic Domain Decomposition Based on Variable-Separation Method</title><categories>math.NA cs.NA</categories><journal-ref>Computer Methods in Applied Mechanics and Engineering, Volume 418,   Part A, 1 January 2024, 116538</journal-ref><doi>10.1016/j.cma.2023.116538</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a new stochastic domain decomposition method for solving steady-state partial differential equations (PDEs) with random inputs. Based on the efficiency of the Variable-separation (VS) method in simulating stochastic partial differential equations (SPDEs), we extend it to stochastic algebraic systems and apply it to stochastic domain decomposition. The resulting Stochastic Domain Decomposition based on the Variable-separation method (SDD-VS) effectively addresses the ``curse of dimensionality" by leveraging the explicit representation of stochastic functions derived from physical systems. The SDD-VS method aims to obtain a separated representation of the solution for the stochastic interface problem. To enhance efficiency, an offline-online computational decomposition is introduced. In the offline phase, the affine representation of stochastic algebraic systems is obtained through the successive application of the VS method. This serves as a crucial foundation for the SDD-VS method. In the online phase, the interface unknowns of SPDEs are estimated using a quasi-optimal separated representation, enabling the construction of efficient surrogate models for subproblems. The effectiveness of the proposed method is demonstrated via the numerical results of three concrete examples. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2304.12012</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2304.12012</id><created>2023-04-24</created><authors><author><keyname>Cremonesi</keyname><forenames>Francesco</forenames></author><author><keyname>Vesin</keyname><forenames>Marc</forenames></author><author><keyname>Cansiz</keyname><forenames>Sergen</forenames></author><author><keyname>Bouillard</keyname><forenames>Yannick</forenames></author><author><keyname>Balelli</keyname><forenames>Irene</forenames></author><author><keyname>Innocenti</keyname><forenames>Lucia</forenames></author><author><keyname>Silva</keyname><forenames>Santiago</forenames></author><author><keyname>Ayed</keyname><forenames>Samy-Safwan</forenames></author><author><keyname>Taiello</keyname><forenames>Riccardo</forenames></author><author><keyname>Kameni</keyname><forenames>Laetita</forenames></author><author><keyname>Vidal</keyname><forenames>Richard</forenames></author><author><keyname>Orlhac</keyname><forenames>Fanny</forenames></author><author><keyname>Nioche</keyname><forenames>Christophe</forenames></author><author><keyname>Lapel</keyname><forenames>Nathan</forenames></author><author><keyname>Houis</keyname><forenames>Bastien</forenames></author><author><keyname>Modzelewski</keyname><forenames>Romain</forenames></author><author><keyname>Humbert</keyname><forenames>Olivier</forenames></author><author><keyname>Önen</keyname><forenames>Melek</forenames></author><author><keyname>Lorenzi</keyname><forenames>Marco</forenames></author></authors><title>Fed-BioMed: Open, Transparent and Trusted Federated Learning for   Real-world Healthcare Applications</title><categories>cs.LG cs.DC</categories><journal-ref>Federated Learning Systems, 2nd ed., Springer, 2025. Part of   Studies in Computational Intelligence, vol. 832. ISBN 978-3-031-78840-6</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The real-world implementation of federated learning is complex and requires research and development actions at the crossroad between different domains ranging from data science, to software programming, networking, and security. While today several FL libraries are proposed to data scientists and users, most of these frameworks are not designed to find seamless application in medical use-cases, due to the specific challenges and requirements of working with medical data and hospital infrastructures. Moreover, governance, design principles, and security assumptions of these frameworks are generally not clearly illustrated, thus preventing the adoption in sensitive applications. Motivated by the current technological landscape of FL in healthcare, in this document we present Fed-BioMed: a research and development initiative aiming at translating federated learning (FL) into real-world medical research applications. We describe our design space, targeted users, domain constraints, and how these factors affect our current and future software architecture. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2305.10427</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2305.10427</id><created>2023-05-17</created><authors><author><keyname>Santilli</keyname><forenames>Andrea</forenames></author><author><keyname>Severino</keyname><forenames>Silvio</forenames></author><author><keyname>Postolache</keyname><forenames>Emilian</forenames></author><author><keyname>Maiorca</keyname><forenames>Valentino</forenames></author><author><keyname>Mancusi</keyname><forenames>Michele</forenames></author><author><keyname>Marin</keyname><forenames>Riccardo</forenames></author><author><keyname>Rodolà</keyname><forenames>Emanuele</forenames></author></authors><title>Accelerating Transformer Inference for Translation via Parallel Decoding</title><categories>cs.CL cs.AI cs.LG</categories><comments>Accepted at ACL 2023 main conference</comments><doi>10.18653/v1/2023.acl-long.689</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Autoregressive decoding limits the efficiency of transformers for Machine Translation (MT). The community proposed specific network architectures and learning-based methods to solve this issue, which are expensive and require changes to the MT model, trading inference speed at the cost of the translation quality. In this paper, we propose to address the problem from the point of view of decoding algorithms, as a less explored but rather compelling direction. We propose to reframe the standard greedy autoregressive decoding of MT with a parallel formulation leveraging Jacobi and Gauss-Seidel fixed-point iteration methods for fast inference. This formulation allows to speed up existing models without training or modifications while retaining translation quality. We present three parallel decoding algorithms and test them on different languages and models showing how the parallelization introduces a speedup up to 38% w.r.t. the standard autoregressive decoding and nearly 2x when scaling the method on parallel resources. Finally, we introduce a decoding dependency graph visualizer (DDGviz) that let us see how the model has learned the conditional dependence between tokens and inspect the decoding procedure. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2305.14883</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2305.14883</id><created>2023-05-24</created><updated>2025-02-05</updated><authors><author><keyname>Ball</keyname><forenames>Simeon</forenames></author><author><keyname>Vilar</keyname><forenames>Ricard</forenames></author></authors><title>Classical and quantum cyclic redundancy check codes</title><categories>quant-ph cs.IT math.IT</categories><msc-class>94B20, 94B15, 94B35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that certain classical cyclic redundancy check codes can be used for classical error correction and not just classical error detection. We extend the idea of classical cyclic redundancy check codes to quantum cyclic redundancy check codes. This allows us to construct quantum stabiliser codes which can correct burst errors where the burst length attains the quantum Reiger bound. We then consider a certain family of quantum cyclic redundancy check codes for which we present a fast linear time decoding algorithm. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2306.07186</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2306.07186</id><created>2023-06-12</created><authors><author><keyname>Ge</keyname><forenames>Wenxuan</forenames></author><author><keyname>Yang</keyname><forenames>Xubing</forenames></author><author><keyname>Zhang</keyname><forenames>Li</forenames></author></authors><title>CD-CTFM: A Lightweight CNN-Transformer Network for Remote Sensing Cloud   Detection Fusing Multiscale Features</title><categories>cs.CV cs.LG eess.IV</categories><comments>IEEE Journal of Selected Topics in Applied Earth Observations and   Remote Sensing, 2024</comments><doi>10.1109/JSTARS.2024.3361933</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clouds in remote sensing images inevitably affect information extraction, which hinder the following analysis of satellite images. Hence, cloud detection is a necessary preprocessing procedure. However, the existing methods have numerous calculations and parameters. In this letter, a lightweight CNN-Transformer network, CD-CTFM, is proposed to solve the problem. CD-CTFM is based on encoder-decoder architecture and incorporates the attention mechanism. In the decoder part, we utilize a lightweight network combing CNN and Transformer as backbone, which is conducive to extract local and global features simultaneously. Moreover, a lightweight feature pyramid module is designed to fuse multiscale features with contextual information. In the decoder part, we integrate a lightweight channel-spatial attention module into each skip connection between encoder and decoder, extracting low-level features while suppressing irrelevant information without introducing many parameters. Finally, the proposed model is evaluated on two cloud datasets, 38-Cloud and MODIS. The results demonstrate that CD-CTFM achieves comparable accuracy as the state-of-art methods. At the same time, CD-CTFM outperforms state-of-art methods in terms of efficiency. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2306.09852</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2306.09852</id><created>2023-06-16</created><updated>2025-02-05</updated><authors><author><keyname>Romero</keyname><forenames>Angel</forenames></author><author><keyname>Aljalbout</keyname><forenames>Elie</forenames></author><author><keyname>Song</keyname><forenames>Yunlong</forenames></author><author><keyname>Scaramuzza</keyname><forenames>Davide</forenames></author></authors><title>Actor-Critic Model Predictive Control: Differentiable Optimization meets   Reinforcement Learning</title><categories>cs.RO</categories><comments>18 pages, 12 figures, extension</comments><acm-class>I.2.9; I.2.6; G.1.6; I.2.8; C.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An open research question in robotics is how to combine the benefits of model-free reinforcement learning (RL) - known for its strong task performance and flexibility in optimizing general reward formulations - with the robustness and online replanning capabilities of model predictive control (MPC). This paper provides an answer by introducing a new framework called Actor-Critic Model Predictive Control. The key idea is to embed a differentiable MPC within an actor-critic RL framework. This integration allows for short-term predictive optimization of control actions through MPC, while leveraging RL for end-to-end learning and exploration over longer horizons. Through various ablation studies, we expose the benefits of the proposed approach: it achieves better out-of-distribution behaviour, better robustness to changes in the dynamics and improved sample efficiency. Additionally, we conduct an empirical analysis that reveals a relationship between the critic's learned value function and the cost function of the differentiable MPC, providing a deeper understanding of the interplay between the critic's value and the MPC cost functions. Finally, we validate our method in a drone racing task on different tracks, in both simulation and the real world. Our method achieves the same superhuman performance as state-of-the-art model-free RL, showcasing speeds of up to 21 m/s. We show that the proposed architecture can achieve real-time control performance, learn complex behaviors via trial and error, and retain the predictive properties of the MPC to better handle out-of-distribution behavior. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2306.15369</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2306.15369</id><created>2023-06-27</created><authors><author><keyname>Awais</keyname><forenames>Ch Muhammad</forenames></author><author><keyname>Gu</keyname><forenames>Wei</forenames></author><author><keyname>Dlamini</keyname><forenames>Gcinizwe</forenames></author><author><keyname>Kholmatova</keyname><forenames>Zamira</forenames></author><author><keyname>Succi</keyname><forenames>Giancarlo</forenames></author></authors><title>A Meta-analytical Comparison of Naive Bayes and Random Forest for   Software Defect Prediction</title><categories>cs.SE cs.LG</categories><comments>11 pages, 8 figures, Conference Paper</comments><journal-ref>Intelligent Systems Design and Applications. ISDA 2022. Lecture   Notes in Networks and Systems, vol 716</journal-ref><doi>10.1007/978-3-031-35501-1_14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Is there a statistical difference between Naive Bayes and Random Forest in terms of recall, f-measure, and precision for predicting software defects? By utilizing systematic literature review and meta-analysis, we are answering this question. We conducted a systematic literature review by establishing criteria to search and choose papers, resulting in five studies. After that, using the meta-data and forest-plots of five chosen papers, we conducted a meta-analysis to compare the two models. The results have shown that there is no significant statistical evidence that Naive Bayes perform differently from Random Forest in terms of recall, f-measure, and precision. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2308.02885</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2308.02885</id><created>2023-08-05</created><updated>2025-02-05</updated><authors><author><keyname>Aikata</keyname><forenames>Aikata</forenames></author><author><keyname>Mert</keyname><forenames>Ahmet Can</forenames></author><author><keyname>Kwon</keyname><forenames>Sunmin</forenames></author><author><keyname>Deryabin</keyname><forenames>Maxim</forenames></author><author><keyname>Roy</keyname><forenames>Sujoy Sinha</forenames></author></authors><title>REED: Chiplet-Based Accelerator for Fully Homomorphic Encryption</title><categories>cs.CR</categories><journal-ref>https://ches.iacr.org/2025/papersubmission.php</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Fully Homomorphic Encryption (FHE) enables privacy-preserving computation and has many applications. However, its practical implementation faces massive computation and memory overheads. To address this bottleneck, several Application-Specific Integrated Circuit (ASIC) FHE accelerators have been proposed. All these prior works put every component needed for FHE onto one chip (monolithic), hence offering high performance. However, they suffer from practical problems associated with large-scale chip design, such as inflexibility, low yield, and high manufacturing cost.   In this paper, we present the first-of-its-kind multi-chiplet-based FHE accelerator `REED' for overcoming the limitations of prior monolithic designs. To utilize the advantages of multi-chiplet structures while matching the performance of larger monolithic systems, we propose and implement several novel strategies in the context of FHE. These include a scalable chiplet design approach, an effective framework for workload distribution, a custom inter-chiplet communication strategy, and advanced pipelined Number Theoretic Transform and automorphism design to enhance performance.   Experimental results demonstrate that REED 2.5D microprocessor consumes 96.7 mm$^2$ chip area, 49.4 W average power in 7nm technology. It could achieve a remarkable speedup of up to 2,991x compared to a CPU (24-core 2xIntel X5690) and offer 1.9x better performance, along with a 50% reduction in development costs when compared to state-of-the-art ASIC FHE accelerators. Furthermore, our work presents the first instance of benchmarking an encrypted deep neural network (DNN) training. Overall, the REED architecture design offers a highly effective solution for accelerating FHE, thereby significantly advancing the practicality and deployability of FHE in real-world applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2308.14536</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2308.14536</id><created>2023-08-28</created><updated>2025-02-04</updated><authors><author><keyname>Peng</keyname><forenames>Linkai</forenames></author><author><keyname>Nuchged</keyname><forenames>Baorian</forenames></author><author><keyname>Gao</keyname><forenames>Yingming</forenames></author></authors><title>Spoken Language Intelligence of Large Language Models for Language   Learning</title><categories>cs.CL cs.AI cs.LG cs.SD eess.AS</categories><comments>28 pages, 7 figures, Preprint Feb 04, 2025 update: Add Deepseek R1   performance</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People have long hoped for a conversational system that can assist in real-life situations, and recent progress on large language models (LLMs) is bringing this idea closer to reality. While LLMs are often impressive in performance, their efficacy in real-world scenarios that demand expert knowledge remains unclear. LLMs are believed to hold the most potential and value in education, especially in the development of Artificial intelligence (AI) based virtual teachers capable of facilitating language learning. Our focus is centered on evaluating the efficacy of LLMs in the realm of education, specifically in the areas of spoken language learning which encompass phonetics, phonology, and second language acquisition. We introduce a new multiple-choice question dataset to evaluate the effectiveness of LLMs in the aforementioned scenarios, including understanding and application of spoken language knowledge. In addition, we investigate the influence of various prompting techniques such as zero- and few-shot method (prepending the question with question-answer exemplars), chain-of-thought (CoT, think step-by-step), in-domain exampler and external tools (Google, Wikipedia). We conducted large-scale evaluation on popular LLMs (20 distinct models) using these methods. We achieved significant performance improvements compared to the zero-shot baseline in the practical questions reasoning (GPT-3.5, 49.1% -&gt; 63.1%; LLaMA2-70B-Chat, 42.2% -&gt; 48.6%). We found that models of different sizes have good understanding of concepts in phonetics, phonology, and second language acquisition, but show limitations in reasoning for real-world problems. Additionally, we also explore preliminary findings on conversational communication. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2309.02705</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2309.02705</id><created>2023-09-06</created><updated>2025-02-04</updated><authors><author><keyname>Kumar</keyname><forenames>Aounon</forenames></author><author><keyname>Agarwal</keyname><forenames>Chirag</forenames></author><author><keyname>Srinivas</keyname><forenames>Suraj</forenames></author><author><keyname>Li</keyname><forenames>Aaron Jiaxun</forenames></author><author><keyname>Feizi</keyname><forenames>Soheil</forenames></author><author><keyname>Lakkaraju</keyname><forenames>Himabindu</forenames></author></authors><title>Certifying LLM Safety against Adversarial Prompting</title><categories>cs.CL cs.AI cs.CR cs.LG</categories><comments>Accepted at COLM 2024: https://openreview.net/forum?id=9Ik05cycLq</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models (LLMs) are vulnerable to adversarial attacks that add malicious tokens to an input prompt to bypass the safety guardrails of an LLM and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework for defending against adversarial prompts with certifiable safety guarantees. Given a prompt, our procedure erases tokens individually and inspects the resulting subsequences using a safety filter. Our safety certificate guarantees that harmful prompts are not mislabeled as safe due to an adversarial attack up to a certain size. We implement the safety filter in two ways, using Llama 2 and DistilBERT, and compare the performance of erase-and-check for the two cases. We defend against three attack modes: i) adversarial suffix, where an adversarial sequence is appended at the end of a harmful prompt; ii) adversarial insertion, where the adversarial sequence is inserted anywhere in the middle of the prompt; and iii) adversarial infusion, where adversarial tokens are inserted at arbitrary positions in the prompt, not necessarily as a contiguous block. Our experimental results demonstrate that this procedure can obtain strong certified safety guarantees on harmful prompts while maintaining good empirical performance on safe prompts. Additionally, we propose three efficient empirical defenses: i) RandEC, a randomized subsampling version of erase-and-check; ii) GreedyEC, which greedily erases tokens that maximize the softmax score of the harmful class; and iii) GradEC, which uses gradient information to optimize tokens to erase. We demonstrate their effectiveness against adversarial prompts generated by the Greedy Coordinate Gradient (GCG) attack algorithm. The code for our experiments is available at https://github.com/aounon/certified-llm-safety. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2309.04843</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2309.04843</id><created>2023-09-09</created><updated>2025-02-04</updated><authors><author><keyname>Wang</keyname><forenames>Zixing</forenames></author><author><keyname>Qureshi</keyname><forenames>Ahmed H.</forenames></author></authors><title>DeRi-IGP: Learning to Manipulate Rigid Objects Using Deformable Objects   via Iterative Grasp-Pull</title><categories>cs.RO</categories><comments>This paper is accepted by RA-L</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robotic manipulation of rigid objects via deformable linear objects (DLO) such as ropes is an emerging field of research with applications in various rigid object transportation tasks. A few methods that exist in this field suffer from limited robot action and operational space, poor generalization ability, and expensive model-based development. To address these challenges, we propose a universally applicable moving primitive called Iterative Grasp-Pull (IGP). We also introduce a novel vision-based neural policy that learns to parameterize the IGP primitive to manipulate DLO and transport their attached rigid objects to the desired goal locations. Additionally, our decentralized algorithm design allows collaboration among multiple agents to manipulate rigid objects using DLO. We evaluated the effectiveness of our approach in both simulated and real-world environments for a variety of soft-rigid body manipulation tasks. In the real world, we also demonstrate the effectiveness of our decentralized approach through human-robot collaborative transportation of rigid objects to given goal locations. We also showcase the large operational space of IGP primitive by solving distant object acquisition tasks. Lastly, we compared our approach with several model-based and learning-based baseline methods. The results indicate that our method surpasses other approaches by a significant margin. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2309.06072</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2309.06072</id><created>2023-09-12</created><updated>2025-02-05</updated><authors><author><keyname>Duraj</keyname><forenames>Lech</forenames></author><author><keyname>Kang</keyname><forenames>Ross J.</forenames></author><author><keyname>La</keyname><forenames>Hoang</forenames></author><author><keyname>Narboni</keyname><forenames>Jonathan</forenames></author><author><keyname>Pokrývka</keyname><forenames>Filip</forenames></author><author><keyname>Rambaud</keyname><forenames>Clément</forenames></author><author><keyname>Reinald</keyname><forenames>Amadeus</forenames></author></authors><title>The $\chi$-binding function of $d$-directional segment graphs</title><categories>math.CO cs.DM</categories><comments>11 pages, 3 figures; v2 includes corrections for referee comments</comments><msc-class>05C15, 05C62, 05C17</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given a positive integer $d$, the class $d$-DIR is defined as all those intersection graphs formed from a finite collection of line segments in ${\mathbb R}^2$ having at most $d$ slopes. Since each slope induces an interval graph, it easily follows for every $G$ in $d$-DIR with clique number at most $\omega$ that the chromatic number $\chi(G)$ of $G$ is at most $d\omega$. We show for every even value of $\omega$ how to construct a graph in $d$-DIR that meets this bound exactly. This partially confirms a conjecture of Bhattacharya, Dvo\v{r}\'ak and Noorizadeh. Furthermore, we show that the $\chi$-binding function of $d$-DIR is $\omega \mapsto d\omega$ for $\omega$ even and $\omega \mapsto d(\omega-1)+1$ for $\omega$ odd. This extends an earlier result by Kostochka and Ne\v{s}et\v{r}il, which treated the special case $d=2$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2309.10523</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2309.10523</id><created>2023-09-19</created><updated>2025-02-05</updated><authors><author><keyname>Zhou</keyname><forenames>Tao</forenames></author><author><keyname>Zhang</keyname><forenames>Yizhe</forenames></author><author><keyname>Chen</keyname><forenames>Geng</forenames></author><author><keyname>Zhou</keyname><forenames>Yi</forenames></author><author><keyname>Wu</keyname><forenames>Ye</forenames></author><author><keyname>Fan</keyname><forenames>Deng-Ping</forenames></author></authors><title>Edge-aware Feature Aggregation Network for Polyp Segmentation</title><categories>cs.CV</categories><comments>Accepted by Machine Intelligence Research (2025)</comments><journal-ref>Machine Intelligence Research. Volume 22, pages 101-116 (2025)</journal-ref><doi>10.1007/s11633-023-1479-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Precise polyp segmentation is vital for the early diagnosis and prevention of colorectal cancer (CRC) in clinical practice. However, due to scale variation and blurry polyp boundaries, it is still a challenging task to achieve satisfactory segmentation performance with different scales and shapes. In this study, we present a novel Edge-aware Feature Aggregation Network (EFA-Net) for polyp segmentation, which can fully make use of cross-level and multi-scale features to enhance the performance of polyp segmentation. Specifically, we first present an Edge-aware Guidance Module (EGM) to combine the low-level features with the high-level features to learn an edge-enhanced feature, which is incorporated into each decoder unit using a layer-by-layer strategy. Besides, a Scale-aware Convolution Module (SCM) is proposed to learn scale-aware features by using dilated convolutions with different ratios, in order to effectively deal with scale variation. Further, a Cross-level Fusion Module (CFM) is proposed to effectively integrate the cross-level features, which can exploit the local and global contextual information. Finally, the outputs of CFMs are adaptively weighted by using the learned edge-aware feature, which are then used to produce multiple side-out segmentation maps. Experimental results on five widely adopted colonoscopy datasets show that our EFA-Net outperforms state-of-the-art polyp segmentation methods in terms of generalization and effectiveness. Our implementation code and segmentation maps will be publicly at https://github.com/taozh2017/EFANet. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2309.13994</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2309.13994</id><created>2023-09-25</created><authors><author><keyname>Poncelet</keyname><forenames>Jakob</forenames></author><author><keyname>Van hamme</keyname><forenames>Hugo</forenames></author></authors><title>Unsupervised Accent Adaptation Through Masked Language Model Correction   Of Discrete Self-Supervised Speech Units</title><categories>eess.AS cs.SD</categories><comments>Submitted to ICASSP2024</comments><journal-ref>2024 IEEE International Conference on Acoustics, Speech and Signal   Processing (ICASSP), pp. 10236-10240</journal-ref><doi>10.1109/ICASSP48485.2024.10446344</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Self-supervised pre-trained speech models have strongly improved speech recognition, yet they are still sensitive to domain shifts and accented or atypical speech. Many of these models rely on quantisation or clustering to learn discrete acoustic units. We propose to correct the discovered discrete units for accented speech back to a standard pronunciation in an unsupervised manner. A masked language model is trained on discrete units from a standard accent and iteratively corrects an accented token sequence by masking unexpected cluster sequences and predicting their common variant. Small accent adapter blocks are inserted in the pre-trained model and fine-tuned by predicting the corrected clusters, which leads to an increased robustness of the pre-trained model towards a target accent, and this without supervision. We are able to improve a state-of-the-art HuBERT Large model on a downstream accented speech recognition task by altering the training regime with the proposed method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2309.16110</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2309.16110</id><created>2023-09-27</created><updated>2025-02-05</updated><authors><author><keyname>Yang</keyname><forenames>Zheyuan</forenames></author><author><keyname>Liu</keyname><forenames>Yibo</forenames></author><author><keyname>Wu</keyname><forenames>Guile</forenames></author><author><keyname>Cao</keyname><forenames>Tongtong</forenames></author><author><keyname>Ren</keyname><forenames>Yuan</forenames></author><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Liu</keyname><forenames>Bingbing</forenames></author></authors><title>Learning Effective NeRFs and SDFs Representations with 3D Generative   Adversarial Networks for 3D Object Generation</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a solution for 3D object generation of ICCV 2023 OmniObject3D Challenge. In recent years, 3D object generation has made great process and achieved promising results, but it remains a challenging task due to the difficulty of generating complex, textured, and high-fidelity results. To resolve this problem, we study learning effective NeRFs and SDFs representations with 3D Generative Adversarial Networks (GANs) for 3D object generation. Specifically, inspired by recent works, we use the efficient geometry-aware 3D GANs as the backbone incorporating with label embedding and color mapping, which enables to train the model on different taxonomies simultaneously. Then, through a decoder, we aggregate the resulting features to generate Neural Radiance Fields (NeRFs) based representations for rendering high-fidelity synthetic images. Meanwhile, we optimize Signed Distance Functions (SDFs) to effectively represent objects with 3D meshes. Besides, we observe that this model can be effectively trained with only a few images of each object from a variety of classes, instead of using a great number of images per object or training one model per class. With this pipeline, we can optimize an effective model for 3D object generation. This solution is among the top 3 in the ICCV 2023 OmniObject3D Challenge. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.13989</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.13989</id><created>2023-10-21</created><updated>2025-02-05</updated><authors><author><keyname>Rehman</keyname><forenames>Asid Ur</forenames></author><author><keyname>Glenis</keyname><forenames>Vassilis</forenames></author><author><keyname>Lewis</keyname><forenames>Elizabeth</forenames></author><author><keyname>Kilsby</keyname><forenames>Chris</forenames></author></authors><title>Multi-objective Optimisation Framework for Blue-Green Infrastructure   Placement Using Detailed Flood Model</title><categories>cs.CE</categories><comments>The paper has been published in the Journal of Hydrology</comments><journal-ref>Journal of Hydrology, 638, 131571 (2024)</journal-ref><doi>10.1016/j.jhydrol.2024.131571</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This study aims to find a cost-effective Blue-Green Infrastructure placement scheme by developing an improved approach called the Cost Optimization Framework for Implementing blue-Green infrastructure (CONFIGURE). The optimisation framework integrates a detailed hydrodynamic flood simulation model with a multi-objective optimisation algorithm (Non-dominated Sorting Genetic Algorithm II). The use of a high-resolution flood simulation model ensures the explicit representation of BGI and other land use features to simulate flow pathways and surface flood risk accurately, while the optimisation algorithm guarantees achieving the best cost-benefit trade-offs for given BGI options. The current study uses the advanced CityCAT hydrodynamic flood model to evaluate the efficiency of the optimisation framework and the impact of location and size of permeable interventions on the optimisation process and subsequent cost-benefit trade-offs. This is achieved by dividing permeable surface areas into intervention zones of varying size and quantity. Furthermore, rainstorm events with 100-year and 30-year return periods are analysed to identify any common optimal solutions for different rainfall intensities. Depending on the number of intervention locations, the automated framework reliably achieves optimal BGI implementation solutions in a fraction of the time required to find the best solutions by trialling all possible options. Designing and optimising interventions with smaller sizes but many permeable zones saves a good fraction of investment. However, such a design scheme requires more computational time to find optimal options. Furthermore, the optimal spatial configuration of BGI varies with different rainstorm severities, suggesting a need for careful selection of the rainstorm return period. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.14483</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.14483</id><created>2023-10-22</created><updated>2025-02-04</updated><authors><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Shen</keyname><forenames>Yanzhen</forenames></author><author><keyname>Kang</keyname><forenames>SeongKu</forenames></author><author><keyname>Chen</keyname><forenames>Xiusi</forenames></author><author><keyname>Jin</keyname><forenames>Bowen</forenames></author><author><keyname>Han</keyname><forenames>Jiawei</forenames></author></authors><title>Chain-of-Factors Paper-Reviewer Matching</title><categories>cs.IR cs.CL cs.DL cs.LG</categories><comments>10 pages; Accepted to WWW 2025 (Code:   https://github.com/yuzhimanhua/CoF)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid increase in paper submissions to academic conferences, the need for automated and accurate paper-reviewer matching is more critical than ever. Previous efforts in this area have considered various factors to assess the relevance of a reviewer's expertise to a paper, such as the semantic similarity, shared topics, and citation connections between the paper and the reviewer's previous works. However, most of these studies focus on only one factor, resulting in an incomplete evaluation of the paper-reviewer relevance. To address this issue, we propose a unified model for paper-reviewer matching that jointly considers semantic, topic, and citation factors. To be specific, during training, we instruction-tune a contextualized language model shared across all factors to capture their commonalities and characteristics; during inference, we chain the three factors to enable step-by-step, coarse-to-fine search for qualified reviewers given a submission. Experiments on four datasets (one of which is newly contributed by us) spanning various fields such as machine learning, computer vision, information retrieval, and data mining consistently demonstrate the effectiveness of our proposed Chain-of-Factors model in comparison with state-of-the-art paper-reviewer matching methods and scientific pre-trained language models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.15318</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.15318</id><created>2023-10-23</created><updated>2025-02-04</updated><authors><author><keyname>Ma</keyname><forenames>Yihong</forenames></author><author><keyname>Yan</keyname><forenames>Ning</forenames></author><author><keyname>Li</keyname><forenames>Jiayu</forenames></author><author><keyname>Mortazavi</keyname><forenames>Masood</forenames></author><author><keyname>Chawla</keyname><forenames>Nitesh V.</forenames></author></authors><title>HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained   Heterogeneous Graph Neural Networks</title><categories>cs.LG cs.AI</categories><comments>Published in The ACM Web Conference 2024 (WWW '24)</comments><doi>10.1145/3589334.3645685</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Graphs have emerged as a natural choice to represent and analyze the intricate patterns and rich information of the Web, enabling applications such as online page classification and social recommendation. The prevailing "pre-train, fine-tune" paradigm has been widely adopted in graph machine learning tasks, particularly in scenarios with limited labeled nodes. However, this approach often exhibits a misalignment between the training objectives of pretext tasks and those of downstream tasks. This gap can result in the "negative transfer" problem, wherein the knowledge gained from pre-training adversely affects performance in the downstream tasks. The surge in prompt-based learning within Natural Language Processing (NLP) suggests the potential of adapting a "pre-train, prompt" paradigm to graphs as an alternative. However, existing graph prompting techniques are tailored to homogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To bridge this gap, we propose HetGPT, a general post-training prompting framework to improve the predictive performance of pre-trained heterogeneous graph neural networks (HGNNs). The key is the design of a novel prompting function that integrates a virtual class prompt and a heterogeneous feature prompt, with the aim to reformulate downstream tasks to mirror pretext tasks. Moreover, HetGPT introduces a multi-view neighborhood aggregation mechanism, capturing the complex neighborhood structure in heterogeneous graphs. Extensive experiments on three benchmark datasets demonstrate HetGPT's capability to enhance the performance of state-of-the-art HGNNs on semi-supervised node classification. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.16613</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.16613</id><created>2023-10-25</created><updated>2025-02-05</updated><authors><author><keyname>Wu</keyname><forenames>Yixin</forenames></author><author><keyname>Yu</keyname><forenames>Ning</forenames></author><author><keyname>Backes</keyname><forenames>Michael</forenames></author><author><keyname>Shen</keyname><forenames>Yun</forenames></author><author><keyname>Zhang</keyname><forenames>Yang</forenames></author></authors><title>On the Proactive Generation of Unsafe Images From Text-To-Image Models   Using Benign Prompts</title><categories>cs.CR</categories><comments>To Appear in the 34th USENIX Security Symposium, August 13-15, 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Malicious or manipulated prompts are known to exploit text-to-image models to generate unsafe images. Existing studies, however, focus on the passive exploitation of such harmful capabilities. In this paper, we investigate the proactive generation of unsafe images from benign prompts (e.g., a photo of a cat) through maliciously modified text-to-image models. Our preliminary investigation demonstrates that poisoning attacks are a viable method to achieve this goal but uncovers significant side effects, where unintended spread to non-targeted prompts compromises attack stealthiness. Root cause analysis identifies conceptual similarity as an important contributing factor to these side effects. To address this, we propose a stealthy poisoning attack method that balances covertness and performance. Our findings highlight the potential risks of adopting text-to-image models in real-world scenarios, thereby calling for future research and safety measures in this space. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2311.01487</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2311.01487</id><created>2023-11-02</created><updated>2025-02-05</updated><authors><author><keyname>Du</keyname><forenames>Yifan</forenames></author><author><keyname>Guo</keyname><forenames>Hangyu</forenames></author><author><keyname>Zhou</keyname><forenames>Kun</forenames></author><author><keyname>Zhao</keyname><forenames>Wayne Xin</forenames></author><author><keyname>Wang</keyname><forenames>Jinpeng</forenames></author><author><keyname>Wang</keyname><forenames>Chuyuan</forenames></author><author><keyname>Cai</keyname><forenames>Mingchen</forenames></author><author><keyname>Song</keyname><forenames>Ruihua</forenames></author><author><keyname>Wen</keyname><forenames>Ji-Rong</forenames></author></authors><title>What Makes for Good Visual Instructions? Synthesizing Complex Visual   Reasoning Instructions for Visual Instruction Tuning</title><categories>cs.CV cs.CL</categories><comments>Accepted by COLING2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual instruction tuning is crucial for enhancing the zero-shot generalization capability of Multi-modal Large Language Models (MLLMs). In this paper, we aim to investigate a fundamental question: ''what makes for good visual instructions''. Through a comprehensive empirical study, we find that instructions focusing on complex visual reasoning tasks are particularly effective in improving the performance of MLLMs, with results correlating to instruction complexity. Based on this insight, we develop a systematic approach to automatically create high-quality complex visual reasoning instructions. Our approach employs a synthesize-complicate-reformulate paradigm, leveraging multiple stages to gradually increase the complexity of the instructions while guaranteeing quality. Based on this approach, we create the ComVint dataset with 32K examples, and fine-tune four MLLMs on it. Experimental results consistently demonstrate the enhanced performance of all compared MLLMs, such as a 27.86% and 27.60% improvement for LLaVA on MME-Perception and MME-Cognition, respectively. Our code and data are publicly available at the link: https://github.com/RUCAIBox/ComVint. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2311.02636</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2311.02636</id><created>2023-11-05</created><updated>2025-02-05</updated><authors><author><keyname>Feibish</keyname><forenames>Shir Landau</forenames></author><author><keyname>Liu</keyname><forenames>Zaoxing</forenames></author><author><keyname>Rexford</keyname><forenames>Jennifer</forenames></author></authors><title>Compact Data Structures for Network Telemetry</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collecting and analyzing of network traffic data (network telemetry) plays a critical role in managing modern networks. Network administrators analyze their traffic to troubleshoot performance and reliability problems, and to detect and block cyberattacks. However, conventional traffic-measurement techniques offer limited visibility into network conditions and rely on offline analysis. Fortunately, network devices -- such as switches and network interface cards -- are increasingly programmable at the packet level, enabling flexible analysis of the traffic in place, as the packets fly by. However, to operate at high speed, these devices have limited memory and computational resources, leading to trade-offs between accuracy and overhead. In response, an exciting research area emerged, bringing ideas from compact data structures and streaming algorithms to bear on important networking telemetry applications and the unique characteristics of high-speed network devices. In this paper, we review the research on compact data structures for network telemetry and discuss promising directions for future research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2311.09051</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2311.09051</id><created>2023-11-15</created><updated>2025-02-04</updated><authors><author><keyname>Chen</keyname><forenames>Long</forenames></author><author><keyname>Huang</keyname><forenames>Xuehai</forenames></author><author><keyname>Zhang</keyname><forenames>Chao</forenames></author></authors><title>Distributional Finite Element curl div Complexes and Application to Quad   Curl Problems</title><categories>math.NA cs.NA</categories><comments>28 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper addresses the challenge of constructing finite element curl div complexes in three dimensions. Tangential-normal continuity is introduced in order to develop distributional finite element curl div complexes. The spaces constructed are applied to discretize the quad curl problem, demonstrating optimal order of convergence. Furthermore, a hybridization technique is proposed, demonstrating its equivalence to nonconforming finite elements and weak Galerkin methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2311.12864</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2311.12864</id><created>2023-10-26</created><updated>2025-02-05</updated><authors><author><keyname>Zou</keyname><forenames>Ding</forenames></author><author><keyname>Lu</keyname><forenames>Wei</forenames></author><author><keyname>Zhu</keyname><forenames>Zhibo</forenames></author><author><keyname>Lu</keyname><forenames>Xingyu</forenames></author><author><keyname>Zhou</keyname><forenames>Jun</forenames></author><author><keyname>Wang</keyname><forenames>Xiaojin</forenames></author><author><keyname>Liu</keyname><forenames>Kangyu</forenames></author><author><keyname>Wang</keyname><forenames>Haiqing</forenames></author><author><keyname>Wang</keyname><forenames>Kefan</forenames></author><author><keyname>Sun</keyname><forenames>Renen</forenames></author></authors><title>OptScaler: A Collaborative Framework for Robust Autoscaling in the Cloud</title><categories>math.OC cs.LG</categories><comments>Proceedings of the VLDB Endowment, Volume 17, Issue 12 Pages 4090 -   4103</comments><doi>10.14778/3685800.3685829</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autoscaling is a critical mechanism in cloud computing, enabling the autonomous adjustment of computing resources in response to dynamic workloads. This is particularly valuable for co-located, long-running applications with diverse workload patterns. The primary objective of autoscaling is to regulate resource utilization at a desired level, effectively balancing the need for resource optimization with the fulfillment of Service Level Objectives (SLOs). Many existing proactive autoscaling frameworks may encounter prediction deviations arising from the frequent fluctuations of cloud workloads. Reactive frameworks, on the other hand, rely on realtime system feedback, but their hysteretic nature could lead to violations of stringent SLOs. Hybrid frameworks, while prevalent, often feature independently functioning proactive and reactive modules, potentially leading to incompatibility and undermining the overall decision-making efficacy. In addressing these challenges, we propose OptScaler, a collaborative autoscaling framework that integrates proactive and reactive modules through an optimization module. The proactive module delivers reliable future workload predictions to the optimization module, while the reactive module offers a self-tuning estimator for real-time updates. By embedding a Model Predictive Control (MPC) mechanism and chance constraints into the optimization module, we further enhance its robustness. Numerical results have demonstrated the superiority of our workload prediction model and the collaborative framework, leading to over a 36% reduction in SLO violations compared to prevalent reactive, proactive, or hybrid autoscalers. Notably, OptScaler has been successfully deployed at Alipay, providing autoscaling support for the world-leading payment platform. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2311.18022</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2311.18022</id><created>2023-11-29</created><updated>2025-02-04</updated><authors><author><keyname>Milkert</keyname><forenames>Max</forenames></author><author><keyname>Hyde</keyname><forenames>David</forenames></author><author><keyname>Laine</keyname><forenames>Forrest</forenames></author></authors><title>Compelling ReLU Networks to Exhibit Exponentially Many Linear Regions at   Initialization and During Training</title><categories>cs.LG cs.AI</categories><comments>24 pages, 16 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In a neural network with ReLU activations, the number of piecewise linear regions in the output can grow exponentially with depth. However, this is highly unlikely to happen when the initial parameters are sampled randomly, which therefore often leads to the use of networks that are unnecessarily large. To address this problem, we introduce a novel parameterization of the network that restricts its weights so that a depth $d$ network produces exactly $2^d$ linear regions at initialization and maintains those regions throughout training under the parameterization. This approach allows us to learn approximations of convex, one dimensional functions that are several orders of magnitude more accurate than their randomly initialized counterparts. We further demonstrate how to extend our approach to multidimensional and non-convex functions, allowing it to replace the dense layers in other networks; preliminary improvements are shown for image classification on CIFAR-10 and ImageNet. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.00326</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.00326</id><created>2023-11-30</created><updated>2025-02-05</updated><authors><author><keyname>Qiang</keyname><forenames>Zhangcheng</forenames></author><author><keyname>Wang</keyname><forenames>Weiqing</forenames></author><author><keyname>Taylor</keyname><forenames>Kerry</forenames></author></authors><title>Agent-OM: Leveraging LLM Agents for Ontology Matching</title><categories>cs.AI cs.CL cs.IR</categories><comments>19 pages, 12 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.00860</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.00860</id><created>2023-12-01</created><updated>2025-02-05</updated><authors><author><keyname>Cen</keyname><forenames>Jiazhong</forenames></author><author><keyname>Fang</keyname><forenames>Jiemin</forenames></author><author><keyname>Yang</keyname><forenames>Chen</forenames></author><author><keyname>Xie</keyname><forenames>Lingxi</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaopeng</forenames></author><author><keyname>Shen</keyname><forenames>Wei</forenames></author><author><keyname>Tian</keyname><forenames>Qi</forenames></author></authors><title>Segment Any 3D Gaussians</title><categories>cs.CV</categories><comments>AAAI-25. Project page: https://jumpat.github.io/SAGA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents SAGA (Segment Any 3D GAussians), a highly efficient 3D promptable segmentation method based on 3D Gaussian Splatting (3D-GS). Given 2D visual prompts as input, SAGA can segment the corresponding 3D target represented by 3D Gaussians within 4 ms. This is achieved by attaching an scale-gated affinity feature to each 3D Gaussian to endow it a new property towards multi-granularity segmentation. Specifically, a scale-aware contrastive training strategy is proposed for the scale-gated affinity feature learning. It 1) distills the segmentation capability of the Segment Anything Model (SAM) from 2D masks into the affinity features and 2) employs a soft scale gate mechanism to deal with multi-granularity ambiguity in 3D segmentation through adjusting the magnitude of each feature channel according to a specified 3D physical scale. Evaluations demonstrate that SAGA achieves real-time multi-granularity segmentation with quality comparable to state-of-the-art methods. As one of the first methods addressing promptable segmentation in 3D-GS, the simplicity and effectiveness of SAGA pave the way for future advancements in this field. Our code will be released. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.02400</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.02400</id><created>2023-12-04</created><updated>2025-02-05</updated><authors><author><keyname>Chilukoti</keyname><forenames>Sai Venkatesh</forenames></author><author><keyname>Hossen</keyname><forenames>Md Imran</forenames></author><author><keyname>Shan</keyname><forenames>Liqun</forenames></author><author><keyname>Tida</keyname><forenames>Vijay Srinivas</forenames></author><author><keyname>Bappy</keyname><forenames>Mahathir Mohammad</forenames></author><author><keyname>Tian</keyname><forenames>Wenmeng</forenames></author><author><keyname>Hei</keyname><forenames>Xiai</forenames></author></authors><title>DP-SGD-Global-Adapt-V2-S: Triad Improvements of Privacy, Accuracy and   Fairness via Step Decay Noise Multiplier and Step Decay Upper Clipping   Threshold</title><categories>cs.LG cs.CR</categories><comments>34 pages single column, 10 figures, 21 tables</comments><msc-class>26, 40</msc-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Differentially Private Stochastic Gradient Descent (DP-SGD) has become a widely used technique for safeguarding sensitive information in deep learning applications. Unfortunately, DPSGD's per-sample gradient clipping and uniform noise addition during training can significantly degrade model utility and fairness. We observe that the latest DP-SGD-Global-Adapt's average gradient norm is the same throughout the training. Even when it is integrated with the existing linear decay noise multiplier, it has little or no advantage. Moreover, we notice that its upper clipping threshold increases exponentially towards the end of training, potentially impacting the models convergence. Other algorithms, DP-PSAC, Auto-S, DP-SGD-Global, and DP-F, have utility and fairness that are similar to or worse than DP-SGD, as demonstrated in experiments. To overcome these problems and improve utility and fairness, we developed the DP-SGD-Global-Adapt-V2-S. It has a step-decay noise multiplier and an upper clipping threshold that is also decayed step-wise. DP-SGD-Global-Adapt-V2-S with a privacy budget ($\epsilon$) of 1 improves accuracy by 0.9795\%, 0.6786\%, and 4.0130\% in MNIST, CIFAR10, and CIFAR100, respectively. It also reduces the privacy cost gap ($\pi$) by 89.8332% and 60.5541% in unbalanced MNIST and Thinwall datasets, respectively. Finally, we develop mathematical expressions to compute the privacy budget using truncated concentrated differential privacy (tCDP) for DP-SGD-Global-Adapt-V2-T and DP-SGD-Global-Adapt-V2-S. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.05153</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.05153</id><created>2023-12-08</created><updated>2025-02-05</updated><authors><author><keyname>Reiser</keyname><forenames>Philipp</forenames></author><author><keyname>Aguilar</keyname><forenames>Javier Enrique</forenames></author><author><keyname>Guthke</keyname><forenames>Anneli</forenames></author><author><keyname>Bürkner</keyname><forenames>Paul-Christian</forenames></author></authors><title>Uncertainty Quantification and Propagation in Surrogate-based Bayesian   Inference</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Surrogate models are statistical or conceptual approximations for more complex simulation models. In this context, it is crucial to propagate the uncertainty induced by limited simulation budget and surrogate approximation error to predictions, inference, and subsequent decision-relevant quantities. However, quantifying and then propagating the uncertainty of surrogates is usually limited to special analytic cases or is otherwise computationally very expensive. In this paper, we propose a framework enabling a scalable, Bayesian approach to surrogate modeling with thorough uncertainty quantification, propagation, and validation. Specifically, we present three methods for Bayesian inference with surrogate models given measurement data. This is a task where the propagation of surrogate uncertainty is especially relevant, because failing to account for it may lead to biased and/or overconfident estimates of the parameters of interest. We showcase our approach in three detailed case studies for linear and nonlinear real-world modeling scenarios. Uncertainty propagation in surrogate models enables more reliable and safe approximation of expensive simulators and will therefore be useful in various fields of applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.08363</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.08363</id><created>2023-12-13</created><updated>2025-02-05</updated><authors><author><keyname>Cavalar</keyname><forenames>Bruno</forenames></author><author><keyname>Goldin</keyname><forenames>Eli</forenames></author><author><keyname>Gray</keyname><forenames>Matthew</forenames></author><author><keyname>Hall</keyname><forenames>Peter</forenames></author><author><keyname>Liu</keyname><forenames>Yanyi</forenames></author><author><keyname>Pelecanos</keyname><forenames>Angelos</forenames></author></authors><title>On the Computational Hardness of Quantum One-Wayness</title><categories>cs.CR cs.CC quant-ph</categories><comments>Abstract modified to fit ArXiv requirements</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  There is a large body of work studying what forms of computational hardness are needed to realize classical cryptography. In particular, one-way functions and pseudorandom generators can be built from each other, and thus require equivalent computational assumptions to be realized. Furthermore, the existence of either of these primitives implies that $\rm{P} \neq \rm{NP}$, which gives a lower bound on the necessary hardness.   One can also define versions of each of these primitives with quantum output: respectively one-way state generators and pseudorandom state generators. Unlike in the classical setting, it is not known whether either primitive can be built from the other. Although it has been shown that pseudorandom state generators for certain parameter regimes can be used to build one-way state generators, the implication has not been previously known in full generality. Furthermore, to the best of our knowledge, the existence of one-way state generators has no known implications in complexity theory.   We show that pseudorandom states compressing $n$ bits to $\log n + 1$ qubits can be used to build one-way state generators and pseudorandom states compressing $n$ bits to $\omega(\log n)$ qubits are one-way state generators. This is a nearly optimal result since pseudorandom states with fewer than $c \log n$-qubit output can be shown to exist unconditionally. We also show that any one-way state generator can be broken by a quantum algorithm with classical access to a $\rm{PP}$ oracle.   An interesting implication of our results is that a $t(n)$-copy one-way state generator exists unconditionally, for every $t(n) = o(n/\log n)$. This contrasts nicely with the previously known fact that $O(n)$-copy one-way state generators require computational hardness. We also outline a new route towards a black-box separation between one-way state generators and quantum bit commitments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.10300</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.10300</id><created>2023-12-15</created><updated>2025-02-05</updated><authors><author><keyname>Han</keyname><forenames>Mingfei</forenames></author><author><keyname>Yang</keyname><forenames>Linjie</forenames></author><author><keyname>Chang</keyname><forenames>Xiaojun</forenames></author><author><keyname>Yao</keyname><forenames>Lina</forenames></author><author><keyname>Wang</keyname><forenames>Heng</forenames></author></authors><title>Shot2Story: A New Benchmark for Comprehensive Understanding of   Multi-shot Videos</title><categories>cs.CV</categories><comments>ICLR 2025. Extended annotation with 43K multi-shot videos in total.   https://mingfei.info/shot2story for updates and more information</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A short clip of video may contain progression of multiple events and an interesting story line. A human need to capture both the event in every shot and associate them together to understand the story behind it. In this work, we present a new multi-shot video understanding benchmark Shot2Story with detailed shot-level captions, comprehensive video summaries and question-answering pairs. To facilitate better semantic understanding of videos, we provide captions for both visual signals and human narrations. We design several distinct tasks including single-shot video captioning, multi-shot video summarization, and multi-shot video question answering. Preliminary experiments show some challenges to generate a long and comprehensive video summary for multi-shot videos. Nevertheless, the generated imperfect summaries can already achieve competitive performance on existing video understanding tasks such as video question-answering, promoting an under-explored setting of video understanding with detailed summaries. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.11017</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.11017</id><created>2023-12-18</created><updated>2025-02-05</updated><authors><author><keyname>Lau</keyname><forenames>Chin Wa</forenames></author><author><keyname>Nair</keyname><forenames>Chandra</forenames></author></authors><title>Information Inequalities via Ideas from Additive Combinatorics</title><categories>cs.IT math.CO math.IT math.NT</categories><comments>21 pages, update appendices; the authors were made aware that some of   the results had been obtained earlier. The revised version acknowledges and   references this work. A conference version of this was published in the   proceeding of IEEE ISIT 2023</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ruzsa's equivalence theorem provided a framework for converting certain families of inequalities in additive combinatorics to entropic inequalities (which sometimes did not possess stand-alone entropic proofs). In this work, we first establish formal equivalences between some families (different from Ruzsa) of inequalities in additive combinatorics and entropic ones. As a first step to further these equivalences, we establish an information-theoretic characterization of the magnification ratio that could also be of independent interest. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.13013</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.13013</id><created>2023-12-20</created><updated>2025-02-05</updated><authors><author><keyname>Guo</keyname><forenames>Xianzhen</forenames></author><author><keyname>Shi</keyname><forenames>Qin</forenames></author><author><keyname>Zhang</keyname><forenames>Shuowen</forenames></author><author><keyname>Xing</keyname><forenames>Chengwen</forenames></author><author><keyname>Liu</keyname><forenames>Liang</forenames></author></authors><title>User Equipment Assisted Localization for 6G Integrated Sensing and   Communication</title><categories>eess.SP cs.IT math.IT</categories><comments>submitted to IEEE TCOM</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates user equipment (UE) assisted device-free networked sensing in the sixth-generation (6G) integrated sensing and communication (ISAC) system, where one base station (BS) and multiple UEs, such as unmanned aerial vehicles (UAVs), serve as anchors to cooperatively localize multiple passive targets based on the range information. Three challenges arise from the above scheme. First, the UEs are not perfectly synchronized with the BSs. Second, the UE (anchor) positions are usually estimated by the Global Positioning System (GPS) and subject to unknown errors. Third, data association is challenging, since it is hard for each anchor to associate each rang estimation to the right target under device-free sensing. We first tackle the above three challenges under a passive UE based sensing mode, where UEs only passively hear the signals over the BS-target-UE paths. A two-phase UE assisted localization protocol is proposed. In Phase I, we design an efficient method to accurately estimate the ranges from the BS to the targets and those from the BS to the targets to the UEs in the presence of synchronization errors between the BS and the UEs. In Phase II, an efficient algorithm is proposed to localize the targets via jointly removing the UEs with quite inaccurate position information from the anchor set and matching the estimated ranges at the BS and the remaining UEs with the targets. Next, we also consider an active UE based sensing mode, where the UEs can actively emit signals to obtain additional range information from them to the targets. We show that this additional range information can be utilized to significantly reduce the complexity of Phase II in the aforementioned two-phase localization protocol. Numerical results show that our proposed UE assisted networked sensing scheme can achieve very high localization accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.15230</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.15230</id><created>2023-12-23</created><updated>2025-02-05</updated><authors><author><keyname>Zimmer</keyname><forenames>Max</forenames></author><author><keyname>Andoni</keyname><forenames>Megi</forenames></author><author><keyname>Spiegel</keyname><forenames>Christoph</forenames></author><author><keyname>Pokutta</keyname><forenames>Sebastian</forenames></author></authors><title>PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs</title><categories>cs.LG cs.AI</categories><comments>32 pages, 7 figures, 24 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Neural Networks can be effectively compressed through pruning, significantly reducing storage and compute demands while maintaining predictive performance. Simple yet effective methods like magnitude pruning remove less important parameters and typically require a costly retraining procedure to restore performance. However, with the rise of LLMs, full retraining has become infeasible due to memory and compute constraints. This study challenges the practice of retraining all parameters by showing that updating a small subset of highly expressive parameters can suffice to recover or even enhance performance after pruning. Surprisingly, retraining just 0.01%-0.05% of the parameters in GPT-architectures can match the performance of full retraining across various sparsity levels, significantly reducing compute and memory requirements, and enabling retraining of models with up to 30 billion parameters on a single GPU in minutes. To bridge the gap to full retraining in the high sparsity regime, we introduce two novel LoRA variants that, unlike standard LoRA, allow merging adapters back without compromising sparsity. Going a step further, we show that these methods can be applied for memory-efficient layer-wise reconstruction, significantly enhancing state-of-the-art retraining-free methods like Wanda (Sun et al., 2023) and SparseGPT (Frantar &amp; Alistarh, 2023). Our findings present a promising alternative to avoiding retraining. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.17183</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.17183</id><created>2023-12-28</created><updated>2025-02-05</updated><authors><author><keyname>Zhao</keyname><forenames>Ziheng</forenames></author><author><keyname>Zhang</keyname><forenames>Yao</forenames></author><author><keyname>Wu</keyname><forenames>Chaoyi</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaoman</forenames></author><author><keyname>Zhang</keyname><forenames>Ya</forenames></author><author><keyname>Wang</keyname><forenames>Yanfeng</forenames></author><author><keyname>Xie</keyname><forenames>Weidi</forenames></author></authors><title>One Model to Rule them All: Towards Universal Segmentation for Medical   Images with Text Prompts</title><categories>eess.IV cs.CV</categories><comments>69 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we aim to build up a model that can Segment Anything in radiology scans, driven by medical terminologies as Text prompts, termed as SAT. Our main contributions are three folds: (i) for dataset construction, we construct the first multi-modal knowledge tree on human anatomy, including 6502 anatomical terminologies; Then, we build up the largest and most comprehensive segmentation dataset for training, by collecting over 22K 3D medical image scans from72 segmentation datasets, across 497 classes, with careful standardization on both image scans and label space; (ii) for architecture design, we propose to inject medical knowledge into a text encoder via contrastive learning, and then formulate a universal segmentation model, that can be prompted by feeding in medical terminologies in text form; (iii) As a result, we have trained SAT-Nano (110M parameters) and SAT-Pro (447M parameters), demonstrating superior or comparable performance to 72 specialist models, i.e., nnU-Nets, U-Mamba or SwinUNETR, trained on each dataset/subsets. We validate SAT as a foundational segmentation model, with better generalization on external (cross-center) datasets, and can be further improved on specific tasks after fine-tuning adaptation. Comparing with state-of-the-art interactive segmentation model MedSAM, SAT demonstrate superior performance, scalability and robustness. We further compare SAT with BiomedParse, and observe SAT is significantly superior in both internal and external evaluation. Through extensive ablation study, we validate the benefit of domain knowledge on universal segmentation, especially on tail categories. As a use case, we demonstrate that SAT can act as a powerful out-of-the-box agent for large language models, enabling visual grounding in versatile application scenarios. All the data, codes, and models in this work have been released. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.00747</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.00747</id><created>2024-01-01</created><updated>2025-02-05</updated><authors><author><keyname>Sun</keyname><forenames>Hongbo</forenames></author><author><keyname>Xia</keyname><forenames>Chongkun</forenames></author><author><keyname>Tan</keyname><forenames>Junbo</forenames></author><author><keyname>Yuan</keyname><forenames>Bo</forenames></author><author><keyname>Wang</keyname><forenames>Xueqian</forenames></author><author><keyname>Liang</keyname><forenames>Bin</forenames></author></authors><title>Geometric Structure and Polynomial-time Algorithm of Game Equilibria</title><categories>cs.GT cs.MA</categories><comments>31 pages, 5 figures, code and animation are available at   https://github.com/shb20tsinghua/PTAS_Game/tree/main</comments><msc-class>90C39, 90C51, 91A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Whether a PTAS (polynomial-time approximation scheme) exists for game equilibria has been an open question, and its absence has indications and consequences in three fields: the practicality of methods in algorithmic game theory, non-stationarity and curse of multiagency in MARL (multi-agent reinforcement learning), and the tractability of PPAD in computational complexity theory. In this paper, we formalize the game equilibrium problem as an optimization problem that splits into two subproblems with respect to policy and value function, which are solved respectively by interior point method and dynamic programming. Combining these two parts, we obtain an FPTAS (fully PTAS) for the weak approximation (approximating to an $\epsilon$-equilibrium) of any perfect equilibrium of any dynamic game, implying PPAD=FP since the weak approximation problem is PPAD-complete. In addition, we introduce a geometric object called equilibrium bundle, regarding which, first, perfect equilibria of dynamic games are formalized as zero points of its canonical section, second, the hybrid iteration of dynamic programming and interior point method is formalized as a line search on it, third, it derives the existence and oddness theorems as an extension of those of Nash equilibria. In experiment, the line search process is animated, and the method is tested on 2000 randomly generated dynamic games where it converges to a perfect equilibrium in every single case. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.02501</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.02501</id><created>2024-01-04</created><updated>2025-02-05</updated><authors><author><keyname>Aho</keyname><forenames>Layton</forenames></author><author><keyname>Winter</keyname><forenames>Mark</forenames></author><author><keyname>DeCarlo</keyname><forenames>Marc</forenames></author><author><keyname>Frismantiene</keyname><forenames>Agne</forenames></author><author><keyname>Blum</keyname><forenames>Yannick</forenames></author><author><keyname>Gagliardi</keyname><forenames>Paolo Armando</forenames></author><author><keyname>Pertz</keyname><forenames>Olivier</forenames></author><author><keyname>Cohen</keyname><forenames>Andrew R.</forenames></author></authors><title>A Kolmogorov metric embedding for live cell microscopy signaling   patterns</title><categories>cs.CV cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a metric embedding that captures spatiotemporal patterns of cell signaling dynamics in 5-D $(x,y,z,channel,time)$ live cell microscopy movies. The embedding uses a metric distance called the normalized information distance (NID) based on Kolmogorov complexity theory, an absolute measure of information content between digital objects. The NID uses statistics of lossless compression to compute a theoretically optimal metric distance between pairs of 5-D movies, requiring no a priori knowledge of expected pattern dynamics, and no training data. The cell signaling structure function (SSF) is defined using a class of metric 3-D image filters that compute at each spatiotemporal cell centroid the voxel intensity configuration of the nucleus w.r.t. the surrounding cytoplasm, or a functional output e.g. velocity. The only parameter is the expected cell radii ($\mu m$). The SSF can be optionally combined with segmentation and tracking algorithms. The resulting lossless compression pipeline represents each 5-D input movie as a single point in a metric embedding space. The utility of a metric embedding follows from Euclidean distance between any points in the embedding space approximating optimally the pattern difference, as measured by the NID, between corresponding pairs of 5-D movies. This is true throughout the embedding space, not only at points corresponding to input images. Examples are shown for synthetic data, for 2-D+time movies of ERK and AKT signaling under different oncogenic mutations in human epithelial (MCF10A) cells, for 3-D MCF10A spheroids under optogenetic manipulation of ERK, and for ERK dynamics during colony differentiation in human induced pluripotent stem cells. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.02564</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.02564</id><created>2024-01-04</created><updated>2025-02-05</updated><authors><author><keyname>Rout</keyname><forenames>Biraaj</forenames></author><author><keyname>Borad</keyname><forenames>Priyanshi</forenames></author><author><keyname>Malidarreh</keyname><forenames>Parisa Boodaghi</forenames></author><author><keyname>Nasr</keyname><forenames>Mohammad Sadegh</forenames></author><author><keyname>Saurav</keyname><forenames>Jillur Rahman</forenames></author><author><keyname>Fenelon</keyname><forenames>Kelli</forenames></author><author><keyname>Veerla</keyname><forenames>Jai Prakash</forenames></author><author><keyname>Luber</keyname><forenames>Jacob M.</forenames></author><author><keyname>Koromila</keyname><forenames>Theodora</forenames></author></authors><title>Predicting Future States with Spatial Point Processes in Single Molecule   Resolution Spatial Transcriptomics</title><categories>q-bio.TO cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In this paper, we introduce a pipeline based on XGboost to predict the future distribution of cells that are expressed by the Sog-D gene (active cells) in both the Anterior to posterior (AP) and the Dorsal to Ventral (DV) axis of the Drosophila in embryogenesis process. This method provides insights about how cells and living organisms control gene expression in super resolution whole embryo spatial transcriptomics imaging at sub cellular, single molecule resolution. An XGboost model was used to predict the next stage active distribution based on the previous one. To achieve this goal, we leveraged temporally resolved, spatial point processes by including Ripley's K-function in conjunction with the cell's state in each stage of embryogenesis, and found average predictive accuracy of active cell distribution. This tool is analogous to RNA Velocity for spatially resolved developmental biology, from one data point we can predict future spatially resolved gene expression using features from the spatial point processes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.03835</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.03835</id><created>2024-01-08</created><updated>2025-02-05</updated><authors><author><keyname>Fu</keyname><forenames>Qiang</forenames></author><author><keyname>Souza</keyname><forenames>Matheus</forenames></author><author><keyname>Choi</keyname><forenames>Eunsue</forenames></author><author><keyname>Shin</keyname><forenames>Suhyun</forenames></author><author><keyname>Baek</keyname><forenames>Seung-Hwan</forenames></author><author><keyname>Heidrich</keyname><forenames>Wolfgang</forenames></author></authors><title>Limitations of Data-Driven Spectral Reconstruction -- Optics-Aware   Analysis and Mitigation</title><categories>cs.CV eess.IV</categories><comments>13 pages, 7 figures, 9 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperspectral imaging empowers machine vision systems with the distinct capability of identifying materials through recording their spectral signatures. Recent efforts in data-driven spectral reconstruction aim at extracting spectral information from RGB images captured by cost-effective RGB cameras, instead of dedicated hardware.   In this paper we systematically analyze the performance of such methods, evaluating both the practical limitations with respect to current datasets and overfitting, as well as fundamental limitations with respect to the nature of the information encoded in the RGB images, and the dependency of this information on the optical system of the camera.   We find that, the current models are not robust under slight variations, e.g., in noise level or compression of the RGB file. Without modeling underrepresented spectral content, existing datasets and the models trained on them are limited in their ability to cope with challenging metameric colors. To mitigate this issue, we propose to exploit the combination of metameric data augmentation and optical lens aberrations to improve the encoding of the metameric information into the RGB image, which paves the road towards higher performing spectral imaging and reconstruction approaches. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.05648</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.05648</id><created>2024-01-10</created><updated>2025-02-05</updated><authors><author><keyname>Curbelo</keyname><forenames>Israel R.</forenames></author><author><keyname>Malko</keyname><forenames>Hannah R.</forenames></author></authors><title>On the on-line coloring of unit interval graphs with proper interval   representation</title><categories>math.CO cs.DS</categories><comments>Alternative title: On the on-line chain partitioning of semi-orders   with proper interval representation</comments><msc-class>05C15 (Primary) 68W27 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define the problem as a two-player game between Algorithm and Builder. The game is played in rounds. Each round, Builder presents an interval that is neither contained in nor contains any previously presented interval. Algorithm immediately and irrevocably assigns the interval a color that has not been assigned to any interval intersecting it. The set of intervals form an interval representation for a unit interval graph and the colors form a proper coloring of that graph. For every positive integer $\omega$, we define the value $R(\omega)$ as the maximum number of colors for which Builder has a strategy that forces Algorithm to use $R(\omega)$ colors with the restriction that the unit interval graph constructed cannot contain a clique of size $\omega$. In 1981, Chrobak and \'{S}lusarek showed that $R(\omega)\leq2\omega -1$. In 2005, Epstein and Levy showed that $R(\omega)\geq\lfloor{3\omega/2\rfloor}$. This problem remained unsolved for $\omega\geq 3$. In 2022, Bir\'o and Curbelo showed that $R(3)=5$. In this paper, we show that $R(4)=7$ </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.13255</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.13255</id><created>2024-01-24</created><updated>2025-02-05</updated><authors><author><keyname>Tuyéras</keyname><forenames>Rémy</forenames></author></authors><title>Constructing a fully homomorphic encryption scheme with the Yoneda Lemma</title><categories>cs.CR math.CT</categories><comments>54 pages; corrected typos; added clarifications; refined statement on   bootstrapping; changed "token morphism" to "garbling operation"; updated   section on refreshable ciphertexts</comments><msc-class>18A35 (Primary) 18C30, 18A05, 68P25 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper redefines the foundations of asymmetric cryptography's homomorphic cryptosystems through the application of the Yoneda Lemma. It demonstrates that widely adopted systems, including ElGamal, RSA, Benaloh, Regev's LWE, and NTRUEncrypt, are directly derived from the principles of the Yoneda Lemma. This synthesis leads to the creation of a holistic homomorphic encryption framework, the Yoneda Encryption Scheme. Within this framework, encryption is modeled using the bijective maps of the Yoneda Lemma Isomorphism, with decryption following naturally from the properties of these maps. This unification suggests a conjecture for a unified model theory framework, offering a foundation for reasoning about both homomorphic and fully homomorphic encryption (FHE) schemes. As a practical demonstration, the paper introduces the FHE scheme ACES, which supports arbitrary finite sequences of encrypted multiplications and additions without relying on conventional bootstrapping techniques for ciphertext refreshment. This highlights the practical implications of the theoretical advancements and proposes a new approach for leveraging model theory and forcing techniques in cryptography, particularly in the design of FHE schemes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.14617</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.14617</id><created>2024-01-25</created><updated>2025-02-05</updated><authors><author><keyname>Cao</keyname><forenames>Sicong</forenames></author><author><keyname>Sun</keyname><forenames>Xiaobing</forenames></author><author><keyname>Widyasari</keyname><forenames>Ratnadira</forenames></author><author><keyname>Lo</keyname><forenames>David</forenames></author><author><keyname>Wu</keyname><forenames>Xiaoxue</forenames></author><author><keyname>Bo</keyname><forenames>Lili</forenames></author><author><keyname>Zhang</keyname><forenames>Jiale</forenames></author><author><keyname>Li</keyname><forenames>Bin</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Wu</keyname><forenames>Di</forenames></author><author><keyname>Chen</keyname><forenames>Yixin</forenames></author></authors><title>A Systematic Literature Review on Explainability for Machine/Deep   Learning-based Software Engineering Research</title><categories>cs.SE cs.AI</categories><comments>Under Review in ACM Computing Surveys (Major Revision)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The remarkable achievements of Artificial Intelligence (AI) algorithms, particularly in Machine Learning (ML) and Deep Learning (DL), have fueled their extensive deployment across multiple sectors, including Software Engineering (SE). However, due to their black-box nature, these promising AI-driven SE models are still far from being deployed in practice. This lack of explainability poses unwanted risks for their applications in critical tasks, such as vulnerability detection, where decision-making transparency is of paramount importance. This paper endeavors to elucidate this interdisciplinary domain by presenting a systematic literature review of approaches that aim to improve the explainability of AI models within the context of SE. The review canvasses work appearing in the most prominent SE &amp; AI conferences and journals, and spans 108 papers across 23 unique SE tasks. Based on three key Research Questions (RQs), we aim to (1) summarize the SE tasks where XAI techniques have shown success to date; (2) classify and analyze different XAI techniques; and (3) investigate existing evaluation approaches. Based on our findings, we identified a set of challenges remaining to be addressed in existing studies, together with a set of guidelines highlighting potential opportunities we deemed appropriate and important for future work. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.15478</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.15478</id><created>2024-01-27</created><updated>2025-02-04</updated><authors><author><keyname>McNeela</keyname><forenames>Daniel</forenames></author><author><keyname>Sala</keyname><forenames>Frederic</forenames></author><author><keyname>Gitter</keyname><forenames>Anthony</forenames></author></authors><title>Product Manifold Representations for Learning on Biological Pathways</title><categories>q-bio.QM cs.LG q-bio.MN</categories><comments>29 pages, 19 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Machine learning models that embed graphs in non-Euclidean spaces have shown substantial benefits in a variety of contexts, but their application has not been studied extensively in the biological domain, particularly with respect to biological pathway graphs. Such graphs exhibit a variety of complex network structures, presenting challenges to existing embedding approaches. Learning high-quality embeddings for biological pathway graphs is important for researchers looking to understand the underpinnings of disease and train high-quality predictive models on these networks. In this work, we investigate the effects of embedding pathway graphs in non-Euclidean mixed-curvature spaces and compare against traditional Euclidean graph representation learning models. We then train a supervised model using the learned node embeddings to predict missing protein-protein interactions in pathway graphs. We find large reductions in distortion and boosts on in-distribution edge prediction performance as a result of using mixed-curvature embeddings and their corresponding graph neural network models. However, we find that mixed-curvature representations underperform existing baselines on out-of-distribution edge prediction performance suggesting that these representations may overfit to the training graph topology. We provide our Mixed-Curvature Product Graph Convolutional Network code at https://github.com/mcneela/Mixed-Curvature-GCN and our pathway analysis code at https://github.com/mcneela/Mixed-Curvature-Pathways. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.17444</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.17444</id><created>2024-01-30</created><updated>2025-02-05</updated><authors><author><keyname>Amrane</keyname><forenames>Amazigh</forenames></author><author><keyname>Bazille</keyname><forenames>Hugo</forenames></author><author><keyname>Clement</keyname><forenames>Emily</forenames></author><author><keyname>Fahrenberg</keyname><forenames>Uli</forenames></author><author><keyname>Schlehuber-Caissier</keyname><forenames>Philipp</forenames></author></authors><title>Higher-Dimensional Timed Automata for Real-Time Concurrency</title><categories>cs.FL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new language semantics for real-time concurrency. Its operational models are higher-dimensional timed automata (HDTAs), a generalization of both higher-dimensional automata and timed automata. In real-time concurrent systems, both concurrency of events and timing and duration of events are of interest. Thus, HDTAs combine the non-interleaving concurrency model of higher-dimensional automata with the real-time modeling, using clocks, of timed automata. We define languages of HDTAs as sets of interval-timed pomsets with interfaces.   We show that language inclusion of HDTAs is undecidable. On the other hand, using a region construction we can show that untimings of HDTA languages have enough regularity so that untimed language inclusion is decidable. On a more practical note, we give new insights on when practical applications, like checking reachability, might benefit from using HDTAs instead of classical timed automata. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.01528</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.01528</id><created>2024-02-02</created><updated>2025-02-04</updated><authors><author><keyname>Yan</keyname><forenames>Minghao</forenames></author><author><keyname>Agarwal</keyname><forenames>Saurabh</forenames></author><author><keyname>Venkataraman</keyname><forenames>Shivaram</forenames></author></authors><title>Decoding Speculative Decoding</title><categories>cs.LG cs.CL</categories><comments>Proceedings of the 2025 Conference of the North American Chapter of   the Association for Computational Linguistics: Human Language Technologies   (NAACL 2025)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without sacrificing quality. When performing inference, speculative decoding uses a smaller draft model to generate speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. In this work, we perform a detailed study comprising over 350 experiments with LLaMA-65B and OPT-66B using speculative decoding and delineate the factors that affect the performance gain provided by speculative decoding. Our experiments indicate that the performance of speculative decoding depends heavily on the latency of the draft model, and the draft model's capability in language modeling does not correlate strongly with its performance in speculative decoding. Based on these insights we explore a new design space for draft models and design hardware-efficient draft models for speculative decoding. Our newly designed draft model can provide 111% higher throughput than existing draft models and our approach generalizes further to all LLaMA models (1/2/3.1) and supervised fine-tuned models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.02168</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.02168</id><created>2024-02-03</created><updated>2025-02-05</updated><authors><author><keyname>Huang</keyname><forenames>Xuanwen</forenames></author><author><keyname>Chow</keyname><forenames>Wei</forenames></author><author><keyname>Zhu</keyname><forenames>Yize</forenames></author><author><keyname>Wang</keyname><forenames>Yang</forenames></author><author><keyname>Chai</keyname><forenames>Ziwei</forenames></author><author><keyname>Wang</keyname><forenames>Chunping</forenames></author><author><keyname>Chen</keyname><forenames>Lei</forenames></author><author><keyname>Yang</keyname><forenames>Yang</forenames></author></authors><title>Enhancing Cross-domain Link Prediction via Evolution Process Modeling</title><categories>cs.LG cs.AI cs.SI</categories><comments>Accepted by WWW'25</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes DyExpert, a dynamic graph model for cross-domain link prediction. It can explicitly model historical evolving processes to learn the evolution pattern of a specific downstream graph and subsequently make pattern-specific link predictions. DyExpert adopts a decode-only transformer and is capable of efficiently parallel training and inference by \textit{conditioned link generation} that integrates both evolution modeling and link prediction. DyExpert is trained by extensive dynamic graphs across diverse domains, comprising 6M dynamic edges. Extensive experiments on eight untrained graphs demonstrate that DyExpert achieves state-of-the-art performance in cross-domain link prediction. Compared to the advanced baseline under the same setting, DyExpert achieves an average of 11.40% improvement Average Precision across eight graphs. More impressive, it surpasses the fully supervised performance of 8 advanced baselines on 6 untrained graphs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.04728</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.04728</id><created>2024-02-07</created><updated>2025-02-05</updated><authors><author><keyname>Forsch</keyname><forenames>Christian</forenames></author><author><keyname>Zillmann</keyname><forenames>Peter</forenames></author><author><keyname>Alrabadi</keyname><forenames>Osama</forenames></author><author><keyname>Brueck</keyname><forenames>Stefan</forenames></author><author><keyname>Gerstacker</keyname><forenames>Wolfgang</forenames></author></authors><title>Detection Schemes with Low-Resolution ADCs and Spatial Oversampling for   Transmission with Higher-Order Constellations in the Terahertz Band</title><categories>cs.IT eess.SP math.IT</categories><comments>16 pages, 17 figures, published in IEEE Access</comments><journal-ref>2025 IEEE Access</journal-ref><doi>10.1109/ACCESS.2025.3537583</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider Terahertz (THz) communications with low-resolution uniform quantization and spatial oversampling at the receiver side, corresponding to a single-input multiple-output (SIMO) transmission. We fairly compare different analog-to-digital converter (ADC) parametrizations by keeping the ADC power consumption constant. Here, 1-, 2-, and 3-bit quantization is investigated with different oversampling factors. We analytically compute the statistics of the detection variable, and we propose the optimal and several suboptimal detection schemes for arbitrary quantization resolutions. Then, we evaluate the symbol error rate (SER) of the different detectors for 16- and 64-ary quadrature amplitude modulation (QAM). The results indicate that there is a noticeable performance degradation of the suboptimal detectors compared to the optimal detector when the constellation size is larger than the number of quantization levels. Furthermore, at low signal-to-noise ratios (SNRs), 1-bit quantization outperforms 2- and 3-bit quantization, respectively, even when employing higher-order constellations. We confirm our analytical results by Monte Carlo simulations. Both a pure line-of-sight (LoS) and a more realistically modeled indoor THz channel are considered. Then, we optimize the input signal constellation with respect to SER for 1- and 2-bit quantization. The results give insights for optimizing higher-order constellations for arbitrary quantization resolutions and show that the minimum SER can be lowered significantly by appropriately placing the constellation points. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.04933</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.04933</id><created>2024-02-07</created><updated>2025-02-05</updated><authors><author><keyname>Liang</keyname><forenames>Biyonka</forenames></author><author><keyname>Xu</keyname><forenames>Lily</forenames></author><author><keyname>Taneja</keyname><forenames>Aparna</forenames></author><author><keyname>Tambe</keyname><forenames>Milind</forenames></author><author><keyname>Janson</keyname><forenames>Lucas</forenames></author></authors><title>Context in Public Health for Underserved Communities: A Bayesian   Approach to Online Restless Bandits</title><categories>cs.LG stat.AP</categories><comments>29 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Public health programs often provide interventions to encourage program adherence, and effectively allocating interventions is vital for producing the greatest overall health outcomes, especially in underserved communities where resources are limited. Such resource allocation problems are often modeled as restless multi-armed bandits (RMABs) with unknown underlying transition dynamics, hence requiring online reinforcement learning (RL). We present Bayesian Learning for Contextual RMABs (BCoR), an online RL approach for RMABs that novelly combines techniques in Bayesian modeling with Thompson sampling to flexibly model the complex RMAB settings present in public health program adherence problems, namely context and non-stationarity. BCoR's key strength is the ability to leverage shared information within and between arms to learn the unknown RMAB transition dynamics quickly in intervention-scarce settings with relatively short time horizons, which is common in public health applications. Empirically, BCoR achieves substantially higher finite-sample performance over a range of experimental settings, including a setting using real-world adherence data that was developed in collaboration with ARMMAN, an NGO in India which runs a large-scale maternal mHealth program, showcasing BCoR practical utility and potential for real-world deployment. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.05971</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.05971</id><created>2024-02-06</created><updated>2025-02-04</updated><authors><author><keyname>Ma</keyname><forenames>Yihong</forenames></author><author><keyname>Huang</keyname><forenames>Xiaobao</forenames></author><author><keyname>Nan</keyname><forenames>Bozhao</forenames></author><author><keyname>Moniz</keyname><forenames>Nuno</forenames></author><author><keyname>Zhang</keyname><forenames>Xiangliang</forenames></author><author><keyname>Wiest</keyname><forenames>Olaf</forenames></author><author><keyname>Chawla</keyname><forenames>Nitesh V.</forenames></author></authors><title>Are we making much progress? Revisiting chemical reaction yield   prediction from an imbalanced regression perspective</title><categories>cs.LG physics.chem-ph</categories><comments>Published in Companion Proceedings of The ACM Web Conference 2024   (WWW '24)</comments><doi>10.1145/3589335.3651470</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The yield of a chemical reaction quantifies the percentage of the target product formed in relation to the reactants consumed during the chemical reaction. Accurate yield prediction can guide chemists toward selecting high-yield reactions during synthesis planning, offering valuable insights before dedicating time and resources to wet lab experiments. While recent advancements in yield prediction have led to overall performance improvement across the entire yield range, an open challenge remains in enhancing predictions for high-yield reactions, which are of greater concern to chemists. In this paper, we argue that the performance gap in high-yield predictions results from the imbalanced distribution of real-world data skewed towards low-yield reactions, often due to unreacted starting materials and inherent ambiguities in the reaction processes. Despite this data imbalance, existing yield prediction methods continue to treat different yield ranges equally, assuming a balanced training distribution. Through extensive experiments on three real-world yield prediction datasets, we emphasize the urgent need to reframe reaction yield prediction as an imbalanced regression problem. Finally, we demonstrate that incorporating simple cost-sensitive re-weighting methods can significantly enhance the performance of yield prediction models on underrepresented high-yield regions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.09803</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.09803</id><created>2024-02-15</created><updated>2024-09-16</updated><authors><author><keyname>Weissinger</keyname><forenames>Lukas</forenames></author><author><keyname>Hubmer</keyname><forenames>Simon</forenames></author><author><keyname>Ramlau</keyname><forenames>Ronny</forenames></author><author><keyname>Voss</keyname><forenames>Henning Uwe</forenames></author></authors><title>An Inverse Problems Approach to Pulse Wave Analysis in the Human Brain</title><categories>math.NA cs.NA</categories><comments>31 pages, 11 figures</comments><msc-class>47J06, 65J22, 65J20, 47A52</msc-class><doi>10.1137/24M163921X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cardiac pulsations in the human brain have received recent interest due to their possible role in the pathogenesis of neurodegenerative diseases. Further interest stems from their possible application as an endogenous signal source that can be utilized for brain imaging in general. The (pulse-)wave describing the blood flow velocity along an intracranial artery consists of a forward (anterograde) and a backward (retrograde, reflected) part, but measurements of this wave usually consist of a superposition of these components. In this paper, we provide a mathematical framework for the inverse problem of estimating the pulse wave velocity, as well as the forward and backward component of the pulse wave separately from MRI measurements on intracranial arteries. After a mathematical analysis of this problem, we consider possible reconstruction approaches, and derive an alternate direction approach for its solution. The resulting methods provide estimates for anterograde/retrograde wave forms and the pulse wave velocity under specified assumptions on a cerebrovascular model system. Numerical experiments on simulated and experimental data demonstrate the applicability and preliminary in vivo feasibility of the proposed methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.12062</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.12062</id><created>2024-02-19</created><updated>2025-02-05</updated><authors><author><keyname>Di Bello</keyname><forenames>Marcello</forenames></author><author><keyname>Cangiotti</keyname><forenames>Nicolò</forenames></author><author><keyname>Loi</keyname><forenames>Michele</forenames></author></authors><title>Causal Equal Protection as Algorithmic Fairness</title><categories>cs.CY cs.AI cs.DS cs.LG</categories><comments>18 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By combining the philosophical literature on statistical evidence and the interdisciplinary literature on algorithmic fairness, we revisit recent objections against classification parity in light of causal analyses of algorithmic fairness and the distinction between predictive and diagnostic evidence. We focus on trial proceedings as a black-box classification algorithm in which defendants are sorted into two groups by convicting or acquitting them. We defend a novel principle, causal equal protection, that combines classification parity with the causal approach. In the do-calculus, causal equal protection requires that individuals should not be subject to uneven risks of classification error because of their protected or socially salient characteristics. The explicit use of protected characteristics, however, may be required if it equalizes these risks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.12806</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.12806</id><created>2024-02-20</created><updated>2025-02-04</updated><authors><author><keyname>Lee</keyname><forenames>Jinu</forenames></author><author><keyname>Hwang</keyname><forenames>Wonseok</forenames></author></authors><title>SymBa: Symbolic Backward Chaining for Structured Natural Language   Reasoning</title><categories>cs.CL</categories><comments>17 pages (8 pages for main text),11 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  To improve the performance and explainability of LLM-based natural language reasoning, structured reasoning can be applied to generate explicitly structured proofs. Among different methods for structured reasoning, we specifically focus on backward chaining, where the proof goal is recursively decomposed to subgoals by searching and applying rules. We argue that current LLM-based backward chaining systems (e.g. Least-to-most prompting and LAMBADA) are incomplete, as they omit crucial algorithmic components identified from the classic backward chaining algorithm in computational logic (SLD Resolution). To this end, we propose a novel backward chaining system, SymBa (Symbolic Backward Chaining), which integrates a symbolic solver and an LLM. In SymBa, the solver controls the proof process, and the LLM is only called when the solver requires new information to complete the proof. Empowered by completeness, SymBa achieves a significant improvement in seven deductive, relational, and arithmetic reasoning benchmarks compared to the baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.14645</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.14645</id><created>2024-02-22</created><updated>2025-02-04</updated><authors><author><keyname>Gupte</keyname><forenames>Aparna</forenames></author><author><keyname>Vafa</keyname><forenames>Neekon</forenames></author><author><keyname>Vaikuntanathan</keyname><forenames>Vinod</forenames></author></authors><title>Sparse Linear Regression and Lattice Problems</title><categories>cs.LG stat.ML</categories><comments>TCC 2024; minor edits</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sparse linear regression (SLR) is a well-studied problem in statistics where one is given a design matrix $X\in\mathbb{R}^{m\times n}$ and a response vector $y=X\theta^*+w$ for a $k$-sparse vector $\theta^*$ (that is, $\|\theta^*\|_0\leq k$) and small, arbitrary noise $w$, and the goal is to find a $k$-sparse $\widehat{\theta} \in \mathbb{R}^n$ that minimizes the mean squared prediction error $\frac{1}{m}\|X\widehat{\theta}-X\theta^*\|^2_2$. While $\ell_1$-relaxation methods such as basis pursuit, Lasso, and the Dantzig selector solve SLR when the design matrix is well-conditioned, no general algorithm is known, nor is there any formal evidence of hardness in an average-case setting with respect to all efficient algorithms.   We give evidence of average-case hardness of SLR w.r.t. all efficient algorithms assuming the worst-case hardness of lattice problems. Specifically, we give an instance-by-instance reduction from a variant of the bounded distance decoding (BDD) problem on lattices to SLR, where the condition number of the lattice basis that defines the BDD instance is directly related to the restricted eigenvalue condition of the design matrix, which characterizes some of the classical statistical-computational gaps for sparse linear regression. Also, by appealing to worst-case to average-case reductions from the world of lattices, this shows hardness for a distribution of SLR instances; while the design matrices are ill-conditioned, the resulting SLR instances are in the identifiable regime.   Furthermore, for well-conditioned (essentially) isotropic Gaussian design matrices, where Lasso is known to behave well in the identifiable regime, we show hardness of outputting any good solution in the unidentifiable regime where there are many solutions, assuming the worst-case hardness of standard and well-studied lattice problems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.16105</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.16105</id><created>2024-02-25</created><updated>2025-02-05</updated><authors><author><keyname>Kobalczyk</keyname><forenames>Katarzyna</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Towards Automated Knowledge Integration From Human-Interpretable   Representations</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A significant challenge in machine learning, particularly in noisy and low-data environments, lies in effectively incorporating inductive biases to enhance data efficiency and robustness. Despite the success of informed machine learning methods, designing algorithms with explicit inductive biases remains largely a manual process. In this work, we explore how prior knowledge represented in its native formats, e.g. in natural language, can be integrated into machine learning models in an automated manner. Inspired by the learning to learn principles of meta-learning, we consider the approach of learning to integrate knowledge via conditional meta-learning, a paradigm we refer to as informed meta-learning. We introduce and motivate theoretically the principles of informed meta-learning enabling automated and controllable inductive bias selection. To illustrate our claims, we implement an instantiation of informed meta-learning--the Informed Neural Process, and empirically demonstrate the potential benefits and limitations of informed meta-learning in improving data efficiency and generalisation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.17467</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.17467</id><created>2024-02-27</created><authors><author><keyname>Le</keyname><forenames>Dinh-Viet-Toan</forenames></author><author><keyname>Bigo</keyname><forenames>Louis</forenames></author><author><keyname>Keller</keyname><forenames>Mikaela</forenames></author><author><keyname>Herremans</keyname><forenames>Dorien</forenames></author></authors><title>Natural Language Processing Methods for Symbolic Music Generation and   Information Retrieval: a Survey</title><categories>cs.IR cs.AI cs.SD eess.AS</categories><comments>36 pages, 5 figures, 4 tables</comments><doi>10.1145/3714457</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Several adaptations of Transformers models have been developed in various domains since its breakthrough in Natural Language Processing (NLP). This trend has spread into the field of Music Information Retrieval (MIR), including studies processing music data. However, the practice of leveraging NLP tools for symbolic music data is not novel in MIR. Music has been frequently compared to language, as they share several similarities, including sequential representations of text and music. These analogies are also reflected through similar tasks in MIR and NLP. This survey reviews NLP methods applied to symbolic music generation and information retrieval studies following two axes. We first propose an overview of representations of symbolic music adapted from natural language sequential representations. Such representations are designed by considering the specificities of symbolic music. These representations are then processed by models. Such models, possibly originally developed for text and adapted for symbolic music, are trained on various tasks. We describe these models, in particular deep learning models, through different prisms, highlighting music-specialized mechanisms. We finally present a discussion surrounding the effective use of NLP tools for symbolic music data. This includes technical issues regarding NLP methods and fundamental differences between text and music, which may open several doors for further research into more effectively adapting NLP tools to symbolic MIR. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.17911</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.17911</id><created>2024-02-27</created><updated>2025-02-04</updated><authors><author><keyname>Hu</keyname><forenames>Hong-Ye</forenames></author><author><keyname>Gu</keyname><forenames>Andi</forenames></author><author><keyname>Majumder</keyname><forenames>Swarnadeep</forenames></author><author><keyname>Ren</keyname><forenames>Hang</forenames></author><author><keyname>Zhang</keyname><forenames>Yipei</forenames></author><author><keyname>Wang</keyname><forenames>Derek S.</forenames></author><author><keyname>You</keyname><forenames>Yi-Zhuang</forenames></author><author><keyname>Minev</keyname><forenames>Zlatko</forenames></author><author><keyname>Yelin</keyname><forenames>Susanne F.</forenames></author><author><keyname>Seif</keyname><forenames>Alireza</forenames></author></authors><title>Demonstration of Robust and Efficient Quantum Property Learning with   Shallow Shadows</title><categories>quant-ph cond-mat.stat-mech cs.IT cs.LG math.IT</categories><comments>Significant update: Added new theorems on calibration sample   complexity and effective noise models. Expanded discussion on time-dependent   Markovian and non-Markovian noise. Included 8 new figures presenting results   on method robustness and calibration sample overhead. 28 pages and 13 figures   in total</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Extracting information efficiently from quantum systems is a major component of quantum information processing tasks. Randomized measurements, or classical shadows, enable predicting many properties of arbitrary quantum states using few measurements. While random single-qubit measurements are experimentally friendly and suitable for learning low-weight Pauli observables, they perform poorly for nonlocal observables. Prepending a shallow random quantum circuit before measurements maintains this experimental friendliness, but also has favorable sample complexities for observables beyond low-weight Paulis, including high-weight Paulis and global low-rank properties such as fidelity. However, in realistic scenarios, quantum noise accumulated with each additional layer of the shallow circuit biases the results. To address these challenges, we propose the \emph{robust shallow shadows protocol}. Our protocol uses Bayesian inference to learn the experimentally relevant noise model and mitigate it in postprocessing. This mitigation introduces a bias-variance trade-off: correcting for noise-induced bias comes at the cost of a larger estimator variance. Despite this increased variance, as we demonstrate on a superconducting quantum processor, our protocol correctly recovers state properties such as expectation values, fidelity, and entanglement entropy, while maintaining a lower sample complexity compared to the random single qubit measurement scheme. We also theoretically analyze the effects of noise on sample complexity and show how the optimal choice of the shallow shadow depth varies with noise strength. This combined theoretical and experimental analysis positions the robust shallow shadow protocol as a scalable, robust, and sample-efficient protocol for characterizing quantum states on current quantum computing platforms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.18213</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.18213</id><created>2024-02-28</created><updated>2025-02-04</updated><authors><author><keyname>Sukthanker</keyname><forenames>Rhea Sanjay</forenames></author><author><keyname>Zela</keyname><forenames>Arber</forenames></author><author><keyname>Staffler</keyname><forenames>Benedikt</forenames></author><author><keyname>Dooley</keyname><forenames>Samuel</forenames></author><author><keyname>Grabocka</keyname><forenames>Josif</forenames></author><author><keyname>Hutter</keyname><forenames>Frank</forenames></author></authors><title>Multi-objective Differentiable Neural Architecture Search</title><categories>cs.LG cs.CV stat.ML</categories><comments>44 pages, 34 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Pareto front profiling in multi-objective optimization (MOO), i.e., finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives that require training a neural network. Typically, in MOO for neural architecture search (NAS), we aim to balance performance and hardware metrics across devices. Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a computationally expensive search for each constraint. In this work, we propose a novel NAS algorithm that encodes user preferences to trade-off performance and hardware metrics, yielding representative and diverse architectures across multiple devices in just a single search run. To this end, we parameterize the joint architectural distribution across devices and multiple objectives via a hypernetwork that can be conditioned on hardware features and preference vectors, enabling zero-shot transferability to new devices. Extensive experiments involving up to 19 hardware devices and 3 different objectives demonstrate the effectiveness and scalability of our method. Finally, we show that, without any additional costs, our method outperforms existing MOO NAS methods across a broad range of qualitatively different search spaces and datasets, including MobileNetV3 on ImageNet-1k, an encoder-decoder transformer space for machine translation and a decoder-only space for language modelling. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.00165</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.00165</id><created>2024-02-29</created><updated>2025-02-05</updated><authors><author><keyname>Zhang</keyname><forenames>Yunyi</forenames></author><author><keyname>Yang</keyname><forenames>Ruozhen</forenames></author><author><keyname>Xu</keyname><forenames>Xueqiang</forenames></author><author><keyname>Li</keyname><forenames>Rui</forenames></author><author><keyname>Xiao</keyname><forenames>Jinfeng</forenames></author><author><keyname>Shen</keyname><forenames>Jiaming</forenames></author><author><keyname>Han</keyname><forenames>Jiawei</forenames></author></authors><title>TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text   Classification with Minimal Supervision</title><categories>cs.CL cs.LG</categories><comments>Accepted to WWW 2025 Research Track</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Hierarchical text classification aims to categorize each document into a set of classes in a label taxonomy, which is a fundamental web text mining task with broad applications such as web content analysis and semantic indexing. Most earlier works focus on fully or semi-supervised methods that require a large amount of human annotated data which is costly and time-consuming to acquire. To alleviate human efforts, in this paper, we work on hierarchical text classification with a minimal amount of supervision: using the sole class name of each node as the only supervision. Recently, large language models (LLM) have shown competitive performance on various tasks through zero-shot prompting, but this method performs poorly in the hierarchical setting because it is ineffective to include the large and structured label space in a prompt. On the other hand, previous weakly-supervised hierarchical text classification methods only utilize the raw taxonomy skeleton and ignore the rich information hidden in the text corpus that can serve as additional class-indicative features. To tackle the above challenges, we propose TELEClass, Taxonomy Enrichment and LLM-Enhanced weakly-supervised hierarchical text Classification, which combines the general knowledge of LLMs and task-specific features mined from an unlabeled corpus. TELEClass automatically enriches the raw taxonomy with class-indicative features for better label space understanding and utilizes novel LLM-based data annotation and generation methods specifically tailored for the hierarchical setting. Experiments show that TELEClass can significantly outperform previous baselines while achieving comparable performance to zero-shot prompting of LLMs with drastically less inference cost. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.02233</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.02233</id><created>2024-03-04</created><updated>2025-02-05</updated><authors><author><keyname>Huang</keyname><forenames>Yu</forenames></author><author><keyname>Wen</keyname><forenames>Zixin</forenames></author><author><keyname>Chi</keyname><forenames>Yuejie</forenames></author><author><keyname>Liang</keyname><forenames>Yingbin</forenames></author></authors><title>A Theoretical Analysis of Self-Supervised Learning for Vision   Transformers</title><categories>cs.LG math.OC stat.ML</categories><comments>Accepted by ICLR 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-supervised learning has become a cornerstone in computer vision, primarily divided into reconstruction-based methods like masked autoencoders (MAE) and discriminative methods such as contrastive learning (CL). Recent empirical observations reveal that MAE and CL capture different types of representations: CL tends to focus on global patterns, while MAE adeptly captures both global and subtle local information simultaneously. Despite a flurry of recent empirical investigations to shed light on this difference, theoretical understanding remains limited, especially on the dominant architecture vision transformers (ViTs). In this paper, to provide rigorous insights, we model the visual data distribution by considering two types of spatial features: dominant global features and comparatively minuscule local features, and study the impact of imbalance among these features. We analyze the training dynamics of one-layer softmax-based ViTs on both MAE and CL objectives using gradient descent. Our analysis shows that as the degree of feature imbalance varies, ViTs trained with the MAE objective effectively learn both global and local features to achieve near-optimal reconstruction, while the CL-trained ViTs favor predominantly global features, even under mild imbalance. These results provide a theoretical explanation for distinct behaviors of MAE and CL observed in empirical studies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.02780</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.02780</id><created>2024-03-05</created><updated>2025-02-04</updated><authors><author><keyname>Nosaka</keyname><forenames>Keiyu</forenames></author><author><keyname>Takano</keyname><forenames>Yuichi</forenames></author><author><keyname>Yoshise</keyname><forenames>Akiko</forenames></author></authors><title>Data Collaboration Analysis with Orthonormal Basis Selection and   Alignment</title><categories>cs.LG math.OC</categories><comments>13 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Data Collaboration (DC) analysis offers a privacy-preserving approach to multi-source machine learning by enabling participants to train a shared model without revealing their raw data. Instead, each participant shares only linearly transformed data through a non-iterative communication protocol, thereby mitigating both privacy risks and communication overhead. The core idea of DC is that while each participant obfuscates their data with a secret linear transformation (or basis), the aggregator aligns these secret bases to a chosen target basis \textit{without knowing the secret bases}. Although DC theory suggests that any target basis spanning the same subspace as the secret bases should suffice, empirical evidence reveals that the choice of target basis can substantially influence model performance. To address this discrepancy, we propose \textbf{Orthonormal DC (ODC)}, a novel framework that enforces orthonormal constraints during the basis selection and alignment phases. Unlike conventional DC -- which allows arbitrary target bases -- ODC restricts the target to orthonormal bases, rendering the specific choice of basis negligible concerning model performance. Furthermore, the alignment step in ODC reduces to the \textbf{Orthogonal Procrustes Problem}, which admits a closed-form solution with favorable computational properties. Empirical evaluations demonstrate that ODC achieves higher accuracy and improved efficiency compared to existing DC methods, aligning with our theoretical findings. Additional evaluations assess performance in non-ideal scenarios with heterogenous distributions, also showing the best overall performance for our method. These findings position ODC as a direct and effective enhancement to current DC frameworks without compromising privacy or communication overhead when orthonormality constraints are applicable. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.02910</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.02910</id><created>2024-03-05</created><updated>2025-02-05</updated><authors><author><keyname>Tao</keyname><forenames>Xijia</forenames></author><author><keyname>Zhong</keyname><forenames>Shuai</forenames></author><author><keyname>Li</keyname><forenames>Lei</forenames></author><author><keyname>Liu</keyname><forenames>Qi</forenames></author><author><keyname>Kong</keyname><forenames>Lingpeng</forenames></author></authors><title>ImgTrojan: Jailbreaking Vision-Language Models with ONE Image</title><categories>cs.CV cs.AI</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  There has been an increasing interest in the alignment of large language models (LLMs) with human values. However, the safety issues of their integration with a vision module, or vision language models (VLMs), remain relatively underexplored. In this paper, we propose a novel jailbreaking attack against VLMs, aiming to bypass their safety barrier when a user inputs harmful instructions. A scenario where our poisoned (image, text) data pairs are included in the training data is assumed. By replacing the original textual captions with malicious jailbreak prompts, our method can perform jailbreak attacks with the poisoned images. Moreover, we analyze the effect of poison ratios and positions of trainable parameters on our attack's success rate. For evaluation, we design two metrics to quantify the success rate and the stealthiness of our attack. Together with a list of curated harmful instructions, a benchmark for measuring attack efficacy is provided. We demonstrate the efficacy of our attack by comparing it with baseline methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.03269</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.03269</id><created>2024-03-05</created><authors><author><keyname>Arnob</keyname><forenames>Raihan Islam</forenames></author><author><keyname>Stein</keyname><forenames>Gregory J.</forenames></author></authors><title>Active Information Gathering for Long-Horizon Navigation Under   Uncertainty by Learning the Value of Information</title><categories>cs.RO</categories><comments>Submitted at IROS'24. arXiv admin note: text overlap with   arXiv:2307.14501</comments><doi>10.1109/IROS58592.2024.10801868</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We address the task of long-horizon navigation in partially mapped environments for which active gathering of information about faraway unseen space is essential for good behavior. We present a novel planning strategy that, at training time, affords tractable computation of the value of information associated with revealing potentially informative regions of unseen space, data used to train a graph neural network to predict the goodness of temporally-extended exploratory actions. Our learning-augmented model-based planning approach predicts the expected value of information of revealing unseen space and is capable of using these predictions to actively seek information and so improve long-horizon navigation. Across two simulated office-like environments, our planner outperforms competitive learned and non-learned baseline navigation strategies, achieving improvements of up to 63.76% and 36.68%, demonstrating its capacity to actively seek performance-critical information. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.03726</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.03726</id><created>2024-03-06</created><updated>2025-02-05</updated><authors><author><keyname>Meshchaninov</keyname><forenames>Viacheslav</forenames></author><author><keyname>Strashnov</keyname><forenames>Pavel</forenames></author><author><keyname>Shevtsov</keyname><forenames>Andrey</forenames></author><author><keyname>Nikolaev</keyname><forenames>Fedor</forenames></author><author><keyname>Ivanisenko</keyname><forenames>Nikita</forenames></author><author><keyname>Kardymon</keyname><forenames>Olga</forenames></author><author><keyname>Vetrov</keyname><forenames>Dmitry</forenames></author></authors><title>Diffusion on language model encodings for protein sequence generation</title><categories>cs.LG cs.AI q-bio.BM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protein sequence design has seen significant advances through discrete diffusion and autoregressive approaches, yet the potential of continuous diffusion remains underexplored. Here, we present DiMA, a latent diffusion framework that operates on protein language model representations. Through systematic exploration of architectural choices and diffusion components, we develop a robust methodology that generalizes across multiple protein encoders ranging from 8M to 3B parameters. We demonstrate that our framework achieves consistently high performance across sequence-only (ESM-2, ESMc), dual-decodable (CHEAP), and multimodal (SaProt) representations using the same architecture and training approach. We extensively evaluate existing methods alongside DiMA using multiple metrics across two protein modalities, covering quality, diversity, novelty, and distribution matching of generated proteins. DiMA consistently produces novel, high-quality and diverse protein sequences and achieves strong results compared to baselines such as autoregressive, discrete diffusion and flow matching language models. The model demonstrates versatile functionality, supporting conditional generation tasks including protein family-generation, motif scaffolding and infilling, and fold-specific sequence design. This work provides a universal continuous diffusion framework for protein sequence generation, offering both architectural insights and practical applicability across various protein design scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.10020</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.10020</id><created>2024-03-15</created><updated>2025-02-05</updated><authors><author><keyname>Luo</keyname><forenames>Yiyang</forenames></author><author><keyname>Lin</keyname><forenames>Ke</forenames></author><author><keyname>Gu</keyname><forenames>Chao</forenames></author><author><keyname>Hou</keyname><forenames>Jiahui</forenames></author><author><keyname>Wen</keyname><forenames>Lijie</forenames></author><author><keyname>Luo</keyname><forenames>Ping</forenames></author></authors><title>Lost in Overlap: Exploring Logit-based Watermark Collision in LLMs</title><categories>cs.CL cs.MM</categories><comments>Long Paper, 9 pages, accepted at NAACL 2025 Findings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of large language models (LLMs) in generating content raises concerns about text copyright. Watermarking methods, particularly logit-based approaches, embed imperceptible identifiers into text to address these challenges. However, the widespread usage of watermarking across diverse LLMs has led to an inevitable issue known as watermark collision during common tasks, such as paraphrasing or translation. In this paper, we introduce watermark collision as a novel and general philosophy for watermark attacks, aimed at enhancing attack performance on top of any other attacking methods. We also provide a comprehensive demonstration that watermark collision poses a threat to all logit-based watermark algorithms, impacting not only specific attack scenarios but also downstream applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.11022</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.11022</id><created>2024-03-16</created><updated>2025-02-05</updated><authors><author><keyname>Banchio</keyname><forenames>Martino</forenames></author><author><keyname>Mehta</keyname><forenames>Aranyak</forenames></author><author><keyname>Perlroth</keyname><forenames>Andres</forenames></author></authors><title>Ads in Conversations</title><categories>econ.TH cs.GT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study the optimal placement of advertisements for interactive platforms like conversational AI assistants. Importantly, conversations add a feature absent in canonical search markets -- time. The evolution of a conversation is informative about ad qualities, thus a platform could delay ad delivery to improve selection. However, delay endogenously shapes the supply of quality ads, possibly affecting revenue. We characterize the equilibria of first- and second-price auctions where the platform can commit to the auction format but not to its timing. We document sharp differences in the mechanisms' outcomes: first-price auctions are efficient but delay ad delivery, while second-price auctions avoid delay but allocate inefficiently. Revenue may be arbitrarily larger in a second-price auction than in a first-price auction. Optimal reserve prices alleviate these differences but flip the revenue ordering. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.11784</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.11784</id><created>2024-03-18</created><updated>2024-08-19</updated><authors><author><keyname>Baumann</keyname><forenames>Nicolas</forenames></author><author><keyname>Ghignone</keyname><forenames>Edoardo</forenames></author><author><keyname>Kühne</keyname><forenames>Jonas</forenames></author><author><keyname>Bastuck</keyname><forenames>Niklas</forenames></author><author><keyname>Becker</keyname><forenames>Jonathan</forenames></author><author><keyname>Imholz</keyname><forenames>Nadine</forenames></author><author><keyname>Kränzlin</keyname><forenames>Tobias</forenames></author><author><keyname>Lim</keyname><forenames>Tian Yi</forenames></author><author><keyname>Lötscher</keyname><forenames>Michael</forenames></author><author><keyname>Schwarzenbach</keyname><forenames>Luca</forenames></author><author><keyname>Tognoni</keyname><forenames>Luca</forenames></author><author><keyname>Vogt</keyname><forenames>Christian</forenames></author><author><keyname>Carron</keyname><forenames>Andrea</forenames></author><author><keyname>Magno</keyname><forenames>Michele</forenames></author></authors><title>ForzaETH Race Stack -- Scaled Autonomous Head-to-Head Racing on Fully   Commercial off-the-Shelf Hardware</title><categories>cs.RO cs.SE cs.SY eess.SY</categories><comments>This paper has been accepted at the Journal of Field Robotics</comments><report-no>ROB22429</report-no><journal-ref>Journal of Field Robotics (Early View, 2024)</journal-ref><doi>10.1002/rob.22429</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous racing in robotics combines high-speed dynamics with the necessity for reliability and real-time decision-making. While such racing pushes software and hardware to their limits, many existing full-system solutions necessitate complex, custom hardware and software, and usually focus on Time-Trials rather than full unrestricted Head-to-Head racing, due to financial and safety constraints. This limits their reproducibility, making advancements and replication feasible mostly for well-resourced laboratories with comprehensive expertise in mechanical, electrical, and robotics fields. Researchers interested in the autonomy domain but with only partial experience in one of these fields, need to spend significant time with familiarization and integration. The ForzaETH Race Stack addresses this gap by providing an autonomous racing software platform designed for F1TENTH, a 1:10 scaled Head-to-Head autonomous racing competition, which simplifies replication by using commercial off-the-shelf hardware. This approach enhances the competitive aspect of autonomous racing and provides an accessible platform for research and development in the field. The ForzaETH Race Stack is designed with modularity and operational ease of use in mind, allowing customization and adaptability to various environmental conditions, such as track friction and layout. Capable of handling both Time-Trials and Head-to-Head racing, the stack has demonstrated its effectiveness, robustness, and adaptability in the field by winning the official F1TENTH international competition multiple times. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.14753</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.14753</id><created>2024-03-21</created><updated>2025-02-05</updated><authors><author><keyname>Evans</keyname><forenames>Ethan N.</forenames></author><author><keyname>Cook</keyname><forenames>Matthew</forenames></author><author><keyname>Bradshaw</keyname><forenames>Zachary P.</forenames></author><author><keyname>LaBorde</keyname><forenames>Margarite L.</forenames></author></authors><title>Learning with SASQuaTCh: a Novel Variational Quantum Transformer   Architecture with Kernel-Based Self-Attention</title><categories>quant-ph cs.LG</categories><comments>12 pages, 8 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The recent exploding growth in size of state-of-the-art machine learning models highlights a well-known issue where exponential parameter growth, which has grown to trillions as in the case of the Generative Pre-trained Transformer (GPT), leads to training time and memory requirements which limit their advancement in the near term. The predominant models use the so-called transformer network and have a large field of applicability, including predicting text and images, classification, and even predicting solutions to the dynamics of physical systems. Here we present a variational quantum circuit architecture named Self-Attention Sequential Quantum Transformer Channel (SASQuaTCh), which builds networks of qubits that perform analogous operations of the transformer network, namely the keystone self-attention operation, and leads to an exponential improvement in parameter complexity and run-time complexity over its classical counterpart. Our approach leverages recent insights from kernel-based operator learning in the context of predicting spatiotemporal systems to represent deep layers of a vision transformer network using simple gate operations and a set of multi-dimensional quantum Fourier transforms. To validate our approach, we consider image classification tasks in simulation and with hardware, where with only 9 qubits and a handful of parameters we are able to simultaneously embed and classify a grayscale image of handwritten digits with high accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.02043</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.02043</id><created>2024-04-02</created><updated>2025-02-04</updated><authors><author><keyname>Dementieva</keyname><forenames>Daryna</forenames></author><author><keyname>Khylenko</keyname><forenames>Valeriia</forenames></author><author><keyname>Groh</keyname><forenames>Georg</forenames></author></authors><title>Cross-lingual Text Classification Transfer: The Case of Ukrainian</title><categories>cs.CL cs.AI</categories><comments>COLING2025, main, short</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Despite the extensive amount of labeled datasets in the NLP text classification field, the persistent imbalance in data availability across various languages remains evident. To support further fair development of NLP models, exploring the possibilities of effective knowledge transfer to new languages is crucial. Ukrainian, in particular, stands as a language that still can benefit from the continued refinement of cross-lingual methodologies. Due to our knowledge, there is a tremendous lack of Ukrainian corpora for typical text classification tasks, i.e., different types of style, or harmful speech, or texts relationships. However, the amount of resources required for such corpora collection from scratch is understandable. In this work, we leverage the state-of-the-art advances in NLP, exploring cross-lingual knowledge transfer methods avoiding manual data curation: large multilingual encoders and translation systems, LLMs, and language adapters. We test the approaches on three text classification tasks -- toxicity classification, formality classification, and natural language inference (NLI) -- providing the ``recipe'' for the optimal setups for each task. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.03515</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.03515</id><created>2024-04-04</created><updated>2025-02-05</updated><authors><author><keyname>Pontiggia</keyname><forenames>Francesco</forenames></author><author><keyname>Bartocci</keyname><forenames>Ezio</forenames></author><author><keyname>Chiari</keyname><forenames>Michele</forenames></author></authors><title>Model Checking Probabilistic Operator Precedence Automata</title><categories>cs.LO cs.PL</categories><comments>37 pages, 9 figures</comments><acm-class>F.3.1; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of model checking context-free specifications for probabilistic pushdown automata, which has relevant applications in the verification of recursive probabilistic programs. Operator Precedence Languages (OPLs) are an expressive subclass of context-free languages suitable for model checking recursive programs. The derived Precedence Oriented Temporal Logic (POTL) can express fundamental OPL specifications such as pre/post-conditions and exception safety.   We introduce probabilistic Operator Precedence Automata (pOPA), a class of probabilistic pushdown automata whose traces are OPLs, and study their model checking problem against POTL specifications. We identify a fragment of POTL, called POTLf$\chi$, for which we develop an EXPTIME algorithm for qualitative probabilistic model checking, and an EXPSPACE algorithm for the quantitative variant. The algorithms rely on the property of separation of automata generated from POTLf$\chi$ formulas. The same property allows us to employ these algorithms for model checking pOPA against Linear Temporal Logic (LTL) specifications. POTLf$\chi$ is then the first context-free logic for which an optimal probabilistic model checking algorithm has been developed, matching its EXPTIME lower bound in complexity. In comparison, the best known algorithm for probabilistic model checking of CaRet, a prominent temporal logic based on Visibly Pushdown Languages (VPL), is doubly exponential. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.05403</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.05403</id><created>2024-04-08</created><updated>2025-02-04</updated><authors><author><keyname>Du</keyname><forenames>Jiacheng</forenames></author><author><keyname>Hu</keyname><forenames>Jiahui</forenames></author><author><keyname>Wang</keyname><forenames>Zhibo</forenames></author><author><keyname>Sun</keyname><forenames>Peng</forenames></author><author><keyname>Gong</keyname><forenames>Neil Zhenqiang</forenames></author><author><keyname>Ren</keyname><forenames>Kui</forenames></author><author><keyname>Chen</keyname><forenames>Chun</forenames></author></authors><title>SoK: On Gradient Leakage in Federated Learning</title><categories>cs.CR cs.AI</categories><comments>Accepted to USENIX Security'25</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Federated learning (FL) facilitates collaborative model training among multiple clients without raw data exposure. However, recent studies have shown that clients' private training data can be reconstructed from shared gradients in FL, a vulnerability known as gradient inversion attacks (GIAs). While GIAs have demonstrated effectiveness under \emph{ideal settings and auxiliary assumptions}, their actual efficacy against \emph{practical FL systems} remains under-explored. To address this gap, we conduct a comprehensive study on GIAs in this work. We start with a survey of GIAs that establishes a timeline to trace their evolution and develops a systematization to uncover their inherent threats. By rethinking GIA in practical FL systems, three fundamental aspects influencing GIA's effectiveness are identified: \textit{training setup}, \textit{model}, and \textit{post-processing}. Guided by these aspects, we perform extensive theoretical and empirical evaluations of SOTA GIAs across diverse settings. Our findings highlight that GIA is notably \textit{constrained}, \textit{fragile}, and \textit{easily defensible}. Specifically, GIAs exhibit inherent limitations against practical local training settings. Additionally, their effectiveness is highly sensitive to the trained model, and even simple post-processing techniques applied to gradients can serve as effective defenses. Our work provides crucial insights into the limited threats of GIAs in practical FL systems. By rectifying prior misconceptions, we hope to inspire more accurate and realistic investigations on this topic. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.07066</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.07066</id><created>2024-04-10</created><updated>2025-02-04</updated><authors><author><keyname>Jin</keyname><forenames>Mingyu</forenames></author><author><keyname>Yu</keyname><forenames>Qinkai</forenames></author><author><keyname>Huang</keyname><forenames>Jingyuan</forenames></author><author><keyname>Zeng</keyname><forenames>Qingcheng</forenames></author><author><keyname>Wang</keyname><forenames>Zhenting</forenames></author><author><keyname>Hua</keyname><forenames>Wenyue</forenames></author><author><keyname>Zhao</keyname><forenames>Haiyan</forenames></author><author><keyname>Mei</keyname><forenames>Kai</forenames></author><author><keyname>Meng</keyname><forenames>Yanda</forenames></author><author><keyname>Ding</keyname><forenames>Kaize</forenames></author><author><keyname>Yang</keyname><forenames>Fan</forenames></author><author><keyname>Du</keyname><forenames>Mengnan</forenames></author><author><keyname>Zhang</keyname><forenames>Yongfeng</forenames></author></authors><title>Exploring Concept Depth: How Large Language Models Acquire Knowledge and   Concept at Different Layers?</title><categories>cs.CL cs.AI cs.LG</categories><comments>COLING 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large language models (LLMs) have shown remarkable performances across a wide range of tasks. However, the mechanisms by which these models encode tasks of varying complexities remain poorly understood. In this paper, we explore the hypothesis that LLMs process concepts of varying complexities in different layers, introducing the idea of "Concept Depth" to suggest that more complex concepts are typically acquired in deeper layers. Specifically, we categorize concepts based on their level of abstraction, defining them in the order of increasing complexity within factual, emotional, and inferential tasks. We conduct extensive probing experiments using layer-wise representations across various LLM families (Gemma, LLaMA, Qwen) on various datasets spanning the three domains of tasks. Our findings reveal that models could efficiently conduct probing for simpler tasks in shallow layers, and more complex tasks typically necessitate deeper layers for accurate understanding. Additionally, we examine how external factors, such as adding noise to the input and quantizing the model weights, might affect layer-wise representations. Our findings suggest that these factors can impede the development of a conceptual understanding of LLMs until deeper layers are explored. We hope that our proposed concept and experimental insights will enhance the understanding of the mechanisms underlying LLMs. Our codes are available at https://github.com/Luckfort/CD. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.10017</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.10017</id><created>2024-04-14</created><authors><author><keyname>Eisenmann</keyname><forenames>Simon</forenames></author><author><keyname>Hein</keyname><forenames>Daniel</forenames></author><author><keyname>Udluft</keyname><forenames>Steffen</forenames></author><author><keyname>Runkler</keyname><forenames>Thomas A.</forenames></author></authors><title>Model-based Offline Quantum Reinforcement Learning</title><categories>quant-ph cs.AI cs.LG</categories><doi>10.1109/QCE60285.2024.00175</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the first algorithm for model-based offline quantum reinforcement learning and demonstrates its functionality on the cart-pole benchmark. The model and the policy to be optimized are each implemented as variational quantum circuits. The model is trained by gradient descent to fit a pre-recorded data set. The policy is optimized with a gradient-free optimization scheme using the return estimate given by the model as the fitness function. This model-based approach allows, in principle, full realization on a quantum computer during the optimization phase and gives hope that a quantum advantage can be achieved as soon as sufficiently powerful quantum computers are available. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.10023</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.10023</id><created>2024-04-15</created><updated>2025-02-05</updated><authors><author><keyname>Gaikwad</keyname><forenames>Ajinkya</forenames></author><author><keyname>Kumar</keyname><forenames>Hitendra</forenames></author><author><keyname>Maity</keyname><forenames>Soumen</forenames></author></authors><title>Parameterized Algorithms for Editing to Uniform Cluster Graph</title><categories>cs.DS cs.CC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study the parameterized complexity of transforming graphs into Uniform Cluster graphs, where each component is an equal-sized clique. We consider Uniform Cluster Vertex Deletion (UCVD), Uniform Cluster Edge Deletion (UCED), Uniform Cluster Edge Addition (UCEA), Uniform Cluster Edge Editing (UCEE), Uniform Cluster Exclusive Vertex Splitting (UCEVS), and Uniform Cluster Inclusive Vertex Splitting (UCIVS). For UCVD, we provide a vertex kernel of size $\mathcal{O}(k^{3})$ and an FPT algorithm with running time $2^{k} \cdot n^{\mathcal{O}(1)}$, improving the known $3^{k} \cdot n^{\mathcal{O}(1)}$ algorithm. For edge-based variants, we obtain a $\mathcal{O}(k^{2})$ vertex kernel for UCEE and linear vertex kernels for UCED and UCEA, improving the best-known results. Additionally, we present a $1.47^{k} \cdot n^{\mathcal{O}(1)}$ algorithm for UCED, improving upon the previous $2^{k} \cdot n^{\mathcal{O}(1)}$ bound. We develop a sub-exponential algorithm for UCED on everywhere dense graphs by reducing it to $d$-Way Cut. Lastly, we study vertex splitting operations and provide vertex kernels of size $4k$ for both UCIVS and UCEVS. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.10718</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.10718</id><created>2024-04-16</created><updated>2025-02-05</updated><authors><author><keyname>Lin</keyname><forenames>Zhi-Yi</forenames></author><author><keyname>Chew</keyname><forenames>Jouh Yeong</forenames></author><author><keyname>van Gemert</keyname><forenames>Jan</forenames></author><author><keyname>Zhang</keyname><forenames>Xucong</forenames></author></authors><title>GazeHTA: End-to-end Gaze Target Detection with Head-Target Association</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Precisely detecting which object a person is paying attention to is critical for human-robot interaction since it provides important cues for the next action from the human user. We propose an end-to-end approach for gaze target detection: predicting a head-target connection between individuals and the target image regions they are looking at. Most of the existing methods use independent components such as off-the-shelf head detectors or have problems in establishing associations between heads and gaze targets. In contrast, we investigate an end-to-end multi-person Gaze target detection framework with Heads and Targets Association (GazeHTA), which predicts multiple head-target instances based solely on input scene image. GazeHTA addresses challenges in gaze target detection by (1) leveraging a pre-trained diffusion model to extract scene features for rich semantic understanding, (2) re-injecting a head feature to enhance the head priors for improved head understanding, and (3) learning a connection map as the explicit visual associations between heads and gaze targets. Our extensive experimental results demonstrate that GazeHTA outperforms state-of-the-art gaze target detection methods and two adapted diffusion-based baselines on two standard datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.12488</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.12488</id><created>2024-04-18</created><updated>2025-02-05</updated><authors><author><keyname>Sobieski</keyname><forenames>Bartlomiej</forenames></author><author><keyname>Biecek</keyname><forenames>Przemysław</forenames></author></authors><title>Global Counterfactual Directions</title><categories>cs.LG cs.AI cs.CV</categories><comments>ECCV 2024</comments><doi>10.1007/978-3-031-73036-8_5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite increasing progress in development of methods for generating visual counterfactual explanations, especially with the recent rise of Denoising Diffusion Probabilistic Models, previous works consider them as an entirely local technique. In this work, we take the first step at globalizing them. Specifically, we discover that the latent space of Diffusion Autoencoders encodes the inference process of a given classifier in the form of global directions. We propose a novel proxy-based approach that discovers two types of these directions with the use of only single image in an entirely black-box manner. Precisely, g-directions allow for flipping the decision of a given classifier on an entire dataset of images, while h-directions further increase the diversity of explanations. We refer to them in general as Global Counterfactual Directions (GCDs). Moreover, we show that GCDs can be naturally combined with Latent Integrated Gradients resulting in a new black-box attribution method, while simultaneously enhancing the understanding of counterfactual explanations. We validate our approach on existing benchmarks and show that it generalizes to real-world use-cases. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.12569</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.12569</id><created>2024-04-18</created><authors><author><keyname>Wang</keyname><forenames>Zhenzhong</forenames></author><author><keyname>Zeng</keyname><forenames>Qingyuan</forenames></author><author><keyname>Lin</keyname><forenames>Wanyu</forenames></author><author><keyname>Jiang</keyname><forenames>Min</forenames></author><author><keyname>Tan</keyname><forenames>Kay Chen</forenames></author></authors><title>Multi-View Subgraph Neural Networks: Self-Supervised Learning with   Scarce Labeled Data</title><categories>cs.LG cs.AI</categories><doi>10.1109/TNNLS.2024.3443074</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While graph neural networks (GNNs) have become the de-facto standard for graph-based node classification, they impose a strong assumption on the availability of sufficient labeled samples. This assumption restricts the classification performance of prevailing GNNs on many real-world applications suffering from low-data regimes. Specifically, features extracted from scarce labeled nodes could not provide sufficient supervision for the unlabeled samples, leading to severe over-fitting. In this work, we point out that leveraging subgraphs to capture long-range dependencies can augment the representation of a node with homophily properties, thus alleviating the low-data regime. However, prior works leveraging subgraphs fail to capture the long-range dependencies among nodes. To this end, we present a novel self-supervised learning framework, called multi-view subgraph neural networks (Muse), for handling long-range dependencies. In particular, we propose an information theory-based identification mechanism to identify two types of subgraphs from the views of input space and latent space, respectively. The former is to capture the local structure of the graph, while the latter captures the long-range dependencies among nodes. By fusing these two views of subgraphs, the learned representations can preserve the topological properties of the graph at large, including the local structure and long-range dependencies, thus maximizing their expressiveness for downstream node classification tasks. Experimental results show that Muse outperforms the alternative methods on node classification tasks with limited labeled data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.14730</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.14730</id><created>2024-04-23</created><updated>2025-02-04</updated><authors><author><keyname>Bateni</keyname><forenames>MohammadHossein</forenames></author><author><keyname>Dhulipala</keyname><forenames>Laxman</forenames></author><author><keyname>Gowda</keyname><forenames>Kishen N</forenames></author><author><keyname>Hershkowitz</keyname><forenames>D Ellis</forenames></author><author><keyname>Jayaram</keyname><forenames>Rajesh</forenames></author><author><keyname>Łącki</keyname><forenames>Jakub</forenames></author></authors><title>It's Hard to HAC with Average Linkage!</title><categories>cs.DS cs.CC cs.DC</categories><comments>At ICALP 2024. Updated AM CC hardness proof</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Average linkage Hierarchical Agglomerative Clustering (HAC) is an extensively studied and applied method for hierarchical clustering. Recent applications to massive datasets have driven significant interest in near-linear-time and efficient parallel algorithms for average linkage HAC.   We provide hardness results that rule out such algorithms. On the sequential side, we establish a runtime lower bound of $n^{3/2-\epsilon}$ on $n$ node graphs for sequential combinatorial algorithms under standard fine-grained complexity assumptions. This essentially matches the best-known running time for average linkage HAC. On the parallel side, we prove that average linkage HAC likely cannot be parallelized even on simple graphs by showing that it is CC-hard on trees of diameter $4$. On the possibility side, we demonstrate that average linkage HAC can be efficiently parallelized (i.e., it is in NC) on paths and can be solved in near-linear time when the height of the output cluster hierarchy is small. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.15745</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.15745</id><created>2024-04-24</created><updated>2025-02-05</updated><authors><author><keyname>Pavlou</keyname><forenames>Georgios E.</forenames></author><author><keyname>Pavlidou</keyname><forenames>Vasiliki</forenames></author><author><keyname>Harmandaris</keyname><forenames>Vagelis</forenames></author></authors><title>Reconstructing the Magnetic Field in an Arbitrary Domain via Data-driven   Bayesian Methods and Numerical Simulations</title><categories>physics.comp-ph astro-ph.HE cs.NA math.NA</categories><comments>28 pages, 11 figures</comments><doi>10.3390/computation13020037</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Inverse problems are prevalent in numerous scientific and engineering disciplines, where the objective is to determine unknown parameters within a physical system using indirect measurements or observations. The inherent challenge lies in deducing the most probable parameter values that align with the collected data. This study introduces an algorithm for reconstructing parameters by addressing an inverse problem formulated through differential equations underpinned by uncertain boundary conditions or variant parameters. We adopt a Bayesian approach for parameter inference, delineating the establishment of prior, likelihood, and posterior distributions, and the subsequent resolution of the maximum a posteriori problem via numerical optimization techniques. The proposed algorithm is applied to the task of magnetic field reconstruction within a conical domain, demonstrating precise recovery of the true parameter values. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.17871</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.17871</id><created>2024-04-27</created><updated>2025-02-04</updated><authors><author><keyname>Zhang</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Jiang</keyname><forenames>Weipeng</forenames></author><author><keyname>Shen</keyname><forenames>Chao</forenames></author><author><keyname>Li</keyname><forenames>Qi</forenames></author><author><keyname>Wang</keyname><forenames>Qian</forenames></author><author><keyname>Lin</keyname><forenames>Chenhao</forenames></author><author><keyname>Guan</keyname><forenames>Xiaohong</forenames></author></authors><title>Deep Learning Library Testing: Definition, Methods and Challenges</title><categories>cs.SE cs.AI</categories><comments>37 pages, 10 figures, 5 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In recent years, software systems powered by deep learning (DL) techniques have significantly facilitated people's lives in many aspects. As the backbone of these DL systems, various DL libraries undertake the underlying optimization and computation. However, like traditional software, DL libraries are not immune to bugs, which can pose serious threats to users' personal property and safety. Studying the characteristics of DL libraries, their associated bugs, and the corresponding testing methods is crucial for enhancing the security of DL systems and advancing the widespread application of DL technology. This paper provides an overview of the testing research related to various DL libraries, discusses the strengths and weaknesses of existing methods, and provides guidance and reference for the application of the DL library. This paper first introduces the workflow of DL underlying libraries and the characteristics of three kinds of DL libraries involved, namely DL framework, DL compiler, and DL hardware library. It then provides definitions for DL underlying library bugs and testing. Additionally, this paper summarizes the existing testing methods and tools tailored to these DL libraries separately and analyzes their effectiveness and limitations. It also discusses the existing challenges of DL library testing and outlines potential directions for future research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.18410</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.18410</id><created>2024-04-28</created><updated>2025-02-05</updated><authors><author><keyname>Xu</keyname><forenames>Bowen</forenames></author><author><keyname>Wu</keyname><forenames>Shaoyu</forenames></author><author><keyname>Liu</keyname><forenames>Kai</forenames></author><author><keyname>Hu</keyname><forenames>Lulu</forenames></author></authors><title>Mixture-of-Instructions: Aligning Large Language Models via Mixture   Prompting</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  With the proliferation of large language models (LLMs), the comprehensive alignment of such models across multiple tasks has emerged as a critical area of research. Existing alignment methodologies primarily address single task, such as multi-turn dialogue, coding, mathematical problem-solving, and tool usage. Although there is a large amount of high-quality data available for those tasks, most of them provide only questions and answers without including the system prompt. Though a detailed analysis of the Qwen language model, we found that the system prompt has a significant impact on both training and inference processes of LLM. We attributes this phenomenon to overfitting to the system prompt. In address this issue, we introduce a novel technique termed Mixture-of-Instructions (MoI), which employs a strategy of instruction packing combined with diverse system prompts to boost the alignment efficiency of language models. We have also compiled a diverse set of seven benchmark datasets to rigorously evaluate the alignment efficacy of the MoI-enhanced language model. Our methodology was applied to the open-source Qwen-7B-chat model, culminating in the development of Qwen-SFT-MoI. This enhanced model demonstrates significant advancements in generative capabilities across coding, mathematics, and tool use tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.01079</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.01079</id><created>2024-05-02</created><updated>2024-07-09</updated><authors><author><keyname>Weissinger</keyname><forenames>Lukas</forenames></author><author><keyname>Hubmer</keyname><forenames>Simon</forenames></author><author><keyname>Stadler</keyname><forenames>Bernadett</forenames></author><author><keyname>Ramlau</keyname><forenames>Ronny</forenames></author></authors><title>Singular Value and Frame Decomposition-based Reconstruction for   Atmospheric Tomography</title><categories>math.NA cs.NA</categories><comments>31 pages, 11 figures</comments><doi>10.1515/9783111357270-008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Atmospheric tomography, the problem of reconstructing atmospheric turbulence profiles from wavefront sensor measurements, is an integral part of many adaptive optics systems used for enhancing the image quality of ground-based telescopes. Singular-value and frame decompositions of the underlying atmospheric tomography operator can reveal useful analytical information on this inverse problem, as well as serve as the basis of efficient numerical reconstruction algorithms. In this paper, we extend existing singular value decompositions to more realistic Sobolev settings including weighted inner products, and derive an explicit representation of a frame-based (approximate) solution operator. These investigations form the basis of efficient numerical solution methods, which we analyze via numerical simulations for the challenging, real-world Adaptive Optics system of the Extremely Large Telescope using the entirely MATLAB-based simulation tool MOST. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.01768</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.01768</id><created>2024-05-02</created><updated>2025-02-05</updated><authors><author><keyname>He</keyname><forenames>Jerry Zhi-Yang</forenames></author><author><keyname>Pandey</keyname><forenames>Sashrika</forenames></author><author><keyname>Schrum</keyname><forenames>Mariah L.</forenames></author><author><keyname>Dragan</keyname><forenames>Anca</forenames></author></authors><title>CoS: Enhancing Personalization and Mitigating Bias with Context Steering</title><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  When querying a large language model (LLM), the context, i.e. personal, demographic, and cultural information specific to an end-user, can significantly shape the response of the LLM. For example, asking the model to explain Newton's second law with the context "I am a toddler" yields a different answer compared to the context "I am a physics professor." Proper usage of the context enables the LLM to generate personalized responses, whereas inappropriate contextual influence can lead to stereotypical and potentially harmful generations (e.g. associating "female" with "housekeeper"). In practice, striking the right balance when leveraging context is a nuanced and challenging problem that is often situation-dependent. One common approach to address this challenge is to fine-tune LLMs on contextually appropriate responses. However, this approach is expensive, time-consuming, and not controllable for end-users in different situations. In this work, we propose Context Steering (CoS) - a simple training-free method that can be easily applied to autoregressive LLMs at inference time. By measuring the contextual influence in terms of token prediction likelihood and modulating it, our method enables practitioners to determine the appropriate level of contextual influence based on their specific use case and end-user base. We showcase a variety of applications of CoS including amplifying the contextual influence to achieve better personalization and mitigating unwanted influence for reducing model bias. In addition, we show that we can combine CoS with Bayesian Inference to quantify the extent of hate speech on the internet. We demonstrate the effectiveness of CoS on state-of-the-art LLMs and benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.02140</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.02140</id><created>2024-05-03</created><updated>2024-06-26</updated><authors><author><keyname>Correia</keyname><forenames>Alvaro H. C.</forenames></author><author><keyname>Massoli</keyname><forenames>Fabio Valerio</forenames></author><author><keyname>Louizos</keyname><forenames>Christos</forenames></author><author><keyname>Behboodi</keyname><forenames>Arash</forenames></author></authors><title>An Information Theoretic Perspective on Conformal Prediction</title><categories>cs.LG cs.IT math.IT stat.ML</categories><journal-ref>Advances in Neural Information Processing Systems 37 (NeurIPS   2024)</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Conformal Prediction (CP) is a distribution-free uncertainty estimation framework that constructs prediction sets guaranteed to contain the true answer with a user-specified probability. Intuitively, the size of the prediction set encodes a general notion of uncertainty, with larger sets associated with higher degrees of uncertainty. In this work, we leverage information theory to connect conformal prediction to other notions of uncertainty. More precisely, we prove three different ways to upper bound the intrinsic uncertainty, as described by the conditional entropy of the target variable given the inputs, by combining CP with information theoretical inequalities. Moreover, we demonstrate two direct and useful applications of such connection between conformal prediction and information theory: (i) more principled and effective conformal training objectives that generalize previous approaches and enable end-to-end training of machine learning models from scratch, and (ii) a natural mechanism to incorporate side information into conformal prediction. We empirically validate both applications in centralized and federated learning settings, showing our theoretical results translate to lower inefficiency (average prediction set size) for popular CP methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.05202</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.05202</id><created>2024-05-08</created><updated>2025-02-05</updated><authors><author><keyname>Chen</keyname><forenames>Yixin</forenames></author><author><keyname>Nath</keyname><forenames>Ankur</forenames></author><author><keyname>Peng</keyname><forenames>Chunli</forenames></author><author><keyname>Kuhnle</keyname><forenames>Alan</forenames></author></authors><title>Discretely Beyond $1/e$: Guided Combinatorial Algorithms for Submodular   Maximization</title><categories>cs.DS cs.DM cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For constrained, not necessarily monotone submodular maximization, all known approximation algorithms with ratio greater than $1/e$ require continuous ideas, such as queries to the multilinear extension of a submodular function and its gradient, which are typically expensive to simulate with the original set function. For combinatorial algorithms, the best known approximation ratios for both size and matroid constraint are obtained by a simple randomized greedy algorithm of Buchbinder et al. [9]: $1/e \approx 0.367$ for size constraint and $0.281$ for the matroid constraint in $\mathcal O (kn)$ queries, where $k$ is the rank of the matroid. In this work, we develop the first combinatorial algorithms to break the $1/e$ barrier: we obtain approximation ratio of $0.385$ in $\mathcal O (kn)$ queries to the submodular set function for size constraint, and $0.305$ for a general matroid constraint. These are achieved by guiding the randomized greedy algorithm with a fast local search algorithm. Further, we develop deterministic versions of these algorithms, maintaining the same ratio and asymptotic time complexity. Finally, we develop a deterministic, nearly linear time algorithm with ratio $0.377$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.06908</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.06908</id><created>2024-05-11</created><updated>2025-02-04</updated><authors><author><keyname>Banerjee</keyname><forenames>Rohan</forenames></author><author><keyname>Jenamani</keyname><forenames>Rajat Kumar</forenames></author><author><keyname>Vasudev</keyname><forenames>Sidharth</forenames></author><author><keyname>Nanavati</keyname><forenames>Amal</forenames></author><author><keyname>Dimitropoulou</keyname><forenames>Katherine</forenames></author><author><keyname>Dean</keyname><forenames>Sarah</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Tapomayukh</forenames></author></authors><title>To Ask or Not To Ask: Human-in-the-loop Contextual Bandits with   Applications in Robot-Assisted Feeding</title><categories>cs.RO</categories><comments>The second and third authors contributed equally. The last two   authors advised equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robot-assisted bite acquisition involves picking up food items with varying shapes, compliance, sizes, and textures. Fully autonomous strategies may not generalize efficiently across this diversity. We propose leveraging feedback from the care recipient when encountering novel food items. However, frequent queries impose a workload on the user. We formulate human-in-the-loop bite acquisition within a contextual bandit framework and introduce LinUCB-QG, a method that selectively asks for help using a predictive model of querying workload based on query types and timings. This model is trained on data collected in an online study involving 14 participants with mobility limitations, 3 occupational therapists simulating physical limitations, and 89 participants without limitations. We demonstrate that our method better balances task performance and querying workload compared to autonomous and always-querying baselines and adjusts its querying behavior to account for higher workload in users with mobility limitations. We validate this through experiments in a simulated food dataset and a user study with 19 participants, including one with severe mobility limitations. Please check out our project website at: http://emprise.cs.cornell.edu/hilbiteacquisition/ </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.07432</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.07432</id><created>2024-05-12</created><updated>2025-02-04</updated><authors><author><keyname>Hou</keyname><forenames>Boya</forenames></author><author><keyname>Sanjari</keyname><forenames>Sina</forenames></author><author><keyname>Dahlin</keyname><forenames>Nathan</forenames></author><author><keyname>Koppel</keyname><forenames>Alec</forenames></author><author><keyname>Bose</keyname><forenames>Subhonmesh</forenames></author></authors><title>Nonparametric Sparse Online Learning of the Koopman Operator</title><categories>stat.ML cs.LG cs.SY eess.SY</categories><comments>49 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The Koopman operator provides a powerful framework for representing the dynamics of general nonlinear dynamical systems. Data-driven techniques to learn the Koopman operator typically assume that the chosen function space is closed under system dynamics. In this paper, we study the Koopman operator via its action on the reproducing kernel Hilbert space (RKHS), and explore the mis-specified scenario where the dynamics may escape the chosen function space. We relate the Koopman operator to the conditional mean embeddings (CME) operator and then present an operator stochastic approximation algorithm to learn the Koopman operator iteratively with control over the complexity of the representation. We provide both asymptotic and finite-time last-iterate guarantees of the online sparse learning algorithm with trajectory-based sampling with an analysis that is substantially more involved than that for finite-dimensional stochastic approximation. Numerical examples confirm the effectiveness of the proposed algorithm. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.08962</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.08962</id><created>2024-05-14</created><authors><author><keyname>Maurya</keyname><forenames>Satvik</forenames></author><author><keyname>Mude</keyname><forenames>Chaithanya Naik</forenames></author><author><keyname>Lienhard</keyname><forenames>Benjamin</forenames></author><author><keyname>Tannu</keyname><forenames>Swamit</forenames></author></authors><title>Understanding Side-Channel Vulnerabilities in Superconducting Qubit   Readout Architectures</title><categories>quant-ph cs.CR</categories><doi>10.1109/QCE60285.2024.00138</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Frequency-multiplexing is an effective method to achieve resource-efficient superconducting qubit readout. Allowing multiple resonators to share a common feedline, the number of cables and passive components involved in the readout of a qubit can be drastically reduced. However, this improvement in scalability comes at the price of a crucial non-ideality -- an increased readout crosstalk. Prior works have targeted building better devices and discriminators to reduce its effects, as readout-crosstalk-induced qubit measurement errors are detrimental to the reliability of a quantum computer. However, in this work, we show that beyond the reliability of a system, readout crosstalk can introduce vulnerabilities in a system being shared among multiple users. These vulnerabilities are directly related to correlated errors due to readout crosstalk. These correlated errors can be exploited by nefarious attackers to predict the state of the victim qubits, resulting in information leakage. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.09369</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.09369</id><created>2024-05-15</created><updated>2025-02-05</updated><authors><author><keyname>Cui</keyname><forenames>Ziqiang</forenames></author><author><keyname>Wu</keyname><forenames>Haolun</forenames></author><author><keyname>He</keyname><forenames>Bowei</forenames></author><author><keyname>Cheng</keyname><forenames>Ji</forenames></author><author><keyname>Ma</keyname><forenames>Chen</forenames></author></authors><title>Diffusion-based Contrastive Learning for Sequential Recommendation</title><categories>cs.IR</categories><comments>Published as a full paper in CIKM'2024</comments><journal-ref>Proceedings of the 33rd ACM International Conference on   Information and Knowledge Management, 2024</journal-ref><doi>10.1145/3627673.3679655</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Contrastive learning has been effectively utilized to enhance the training of sequential recommendation models by leveraging informative self-supervised signals. Most existing approaches generate augmented views of the same user sequence through random augmentation and subsequently maximize their agreement in the representation space. However, these methods often neglect the rationality of the augmented samples. Due to significant uncertainty, random augmentation can disrupt the semantic information and interest evolution patterns inherent in the original user sequences. Moreover, pulling semantically inconsistent sequences closer in the representation space can render the user sequence embeddings insensitive to variations in user preferences, which contradicts the primary objective of sequential recommendation. To address these limitations, we propose the Context-aware Diffusion-based Contrastive Learning for Sequential Recommendation, named CaDiRec. The core idea is to leverage context information to generate more reasonable augmented views. Specifically, CaDiRec employs a context-aware diffusion model to generate alternative items for the given positions within a sequence. These generated items are aligned with their respective context information and can effectively replace the corresponding original items, thereby generating a positive view of the original sequence. By considering two different augmentations of the same user sequence, we can construct a pair of positive samples for contrastive learning. To ensure representation cohesion, we train the entire framework in an end-to-end manner, with shared item embeddings between the diffusion model and the recommendation model. Extensive experiments on five benchmark datasets demonstrate the advantages of our proposed method over existing baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.10121</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.10121</id><created>2024-05-16</created><updated>2025-02-05</updated><authors><author><keyname>Zhang</keyname><forenames>Bo</forenames></author><author><keyname>Ma</keyname><forenames>Hui</forenames></author><author><keyname>Ding</keyname><forenames>Jian</forenames></author><author><keyname>Wang</keyname><forenames>Jian</forenames></author><author><keyname>Xu</keyname><forenames>Bo</forenames></author><author><keyname>Lin</keyname><forenames>Hongfei</forenames></author></authors><title>Distilling Implicit Multimodal Knowledge into Large Language Models for   Zero-Resource Dialogue Generation</title><categories>cs.CL cs.MM</categories><comments>Accepted by Information Fusion. The code is available at   https://github.com/zhangbo-nlp/VIKDF</comments><doi>10.1016/j.inffus.2025.102985</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Integrating multimodal knowledge into large language models (LLMs) represents a significant advancement in dialogue generation capabilities. However, the effective incorporation of such knowledge in zero-resource scenarios remains a substantial challenge due to the scarcity of diverse, high-quality dialogue datasets. To address this, we propose the Visual Implicit Knowledge Distillation Framework (VIKDF), an innovative approach aimed at enhancing LLMs for enriched dialogue generation in zero-resource contexts by leveraging implicit multimodal knowledge. VIKDF comprises two main stages: knowledge distillation, using an Implicit Query Transformer to extract and encode visual implicit knowledge from image-text pairs into knowledge vectors; and knowledge integration, employing a novel Bidirectional Variational Information Fusion technique to seamlessly integrate these distilled vectors into LLMs. This enables the LLMs to generate dialogues that are not only coherent and engaging but also exhibit a deep understanding of the context through implicit multimodal cues, effectively overcoming the limitations of zero-resource scenarios. Our extensive experimentation across two dialogue datasets shows that VIKDF outperforms existing state-of-the-art models in generating high-quality dialogues. The code is available at https://github.com/zhangbo-nlp/VIKDF. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.11751</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.11751</id><created>2024-05-19</created><updated>2025-02-04</updated><authors><author><keyname>Lu</keyname><forenames>Yue M.</forenames></author><author><keyname>Letey</keyname><forenames>Mary I.</forenames></author><author><keyname>Zavatone-Veth</keyname><forenames>Jacob A.</forenames></author><author><keyname>Maiti</keyname><forenames>Anindita</forenames></author><author><keyname>Pehlevan</keyname><forenames>Cengiz</forenames></author></authors><title>Asymptotic theory of in-context learning by linear attention</title><categories>stat.ML cond-mat.dis-nn cs.LG</categories><comments>17 pages (main doc), 6 figures, and supplementary information (23   pages)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Transformers have a remarkable ability to learn and execute tasks based on examples provided within the input itself, without explicit prior training. It has been argued that this capability, known as in-context learning (ICL), is a cornerstone of Transformers' success, yet questions about the necessary sample complexity, pretraining task diversity, and context length for successful ICL remain unresolved. Here, we provide a precise answer to these questions in an exactly solvable model of ICL of a linear regression task by linear attention. We derive sharp asymptotics for the learning curve in a phenomenologically-rich scaling regime where the token dimension is taken to infinity; the context length and pretraining task diversity scale proportionally with the token dimension; and the number of pretraining examples scales quadratically. We demonstrate a double-descent learning curve with increasing pretraining examples, and uncover a phase transition in the model's behavior between low and high task diversity regimes: In the low diversity regime, the model tends toward memorization of training tasks, whereas in the high diversity regime, it achieves genuine in-context learning and generalization beyond the scope of pretrained tasks. These theoretical insights are empirically validated through experiments with both linear attention and full nonlinear Transformer architectures. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.11831</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.11831</id><created>2024-05-20</created><updated>2025-02-04</updated><authors><author><keyname>Shams</keyname><forenames>Siavash</forenames></author><author><keyname>Dindar</keyname><forenames>Sukru Samet</forenames></author><author><keyname>Jiang</keyname><forenames>Xilin</forenames></author><author><keyname>Mesgarani</keyname><forenames>Nima</forenames></author></authors><title>SSAMBA: Self-Supervised Audio Representation Learning with Mamba State   Space Model</title><categories>eess.AS cs.LG</categories><comments>Code at https://github.com/SiavashShams/ssamba</comments><journal-ref>2024 IEEE Spoken Language Technology Workshop (SLT), Macao, pp.   1053-1059</journal-ref><doi>10.1109/SLT61566.2024.10832304</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Transformers have revolutionized deep learning across various tasks, including audio representation learning, due to their powerful modeling capabilities. However, they often suffer from quadratic complexity in both GPU memory usage and computational inference time, affecting their efficiency. Recently, state space models (SSMs) like Mamba have emerged as a promising alternative, offering a more efficient approach by avoiding these complexities. Given these advantages, we explore the potential of SSM-based models in audio tasks. In this paper, we introduce Self-Supervised Audio Mamba (SSAMBA), the first self-supervised, attention-free, and SSM-based model for audio representation learning. SSAMBA leverages the bidirectional Mamba to capture complex audio patterns effectively. We incorporate a self-supervised pretraining framework that optimizes both discriminative and generative objectives, enabling the model to learn robust audio representations from large-scale, unlabeled datasets. We evaluated SSAMBA on various tasks such as audio classification, keyword spotting, and speaker identification. Our results demonstrate that SSAMBA outperforms the Self-Supervised Audio Spectrogram Transformer (SSAST) in most tasks. Notably, SSAMBA is approximately 92.7% faster in batch inference speed and 95.4% more memory-efficient than SSAST for the tiny model size with an input token size of 22k. These efficiency gains, combined with superior performance, underscore the effectiveness of SSAMBA's architectural innovation, making it a compelling choice for a wide range of audio processing applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.12221</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.12221</id><created>2024-05-20</created><updated>2025-02-04</updated><authors><author><keyname>Chen</keyname><forenames>Ziyang</forenames></author><author><keyname>Geng</keyname><forenames>Daniel</forenames></author><author><keyname>Owens</keyname><forenames>Andrew</forenames></author></authors><title>Images that Sound: Composing Images and Sounds on a Single Canvas</title><categories>cs.CV cs.LG cs.MM cs.SD eess.AS</categories><comments>Accepted to NeurIPS 2024. Project site:   https://ificl.github.io/images-that-sound/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectrograms are 2D representations of sound that look very different from the images found in our visual world. And natural images, when played as spectrograms, make unnatural sounds. In this paper, we show that it is possible to synthesize spectrograms that simultaneously look like natural images and sound like natural audio. We call these visual spectrograms images that sound. Our approach is simple and zero-shot, and it leverages pre-trained text-to-image and text-to-spectrogram diffusion models that operate in a shared latent space. During the reverse process, we denoise noisy latents with both the audio and image diffusion models in parallel, resulting in a sample that is likely under both models. Through quantitative evaluations and perceptual studies, we find that our method successfully generates spectrograms that align with a desired audio prompt while also taking the visual appearance of a desired image prompt. Please see our project page for video results: https://ificl.github.io/images-that-sound/ </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.12800</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.12800</id><created>2024-05-21</created><updated>2024-05-22</updated><authors><author><keyname>Ewers</keyname><forenames>Jan-Hendrik</forenames></author><author><keyname>Anderson</keyname><forenames>David</forenames></author><author><keyname>Thomson</keyname><forenames>Douglas</forenames></author></authors><title>Deep Reinforcement Learning for Time-Critical Wilderness Search And   Rescue Using Drones</title><categories>cs.RO cs.LG cs.SY eess.SY</categories><comments>16 pages, 19 figures. Submitted</comments><doi>10.3389/frobt.2024.1527095</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional search and rescue methods in wilderness areas can be time-consuming and have limited coverage. Drones offer a faster and more flexible solution, but optimizing their search paths is crucial. This paper explores the use of deep reinforcement learning to create efficient search missions for drones in wilderness environments. Our approach leverages a priori data about the search area and the missing person in the form of a probability distribution map. This allows the deep reinforcement learning agent to learn optimal flight paths that maximize the probability of finding the missing person quickly. Experimental results show that our method achieves a significant improvement in search times compared to traditional coverage planning and search planning algorithms. In one comparison, deep reinforcement learning is found to outperform other algorithms by over $160\%$, a difference that can mean life or death in real-world search operations. Additionally, unlike previous work, our approach incorporates a continuous action space enabled by cubature, allowing for more nuanced flight patterns. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.15231</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.15231</id><created>2024-05-24</created><updated>2025-02-05</updated><authors><author><keyname>Teng</keyname><forenames>Fei</forenames></author><author><keyname>Li</keyname><forenames>Haoyang</forenames></author><author><keyname>Di</keyname><forenames>Shimin</forenames></author><author><keyname>Chen</keyname><forenames>Lei</forenames></author></authors><title>Cardinality Estimation on Hyper-relational Knowledge Graphs</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cardinality Estimation (CE) for query is to estimate the number of results without execution, which is an effective index in query optimization. Recently, CE for queries over knowlege graph (KGs) with triple facts has achieved great success. To more precisely represent facts, current researchers propose hyper-relational KGs (HKGs) to represent a triple fact with qualifiers providing additional context to the fact. However, existing CE methods, such as sampling and summary methods over KGs, perform unsatisfactorily on HKGs due to the complexity of qualifiers. Learning-based CE methods do not utilize qualifier information to learn query representation accurately, leading to poor performance. Also, there is only one limited CE benchmark for HKG query, which is not comprehensive and only covers limited patterns. The lack of querysets over HKG also becomes a bottleneck to comprehensively investigate CE problems on HKGs. In this work, we first construct diverse and unbiased hyper-relational querysets over three popular HKGs for investigating CE. Besides, we also propose a novel qualifier-aware graph neural network (GNN) model that effectively incorporates qualifier information and adaptively combines outputs from multiple GNN layers, to accurately predict the cardinality. Our experiments demonstrate that our model outperforms all state-of-the-art CE methods over three benchmarks on popular HKGs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.15885</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.15885</id><created>2024-05-24</created><updated>2025-02-05</updated><authors><author><keyname>Zheng</keyname><forenames>Kaiwen</forenames></author><author><keyname>He</keyname><forenames>Guande</forenames></author><author><keyname>Chen</keyname><forenames>Jianfei</forenames></author><author><keyname>Bao</keyname><forenames>Fan</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author></authors><title>Diffusion Bridge Implicit Models</title><categories>cs.LG stat.ML</categories><comments>Accepted at ICLR 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Denoising diffusion bridge models (DDBMs) are a powerful variant of diffusion models for interpolating between two arbitrary paired distributions given as endpoints. Despite their promising performance in tasks like image translation, DDBMs require a computationally intensive sampling process that involves the simulation of a (stochastic) differential equation through hundreds of network evaluations. In this work, we take the first step in fast sampling of DDBMs without extra training, motivated by the well-established recipes in diffusion models. We generalize DDBMs via a class of non-Markovian diffusion bridges defined on the discretized timesteps concerning sampling, which share the same marginal distributions and training objectives, give rise to generative processes ranging from stochastic to deterministic, and result in diffusion bridge implicit models (DBIMs). DBIMs are not only up to 25$\times$ faster than the vanilla sampler of DDBMs but also induce a novel, simple, and insightful form of ordinary differential equation (ODE) which inspires high-order numerical solvers. Moreover, DBIMs maintain the generation diversity in a distinguished way, by using a booting noise in the initial sampling step, which enables faithful encoding, reconstruction, and semantic interpolation in image translation tasks. Code is available at https://github.com/thu-ml/DiffusionBridge. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.17424</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.17424</id><created>2024-05-27</created><updated>2025-02-05</updated><authors><author><keyname>Li</keyname><forenames>Zhuoling</forenames></author><author><keyname>Xu</keyname><forenames>Xiaogang</forenames></author><author><keyname>Xu</keyname><forenames>Zhenhua</forenames></author><author><keyname>Lim</keyname><forenames>SerNam</forenames></author><author><keyname>Zhao</keyname><forenames>Hengshuang</forenames></author></authors><title>LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent embodied agents are primarily built based on reinforcement learning (RL) or large language models (LLMs). Among them, RL agents are efficient for deployment but only perform very few tasks. By contrast, giant LLM agents (often more than 1000B parameters) present strong generalization while demanding enormous computing resources. In this work, we combine their advantages while avoiding the drawbacks by conducting the proposed referee RL on our developed large auto-regressive model (LARM). Specifically, LARM is built upon a lightweight LLM (fewer than 5B parameters) and directly outputs the next action to execute rather than text. We mathematically reveal that classic RL feedbacks vanish in long-horizon embodied exploration and introduce a giant LLM based referee to handle this reward vanishment during training LARM. In this way, LARM learns to complete diverse open-world tasks without human intervention. Especially, LARM successfully harvests enchanted diamond equipment in Minecraft, which demands significantly longer decision-making chains than the highest achievements of prior best methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.18087</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.18087</id><created>2024-05-28</created><updated>2025-02-05</updated><authors><author><keyname>Bogensperger</keyname><forenames>Lea</forenames></author><author><keyname>Narnhofer</keyname><forenames>Dominik</forenames></author><author><keyname>Falk</keyname><forenames>Alexander</forenames></author><author><keyname>Schindler</keyname><forenames>Konrad</forenames></author><author><keyname>Pock</keyname><forenames>Thomas</forenames></author></authors><title>FlowSDF: Flow Matching for Medical Image Segmentation Using Distance   Transforms</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Medical image segmentation plays an important role in accurately identifying and isolating regions of interest within medical images. Generative approaches are particularly effective in modeling the statistical properties of segmentation masks that are closely related to the respective structures. In this work we introduce FlowSDF, an image-guided conditional flow matching framework, designed to represent the signed distance function (SDF), and, in turn, to represent an implicit distribution of segmentation masks. The advantage of leveraging the SDF is a more natural distortion when compared to that of binary masks. Through the learning of a vector field associated with the probability path of conditional SDF distributions, our framework enables accurate sampling of segmentation masks and the computation of relevant statistical measures. This probabilistic approach also facilitates the generation of uncertainty maps represented by the variance, thereby supporting enhanced robustness in prediction and further analysis. We qualitatively and quantitatively illustrate competitive performance of the proposed method on a public nuclei and gland segmentation data set, highlighting its utility in medical image segmentation applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.19466</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.19466</id><created>2024-05-29</created><updated>2025-02-05</updated><authors><author><keyname>Cai</keyname><forenames>Tiffany Tianhui</forenames></author><author><keyname>Namkoong</keyname><forenames>Hongseok</forenames></author><author><keyname>Russo</keyname><forenames>Daniel</forenames></author><author><keyname>Zhang</keyname><forenames>Kelly W</forenames></author></authors><title>Active Exploration via Autoregressive Generation of Missing Data</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  We pose uncertainty quantification and exploration in online decision-making as a problem of training and generation from an autoregressive sequence model, an area experiencing rapid innovation. Our approach rests on viewing uncertainty as arising from missing future outcomes that would be revealed through appropriate action choices, rather than from unobservable latent parameters of the environment. This reformulation aligns naturally with modern machine learning capabilities: we can i) train generative models through next-outcome prediction rather than fit explicit priors, ii) assess uncertainty through autoregressive generation rather than parameter sampling, and iii) adapt to new information through in-context learning rather than explicit posterior updating. To showcase these ideas, we formulate a challenging meta-bandit problem where effective performance requires leveraging unstructured prior information (like text features) while exploring judiciously to resolve key remaining uncertainties. We validate our approach through both theory and experiments. Our theory establishes a reduction, showing success at offline next-outcome prediction translates to reliable online uncertainty quantification and decision-making, even with strategically collected data. Semi-synthetic experiments show our insights bear out in a news-article recommendation task, where article text can be leveraged to minimize exploration. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.00595</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.00595</id><created>2024-06-01</created><updated>2025-02-05</updated><authors><author><keyname>Sakurai</keyname><forenames>Akira</forenames></author><author><keyname>Shudo</keyname><forenames>Kazuyuki</forenames></author></authors><title>Model-based Analysis of Mining Fairness in a Blockchain</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mining fairness in blockchain refers to equality between the computational resources invested in mining and the block rewards received. There exists a dilemma wherein increasing the transaction processing capacity of a blockchain compromises mining fairness, which consequently undermines its decentralization. This dilemma remains unresolved even with methods such as the greedy heaviest observed subtree (GHOST) protocol, indicating that mining fairness is an inherent bottleneck in the transaction processing capacity of the blockchain system. However, despite its significance, there have been insufficient research studies that have quantitatively analyzed mining fairness. In this paper, we propose a method for calculating mining fairness. First, we approximated a complex blockchain network using a simple mathematical model, assuming that no more than two blocks are generated per round. Within this model, we quantitatively determined local mining fairness and derived several measures of global mining fairness based on local mining fairness. Subsequently, we validated by blockchain network simulations that our calculation method computes mining fairness in networks much more accurately than existing methods. Finally, we analyzed various networks from the perspective of mining fairness. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.00704</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.00704</id><created>2024-06-02</created><updated>2025-02-05</updated><authors><author><keyname>Grønningsæter</keyname><forenames>Ylva</forenames></author><author><keyname>Smørvik</keyname><forenames>Halvor S.</forenames></author><author><keyname>Granmo</keyname><forenames>Ole-Christoffer</forenames></author></authors><title>An Optimized Toolbox for Advanced Image Processing with Tsetlin Machine   Composites</title><categories>cs.CV cs.AI cs.LG</categories><comments>8 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Tsetlin Machine (TM) has achieved competitive results on several image classification benchmarks, including MNIST, K-MNIST, F-MNIST, and CIFAR-2. However, color image classification is arguably still in its infancy for TMs, with CIFAR-10 being a focal point for tracking progress. Over the past few years, TM's CIFAR-10 accuracy has increased from around 61% in 2020 to 75.1% in 2023 with the introduction of Drop Clause. In this paper, we leverage the recently proposed TM Composites architecture and introduce a range of TM Specialists that use various image processing techniques. These include Canny edge detection, Histogram of Oriented Gradients, adaptive mean thresholding, adaptive Gaussian thresholding, Otsu's thresholding, color thermometers, and adaptive color thermometers. In addition, we conduct a rigorous hyperparameter search, where we uncover optimal hyperparameters for several of the TM Specialists. The result is a toolbox that provides new state-of-the-art results on CIFAR-10 for TMs with an accuracy of 82.8%. In conclusion, our toolbox of TM Specialists forms a foundation for new TM applications and a landmark for further research on TM Composites in image analysis. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.03298</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.03298</id><created>2024-06-05</created><updated>2025-02-05</updated><authors><author><keyname>Liu</keyname><forenames>Yibo</forenames></author><author><keyname>Shan</keyname><forenames>Jinjun</forenames></author><author><keyname>Haridevan</keyname><forenames>Amaldev</forenames></author><author><keyname>Zhang</keyname><forenames>Shuo</forenames></author></authors><title>L-PR: Exploiting LiDAR Fiducial Marker for Unordered Low Overlap   Multiview Point Cloud Registration</title><categories>cs.CV cs.RO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Point cloud registration is a prerequisite for many applications in computer vision and robotics. Most existing methods focus on pairwise registration of two point clouds with high overlap. Although there have been some methods for low overlap cases, they struggle in degraded scenarios. This paper introduces a novel framework dubbed L-PR, designed to register unordered low overlap multiview point clouds leveraging LiDAR fiducial markers. We refer to them as LiDAR fiducial markers, but they are the same as the popular AprilTag and ArUco markers, thin sheets of paper that do not affect the 3D geometry of the environment. We first propose an improved adaptive threshold marker detection method to provide robust detection results when the viewpoints among point clouds change dramatically. Then, we formulate the unordered multiview point cloud registration problem as a maximum a-posteriori (MAP) problem and develop a framework consisting of two levels of graphs to address it. The first-level graph, constructed as a weighted graph, is designed to efficiently and optimally infer initial values of scan poses from the unordered set. The second-level graph is constructed as a factor graph. By globally optimizing the variables on the graph, including scan poses, marker poses, and marker corner positions, we tackle the MAP problem. We conduct both qualitative and quantitative experiments to demonstrate that the proposed method surpasses previous state-of-the-art (SOTA) methods and to showcase that L-PR can serve as a low-cost and efficient tool for 3D asset collection and training data collection. In particular, we collect a new dataset named Livox-3DMatch using L-PR and incorporate it into the training of the SOTA learning-based method, SGHR, which brings evident improvements for SGHR on various benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.03773</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.03773</id><created>2024-06-06</created><authors><author><keyname>Nguyen</keyname><forenames>Loc X.</forenames></author><author><keyname>Kim</keyname><forenames>Kitae</forenames></author><author><keyname>Tun</keyname><forenames>Ye Lin</forenames></author><author><keyname>Hassan</keyname><forenames>Sheikh Salman</forenames></author><author><keyname>Tun</keyname><forenames>Yan Kyaw</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author><author><keyname>Hong</keyname><forenames>Choong Seon</forenames></author></authors><title>Optimizing Multi-User Semantic Communication via Transfer Learning and   Knowledge Distillation</title><categories>cs.IT math.IT</categories><comments>5 pages, 5 figures</comments><journal-ref>in IEEE Communications Letters, vol. 29, no. 1, pp. 90-94, Jan.   2025</journal-ref><doi>10.1109/LCOMM.2024.3499956</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semantic communication, notable for ensuring quality of service by jointly optimizing source and channel coding, effectively extracts data semantics, reduces transmission length, and mitigates channel noise. However, most studies overlook multi-user scenarios and resource availability, limiting real-world application. This paper addresses this gap by focusing on downlink communication from a base station to multiple users with varying computing capacities. Users employ variants of Swin transformer models for source decoding and a simple architecture for channel decoding. We propose a novel training regimen, incorporating transfer learning and knowledge distillation to improve low-computing users' performance. Extensive simulations validate the proposed methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.05498</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.05498</id><created>2024-06-08</created><updated>2025-02-05</updated><authors><author><keyname>Wang</keyname><forenames>Xunguang</forenames></author><author><keyname>Wu</keyname><forenames>Daoyuan</forenames></author><author><keyname>Ji</keyname><forenames>Zhenlan</forenames></author><author><keyname>Li</keyname><forenames>Zongjie</forenames></author><author><keyname>Ma</keyname><forenames>Pingchuan</forenames></author><author><keyname>Wang</keyname><forenames>Shuai</forenames></author><author><keyname>Li</keyname><forenames>Yingjiu</forenames></author><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Liu</keyname><forenames>Ning</forenames></author><author><keyname>Rahmel</keyname><forenames>Juergen</forenames></author></authors><title>SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a   Practical Manner</title><categories>cs.CR cs.AI</categories><comments>Accepted by USENIX Security Symposium 2025. Please cite the   conference version of this paper, i.e., "Xunguang Wang, Daoyuan Wu, Zhenlan   Ji, Zongjie Li, Pingchuan Ma, Shuai Wang, Yingjiu Li, Yang Liu, Ning Liu, and   Juergen Rahmel. SelfDefend: LLMs Can Defend Themselves against Jailbreaking   in a Practical Manner. In Proc. USENIX Security, 2025."</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Jailbreaking is an emerging adversarial attack that bypasses the safety alignment deployed in off-the-shelf large language models (LLMs) and has evolved into multiple categories: human-based, optimization-based, generation-based, and the recent indirect and multilingual jailbreaks. However, delivering a practical jailbreak defense is challenging because it needs to not only handle all the above jailbreak attacks but also incur negligible delays to user prompts, as well as be compatible with both open-source and closed-source LLMs. Inspired by how the traditional security concept of shadow stacks defends against memory overflow attacks, this paper introduces a generic LLM jailbreak defense framework called SelfDefend, which establishes a shadow LLM as a defense instance (in detection state) to concurrently protect the target LLM instance (in normal answering state) in the normal stack and collaborate with it for checkpoint-based access control. The effectiveness of SelfDefend builds upon our observation that existing LLMs can identify harmful prompts or intentions in user queries, which we empirically validate using mainstream GPT-3.5/4 models against major jailbreak attacks. To further improve the defense's robustness and minimize costs, we employ a data distillation approach to tune dedicated open-source defense models. When deployed to protect GPT-3.5/4, Claude, Llama-2-7b/13b, and Mistral, these models outperform seven state-of-the-art defenses and match the performance of GPT-4-based SelfDefend, with significantly lower extra delays. Further experiments show that the tuned models are robust to adaptive jailbreaks and prompt injections. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.05702</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.05702</id><created>2024-06-09</created><updated>2025-02-05</updated><authors><author><keyname>Aliyev</keyname><forenames>Yagub N.</forenames></author></authors><title>The dual of Philo's shortest line segment problem</title><categories>cs.CG</categories><msc-class>51M16, 51M25, 51M04, 52A38, 52A40</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study the dual of Philo's shortest line segment problem which asks to find the optimal line segments passing through two given points, with a common endpoint, and with the other endpoints on a given line. The provided solution uses multivariable calculus and geometry methods. Interesting connections with the angle bisector of the triangle are explored. A generalization of the problem using $L_p$ ($p\ge 1$) norm is proposed. Particular case $p=\infty$ is studied. Interesting case $p=2$ is proposed as an open problem and related property of a symedian of a triangle is conjectured. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.06494</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.06494</id><created>2024-06-10</created><updated>2025-02-05</updated><authors><author><keyname>Gala</keyname><forenames>Gennaro</forenames></author><author><keyname>de Campos</keyname><forenames>Cassio</forenames></author><author><keyname>Vergari</keyname><forenames>Antonio</forenames></author><author><keyname>Quaeghebeur</keyname><forenames>Erik</forenames></author></authors><title>Scaling Continuous Latent Variable Models as Probabilistic Integral   Circuits</title><categories>cs.LG cs.AI</categories><journal-ref>NeurIPS 2024</journal-ref><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Probabilistic integral circuits (PICs) have been recently introduced as probabilistic models enjoying the key ingredient behind expressive generative models: continuous latent variables (LVs). PICs are symbolic computational graphs defining continuous LV models as hierarchies of functions that are summed and multiplied together, or integrated over some LVs. They are tractable if LVs can be analytically integrated out, otherwise they can be approximated by tractable probabilistic circuits (PC) encoding a hierarchical numerical quadrature process, called QPCs.   So far, only tree-shaped PICs have been explored, and training them via numerical quadrature requires memory-intensive processing at scale. In this paper, we address these issues, and present: (i) a pipeline for building DAG-shaped PICs out of arbitrary variable decompositions, (ii) a procedure for training PICs using tensorized circuit architectures, and (iii) neural functional sharing techniques to allow scalable training. In extensive experiments, we showcase the effectiveness of functional sharing and the superiority of QPCs over traditional PCs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.07255</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.07255</id><created>2024-06-11</created><updated>2025-02-05</updated><authors><author><keyname>Peng</keyname><forenames>Long</forenames></author><author><keyname>Li</keyname><forenames>Wenbo</forenames></author><author><keyname>Pei</keyname><forenames>Renjing</forenames></author><author><keyname>Ren</keyname><forenames>Jingjing</forenames></author><author><keyname>Xu</keyname><forenames>Jiaqi</forenames></author><author><keyname>Wang</keyname><forenames>Yang</forenames></author><author><keyname>Cao</keyname><forenames>Yang</forenames></author><author><keyname>Zha</keyname><forenames>Zheng-Jun</forenames></author></authors><title>Towards Realistic Data Generation for Real-World Super-Resolution</title><categories>cs.CV eess.IV</categories><comments>accepted by ICLR 2025</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Existing image super-resolution (SR) techniques often fail to generalize effectively in complex real-world settings due to the significant divergence between training data and practical scenarios. To address this challenge, previous efforts have either manually simulated intricate physical-based degradations or utilized learning-based techniques, yet these approaches remain inadequate for producing large-scale, realistic, and diverse data simultaneously. In this paper, we introduce a novel Realistic Decoupled Data Generator (RealDGen), an unsupervised learning data generation framework designed for real-world super-resolution. We meticulously develop content and degradation extraction strategies, which are integrated into a novel content-degradation decoupled diffusion model to create realistic low-resolution images from unpaired real LR and HR images. Extensive experiments demonstrate that RealDGen excels in generating large-scale, high-quality paired data that mirrors real-world degradations, significantly advancing the performance of popular SR models on various real-world benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.07338</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.07338</id><created>2024-06-11</created><updated>2025-02-05</updated><authors><author><keyname>Qi</keyname><forenames>Ning</forenames></author><author><keyname>Pinson</keyname><forenames>Pierre</forenames></author><author><keyname>Almassalkhi</keyname><forenames>Mads R.</forenames></author><author><keyname>Zhuang</keyname><forenames>Yingrui</forenames></author><author><keyname>Su</keyname><forenames>Yifan</forenames></author><author><keyname>Liu</keyname><forenames>Feng</forenames></author></authors><title>Capacity Credit Evaluation of Generalized Energy Storage Considering   Strategic Capacity Withholding and Decision-Dependent Uncertainty</title><categories>eess.SY cs.SY</categories><comments>This is a manuscript submitted to Applied Energy</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper proposes a novel capacity credit evaluation framework to accurately quantify the contribution of generalized energy storage (GES) to resource adequacy, considering both strategic capacity withholding and decision-dependent uncertainty (DDU). To this end, we establish a market-oriented risk-averse coordinated dispatch method to capture the cross-market reliable operation of GES. The proposed method is sequentially implemented along with the Monte Carlo simulation process, coordinating the pre-dispatched price arbitrage and capacity withholding in the energy market with adequacy-oriented re-dispatch during capacity market calls. In addition to decision-independent uncertainties in operational states and baseline behavior, we explicitly address the inherent DDU of GES (i.e., the uncertainty of available discharge capacity affected by the incentives and accumulated discomfort) during the re-dispatch stage using the proposed distributional robust chance-constrained approach. Furthermore, a capacity credit metric called equivalent storage capacity substitution is introduced to quantify the equivalent deterministic storage capacity of uncertain GES. Simulations on the modified IEEE RTS-79 benchmark system with 20 years real-world data from Elia demonstrate that the proposed method yields accurate capacity credit and improved economic performance. We show that the capacity credit of GES increases with more strategic capacity withholding but decreases with more DDU levels. Key factors, such as capacity withholding and DDU structure impacting GES's capacity credit are analyzed with insights into capacity market decision-making. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.09570</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.09570</id><created>2024-06-13</created><updated>2025-02-05</updated><authors><author><keyname>Issenhuth</keyname><forenames>Thibaut</forenames></author><author><keyname>Lee</keyname><forenames>Sangchul</forenames></author><author><keyname>Santos</keyname><forenames>Ludovic Dos</forenames></author><author><keyname>Franceschi</keyname><forenames>Jean-Yves</forenames></author><author><keyname>Kim</keyname><forenames>Chansoo</forenames></author><author><keyname>Rakotomamonjy</keyname><forenames>Alain</forenames></author></authors><title>Improving Consistency Models with Generator-Augmented Flows</title><categories>cs.LG cs.AI cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Consistency models imitate the multi-step sampling of score-based diffusion in a single forward pass of a neural network. They can be learned in two ways: consistency distillation and consistency training. The former relies on the true velocity field of the corresponding differential equation, approximated by a pre-trained neural network. In contrast, the latter uses a single-sample Monte Carlo estimate of this velocity field. The related estimation error induces a discrepancy between consistency distillation and training that, we show, still holds in the continuous-time limit. To alleviate this issue, we propose a novel flow that transports noisy data towards their corresponding outputs derived from a consistency model. We prove that this flow reduces the previously identified discrepancy and the noise-data transport cost. Consequently, our method not only accelerates consistency training convergence but also enhances its overall performance. The code is available at: https://github.com/thibautissenhuth/consistency_GC. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.09621</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.09621</id><created>2024-06-13</created><authors><author><keyname>Ghali</keyname><forenames>Mohammed-Khalil</forenames></author><author><keyname>Farrag</keyname><forenames>Abdelrahman</forenames></author><author><keyname>Won</keyname><forenames>Daehan</forenames></author><author><keyname>Jin</keyname><forenames>Yu</forenames></author></authors><title>Enhancing Knowledge Retrieval with In-Context Learning and Semantic   Search through Generative AI</title><categories>cs.IR</categories><doi>10.1016/j.knosys.2025.113047</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Retrieving and extracting knowledge from extensive research documents and large databases presents significant challenges for researchers, students, and professionals in today's information-rich era. Existing retrieval systems, which rely on general-purpose Large Language Models (LLMs), often fail to provide accurate responses to domain-specific inquiries. Additionally, the high cost of pretraining or fine-tuning LLMs for specific domains limits their widespread adoption. To address these limitations, we propose a novel methodology that combines the generative capabilities of LLMs with the fast and accurate retrieval capabilities of vector databases. This advanced retrieval system can efficiently handle both tabular and non-tabular data, understand natural language user queries, and retrieve relevant information without fine-tuning. The developed model, Generative Text Retrieval (GTR), is adaptable to both unstructured and structured data with minor refinement. GTR was evaluated on both manually annotated and public datasets, achieving over 90% accuracy and delivering truthful outputs in 87% of cases. Our model achieved state-of-the-art performance with a Rouge-L F1 score of 0.98 on the MSMARCO dataset. The refined model, Generative Tabular Text Retrieval (GTR-T), demonstrated its efficiency in large database querying, achieving an Execution Accuracy (EX) of 0.82 and an Exact-Set-Match (EM) accuracy of 0.60 on the Spider dataset, using an open-source LLM. These efforts leverage Generative AI and In-Context Learning to enhance human-text interaction and make advanced AI capabilities more accessible. By integrating robust retrieval systems with powerful LLMs, our approach aims to democratize access to sophisticated AI tools, improving the efficiency, accuracy, and scalability of AI-driven information retrieval and database querying. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.09630</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.09630</id><created>2024-06-13</created><updated>2025-02-04</updated><authors><author><keyname>Saeed</keyname><forenames>Mehreen</forenames></author><author><keyname>Chan</keyname><forenames>Adrian</forenames></author><author><keyname>Mijar</keyname><forenames>Anupam</forenames></author><author><keyname>Moukarzel</keyname><forenames>Joseph</forenames></author><author><keyname>Habchi</keyname><forenames>Georges</forenames></author><author><keyname>Younes</keyname><forenames>Carlos</forenames></author><author><keyname>Elias</keyname><forenames>Amin</forenames></author><author><keyname>Wong</keyname><forenames>Chau-Wai</forenames></author><author><keyname>Khater</keyname><forenames>Akram</forenames></author></authors><title>Muharaf: Manuscripts of Handwritten Arabic Dataset for Cursive Text   Recognition</title><categories>cs.CV cs.LG</categories><journal-ref>Published in NeurIPS 2024</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We present the Manuscripts of Handwritten Arabic~(Muharaf) dataset, which is a machine learning dataset consisting of more than 1,600 historic handwritten page images transcribed by experts in archival Arabic. Each document image is accompanied by spatial polygonal coordinates of its text lines as well as basic page elements. This dataset was compiled to advance the state of the art in handwritten text recognition (HTR), not only for Arabic manuscripts but also for cursive text in general. The Muharaf dataset includes diverse handwriting styles and a wide range of document types, including personal letters, diaries, notes, poems, church records, and legal correspondences. In this paper, we describe the data acquisition pipeline, notable dataset features, and statistics. We also provide a preliminary baseline result achieved by training convolutional neural networks using this data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.10174</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.10174</id><created>2024-06-14</created><authors><author><keyname>Elzohbi</keyname><forenames>Mohamad</forenames></author><author><keyname>Zhao</keyname><forenames>Richard</forenames></author></authors><title>Let the Poem Hit the Rhythm: Using a Byte-Based Transformer for   Beat-Aligned Poetry Generation</title><categories>cs.CL</categories><comments>5 pages, 3 figures, accepted for the 15th International Conference on   Computational Creativity, ICCC'24</comments><journal-ref>Proceedings of 15th International Conference on Computational   Creativity (ICCC), J\"onk\"oping, Sweden, June, 2024</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The intersection between poetry and music provides an interesting case for computational creativity, yet remains relatively unexplored. This paper explores the integration of poetry and music through the lens of beat patterns, investigating whether a byte-based language model can generate words that fit specific beat patterns within the context of poetry. Drawing on earlier studies, we developed a method to train a byte-based transformer model, ByT5, to align poems with beat patterns. The results demonstrate a high level of beat alignment while maintaining semantic coherence. Future work will aim to improve the model's ability to create complete beat-aligned poems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.11403</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.11403</id><created>2024-06-17</created><updated>2025-02-04</updated><authors><author><keyname>Cesista</keyname><forenames>Franz Louis</forenames></author></authors><title>Multimodal Structured Generation: CVPR's 2nd MMFM Challenge Technical   Report</title><categories>cs.CV cs.CL</categories><comments>Conference on Computer Vision and Pattern Recognition's 2nd   Multimodal Foundation Models Challenge</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multimodal Foundation Models (MMFMs) have demonstrated strong performance in both computer vision and natural language processing tasks. However, their performance diminishes in tasks that require a high degree of integration between these modalities, such as document understanding. Moreover, finetuning these models and deploying them requires significantly more compute and more engineering effort than unimodal models. In this work, we present Multimodal Structured Generation, a framework that forces (frozen) MMFMs to produce outputs in a strictly structured format by applying hard constraints directly to the output logits. This approach not only ensures that the model generates parseable outputs that downstream APIs can easily ingest but also allows us to force the model to reason before answering, which significantly boosts performance without the need for expensive fine-tuning. We demonstrate the effectiveness of our method through competitive results in the CVPR 2nd MMFM Challenge, highlighting that carefully designed lightweight engineering can outperform expensive and complicated modeling approaches. All of our scripts, deployment steps, and evaluation results can be accessed in https://github.com/leloykun/MMFM-Challenge </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.11629</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.11629</id><created>2024-06-17</created><updated>2025-02-05</updated><authors><author><keyname>Song</keyname><forenames>Mingyang</forenames></author><author><keyname>Zheng</keyname><forenames>Mao</forenames></author><author><keyname>Luo</keyname><forenames>Xuan</forenames></author><author><keyname>Pan</keyname><forenames>Yue</forenames></author></authors><title>Can Many-Shot In-Context Learning Help LLMs as Evaluators? A Preliminary   Empirical Study</title><categories>cs.CL</categories><comments>Accepted by COLING 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Utilizing Large Language Models (LLMs) as evaluators to assess the performance of LLMs has garnered attention. However, this kind of evaluation approach is affected by potential biases within LLMs, raising concerns about the accuracy and reliability of the evaluation results of LLMs. To address this problem, we propose and study two many-shot In-Context Learning (ICL) prompt templates to help LLM evaluators mitigate potential biases: Many-Shot with Reference (MSwR) and Many-Shot without Reference (MSoR). Specifically, the former utilizes in-context examples with model-generated evaluation rationales as references, while the latter does not include these references. Using these prompt designs, we investigate the impact of increasing the number of in-context examples on the consistency and quality of the evaluation results. Experimental results show that advanced LLMs, such as GPT-4o, perform better in the many-shot regime than in the zero-shot and few-shot regimes. Furthermore, when using GPT-4o as an evaluator in the many-shot regime, adopting MSwR as the prompt template performs better than MSoR. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.12502</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.12502</id><created>2024-06-18</created><updated>2025-02-05</updated><authors><author><keyname>Gee</keyname><forenames>Leonidas</forenames></author><author><keyname>Gritta</keyname><forenames>Milan</forenames></author><author><keyname>Lampouras</keyname><forenames>Gerasimos</forenames></author><author><keyname>Iacobacci</keyname><forenames>Ignacio</forenames></author></authors><title>Code-Optimise: Self-Generated Preference Data for Correctness and   Efficiency</title><categories>cs.CL</categories><comments>NAACL 2025 (Findings)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Code Language Models have been trained to generate accurate solutions, typically with no regard for runtime. On the other hand, previous works that explored execution optimisation have observed corresponding drops in functional correctness. To that end, we introduce Code-Optimise, a framework that incorporates both correctness (passed, failed) and runtime (quick, slow) as learning signals via self-generated preference data. Our framework is both lightweight and robust as it dynamically selects solutions to reduce overfitting while avoiding a reliance on larger models for learning signals. Code-Optimise achieves significant improvements in pass@k while decreasing the competitive baseline runtimes by an additional 6% for in-domain data and up to 3% for out-of-domain data. As a by-product, the average length of the generated solutions is reduced by up to 48% on MBPP and 23% on HumanEval, resulting in faster and cheaper inference. The generated data and codebase is open-sourced at https://github.com/huawei-noah/HEBO/tree/Code_Optimise. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.12814</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.12814</id><created>2024-06-18</created><updated>2025-02-04</updated><authors><author><keyname>Wu</keyname><forenames>Chen Henry</forenames></author><author><keyname>Shah</keyname><forenames>Rishi</forenames></author><author><keyname>Koh</keyname><forenames>Jing Yu</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author><author><keyname>Fried</keyname><forenames>Daniel</forenames></author><author><keyname>Raghunathan</keyname><forenames>Aditi</forenames></author></authors><title>Dissecting Adversarial Robustness of Multimodal LM Agents</title><categories>cs.LG cs.CL cs.CR cs.CV</categories><comments>ICLR 2025. Also oral at NeurIPS 2024 Open-World Agents Workshop</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As language models (LMs) are used to build autonomous agents in real environments, ensuring their adversarial robustness becomes a critical challenge. Unlike chatbots, agents are compound systems with multiple components taking actions, which existing LMs safety evaluations do not adequately address. To bridge this gap, we manually create 200 targeted adversarial tasks and evaluation scripts in a realistic threat model on top of VisualWebArena, a real environment for web agents. To systematically examine the robustness of agents, we propose the Agent Robustness Evaluation (ARE) framework. ARE views the agent as a graph showing the flow of intermediate outputs between components and decomposes robustness as the flow of adversarial information on the graph. We find that we can successfully break latest agents that use black-box frontier LMs, including those that perform reflection and tree search. With imperceptible perturbations to a single image (less than 5% of total web page pixels), an attacker can hijack these agents to execute targeted adversarial goals with success rates up to 67%. We also use ARE to rigorously evaluate how the robustness changes as new components are added. We find that inference-time compute that typically improves benign performance can open up new vulnerabilities and harm robustness. An attacker can compromise the evaluator used by the reflexion agent and the value function of the tree search agent, which increases the attack success relatively by 15% and 20%. Our data and code for attacks, defenses, and evaluation are at https://github.com/ChenWu98/agent-attack </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.14324</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.14324</id><created>2024-06-20</created><updated>2025-02-05</updated><authors><author><keyname>Beylier</keyname><forenames>Charlotte</forenames></author><author><keyname>Hofmann</keyname><forenames>Simon M.</forenames></author><author><keyname>Scherf</keyname><forenames>Nico</forenames></author></authors><title>Revealing the Learning Process in Reinforcement Learning Agents Through   Attention-Oriented Metrics</title><categories>cs.LG</categories><comments>Workshop on Scientific Methods for Understanding Deep Learning,   NeurIPS 2024</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The learning process of a reinforcement learning (RL) agent remains poorly understood beyond the mathematical formulation of its learning algorithm. To address this gap, we introduce attention-oriented metrics (ATOMs) to investigate the development of an RL agent's attention during training. In a controlled experiment, we tested ATOMs on three variations of a Pong game, each designed to teach the agent distinct behaviours, complemented by a behavioural assessment. ATOMs successfully delineate the attention patterns of an agent trained on each game variation, and that these differences in attention patterns translate into differences in the agent's behaviour. Through continuous monitoring of ATOMs during training, we observed that the agent's attention developed in phases, and that these phases were consistent across game variations. Overall, we believe that ATOM could help improve our understanding of the learning processes of RL agents and better understand the relationship between attention and learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.15007</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.15007</id><created>2024-06-21</created><updated>2025-02-05</updated><authors><author><keyname>Berto</keyname><forenames>Federico</forenames></author><author><keyname>Hua</keyname><forenames>Chuanbo</forenames></author><author><keyname>Zepeda</keyname><forenames>Nayeli Gast</forenames></author><author><keyname>Hottung</keyname><forenames>André</forenames></author><author><keyname>Wouda</keyname><forenames>Niels</forenames></author><author><keyname>Lan</keyname><forenames>Leon</forenames></author><author><keyname>Park</keyname><forenames>Junyoung</forenames></author><author><keyname>Tierney</keyname><forenames>Kevin</forenames></author><author><keyname>Park</keyname><forenames>Jinkyoo</forenames></author></authors><title>RouteFinder: Towards Foundation Models for Vehicle Routing Problems</title><categories>cs.AI</categories><comments>A version of this work has been presented as an Oral at the ICML 2024   FM-Wild Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces RouteFinder, a comprehensive foundation model framework to tackle different Vehicle Routing Problem (VRP) variants. Our core idea is that a foundation model for VRPs should be able to represent variants by treating each as a subset of a generalized problem equipped with different attributes. We propose a unified VRP environment capable of efficiently handling any attribute combination. The RouteFinder model leverages a modern transformer-based encoder and global attribute embeddings to improve task representation. Additionally, we introduce two reinforcement learning techniques to enhance multi-task performance: mixed batch training, which enables training on different variants at once, and multi-variant reward normalization to balance different reward scales. Finally, we propose efficient adapter layers that enable fine-tuning for new variants with unseen attributes. Extensive experiments on 48 VRP variants show RouteFinder outperforms recent state-of-the-art learning methods. Code: https://github.com/ai4co/routefinder. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.16061</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.16061</id><created>2024-06-23</created><updated>2025-02-04</updated><authors><author><keyname>Lahlou</keyname><forenames>Salem</forenames></author><author><keyname>Abubaker</keyname><forenames>Abdalgader</forenames></author><author><keyname>Hacid</keyname><forenames>Hakim</forenames></author></authors><title>PORT: Preference Optimization on Reasoning Traces</title><categories>cs.LG cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Preference optimization methods have been successfully applied to improve not only the alignment of large language models (LLMs) with human values, but also specific natural language tasks such as summarization and stylistic continuations. This paper proposes using preference optimization methods on Chain-of-Thought steps in order to improve the mathematical reasoning performances of language models. While the chosen answers are obtained from datasets that include reasoning traces, we propose two complementary schemes for generating rejected answers: weak LLM prompting, and digit corruption. Our approach leads to increased accuracy on the GSM8K and AQuA-RAT mathematical reasoning benchmarks for Falcon2-11B and Mistral-7B. Additionally, the improved abilities transfer to non-mathematical tasks, including the ARC benchmark and symbolic reasoning challenges. For example, our method can lead to up to relative 8.47% and 18.73% increases in accuracy on the GSM8K and AQuA benchmarks respectively, without any extra annotations. This work suggests that the path towards better language reasoning abilities goes through spending resources on creating high-quality datasets of reasoning traces. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.16535</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.16535</id><created>2024-06-24</created><updated>2025-02-05</updated><authors><author><keyname>Cho</keyname><forenames>Hakaze</forenames></author><author><keyname>Sakai</keyname><forenames>Yoshihiro</forenames></author><author><keyname>Kato</keyname><forenames>Mariko</forenames></author><author><keyname>Tanaka</keyname><forenames>Kenshiro</forenames></author><author><keyname>Ishii</keyname><forenames>Akira</forenames></author><author><keyname>Inoue</keyname><forenames>Naoya</forenames></author></authors><title>Token-based Decision Criteria Are Suboptimal in In-context Learning</title><categories>cs.CL cs.AI cs.LG</categories><comments>24 pages, 15 figures, 13 tables. NAACL 2025 Main Conference Accepted.   Camera-ready version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In-Context Learning (ICL) typically utilizes classification criteria from output probabilities of manually selected label tokens. However, we argue that such token-based classification criteria lead to suboptimal decision boundaries, despite delicate calibrations through translation and constrained rotation applied. To address this problem, we propose Hidden Calibration, which renounces token probabilities and uses the nearest centroid classifier on the LM's last hidden states. In detail, we assign the label of the nearest centroid previously estimated from a calibration set to the test sample as the predicted label. Our experiments on 6 models and 10 classification datasets indicate that Hidden Calibration consistently outperforms current token-based baselines by about 20%~50%, achieving a strong state-of-the-art in ICL. Our further analysis demonstrates that Hidden Calibration finds better classification criteria with less inter-class overlap, and LMs provide linearly separable intra-class clusters with the help of demonstrations, which supports Hidden Calibration and gives new insights into the principle of ICL. Our official code implementation can be found at https://github.com/hc495/Hidden_Calibration. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.17119</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.17119</id><created>2024-06-24</created><updated>2024-07-08</updated><authors><author><keyname>Bonneville</keyname><forenames>Christophe</forenames></author><author><keyname>Bieberdorf</keyname><forenames>Nathan</forenames></author><author><keyname>Hegde</keyname><forenames>Arun</forenames></author><author><keyname>Asta</keyname><forenames>Mark</forenames></author><author><keyname>Najm</keyname><forenames>Habib N.</forenames></author><author><keyname>Capolungo</keyname><forenames>Laurent</forenames></author><author><keyname>Safta</keyname><forenames>Cosmin</forenames></author></authors><title>Accelerating Phase Field Simulations Through a Hybrid Adaptive Fourier   Neural Operator with U-Net Backbone</title><categories>cs.CE cs.CV cs.LG cs.NA math.NA</categories><journal-ref>npj Computational Materials, 11-14 (2025)</journal-ref><doi>10.1038/s41524-024-01488-z</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Prolonged contact between a corrosive liquid and metal alloys can cause progressive dealloying. For such liquid-metal dealloying (LMD) process, phase field models have been developed. However, the governing equations often involve coupled non-linear partial differential equations (PDE), which are challenging to solve numerically. In particular, stiffness in the PDEs requires an extremely small time steps (e.g. $10^{-12}$ or smaller). This computational bottleneck is especially problematic when running LMD simulation until a late time horizon is required. This motivates the development of surrogate models capable of leaping forward in time, by skipping several consecutive time steps at-once. In this paper, we propose U-Shaped Adaptive Fourier Neural Operators (U-AFNO), a machine learning (ML) model inspired by recent advances in neural operator learning. U-AFNO employs U-Nets for extracting and reconstructing local features within the physical fields, and passes the latent space through a vision transformer (ViT) implemented in the Fourier space (AFNO). We use U-AFNOs to learn the dynamics mapping the field at a current time step into a later time step. We also identify global quantities of interest (QoI) describing the corrosion process (e.g. the deformation of the liquid-metal interface) and show that our proposed U-AFNO model is able to accurately predict the field dynamics, in-spite of the chaotic nature of LMD. Our model reproduces the key micro-structure statistics and QoIs with a level of accuracy on-par with the high-fidelity numerical solver. We also investigate the opportunity of using hybrid simulations, in which we alternate forward leap in time using the U-AFNO with high-fidelity time stepping. We demonstrate that while advantageous for some surrogate model design choices, our proposed U-AFNO model in fully auto-regressive settings consistently outperforms hybrid schemes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.00400</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.00400</id><created>2024-06-29</created><updated>2025-02-04</updated><authors><author><keyname>Sargeant</keyname><forenames>Holli</forenames></author><author><keyname>Magnusson</keyname><forenames>Måns</forenames></author></authors><title>Formalising Anti-Discrimination Law in Automated Decision Systems</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Algorithmic discrimination is a critical concern as machine learning models are used in high-stakes decision-making in legally protected contexts. Although substantial research on algorithmic bias and discrimination has led to the development of fairness metrics, several critical legal issues remain unaddressed in practice. To address these gaps, we introduce a novel decision-theoretic framework grounded in anti-discrimination law of the United Kingdom, which has global influence and aligns more closely with European and Commonwealth legal systems. We propose the 'conditional estimation parity' metric, which accounts for estimation error and the underlying data-generating process, aligning with legal standards. Through a real-world example based on an algorithmic credit discrimination case, we demonstrate the practical application of our formalism and provide insights for aligning fairness metrics with legal principles. Our approach bridges the divide between machine learning fairness metrics and anti-discrimination law, offering a legally grounded framework for developing non-discriminatory automated decision systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.01449</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.01449</id><created>2024-06-27</created><updated>2025-02-05</updated><authors><author><keyname>Faysse</keyname><forenames>Manuel</forenames></author><author><keyname>Sibille</keyname><forenames>Hugues</forenames></author><author><keyname>Wu</keyname><forenames>Tony</forenames></author><author><keyname>Omrani</keyname><forenames>Bilel</forenames></author><author><keyname>Viaud</keyname><forenames>Gautier</forenames></author><author><keyname>Hudelot</keyname><forenames>Céline</forenames></author><author><keyname>Colombo</keyname><forenames>Pierre</forenames></author></authors><title>ColPali: Efficient Document Retrieval with Vision Language Models</title><categories>cs.IR cs.CL cs.CV</categories><comments>Published as a conference paper at ICLR 2025</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Documents are visually rich structures that convey information through text, but also figures, page layouts, tables, or even fonts. Since modern retrieval systems mainly rely on the textual information they extract from document pages to index documents -often through lengthy and brittle processes-, they struggle to exploit key visual cues efficiently. This limits their capabilities in many practical document retrieval applications such as Retrieval Augmented Generation (RAG). To benchmark current systems on visually rich document retrieval, we introduce the Visual Document Retrieval Benchmark ViDoRe, composed of various page-level retrieval tasks spanning multiple domains, languages, and practical settings. The inherent complexity and performance shortcomings of modern systems motivate a new concept; doing document retrieval by directly embedding the images of the document pages. We release ColPali, a Vision Language Model trained to produce high-quality multi-vector embeddings from images of document pages. Combined with a late interaction matching mechanism, ColPali largely outperforms modern document retrieval pipelines while being drastically simpler, faster and end-to-end trainable. We release models, data, code and benchmarks under open licenses at https://huggingface.co/vidore. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.02700</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.02700</id><created>2024-07-02</created><updated>2025-02-04</updated><authors><author><keyname>Rojas</keyname><forenames>Helder</forenames></author><author><keyname>Rojas</keyname><forenames>Nilton</forenames></author><author><keyname>B.</keyname><forenames>Espinoza J.</forenames></author><author><keyname>Huamanchumo</keyname><forenames>Luis</forenames></author></authors><title>A simple algorithm for output range analysis for deep neural networks</title><categories>cs.LG math.PR stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents a novel approach for the output range estimation problem in Deep Neural Networks (DNNs) by integrating a Simulated Annealing (SA) algorithm tailored to operate within constrained domains and ensure convergence towards global optima. The method effectively addresses the challenges posed by the lack of local geometric information and the high non-linearity inherent to DNNs, making it applicable to a wide variety of architectures, with a special focus on Residual Networks (ResNets) due to their practical importance. Unlike existing methods, our algorithm imposes minimal assumptions on the internal architecture of neural networks, thereby extending its usability to complex models. Theoretical analysis guarantees convergence, while extensive empirical evaluations-including optimization tests involving functions with multiple local minima-demonstrate the robustness of our algorithm in navigating non-convex response surfaces. The experimental results highlight the algorithm's efficiency in accurately estimating DNN output ranges, even in scenarios characterized by high non-linearity and complex constraints. For reproducibility, Python codes and datasets used in the experiments are publicly available through our GitHub repository. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.03073</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.03073</id><created>2024-07-03</created><updated>2025-02-05</updated><authors><author><keyname>Buhrman</keyname><forenames>Harry</forenames></author><author><keyname>Gharibian</keyname><forenames>Sevag</forenames></author><author><keyname>Landau</keyname><forenames>Zeph</forenames></author><author><keyname>Gall</keyname><forenames>François Le</forenames></author><author><keyname>Schuch</keyname><forenames>Norbert</forenames></author><author><keyname>Tamaki</keyname><forenames>Suguru</forenames></author></authors><title>Beating the natural Grover bound for low-energy estimation and state   preparation</title><categories>quant-ph cs.CC</categories><comments>9 pages; v2: modified title and minor changes</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating ground state energies of many-body Hamiltonians is a central task in many areas of quantum physics. In this work, we give quantum algorithms which, given any $k$-body Hamiltonian $H$, compute an estimate for the ground state energy and prepare a quantum state achieving said energy, respectively. Specifically, for any $\varepsilon&gt;0$, our algorithms return, with high probability, an estimate of the ground state energy of $H$ within additive error $\varepsilon M$, or a quantum state with the corresponding energy. Here, $M$ is the total strength of all interaction terms, which in general is extensive in the system size. Our approach makes no assumptions about the geometry or spatial locality of interaction terms of the input Hamiltonian and thus handles even long-range or all-to-all interactions, such as in quantum chemistry, where lattice-based techniques break down. In this fully general setting, the runtime of our algorithms scales as $2^{cn/2}$ for $c&lt;1$, yielding the first quantum algorithms for low-energy estimation breaking a standard square root Grover speedup for unstructured search. The core of our approach is remarkably simple, and relies on showing that an extensive fraction of the interactions can be neglected with a controlled error. What this ultimately implies is that even arbitrary $k$-local Hamiltonians have structure in their low energy space, in the form of an exponential-dimensional low energy subspace. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.03187</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.03187</id><created>2024-07-03</created><updated>2025-02-05</updated><authors><author><keyname>Li</keyname><forenames>Tao</forenames></author><author><keyname>Dong</keyname><forenames>Xiang</forenames></author><author><keyname>Hao</keyname><forenames>Junfeng</forenames></author><author><keyname>Yin</keyname><forenames>Ping</forenames></author><author><keyname>Xu</keyname><forenames>Xiaoxue</forenames></author><author><keyname>Lai</keyname><forenames>Maokai</forenames></author><author><keyname>Li</keyname><forenames>Yuan</forenames></author><author><keyname>Peng</keyname><forenames>Ting</forenames></author></authors><title>Holistic view of the road transportation system based on real-time data   sharing mechanism</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional manual driving and single-vehicle-based intelligent driving have limitations in real-time and accurate acquisition of the current driving status and intentions of surrounding vehicles, leading to vehicles typically maintaining appropriate safe distances from each other. Yet, accidents still frequently occur, especially in merging areas; meanwhile, it is difficult to comprehensively obtain the conditions of road infrastructure. These limitations not only restrict the further improvement of road capacity but also result in irreparable losses of life and property. To overcome this bottleneck, this paper constructs a space-time global view of the road traffic system based on a real-time sharing mechanism, enabling both road users and managers to timely access the driving intentions of nearby vehicles and the real-time status of road infrastructure. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.08192</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.08192</id><created>2024-07-11</created><updated>2024-07-22</updated><authors><author><keyname>Fayyazi</keyname><forenames>Arya</forenames></author><author><keyname>Kamal</keyname><forenames>Mehdi</forenames></author><author><keyname>Pedram</keyname><forenames>Massoud</forenames></author></authors><title>ARCO:Adaptive Multi-Agent Reinforcement Learning-Based Hardware/Software   Co-Optimization Compiler for Improved Performance in DNN Accelerator Design</title><categories>cs.LG cs.AI cs.AR</categories><comments>Under review</comments><doi>10.1145/3658617.3697547</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents ARCO, an adaptive Multi-Agent Reinforcement Learning (MARL)-based co-optimizing compilation framework designed to enhance the efficiency of mapping machine learning (ML) models - such as Deep Neural Networks (DNNs) - onto diverse hardware platforms. The framework incorporates three specialized actor-critic agents within MARL, each dedicated to a distinct aspect of compilation/optimization at an abstract level: one agent focuses on hardware, while two agents focus on software optimizations. This integration results in a collaborative hardware/software co-optimization strategy that improves the precision and speed of DNN deployments. Concentrating on high-confidence configurations simplifies the search space and delivers superior performance compared to current optimization methods. The ARCO framework surpasses existing leading frameworks, achieving a throughput increase of up to 37.95% while reducing the optimization time by up to 42.2% across various DNNs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.09619</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.09619</id><created>2024-07-12</created><updated>2025-02-05</updated><authors><author><keyname>Santos</keyname><forenames>Joao F.</forenames></author><author><keyname>Huff</keyname><forenames>Alexandre</forenames></author><author><keyname>Campos</keyname><forenames>Daniel</forenames></author><author><keyname>Cardoso</keyname><forenames>Kleber V.</forenames></author><author><keyname>Both</keyname><forenames>Cristiano B.</forenames></author><author><keyname>DaSilva</keyname><forenames>Luiz A.</forenames></author></authors><title>Managing O-RAN Networks: xApp Development from Zero to Hero</title><categories>cs.NI cs.SY eess.SY</categories><doi>10.1109/COMST.2025.3539687 10.1109/COMST.2025.3539687   10.1109/COMST.2025.3539687</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Open Radio Access Network (O-RAN) Alliance proposes an open architecture that disaggregates the RAN and supports executing custom control logic in near-real time from third-party applications, the xApps. Despite O-RAN's efforts, the creation of xApps remains a complex and time-consuming endeavor, aggravated by the sometimes fragmented, outdated, or deprecated documentation from the O-RAN Software Community (OSC). These challenges hinder academia and industry from developing and validating solutions and algorithms on O-RAN networks. This tutorial addresses this gap by providing the first comprehensive guide for developing xApps to manage the O-RAN ecosystem from theory to practice. We provide a thorough theoretical foundation of the O-RAN architecture and detail the functionality offered by Near Real-Time RAN Intelligent Controller (Near-RT RIC) components. We examine the xApp design and configuration. We explore the xApp lifecycle and demonstrate how to deploy and manage xApps on a Near-RT RIC. We address the xApps' interfaces and capabilities, accompanied by practical examples. We provide comprehensive details on how xApps can control the RAN. We discuss debugging strategies and good practices to aid the xApp developers in testing their xApps. Finally, we review the current landscape and open challenges for creating xApps. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.09896</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.09896</id><created>2024-07-13</created><updated>2025-02-05</updated><authors><author><keyname>Elata</keyname><forenames>Noam</forenames></author><author><keyname>Michaeli</keyname><forenames>Tomer</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author></authors><title>PSC: Posterior Sampling-Based Compression</title><categories>cs.CV eess.IV</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Diffusion models have transformed the landscape of image generation and now show remarkable potential for image compression. Most of the recent diffusion-based compression methods require training and are tailored for a specific bit-rate. In this work, we propose Posterior Sampling-based Compression (PSC) - a zero-shot compression method that leverages a pre-trained diffusion model as its sole neural network component, thus enabling the use of diverse, publicly available models without additional training. Our approach is inspired by transform coding methods, which encode the image in some pre-chosen transform domain. However, PSC constructs a transform that is adaptive to the image. This is done by employing a zero-shot diffusion-based posterior sampler so as to progressively construct the rows of the transform matrix. Each new chunk of rows is chosen to reduce the uncertainty about the image given the quantized measurements collected thus far. Importantly, the same adaptive scheme can be replicated at the decoder, thus avoiding the need to encode the transform itself. We demonstrate that even with basic quantization and entropy coding, PSC's performance is comparable to established training-based methods in terms of rate, distortion, and perceptual quality. This is while providing greater flexibility, allowing to choose at inference time any desired rate or distortion. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.11784</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.11784</id><created>2024-07-16</created><updated>2025-02-04</updated><authors><author><keyname>Chen</keyname><forenames>Daoyuan</forenames></author><author><keyname>Wang</keyname><forenames>Haibin</forenames></author><author><keyname>Huang</keyname><forenames>Yilun</forenames></author><author><keyname>Ge</keyname><forenames>Ce</forenames></author><author><keyname>Li</keyname><forenames>Yaliang</forenames></author><author><keyname>Ding</keyname><forenames>Bolin</forenames></author><author><keyname>Zhou</keyname><forenames>Jingren</forenames></author></authors><title>Data-Juicer Sandbox: A Feedback-Driven Suite for Multimodal Data-Model   Co-development</title><categories>cs.AI cs.CV cs.LG</categories><comments>31 pages, 12 tables, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of multimodal large models has advanced artificial intelligence, introducing unprecedented levels of performance and functionality. However, optimizing these models remains challenging due to historically isolated paths of model-centric and data-centric developments, leading to suboptimal outcomes and inefficient resource utilization. In response, we present a new sandbox suite tailored for integrated data-model co-development. This sandbox provides a feedback-driven experimental platform, enabling cost-effective iteration and guided refinement of both data and models. Our proposed ``Probe-Analyze-Refine'' workflow, validated through practical use cases on multimodal tasks such as image-text pre-training with CLIP, image-to-text generation with LLaVA-like models, and text-to-video generation with DiT-based models, yields transferable and notable performance boosts, such as topping the VBench leaderboard. Extensive experiments also uncover fruitful insights into the interplay between data quality, diversity, model behavior, and computational costs. All codes, datasets, and models are open-sourced to foster future research and applications that would otherwise be infeasible due to the lack of a dedicated co-development infrastructure. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.12540</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.12540</id><created>2024-07-17</created><authors><author><keyname>Izzo</keyname><forenames>Giuseppe</forenames></author><author><keyname>Messina</keyname><forenames>Eleonora</forenames></author><author><keyname>Pezzella</keyname><forenames>Mario</forenames></author><author><keyname>Vecchio</keyname><forenames>Antonia</forenames></author></authors><title>Modified Patankar Linear Multistep methods for production-destruction   systems</title><categories>math.NA cs.NA math.DS</categories><msc-class>65L05 (Primary) 65L06, 34B18 (Secondary)</msc-class><journal-ref>J Sci Comput 102, 87 (2025)</journal-ref><doi>10.1007/s10915-025-02804-5</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Modified Patankar schemes are linearly implicit time integration methods designed to be unconditionally positive and conservative. In the present work we extend the Patankar-type approach to linear multistep methods and prove that the resulting discretizations retain, with no restrictions on the step size, the positivity of the solution and the linear invariant of the continuous-time system. Moreover, we provide results on arbitrarily high order of convergence and we introduce an embedding technique for the Patankar weight denominators to achieve it. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.14111</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.14111</id><created>2024-07-19</created><updated>2025-02-05</updated><authors><author><keyname>Wang</keyname><forenames>Shuche</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author></authors><title>A Mirror Descent-Based Algorithm for Corruption-Tolerant Distributed   Gradient Descent</title><categories>eess.SP cs.IT cs.LG math.IT</categories><comments>Accepted to the IEEE Transactions on Signal Processing</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Distributed gradient descent algorithms have come to the fore in modern machine learning, especially in parallelizing the handling of large datasets that are distributed across several workers. However, scant attention has been paid to analyzing the behavior of distributed gradient descent algorithms in the presence of adversarial corruptions instead of random noise. In this paper, we formulate a novel problem in which adversarial corruptions are present in a distributed learning system. We show how to use ideas from (lazy) mirror descent to design a corruption-tolerant distributed optimization algorithm. Extensive convergence analysis for (strongly) convex loss functions is provided for different choices of the stepsize. We carefully optimize the stepsize schedule to accelerate the convergence of the algorithm, while at the same time amortizing the effect of the corruption over time. Experiments based on linear regression, support vector classification, and softmax classification on the MNIST dataset corroborate our theoretical findings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.14206</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.14206</id><created>2024-07-19</created><updated>2025-02-05</updated><authors><author><keyname>Chang</keyname><forenames>Hongyan</forenames></author><author><keyname>Hassani</keyname><forenames>Hamed</forenames></author><author><keyname>Shokri</keyname><forenames>Reza</forenames></author></authors><title>Watermark Smoothing Attacks against Language Models</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Watermarking is a key technique for detecting AI-generated text. In this work, we study its vulnerabilities and introduce the Smoothing Attack, a novel watermark removal method. By leveraging the relationship between the model's confidence and watermark detectability, our attack selectively smoothes the watermarked content, erasing watermark traces while preserving text quality. We validate our attack on open-source models ranging from $1.3$B to $30$B parameters on $10$ different watermarks, demonstrating its effectiveness. Our findings expose critical weaknesses in existing watermarking schemes and highlight the need for stronger defenses. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.16693</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.16693</id><created>2024-07-23</created><updated>2025-02-05</updated><authors><author><keyname>Ferreira</keyname><forenames>Pedro</forenames></author><author><keyname>Titov</keyname><forenames>Ivan</forenames></author><author><keyname>Aziz</keyname><forenames>Wilker</forenames></author></authors><title>Explanation Regularisation through the Lens of Attributions</title><categories>cs.CL</categories><comments>COLING 2025 Camera-ready</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Explanation regularisation (ER) has been introduced as a way to guide text classifiers to form their predictions relying on input tokens that humans consider plausible. This is achieved by introducing an auxiliary explanation loss that measures how well the output of an input attribution technique for the model agrees with human-annotated rationales. The guidance appears to benefit performance in out-of-domain (OOD) settings, presumably due to an increased reliance on "plausible" tokens. However, previous work has under-explored the impact of guidance on that reliance, particularly when reliance is measured using attribution techniques different from those used to guide the model. In this work, we seek to close this gap, and also explore the relationship between reliance on plausible features and OOD performance. We find that the connection between ER and the ability of a classifier to rely on plausible features has been overstated and that a stronger reliance on plausible tokens does not seem to be the cause for OOD improvements. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.16920</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.16920</id><created>2024-07-23</created><updated>2025-02-05</updated><authors><author><keyname>Seo</keyname><forenames>Yeongbin</forenames></author><author><keyname>Lee</keyname><forenames>Dongha</forenames></author><author><keyname>Yeo</keyname><forenames>Jinyoung</forenames></author></authors><title>Train-Attention: Meta-Learning Where to Focus in Continual Knowledge   Learning</title><categories>cs.CL</categories><msc-class>68T50</msc-class><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous studies on continual knowledge learning (CKL) in large language models (LLMs) have predominantly focused on approaches such as regularization, architectural modifications, and rehearsal techniques to mitigate catastrophic forgetting. However, these methods naively inherit the inefficiencies of standard training procedures, indiscriminately applying uniform weight across all tokens, which can lead to unnecessary parameter updates and increased forgetting. To address these shortcomings, we propose a novel CKL approach termed Train-Attention-Augmented Language Model (TAALM), which enhances learning efficiency by dynamically predicting and applying weights to tokens based on their usefulness. This method employs a meta-learning framework that optimizes token importance predictions, facilitating targeted knowledge updates and minimizing forgetting. Also, we observe that existing benchmarks do not clearly exhibit the trade-off between learning and retaining, therefore we propose a new benchmark, \textsc{LAMA-ckl}, to address this issue. Through experiments conducted on both newly introduced and established CKL benchmarks, TAALM proves the state-of-the-art performance upon the baselines, and also shows synergistic compatibility when integrated with previous CKL approaches. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.17678</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.17678</id><created>2024-07-24</created><updated>2025-02-05</updated><authors><author><keyname>Lin</keyname><forenames>Xihui</forenames></author><author><keyname>Zhang</keyname><forenames>Yunan</forenames></author><author><keyname>Ge</keyname><forenames>Suyu</forenames></author><author><keyname>Ren</keyname><forenames>Liliang</forenames></author><author><keyname>Patra</keyname><forenames>Barun</forenames></author><author><keyname>Chaudhary</keyname><forenames>Vishrav</forenames></author><author><keyname>Peng</keyname><forenames>Hao</forenames></author><author><keyname>Song</keyname><forenames>Xia</forenames></author></authors><title>S2-Attention: Hardware-Aware Context Sharding Among Attention Heads</title><categories>cs.CL</categories><comments>10 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sparse attention, which selectively attends to a subset of tokens in the context was supposed to be efficient. However, its theoretical reduction in FLOPs has rarely translated into wall-clock speed-up over its dense attention counterparts due to the lack of hardware-aware optimizations like FlashAttention. Meanwhile, it remains unclear whether sparse attention can maintain the model's quality at a scale of today's large language models (LLMs) and how. This paper presents Sparsely-Sharded(S2) Attention, a Triton library that provides kernel optimization for sparse attention customizable at both per-head and per-context-range levels. S2-Attention enables the exploration of novel and high-performance sparse attention techniques, which we demonstrate through extensive ablations across a wide range of sparse attention designs at various model scales. From these insights, we present several basic guidelines to design sparse attention that can achieve not only practical efficiency improvements, but also strong downstream performance. To achieve high parallelization and optimized memory IO, sparse attention should shard the context heterogeneously across attention heads, where each head attends to a different subset of tokens while collectively covering the full context. Meanwhile, we find hybrid architectures combining sparse and dense attention particularly beneficial in practice. S2-Attention achieves wall-clock speedup of 8.79X, 15.87X, 25.3X compared to the strong FlashAttention-2 baseline with strong downstream performance on-par with full attention and perfect retrieval performance at a 128k context length. At inference, for 7B models, our model, with the help of our S2-Attention kernel, achieves 4.5x speed-up compared to dense counterparts. S2-Attention is released with easy-to-customize APIs for direct usage in Megatron and vLLM. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.17754</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.17754</id><created>2024-07-25</created><updated>2025-02-05</updated><authors><author><keyname>Zhu</keyname><forenames>Guogang</forenames></author><author><keyname>Liu</keyname><forenames>Xuefeng</forenames></author><author><keyname>Niu</keyname><forenames>Jianwei</forenames></author><author><keyname>Tang</keyname><forenames>Shaojie</forenames></author><author><keyname>Wu</keyname><forenames>Xinghao</forenames></author><author><keyname>Zhang</keyname><forenames>Jiayuan</forenames></author></authors><title>DualFed: Enjoying both Generalization and Personalization in Federated   Learning via Hierachical Representations</title><categories>cs.LG cs.DC</categories><comments>Accepted by ACM Multimedia 2024</comments><doi>10.1145/3664647.3681260</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In personalized federated learning (PFL), it is widely recognized that achieving both high model generalization and effective personalization poses a significant challenge due to their conflicting nature. As a result, existing PFL methods can only manage a trade-off between these two objectives. This raises an interesting question: Is it feasible to develop a model capable of achieving both objectives simultaneously? Our paper presents an affirmative answer, and the key lies in the observation that deep models inherently exhibit hierarchical architectures, which produce representations with various levels of generalization and personalization at different stages. A straightforward approach stemming from this observation is to select multiple representations from these layers and combine them to concurrently achieve generalization and personalization. However, the number of candidate representations is commonly huge, which makes this method infeasible due to high computational costs.To address this problem, we propose DualFed, a new method that can directly yield dual representations correspond to generalization and personalization respectively, thereby simplifying the optimization task. Specifically, DualFed inserts a personalized projection network between the encoder and classifier. The pre-projection representations are able to capture generalized information shareable across clients, and the post-projection representations are effective to capture task-specific information on local clients. This design minimizes the mutual interference between generalization and personalization, thereby achieving a win-win situation. Extensive experiments show that DualFed can outperform other FL methods. Code is available at https://github.com/GuogangZhu/DualFed. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.19464</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.19464</id><created>2024-07-28</created><updated>2025-02-05</updated><authors><author><keyname>Walch</keyname><forenames>Andreas</forenames></author><author><keyname>Szabo</keyname><forenames>Attila</forenames></author><author><keyname>Steinlechner</keyname><forenames>Harald</forenames></author><author><keyname>Ortner</keyname><forenames>Thomas</forenames></author><author><keyname>Gröller</keyname><forenames>Eduard</forenames></author><author><keyname>Schmidt</keyname><forenames>Johanna</forenames></author></authors><title>BEMTrace: Visualization-driven approach for deriving Building Energy   Models from BIM</title><categories>cs.HC</categories><comments>9 pages and 2 pages references, 12 figures</comments><acm-class>H.4.0; I.3.0</acm-class><journal-ref>IEEE Transactions on Visualization and Computer Graphics,   31(01):240-250, 2025</journal-ref><doi>10.1109/TVCG.2024.3456315</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Building Information Modeling (BIM) describes a central data pool covering the entire life cycle of a construction project. Similarly, Building Energy Modeling (BEM) describes the process of using a 3D representation of a building as a basis for thermal simulations to assess the building's energy performance. This paper explores the intersection of BIM and BEM, focusing on the challenges and methodologies in converting BIM data into BEM representations for energy performance analysis. BEMTrace integrates 3D data wrangling techniques with visualization methodologies to enhance the accuracy and traceability of the BIM-to-BEM conversion process. Through parsing, error detection, and algorithmic correction of BIM data, our methods generate valid BEM models suitable for energy simulation. Visualization techniques provide transparent insights into the conversion process, aiding error identification, validation, and user comprehension. We introduce context-adaptive selections to facilitate user interaction and to show that the BEMTrace workflow helps users understand complex 3D data wrangling processes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.20959</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.20959</id><created>2024-07-30</created><updated>2025-02-05</updated><authors><author><keyname>Cruz</keyname><forenames>Ricardo P. M.</forenames></author><author><keyname>Cristino</keyname><forenames>Rafael</forenames></author><author><keyname>Cardoso</keyname><forenames>Jaime S.</forenames></author></authors><title>Learning Ordinality in Semantic Segmentation</title><categories>cs.CV cs.LG</categories><comments>13 pages</comments><journal-ref>IEEE Access (2025)</journal-ref><doi>10.1109/ACCESS.2025.3537601</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Semantic segmentation consists of predicting a semantic label for each image pixel. While existing deep learning approaches achieve high accuracy, they often overlook the ordinal relationships between classes, which can provide critical domain knowledge (e.g., the pupil lies within the iris, and lane markings are part of the road). This paper introduces novel methods for spatial ordinal segmentation that explicitly incorporate these inter-class dependencies. By treating each pixel as part of a structured image space rather than as an independent observation, we propose two regularization terms and a new metric to enforce ordinal consistency between neighboring pixels. Two loss regularization terms and one metric are proposed for structural ordinal segmentation, which penalizes predictions of non-ordinal adjacent classes. Five biomedical datasets and multiple configurations of autonomous driving datasets demonstrate the efficacy of the proposed methods. Our approach achieves improvements in ordinal metrics and enhances generalization, with up to a 15.7% relative increase in the Dice coefficient. Importantly, these benefits come without additional inference time costs. This work highlights the significance of spatial ordinal relationships in semantic segmentation and provides a foundation for further exploration in structured image representations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.01063</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.01063</id><created>2024-08-02</created><updated>2025-02-05</updated><authors><author><keyname>Motger</keyname><forenames>Quim</forenames></author><author><keyname>Miaschi</keyname><forenames>Alessio</forenames></author><author><keyname>Dell'Orletta</keyname><forenames>Felice</forenames></author><author><keyname>Franch</keyname><forenames>Xavier</forenames></author><author><keyname>Marco</keyname><forenames>Jordi</forenames></author></authors><title>Leveraging Encoder-only Large Language Models for Mobile App Review   Feature Extraction</title><categories>cs.CL cs.SE</categories><comments>46 pages, 7 tables, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile app review analysis presents unique challenges due to the low quality, subjective bias, and noisy content of user-generated documents. Extracting features from these reviews is essential for tasks such as feature prioritization and sentiment analysis, but it remains a challenging task. Meanwhile, encoder-only models based on the Transformer architecture have shown promising results for classification and information extraction tasks for multiple software engineering processes. This study explores the hypothesis that encoder-only large language models can enhance feature extraction from mobile app reviews. By leveraging crowdsourced annotations from an industrial context, we redefine feature extraction as a supervised token classification task. Our approach includes extending the pre-training of these models with a large corpus of user reviews to improve contextual understanding and employing instance selection techniques to optimize model fine-tuning. Empirical evaluations demonstrate that this method improves the precision and recall of extracted features and enhances performance efficiency. Key contributions include a novel approach to feature extraction, annotated datasets, extended pre-trained models, and an instance selection mechanism for cost-effective fine-tuning. This research provides practical methods and empirical evidence in applying large language models to natural language processing tasks within mobile app reviews, offering improved performance in feature extraction. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.03356</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.03356</id><created>2024-08-06</created><updated>2025-02-04</updated><authors><author><keyname>Blanc</keyname><forenames>Hugo</forenames></author><author><keyname>Deschaud</keyname><forenames>Jean-Emmanuel</forenames></author><author><keyname>Paljic</keyname><forenames>Alexis</forenames></author></authors><title>RayGauss: Volumetric Gaussian-Based Ray Casting for Photorealistic Novel   View Synthesis</title><categories>cs.CV cs.GR</categories><comments>Project page with videos and code: https://raygauss.github.io/</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Differentiable volumetric rendering-based methods made significant progress in novel view synthesis. On one hand, innovative methods have replaced the Neural Radiance Fields (NeRF) network with locally parameterized structures, enabling high-quality renderings in a reasonable time. On the other hand, approaches have used differentiable splatting instead of NeRF's ray casting to optimize radiance fields rapidly using Gaussian kernels, allowing for fine adaptation to the scene. However, differentiable ray casting of irregularly spaced kernels has been scarcely explored, while splatting, despite enabling fast rendering times, is susceptible to clearly visible artifacts.   Our work closes this gap by providing a physically consistent formulation of the emitted radiance c and density {\sigma}, decomposed with Gaussian functions associated with Spherical Gaussians/Harmonics for all-frequency colorimetric representation. We also introduce a method enabling differentiable ray casting of irregularly distributed Gaussians using an algorithm that integrates radiance fields slab by slab and leverages a BVH structure. This allows our approach to finely adapt to the scene while avoiding splatting artifacts. As a result, we achieve superior rendering quality compared to the state-of-the-art while maintaining reasonable training times and achieving inference speeds of 25 FPS on the Blender dataset. Project page with videos and code: https://raygauss.github.io/ </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.03455</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.03455</id><created>2024-08-06</created><updated>2025-02-05</updated><authors><author><keyname>McQuarrie</keyname><forenames>Shane A.</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Anirban</forenames></author><author><keyname>Willcox</keyname><forenames>Karen E.</forenames></author><author><keyname>Guo</keyname><forenames>Mengwu</forenames></author></authors><title>Bayesian learning with Gaussian processes for low-dimensional   representations of time-dependent nonlinear systems</title><categories>math.NA cs.NA</categories><comments>https://github.com/Sandialabs/GP-BayesOpInf</comments><msc-class>62F15, 60G15, 65C05, 35B30</msc-class><acm-class>G.3</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This work presents a data-driven method for learning low-dimensional time-dependent physics-based surrogate models whose predictions are endowed with uncertainty estimates. We use the operator inference approach to model reduction that poses the problem of learning low-dimensional model terms as a regression of state space data and corresponding time derivatives by minimizing the residual of reduced system equations. Standard operator inference models perform well with accurate training data that are dense in time, but producing stable and accurate models when the state data are noisy and/or sparse in time remains a challenge. Another challenge is the lack of uncertainty estimation for the predictions from the operator inference models. Our approach addresses these challenges by incorporating Gaussian process surrogates into the operator inference framework to (1) probabilistically describe uncertainties in the state predictions and (2) procure analytical time derivative estimates with quantified uncertainties. The formulation leads to a generalized least-squares regression and, ultimately, reduced-order models that are described probabilistically with a closed-form expression for the posterior distribution of the operators. The resulting probabilistic surrogate model propagates uncertainties from the observed state data to reduced-order predictions. We demonstrate the method is effective for constructing low-dimensional models of two nonlinear partial differential equations representing a compressible flow and a nonlinear diffusion-reaction process, as well as for estimating the parameters of a low-dimensional system of nonlinear ordinary differential equations representing compartmental models in epidemiology. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.05212</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.05212</id><created>2024-08-10</created><authors><author><keyname>Miranda</keyname><forenames>Michele</forenames></author><author><keyname>Ruzzetti</keyname><forenames>Elena Sofia</forenames></author><author><keyname>Santilli</keyname><forenames>Andrea</forenames></author><author><keyname>Zanzotto</keyname><forenames>Fabio Massimo</forenames></author><author><keyname>Bratières</keyname><forenames>Sébastien</forenames></author><author><keyname>Rodolà</keyname><forenames>Emanuele</forenames></author></authors><title>Preserving Privacy in Large Language Models: A Survey on Current Threats   and Solutions</title><categories>cs.CR cs.AI cs.CL cs.LG</categories><comments>GitHub repository:   https://github.com/michele17284/Awesome-Privacy-Preserving-LLMs</comments><journal-ref>Transactions on Machine Learning Research, 2025</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large Language Models (LLMs) represent a significant advancement in artificial intelligence, finding applications across various domains. However, their reliance on massive internet-sourced datasets for training brings notable privacy issues, which are exacerbated in critical domains (e.g., healthcare). Moreover, certain application-specific scenarios may require fine-tuning these models on private data. This survey critically examines the privacy threats associated with LLMs, emphasizing the potential for these models to memorize and inadvertently reveal sensitive information. We explore current threats by reviewing privacy attacks on LLMs and propose comprehensive solutions for integrating privacy mechanisms throughout the entire learning pipeline. These solutions range from anonymizing training datasets to implementing differential privacy during training or inference and machine unlearning after training. Our comprehensive review of existing literature highlights ongoing challenges, available tools, and future directions for preserving privacy in LLMs. This work aims to guide the development of more secure and trustworthy AI systems by providing a thorough understanding of privacy preservation methods and their effectiveness in mitigating risks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.05534</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.05534</id><created>2024-08-10</created><updated>2025-02-04</updated><authors><author><keyname>Ahmed</keyname><forenames>Toufique</forenames></author><author><keyname>Devanbu</keyname><forenames>Premkumar</forenames></author><author><keyname>Treude</keyname><forenames>Christoph</forenames></author><author><keyname>Pradel</keyname><forenames>Michael</forenames></author></authors><title>Can LLMs Replace Manual Annotation of Software Engineering Artifacts?</title><categories>cs.SE cs.HC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Experimental evaluations of software engineering innovations, e.g., tools and processes, often include human-subject studies as a component of a multi-pronged strategy to obtain greater generalizability of the findings. However, human-subject studies in our field are challenging, due to the cost and difficulty of finding and employing suitable subjects, ideally, professional programmers with varying degrees of experience. Meanwhile, large language models (LLMs) have recently started to demonstrate human-level performance in several areas. This paper explores the possibility of substituting costly human subjects with much cheaper LLM queries in evaluations of code and code-related artifacts. We study this idea by applying six state-of-the-art LLMs to ten annotation tasks from five datasets created by prior work, such as judging the accuracy of a natural language summary of a method or deciding whether a code change fixes a static analysis warning. Our results show that replacing some human annotation effort with LLMs can produce inter-rater agreements equal or close to human-rater agreement. To help decide when and how to use LLMs in human-subject studies, we propose model-model agreement as a predictor of whether a given task is suitable for LLMs at all, and model confidence as a means to select specific samples where LLMs can safely replace human annotators. Overall, our work is the first step toward mixed human-LLM evaluations in software engineering. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.07471</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.07471</id><created>2024-08-14</created><updated>2025-02-05</updated><authors><author><keyname>Jiang</keyname><forenames>Yuxin</forenames></author><author><keyname>Huang</keyname><forenames>Bo</forenames></author><author><keyname>Wang</keyname><forenames>Yufei</forenames></author><author><keyname>Zeng</keyname><forenames>Xingshan</forenames></author><author><keyname>Li</keyname><forenames>Liangyou</forenames></author><author><keyname>Wang</keyname><forenames>Yasheng</forenames></author><author><keyname>Jiang</keyname><forenames>Xin</forenames></author><author><keyname>Shang</keyname><forenames>Lifeng</forenames></author><author><keyname>Tang</keyname><forenames>Ruiming</forenames></author><author><keyname>Wang</keyname><forenames>Wei</forenames></author></authors><title>Bridging and Modeling Correlations in Pairwise Data for Direct   Preference Optimization</title><categories>cs.CL</categories><comments>20 pages, 9 figures, 12 tables. Accepted at ICLR 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Direct preference optimization (DPO), a widely adopted offline preference optimization algorithm, aims to align large language models (LLMs) with human-desired behaviors using pairwise preference data. However, the generation of the winning response and the losing response within pairwise data are typically isolated, leading to weak correlations between them as well as suboptimal alignment performance. To address this issue, we propose an effective framework for Bridging and Modeling Correlations in pairwise data, named BMC. Firstly, we increase the consistency and informativeness of the pairwise preference signals through targeted modifications, synthesizing a pseudo-winning response by improving the losing response with the winning response as a reference. Secondly, we identify that DPO alone is insufficient to model these correlations and capture nuanced variations. Therefore, we propose learning token-level correlations by dynamically leveraging the policy model's confidence during training. Comprehensive experiments on QA, math, and instruction-following tasks demonstrate the effectiveness of our approach, significantly surpassing competitive baselines, including DPO. Additionally, our in-depth quantitative analysis reveals the reasons behind our method's superior performance over DPO and showcases its versatility to other DPO variants. We release our repository at https://github.com/YJiangcm/BMC. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.08761</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.08761</id><created>2024-08-16</created><updated>2025-02-05</updated><authors><author><keyname>Marton</keyname><forenames>Sascha</forenames></author><author><keyname>Grams</keyname><forenames>Tim</forenames></author><author><keyname>Vogt</keyname><forenames>Florian</forenames></author><author><keyname>Lüdtke</keyname><forenames>Stefan</forenames></author><author><keyname>Bartelt</keyname><forenames>Christian</forenames></author><author><keyname>Stuckenschmidt</keyname><forenames>Heiner</forenames></author></authors><title>Mitigating Information Loss in Tree-Based Reinforcement Learning via   Direct Optimization</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Reinforcement learning (RL) has seen significant success across various domains, but its adoption is often limited by the black-box nature of neural network policies, making them difficult to interpret. In contrast, symbolic policies allow representing decision-making strategies in a compact and interpretable way. However, learning symbolic policies directly within on-policy methods remains challenging. In this paper, we introduce SYMPOL, a novel method for SYMbolic tree-based on-POLicy RL. SYMPOL employs a tree-based model integrated with a policy gradient method, enabling the agent to learn and adapt its actions while maintaining a high level of interpretability. We evaluate SYMPOL on a set of benchmark RL tasks, demonstrating its superiority over alternative tree-based RL approaches in terms of performance and interpretability. Unlike existing methods, it enables gradient-based, end-to-end learning of interpretable, axis-aligned decision trees within standard on-policy RL algorithms. Therefore, SYMPOL can become the foundation for a new class of interpretable RL based on decision trees. Our implementation is available under: https://github.com/s-marton/sympol </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.10069</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.10069</id><created>2024-08-19</created><updated>2025-02-05</updated><authors><author><keyname>Dorent</keyname><forenames>Reuben</forenames></author><author><keyname>Khajavi</keyname><forenames>Roya</forenames></author><author><keyname>Idris</keyname><forenames>Tagwa</forenames></author><author><keyname>Ziegler</keyname><forenames>Erik</forenames></author><author><keyname>Somarouthu</keyname><forenames>Bhanusupriya</forenames></author><author><keyname>Jacene</keyname><forenames>Heather</forenames></author><author><keyname>LaCasce</keyname><forenames>Ann</forenames></author><author><keyname>Deissler</keyname><forenames>Jonathan</forenames></author><author><keyname>Ehrhardt</keyname><forenames>Jan</forenames></author><author><keyname>Engelson</keyname><forenames>Sofija</forenames></author><author><keyname>Fischer</keyname><forenames>Stefan M.</forenames></author><author><keyname>Gu</keyname><forenames>Yun</forenames></author><author><keyname>Handels</keyname><forenames>Heinz</forenames></author><author><keyname>Kasai</keyname><forenames>Satoshi</forenames></author><author><keyname>Kondo</keyname><forenames>Satoshi</forenames></author><author><keyname>Maier-Hein</keyname><forenames>Klaus</forenames></author><author><keyname>Schnabel</keyname><forenames>Julia A.</forenames></author><author><keyname>Wang</keyname><forenames>Guotai</forenames></author><author><keyname>Wang</keyname><forenames>Litingyu</forenames></author><author><keyname>Wald</keyname><forenames>Tassilo</forenames></author><author><keyname>Yang</keyname><forenames>Guang-Zhong</forenames></author><author><keyname>Zhang</keyname><forenames>Hanxiao</forenames></author><author><keyname>Zhang</keyname><forenames>Minghui</forenames></author><author><keyname>Pieper</keyname><forenames>Steve</forenames></author><author><keyname>Harris</keyname><forenames>Gordon</forenames></author><author><keyname>Kikinis</keyname><forenames>Ron</forenames></author><author><keyname>Kapur</keyname><forenames>Tina</forenames></author></authors><title>LNQ 2023 challenge: Benchmark of weakly-supervised techniques for   mediastinal lymph node quantification</title><categories>cs.CV</categories><comments>Submitted to MELBA; Accepted for publication at the Journal of   Machine Learning for Biomedical Imaging (MELBA)   https://melba-journal.org/2025:001</comments><journal-ref>Machine.Learning.for.Biomedical.Imaging. 3 (2025)</journal-ref><doi>10.59275/j.melba.2025-d482</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Accurate assessment of lymph node size in 3D CT scans is crucial for cancer staging, therapeutic management, and monitoring treatment response. Existing state-of-the-art segmentation frameworks in medical imaging often rely on fully annotated datasets. However, for lymph node segmentation, these datasets are typically small due to the extensive time and expertise required to annotate the numerous lymph nodes in 3D CT scans. Weakly-supervised learning, which leverages incomplete or noisy annotations, has recently gained interest in the medical imaging community as a potential solution. Despite the variety of weakly-supervised techniques proposed, most have been validated only on private datasets or small publicly available datasets. To address this limitation, the Mediastinal Lymph Node Quantification (LNQ) challenge was organized in conjunction with the 26th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2023). This challenge aimed to advance weakly-supervised segmentation methods by providing a new, partially annotated dataset and a robust evaluation framework. A total of 16 teams from 5 countries submitted predictions to the validation leaderboard, and 6 teams from 3 countries participated in the evaluation phase. The results highlighted both the potential and the current limitations of weakly-supervised approaches. On one hand, weakly-supervised approaches obtained relatively good performance with a median Dice score of $61.0\%$. On the other hand, top-ranked teams, with a median Dice score exceeding $70\%$, boosted their performance by leveraging smaller but fully annotated datasets to combine weak supervision and full supervision. This highlights both the promise of weakly-supervised methods and the ongoing need for high-quality, fully annotated data to achieve higher segmentation performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.11085</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.11085</id><created>2024-08-20</created><updated>2025-02-05</updated><authors><author><keyname>Liu</keyname><forenames>Changkun</forenames></author><author><keyname>Chen</keyname><forenames>Shuai</forenames></author><author><keyname>Bhalgat</keyname><forenames>Yash</forenames></author><author><keyname>Hu</keyname><forenames>Siyan</forenames></author><author><keyname>Cheng</keyname><forenames>Ming</forenames></author><author><keyname>Wang</keyname><forenames>Zirui</forenames></author><author><keyname>Prisacariu</keyname><forenames>Victor Adrian</forenames></author><author><keyname>Braud</keyname><forenames>Tristan</forenames></author></authors><title>GS-CPR: Efficient Camera Pose Refinement via 3D Gaussian Splatting</title><categories>cs.CV</categories><comments>Accepted to ICLR2025. During the ICLR review process, we changed the   name of our framework from GSLoc to GS-CPR (Camera Pose Refinement) according   to the comments of reviewers. The project page is available at   https://gsloc.active.vision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We leverage 3D Gaussian Splatting (3DGS) as a scene representation and propose a novel test-time camera pose refinement (CPR) framework, GS-CPR. This framework enhances the localization accuracy of state-of-the-art absolute pose regression and scene coordinate regression methods. The 3DGS model renders high-quality synthetic images and depth maps to facilitate the establishment of 2D-3D correspondences. GS-CPR obviates the need for training feature extractors or descriptors by operating directly on RGB images, utilizing the 3D foundation model, MASt3R, for precise 2D matching. To improve the robustness of our model in challenging outdoor environments, we incorporate an exposure-adaptive module within the 3DGS framework. Consequently, GS-CPR enables efficient one-shot pose refinement given a single RGB query and a coarse initial pose estimation. Our proposed approach surpasses leading NeRF-based optimization methods in both accuracy and runtime across indoor and outdoor visual localization benchmarks, achieving new state-of-the-art accuracy on two indoor datasets. The project page is available at https://gsloc.active.vision. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.11386</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.11386</id><created>2024-08-21</created><updated>2024-09-12</updated><authors><author><keyname>Klessascheck</keyname><forenames>Finn</forenames></author><author><keyname>Fahrenkrog-Petersen</keyname><forenames>Stephan A.</forenames></author><author><keyname>Mendling</keyname><forenames>Jan</forenames></author><author><keyname>Pufahl</keyname><forenames>Luise</forenames></author></authors><title>Unlocking Sustainability Compliance: Characterizing the EU Taxonomy for   Business Process Management</title><categories>cs.CY cs.DB</categories><journal-ref>Lecture Notes in Computer Science 15409 (2025) 339-359</journal-ref><doi>10.1007/978-3-031-78338-8_18</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  To promote sustainable business practices, and to achieve climate neutrality by 2050, the EU has developed the taxonomy of sustainable activities, which describes when exactly business practices can be considered sustainable. While the taxonomy has only been recently established, progressively more companies will have to report how much of their revenue was created via sustainably executed business processes. To help companies prepare to assess whether their business processes comply with the constraints outlined in the taxonomy, we investigate in how far these criteria can be used for conformance checking, that is, assessing in a data-driven manner, whether business process executions adhere to regulatory constraints. For this, we develop a few-shot learning pipeline to characterize the constraints of the taxonomy with the help of an LLM as to the process dimensions they relate to. We find that many constraints of the taxonomy are useable for conformance checking, particularly in the sectors of energy, manufacturing, and transport. This will aid companies in preparing to monitor regulatory compliance with the taxonomy automatically, by characterizing what kind of information they need to extract, and by providing a better understanding of sectors where such an assessment is feasible and where it is not. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.12830</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.12830</id><created>2024-08-23</created><updated>2025-02-05</updated><authors><author><keyname>Luo</keyname><forenames>Wang</forenames></author><author><keyname>Li</keyname><forenames>Haoran</forenames></author><author><keyname>Zhang</keyname><forenames>Zicheng</forenames></author><author><keyname>Han</keyname><forenames>Congying</forenames></author><author><keyname>Lv</keyname><forenames>Jiayu</forenames></author><author><keyname>Guo</keyname><forenames>Tiande</forenames></author></authors><title>SAMBO-RL: Shifts-aware Model-based Offline Reinforcement Learning</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-based offline reinforcement learning trains policies using pre-collected datasets and learned environment models, eliminating the need for direct real-world environment interaction. However, this paradigm is inherently challenged by distribution shift (DS). Existing methods address this issue by leveraging off-policy mechanisms and estimating model uncertainty, but they often result in inconsistent objectives and lack a unified theoretical foundation. This paper offers a comprehensive analysis that disentangles the problem into two fundamental components: model bias and policy shift. Our theoretical and empirical investigations reveal how these factors distort value estimation and restrict policy optimization. To tackle these challenges, we derive a novel Shifts-aware Reward (SAR) through a unified probabilistic inference framework, which modifies the vanilla reward to refine value learning and facilitate policy training. Building on this, we introduce Shifts-aware Model-based Offline Reinforcement Learning (SAMBO-RL), a practical framework that efficiently trains classifiers to approximate SAR for policy optimization. Empirical experiments show that SAR effectively mitigates DS, and SAMBO-RL achieves superior or comparable performance across various benchmarks, underscoring its effectiveness and validating our theoretical analysis. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.13257</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.13257</id><created>2024-08-23</created><updated>2025-02-05</updated><authors><author><keyname>Zhang</keyname><forenames>Yi-Fan</forenames></author><author><keyname>Zhang</keyname><forenames>Huanyu</forenames></author><author><keyname>Tian</keyname><forenames>Haochen</forenames></author><author><keyname>Fu</keyname><forenames>Chaoyou</forenames></author><author><keyname>Zhang</keyname><forenames>Shuangqing</forenames></author><author><keyname>Wu</keyname><forenames>Junfei</forenames></author><author><keyname>Li</keyname><forenames>Feng</forenames></author><author><keyname>Wang</keyname><forenames>Kun</forenames></author><author><keyname>Wen</keyname><forenames>Qingsong</forenames></author><author><keyname>Zhang</keyname><forenames>Zhang</forenames></author><author><keyname>Wang</keyname><forenames>Liang</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Tan</keyname><forenames>Tieniu</forenames></author></authors><title>MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution   Real-World Scenarios that are Difficult for Humans?</title><categories>cs.CV</categories><comments>Project Page: https://mme-realworld.github.io/; accepted by ICLR 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comprehensive evaluation of Multimodal Large Language Models (MLLMs) has recently garnered widespread attention in the research community. However, we observe that existing benchmarks present several common barriers that make it difficult to measure the significant challenges that models face in the real world, including: 1) small data scale leads to a large performance variance; 2) reliance on model-based annotations results in restricted data quality; 3) insufficient task difficulty, especially caused by the limited image resolution. To tackle these issues, we introduce MME-RealWorld. Specifically, we collect more than $300$K images from public datasets and the Internet, filtering $13,366$ high-quality images for annotation. This involves the efforts of professional $25$ annotators and $7$ experts in MLLMs, contributing to $29,429$ question-answer pairs that cover $43$ subtasks across $5$ real-world scenarios, extremely challenging even for humans. As far as we know, MME-RealWorld is the largest manually annotated benchmark to date, featuring the highest resolution and a targeted focus on real-world applications. We further conduct a thorough evaluation involving $28$ prominent MLLMs, such as GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. Our results show that even the most advanced models struggle with our benchmarks, where none of them reach $60\%$ accuracy. The challenges of perceiving high-resolution images and understanding complex real-world scenarios remain urgent issues to be addressed. The data and evaluation code are released at https://mme-realworld.github.io/ . </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.13849</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.13849</id><created>2024-08-25</created><updated>2025-01-21</updated><authors><author><keyname>Xu</keyname><forenames>Weida</forenames></author><author><keyname>Xu</keyname><forenames>Yang</forenames></author><author><keyname>Zhang</keyname><forenames>Sicong</forenames></author></authors><title>Sample-Independent Federated Learning Backdoor Attack in Speaker   Recognition</title><categories>cs.CR</categories><journal-ref>Cluster Comput 28, 158 (2025)</journal-ref><doi>10.1007/s10586-024-04837-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In federated learning, backdoor attacks embed triggers in the adversarial client's data to inject a backdoor into the model. In order to enhance the stealth, an attack method based on the dropout layer has been proposed, which can implant the backdoor without modifying the sample. However, these methods struggle to covertly utilize dropout in evaluation mode, thus hindering their deployment in real-world scenarios. To address these, this paper introduces GhostB, a novel approach to federated learning backdoor attacks in speaker recognition that neither alters samples nor relies on dropout. This method employs the behavior of neurons producing specific values as triggers. By mapping these neuronal values to categories specified by the adversary, the backdoor is implanted and activated when particular feature values are detected at designated neurons. Our experiments conducted on TIMIT, LibriSpeech, and VoxCeleb2 databases in both Closed Set Identification (CSI) and Open Set Identification (OSI) scenarios demonstrate that GhostB achieves a 100% success rate upon activation in speaker recognition, with this rate maintained across experiments involving 1 to 50 ghost neurons. This paper investigates how the dispersion of neurons and their depth within hidden layers affect the success rate, revealing that increased dispersion and positioning of neurons can significantly decrease effectiveness, potentially rendering the attack unsuccessful. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.15408</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.15408</id><created>2024-08-27</created><updated>2025-02-04</updated><authors><author><keyname>Khorrami</keyname><forenames>Mohammad S.</forenames></author><author><keyname>Goyal</keyname><forenames>Pawan</forenames></author><author><keyname>Mianroodi</keyname><forenames>Jaber R.</forenames></author><author><keyname>Svendsen</keyname><forenames>Bob</forenames></author><author><keyname>Benner</keyname><forenames>Peter</forenames></author><author><keyname>Raabe</keyname><forenames>Dierk</forenames></author></authors><title>A physics-encoded Fourier neural operator approach for surrogate   modeling of divergence-free stress fields in solids</title><categories>cs.CE cond-mat.mtrl-sci cs.LG math.AP</categories><comments>17 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of the current work is the development of a so-called physics-encoded Fourier neural operator (PeFNO) for surrogate modeling of the quasi-static equilibrium stress field in solids. Rather than accounting for constraints from physics in the loss function as done in the (now standard) physics-informed approach, the physics-encoded approach incorporates or "encodes" such constraints directly into the network or operator architecture. As a result, in contrast to the physics-informed approach in which only training is physically constrained, both training and output are physically constrained in the physics-encoded approach. For the current constraint of divergence-free stress, a novel encoding approach based on a stress potential is proposed.   As a "proof-of-concept" example application of the proposed PeFNO, a heterogeneous polycrystalline material consisting of isotropic elastic grains subject to uniaxial extension is considered. Stress field data for training are obtained from the numerical solution of a corresponding boundary-value problem for quasi-static mechanical equilibrium. This data is also employed to train an analogous physics-guided FNO (PgFNO) and physics-informed FNO (PiFNO) for comparison. As confirmed by this comparison and as expected on the basis of their differences, the output of the trained PeFNO is significantly more accurate in satisfying mechanical equilibrium than the output of either the trained PgFNO or the trained PiFNO. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.00672</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.00672</id><created>2024-09-01</created><updated>2025-02-05</updated><authors><author><keyname>Mitchell</keyname><forenames>Chris J</forenames></author><author><keyname>Wild</keyname><forenames>Peter R</forenames></author></authors><title>Orientable and negative orientable sequences</title><categories>math.CO cs.DM</categories><comments>Numerical tables enlarged</comments><msc-class>94A55, 05C38</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analogously to de Bruijn sequences, orientable sequences have application in automatic position-location applications and, until recently, studies of these sequences focused on the binary case. In recent work by Alhakim et al., a range of methods of construction were described for orientable sequences over arbitrary finite alphabets; some of these methods involve using negative orientable sequences as a building block. In this paper we describe three techniques for generating such negative orientable sequences, as well as upper bounds on their period. We then go on to show how these negative orientable sequences can be used to generate orientable sequences with period close to the maximum possible for every non-binary alphabet size and for every tuple length. In doing so we use two closely related approaches described by Alhakim et al. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.01658</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.01658</id><created>2024-09-03</created><updated>2025-02-05</updated><authors><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Huang</keyname><forenames>Zhen</forenames></author><author><keyname>Xie</keyname><forenames>Liang</forenames></author><author><keyname>Lin</keyname><forenames>Binbin</forenames></author><author><keyname>Li</keyname><forenames>Houqiang</forenames></author><author><keyname>Lu</keyname><forenames>Le</forenames></author><author><keyname>Tian</keyname><forenames>Xinmei</forenames></author><author><keyname>Cai</keyname><forenames>Deng</forenames></author><author><keyname>Zhang</keyname><forenames>Yonggang</forenames></author><author><keyname>Wang</keyname><forenames>Wenxiao</forenames></author><author><keyname>Shen</keyname><forenames>Xu</forenames></author><author><keyname>Ye</keyname><forenames>Jieping</forenames></author></authors><title>From Yes-Men to Truth-Tellers: Addressing Sycophancy in Large Language   Models with Pinpoint Tuning</title><categories>cs.CL</categories><comments>accepted by ICML 2024, code and data are available at   https://github.com/yellowtownhz/sycophancy-interpretability</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large Language Models (LLMs) tend to prioritize adherence to user prompts over providing veracious responses, leading to the sycophancy issue. When challenged by users, LLMs tend to admit mistakes and provide inaccurate responses even if they initially provided the correct answer. Recent works propose to employ supervised fine-tuning (SFT) to mitigate the sycophancy issue, while it typically leads to the degeneration of LLMs' general capability. To address the challenge, we propose a novel supervised pinpoint tuning (SPT), where the region-of-interest modules are tuned for a given objective. Specifically, SPT first reveals and verifies a small percentage (&lt;5%) of the basic modules, which significantly affect a particular behavior of LLMs. i.e., sycophancy. Subsequently, SPT merely fine-tunes these identified modules while freezing the rest. To verify the effectiveness of the proposed SPT, we conduct comprehensive experiments, demonstrating that SPT significantly mitigates the sycophancy issue of LLMs (even better than SFT). Moreover, SPT introduces limited or even no side effects on the general capability of LLMs. Our results shed light on how to precisely, effectively, and efficiently explain and improve the targeted ability of LLMs. Code and data are available at https://github.com/yellowtownhz/sycophancy-interpretability. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.02565</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.02565</id><created>2024-09-04</created><authors><author><keyname>Poncelet</keyname><forenames>Jakob</forenames></author><author><keyname>Wang</keyname><forenames>Yujun</forenames></author><author><keyname>Van hamme</keyname><forenames>Hugo</forenames></author></authors><title>Efficient Extraction of Noise-Robust Discrete Units from Self-Supervised   Speech Models</title><categories>eess.AS cs.SD</categories><comments>Accepted at SLT2024</comments><journal-ref>2024 IEEE Spoken Language Technology Workshop (SLT), pp. 200-207</journal-ref><doi>10.1109/SLT61566.2024.10832141</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous speech can be converted into a discrete sequence by deriving discrete units from the hidden features of self-supervised learned (SSL) speech models. Although SSL models are becoming larger and trained on more data, they are often sensitive to real-life distortions like additive noise or reverberation, which translates to a shift in discrete units. We propose a parameter-efficient approach to generate noise-robust discrete units from pre-trained SSL models by training a small encoder-decoder model, with or without adapters, to simultaneously denoise and discretise the hidden features of the SSL model. The model learns to generate a clean discrete sequence for a noisy utterance, conditioned on the SSL features. The proposed denoiser outperforms several pre-training methods on the tasks of noisy discretisation and noisy speech recognition, and can be finetuned to the target environment with a few recordings of unlabeled target data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.03594</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.03594</id><created>2024-09-05</created><updated>2025-02-04</updated><authors><author><keyname>Li</keyname><forenames>Bo</forenames></author><author><keyname>Li</keyname><forenames>Minming</forenames></author><author><keyname>Wei</keyname><forenames>Tianze</forenames></author><author><keyname>Wu</keyname><forenames>Zekai</forenames></author><author><keyname>Zhou</keyname><forenames>Yu</forenames></author></authors><title>A Complete Landscape of EFX Allocations on Graphs: Goods, Chores and   Mixed Manna</title><categories>cs.GT</categories><comments>A preliminary version of this paper appeared in the proceeding of   IJCAI 2024. This version includes a new polynomial-time algorithm to   determine the existence of EFX_0 orientations for chores, which resolves an   open problem left in the preliminary version</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study envy-free up to any item (EFX) allocations on simple graphs where vertices and edges represent agents and items respectively. An agent (vertex) is only interested in items (edges) that are incident to her and all other items always have zero marginal value to her. Christodoulou et al. [EC, 2023] first proposed this setting and studied the case of goods where every item has non-negative marginal values to every agent. In this work, we significantly generalize this setting and provide a complete set of results by considering the allocation of arbitrary items that can be goods, chores, or mixed manna under doubly monotone valuations with a mild assumption. For goods, we complement the results by Christodoulou et al. [EC, 2023] by considering another weaker notion of EFX in the literature and showing that an orientation -- a special allocation where each edge must be allocated to one of its endpoint agents -- that satisfies the weaker notion always exists and can be computed in polynomial time, contrary to the stronger notion for which an orientation may not exist and determining its existence is NP-complete. For chores, we show that an envy-free allocation always exists, and an EFX orientation may not exist but its existence can be determined in polynomial time. For mixed manna, we consider the four notions of EFX in the literature. We prove that an allocation that satisfies the strongest notion of EFX may not exist and determining its existence is NP-complete, while one that satisfies any of the other three notions always exists and can be computed in polynomial time. We also prove that an orientation that satisfies any of the four notions may not exist and determining its existence is NP-complete. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.03735</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.03735</id><created>2024-09-05</created><updated>2025-02-05</updated><authors><author><keyname>Shvartzshnaider</keyname><forenames>Yan</forenames></author><author><keyname>Duddu</keyname><forenames>Vasisht</forenames></author></authors><title>Investigating Privacy Bias in Training Data of Language Models</title><categories>cs.LG cs.AI cs.CR cs.CY</categories><comments>16 pages, 4 Figures, 1 Table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As LLMs are integrated into sociotechnical systems, it is crucial to examine the privacy biases they exhibit. A privacy bias refers to the skew in the appropriateness of information flows within a given context that LLMs acquire from large amounts of non-publicly available training data. This skew may either align with existing expectations or signal a symptom of systemic issues reflected in the training datasets.   We formulate a novel research question: how can we examine privacy biases in the training data of LLMs? We present a novel approach to assess the privacy biases using a contextual integrity-based methodology to evaluate the responses from different LLMs. Our approach accounts for the sensitivity of responses across prompt variations, which hinders the evaluation of privacy biases. We investigate how privacy biases are affected by model capacities and optimizations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.03811</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.03811</id><created>2024-09-05</created><updated>2025-02-05</updated><authors><author><keyname>Berto</keyname><forenames>Federico</forenames></author><author><keyname>Hua</keyname><forenames>Chuanbo</forenames></author><author><keyname>Luttmann</keyname><forenames>Laurin</forenames></author><author><keyname>Son</keyname><forenames>Jiwoo</forenames></author><author><keyname>Park</keyname><forenames>Junyoung</forenames></author><author><keyname>Ahn</keyname><forenames>Kyuree</forenames></author><author><keyname>Kwon</keyname><forenames>Changhyun</forenames></author><author><keyname>Xie</keyname><forenames>Lin</forenames></author><author><keyname>Park</keyname><forenames>Jinkyoo</forenames></author></authors><title>Parallel AutoRegressive Models for Multi-Agent Combinatorial   Optimization</title><categories>cs.MA cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Combinatorial optimization problems involving multiple agents are notoriously challenging due to their NP-hard nature and the necessity for effective agent coordination. Despite advancements in learning-based methods, existing approaches often face critical limitations, including suboptimal agent coordination, poor generalizability, and high computational latency. To address these issues, we propose Parallel AutoRegressive Combinatorial Optimization (PARCO), a reinforcement learning framework designed to construct high-quality solutions for multi-agent combinatorial tasks efficiently. To this end, PARCO integrates three key components: (1) transformer-based communication layers to enable effective agent collaboration during parallel solution construction, (2) a multiple pointer mechanism for low-latency, parallel agent decision-making, and (3) priority-based conflict handlers to resolve decision conflicts via learned priorities. We evaluate PARCO in multi-agent vehicle routing and scheduling problems where our approach outperforms state-of-the-art learning methods and demonstrates strong generalization ability and remarkable computational efficiency. Code available at: https://github.com/ai4co/parco. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.03845</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.03845</id><created>2024-09-05</created><updated>2025-02-05</updated><authors><author><keyname>Cheng</keyname><forenames>Sheng</forenames></author><author><keyname>Kong</keyname><forenames>Deqian</forenames></author><author><keyname>Xie</keyname><forenames>Jianwen</forenames></author><author><keyname>Lee</keyname><forenames>Kookjin</forenames></author><author><keyname>Wu</keyname><forenames>Ying Nian</forenames></author><author><keyname>Yang</keyname><forenames>Yezhou</forenames></author></authors><title>Latent Space Energy-based Neural ODEs</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper introduces novel deep dynamical models designed to represent continuous-time sequences. Our approach employs a neural emission model to generate each data point in the time series through a non-linear transformation of a latent state vector. The evolution of these latent states is implicitly defined by a neural ordinary differential equation (ODE), with the initial state drawn from an informative prior distribution parameterized by an Energy-based model (EBM). This framework is extended to disentangle dynamic states from underlying static factors of variation, represented as time-invariant variables in the latent space. We train the model using maximum likelihood estimation with Markov chain Monte Carlo (MCMC) in an end-to-end manner. Experimental results on oscillating systems, videos and real-world state sequences (MuJoCo) demonstrate that our model with the learnable energy-based prior outperforms existing counterparts, and can generalize to new dynamic parameterization, enabling long-horizon predictions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.05619</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.05619</id><created>2024-09-09</created><authors><author><keyname>Dasgupta</keyname><forenames>Ayanava</forenames></author><author><keyname>Warsi</keyname><forenames>Naqueeb Ahmad</forenames></author><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author></authors><title>Universal tester for multiple independence testing and classical-quantum   arbitrarily varying multiple access channel</title><categories>cs.IT math.IT</categories><comments>60 pages</comments><doi>10.1109/TIT.2025.3538870</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study two kinds of different problems. One is the multiple independence testing, which can be considered as a kind of generalization of quantum Stein's lemma. We test whether the quantum system is correlated to the classical system or is independent of it. Here, the null hypothesis is composed of states having the quantum system is correlated to the classical system in an arbitrarily varying form. The second problem is the problem of reliable communication over classical-quantum arbitrarily varying multiple access channels (CQ-AVMAC) and establishing its capacity region by giving multiple achievability techniques. We prove that each of these techniques is optimal by proving a converse. Further, for both these techniques, the decoder designed is a \emph{universal} decoder and can achieve any rate pair in the capacity region without time sharing and also these decoders do not depend on the channel and therefore they are universal. Our result covers the case when the channel parameter is continuous, which has not been studied even in the classical case. Further, both these techniques can be easily generalized to the case when there are $T (T&gt;2)$ senders. The design of each of these decoders is based on the study of multiple independence testing. This approach allows us to study the problem of reliable communication over CQ-AVMAC from the point of view of hypothesis testing. Further, we also give a necessary and sufficient condition for the deterministic code capacity region of CQ-AVMAC to be non-empty. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.06807</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.06807</id><created>2024-09-10</created><updated>2025-02-04</updated><authors><author><keyname>Perrault</keyname><forenames>Nicolas</forenames></author><author><keyname>Ho</keyname><forenames>Qi Heng</forenames></author><author><keyname>Lahijanian</keyname><forenames>Morteza</forenames></author></authors><title>Kino-PAX: Highly Parallel Kinodynamic Sampling-based Planner</title><categories>cs.RO cs.DC cs.PF</categories><comments>To appear in the Robotics and Automation Letters (RAL), March 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling-based motion planners (SBMPs) are effective for planning with complex kinodynamic constraints in high-dimensional spaces, but they still struggle to achieve real-time performance, which is mainly due to their serial computation design. We present Kinodynamic Parallel Accelerated eXpansion (Kino-PAX), a novel highly parallel kinodynamic SBMP designed for parallel devices such as GPUs. Kino-PAX grows a tree of trajectory segments directly in parallel. Our key insight is how to decompose the iterative tree growth process into three massively parallel subroutines. Kino-PAX is designed to align with the parallel device execution hierarchies, through ensuring that threads are largely independent, share equal workloads, and take advantage of low-latency resources while minimizing high-latency data transfers and process synchronization. This design results in a very efficient GPU implementation. We prove that Kino-PAX is probabilistically complete and analyze its scalability with compute hardware improvements. Empirical evaluations demonstrate solutions in the order of 10 ms on a desktop GPU and in the order of 100 ms on an embedded GPU, representing up to 1000 times improvement compared to coarse-grained CPU parallelization of state-of-the-art sequential algorithms over a range of complex environments and systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.07510</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.07510</id><created>2024-09-11</created><updated>2025-02-04</updated><authors><author><keyname>Khan</keyname><forenames>Falaah Arif</forenames></author><author><keyname>Herasymuk</keyname><forenames>Denys</forenames></author><author><keyname>Protsiv</keyname><forenames>Nazar</forenames></author><author><keyname>Stoyanovich</keyname><forenames>Julia</forenames></author></authors><title>Still More Shades of Null: An Evaluation Suite for Responsible Missing   Value Imputation</title><categories>cs.AI cs.CY cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Data missingness is a practical challenge of sustained interest to the scientific community. In this paper, we present Shades-of-NULL, an evaluation suite for responsible missing value imputation. Our work is novel in two ways (i) we model realistic and socially-salient missingness scenarios that go beyond Rubin's classic Missing Completely at Random (MCAR), Missing At Random (MAR) and Missing Not At Random (MNAR) settings, to include multi-mechanism missingness (when different missingness patterns co-exist in the data) and missingness shift (when the missingness mechanism changes between training and test) (ii) we evaluate imputers holistically, based on imputation quality and imputation fairness, as well as on the predictive performance, fairness and stability of the models that are trained and tested on the data post-imputation. We use Shades-of-NULL to conduct a large-scale empirical study involving 29,736 experimental pipelines, and find that while there is no single best-performing imputation approach for all missingness types, interesting trade-offs arise between predictive performance, fairness and stability, based on the combination of missingness scenario, imputer choice, and the architecture of the predictive model. We make Shades-of-NULL publicly available, to enable researchers to rigorously evaluate missing value imputation methods on a wide range of metrics in plausible and socially meaningful scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.08362</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.08362</id><created>2024-09-12</created><updated>2025-02-05</updated><authors><author><keyname>Grekas</keyname><forenames>Georgios</forenames></author><author><keyname>Makridakis</keyname><forenames>Charalambos G.</forenames></author></authors><title>Deep Ritz-Finite Element methods: Neural Network Methods trained with   Finite Elements</title><categories>math.NA cs.NA physics.comp-ph</categories><msc-class>65M15, 65M12</msc-class><doi>10.1016/j.cma.2025.117798</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While much attention of neural network methods is devoted to high-dimensional PDE problems, in this work we consider methods designed to work for elliptic problems on domains $\Omega \subset \mathbb{R} ^d, $ $d=1,2,3$ in association with more standard finite elements. We suggest to connect finite elements and neural network approximations through training, i.e., using finite element spaces to compute the integrals appearing in the loss functionals. This approach, retains the simplicity of classical neural network methods for PDEs, uses well established finite element tools (and software) to compute the integrals involved and it gains in efficiency and accuracy. We demonstrate that the proposed methods are stable and furthermore, we establish that the resulting approximations converge to the solutions of the PDE. Numerical results indicating the efficiency and robustness of the proposed algorithms are presented. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.09662</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.09662</id><created>2024-09-15</created><updated>2025-02-05</updated><authors><author><keyname>Song</keyname><forenames>Inhwa</forenames></author><author><keyname>Park</keyname><forenames>SoHyun</forenames></author><author><keyname>Pendse</keyname><forenames>Sachin R.</forenames></author><author><keyname>Schleider</keyname><forenames>Jessica Lee</forenames></author><author><keyname>De Choudhury</keyname><forenames>Munmun</forenames></author><author><keyname>Kim</keyname><forenames>Young-Ho</forenames></author></authors><title>ExploreSelf: Fostering User-driven Exploration and Reflection on   Personal Challenges with Adaptive Guidance by Large Language Models</title><categories>cs.HC cs.AI cs.CL</categories><comments>17 pages excluding reference and appendix. Accepted at ACM CHI 2025.   https://naver-ai.github.io/exploreself</comments><acm-class>H.5.2; I.2.7</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Expressing stressful experiences in words is proven to improve mental and physical health, but individuals often disengage with writing interventions as they struggle to organize their thoughts and emotions. Reflective prompts have been used to provide direction, and large language models (LLMs) have demonstrated the potential to provide tailored guidance. However, current systems often limit users' flexibility to direct their reflections. We thus present ExploreSelf, an LLM-driven application designed to empower users to control their reflective journey, providing adaptive support through dynamically generated questions. Through an exploratory study with 19 participants, we examine how participants explore and reflect on personal challenges using ExploreSelf. Our findings demonstrate that participants valued the flexible navigation of adaptive guidance to control their reflective journey, leading to deeper engagement and insight. Building on our findings, we discuss the implications of designing LLM-driven tools that facilitate user-driven and effective reflection of personal challenges. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.11828</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.11828</id><created>2024-09-18</created><updated>2025-02-05</updated><authors><author><keyname>Shahna</keyname><forenames>Mehdi Heydari</forenames></author><author><keyname>Mattila</keyname><forenames>Jouni</forenames></author></authors><title>Model-Free Generic Robust Control for Servo-Driven Actuation Mechanisms   with Layered Insight into Energy Conversions</title><categories>eess.SY cs.SY</categories><comments>This work has been accepted for presentation on the 2025 American   Control Conference (ACC)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  To advance theoretical solutions and address limitations in modeling complex servo-driven actuation systems experiencing high non-linearity and load disturbances, this paper aims to design a practical model-free generic robust control (GRC) framework for these mechanisms. This framework is intended to be applicable across all actuator systems encompassing electrical, hydraulic, or pneumatic servomechanisms, while also functioning within complex interactions among dynamic components and adhering to control input constraints. In this respect, the state-space model of actuator systems is decomposed into smaller subsystems that incorporate the first principle equation of actuator motion dynamics and interactive energy conversion equations. This decomposition operates under the assumption that the comprehensive model of the servo-driven actuator system and energy conversion, uncertainties, load disturbances, and their bounds are unknown. Then, the GRC employs subsystem-based adaptive control strategies for each state-variant subsystem separately. Despite control input constraints and the unknown interactive system model, the GRC-applied actuator mechanism ensures uniform exponential stability and robustness in tracking desired motions. It features straightforward implementation, experimentally evaluated by applying it to two industrial applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.13864</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.13864</id><created>2024-09-20</created><authors><author><keyname>Guo</keyname><forenames>Zhen</forenames></author><author><keyname>Kumar</keyname><forenames>Abhinav</forenames></author><author><keyname>Tourani</keyname><forenames>Reza</forenames></author></authors><title>Persistent Backdoor Attacks in Continual Learning</title><categories>cs.LG cs.CR</categories><comments>18 pages, 15 figures, 6 tables</comments><journal-ref>Proceedings of the 2025 USENIX Security Symposium</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Backdoor attacks pose a significant threat to neural networks, enabling adversaries to manipulate model outputs on specific inputs, often with devastating consequences, especially in critical applications. While backdoor attacks have been studied in various contexts, little attention has been given to their practicality and persistence in continual learning, particularly in understanding how the continual updates to model parameters, as new data distributions are learned and integrated, impact the effectiveness of these attacks over time. To address this gap, we introduce two persistent backdoor attacks-Blind Task Backdoor and Latent Task Backdoor-each leveraging minimal adversarial influence. Our blind task backdoor subtly alters the loss computation without direct control over the training process, while the latent task backdoor influences only a single task's training, with all other tasks trained benignly. We evaluate these attacks under various configurations, demonstrating their efficacy with static, dynamic, physical, and semantic triggers. Our results show that both attacks consistently achieve high success rates across different continual learning algorithms, while effectively evading state-of-the-art defenses, such as SentiNet and I-BAU. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.14557</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.14557</id><created>2024-09-22</created><updated>2025-02-05</updated><authors><author><keyname>Wan</keyname><forenames>Jia</forenames></author><author><keyname>Sinclair</keyname><forenames>Sean R.</forenames></author><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author></authors><title>Exploiting Exogenous Structure for Sample-Efficient Reinforcement   Learning</title><categories>stat.ML cs.LG math.OC</categories><comments>43 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study Exo-MDPs, a structured class of Markov Decision Processes (MDPs) where the state space is partitioned into exogenous and endogenous components. Exogenous states evolve stochastically, independent of the agent's actions, while endogenous states evolve deterministically based on both state components and actions. Exo-MDPs are useful for applications including inventory control, portfolio management, and ride-sharing. Our first result is structural, establishing a representational equivalence between the classes of discrete MDPs, Exo-MDPs, and discrete linear mixture MDPs. Specifically, any discrete MDP can be represented as an Exo-MDP, and the transition and reward dynamics can be written as linear functions of the exogenous state distribution, showing that Exo-MDPs are instances of linear mixture MDPs. For unobserved exogenous states, we prove a regret upper bound of $O(H^{3/2}d\sqrt{K})$ over $K$ trajectories of horizon $H$, with $d$ as the size of the exogenous state space, and establish nearly-matching lower bounds. Our findings demonstrate how Exo-MDPs decouple sample complexity from action and endogenous state sizes, and we validate our theoretical insights with experiments on inventory control. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.16040</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.16040</id><created>2024-09-24</created><updated>2025-02-05</updated><authors><author><keyname>Shi</keyname><forenames>Xiaoming</forenames></author><author><keyname>Wang</keyname><forenames>Shiyu</forenames></author><author><keyname>Nie</keyname><forenames>Yuqi</forenames></author><author><keyname>Li</keyname><forenames>Dianqi</forenames></author><author><keyname>Ye</keyname><forenames>Zhou</forenames></author><author><keyname>Wen</keyname><forenames>Qingsong</forenames></author><author><keyname>Jin</keyname><forenames>Ming</forenames></author></authors><title>Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of   Experts</title><categories>cs.LG cs.AI</categories><comments>Accepted by the 13th International Conference on Learning   Representations (ICLR 2025)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of larger capable forecasting models in real-world applications. In response, we introduce Time-MoE, a scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows Time-MoE to scale effectively without a corresponding increase in inference costs. Time-MoE comprises a family of decoder-only transformer models that operate in an auto-regressive manner and support flexible forecasting horizons with varying input context lengths. We pre-trained these models on our newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, we scaled a time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Our results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, our models consistently outperform them by large margin. These advancements position Time-MoE as a state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.16295</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.16295</id><created>2024-09-09</created><updated>2025-02-04</updated><authors><author><keyname>Liu</keyname><forenames>Andy T.</forenames></author><author><keyname>Lin</keyname><forenames>Yi-Cheng</forenames></author><author><keyname>Wu</keyname><forenames>Haibin</forenames></author><author><keyname>Winkler</keyname><forenames>Stefan</forenames></author><author><keyname>Lee</keyname><forenames>Hung-yi</forenames></author></authors><title>Efficient Training of Self-Supervised Speech Foundation Models on a   Compute Budget</title><categories>eess.AS cs.CL cs.LG cs.SD</categories><comments>Accepted to IEEE SLT 2024</comments><journal-ref>2024 IEEE Spoken Language Technology Workshop (SLT)</journal-ref><doi>10.1109/SLT61566.2024.10832361</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Despite their impressive success, training foundation models remains computationally costly. This paper investigates how to efficiently train speech foundation models with self-supervised learning (SSL) under a limited compute budget. We examine critical factors in SSL that impact the budget, including model architecture, model size, and data size. Our goal is to make analytical steps toward understanding the training dynamics of speech foundation models. We benchmark SSL objectives in an entirely comparable setting and find that other factors contribute more significantly to the success of SSL. Our results show that slimmer model architectures outperform common small architectures under the same compute and parameter budget. We demonstrate that the size of the pre-training data remains crucial, even with data augmentation during SSL training, as performance suffers when iterating over limited data. Finally, we identify a trade-off between model size and data size, highlighting an optimal model size for a given compute budget. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.16850</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.16850</id><created>2024-09-25</created><updated>2025-02-05</updated><authors><author><keyname>Lin</keyname><forenames>Chun-Jung</forenames></author><author><keyname>Garg</keyname><forenames>Sourav</forenames></author><author><keyname>Chin</keyname><forenames>Tat-Jun</forenames></author><author><keyname>Dayoub</keyname><forenames>Feras</forenames></author></authors><title>Robust Scene Change Detection Using Visual Foundation Models and   Cross-Attention Mechanisms</title><categories>cs.CV</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel method for scene change detection that leverages the robust feature extraction capabilities of a visual foundational model, DINOv2, and integrates full-image cross-attention to address key challenges such as varying lighting, seasonal variations, and viewpoint differences. In order to effectively learn correspondences and mis-correspondences between an image pair for the change detection task, we propose to a) ``freeze'' the backbone in order to retain the generality of dense foundation features, and b) employ ``full-image'' cross-attention to better tackle the viewpoint variations between the image pair. We evaluate our approach on two benchmark datasets, VL-CMU-CD and PSCD, along with their viewpoint-varied versions. Our experiments demonstrate significant improvements in F1-score, particularly in scenarios involving geometric changes between image pairs. The results indicate our method's superior generalization capabilities over existing state-of-the-art approaches, showing robustness against photometric and geometric variations as well as better overall generalization when fine-tuned to adapt to new environments. Detailed ablation studies further validate the contributions of each component in our architecture. Our source code is available at: https://github.com/ChadLin9596/Robust-Scene-Change-Detection. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.16948</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.16948</id><created>2024-09-25</created><updated>2024-10-09</updated><authors><author><keyname>Tebaldi</keyname><forenames>Davide</forenames></author><author><keyname>Zanasi</keyname><forenames>Roberto</forenames></author></authors><title>The Power-Oriented Graphs Modeling Technique: From the Fundamental   Principles to the Systematic, Step-by-Step Modeling of Complex Physical   Systems</title><categories>eess.SY cs.SY</categories><doi>10.1109/ACCESS.2025.3537862</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Modeling physical systems is an essential skill for a control engineer, since it enables to achieve a deep understanding of their dynamic behavior and, consequently, the development of effective control strategies. The first part of this article provides a tutorial description of the fundamental principles and properties of the Power-Oriented Graphs (POG) modeling technique. Various case studies in different energetic domains are then presented to consolidate the fundamental principles, each highlighting different features of the POG modeling technique. The latter is then compared with the other two main graphical modeling techniques available in the literature, namely Bond Graph (BG) and Energetic Macroscopic Representation (EMR). The second part of this article assumes once again a tutorial nature, in order to introduce the new Fast Modeling POG (FMPOG) procedure. The FMPOG, which operates in the POG framework, is a methodical step-by-step procedure that enables the readers to quickly derive the power-oriented graphical model of physical systems starting from their schematics. From the power-oriented graphical model, the state-space model can then be directly determined. To ensure the FMPOG procedure is easily usable by the entire community, we apply it to three examples in different energetic domains in this article, guiding the reader step-by-step through the derivation of the physical systems models. A freely available Matlab/Simulink program is provided in a repository, allowing the users to automatically apply the FMPOG procedure to various classes of physical systems. This program allows to convert the physical systems schematics into the corresponding POG block schemes and, ultimately, into the state-space mathematical models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.19154</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.19154</id><created>2024-09-27</created><authors><author><keyname>Esmaeili</keyname><forenames>Amir</forenames></author><author><keyname>Mtibaa</keyname><forenames>Abderrahmen</forenames></author></authors><title>SAMBA: Scalable Approximate Forwarding For NDN Implicit FIB Aggregation</title><categories>cs.NI</categories><comments>9 pages</comments><doi>10.1109/CloudNet62863.2024.10815897</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Internet landscape has witnessed a significant shift toward Information Centric Networking (ICN) due to the exponential growth of data-driven applications. Similar to routing tables in TCP/IP architectures, ICN uses Forward Information Base (FIB) tables. However, FIB tables can grow exponentially due to their URL-like naming scheme, introducing major delays in the prefix lookup process. Existing explicit FIB aggregation solutions are very complex to run, and ICN on-demand routing schemes, which use a discovery mechanism to help reduce the number of FIB records and thus have shorter lookup times, rely on flooding-based mechanisms and building routes for all requests, introducing additional scalability challenges. In this paper, we propose SAMBA, an Approximate Forwarding-based Self Learning, that uses the nearest FIB trie record to the given prefix for reducing the number of discoveries thus keeping the FIB table small. By choosing the nearest prefix to a given name prefix, SAMBA uses Implicit Prefix Aggregation (IPA) which implicitly aggregates the FIB records and reduces the number of Self Learning discoveries required. Coupled with the approximate forwarding, SAMBA can achieve efficient and scalable forwarding </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.19359</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.19359</id><created>2024-09-28</created><authors><author><keyname>Li</keyname><forenames>Weikang</forenames></author><author><keyname>Deng</keyname><forenames>Dong-Ling</forenames></author></authors><title>Quantum delegated and federated learning via quantum homomorphic   encryption</title><categories>quant-ph cs.CR cs.LG</categories><comments>5 pages, 1 figure, 1 table</comments><journal-ref>Research Directions: Quantum Technologies 2025</journal-ref><doi>10.1017/qut.2025.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum learning models hold the potential to bring computational advantages over the classical realm. As powerful quantum servers become available on the cloud, ensuring the protection of clients' private data becomes crucial. By incorporating quantum homomorphic encryption schemes, we present a general framework that enables quantum delegated and federated learning with a computation-theoretical data privacy guarantee. We show that learning and inference under this framework feature substantially lower communication complexity compared with schemes based on blind quantum computing. In addition, in the proposed quantum federated learning scenario, there is less computational burden on local quantum devices from the client side, since the server can operate on encrypted quantum data without extracting any information. We further prove that certain quantum speedups in supervised learning carry over to private delegated learning scenarios employing quantum kernel methods. Our results provide a valuable guide toward privacy-guaranteed quantum learning on the cloud, which may benefit future studies and security-related applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.19457</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.19457</id><created>2024-09-28</created><updated>2025-02-04</updated><authors><author><keyname>Yu</keyname><forenames>Houjian</forenames></author><author><keyname>Li</keyname><forenames>Mingen</forenames></author><author><keyname>Rezazadeh</keyname><forenames>Alireza</forenames></author><author><keyname>Yang</keyname><forenames>Yang</forenames></author><author><keyname>Choi</keyname><forenames>Changhyun</forenames></author></authors><title>A Parameter-Efficient Tuning Framework for Language-guided Object   Grounding and Robot Grasping</title><categories>cs.RO</categories><comments>Accepted for ICRA 2025. Project page:   https://sites.google.com/umn.edu/etog-etrg/home</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The language-guided robot grasping task requires a robot agent to integrate multimodal information from both visual and linguistic inputs to predict actions for target-driven grasping. While recent approaches utilizing Multimodal Large Language Models (MLLMs) have shown promising results, their extensive computation and data demands limit the feasibility of local deployment and customization. To address this, we propose a novel CLIP-based multimodal parameter-efficient tuning (PET) framework designed for three language-guided object grounding and grasping tasks: (1) Referring Expression Segmentation (RES), (2) Referring Grasp Synthesis (RGS), and (3) Referring Grasp Affordance (RGA). Our approach introduces two key innovations: a bi-directional vision-language adapter that aligns multimodal inputs for pixel-level language understanding and a depth fusion branch that incorporates geometric cues to facilitate robot grasping predictions. Experiment results demonstrate superior performance in the RES object grounding task compared with existing CLIP-based full-model tuning or PET approaches. In the RGS and RGA tasks, our model not only effectively interprets object attributes based on simple language descriptions but also shows strong potential for comprehending complex spatial reasoning scenarios, such as multiple identical objects present in the workspace. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.01153</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.01153</id><created>2024-10-01</created><updated>2025-02-04</updated><authors><author><keyname>Zhou</keyname><forenames>Anthony</forenames></author><author><keyname>Li</keyname><forenames>Zijie</forenames></author><author><keyname>Schneier</keyname><forenames>Michael</forenames></author><author><keyname>Buchanan</keyname><forenames>John R</forenames><suffix>Jr</suffix></author><author><keyname>Farimani</keyname><forenames>Amir Barati</forenames></author></authors><title>Text2PDE: Latent Diffusion Models for Accessible Physics Simulation</title><categories>cs.LG</categories><comments>Published at ICLR 2025. Github:   http://github.com/anthonyzhou-1/ldm_pdes</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent advances in deep learning have inspired numerous works on data-driven solutions to partial differential equation (PDE) problems. These neural PDE solvers can often be much faster than their numerical counterparts; however, each presents its unique limitations and generally balances training cost, numerical accuracy, and ease of applicability to different problem setups. To address these limitations, we introduce several methods to apply latent diffusion models to physics simulation. Firstly, we introduce a mesh autoencoder to compress arbitrarily discretized PDE data, allowing for efficient diffusion training across various physics. Furthermore, we investigate full spatio-temporal solution generation to mitigate autoregressive error accumulation. Lastly, we investigate conditioning on initial physical quantities, as well as conditioning solely on a text prompt to introduce text2PDE generation. We show that language can be a compact, interpretable, and accurate modality for generating physics simulations, paving the way for more usable and accessible PDE solvers. Through experiments on both uniform and structured grids, we show that the proposed approach is competitive with current neural PDE solvers in both accuracy and efficiency, with promising scaling behavior up to $\sim$3 billion parameters. By introducing a scalable, accurate, and usable physics simulator, we hope to bring neural PDE solvers closer to practical use. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.01506</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.01506</id><created>2024-10-02</created><updated>2025-02-05</updated><authors><author><keyname>Ding</keyname><forenames>Dexuan</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Zhu</keyname><forenames>Liyun</forenames></author><author><keyname>Gedeon</keyname><forenames>Tom</forenames></author><author><keyname>Koniusz</keyname><forenames>Piotr</forenames></author></authors><title>Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion</title><categories>cs.CV cs.AI cs.LG</categories><comments>Accepted at the Thirteenth International Conference on Learning   Representations (ICLR 2025)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In computer vision tasks, features often come from diverse representations, domains (e.g., indoor and outdoor), and modalities (e.g., text, images, and videos). Effectively fusing these features is essential for robust performance, especially with the availability of powerful pre-trained models like vision-language models. However, common fusion methods, such as concatenation, element-wise operations, and non-linear techniques, often fail to capture structural relationships, deep feature interactions, and suffer from inefficiency or misalignment of features across domains or modalities. In this paper, we shift from high-dimensional feature space to a lower-dimensional, interpretable graph space by constructing relationship graphs that encode feature relationships at different levels, e.g., clip, frame, patch, token, etc. To capture deeper interactions, we use graph power expansions and introduce a learnable graph fusion operator to combine these graph powers for more effective fusion. Our approach is relationship-centric, operates in a homogeneous space, and is mathematically principled, resembling element-wise relationship score aggregation via multilinear polynomials. We demonstrate the effectiveness of our graph-based fusion method on video anomaly detection, showing strong performance across multi-representational, multi-modal, and multi-domain feature fusion tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.01620</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.01620</id><created>2024-10-02</created><updated>2025-02-05</updated><authors><author><keyname>Qin</keyname><forenames>Zhenyue</forenames></author><author><keyname>Yin</keyname><forenames>Yu</forenames></author><author><keyname>Campbell</keyname><forenames>Dylan</forenames></author><author><keyname>Wu</keyname><forenames>Xuansheng</forenames></author><author><keyname>Zou</keyname><forenames>Ke</forenames></author><author><keyname>Tham</keyname><forenames>Yih-Chung</forenames></author><author><keyname>Liu</keyname><forenames>Ninghao</forenames></author><author><keyname>Zhang</keyname><forenames>Xiuzhen</forenames></author><author><keyname>Chen</keyname><forenames>Qingyu</forenames></author></authors><title>LMOD: A Large Multimodal Ophthalmology Dataset and Benchmark for Large   Vision-Language Models</title><categories>cs.CV</categories><comments>2025 NAACL: Annual Conference of the Nations of the Americas Chapter   of the Association for Computational Linguistics Project Page:   https://kfzyqin.github.io/lmod/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The prevalence of vision-threatening eye diseases is a significant global burden, with many cases remaining undiagnosed or diagnosed too late for effective treatment. Large vision-language models (LVLMs) have the potential to assist in understanding anatomical information, diagnosing eye diseases, and drafting interpretations and follow-up plans, thereby reducing the burden on clinicians and improving access to eye care. However, limited benchmarks are available to assess LVLMs' performance in ophthalmology-specific applications. In this study, we introduce LMOD, a large-scale multimodal ophthalmology benchmark consisting of 21,993 instances across (1) five ophthalmic imaging modalities: optical coherence tomography, color fundus photographs, scanning laser ophthalmoscopy, lens photographs, and surgical scenes; (2) free-text, demographic, and disease biomarker information; and (3) primary ophthalmology-specific applications such as anatomical information understanding, disease diagnosis, and subgroup analysis. In addition, we benchmarked 13 state-of-the-art LVLM representatives from closed-source, open-source, and medical domains. The results demonstrate a significant performance drop for LVLMs in ophthalmology compared to other domains. Systematic error analysis further identified six major failure modes: misclassification, failure to abstain, inconsistent reasoning, hallucination, assertions without justification, and lack of domain-specific knowledge. In contrast, supervised neural networks specifically trained on these tasks as baselines demonstrated high accuracy. These findings underscore the pressing need for benchmarks in the development and validation of ophthalmology-specific LVLMs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.02298</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.02298</id><created>2024-10-03</created><updated>2025-02-05</updated><authors><author><keyname>Shen</keyname><forenames>Guobin</forenames></author><author><keyname>Zhao</keyname><forenames>Dongcheng</forenames></author><author><keyname>Dong</keyname><forenames>Yiting</forenames></author><author><keyname>He</keyname><forenames>Xiang</forenames></author><author><keyname>Zeng</keyname><forenames>Yi</forenames></author></authors><title>Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse   Representation Adjustment in Large Language Models</title><categories>cs.CR cs.CL</categories><comments>Accepted by ICLR2025. url: https://openreview.net/forum?id=s20W12XTF8</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As large language models (LLMs) become integral to various applications, ensuring both their safety and utility is paramount. Jailbreak attacks, which manipulate LLMs into generating harmful content, pose significant challenges to this balance. Existing defenses, such as prompt engineering and safety fine-tuning, often introduce computational overhead, increase inference latency, and lack runtime flexibility. Moreover, overly restrictive safety measures can degrade model utility by causing refusals of benign queries. In this paper, we introduce Jailbreak Antidote, a method that enables real-time adjustment of LLM safety preferences by manipulating a sparse subset of the model's internal states during inference. By shifting the model's hidden representations along a safety direction with varying strengths, we achieve flexible control over the safety-utility balance without additional token overhead or inference delays. Our analysis reveals that safety-related information in LLMs is sparsely distributed; adjusting approximately 5% of the internal state is as effective as modifying the entire state. Extensive experiments on nine LLMs (ranging from 2 billion to 72 billion parameters), evaluated against ten jailbreak attack methods and compared with six defense strategies, validate the effectiveness and efficiency of our approach. By directly manipulating internal states during reasoning, Jailbreak Antidote offers a lightweight, scalable solution that enhances LLM safety while preserving utility, opening new possibilities for real-time safety mechanisms in widely-deployed AI systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.02907</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.02907</id><created>2024-10-03</created><updated>2025-02-05</updated><authors><author><keyname>Murty</keyname><forenames>Shikhar</forenames></author><author><keyname>Zhu</keyname><forenames>Hao</forenames></author><author><keyname>Bahdanau</keyname><forenames>Dzmitry</forenames></author><author><keyname>Manning</keyname><forenames>Christopher D.</forenames></author></authors><title>NNetNav: Unsupervised Learning of Browser Agents Through Environment   Interaction in the Wild</title><categories>cs.CL</categories><comments>Code, Data and Models available at https://www.nnetnav.dev</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce NNetNav, a method for unsupervised interaction with websites that generates synthetic demonstrations for training browser agents. Given any website, NNetNav produces these demonstrations by retroactively labeling action sequences from an exploration policy. Most work on training browser agents has relied on expensive human supervision, and the limited prior work on such interaction-based techniques has failed to provide effective search through the exponentially large space of exploration. In contrast, NNetNav exploits the hierarchical structure of language instructions to make this search more tractable: Complex instructions are typically decomposable into simpler sub-tasks, allowing NNetNav to automatically prune interaction episodes when an intermediate trajectory cannot be annotated with a meaningful sub-task. \texttt{LLama-3.1-8b} finetuned on 10k NNetNav self-generated demonstrations obtains over 16\% success rate on WebArena, and 35\% on WebVoyager, an improvement of 15pts and 31pts respectively over zero-shot \texttt{LLama-3.1-8b}, outperforming zero-shot GPT-4 and reaching the state-of-the-art among unsupervised methods, for both benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.03021</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.03021</id><created>2024-10-03</created><updated>2025-02-04</updated><authors><author><keyname>Zamzam</keyname><forenames>Omar</forenames></author></authors><title>PixelShuffler: A Simple Image Translation Through Pixel Rearrangement</title><categories>cs.CV eess.IV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Image-to-image translation is a topic in computer vision that has a vast range of use cases ranging from medical image translation, such as converting MRI scans to CT scans or to other MRI contrasts, to image colorization, super-resolution, domain adaptation, and generating photorealistic images from sketches or semantic maps. Image style transfer is also a widely researched application of image-to-image translation, where the goal is to synthesize an image that combines the content of one image with the style of another. Existing state-of-the-art methods often rely on complex neural networks, including diffusion models and language models, to achieve high-quality style transfer, but these methods can be computationally expensive and intricate to implement. In this paper, we propose a novel pixel shuffle method that addresses the image-to-image translation problem generally with a specific demonstrative application in style transfer. The proposed method approaches style transfer by shuffling the pixels of the style image such that the mutual information between the shuffled image and the content image is maximized. This approach inherently preserves the colors of the style image while ensuring that the structural details of the content image are retained in the stylized output. We demonstrate that this simple and straightforward method produces results that are comparable to state-of-the-art techniques, as measured by the Learned Perceptual Image Patch Similarity (LPIPS) loss for content preservation and the Fr\'echet Inception Distance (FID) score for style similarity. Our experiments validate that the proposed pixel shuffle method achieves competitive performance with significantly reduced complexity, offering a promising alternative for efficient image style transfer, as well as a promise in usability of the method in general image-to-image translation tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.03453</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.03453</id><created>2024-10-04</created><updated>2025-02-05</updated><authors><author><keyname>Behera</keyname><forenames>Amit</forenames></author><author><keyname>Malavolta</keyname><forenames>Giulio</forenames></author><author><keyname>Morimae</keyname><forenames>Tomoyuki</forenames></author><author><keyname>Mour</keyname><forenames>Tamer</forenames></author><author><keyname>Yamakawa</keyname><forenames>Takashi</forenames></author></authors><title>A New World in the Depths of Microcrypt: Separating OWSGs and Quantum   Money from QEFID</title><categories>quant-ph cs.CR</categories><comments>Minor revisions in the related and concurrent works section were made   in version 3</comments><report-no>YITP-24-127</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While in classical cryptography, one-way functions (OWFs) are widely regarded as the "minimal assumption," the situation in quantum cryptography is less clear. Recent works have put forward two concurrent candidates for the minimal assumption in quantum cryptography: One-way state generators (OWSGs), postulating the existence of a hard search problem with an efficient verification algorithm, and EFI pairs, postulating the existence of a hard distinguishing problem. Two recent papers [Khurana and Tomer STOC'24; Batra and Jain FOCS'24] showed that OWSGs imply EFI pairs, but the reverse direction remained open. In this work, we give strong evidence that the opposite direction does not hold: We show that there is a quantum unitary oracle relative to which EFI pairs exist, but OWSGs do not. In fact, we show a slightly stronger statement that holds also for EFI pairs that output classical bits (QEFID). As a consequence, we separate, via our oracle, QEFID, and one-way puzzles from OWSGs and several other Microcrypt primitives, including efficiently verifiable one-way puzzles and unclonable state generators. In particular, this solves a problem left open in [Chung, Goldin, and Gray Crypto'24]. Using similar techniques, we also establish a fully black-box separation (which is slightly weaker than an oracle separation) between private-key quantum money schemes and QEFID pairs. One conceptual implication of our work is that the existence of an efficient verification algorithm may lead to qualitatively stronger primitives in quantum cryptography. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.04868</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.04868</id><created>2024-10-07</created><updated>2024-11-28</updated><authors><author><keyname>Baumann</keyname><forenames>Nicolas</forenames></author><author><keyname>Ghignone</keyname><forenames>Edoardo</forenames></author><author><keyname>Hu</keyname><forenames>Cheng</forenames></author><author><keyname>Hildisch</keyname><forenames>Benedict</forenames></author><author><keyname>Hämmerle</keyname><forenames>Tino</forenames></author><author><keyname>Bettoni</keyname><forenames>Alessandro</forenames></author><author><keyname>Carron</keyname><forenames>Andrea</forenames></author><author><keyname>Xie</keyname><forenames>Lei</forenames></author><author><keyname>Magno</keyname><forenames>Michele</forenames></author></authors><title>Predictive Spliner: Data-Driven Overtaking in Autonomous Racing Using   Opponent Trajectory Prediction</title><categories>cs.RO cs.SY eess.SY</categories><comments>Accepted to RA-L</comments><report-no>LRA.2024.3519878</report-no><journal-ref>IEEE Robotics and Automation Letters ( Volume: 10, Issue: 2,   February 2025)</journal-ref><doi>10.1109/LRA.2024.3519878</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Head-to-head racing against opponents is a challenging and emerging topic in the domain of autonomous racing. We propose Predictive Spliner, a data-driven overtaking planner that learns the behavior of opponents through Gaussian Process (GP) regression, which is then leveraged to compute viable overtaking maneuvers in future sections of the racing track. Experimentally validated on a 1:10 scale autonomous racing platform using Light Detection and Ranging (LiDAR) information to perceive the opponent, Predictive Spliner outperforms State-of-the-Art (SotA) algorithms by overtaking opponents at up to 83.1% of its own speed, being on average 8.4% faster than the previous best-performing method. Additionally, it achieves an average success rate of 84.5%, which is 47.6% higher than the previous best-performing method. The method maintains computational efficiency with a Central Processing Unit (CPU) load of 22.79% and a computation time of 8.4 ms, evaluated on a Commercial off-the-Shelf (CotS) Intel i7-1165G7, making it suitable for real-time robotic applications. These results highlight the potential of Predictive Spliner to enhance the performance and safety of autonomous racing vehicles. The code for Predictive Spliner is available at: https://github.com/ForzaETH/predictive-spliner. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.05985</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.05985</id><created>2024-10-08</created><updated>2025-02-05</updated><authors><author><keyname>Fokam</keyname><forenames>Cabrel Teguemne</forenames></author><author><keyname>Nazeer</keyname><forenames>Khaleelulla Khan</forenames></author><author><keyname>König</keyname><forenames>Lukas</forenames></author><author><keyname>Kappel</keyname><forenames>David</forenames></author><author><keyname>Subramoney</keyname><forenames>Anand</forenames></author></authors><title>Asynchronous Stochastic Gradient Descent with Decoupled Backpropagation   and Layer-Wise Updates</title><categories>cs.LG cs.AI cs.NE</categories><comments>17 pages, 5 figures</comments><msc-class>G.1.6</msc-class><acm-class>I.2.6; I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing size of deep learning models has made distributed training across multiple devices essential. However, current methods such as distributed data-parallel training suffer from large communication and synchronization overheads when training across devices, leading to longer training times as a result of suboptimal hardware utilization. Asynchronous stochastic gradient descent (ASGD) methods can improve training speed, but are sensitive to delays due to both communication and differences throughput. Moreover, the backpropagation algorithm used within ASGD workers is bottlenecked by the interlocking between its forward and backward passes. Current methods also do not take advantage of the large differences in the computation required for the forward and backward passes. Therefore, we propose an extension to ASGD called Partial Decoupled ASGD (PD-ASGD) that addresses these issues. PD-ASGD uses separate threads for the forward and backward passes, decoupling the updates and allowing for a higher ratio of forward to backward threads than the usual 1:1 ratio, leading to higher throughput. PD-ASGD also performs layer-wise (partial) model updates concurrently across multiple threads. This reduces parameter staleness and consequently improves robustness to delays. Our approach yields close to state-of-the-art results while running up to $5.95\times$ faster than synchronous data parallelism in the presence of delays, and up to $2.14\times$ times faster than comparable ASGD algorithms by achieving higher model flops utilization. We mathematically describe the gradient bias introduced by our method, establish an upper bound, and prove convergence. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.06479</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.06479</id><created>2024-10-08</created><updated>2025-02-05</updated><authors><author><keyname>Sukthanker</keyname><forenames>Rhea Sanjay</forenames></author><author><keyname>Staffler</keyname><forenames>Benedikt</forenames></author><author><keyname>Hutter</keyname><forenames>Frank</forenames></author><author><keyname>Klein</keyname><forenames>Aaron</forenames></author></authors><title>Compressing Large Language Models with Automated Sub-Network Search</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) demonstrate exceptional reasoning abilities, enabling strong generalization across diverse tasks such as commonsense reasoning and instruction following. However, as LLMs scale, inference costs become increasingly prohibitive, accumulating significantly over their life cycle. In this paper we consider model compression for LLMs to reduce model size while improving downstream task performance. We phrase this as a neural architecture search problem that automatically prunes structural components, such as attention heads, neurons, and layers by searching for the Pareto-optimal set of sub-networks balancing between performance and on-device latency. Compared to state-of-the-art structural pruning approaches and fine-tuned smaller sub-networks extracted from the pre-trained model, our method achieves upto 9.85% improvement on average on 11 diverse downstream tasks, while achieving up to 22% improvement of on-device latency. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.06845</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.06845</id><created>2024-10-09</created><updated>2025-02-05</updated><authors><author><keyname>Li</keyname><forenames>Cheng</forenames></author><author><keyname>Fung</keyname><forenames>May</forenames></author><author><keyname>Wang</keyname><forenames>Qingyun</forenames></author><author><keyname>Han</keyname><forenames>Chi</forenames></author><author><keyname>Li</keyname><forenames>Manling</forenames></author><author><keyname>Wang</keyname><forenames>Jindong</forenames></author><author><keyname>Ji</keyname><forenames>Heng</forenames></author></authors><title>MentalArena: Self-play Training of Language Models for Diagnosis and   Treatment of Mental Health Disorders</title><categories>cs.CL cs.AI cs.MA</categories><comments>Technical Report; 26 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Mental health disorders are one of the most serious diseases in the world. Most people with such a disease lack access to adequate care, which highlights the importance of training models for the diagnosis and treatment of mental health disorders. However, in the mental health domain, privacy concerns limit the accessibility of personalized treatment data, making it challenging to build powerful models. In this paper, we introduce MentalArena, a self-play framework to train language models by generating domain-specific personalized data, where we obtain a better model capable of making a personalized diagnosis and treatment (as a therapist) and providing information (as a patient). To accurately model human-like mental health patients, we devise Symptom Encoder, which simulates a real patient from both cognition and behavior perspectives. To address intent bias during patient-therapist interactions, we propose Symptom Decoder to compare diagnosed symptoms with encoded symptoms, and dynamically manage the dialogue between patient and therapist according to the identified deviations. We evaluated MentalArena against 6 benchmarks, including biomedicalQA and mental health tasks, compared to 6 advanced models. Our models, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform their counterparts, including GPT-4o. We hope that our work can inspire future research on personalized care. Code is available in https://github.com/Scarelette/MentalArena/tree/main </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.07171</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.07171</id><created>2024-10-09</created><updated>2025-02-05</updated><authors><author><keyname>Zhang</keyname><forenames>Xinchen</forenames></author><author><keyname>Yang</keyname><forenames>Ling</forenames></author><author><keyname>Li</keyname><forenames>Guohao</forenames></author><author><keyname>Cai</keyname><forenames>Yaqi</forenames></author><author><keyname>Xie</keyname><forenames>Jiake</forenames></author><author><keyname>Tang</keyname><forenames>Yong</forenames></author><author><keyname>Yang</keyname><forenames>Yujiu</forenames></author><author><keyname>Wang</keyname><forenames>Mengdi</forenames></author><author><keyname>Cui</keyname><forenames>Bin</forenames></author></authors><title>IterComp: Iterative Composition-Aware Feedback Learning from Model   Gallery for Text-to-Image Generation</title><categories>cs.CV</categories><comments>ICLR 2025. Project: https://github.com/YangLing0818/IterComp</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advanced diffusion models like RPG, Stable Diffusion 3 and FLUX have made notable strides in compositional text-to-image generation. However, these methods typically exhibit distinct strengths for compositional generation, with some excelling in handling attribute binding and others in spatial relationships. This disparity highlights the need for an approach that can leverage the complementary strengths of various models to comprehensively improve the composition capability. To this end, we introduce IterComp, a novel framework that aggregates composition-aware model preferences from multiple models and employs an iterative feedback learning approach to enhance compositional generation. Specifically, we curate a gallery of six powerful open-source diffusion models and evaluate their three key compositional metrics: attribute binding, spatial relationships, and non-spatial relationships. Based on these metrics, we develop a composition-aware model preference dataset comprising numerous image-rank pairs to train composition-aware reward models. Then, we propose an iterative feedback learning method to enhance compositionality in a closed-loop manner, enabling the progressive self-refinement of both the base diffusion model and reward models over multiple iterations. Theoretical proof demonstrates the effectiveness and extensive experiments show our significant superiority over previous SOTA methods (e.g., Omost and FLUX), particularly in multi-category object composition and complex semantic alignment. IterComp opens new research avenues in reward feedback learning for diffusion models and compositional generation. Code: https://github.com/YangLing0818/IterComp </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.08003</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.08003</id><created>2024-10-10</created><updated>2025-02-05</updated><authors><author><keyname>Shaier</keyname><forenames>Sagi</forenames></author><author><keyname>Pereira</keyname><forenames>Francisco</forenames></author><author><keyname>von der Wense</keyname><forenames>Katharina</forenames></author><author><keyname>Hunter</keyname><forenames>Lawrence E</forenames></author><author><keyname>Jones</keyname><forenames>Matt</forenames></author></authors><title>More Experts Than Galaxies: Conditionally-overlapping Experts With   Biologically-Inspired Fixed Routing</title><categories>cs.LG</categories><comments>Published as a conference paper at ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The evolution of biological neural systems has led to both modularity and sparse coding, which enables energy efficiency and robustness across the diversity of tasks in the lifespan. In contrast, standard neural networks rely on dense, non-specialized architectures, where all model parameters are simultaneously updated to learn multiple tasks, leading to interference. Current sparse neural network approaches aim to alleviate this issue but are hindered by limitations such as 1) trainable gating functions that cause representation collapse, 2) disjoint experts that result in redundant computation and slow learning, and 3) reliance on explicit input or task IDs that limit flexibility and scalability. In this paper we propose Conditionally Overlapping Mixture of ExperTs (COMET), a general deep learning method that addresses these challenges by inducing a modular, sparse architecture with an exponential number of overlapping experts. COMET replaces the trainable gating function used in Sparse Mixture of Experts with a fixed, biologically inspired random projection applied to individual input representations. This design causes the degree of expert overlap to depend on input similarity, so that similar inputs tend to share more parameters. This results in faster learning per update step and improved out-of-sample generalization. We demonstrate the effectiveness of COMET on a range of tasks, including image classification, language modeling, and regression, using several popular deep learning architectures. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.08854</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.08854</id><created>2024-10-11</created><updated>2025-02-04</updated><authors><author><keyname>Yan</keyname><forenames>Zijiang</forenames></author><author><keyname>Zhou</keyname><forenames>Hao</forenames></author><author><keyname>Tabassum</keyname><forenames>Hina</forenames></author><author><keyname>Liu</keyname><forenames>Xue</forenames></author></authors><title>Hybrid LLM-DDQN based Joint Optimization of V2I Communication and   Autonomous Driving</title><categories>cs.LG cs.AI cs.NI cs.SY eess.SY</categories><comments>Accepted by IEEE Wireless Communications Letters</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Large language models (LLMs) have received considerable interest recently due to their outstanding reasoning and comprehension capabilities. This work explores applying LLMs to vehicular networks, aiming to jointly optimize vehicle-to-infrastructure (V2I) communications and autonomous driving (AD) policies. We deploy LLMs for AD decision-making to maximize traffic flow and avoid collisions for road safety, and a double deep Q-learning algorithm (DDQN) is used for V2I optimization to maximize the received data rate and reduce frequent handovers. In particular, for LLM-enabled AD, we employ the Euclidean distance to identify previously explored AD experiences, and then LLMs can learn from past good and bad decisions for further improvement. Then, LLM-based AD decisions will become part of states in V2I problems, and DDQN will optimize the V2I decisions accordingly. After that, the AD and V2I decisions are iteratively optimized until convergence. Such an iterative optimization approach can better explore the interactions between LLMs and conventional reinforcement learning techniques, revealing the potential of using LLMs for network optimization and management. Finally, the simulations demonstrate that our proposed hybrid LLM-DDQN approach outperforms the conventional DDQN algorithm, showing faster convergence and higher average rewards. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.09008</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.09008</id><created>2024-10-11</created><updated>2025-02-05</updated><authors><author><keyname>Yang</keyname><forenames>Ling</forenames></author><author><keyname>Yu</keyname><forenames>Zhaochen</forenames></author><author><keyname>Zhang</keyname><forenames>Tianjun</forenames></author><author><keyname>Xu</keyname><forenames>Minkai</forenames></author><author><keyname>Gonzalez</keyname><forenames>Joseph E.</forenames></author><author><keyname>Cui</keyname><forenames>Bin</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>SuperCorrect: Supervising and Correcting Language Models with   Error-Driven Insights</title><categories>cs.CL</categories><comments>ICLR 2025. Project: https://github.com/YangLing0818/SuperCorrect-llm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown significant improvements in various reasoning tasks. However, smaller models such as Llama-3-8B and DeepSeekMath-Base still struggle with complex mathematical reasoning because they fail to effectively identify and correct reasoning errors. Recent reflection-based methods aim to address these issues by enabling self-reflection and self-correction, but they still face challenges in independently detecting errors in their reasoning steps. To overcome these limitations, we propose SuperCorrect, a novel two-stage framework that uses a large teacher model to supervise and correct both the reasoning and reflection processes of a smaller student model. In the first stage, we extract hierarchical high-level and detailed thought templates from the teacher model to guide the student model in eliciting more fine-grained reasoning thoughts. In the second stage, we introduce cross-model collaborative direct preference optimization (DPO) to enhance the self-correction abilities of the student model by following the teacher's correction traces during training. This cross-model DPO approach teaches the student model to effectively locate and resolve erroneous thoughts with error-driven insights from the teacher model, breaking the bottleneck of its thoughts and acquiring new skills and knowledge to tackle challenging problems. Extensive experiments consistently demonstrate our superiority over previous methods. Notably, our SuperCorrect-7B model significantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and Qwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA performance among all 7B models. Code: https://github.com/YangLing0818/SuperCorrect-llm </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.11061</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.11061</id><created>2024-10-14</created><updated>2025-02-05</updated><authors><author><keyname>Tang</keyname><forenames>Bo</forenames></author><author><keyname>Khalil</keyname><forenames>Elias B.</forenames></author><author><keyname>Drgoňa</keyname><forenames>Ján</forenames></author></authors><title>Learning to Optimize for Mixed-Integer Non-linear Programming</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixed-integer nonlinear programs (MINLPs) arise in diverse domains such as energy systems and transportation but are notoriously difficult to solve, particularly on a large scale. While learning-to-optimize methods have been successful at continuous optimization, extending them to MINLPs is still challenging due to the integer constraints. To overcome this, we propose a novel deep-learning approach with two learnable correction layers to ensure solution integrality and a post-processing step to improve solution feasibility. Our experiments show that this is the first general method capable of efficiently solving large-scale MINLPs with up to tens of thousands of variables in milliseconds, delivering high-quality solutions even when traditional solvers and heuristics fail. This is the first general learning method for MINLP, successfully solving some of the largest instances reported to date. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.11767</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.11767</id><created>2024-10-15</created><updated>2025-02-05</updated><authors><author><keyname>Menon</keyname><forenames>Abhinav</forenames></author><author><keyname>Shrivastava</keyname><forenames>Manish</forenames></author><author><keyname>Krueger</keyname><forenames>David</forenames></author><author><keyname>Lubana</keyname><forenames>Ekdeep Singh</forenames></author></authors><title>Analyzing (In)Abilities of SAEs via Formal Languages</title><categories>cs.LG</categories><comments>NeurIPS workshop on Foundation Model Interventions (Awarded best   paper); North American Association of Computational Linguistics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autoencoders have been used for finding interpretable and disentangled features underlying neural network representations in both image and text domains. While the efficacy and pitfalls of such methods are well-studied in vision, there is a lack of corresponding results, both qualitative and quantitative, for the text domain. We aim to address this gap by training sparse autoencoders (SAEs) on a synthetic testbed of formal languages. Specifically, we train SAEs on the hidden representations of models trained on formal languages (Dyck-2, Expr, and English PCFG) under a wide variety of hyperparameter settings, finding interpretable latents often emerge in the features learned by our SAEs. However, similar to vision, we find performance turns out to be highly sensitive to inductive biases of the training pipeline. Moreover, we show latents correlating to certain features of the input do not always induce a causal impact on model's computation. We thus argue that causality has to become a central target in SAE training: learning of causal features should be incentivized from the ground-up. Motivated by this, we propose and perform preliminary investigations for an approach that promotes learning of causally relevant features in our formal language setting. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.13284</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.13284</id><created>2024-10-17</created><updated>2025-02-04</updated><authors><author><keyname>Chuang</keyname><forenames>Yu-Neng</forenames></author><author><keyname>Zhou</keyname><forenames>Helen</forenames></author><author><keyname>Sarma</keyname><forenames>Prathusha Kameswara</forenames></author><author><keyname>Gopalan</keyname><forenames>Parikshit</forenames></author><author><keyname>Boccio</keyname><forenames>John</forenames></author><author><keyname>Bolouki</keyname><forenames>Sara</forenames></author><author><keyname>Hu</keyname><forenames>Xia</forenames></author></authors><title>Learning to Route LLMs with Confidence Tokens</title><categories>cs.CL cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large language models (LLMs) have demonstrated impressive performance on several tasks and are increasingly deployed in real-world applications. However, especially in high-stakes settings, it becomes vital to know when the output of an LLM may be unreliable. Depending on whether an answer is trustworthy, a system can then choose to route the question to another expert, or otherwise fall back on a safe default behavior. In this work, we study the extent to which LLMs can reliably indicate confidence in their answers, and how this notion of confidence can translate into downstream accuracy gains. We propose Self-REF, a lightweight training strategy to teach LLMs to express confidence in whether their answers are correct in a reliable manner. Self-REF introduces confidence tokens into the LLM, from which a confidence score can be extracted. Compared to conventional approaches such as verbalizing confidence and examining token probabilities, we demonstrate empirically that confidence tokens show significant improvements in downstream routing and rejection learning tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.13776</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.13776</id><created>2024-10-17</created><updated>2025-02-04</updated><authors><author><keyname>Chochlakis</keyname><forenames>Georgios</forenames></author><author><keyname>Potamianos</keyname><forenames>Alexandros</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author><author><keyname>Narayanan</keyname><forenames>Shrikanth</forenames></author></authors><title>Aggregation Artifacts in Subjective Tasks Collapse Large Language   Models' Posteriors</title><categories>cs.CL cs.AI</categories><comments>16 pages, 12 figures, 3 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In-context Learning (ICL) has become the primary method for performing natural language tasks with Large Language Models (LLMs). The knowledge acquired during pre-training is crucial for this few-shot capability, providing the model with task priors. However, recent studies have shown that ICL predominantly relies on retrieving task priors rather than "learning" to perform tasks. This limitation is particularly evident in complex subjective domains such as emotion and morality, where priors significantly influence posterior predictions. In this work, we examine whether this is the result of the aggregation used in corresponding datasets, where trying to combine low-agreement, disparate annotations might lead to annotation artifacts that create detrimental noise in the prompt. Moreover, we evaluate the posterior bias towards certain annotators by grounding our study in appropriate, quantitative measures of LLM priors. Our results indicate that aggregation is a confounding factor in the modeling of subjective tasks, and advocate focusing on modeling individuals instead. However, aggregation does not explain the entire gap between ICL and the state of the art, meaning other factors in such tasks also account for the observed phenomena. Finally, by rigorously studying annotator-level labels, we find that it is possible for minority annotators to both better align with LLMs and have their perspectives further amplified. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.13784</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.13784</id><created>2024-10-17</created><updated>2025-02-05</updated><authors><author><keyname>Saraswathi</keyname><forenames>Sindura</forenames></author><author><keyname>Kümmerle</keyname><forenames>Christian</forenames></author></authors><title>An Exposition of Pathfinding Strategies Within Lightning Network Clients</title><categories>cs.NI cs.CE cs.CR cs.SI</categories><comments>19 pages, 5 figures, 12 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The Lightning Network is a peer-to-peer network designed to address Bitcoin's scalability challenges, facilitating rapid, cost-effective, and instantaneous transactions through bidirectional, blockchain-backed payment channels among network peers. Due to a source-based routing of payments, different pathfinding strategies are used in practice, trading off different objectives for each other such as payment reliability and routing fees. This paper explores differences within pathfinding strategies used by prominent Lightning Network node implementations, which include different underlying cost functions and different constraints, as well as different greedy algorithms of shortest path-type. Surprisingly, we observe that the pathfinding problems that most LN node implementations attempt to solve are NP-complete, and cannot be guaranteed to be optimally solved by the variants of Dijkstra's algorithm currently deployed in production. Through comparative analysis and simulations, we evaluate efficacy of different pathfinding strategies across metrics such as success rate, fees, path length, and timelock. Our experiments indicate that the strategies used by Eclair are advantageous in terms of payment reliability and result in paths with low fees. LND exhibits moderate success rates, while LDK results in paths with higher fee levels for smaller payment amounts; furthermore, CLN stands out for its minimal timelock paths. Additionally, we investigate the impact of Lightning node connectivity levels on routing efficiency. The findings of our analysis provide insights towards future improvements of pathfinding strategies and algorithms used within the Lightning Network. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.14159</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.14159</id><created>2024-10-17</created><updated>2025-02-05</updated><authors><author><keyname>Laria</keyname><forenames>Héctor</forenames></author><author><keyname>Gomez-Villa</keyname><forenames>Alex</forenames></author><author><keyname>Wang</keyname><forenames>Kai</forenames></author><author><keyname>Raducanu</keyname><forenames>Bogdan</forenames></author><author><keyname>van de Weijer</keyname><forenames>Joost</forenames></author></authors><title>Assessing Open-world Forgetting in Generative Image Model Customization</title><categories>cs.CV cs.GR cs.LG</categories><comments>Update: Added feedback; Project page:   https://hecoding.github.io/open-world-forgetting/</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Recent advances in diffusion models have significantly enhanced image generation capabilities. However, customizing these models with new classes often leads to unintended consequences that compromise their reliability. We introduce the concept of open-world forgetting to characterize the vast scope of these unintended alterations. Our work presents the first systematic investigation into open-world forgetting in diffusion models, focusing on semantic and appearance drift of representations. Using zero-shot classification, we demonstrate that even minor model adaptations can lead to significant semantic drift affecting areas far beyond newly introduced concepts, with accuracy drops of up to 60% on previously learned concepts. Our analysis of appearance drift reveals substantial changes in texture and color distributions of generated content. To address these issues, we propose a functional regularization strategy that effectively preserves original capabilities while accommodating new concepts. Through extensive experiments across multiple datasets and evaluation metrics, we demonstrate that our approach significantly reduces both semantic and appearance drift. Our study highlights the importance of considering open-world forgetting in future research on model customization and finetuning methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.14202</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.14202</id><created>2024-10-18</created><updated>2025-02-05</updated><authors><author><keyname>Chu</keyname><forenames>SeongYeub</forenames></author><author><keyname>Kim</keyname><forenames>JongWoo</forenames></author><author><keyname>Wong</keyname><forenames>Bryan</forenames></author><author><keyname>Yi</keyname><forenames>MunYong</forenames></author></authors><title>Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay   Scoring with Rationale Generated by LLMs</title><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing automated essay scoring (AES) has solely relied on essay text without using explanatory rationales for the scores, thereby forgoing an opportunity to capture the specific aspects evaluated by rubric indicators in a fine-grained manner. This paper introduces Rationale-based Multiple Trait Scoring (RMTS), a novel approach for multi-trait essay scoring that integrates prompt-engineering-based large language models (LLMs) with a fine-tuning-based essay scoring model using a smaller large language model (S-LLM). RMTS uses an LLM-based trait-wise rationale generation system where a separate LLM agent generates trait-specific rationales based on rubric guidelines, which the scoring model uses to accurately predict multi-trait scores. Extensive experiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize, show that RMTS significantly outperforms state-of-the-art models and vanilla S-LLMs in trait-specific scoring. By assisting quantitative assessment with fine-grained qualitative rationales, RMTS enhances the trait-wise reliability, providing partial explanations about essays. The code is available at https://github.com/BBeeChu/RMTS.git. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.14442</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.14442</id><created>2024-10-18</created><updated>2025-02-05</updated><authors><author><keyname>Wu</keyname><forenames>You</forenames></author><author><keyname>Wu</keyname><forenames>Haoyi</forenames></author><author><keyname>Tu</keyname><forenames>Kewei</forenames></author></authors><title>A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference</title><categories>cs.CL</categories><comments>Accepted to NAACL2025 main conference</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recently, sharing key-value (KV) cache across layers has been found effective in efficient inference of large language models (LLMs). To systematically investigate different techniques of cross-layer KV sharing, we propose a unified framework that covers several recent methods and their novel variants. We conduct comprehensive experiments on all the configurations of the framework, evaluating their generation throughput and performance in language modeling and downstream tasks. We find that when reducing the size of the KV cache by 2$\times$, most configurations can achieve higher throughput than standard transformers while maintaining competitive performance. When further reducing the size of the KV cache, however, pairing queries of all layers with KVs of upper layers performs better, at the expense of additional training cost and prefilling latency. We hope that this work will help users make more informed choices of cross-layer KV sharing approaches and facilitate future research on efficient LLM inference. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.14501</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.14501</id><created>2024-10-18</created><updated>2025-02-05</updated><authors><author><keyname>van Bekkum</keyname><forenames>Marvin</forenames></author></authors><title>Using sensitive data to de-bias AI systems: Article 10(5) of the EU AI   Act</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In June 2024, the EU AI Act came into force. The AI Act includes obligations for the provider of an AI system. Article 10 of the AI Act includes a new obligation for providers to evaluate whether their training, validation and testing datasets meet certain quality criteria, including an appropriate examination of biases in the datasets and correction measures. With the obligation comes a new provision in Article 10(5) AI Act, allowing providers to collect sensitive data to fulfil the obligation. The exception aims to prevent discrimination. In this paper, I research the scope and implications of Article 10(5) AI Act. The paper primarily concerns European Union law, but may be relevant in other parts of the world, as policymakers aim to regulate biases in AI systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.15644</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.15644</id><created>2024-10-21</created><authors><author><keyname>Maleki</keyname><forenames>Mahdi Farrokhi</forenames></author><author><keyname>Zhao</keyname><forenames>Richard</forenames></author></authors><title>Procedural Content Generation in Games: A Survey with Insights on   Emerging LLM Integration</title><categories>cs.AI</categories><journal-ref>Proceedings of the Twentieth AAAI Conference on Artificial   Intelligence and Interactive Digital Entertainment (AIIDE-24), Lexington,   USA, November, 2024</journal-ref><doi>10.1609/aiide.v20i1.31877</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Procedural Content Generation (PCG) is defined as the automatic creation of game content using algorithms. PCG has a long history in both the game industry and the academic world. It can increase player engagement and ease the work of game designers. While recent advances in deep learning approaches in PCG have enabled researchers and practitioners to create more sophisticated content, it is the arrival of Large Language Models (LLMs) that truly disrupted the trajectory of PCG advancement.   This survey explores the differences between various algorithms used for PCG, including search-based methods, machine learning-based methods, other frequently used methods (e.g., noise functions), and the newcomer, LLMs. We also provide a detailed discussion on combined methods. Furthermore, we compare these methods based on the type of content they generate and the publication dates of their respective papers. Finally, we identify gaps in the existing academic work and suggest possible directions for future research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.15929</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.15929</id><created>2024-10-21</created><updated>2025-02-05</updated><authors><author><keyname>Inoue</keyname><forenames>Koji</forenames></author><author><keyname>Lala</keyname><forenames>Divesh</forenames></author><author><keyname>Skantze</keyname><forenames>Gabriel</forenames></author><author><keyname>Kawahara</keyname><forenames>Tatsuya</forenames></author></authors><title>Yeah, Un, Oh: Continuous and Real-time Backchannel Prediction with   Fine-tuning of Voice Activity Projection</title><categories>cs.CL cs.HC cs.SD eess.AS</categories><comments>This paper has been accepted for presentation at the main conference   of 2025 Annual Conference of the Nations of the Americas Chapter of the   Association for Computational Linguistics (NAACL 2025) and represents the   author's version of the work</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In human conversations, short backchannel utterances such as "yeah" and "oh" play a crucial role in facilitating smooth and engaging dialogue. These backchannels signal attentiveness and understanding without interrupting the speaker, making their accurate prediction essential for creating more natural conversational agents. This paper proposes a novel method for real-time, continuous backchannel prediction using a fine-tuned Voice Activity Projection (VAP) model. While existing approaches have relied on turn-based or artificially balanced datasets, our approach predicts both the timing and type of backchannels in a continuous and frame-wise manner on unbalanced, real-world datasets. We first pre-train the VAP model on a general dialogue corpus to capture conversational dynamics and then fine-tune it on a specialized dataset focused on backchannel behavior. Experimental results demonstrate that our model outperforms baseline methods in both timing and type prediction tasks, achieving robust performance in real-time environments. This research offers a promising step toward more responsive and human-like dialogue systems, with implications for interactive spoken dialogue applications such as virtual assistants and robots. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.16534</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.16534</id><created>2024-10-21</created><updated>2025-02-04</updated><authors><author><keyname>DeSalvo</keyname><forenames>Giulia</forenames></author><author><keyname>Kagy</keyname><forenames>Jean-Fracois</forenames></author><author><keyname>Karydas</keyname><forenames>Lazaros</forenames></author><author><keyname>Rostamizadeh</keyname><forenames>Afshin</forenames></author><author><keyname>Kumar</keyname><forenames>Sanjiv</forenames></author></authors><title>SoftSRV: Learn to Generate Targeted Synthetic Data</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We present a novel framework, SoftSRV, that is used to generate targeted synthetic fine-tuning data for improving task-specific model performance. Given a sample from a target distribution, our proposed framework uses a data-driven loss minimization approach to steer a frozen large language model (LLM) to generate synthetic sequences that are similar to those from the target distribution. SoftSRV provides a practical improvement over common prompt engineering approaches that rely on human-engineered prompt-templates, which can be idiosyncratic, labor-intensive to craft, and may need to be specialized per domain. We empirically evaluate our method against standard baselines guiding a large LLM to generate synthetic data to fine-tune a smaller language model on three different domains (coding, math, reasoning). We perform these evaluations without any particular specialization of the framework to each domain, emphasizing the generality of our approach. We find that SoftSRV improves upon typical prompt engineering approaches, generating targeted data that leads to fine-tuned models with significantly better task-specific performance. In addition, SoftSRV-generated data better matches the target distribution according to the MAUVE similarity metric. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.16653</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.16653</id><created>2024-10-21</created><authors><author><keyname>Saadat</keyname><forenames>Kimiya</forenames></author><author><keyname>Zhao</keyname><forenames>Richard</forenames></author></authors><title>Enhancing Two-Player Performance Through Single-Player Knowledge   Transfer: An Empirical Study on Atari 2600 Games</title><categories>cs.LG cs.AI</categories><journal-ref>Proceedings of the Twentieth AAAI Conference on Artificial   Intelligence and Interactive Digital Entertainment (AIIDE-24), Lexington,   USA, November, 2024</journal-ref><doi>10.1609/aiide.v20i1.31871</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Playing two-player games using reinforcement learning and self-play can be challenging due to the complexity of two-player environments and the possible instability in the training process. We propose that a reinforcement learning algorithm can train more efficiently and achieve improved performance in a two-player game if it leverages the knowledge from the single-player version of the same game. This study examines the proposed idea in ten different Atari 2600 environments using the Atari 2600 RAM as the input state. We discuss the advantages of using transfer learning from a single-player training process over training in a two-player setting from scratch, and demonstrate our results in a few measures such as training time and average total reward. We also discuss a method of calculating RAM complexity and its relationship to performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.18959</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.18959</id><created>2024-10-24</created><updated>2025-02-04</updated><authors><author><keyname>Williams</keyname><forenames>Andrew Robert</forenames></author><author><keyname>Ashok</keyname><forenames>Arjun</forenames></author><author><keyname>Marcotte</keyname><forenames>Étienne</forenames></author><author><keyname>Zantedeschi</keyname><forenames>Valentina</forenames></author><author><keyname>Subramanian</keyname><forenames>Jithendaraa</forenames></author><author><keyname>Riachi</keyname><forenames>Roland</forenames></author><author><keyname>Requeima</keyname><forenames>James</forenames></author><author><keyname>Lacoste</keyname><forenames>Alexandre</forenames></author><author><keyname>Rish</keyname><forenames>Irina</forenames></author><author><keyname>Chapados</keyname><forenames>Nicolas</forenames></author><author><keyname>Drouin</keyname><forenames>Alexandre</forenames></author></authors><title>Context is Key: A Benchmark for Forecasting with Essential Textual   Information</title><categories>cs.LG cs.AI stat.ML</categories><comments>Preprint; under review. First two authors contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forecasting is a critical task in decision-making across numerous domains. While historical numerical data provide a start, they fail to convey the complete context for reliable and accurate predictions. Human forecasters frequently rely on additional information, such as background knowledge and constraints, which can efficiently be communicated through natural language. However, in spite of recent progress with LLM-based forecasters, their ability to effectively integrate this textual information remains an open question. To address this, we introduce "Context is Key" (CiK), a time-series forecasting benchmark that pairs numerical data with diverse types of carefully crafted textual context, requiring models to integrate both modalities; crucially, every task in CiK requires understanding textual context to be solved successfully. We evaluate a range of approaches, including statistical models, time series foundation models, and LLM-based forecasters, and propose a simple yet effective LLM prompting method that outperforms all other tested methods on our benchmark. Our experiments highlight the importance of incorporating contextual information, demonstrate surprising performance when using LLM-based forecasting models, and also reveal some of their critical shortcomings. This benchmark aims to advance multimodal forecasting by promoting models that are both accurate and accessible to decision-makers with varied technical expertise. The benchmark can be visualized at https://anon-forecast.github.io/benchmark_report_dev/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.20542</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.20542</id><created>2024-10-27</created><updated>2025-02-05</updated><authors><author><keyname>Pillai</keyname><forenames>Arvind</forenames></author><author><keyname>Spathis</keyname><forenames>Dimitris</forenames></author><author><keyname>Kawsar</keyname><forenames>Fahim</forenames></author><author><keyname>Malekzadeh</keyname><forenames>Mohammad</forenames></author></authors><title>PaPaGei: Open Foundation Models for Optical Physiological Signals</title><categories>cs.LG eess.SP</categories><comments>Accepted at ICLR 2025. Improved version with new experiments and   results. Code and models:   https://github.com/nokia-bell-labs/papagei-foundation-model</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Photoplethysmography (PPG) is the leading non-invasive technique for monitoring biosignals and cardiovascular health, with widespread adoption in both clinical settings and consumer wearable devices. While machine learning models trained on PPG signals have shown promise, they tend to be task-specific and struggle with generalization. Current research is limited by the use of single-device datasets, insufficient exploration of out-of-domain generalization, and a lack of publicly available models, which hampers reproducibility. To address these limitations, we present PaPaGei, the first open foundation model for PPG signals. The model is pre-trained on over 57,000 hours of data, comprising 20 million unlabeled PPG segments from publicly available datasets. We introduce a novel representation learning approach that leverages domain knowledge of PPG signal morphology across individuals, enabling the capture of richer representations compared to traditional contrastive learning methods. We evaluate PaPaGei against state-of-the-art time-series foundation models and self-supervised learning benchmarks across 20 tasks from 10 diverse datasets, spanning cardiovascular health, sleep disorders, pregnancy monitoring, and wellbeing assessment. Our model demonstrates superior performance, improving classification and regression metrics by 6.3% and 2.9% respectively in at least 14 tasks. Notably, PaPaGei achieves these results while being more data- and parameter-efficient, outperforming models that are 70x larger. Beyond accuracy, we examine model robustness across different skin tones, establishing a benchmark for bias evaluation in future models. PaPaGei can serve as both a feature extractor and an encoder for multimodal models, opening up new opportunities for multimodal health monitoring. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.20724</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.20724</id><created>2024-10-28</created><updated>2025-02-05</updated><authors><author><keyname>Li</keyname><forenames>Mufei</forenames></author><author><keyname>Miao</keyname><forenames>Siqi</forenames></author><author><keyname>Li</keyname><forenames>Pan</forenames></author></authors><title>Simple Is Effective: The Roles of Graphs and Large Language Models in   Knowledge-Graph-Based Retrieval-Augmented Generation</title><categories>cs.CL cs.LG</categories><comments>Accepted by ICLR 2025; Code available at   https://github.com/Graph-COM/SubgraphRAG</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) demonstrate strong reasoning abilities but face limitations such as hallucinations and outdated knowledge. Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by grounding LLM outputs in structured external knowledge from KGs. However, current KG-based RAG frameworks still struggle to optimize the trade-off between retrieval effectiveness and efficiency in identifying a suitable amount of relevant graph information for the LLM to digest. We introduce SubgraphRAG, extending the KG-based RAG framework that retrieves subgraphs and leverages LLMs for reasoning and answer prediction. Our approach innovatively integrates a lightweight multilayer perceptron with a parallel triple-scoring mechanism for efficient and flexible subgraph retrieval while encoding directional structural distances to enhance retrieval effectiveness. The size of retrieved subgraphs can be flexibly adjusted to match the query's need and the downstream LLM's capabilities. This design strikes a balance between model complexity and reasoning power, enabling scalable and generalizable retrieval processes. Notably, based on our retrieved subgraphs, smaller LLMs like Llama3.1-8B-Instruct deliver competitive results with explainable reasoning, while larger models like GPT-4o achieve state-of-the-art accuracy compared with previous baselines -- all without fine-tuning. Extensive evaluations on the WebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency, accuracy, and reliability by reducing hallucinations and improving response grounding. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.21000</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.21000</id><created>2024-10-28</created><updated>2025-02-04</updated><authors><author><keyname>Zhang</keyname><forenames>Zhilin</forenames></author><author><keyname>Wang</keyname><forenames>Jie</forenames></author><author><keyname>Zhu</keyname><forenames>Ruiqi</forenames></author><author><keyname>Gong</keyname><forenames>Xiaoliang</forenames></author></authors><title>Efficient Bilinear Attention-based Fusion for Medical Visual Question   Answering</title><categories>eess.IV cs.AI cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Medical Visual Question Answering (MedVQA) has attracted growing interest at the intersection of computer vision and natural language processing. By interpreting medical images and providing precise answers to relevant clinical inquiries, MedVQA has the potential to support diagnostic decision-making and reduce workload across various domains, particularly radiology. While recent approaches rely heavily on unified large pre-trained Visual-Language Models, research on more efficient fusion mechanisms remains relatively limited in this domain. In this paper, we introduce a novel fusion model, OMniBAN, that integrates Orthogonality loss, Multi-head attention, and a Bilinear Attention Network to achieve high computational efficiency alongside solid performance. We conduct comprehensive experiments and provide insights into how bilinear attention fusion can approximate the performance of larger fusion models like cross-modal Transformer. Our results demonstrate that OMniBAN outperforms traditional approaches on key MedVQA benchmarks while maintaining a lower computational cost. This balance between efficiency and accuracy suggests that OMniBAN could be a viable option for real-world medical image question answering, where computational resources are often constrained. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.21582</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.21582</id><created>2024-10-28</created><updated>2025-02-04</updated><authors><author><keyname>Hwang</keyname><forenames>Jaedong</forenames></author><author><keyname>Cheung</keyname><forenames>Brian</forenames></author><author><keyname>Hong</keyname><forenames>Zhang-Wei</forenames></author><author><keyname>Boopathy</keyname><forenames>Akhilan</forenames></author><author><keyname>Agrawal</keyname><forenames>Pulkit</forenames></author><author><keyname>Fiete</keyname><forenames>Ila</forenames></author></authors><title>ImageNet-RIB Benchmark: Large Pre-Training Datasets Don't Always   Guarantee Robustness after Fine-Tuning</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Highly performant large-scale pre-trained models promise to also provide a valuable foundation for learning specialized tasks, by fine-tuning the model to the desired task. By starting from a good general-purpose model, the goal is to achieve both specialization in the target task and maintain robustness. To assess the robustness of models on out-of-distribution samples after fine-tuning on downstream datasets, we introduce a new robust fine-tuning benchmark, ImageNet-RIB (Robustness Inheritance Benchmark). The benchmark consists of a set of related but distinct specialized (downstream) datasets; pre-trained models are fine-tuned on one dataset in the set and their robustness is assessed on the rest, iterating across all tasks for fine-tuning and assessment. The distance between the pre-training and downstream datasets, measured by optimal transport, predicts this performance degradation on the pre-training dataset. Though continual learning methods help maintain robustness, fine-tuning generally reduces generalization performance on related downstream tasks across models. Counterintuitively, model robustness after fine-tuning on related downstream tasks is the worst when the pre-training dataset is the richest and the most diverse. This suggests that starting with the strongest foundation model is not necessarily the best approach for performance on specialist tasks. ImageNet-RIB thus offers key insights for developing more resilient fine-tuning strategies and building robust machine learning models. https://jd730.github.io/projects/ImageNet-RIB </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.22110</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.22110</id><created>2024-10-29</created><updated>2025-02-05</updated><authors><author><keyname>Dikstein</keyname><forenames>Lior</forenames></author><author><keyname>Lapid</keyname><forenames>Ariel</forenames></author><author><keyname>Netzer</keyname><forenames>Arnon</forenames></author><author><keyname>Habi</keyname><forenames>Hai Victor</forenames></author></authors><title>Data Generation for Hardware-Friendly Post-Training Quantization</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Zero-shot quantization (ZSQ) using synthetic data is a key approach for post-training quantization (PTQ) under privacy and security constraints. However, existing data generation methods often struggle to effectively generate data suitable for hardware-friendly quantization, where all model layers are quantized. We analyze existing data generation methods based on batch normalization (BN) matching and identify several gaps between synthetic and real data: 1) Current generation algorithms do not optimize the entire synthetic dataset simultaneously; 2) Data augmentations applied during training are often overlooked; and 3) A distribution shift occurs in the final model layers due to the absence of BN in those layers. These gaps negatively impact ZSQ performance, particularly in hardware-friendly quantization scenarios. In this work, we propose Data Generation for Hardware-friendly quantization (DGH), a novel method that addresses these gaps. DGH jointly optimizes all generated images, regardless of the image set size or GPU memory constraints. To address data augmentation mismatches, DGH includes a preprocessing stage that mimics the augmentation process and enhances image quality by incorporating natural image priors. Finally, we propose a new distribution-stretching loss that aligns the support of the feature map distribution between real and synthetic data. This loss is applied to the model's output and can be adapted to various tasks. DGH demonstrates significant improvements in quantization performance across multiple tasks, achieving up to a 30% increase in accuracy for hardware-friendly ZSQ in both classification and object detection, often performing on par with real data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.22353</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.22353</id><created>2024-10-15</created><updated>2025-02-05</updated><authors><author><keyname>Chen</keyname><forenames>Zhongwu</forenames></author><author><keyname>Xu</keyname><forenames>Chengjin</forenames></author><author><keyname>Wang</keyname><forenames>Dingmin</forenames></author><author><keyname>Huang</keyname><forenames>Zhen</forenames></author><author><keyname>Dou</keyname><forenames>Yong</forenames></author><author><keyname>Guo</keyname><forenames>Jian</forenames></author></authors><title>RuleRAG: Rule-Guided Retrieval-Augmented Generation with Language Models   for Question Answering</title><categories>cs.IR cs.AI cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Retrieval-augmented generation (RAG) has shown promising potential in knowledge intensive question answering (QA). However, existing approaches only consider the query itself, neither specifying the retrieval preferences for the retrievers nor informing the generators of how to refer to the retrieved documents for the answers, which poses a significant challenge to the QA performance. To address these issues, we propose Rule-guided Retrieval-Augmented Generation with LMs, which explicitly introduces rules for in-context learning (RuleRAG-ICL) to guide retrievers to recall related documents in the directions of rules and uniformly guide generators to reason attributed by the same rules. Moreover, most existing RAG datasets were constructed without considering rules and Knowledge Graphs (KGs) are recognized as providing high-quality rules. Therefore, we construct five rule-aware RAG benchmarks for QA, RuleQA, based on KGs to stress the significance of retrieval and reasoning with rules. Experiments on RuleQA demonstrate RuleRAG-ICL improves the retrieval quality of +89.2% in Recall@10 and answer accuracy of +103.1% in Exact Match, and RuleRAG-FT yields more enhancement. In addition, experiments on four existing RAG datasets show RuleRAG is also effective by offering rules in RuleQA to them, further proving the generalization of rule guidance in RuleRAG. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.23682</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.23682</id><created>2024-10-31</created><authors><author><keyname>Inoue</keyname><forenames>Shintaro</forenames></author><author><keyname>Kawaharazuka</keyname><forenames>Kento</forenames></author><author><keyname>Suzuki</keyname><forenames>Temma</forenames></author><author><keyname>Yuzaki</keyname><forenames>Sota</forenames></author><author><keyname>Ribayashi</keyname><forenames>Yoshimoto</forenames></author><author><keyname>Sahara</keyname><forenames>Yuta</forenames></author><author><keyname>Okada</keyname><forenames>Kei</forenames></author></authors><title>CubiXMusashi: Fusion of Wire-Driven CubiX and Musculoskeletal Humanoid   Musashi toward Unlimited Performance</title><categories>cs.RO</categories><comments>Accepted Humanoids2024, website -   https://shin0805.github.io/cubixmusashi/, YouTube -   https://youtu.be/IvzP98-r_mo</comments><doi>10.1109/Humanoids58906.2024.10769840</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humanoids exhibit a wide variety in terms of joint configuration, actuators, and degrees of freedom, resulting in different achievable movements and tasks for each type. Particularly, musculoskeletal humanoids are developed to closely emulate human body structure and movement functions, consisting of a skeletal framework driven by numerous muscle actuators. The redundant arrangement of muscles relative to the skeletal degrees of freedom has been used to represent the flexible and complex body movements observed in humans. However, due to this flexible body and high degrees of freedom, modeling, simulation, and control become extremely challenging, limiting the feasible movements and tasks. In this study, we integrate the musculoskeletal humanoid Musashi with the wire-driven robot CubiX, capable of connecting to the environment, to form CubiXMusashi. This combination addresses the shortcomings of traditional musculoskeletal humanoids and enables movements beyond the capabilities of other humanoids. CubiXMusashi connects to the environment with wires and drives by winding them, successfully achieving movements such as pull-up, rising from a lying pose, and mid-air kicking, which are difficult for Musashi alone. This concept demonstrates that various humanoids, not limited to musculoskeletal humanoids, can mitigate their physical constraints and acquire new abilities by connecting to the environment and driving through wires. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.01918</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.01918</id><created>2024-11-04</created><updated>2025-02-05</updated><authors><author><keyname>Peng</keyname><forenames>Ting</forenames></author><author><keyname>Li</keyname><forenames>Yuan</forenames></author><author><keyname>Li</keyname><forenames>Tao</forenames></author><author><keyname>Xu</keyname><forenames>Xiaoxue</forenames></author><author><keyname>Dong</keyname><forenames>Xiang</forenames></author><author><keyname>Cai</keyname><forenames>Yincai</forenames></author></authors><title>Preemptive Holistic Collaborative System and Its Application in Road   Transportation</title><categories>eess.SY cs.SY</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Numerous real-world systems, including manufacturing processes, supply chains, and robotic systems, involve multiple independent entities with diverse objectives. The potential for conflicts arises from the inability of these entities to accurately predict and anticipate each other's actions. To address this challenge, we propose the Preemptive Holistic Collaborative System (PHCS) framework. By enabling information sharing and collaborative planning among independent entities, the PHCS facilitates the preemptive resolution of potential conflicts. We apply the PHCS framework to the specific context of road transportation, resulting in the Preemptive Holistic Collaborative Road Transportation System (PHCRTS). This system leverages shared driving intentions and pre-planned trajectories to optimize traffic flow and enhance safety. Simulation experiments in a two-lane merging scenario demonstrate the effectiveness of PHCRTS, reducing vehicle time delays by 90%, increasing traffic capacity by 300%, and eliminating accidents. The PHCS framework offers a promising approach to optimize the performance and safety of complex systems with multiple independent entities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.01970</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.01970</id><created>2024-11-04</created><authors><author><keyname>Horoschenkoff</keyname><forenames>Peter</forenames></author><author><keyname>Rödiger</keyname><forenames>Jasper</forenames></author><author><keyname>Wilske</keyname><forenames>Martin</forenames></author></authors><title>A new control- and management architecture for SDN-enabled quantum key   distribution networks</title><categories>cs.NI</categories><comments>This project has received funding from the German research ministry   "Bundesministerium fuer Bildung, Wissenschaft, Forschung und Technologie"   (BMBF) as part of the DemoQuanDT research and innovation programm under grand   agreement No. 16KISQ074</comments><doi>10.1364/JOCN.547074</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims to address the challenge of designing secure and high performance Quantum Key Distribution Networks (QKDN), which are essential for encrypted communication in the era of quantum computing. Focusing on the control and management (CM) layer essential for monitoring and routing, the study emphasizes centrally managed software defined networks (SDN). We begin by analyzing QKDN routing characteristics needed for evaluating two existed architectures and the proposed, new CM layer implementation. Following the theoretical analysis, we conduct a discrete-event based simulation in which the proposed architecture is compared to an existent serving as performance-baseline. The results provide recommendations based on use cases for which different architectures show superiority and offer valuable insights into the development and evaluation of CM architectures for QKDNs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.04586</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.04586</id><created>2024-11-07</created><updated>2025-02-04</updated><authors><author><keyname>Martinez-Seras</keyname><forenames>Aitor</forenames></author><author><keyname>Del Ser</keyname><forenames>Javier</forenames></author><author><keyname>Olivares-Rad</keyname><forenames>Aitzol</forenames></author><author><keyname>Andres</keyname><forenames>Alain</forenames></author><author><keyname>Garcia-Bringas</keyname><forenames>Pablo</forenames></author></authors><title>On the Inherent Robustness of One-Stage Object Detection against   Out-of-Distribution Data</title><categories>cs.CV cs.AI cs.LG</categories><comments>13 figures, 4 tables, under review</comments><msc-class>68T45, 68T07</msc-class><acm-class>I.2.10</acm-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Robustness is a fundamental aspect for developing safe and trustworthy models, particularly when they are deployed in the open world. In this work we analyze the inherent capability of one-stage object detectors to robustly operate in the presence of out-of-distribution (OoD) data. Specifically, we propose a novel detection algorithm for detecting unknown objects in image data, which leverages the features extracted by the model from each sample. Differently from other recent approaches in the literature, our proposal does not require retraining the object detector, thereby allowing for the use of pretrained models. Our proposed OoD detector exploits the application of supervised dimensionality reduction techniques to mitigate the effects of the curse of dimensionality on the features extracted by the model. Furthermore, it utilizes high-resolution feature maps to identify potential unknown objects in an unsupervised fashion. Our experiments analyze the Pareto trade-off between the performance detecting known and unknown objects resulting from different algorithmic configurations and inference confidence thresholds. We also compare the performance of our proposed algorithm to that of logits-based post-hoc OoD methods, as well as possible fusion strategies. Finally, we discuss on the competitiveness of all tested methods against state-of-the-art OoD approaches for object detection models over the recently published Unknown Object Detection benchmark. The obtained results verify that the performance of avant-garde post-hoc OoD detectors can be further improved when combined with our proposed algorithm. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.06568</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.06568</id><created>2024-11-10</created><updated>2025-02-04</updated><authors><author><keyname>Alfano</keyname><forenames>Carlo</forenames></author><author><keyname>Sapora</keyname><forenames>Silvia</forenames></author><author><keyname>Foerster</keyname><forenames>Jakob Nicolaus</forenames></author><author><keyname>Rebeschini</keyname><forenames>Patrick</forenames></author><author><keyname>Teh</keyname><forenames>Yee Whye</forenames></author></authors><title>Meta-Learning Objectives for Preference Optimization</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluating preference optimization (PO) algorithms on LLM alignment is a challenging task that presents prohibitive costs, noise, and several variables like model size and hyper-parameters. In this work, we show that it is possible to gain insights on the efficacy of PO algorithm on much simpler benchmarks. We design a diagnostic suite of MuJoCo tasks and datasets, which we use to systematically evaluate PO algorithms, establishing a more controlled and cheaper benchmark. We then propose a novel family of PO algorithms based on mirror descent, which we call Mirror Preference Optimization (MPO). Through evolutionary strategies, we search this class to discover algorithms specialized to specific properties of preference datasets, such as mixed-quality or noisy data. We demonstrate that our discovered PO algorithms outperform all known algorithms in the targeted MuJoCo settings. Finally, based on the insights gained from our MuJoCo experiments, we design a novel PO algorithm that significantly outperforms existing baselines in an LLM alignment task. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.07463</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.07463</id><created>2024-11-11</created><updated>2025-02-04</updated><authors><author><keyname>Maduabuchi</keyname><forenames>Chika</forenames></author><author><keyname>Jossou</keyname><forenames>Ericmoore</forenames></author><author><keyname>Bucci</keyname><forenames>Matteo</forenames></author></authors><title>MSEG-VCUQ: Multimodal SEGmentation with Enhanced Vision Foundation   Models, Convolutional Neural Networks, and Uncertainty Quantification for   High-Speed Video Phase Detection Data</title><categories>cs.CV cs.LG eess.IV</categories><comments>Under Review in ICML 25</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  High-speed video (HSV) phase detection (PD) segmentation is crucial for monitoring vapor, liquid, and microlayer phases in industrial processes. While CNN-based models like U-Net have shown success in simplified shadowgraphy-based two-phase flow (TPF) analysis, their application to complex HSV PD tasks remains unexplored, and vision foundation models (VFMs) have yet to address the complexities of either shadowgraphy-based or PD TPF video segmentation. Existing uncertainty quantification (UQ) methods lack pixel-level reliability for critical metrics like contact line density and dry area fraction, and the absence of large-scale, multimodal experimental datasets tailored to PD segmentation further impedes progress. To address these gaps, we propose MSEG-VCUQ. This hybrid framework integrates U-Net CNNs with the transformer-based Segment Anything Model (SAM) to achieve enhanced segmentation accuracy and cross-modality generalization. Our approach incorporates systematic UQ for robust error assessment and introduces the first open-source multimodal HSV PD datasets. Empirical results demonstrate that MSEG-VCUQ outperforms baseline CNNs and VFMs, enabling scalable and reliable PD segmentation for real-world boiling dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.07501</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.07501</id><created>2024-11-11</created><updated>2025-02-04</updated><authors><author><keyname>Menghani</keyname><forenames>Gaurav</forenames></author><author><keyname>Kumar</keyname><forenames>Ravi</forenames></author><author><keyname>Kumar</keyname><forenames>Sanjiv</forenames></author></authors><title>LAuReL: Learned Augmented Residual Layer</title><categories>cs.LG cs.AI cs.CV</categories><comments>Accepted at the 2nd Efficient Systems for Foundation Models Workshop   at the International Conference on Machine Learning (ICML) 2024</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the core pillars of efficient deep learning methods is architectural improvements such as the residual/skip connection, which has led to significantly better model convergence and quality. Since then the residual connection has become ubiquitous in not just convolutional neural networks but also transformer-based architectures, the backbone of LLMs.   In this paper we introduce \emph{Learned Augmented Residual Layer} (LAuReL) -- a novel generalization of the canonical residual connection -- with the goal to be an in-situ replacement of the latter while outperforming on both model quality and footprint metrics. Our experiments show that using \laurel can help boost performance for both vision and language models. For example, on the ResNet-50, ImageNet 1K task, it achieves $60\%$ of the gains from adding an extra layer, while only adding $0.003\%$ more parameters, and matches it while adding $2.6\times$ fewer parameters. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.07770</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.07770</id><created>2024-11-12</created><updated>2025-02-04</updated><authors><author><keyname>Di Teodoro</keyname><forenames>Giulia</forenames></author><author><keyname>Siciliano</keyname><forenames>Federico</forenames></author><author><keyname>Tonellotto</keyname><forenames>Nicola</forenames></author><author><keyname>Silvestri</keyname><forenames>Fabrizio</forenames></author></authors><title>A Theoretical Analysis of Recommendation Loss Functions under Negative   Sampling</title><categories>cs.IR</categories><comments>main paper 8 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Loss functions like Categorical Cross Entropy (CCE), Binary Cross Entropy (BCE), and Bayesian Personalized Ranking (BPR) are commonly used in training Recommender Systems (RSs) to differentiate positive items - those interacted with by users - and negative items. While prior works empirically showed that CCE outperforms BCE and BPR when using the full set of negative items, we provide a theoretical explanation for this by proving that CCE offers the tightest lower bound on ranking metrics like Normalized Discounted Cumulative Gain (NDCG) and Mean Reciprocal Rank (MRR), followed by BPR and BCE. However, using the full set of negative items is computationally infeasible for large-scale RSs, prompting the use of negative sampling techniques. Under negative sampling, we reveal that BPR and CCE are equivalent when a single negative sample is drawn, and all three losses converge to the same global minimum. We further demonstrate that the sampled losses remain lower bounds for NDCG (MRR), albeit in a probabilistic sense. Our worst-case analysis shows that BCE offers the strongest bound on NDCG (MRR). Experiments on five datasets and four models empirically support these theoretical findings. Our code is available at https://anonymous.4open.science/r/recsys_losses . </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.13777</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.13777</id><created>2024-11-20</created><updated>2025-02-05</updated><authors><author><keyname>Chowdhury</keyname><forenames>Shaiful</forenames></author><author><keyname>Kidwai</keyname><forenames>Hisham</forenames></author><author><keyname>Asaduzzaman</keyname><forenames>Muhammad</forenames></author></authors><title>Evidence is All We Need: Do Self-Admitted Technical Debts Impact   Method-Level Maintenance?</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Self-Admitted Technical Debt (SATD) refers to the phenomenon where developers explicitly acknowledge technical debt through comments in the source code. While considerable research has focused on detecting and addressing SATD, its true impact on software maintenance remains underexplored. The few studies that have examined this critical aspect have not provided concrete evidence linking SATD to negative effects on software maintenance. These studies, however, focused only on file- or class-level code granularity. This paper aims to empirically investigate the influence of SATD on various facets of software maintenance at the method level. We assess SATD's effects on code quality, bug susceptibility, change frequency, and the time practitioners typically take to resolve SATD.   By analyzing a dataset of 774,051 methods from 49 open-source projects, we discovered that methods containing SATD are not only larger and more complex but also exhibit lower readability and a higher tendency for bugs and changes. We also found that SATD often remains unresolved for extended periods, adversely affecting code quality and maintainability. Our results provide empirical evidence highlighting the necessity of early identification, resource allocation, and proactive management of SATD to mitigate its long-term impacts on software quality and maintenance costs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.14473</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.14473</id><created>2024-11-18</created><updated>2025-02-04</updated><authors><author><keyname>Barros</keyname><forenames>Cauã Ferreira</forenames></author><author><keyname>Azevedo</keyname><forenames>Bruna Borges</forenames></author><author><keyname>Neto</keyname><forenames>Valdemar Vicente Graciano</forenames></author><author><keyname>Kassab</keyname><forenames>Mohamad</forenames></author><author><keyname>Kalinowski</keyname><forenames>Marcos</forenames></author><author><keyname>Nascimento</keyname><forenames>Hugo Alexandre D. do</forenames></author><author><keyname>Bandeira</keyname><forenames>Michelle C. G. S. P.</forenames></author></authors><title>Large Language Model for Qualitative Research -- A Systematic Mapping   Study</title><categories>cs.CL cs.AI</categories><comments>8 pages, includes 1 figures and 3 tables. Submitted and Accepted to   the WSESE 2025 ICSE Workshop</comments><acm-class>I.2.7; I.2.10; H.3.3</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The exponential growth of text-based data in domains such as healthcare, education, and social sciences has outpaced the capacity of traditional qualitative analysis methods, which are time-intensive and prone to subjectivity. Large Language Models (LLMs), powered by advanced generative AI, have emerged as transformative tools capable of automating and enhancing qualitative analysis. This study systematically maps the literature on the use of LLMs for qualitative research, exploring their application contexts, configurations, methodologies, and evaluation metrics. Findings reveal that LLMs are utilized across diverse fields, demonstrating the potential to automate processes traditionally requiring extensive human input. However, challenges such as reliance on prompt engineering, occasional inaccuracies, and contextual limitations remain significant barriers. This research highlights opportunities for integrating LLMs with human expertise, improving model robustness, and refining evaluation methodologies. By synthesizing trends and identifying research gaps, this study aims to guide future innovations in the application of LLMs for qualitative analysis. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.14727</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.14727</id><created>2024-11-21</created><updated>2025-02-04</updated><authors><author><keyname>Chen</keyname><forenames>Junyang</forenames></author><author><keyname>Lu</keyname><forenames>Yang</forenames></author><author><keyname>Li</keyname><forenames>Mengke</forenames></author><author><keyname>Yang</keyname><forenames>Cuie</forenames></author><author><keyname>Zhang</keyname><forenames>Yiqun</forenames></author><author><keyname>Cheung</keyname><forenames>Yiu-ming</forenames></author></authors><title>HyReaL: Clustering Attributed Graph via Hyper-Complex Space   Representation Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering complex data in the form of attributed graphs has attracted increasing attention, where powerful graph representation is a critical prerequisite. However, the well-known Over-Smoothing (OS) effect makes Graph Convolutional Networks tend to homogenize the representation of graph nodes, while the existing OS solutions focus on alleviating the homogeneity of nodes' embeddings from the aspect of graph topology information, which is inconsistent with the attributed graph clustering objective. Therefore, we introduce hyper-complex space with powerful quaternion feature transformation to enhance the representation learning of the attributes. A generalized \textbf{Hy}per-complex space \textbf{Re}present\textbf{a}tion \textbf{L}earning (\textbf{HyReaL}) model is designed to: 1) bridge arbitrary dimensional attributes to the well-developed quaternion algebra with four parts, and 2) connect the learned representations to more generalized clustering objective without being restricted to a given number of clusters $k$. The novel introduction of quaternion benefits attributed graph clustering from two aspects: 1) enhanced attribute coupling learning capability allows complex attribute information to be sufficiently exploited in clustering, and 2) stronger learning capability makes it unnecessary to stack too many graph convolution layers, naturally alleviating the OS problem. It turns out that the node representations learned by HyReaL are more discriminative and widely suit downstream clustering with different $k$s. Extensive experiments including significance tests, ablation studies, qualitative results, etc., show the superiority of HyReaL. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.15204</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.15204</id><created>2024-11-20</created><updated>2025-02-04</updated><authors><author><keyname>Jang</keyname><forenames>Minguk</forenames></author><author><keyname>Chung</keyname><forenames>Hye Won</forenames></author></authors><title>Label Distribution Shift-Aware Prediction Refinement for Test-Time   Adaptation</title><categories>cs.LG cs.AI cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Test-time adaptation (TTA) is an effective approach to mitigate performance degradation of trained models when encountering input distribution shifts at test time. However, existing TTA methods often suffer significant performance drops when facing additional class distribution shifts. We first analyze TTA methods under label distribution shifts and identify the presence of class-wise confusion patterns commonly observed across different covariate shifts. Based on this observation, we introduce label Distribution shift-Aware prediction Refinement for Test-time adaptation (DART), a novel TTA method that refines the predictions by focusing on class-wise confusion patterns. DART trains a prediction refinement module during an intermediate time by exposing it to several batches with diverse class distributions using the training dataset. This module is then used during test time to detect and correct class distribution shifts, significantly improving pseudo-label accuracy for test data. Our method exhibits 5-18% gains in accuracy under label distribution shifts on CIFAR-10C, without any performance degradation when there is no label distribution shift. Extensive experiments on CIFAR, PACS, OfficeHome, and ImageNet benchmarks demonstrate DART's ability to correct inaccurate predictions caused by test-time distribution shifts. This improvement leads to enhanced performance in existing TTA methods, making DART a valuable plug-in tool. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.15363</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.15363</id><created>2024-11-22</created><updated>2025-02-05</updated><authors><author><keyname>Streit</keyname><forenames>Robert</forenames></author><author><keyname>Garg</keyname><forenames>Vijay K.</forenames></author></authors><title>The Polymatroid Representation of a Greedoid, and Associated Galois   Connections</title><categories>math.CO cs.DM cs.DS</categories><comments>26 pages, 4 figures. In versions 1 and 2 there is an error in the   proof of the main claim. This has been noted on the first page, and the   authors are currently revising the manuscript to resolve this issue</comments><acm-class>G.2.1</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The greedoid is a significant abstraction of the matroid allowing for a more flexible analysis of structures in which the greedy algorithm "works." However, their diverse structure imposes difficulties towards their application in combinatorial optimization [Sze21]. In response, we revisit the polymatroid greedoid [KL85a] to characterize it by properties approximating those of matroids, by using the submodularity of its polymatroid representation in particular. Towards doing so, our main contribution is a full description of this class. Specifically, we show that a greedoid is a polymatroid greedoid if and only if it is an optimistic interval greedoid whose kernels are closed under intersection. This constitutes the first necessary and sufficient characterization of the polymatroid greedoid in terms of its combinatorial attributes, thereby resolving a central open question of Korte and Lov\'asz [KL85a]. Here, we introduce the optimism property to approximate properties of a matroid's continuations which are implied by the closure axioms of its span, which no longer hold for greedoids. And, because the kernels of an interval greedoid are in many ways an extension of a matroid's closed sets, our direction of necessity is a direct generalization of Birkhoff and Edmond's characterization of the meet in the lattice of a matroid's closed sets [Bir35, Edm03]. Towards achieving this result, our main technical insights arise from relating the lattice of flats of a polymatroid greedoid to that of the closed sets of its representation through order preserving mappings. Specifically, we will show the novel insight that the notion of polymatroid representation considered in [KL85a] is equivalent to the existence of a certain Galois connection. As a consequence, the representation of a greedoid via a polymatroid is an order theoretic concept in disguise. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.15486</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.15486</id><created>2024-11-23</created><updated>2025-02-05</updated><authors><author><keyname>Saqr</keyname><forenames>Mohammed</forenames></author><author><keyname>López-Pernas</keyname><forenames>Sonsoles</forenames></author><author><keyname>Törmänen</keyname><forenames>Tiina</forenames></author><author><keyname>Kaliisa</keyname><forenames>Rogers</forenames></author><author><keyname>Misiejuk</keyname><forenames>Kamila</forenames></author><author><keyname>Tikka</keyname><forenames>Santtu</forenames></author></authors><title>Transition Network Analysis: A Novel Framework for Modeling,   Visualizing, and Identifying the Temporal Patterns of Learners and Learning   Processes</title><categories>cs.SI cs.CL cs.CY</categories><comments>Accepted at Learning Analytics &amp; Knowledge (LAK '25)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents a novel learning analytics method: Transition Network Analysis (TNA), a method that integrates Stochastic Process Mining and probabilistic graph representation to model, visualize, and identify transition patterns in the learning process data. Combining the relational and temporal aspects into a single lens offers capabilities beyond either framework, including centralities to capture important learning events, community detection to identify behavior patterns, and clustering to reveal temporal patterns. Furthermore, TNA introduces several significance tests that go beyond either method and add rigor to the analysis. Here, we introduce the theoretical and mathematical foundations of TNA and we demonstrate the functionalities of TNA with a case study where students (n=191) engaged in small-group collaboration to map patterns of group dynamics using the theories of co-regulation and socially-shared regulated learning. The analysis revealed that TNA can map the regulatory processes as well as identify important events, patterns, and clusters. Bootstrap validation established the significant transitions and eliminated spurious transitions. As such, TNA can capture learning dynamics and provide a robust framework for investigating the temporal evolution of learning processes. Future directions include -- inter alia -- expanding estimation methods, reliability assessment, and building longitudinal TNA. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.16303</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.16303</id><created>2024-11-25</created><updated>2025-02-05</updated><authors><author><keyname>Zeng</keyname><forenames>Dun</forenames></author><author><keyname>Wu</keyname><forenames>Zheshun</forenames></author><author><keyname>Liu</keyname><forenames>Shiyu</forenames></author><author><keyname>Pan</keyname><forenames>Yu</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoying</forenames></author><author><keyname>Xu</keyname><forenames>Zenglin</forenames></author></authors><title>Understanding Generalization of Federated Learning: the Trade-off   between Model Stability and Optimization</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Federated Learning (FL) is a distributed learning approach that trains machine learning models across multiple devices while keeping their local data private. However, FL often faces challenges due to data heterogeneity, leading to inconsistent local optima among clients. These inconsistencies can cause unfavorable convergence behavior and generalization performance degradation. Existing studies mainly describe this issue through \textit{convergence analysis}, focusing on how well a model fits training data, or through \textit{algorithmic stability}, which examines the generalization gap. However, neither approach precisely captures the generalization performance of FL algorithms, especially for neural networks. This paper introduces an innovative generalization dynamics analysis framework, named as Libra, for algorithm-dependent excess risk minimization, highlighting the trade-offs between model stability and optimization. Through this framework, we show how the generalization of FL algorithms is affected by the interplay of algorithmic stability and optimization. This framework applies to standard federated optimization and its advanced variants, such as server momentum. Our findings suggest that larger local steps or momentum accelerate convergence but enlarge stability, while yielding a better minimum excess risk. These insights can guide the design of future algorithms to achieve stronger generalization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.16598</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.16598</id><created>2024-11-25</created><updated>2025-02-04</updated><authors><author><keyname>Kassis</keyname><forenames>Andre</forenames></author><author><keyname>Hengartner</keyname><forenames>Urs</forenames></author><author><keyname>Yu</keyname><forenames>Yaoliang</forenames></author></authors><title>DiffBreak: Breaking Diffusion-Based Purification with Adaptive Attacks</title><categories>cs.CR cs.CV cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Diffusion-based purification (DBP) has emerged as a cornerstone defense against adversarial examples (AEs), widely regarded as robust due to its use of diffusion models (DMs) that project AEs onto the natural data distribution. However, contrary to prior assumptions, we theoretically prove that adaptive gradient-based attacks nullify this foundational claim, effectively targeting the DM rather than the classifier and causing purified outputs to align with adversarial distributions. This surprising discovery prompts a reassessment of DBP's robustness, revealing it stems from critical flaws in backpropagation techniques used so far for attacking DBP. To address these gaps, we introduce DiffBreak, a novel and reliable gradient library for DBP, which exposes how adaptive attacks drastically degrade its robustness. In stricter majority-vote settings, where classifier decisions aggregate predictions over multiple purified inputs, DBP retains partial robustness to traditional norm-bounded AEs due to its stochasticity disrupting adversarial alignment. However, we propose a novel adaptation of a recent optimization method against deepfake watermarking, crafting systemic adversarial perturbations that defeat DBP even under these conditions, ultimately challenging its viability as a defense without improvements. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.17508</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.17508</id><created>2024-11-26</created><authors><author><keyname>Dikici</keyname><forenames>Onur</forenames></author><author><keyname>Ghignone</keyname><forenames>Edoardo</forenames></author><author><keyname>Hu</keyname><forenames>Cheng</forenames></author><author><keyname>Baumann</keyname><forenames>Nicolas</forenames></author><author><keyname>Xie</keyname><forenames>Lei</forenames></author><author><keyname>Carron</keyname><forenames>Andrea</forenames></author><author><keyname>Magno</keyname><forenames>Michele</forenames></author><author><keyname>Corno</keyname><forenames>Matteo</forenames></author></authors><title>Learning-Based On-Track System Identification for Scaled Autonomous   Racing in Under a Minute</title><categories>cs.RO</categories><journal-ref>IEEE Robotics and Automation Letters ( Volume: 10, Issue: 2,   February 2025)</journal-ref><doi>10.1109/LRA.2025.3527336</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Accurate tire modeling is crucial for optimizing autonomous racing vehicles, as state-of-the-art (SotA) model-based techniques rely on precise knowledge of the vehicle's parameters. Yet, system identification in dynamic racing conditions is challenging due to varying track and tire conditions. Traditional methods require extensive operational ranges, often impractical in racing scenarios. Machine learning (ML)-based methods, while improving performance, struggle with generalization and depend on accurate initialization. This paper introduces a novel on-track system identification algorithm, incorporating a neural network (NN) for error correction, which is then employed for traditional system identification with virtually generated data. Crucially, the process is iteratively reapplied, with tire parameters updated at each cycle, leading to notable improvements in accuracy in tests on a scaled vehicle. Experiments show that it is possible to learn a tire model without prior knowledge with only 30 seconds of driving data and 3 seconds of training time. This method demonstrates greater one-step prediction accuracy than the baseline nonlinear least squares (NLS) method under noisy conditions, achieving a 3.3x lower root mean square error (RMSE), and yields tire models with comparable accuracy to traditional steady-state system identification. Furthermore, unlike steady-state methods requiring large spaces and specific experimental setups, the proposed approach identifies tire parameters directly on a race track in dynamic racing environments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.17547</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.17547</id><created>2024-11-26</created><updated>2025-02-05</updated><authors><author><keyname>Calsi</keyname><forenames>Davide Li</forenames></author><author><keyname>Chaudhary</keyname><forenames>Sumit</forenames></author><author><keyname>Choi</keyname><forenames>JinHyeock</forenames></author><author><keyname>Geitz</keyname><forenames>Marc</forenames></author><author><keyname>Nötzel</keyname><forenames>Janis</forenames></author></authors><title>End-to-end QKD network with non-localized trust</title><categories>quant-ph cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum Key Distribution (QKD) systems are infamously known for their high demand on hardware, their extremely low key generation rates and their lack of security resulting from a need for trusted nodes which is implied by the absence of quantum repeaters. While they theoretically offer unlimited security, they are therefore practically limited in several regards. In this work we focus on the lack of options to guarantee an end-to-end security service with the currently available technology and infrastructure and propose a novel protocol. We find that one of the stumbling stones on the path towards an end-to-end security service guaranteed by quantum key distribution may be removed by using this protocol. Our proposal combines several parallel instances of twinfield QKD followed by classical postprocessing and communication to allow Alice and Bob to share a secret key. This hybrid approach improves the key rate and range w.r.t. to previous QKD approaches at a contained cost in security. We show that a coalition of intermediary nodes between Alice and Bob is needed to break the new scheme, sharply outperforming the trusted node approach in terms of security. Furthermore, the protocols do not require complex quantum measurements on Alice and Bob's sides, thus being truly end-to-end. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.17928</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.17928</id><created>2024-11-26</created><updated>2025-02-04</updated><authors><author><keyname>Hu</keyname><forenames>Xiangcheng</forenames></author><author><keyname>Wu</keyname><forenames>Jin</forenames></author><author><keyname>Jia</keyname><forenames>Mingkai</forenames></author><author><keyname>Yan</keyname><forenames>Hongyu</forenames></author><author><keyname>Jiang</keyname><forenames>Yi</forenames></author><author><keyname>Jiang</keyname><forenames>Binqian</forenames></author><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>He</keyname><forenames>Wei</forenames></author><author><keyname>Tan</keyname><forenames>Ping</forenames></author></authors><title>MapEval: Towards Unified, Robust and Efficient SLAM Map Evaluation   Framework</title><categories>cs.RO</categories><comments>8 pages, 7 figures, 7 tables</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Evaluating massive-scale point cloud maps in Simultaneous Localization and Mapping (SLAM) remains challenging, primarily due to the absence of unified, robust and efficient evaluation frameworks. We present MapEval, an open-source framework for comprehensive quality assessment of point cloud maps, specifically addressing SLAM scenarios where ground truth map is inherently sparse compared to the mapped environment. Through systematic analysis of existing evaluation metrics in SLAM applications, we identify their fundamental limitations and establish clear guidelines for consistent map quality assessment. Building upon these insights, we propose a novel Gaussian-approximated Wasserstein distance in voxelized space, enabling two complementary metrics under the same error standard: Voxelized Average Wasserstein Distance (AWD) for global geometric accuracy and Spatial Consistency Score (SCS) for local consistency evaluation. This theoretical foundation leads to significant improvements in both robustness against noise and computational efficiency compared to conventional metrics. Extensive experiments on both simulated and real-world datasets demonstrate that MapEval achieves at least \SI{100}{}-\SI{500}{} times faster while maintaining evaluation integrity. The MapEval library\footnote{\texttt{https://github.com/JokerJohn/Cloud\_Map\_Evaluation}} will be publicly available to promote standardized map evaluation practices in the robotics community. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.19016</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.19016</id><created>2024-11-28</created><updated>2025-02-05</updated><authors><author><keyname>Mokadem</keyname><forenames>Riad</forenames><affiliation>IRIT-PYRAMIDE, IRIT</affiliation></author></authors><title>A Data Source Discovery Method using Several Domain Ontologies in P2P   Environments (2014, IRIT research report)</title><categories>cs.DB</categories><comments>IRIT report research (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several data source discovery methods take into account the semantic heterogeneity problems by using several Domain Ontologies (DOs). However, most of them impose a topology of mapping links between DOs. DOs and mapping links are available on Internet but with an arbitrary topology. In this paper, we propose a data source Discovery method Adapted to any Mapping links Topology (DAMT) and taking into account semantic problems. Peers using the same DO are grouped in a Virtual Organization (VO) and connected in a Distributed Hash Table (DHT). Lookups within a same VO consists in a classical search in a DHT. Regarding the inter-VO discovery process, we propose an addressing system, based on the existing mapping links between DOs, to interconnect VOs. Furthermore, we adopt a lazy maintenance in order to reduce the number of messages required to update the system due to the dynamicity of peers. The performance analysis of the proposed method shows good results for inter-VO lookup queries. Also, it confirms a significant maintenance cost reduction when peers join and leave the system. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.00029</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.00029</id><created>2024-11-19</created><updated>2025-02-05</updated><authors><author><keyname>Redkar</keyname><forenames>Neel</forenames></author></authors><title>Planning vs Reasoning: Ablations to Test Capabilities of LoRA layers</title><categories>cs.AI</categories><comments>7 pages, 5 figures, preprint</comments><msc-class>I.2.7, I.2.6</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Low-Rank Adaptation (LoRA) layers have emerged as a promising approach for efficient model fine-tuning, but their capabilities and limitations have not been fully explored. This paper: 1) Investigates the fundamental question of whether LoRA layers are effective at increasing reasoning + planning abilities 2) We introduce HashChain Reasoning, a novel evaluation dataset that deterministically tests reasoning capabilities.   Through systematic ablation studies on GPT-2, we demonstrate that reasoning capabilities appear to exist primarily in low-rank spaces and can be effectively enhanced using LoRA layers. The effective rank analysis of trained LoRA matrices reveals a 2-3x lower rank requirement for reasoning tasks compared to planning tasks, giving context on where LoRA layers would be effective. This also provides evidence for reasoning fundamentally preferring low-parameter spaces for generalization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.00087</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.00087</id><created>2024-11-27</created><updated>2025-02-05</updated><authors><author><keyname>Wang</keyname><forenames>Cong</forenames></author><author><keyname>Yang</keyname><forenames>Weizhe</forenames></author><author><keyname>Wang</keyname><forenames>Haiping</forenames></author><author><keyname>Yang</keyname><forenames>Renjie</forenames></author><author><keyname>Li</keyname><forenames>Jing</forenames></author><author><keyname>Wang</keyname><forenames>Zhijun</forenames></author><author><keyname>Yu</keyname><forenames>Xinyao</forenames></author><author><keyname>Wei</keyname><forenames>Yixiong</forenames></author><author><keyname>Huang</keyname><forenames>Xianli</forenames></author><author><keyname>Hu</keyname><forenames>Chenshu</forenames></author><author><keyname>Liu</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Zou</keyname><forenames>Changqing</forenames></author><author><keyname>Zhao</keyname><forenames>Zhifeng</forenames></author></authors><title>Physics-Informed Deep Learning Model for Line-integral Diagnostics   Across Fusion Devices</title><categories>cs.LG cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rapid reconstruction of 2D plasma profiles from line-integral measurements is important in nuclear fusion. This paper introduces a physics-informed model architecture called Onion, that can enhance the performance of models and be adapted to various backbone networks. The model under Onion incorporates physical information by a multiplication process and applies the physics-informed loss function according to the principle of line integration. Experimental results demonstrate that the additional input of physical information improves the model's ability, leading to a reduction in the average relative error E_1 between the reconstruction profiles and the target profiles by approximately 52% on synthetic datasets and about 15% on experimental datasets. Furthermore, the implementation of the Softplus activation function in the final two fully connected layers improves model performance. This enhancement results in a reduction in the E_1 by approximately 71% on synthetic datasets and about 27% on experimental datasets. The incorporation of the physics-informed loss function has been shown to correct the model's predictions, bringing the back-projections closer to the actual inputs and reducing the errors associated with inversion algorithms. Besides, we have developed a synthetic data model to generate customized line-integral diagnostic datasets and have also collected soft x-ray diagnostic datasets from EAST and HL-2A. This study achieves reductions in reconstruction errors, and accelerates the development of diagnostic surrogate models in fusion research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.00416</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.00416</id><created>2024-11-30</created><updated>2025-02-05</updated><authors><author><keyname>Huang</keyname><forenames>Shaofei</forenames></author><author><keyname>Poskitt</keyname><forenames>Christopher M.</forenames></author><author><keyname>Shar</keyname><forenames>Lwin Khin</forenames></author></authors><title>ACTISM: Threat-informed Dynamic Security Modelling for Automotive   Systems</title><categories>cs.CR</categories><comments>Preprint under submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolving cybersecurity threats in complex cyber-physical systems pose significant risks to system functionality and safety. This experience report introduces ACTISM (Automotive Consequence-Driven and Threat-Informed Security Modelling), an integrated security modelling framework that enhances the resilience of automotive systems by dynamically updating their cybersecurity posture in response to prevailing and evolving threats, attacker tactics, and their impact on system functionality and safety. ACTISM addresses the existing knowledge gap in static security assessment methodologies by providing a dynamic and iterative framework. We demonstrate the effectiveness of ACTISM by applying it to a real-world example of the Tesla Electric Vehicle's In-Vehicle Infotainment system, illustrating how the security model can be adapted as new threats emerge. We also report the results of a practitioners' survey on the usefulness of ACTISM and its future directions. The survey highlights avenues for future research and development in this area, including automated vulnerability management workflows for automotive systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.02403</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.02403</id><created>2024-12-03</created><updated>2025-02-05</updated><authors><author><keyname>Braeutigam</keyname><forenames>Valentin</forenames></author><author><keyname>Wirth</keyname><forenames>Vanessa</forenames></author><author><keyname>Ullmann</keyname><forenames>Ingrid</forenames></author><author><keyname>Schüßler</keyname><forenames>Christian</forenames></author><author><keyname>Vossiek</keyname><forenames>Martin</forenames></author><author><keyname>Berking</keyname><forenames>Matthias</forenames></author><author><keyname>Egger</keyname><forenames>Bernhard</forenames></author></authors><title>3D Face Reconstruction From Radar Images</title><categories>cs.CV cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The 3D reconstruction of faces gains wide attention in computer vision and is used in many fields of application, for example, animation, virtual reality, and even forensics. This work is motivated by monitoring patients in sleep laboratories. Due to their unique characteristics, sensors from the radar domain have advantages compared to optical sensors, namely penetration of electrically non-conductive materials and independence of light. These advantages of radar signals unlock new applications and require adaptation of 3D reconstruction frameworks. We propose a novel model-based method for 3D reconstruction from radar images. We generate a dataset of synthetic radar images with a physics-based but non-differentiable radar renderer. This dataset is used to train a CNN-based encoder to estimate the parameters of a 3D morphable face model. Whilst the encoder alone already leads to strong reconstructions of synthetic data, we extend our reconstruction in an Analysis-by-Synthesis fashion to a model-based autoencoder. This is enabled by learning the rendering process in the decoder, which acts as an object-specific differentiable radar renderer. Subsequently, the combination of both network parts is trained to minimize both, the loss of the parameters and the loss of the resulting reconstructed radar image. This leads to the additional benefit, that at test time the parameters can be further optimized by finetuning the autoencoder unsupervised on the image loss. We evaluated our framework on generated synthetic face images as well as on real radar images with 3D ground truth of four individuals. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.02529</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.02529</id><created>2024-12-03</created><updated>2025-02-04</updated><authors><author><keyname>Wagenmaker</keyname><forenames>Andrew</forenames></author><author><keyname>Mi</keyname><forenames>Lu</forenames></author><author><keyname>Rozsa</keyname><forenames>Marton</forenames></author><author><keyname>Bull</keyname><forenames>Matthew S.</forenames></author><author><keyname>Svoboda</keyname><forenames>Karel</forenames></author><author><keyname>Daie</keyname><forenames>Kayvon</forenames></author><author><keyname>Golub</keyname><forenames>Matthew D.</forenames></author><author><keyname>Jamieson</keyname><forenames>Kevin</forenames></author></authors><title>Active learning of neural population dynamics using two-photon   holographic optogenetics</title><categories>q-bio.NC cs.LG stat.ML</categories><comments>NeurIPS 2024</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in techniques for monitoring and perturbing neural populations have greatly enhanced our ability to study circuits in the brain. In particular, two-photon holographic optogenetics now enables precise photostimulation of experimenter-specified groups of individual neurons, while simultaneous two-photon calcium imaging enables the measurement of ongoing and induced activity across the neural population. Despite the enormous space of potential photostimulation patterns and the time-consuming nature of photostimulation experiments, very little algorithmic work has been done to determine the most effective photostimulation patterns for identifying the neural population dynamics. Here, we develop methods to efficiently select which neurons to stimulate such that the resulting neural responses will best inform a dynamical model of the neural population activity. Using neural population responses to photostimulation in mouse motor cortex, we demonstrate the efficacy of a low-rank linear dynamical systems model, and develop an active learning procedure which takes advantage of low-rank structure to determine informative photostimulation patterns. We demonstrate our approach on both real and synthetic data, obtaining in some cases as much as a two-fold reduction in the amount of data required to reach a given predictive power. Our active stimulation design method is based on a novel active learning procedure for low-rank regression, which may be of independent interest. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.05055</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.05055</id><created>2024-12-06</created><updated>2025-02-05</updated><authors><author><keyname>Stenger</keyname><forenames>Brad</forenames></author><author><keyname>Feng</keyname><forenames>Yuanyuan</forenames></author></authors><title>Information Flows for Athletes' Health and Performance Data</title><categories>cs.HC</categories><acm-class>H.5; J.3; K.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increasing numbers of athletes and sports teams use data collection technologies to improve athletic development and athlete health with the goal of improving competitive performance. Personal data privacy is managed but it is not always a priority for the coaches who are in charge of athletes. There is a pressing need to investigate what are appropriate information flows as described by contextual integrity for these data technologies and these use cases. We propose two main types of information flows for athletes' health and performance data -- team-centric and athlete-centric -- designed to characterize data used for the collective and individual physical, psychological and social development of athletes. We also present a scenario for applying differential privacy to athletes' data and propose two new information flows -- research-centric and community-centric -- which envision larger-scale, more collaborative sharing of athletes' data in the future. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.06279</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.06279</id><created>2024-12-09</created><updated>2025-02-05</updated><authors><author><keyname>Li</keyname><forenames>Qian</forenames></author><author><keyname>Yang</keyname><forenames>Ziang</forenames></author><author><keyname>Li</keyname><forenames>Dou</forenames></author><author><keyname>Zhang</keyname><forenames>Hongliang</forenames></author></authors><title>Reconfigurable Holographic Surface-aided Distributed MIMO Radar Systems</title><categories>eess.SP cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed phased Multiple-Input Multiple-Output (phased-MIMO) radar systems have attracted wide attention in target detection and tracking. However, the phase-shifting circuits in phased subarrays contribute to high power consumption and hardware cost. To address this issue, an energy-efficient and cost-efficient metamaterial antenna array, i.e., reconfigurable holographic surface (RHS), has been developed. In this letter, we propose RHS-aided distributed MIMO radar systems to achieve more accurate multi-target detection under equivalent power consumption and hardware cost as that of distributed phased-MIMO radar systems. Different from phased arrays, the RHS achieves beam steering by regulating the radiation amplitude of its elements, and thus conventional beamforming schemes designed for phased arrays are no longer applicable. Aiming to maximize detection accuracy, we design an amplitude-controlled beamforming scheme for multiple RHS transceiver subarrays. The simulations validate the superiority of the proposed scheme over the distributed phased-MIMO radar scheme and reveal the optimal allocation of spatial diversity and coherent processing gain that leads to the best system performance when hardware resources are fixed. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.06540</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.06540</id><created>2024-12-09</created><updated>2025-02-04</updated><authors><author><keyname>Polo</keyname><forenames>Felipe Maia</forenames></author><author><keyname>Somerstep</keyname><forenames>Seamus</forenames></author><author><keyname>Choshen</keyname><forenames>Leshem</forenames></author><author><keyname>Sun</keyname><forenames>Yuekai</forenames></author><author><keyname>Yurochkin</keyname><forenames>Mikhail</forenames></author></authors><title>Sloth: scaling laws for LLM skills to predict multi-benchmark   performance across families</title><categories>cs.LG cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Scaling laws for large language models (LLMs) predict model performance based on parameters like size and training data. However, differences in training configurations and data processing across model families lead to significant variations in benchmark performance, making it difficult for a single scaling law to generalize across all LLMs. On the other hand, training family-specific scaling laws requires training models of varying sizes for every family. In this work, we propose Skills Scaling Laws (SSLaws, pronounced as Sloth), a novel scaling law that leverages publicly available benchmark data and assumes LLM performance is driven by low-dimensional latent skills, such as reasoning and instruction following. These latent skills are influenced by computational resources like model size and training tokens but with varying efficiencies across model families. Sloth exploits correlations across benchmarks to provide more accurate and interpretable predictions while alleviating the need to train multiple LLMs per family. We present both theoretical results on parameter identification and empirical evaluations on 12 prominent benchmarks, from Open LLM Leaderboard v1/v2, demonstrating that Sloth predicts LLM performance efficiently and offers insights into scaling behaviors for complex downstream tasks and increased test-time compute. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.08933</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.08933</id><created>2024-12-11</created><updated>2025-02-05</updated><authors><author><keyname>Lim</keyname><forenames>Kart-Leong</forenames></author></authors><title>Deep clustering using adversarial net based clustering loss</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep clustering is a recent deep learning technique which combines deep learning with traditional unsupervised clustering. At the heart of deep clustering is a loss function which penalizes samples for being an outlier from their ground truth cluster centers in the latent space. The probabilistic variant of deep clustering reformulates the loss using KL divergence. Often, the main constraint of deep clustering is the necessity of a closed form loss function to make backpropagation tractable. Inspired by deep clustering and adversarial net, we reformulate deep clustering as an adversarial net over traditional closed form KL divergence. Training deep clustering becomes a task of minimizing the encoder and maximizing the discriminator. At optimality, this method theoretically approaches the JS divergence between the distribution assumption of the encoder and the discriminator. We demonstrated the performance of our proposed method on several well cited datasets such as MNIST, REUTERS10K and CIFAR10, achieving on-par or better performance with some of the state-of-the-art deep clustering methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.08948</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.08948</id><created>2024-12-12</created><updated>2025-02-05</updated><authors><author><keyname>He</keyname><forenames>Xuehai</forenames></author><author><keyname>Wang</keyname><forenames>Shuohang</forenames></author><author><keyname>Yang</keyname><forenames>Jianwei</forenames></author><author><keyname>Wu</keyname><forenames>Xiaoxia</forenames></author><author><keyname>Wang</keyname><forenames>Yiping</forenames></author><author><keyname>Wang</keyname><forenames>Kuan</forenames></author><author><keyname>Zhan</keyname><forenames>Zheng</forenames></author><author><keyname>Ruwase</keyname><forenames>Olatunji</forenames></author><author><keyname>Shen</keyname><forenames>Yelong</forenames></author><author><keyname>Wang</keyname><forenames>Xin Eric</forenames></author></authors><title>Mojito: Motion Trajectory and Intensity Control for Video Generation</title><categories>cs.CV cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent advancements in diffusion models have shown great promise in producing high-quality video content. However, efficiently training video diffusion models capable of integrating directional guidance and controllable motion intensity remains a challenging and under-explored area. To tackle these challenges, this paper introduces Mojito, a diffusion model that incorporates both motion trajectory and intensity control for text-to-video generation. Specifically, Mojito features a Directional Motion Control (DMC) module that leverages cross-attention to efficiently direct the generated object's motion without training, alongside a Motion Intensity Modulator (MIM) that uses optical flow maps generated from videos to guide varying levels of motion intensity. Extensive experiments demonstrate Mojito's effectiveness in achieving precise trajectory and intensity control with high computational efficiency, generating motion patterns that closely match specified directions and intensities, providing realistic dynamics that align well with natural motion in real-world scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.09584</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.09584</id><created>2024-12-12</created><updated>2025-02-05</updated><authors><author><keyname>Shen</keyname><forenames>Keyi</forenames></author><author><keyname>Yu</keyname><forenames>Jiangwei</forenames></author><author><keyname>Barreiros</keyname><forenames>Jose</forenames></author><author><keyname>Zhang</keyname><forenames>Huan</forenames></author><author><keyname>Li</keyname><forenames>Yunzhu</forenames></author></authors><title>BaB-ND: Long-Horizon Motion Planning with Branch-and-Bound and Neural   Dynamics</title><categories>cs.RO</categories><comments>The first two authors contributed equally. Project Page:   https://robopil.github.io/bab-nd/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural-network-based dynamics models learned from observational data have shown strong predictive capabilities for scene dynamics in robotic manipulation tasks. However, their inherent non-linearity presents significant challenges for effective planning. Current planning methods, often dependent on extensive sampling or local gradient descent, struggle with long-horizon motion planning tasks involving complex contact events. In this paper, we present a GPU-accelerated branch-and-bound (BaB) framework for motion planning in manipulation tasks that require trajectory optimization over neural dynamics models. Our approach employs a specialized branching heuristics to divide the search space into subdomains, and applies a modified bound propagation method, inspired by the state-of-the-art neural network verifier alpha-beta-CROWN, to efficiently estimate objective bounds within these subdomains. The branching process guides planning effectively, while the bounding process strategically reduces the search space. Our framework achieves superior planning performance, generating high-quality state-action trajectories and surpassing existing methods in challenging, contact-rich manipulation tasks such as non-prehensile planar pushing with obstacles, object sorting, and rope routing in both simulated and real-world settings. Furthermore, our framework supports various neural network architectures, ranging from simple multilayer perceptrons to advanced graph neural dynamics models, and scales efficiently with different model sizes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.10146</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.10146</id><created>2024-12-13</created><updated>2025-02-05</updated><authors><author><keyname>Gabdullin</keyname><forenames>Nikita</forenames></author></authors><title>Investigating generalization capabilities of neural networks by means of   loss landscapes and Hessian analysis</title><categories>cs.LG cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies generalization capabilities of neural networks (NNs) using new and improved PyTorch library Loss Landscape Analysis (LLA). LLA facilitates visualization and analysis of loss landscapes along with the properties of NN Hessian. Different approaches to NN loss landscape plotting are discussed with particular focus on normalization techniques showing that conventional methods cannot always ensure correct visualization when batch normalization layers are present in NN architecture. The use of Hessian axes is shown to be able to mitigate this effect, and methods for choosing Hessian axes are proposed. In addition, spectra of Hessian eigendecomposition are studied and it is shown that typical spectra exist for a wide range of NNs. This allows to propose quantitative criteria for Hessian analysis that can be applied to evaluate NN performance and assess its generalization capabilities. Generalization experiments are conducted using ImageNet-1K pre-trained models along with several models trained as part of this study. The experiment include training models on one dataset and testing on another one to maximize experiment similarity to model performance in the Wild. It is shown that when datasets change, the changes in criteria correlate with the changes in accuracy, making the proposed criteria a computationally efficient estimate of generalization ability, which is especially useful for extremely large datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.11667</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.11667</id><created>2024-12-16</created><authors><author><keyname>Di Santo</keyname><forenames>Alessio</forenames></author><author><keyname>Tiberti</keyname><forenames>Walter</forenames></author><author><keyname>Cassioli</keyname><forenames>Dajana</forenames></author></authors><title>Security and Fairness in Multi-Party Quantum Secret Sharing Protocol</title><categories>quant-ph cs.NI</categories><comments>pre-print, 24 pages, 4 figures</comments><report-no>2689-1808</report-no><journal-ref>IEEE Transaction on Quantum Engineering (2025)</journal-ref><doi>10.1109/TQE.2025.3535823</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Quantum secret sharing (QSS) is a cryptographic protocol that leverages quantum mechanics to distribute a secret among multiple parties. With respect to the classical counterpart, in QSS the secret is encoded into quantum states and shared by a dealer such that only an authorized subsets of participants, i.e., the players, can reconstruct it. Several state-of-the-art studies aim to transpose classical Secret Sharing into the quantum realm, while maintaining their reliance on traditional network topologies (e.g., star, ring, fully-connected) and require that all the n players calculate the secret. These studies exploit the Greenberger-Horne-Zeilinger (GHZ) state, which is a type of maximally entangled quantum state involving three or more qubits. However, none of these works account for redundancy, enhanced security/privacy features or authentication mechanisms able to fingerprint players. To address these gaps, in this paper we introduce a new concept of QSS which leans on a generic distributed quantum-network, based on a threshold scheme, where all the players collaborate also to the routing of quantum information among them. The dealer, by exploiting a custom flexible weighting system, takes advantage of a newly defined quantum Dijkstra algorithm to select the most suitable subset of t players, out of the entire set on n players, to involve in the computation. To fingerprint and authenticate users, CRYSTAL-Kyber primitives are adopted, while also protecting each player's privacy by hiding their identities. We show the effectiveness and performance of the proposed protocol by testing it against the main classical and quantum attacks, thereby improving the state-of-the-art security measures. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.12113</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.12113</id><created>2024-12-01</created><updated>2025-02-05</updated><authors><author><keyname>Arora</keyname><forenames>Sarthak</forenames></author><author><keyname>Warner</keyname><forenames>Michael</forenames></author><author><keyname>Chamberlain</keyname><forenames>Ariel</forenames></author><author><keyname>Smoot</keyname><forenames>James C.</forenames></author><author><keyname>Deep</keyname><forenames>Nikhil Raj</forenames></author><author><keyname>Gorman</keyname><forenames>Claire</forenames></author><author><keyname>Acciavatti</keyname><forenames>Anthony</forenames></author></authors><title>Application of Analytical Hierarchical Process and its Variants on   Remote Sensing Datasets</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The river Ganga is one of the Earth's most critically important river basins, yet it faces significant pollution challenges, making it crucial to evaluate its vulnerability for effective and targeted remediation efforts. While the Analytic Hierarchy Process (AHP) is widely regarded as the standard in decision making methodologies, uncertainties arise from its dependence on expert judgments, which can introduce subjectivity, especially when applied to remote sensing data, where expert knowledge might not fully capture spatial and spectral complexities inherent in such data. To address that, in this paper, we applied AHP alongside a suite of alternative existing and novel variants of AHP-based decision analysis on remote sensing data to assess the vulnerability of the river Ganga to pollution. We then compared the areas where the outputs of each variant may provide additional insights over AHP. Lastly, we utilized our learnings to design a composite variable to robustly define the vulnerability of the river Ganga to pollution. This approach contributes to a more comprehensive understanding of remote sensing data applications in environmental assessment, and these decision making variants can also have broader applications in other areas of environment management and sustainability, facilitating more precise and adaptable decision support frameworks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.12300</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.12300</id><created>2024-12-16</created><updated>2025-02-05</updated><authors><author><keyname>Peng</keyname><forenames>Xiangyu</forenames></author><author><keyname>Choubey</keyname><forenames>Prafulla Kumar</forenames></author><author><keyname>Xiong</keyname><forenames>Caiming</forenames></author><author><keyname>Wu</keyname><forenames>Chien-Sheng</forenames></author></authors><title>Unanswerability Evaluation for Retrieval Augmented Generation</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing evaluation frameworks for retrieval-augmented generation (RAG) systems focus on answerable queries, but they overlook the importance of appropriately rejecting unanswerable requests. In this paper, we introduce UAEval4RAG, a framework designed to evaluate whether RAG systems can handle unanswerable queries effectively. We define a taxonomy with six unanswerable categories, and UAEval4RAG automatically synthesizes diverse and challenging queries for any given knowledge base with unanswered ratio and acceptable ratio metrics. We conduct experiments with various RAG components, including retrieval models, rewriting methods, rerankers, language models, and prompting strategies, and reveal hidden trade-offs in performance of RAG systems. Our findings highlight the critical role of component selection and prompt design in optimizing RAG systems to balance the accuracy of answerable queries with high rejection rates of unanswerable ones. UAEval4RAG provides valuable insights and tools for developing more robust and reliable RAG systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.12505</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.12505</id><created>2024-12-16</created><authors><author><keyname>Chai</keyname><forenames>Mingxu</forenames></author><author><keyname>Shen</keyname><forenames>Ziyu</forenames></author><author><keyname>Zhang</keyname><forenames>Chong</forenames></author><author><keyname>Zhang</keyname><forenames>Yue</forenames></author><author><keyname>Wang</keyname><forenames>Xiao</forenames></author><author><keyname>Dou</keyname><forenames>Shihan</forenames></author><author><keyname>Kang</keyname><forenames>Jihua</forenames></author><author><keyname>Zhang</keyname><forenames>Jiazheng</forenames></author><author><keyname>Zhang</keyname><forenames>Qi</forenames></author></authors><title>DocFusion: A Unified Framework for Document Parsing Tasks</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Document parsing is essential for analyzing complex document structures and extracting fine-grained information, supporting numerous downstream applications. However, existing methods often require integrating multiple independent models to handle various parsing tasks, leading to high complexity and maintenance overhead. To address this, we propose DocFusion, a lightweight generative model with only 0.28B parameters. It unifies task representations and achieves collaborative training through an improved objective function. Experiments reveal and leverage the mutually beneficial interaction among recognition tasks, and integrating recognition data significantly enhances detection performance. The final results demonstrate that DocFusion achieves state-of-the-art (SOTA) performance across four key tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.13693</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.13693</id><created>2024-12-18</created><updated>2025-02-05</updated><authors><author><keyname>Gong</keyname><forenames>Lina</forenames></author><author><keyname>Wang</keyname><forenames>Chen</forenames></author><author><keyname>Huang</keyname><forenames>Yujun</forenames></author><author><keyname>Cui</keyname><forenames>Di</forenames></author><author><keyname>Wei</keyname><forenames>Mingqiang</forenames></author></authors><title>UITrans: Seamless UI Translation from Android to HarmonyOS</title><categories>cs.SE</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Seamless user interface (i.e., UI) translation has emerged as a pivotal technique for modern mobile developers, addressing the challenge of developing separate UI applications for Android and HarmonyOS platforms due to fundamental differences in layout structures and development paradigms. In this paper, we present UITrans, the first automated UI translation tool designed for Android to HarmonyOS. UITrans leverages an LLM-driven multi-agent reflective collaboration framework to convert Android XML layouts into HarmonyOS ArkUI layouts. It not only maps component-level and page-level elements to ArkUI equivalents but also handles project-level challenges, including complex layouts and interaction logic. Our evaluation of six Android applications demonstrates that our UITrans achieves translation success rates of over 90.1%, 89.3%, and 89.2% at the component, page, and project levels, respectively. UITrans is available at https://github.com/OpenSELab/UITrans and the demo video can be viewed at https://www.youtube.com/watch?v=iqKOSmCnJG0. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.15122</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.15122</id><created>2024-12-19</created><updated>2025-02-05</updated><authors><author><keyname>Liu</keyname><forenames>Gangli</forenames></author></authors><title>Solving the all pairs shortest path problem after minor update of a   large dense graph</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The all pairs shortest path problem is a fundamental optimization problem in graph theory. We deal with re-calculating the all-pairs shortest path (APSP) matrix after a minor modification of a weighted dense graph, e.g., adding a node, removing a node, or updating an edge. We assume the APSP matrix for the original graph is already known. The graph can be directed or undirected. A cold-start calculation of the new APSP matrix by traditional algorithms, like the Floyd-Warshall algorithm or Dijkstra's algorithm, needs $ O(n^3) $ time. We propose two algorithms for warm-start calculation of the new APSP matrix. The best case complexity for a warm-start calculation is $ O(n^2) $, the worst case complexity is $ O(n^3) $. We implemented the algorithms and tested their performance with experiments. The result shows a warm-start calculation can save a great portion of calculation time, compared with cold-start calculation. In addition, another algorithm is devised to warm-start calculate of the shortest path between two nodes. Experiment shows warm-start calculation can save 99\% of calculation time, compared with cold-start calculation by Dijkstra's algorithm, on directed complete graphs of large sizes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.15188</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.15188</id><created>2024-12-19</created><updated>2025-02-04</updated><authors><author><keyname>Shi</keyname><forenames>Weijia</forenames></author><author><keyname>Han</keyname><forenames>Xiaochuang</forenames></author><author><keyname>Zhou</keyname><forenames>Chunting</forenames></author><author><keyname>Liang</keyname><forenames>Weixin</forenames></author><author><keyname>Lin</keyname><forenames>Xi Victoria</forenames></author><author><keyname>Zettlemoyer</keyname><forenames>Luke</forenames></author><author><keyname>Yu</keyname><forenames>Lili</forenames></author></authors><title>LMFusion: Adapting Pretrained Language Models for Multimodal Generation</title><categories>cs.CL cs.AI cs.CV cs.LG</categories><comments>Name change: LlamaFusion to LMFusion</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We present LMFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LMFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LMFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LMFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.16502</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.16502</id><created>2024-12-21</created><updated>2025-02-05</updated><authors><author><keyname>Zhao</keyname><forenames>Shuyuan</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Shi</keyname><forenames>Boyan</forenames></author><author><keyname>Zhou</keyname><forenames>Liyong</forenames></author><author><keyname>Lin</keyname><forenames>Shuohao</forenames></author><author><keyname>Wan</keyname><forenames>Huaiyu</forenames></author></authors><title>Spatial-Temporal Knowledge Distillation for Takeaway Recommendation</title><categories>cs.LG cs.IR</categories><comments>Accepted by AAAI2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The takeaway recommendation system aims to recommend users' future takeaway purchases based on their historical purchase behaviors, thereby improving user satisfaction and boosting merchant sales. Existing methods focus on incorporating auxiliary information or leveraging knowledge graphs to alleviate the sparsity issue of user purchase sequences. However, two main challenges limit the performance of these approaches: (1) capturing dynamic user preferences on complex geospatial information and (2) efficiently integrating spatial-temporal knowledge from both graphs and sequence data with low computational costs. In this paper, we propose a novel spatial-temporal knowledge distillation model for takeaway recommendation (STKDRec) based on the two-stage training process. Specifically, during the first pre-training stage, a spatial-temporal knowledge graph (STKG) encoder is trained to extract high-order spatial-temporal dependencies and collaborative associations from the STKG. During the second spatial-temporal knowledge distillation (STKD) stage, a spatial-temporal Transformer (ST-Transformer) is employed to comprehensively model dynamic user preferences on various types of fine-grained geospatial information from a sequential perspective. Furthermore, the STKD strategy is introduced to transfer graph-based spatial-temporal knowledge to the ST-Transformer, facilitating the adaptive fusion of rich knowledge derived from both the STKG and sequence data while reducing computational overhead. Extensive experiments on three real-world datasets show that STKDRec significantly outperforms the state-of-the-art baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.17920</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.17920</id><created>2024-12-23</created><updated>2025-02-05</updated><authors><author><keyname>Lin</keyname><forenames>Haohong</forenames></author><author><keyname>Huang</keyname><forenames>Xin</forenames></author><author><keyname>Phan-Minh</keyname><forenames>Tung</forenames></author><author><keyname>Hayden</keyname><forenames>David S.</forenames></author><author><keyname>Zhang</keyname><forenames>Huan</forenames></author><author><keyname>Zhao</keyname><forenames>Ding</forenames></author><author><keyname>Srinivasa</keyname><forenames>Siddhartha</forenames></author><author><keyname>Wolff</keyname><forenames>Eric M.</forenames></author><author><keyname>Chen</keyname><forenames>Hongge</forenames></author></authors><title>Causal Composition Diffusion Model for Closed-loop Traffic Generation</title><categories>cs.AI cs.LG cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulation is critical for safety evaluation in autonomous driving, particularly in capturing complex interactive behaviors. However, generating realistic and controllable traffic scenarios in long-tail situations remains a significant challenge. Existing generative models suffer from the conflicting objective between user-defined controllability and realism constraints, which is amplified in safety-critical contexts. In this work, we introduce the Causal Compositional Diffusion Model (CCDiff), a structure-guided diffusion framework to address these challenges. We first formulate the learning of controllable and realistic closed-loop simulation as a constrained optimization problem. Then, CCDiff maximizes controllability while adhering to realism by automatically identifying and injecting causal structures directly into the diffusion process, providing structured guidance to enhance both realism and controllability. Through rigorous evaluations on benchmark datasets and in a closed-loop simulator, CCDiff demonstrates substantial gains over state-of-the-art approaches in generating realistic and user-preferred trajectories. Our results show CCDiff's effectiveness in extracting and leveraging causal structures, showing improved closed-loop performance based on key metrics such as collision rate, off-road rate, FDE, and comfort. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.19744</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.19744</id><created>2024-12-27</created><updated>2025-02-04</updated><authors><author><keyname>Yang</keyname><forenames>William</forenames></author><author><keyname>Kona</keyname><forenames>Karthikeya</forenames></author><author><keyname>Jain</keyname><forenames>Yashveer</forenames></author><author><keyname>Atzili</keyname><forenames>Tomer</forenames></author><author><keyname>Bhamidipati</keyname><forenames>Abhinav</forenames></author><author><keyname>Lin</keyname><forenames>Xiaomin</forenames></author><author><keyname>Zha</keyname><forenames>Yantian</forenames></author></authors><title>AAM-SEALS: Developing Aerial-Aquatic Manipulators in SEa, Air, and Land   Simulator</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current mobile manipulators and high-fidelity simulators lack the ability to seamlessly operate and simulate across integrated environments spanning sea, air, and land. To address this gap, we introduce Aerial-Aquatic Manipulators (AAMs) in SEa, Air, and Land Simulator (SEALS), a comprehensive and photorealistic simulator designed for AAMs to operate and learn in these diverse environments. The development of AAM-SEALS tackles several significant challenges, including the creation of integrated controllers for flying, swimming, and manipulation, and the high-fidelity simulation of aerial dynamics and hydrodynamics leveraging particle-based hydrodynamics. Our evaluation demonstrates smooth operation and photorealistic transitions across air, water, and their interfaces. We quantitatively validate the fidelity of particle-based hydrodynamics by comparing position-tracking errors across real-world and simulated systems. AAM-SEALS benefits a broad range of robotics communities, including robot learning, aerial robotics, underwater robotics, mobile manipulation, and robotic simulators. We will open-source our code and data to foster the advancement of research in these fields. The overview video is available at https://youtu.be/MbqIIrYvR78. Visit our project website at https://aam-seals-v1.umd.edu for more details. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.20036</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.20036</id><created>2024-12-28</created><updated>2025-02-05</updated><authors><author><keyname>Bai</keyname><forenames>Ting</forenames></author><author><keyname>Chen</keyname><forenames>Weijie</forenames></author><author><keyname>Yang</keyname><forenames>Cheng</forenames></author><author><keyname>Shi</keyname><forenames>Chuan</forenames></author></authors><title>Invariant debiasing learning for recommendation via biased imputation</title><categories>cs.IR</categories><journal-ref>Information Processing &amp; Management,Volume 62, Issue 3, May 2025,   104028</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous debiasing studies utilize unbiased data to make supervision of model training. They suffer from the high trial risks and experimental costs to obtain unbiased data. Recent research attempts to use invariant learning to detach the invariant preference of users for unbiased recommendations in an unsupervised way. However, it faces the drawbacks of low model accuracy and unstable prediction performance due to the losing cooperation with variant preference. In this paper, we experimentally demonstrate that invariant learning causes information loss by directly discarding the variant information, which reduces the generalization ability and results in the degradation of model performance in unbiased recommendations. Based on this consideration, we propose a novel lightweight knowledge distillation framework (KDDebias) to automatically learn the unbiased preference of users from both invariant and variant information. Specifically, the variant information is imputed to the invariant user preference in the distance-aware knowledge distillation process. Extensive experiments on three public datasets, i.e., Yahoo!R3, Coat, and MIND, show that with the biased imputation from the variant preference of users, our proposed method achieves significant improvements with less than 50% learning parameters compared to the SOTA unsupervised debiasing model in recommender systems. Our code is publicly available at https://github.com/BAI-LAB/KD-Debias. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.20161</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.20161</id><created>2024-12-28</created><updated>2025-02-05</updated><authors><author><keyname>Kilic</keyname><forenames>Boran A.</forenames></author><author><keyname>Akan</keyname><forenames>Ozgur B.</forenames></author></authors><title>Multi Ratio Shift Keying (MRSK) for Molecular Communication</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Molecular Communication (MC) leverages the power of diffusion to transmit molecules from a transmitter to a receiver. A wide variety of modulation techniques based on molecule concentration, type, and release time have been extensively studied in the literature. In this paper, we propose a novel modulation technique that encodes the information into the relative concentrations of multiple molecules called Multi Ratio Shift Keying (MRSK) designed for diffusion-based MC without drift. We show that leveraging all possible ratios in a set of molecules can help mitigate the effects of intersymbol interference (ISI) and provide a flexible communication channel. To evaluate the performance of the MRSK, we develop a mathematical framework for studying the statistics of the ratio of random variables, focusing on noncentral Gaussian distributions. We then assess MRSK performance both analytically and through particle-based simulations under various channel conditions, identifying potential sources of error in our system model. Additionally, we conduct a comparative analysis of commonly used modulation schemes in the literature based on bit error rate (BER). The results show that MRSK significantly outperforms all traditional modulation schemes considered in this study in terms of BER. MRSK offers a promising, flexible, and more reliable communication method for the future of the MC paradigm. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.20824</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.20824</id><created>2024-12-30</created><updated>2025-02-05</updated><authors><author><keyname>Jorge</keyname><forenames>Emilio</forenames></author><author><keyname>Dimitrakakis</keyname><forenames>Christos</forenames></author><author><keyname>Basu</keyname><forenames>Debabrota</forenames></author></authors><title>Isoperimetry is All We Need: Langevin Posterior Sampling for RL with   Sublinear Regret</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Common assumptions, like linear or RKHS models, and Gaussian or log-concave posteriors over the models, do not explain practical success of RL across a wider range of distributions and models. Thus, we study how to design RL algorithms with sublinear regret for isoperimetric distributions, specifically the ones satisfying the Log-Sobolev Inequality (LSI). LSI distributions include the standard setups of RL theory, and others, such as many non-log-concave and perturbed distributions. First, we show that the Posterior Sampling-based RL (PSRL) algorithm yields sublinear regret if the data distributions satisfy LSI and some mild additional assumptions. Also, when we cannot compute or sample from an exact posterior, we propose a Langevin sampling-based algorithm design: LaPSRL. We show that LaPSRL achieves order-optimal regret and subquadratic complexity per episode. Finally, we deploy LaPSRL with a Langevin sampler -- SARAH-LD, and test it for different bandit and MDP environments. Experimental results validate the generality of LaPSRL across environments and its competitive performance with respect to the baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.21145</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.21145</id><created>2024-12-30</created><updated>2025-02-05</updated><authors><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author></authors><title>The Shortest Interesting Binary Words</title><categories>math.CO cs.FL</categories><msc-class>68R15</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  I will show that there exist two binary words (one of length 4 and one of length 6) that play a special role in many different problems in combinatorics on words. They can therefore be considered \textit{the shortest interesting binary words}. My claim is supported by the fact that these two words appear in dozens of papers in combinatorics on words. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.01904</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.01904</id><created>2025-01-03</created><updated>2025-02-05</updated><authors><author><keyname>Du</keyname><forenames>Yifan</forenames></author><author><keyname>Liu</keyname><forenames>Zikang</forenames></author><author><keyname>Li</keyname><forenames>Yifan</forenames></author><author><keyname>Zhao</keyname><forenames>Wayne Xin</forenames></author><author><keyname>Huo</keyname><forenames>Yuqi</forenames></author><author><keyname>Wang</keyname><forenames>Bingning</forenames></author><author><keyname>Chen</keyname><forenames>Weipeng</forenames></author><author><keyname>Liu</keyname><forenames>Zheng</forenames></author><author><keyname>Wang</keyname><forenames>Zhongyuan</forenames></author><author><keyname>Wen</keyname><forenames>Ji-Rong</forenames></author></authors><title>Virgo: A Preliminary Exploration on Reproducing o1-like MLLM</title><categories>cs.CV cs.AI</categories><comments>Technical Report on Slow Thinking with LLMs: Visual Reasoning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, slow-thinking reasoning systems, built upon large language models (LLMs), have garnered widespread attention by scaling the thinking time during inference. There is also growing interest in adapting this capability to multimodal large language models (MLLMs). Given that MLLMs handle more complex data semantics across different modalities, it is intuitively more challenging to implement multimodal slow-thinking systems.   To address this issue, in this paper, we explore a straightforward approach by fine-tuning a capable MLLM with a small amount of textual long-form thought data, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning with long thought). We find that these long-form reasoning processes, expressed in natural language, can be effectively transferred to MLLMs. Moreover, it seems that such textual reasoning data can be even more effective than visual reasoning data in eliciting the slow-thinking capacities of MLLMs. While this work is preliminary, it demonstrates that slow-thinking capacities are fundamentally associated with the language model component, which can be transferred across modalities or domains. This finding can be leveraged to guide the development of more powerful slow-thinking reasoning systems. We release our resources at https://github.com/RUCAIBox/Virgo. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.02370</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.02370</id><created>2025-01-04</created><updated>2025-02-05</updated><authors><author><keyname>Lam</keyname><forenames>Tsz Kin</forenames></author><author><keyname>Gaido</keyname><forenames>Marco</forenames></author><author><keyname>Papi</keyname><forenames>Sara</forenames></author><author><keyname>Bentivogli</keyname><forenames>Luisa</forenames></author><author><keyname>Haddow</keyname><forenames>Barry</forenames></author></authors><title>Prepending or Cross-Attention for Speech-to-Text? An Empirical   Comparison</title><categories>cs.CL cs.SD eess.AS</categories><comments>Accepted at NAACL 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Following the remarkable success of Large Language Models (LLMs) in NLP tasks, there is increasing interest in extending their capabilities to speech -- the most common form of communication. The most widespread approach to integrating speech into LLMs is dense feature prepending (DFP), which prepends the projected speech representations to the textual representations, allowing end-to-end training with a speech encoder. This raises questions about the need for a sophisticated speech encoder for DFP and how its performance compares with a standard encoder-decoder (i.e., cross-attention) architecture. We compare DFP and cross-attention under a variety of configurations, such as CTC compression, sequence-level knowledge distillation, on monolingual, bilingual, and multilingual models. To perform a controlled architectural comparison, we train all models from scratch rather than using large pretrained models and use comparable data and parameter settings, testing speech-to-text recognition (ASR) and translation (ST) on MuST-C v1.0 and CoVoST2 datasets. Despite the wide adoption of DFP, our results do not indicate a clear advantage of DFP over cross-attention. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.02556</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.02556</id><created>2025-01-05</created><updated>2025-02-05</updated><authors><author><keyname>Zhong</keyname><forenames>Yi</forenames></author><author><keyname>Zhou</keyname><forenames>Xiaohang</forenames></author><author><keyname>Feng</keyname><forenames>Ke</forenames></author></authors><title>Spatial Network Calculus: Toward Deterministic Wireless Networking</title><categories>cs.NI cs.IT math.IT</categories><comments>13 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper extends the classical network calculus to spatial scenarios, focusing on wireless networks with heterogeneous traffic and varying transmit power levels. Building on spatial network calculus, a prior extension of network calculus to spatial settings, we propose a generalized framework by introducing spatial regulations for stationary marked point processes. The regulations correspond to two key constraints: the total transmit power within a spatial region and the cumulative received power at a receiver. Then we prove the equivalence of ball regulation and shot-noise regulation for stationary marked point processes and establish a universal lower bound on the performance of all network links under these constraints. This framework is applicable to diverse network scenarios, as demonstrated by the analysis of performance guarantees for networks with multi-class users. In addition, we propose an SINR-based power control scheme adapted to user traffic, which ensures differentiated quality of service (QoS) for different user classes. We derive deterministic performance guarantees for all links in complex and heterogeneous wireless networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.02770</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.02770</id><created>2025-01-06</created><updated>2025-02-05</updated><authors><author><keyname>Bui</keyname><forenames>Hoang-Dung</forenames></author><author><keyname>Plaku</keyname><forenames>Erion</forenames></author><author><keyname>Stein</keyname><forenames>Gregoy J.</forenames></author></authors><title>Multi-Agent Path Finding under Limited Communication Range Constraint   via Dynamic Leading</title><categories>cs.AI cs.MA cs.RO</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper proposes a novel framework to handle a multi-agent path finding problem under a limited communication range constraint, where all agents must have a connected communication channel to the rest of the team. Many existing approaches to multi-agent path finding (e.g., leader-follower platooning) overcome computational challenges of planning in this domain by planning one agent at a time in a fixed order. However, fixed leader-follower approaches can become stuck during planning, limiting their practical utility in dense-clutter environments. To overcome this limitation, we develop dynamic leading multi-agent path finding, which allows for dynamic reselection of the leading agent during path planning whenever progress cannot be made. The experiments show the efficiency of our framework, which can handle up to 25 agents with more than 90% success-rate across five environment types where baselines routinely fail. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.04871</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.04871</id><created>2025-01-08</created><updated>2025-02-04</updated><authors><author><keyname>Lee</keyname><forenames>Kaitlyn J.</forenames></author><author><keyname>Schuler</keyname><forenames>Alejandro</forenames></author></authors><title>RieszBoost: Gradient Boosting for Riesz Regression</title><categories>stat.ML cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Answering causal questions often involves estimating linear functionals of conditional expectations, such as the average treatment effect or the effect of a longitudinal modified treatment policy. By the Riesz representation theorem, these functionals can be expressed as the expected product of the conditional expectation of the outcome and the Riesz representer, a key component in doubly robust estimation methods. Traditionally, the Riesz representer is estimated indirectly by deriving its explicit analytical form, estimating its components, and substituting these estimates into the known form (e.g., the inverse propensity score). However, deriving or estimating the analytical form can be challenging, and substitution methods are often sensitive to practical positivity violations, leading to higher variance and wider confidence intervals. In this paper, we propose a novel gradient boosting algorithm to directly estimate the Riesz representer without requiring its explicit analytical form. This method is particularly suited for tabular data, offering a flexible, nonparametric, and computationally efficient alternative to existing methods for Riesz regression. Through simulation studies, we demonstrate that our algorithm performs on par with or better than indirect estimation techniques across a range of functionals, providing a user-friendly and robust solution for estimating causal quantities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.07306</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.07306</id><created>2025-01-13</created><updated>2025-02-05</updated><authors><author><keyname>Martin</keyname><forenames>Ségolène</forenames></author><author><keyname>Pesquet</keyname><forenames>Jean-Christophe</forenames></author><author><keyname>Steidl</keyname><forenames>Gabriele</forenames></author><author><keyname>Ayed</keyname><forenames>Ismail Ben</forenames></author></authors><title>Variable Bregman Majorization-Minimization Algorithm and its Application   to Dirichlet Maximum Likelihood Estimation</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel Bregman descent algorithm for minimizing a convex function that is expressed as the sum of a differentiable part (defined over an open set) and a possibly nonsmooth term. The approach, referred to as the Variable Bregman Majorization-Minimization (VBMM) algorithm, extends the Bregman Proximal Gradient method by allowing the Bregman function used in the divergence to adaptively vary at each iteration, provided it satisfies a majorizing condition on the objective function. This adaptive framework enables the algorithm to approximate the objective more precisely at each iteration, thereby allowing for accelerated convergence compared to the traditional Bregman Proximal Gradient descent. We establish the convergence of the VBMM algorithm to a minimizer under mild assumptions on the family of metrics used. Furthermore, we introduce a novel application of both the Bregman Proximal Gradient method and the VBMM algorithm to the estimation of the multidimensional parameters of a Dirichlet distribution through the maximization of its log-likelihood. Numerical experiments confirm that the VBMM algorithm outperforms existing approaches in terms of convergence speed. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.07863</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.07863</id><created>2025-01-14</created><updated>2025-02-05</updated><authors><author><keyname>Luo</keyname><forenames>Hao</forenames></author><author><keyname>Tang</keyname><forenames>Liping</forenames></author><author><keyname>Yang</keyname><forenames>Xinmin</forenames></author></authors><title>An accelerated gradient method with adaptive restart for convex   multiobjective optimization problems</title><categories>math.OC cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, based on the continuous time approach, we propose an accelerated gradient method with adaptive residual restart for convex multiobjective optimization problems. For the first, we derive rigorously the continuous limit of the multiobjective accelerated proximal gradient method by Tanabe et al. [Comput. Optim. Appl., 2023]. It is a second-order ordinary differential equation (ODE) that involves a special projection operator and can be viewed as an extension of the ODE by Su et al. [J. Mach. Learn. Res., 2016] for Nesterov acceleration. Then, we introduce a novel accelerated multiobjective gradient (AMG) flow with tailored time scaling that adapts automatically to the convex case and the strongly convex case, and the exponential decay rate of a merit function along with the solution trajectory of AMG flow is established via the Lyapunov analysis. After that, we consider an implicit-explicit time discretization and obtain an accelerated multiobjective gradient method with a convex quadratic programming subproblem. The fast sublinear rate and linear rate are proved respectively for convex and strongly convex problems. In addition, we present an efficient residual based adaptive restart technique to overcome the oscillation issue and improve the convergence significantly. Numerical results are provided to validate the practical performance of the proposed method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.08418</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.08418</id><created>2025-01-14</created><updated>2025-02-04</updated><authors><author><keyname>Yan</keyname><forenames>Zijiang</forenames></author><author><keyname>Zhou</keyname><forenames>Hao</forenames></author><author><keyname>Pei</keyname><forenames>Jianhua</forenames></author><author><keyname>Kaushik</keyname><forenames>Aryan</forenames></author><author><keyname>Tabassum</keyname><forenames>Hina</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author></authors><title>CVaR-Based Variational Quantum Optimization for User Association in   Handoff-Aware Vehicular Networks</title><categories>cs.LG cs.AI cs.NI cs.SY eess.SY</categories><comments>Accepted in IEEE International Conference on Communications (ICC   2025)</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Efficient resource allocation is essential for optimizing various tasks in wireless networks, which are usually formulated as generalized assignment problems (GAP). GAP, as a generalized version of the linear sum assignment problem, involves both equality and inequality constraints that add computational challenges. In this work, we present a novel Conditional Value at Risk (CVaR)-based Variational Quantum Eigensolver (VQE) framework to address GAP in vehicular networks (VNets). Our approach leverages a hybrid quantum-classical structure, integrating a tailored cost function that balances both objective and constraint-specific penalties to improve solution quality and stability. Using the CVaR-VQE model, we handle the GAP efficiently by focusing optimization on the lower tail of the solution space, enhancing both convergence and resilience on noisy intermediate-scale quantum (NISQ) devices. We apply this framework to a user-association problem in VNets, where our method achieves 23.5% improvement compared to the deep neural network (DNN) approach. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.08774</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.08774</id><created>2025-01-15</created><updated>2025-02-05</updated><authors><author><keyname>Treude</keyname><forenames>Christoph</forenames></author><author><keyname>Gerosa</keyname><forenames>Marco A.</forenames></author></authors><title>How Developers Interact with AI: A Taxonomy of Human-AI Collaboration in   Software Engineering</title><categories>cs.SE cs.AI cs.HC</categories><comments>Accepted at 2nd ACM International Conference on AI Foundation Models   and Software Engineering (FORGE 2025)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial intelligence (AI), including large language models and generative AI, is emerging as a significant force in software development, offering developers powerful tools that span the entire development lifecycle. Although software engineering research has extensively studied AI tools in software development, the specific types of interactions between developers and these AI-powered tools have only recently begun to receive attention. Understanding and improving these interactions has the potential to enhance productivity, trust, and efficiency in AI-driven workflows. In this paper, we propose a taxonomy of interaction types between developers and AI tools, identifying eleven distinct interaction types, such as auto-complete code suggestions, command-driven actions, and conversational assistance. Building on this taxonomy, we outline a research agenda focused on optimizing AI interactions, improving developer control, and addressing trust and usability challenges in AI-assisted development. By establishing a structured foundation for studying developer-AI interactions, this paper aims to stimulate research on creating more effective, adaptive AI tools for software development. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.08958</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.08958</id><created>2025-01-15</created><updated>2025-02-05</updated><authors><author><keyname>Liu</keyname><forenames>Meiliang</forenames></author><author><keyname>Xu</keyname><forenames>Yunfang</forenames></author><author><keyname>Li</keyname><forenames>Zijin</forenames></author><author><keyname>Si</keyname><forenames>Zhengye</forenames></author><author><keyname>Yang</keyname><forenames>Xiaoxiao</forenames></author><author><keyname>Yang</keyname><forenames>Xinyue</forenames></author><author><keyname>Zhao</keyname><forenames>Zhiwen</forenames></author></authors><title>Kolmogorov-Arnold Networks for Time Series Granger Causality Inference</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We propose the Granger causality inference Kolmogorov-Arnold Networks (KANGCI), a novel architecture that extends the recently proposed Kolmogorov-Arnold Networks (KAN) to the domain of causal inference. By extracting base weights from KAN layers and incorporating the sparsity-inducing penalty and ridge regularization, KANGCI effectively infers the Granger causality from time series. Additionally, we propose an algorithm based on time-reversed Granger causality that automatically selects causal relationships with better inference performance from the original or time-reversed time series or integrates the results to mitigate spurious connectivities. Comprehensive experiments conducted on Lorenz-96, Gene regulatory networks, fMRI BOLD signals, VAR, and real-world EEG datasets demonstrate that the proposed model achieves competitive performance to state-of-the-art methods in inferring Granger causality from nonlinear, high-dimensional, and limited-sample time series. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.09520</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.09520</id><created>2025-01-16</created><updated>2025-02-05</updated><authors><author><keyname>Shi</keyname><forenames>Yuxuan</forenames></author><author><keyname>Shao</keyname><forenames>Shuo</forenames></author><author><keyname>Wu</keyname><forenames>Yongpeng</forenames></author><author><keyname>Zhang</keyname><forenames>Wenjun</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author></authors><title>RWZC: A Model-Driven Approach for Learning-based Robust Wyner-Ziv Coding</title><categories>cs.IT math.IT</categories><comments>14 pages, 17 figures, accepted by IEEE Journal on Selected Areas in   Communications</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, a novel learning-based Wyner-Ziv coding framework is considered under a distributed image transmission scenario, where the correlated source is only available at the receiver. Unlike other learnable frameworks, our approach demonstrates robustness to non-stationary source correlation, where the overlapping information between image pairs varies. Specifically, we first model the affine relationship between correlated images and leverage this model for learnable mask generation and rate-adaptive joint source-channel coding. Moreover, we also provide a warping-prediction network to remove the distortion from channel interference and affine transform. Intuitively, the observed performance improvement is largely due to focusing on the simple geometric relationship, rather than the complex joint distribution between the sources. Numerical results show that our framework achieves a 1.5 dB gain in PSNR and a 0.2 improvement in MS-SSIM, along with a significant superiority in perceptual metrics, compared to state-of-the-art methods when applied to real-world samples with non-stationary correlations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.09683</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.09683</id><created>2025-01-16</created><updated>2025-02-05</updated><authors><author><keyname>Cirone</keyname><forenames>Nicola Muca</forenames></author><author><keyname>Salvi</keyname><forenames>Cristopher</forenames></author></authors><title>Rough kernel hedging</title><categories>math.FA cs.LG stat.ML</categories><comments>v2. minor corrections to presentation</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Building on the functional-analytic framework of operator-valued kernels and un-truncated signature kernels, we propose a scalable, provably convergent signature-based algorithm for a broad class of high-dimensional, path-dependent hedging problems. We make minimal assumptions about market dynamics by modelling them as general geometric rough paths, yielding a fully model-free approach. Furthermore, through a representer theorem, we provide theoretical guarantees on the existence and uniqueness of a global minimum for the resulting optimization problem and derive an analytic solution under highly general loss functions. Similar to the popular deep hedging approach, but in a more rigorous fashion, our method can also incorporate additional features via the underlying operator-valued kernel, such as trading signals, news analytics, and past hedging decisions, closely aligning with true machine-learning practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.09768</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.09768</id><created>2025-01-15</created><updated>2025-02-05</updated><authors><author><keyname>Kmainasi</keyname><forenames>Mohamed Bayan</forenames></author><author><keyname>Shahroor</keyname><forenames>Ali Ezzat</forenames></author><author><keyname>Al-Ghraibah</keyname><forenames>Amani</forenames></author></authors><title>Can Large Language Models Predict the Outcome of Judicial Decisions?</title><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large Language Models (LLMs) have shown exceptional capabilities in Natural Language Processing (NLP) across diverse domains. However, their application in specialized tasks such as Legal Judgment Prediction (LJP) for low-resource languages like Arabic remains underexplored. In this work, we address this gap by developing an Arabic LJP dataset, collected and preprocessed from Saudi commercial court judgments. We benchmark state-of-the-art open-source LLMs, including LLaMA-3.2-3B and LLaMA-3.1-8B, under varying configurations such as zero-shot, one-shot, and fine-tuning using QLoRA. Additionally, we used a comprehensive evaluation framework combining quantitative metrics (BLEU and ROUGE) and qualitative assessments (Coherence, legal language, clarity). Our results demonstrate that fine-tuned smaller models achieve comparable performance to larger models in task-specific contexts while offering significant resource efficiency. Furthermore, we investigate the effects of prompt engineering and fine-tuning on model outputs, providing insights into performance variability and instruction sensitivity. By making the dataset, implementation code, and models publicly available, we establish a robust foundation for future research in Arabic legal NLP. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10910</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10910</id><created>2025-01-18</created><updated>2025-02-05</updated><authors><author><keyname>Kowsar</keyname><forenames>Ibna</forenames></author><author><keyname>Rabbani</keyname><forenames>Shourav B.</forenames></author><author><keyname>Hou</keyname><forenames>Yina</forenames></author><author><keyname>Samad</keyname><forenames>Manar D.</forenames></author></authors><title>DeepIFSAC: Deep Imputation of Missing Values Using Feature and Sample   Attention within Contrastive Framework</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Missing values of varying patterns and rates in real-world tabular data pose a significant challenge in developing reliable data-driven models. Existing missing value imputation methods use statistical and traditional machine learning and are ineffective when the missing rate is high and not at random. This paper explores row and column attention in tabular data as between-feature and between-sample attention in a novel framework to reconstruct missing values. The proposed method uses the CutMix data augmentation within a contrastive learning framework to improve the uncertainty of missing value estimation. The performance and generalizability of trained imputation models are evaluated on set-aside test data folds with missing values. The proposed framework outperforms nine state-of-the-art imputation methods across several missing value types and rates (10\%-50\%) on a diverse selection of twelve tabular data sets. We evaluate the quality of imputed data using real-world electronic health records with missing values, demonstrating our proposed framework's superiority to state-of-the-art statistical, machine learning, and deep imputation methods. This paper highlights the heterogeneity of tabular data sets to recommend imputation methods based on missing value types and data characteristics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10970</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10970</id><created>2025-01-19</created><updated>2025-02-05</updated><authors><author><keyname>Calderon</keyname><forenames>Nitay</forenames></author><author><keyname>Reichart</keyname><forenames>Roi</forenames></author><author><keyname>Dror</keyname><forenames>Rotem</forenames></author></authors><title>The Alternative Annotator Test for LLM-as-a-Judge: How to Statistically   Justify Replacing Human Annotators with LLMs</title><categories>cs.CL cs.AI cs.HC</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  The "LLM-as-a-judge" paradigm employs Large Language Models (LLMs) as annotators and evaluators in tasks traditionally performed by humans. LLM annotations are widely used, not only in NLP research but also in fields like medicine, psychology, and social science. Despite their role in shaping study results and insights, there is no standard or rigorous procedure to determine whether LLMs can replace human annotators. In this paper, we propose a novel statistical procedure -- the Alternative Annotator Test (alt-test) -- that requires only a modest subset of annotated examples to justify using LLM annotations. Additionally, we introduce a versatile and interpretable measure for comparing LLM judges. To demonstrate our procedure, we curated a diverse collection of ten datasets, consisting of language and vision-language tasks, and conducted experiments with six LLMs and four prompting techniques. Our results show that LLMs can sometimes replace humans with closed-source LLMs (such as GPT-4o), outperforming open-source LLMs, and that prompting techniques yield judges of varying quality. We hope this study encourages more rigorous and reliable practices. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.11280</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.11280</id><created>2025-01-20</created><updated>2025-02-04</updated><authors><author><keyname>Yoshida</keyname><forenames>Tsukasa</forenames></author><author><keyname>Watanabe</keyname><forenames>Kazuho</forenames></author></authors><title>Empirical Bayes Estimation for Lasso-Type Regularizers: Analysis of   Automatic Relevance Determination</title><categories>math.ST cs.IT cs.LG math.IT stat.TH</categories><comments>8 pages, 1 figure</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper focuses on linear regression models with non-conjugate sparsity-inducing regularizers such as lasso and group lasso. Although empirical Bayes approach enables us to estimate the regularization parameter, little is known on the properties of the estimators. In particular, there are many unexplained aspects regarding the specific conditions under which the mechanism of automatic relevance determination (ARD) occurs. In this paper, we derive the empirical Bayes estimators for the group lasso regularized linear regression models with a limited number of parameters. It is shown that the estimators diverge under a certain condition, giving rise to the ARD mechanism. We also prove that empirical Bayes methods can produce ARD mechanism in general regularized linear regression models and clarify the conditions under which models such as ridge, lasso, and group lasso can produce ARD mechanism. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.11613</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.11613</id><created>2025-01-20</created><updated>2025-02-05</updated><authors><author><keyname>Robino</keyname><forenames>Giorgio</forenames></author></authors><title>Conversation Routines: A Prompt Engineering Framework for Task-Oriented   Dialog Systems</title><categories>cs.CL cs.AI cs.ET cs.HC cs.PL</categories><comments>In version 3, we added Subsection 1.2, "Single-Agent vs. Multi-Agent   Architectures," and Figure 1 to clarify CAS prompt composition. We also   refined code block and appendix log formatting for improved readability, with   minor formatting corrections throughout</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This study introduces Conversation Routines (CR), a structured prompt engineering framework for developing task-oriented dialog systems using Large Language Models (LLMs). While LLMs demonstrate remarkable natural language understanding capabilities, engineering them to reliably execute complex business workflows remains challenging. The proposed CR framework enables the development of Conversation Agentic Systems (CAS) through natural language specifications, embedding task-oriented logic within LLM prompts. This approach provides a systematic methodology for designing and implementing complex conversational workflows while maintaining behavioral consistency. We demonstrate the framework's effectiveness through two proof-of-concept implementations: a Train Ticket Booking System and an Interactive Troubleshooting Copilot. These case studies validate CR's capability to encode sophisticated behavioral patterns and decision logic while preserving natural conversational flexibility. Results show that CR enables domain experts to design conversational workflows in natural language while leveraging custom functions (tools) developed by software engineers, creating an efficient division of responsibilities where developers focus on core API implementation and domain experts handle conversation design. While the framework shows promise in accessibility and adaptability, we identify key challenges including computational overhead, non-deterministic behavior, and domain-specific logic optimization. Future research directions include CR evaluation methods based on prompt engineering frameworks driven by goal-oriented grading criteria, improving scalability for complex multi-agent interactions, and enhancing system robustness to address the identified limitations across diverse business applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.11689</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.11689</id><created>2025-01-20</created><updated>2025-02-05</updated><authors><author><keyname>Vovk</keyname><forenames>Vladimir</forenames></author></authors><title>Randomness, exchangeability, and conformal prediction</title><categories>cs.LG math.ST stat.ML stat.TH</categories><comments>24 pages, 1 figure; v2 includes several new results about the   optimality of results in v1</comments><msc-class>68Q32 (Primary) 62G15, 68T05, 03D32 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper continues development of the functional theory of randomness, a modification of the algorithmic theory of randomness getting rid of unspecified additive constants. It introduces new kinds of confidence predictors, including randomness predictors (the most general confidence predictors based on the assumption of IID observations) and exchangeability predictors (the most general confidence predictors based on the assumption of exchangeable observations). The main result implies that both are close to conformal predictors and quantifies the difference between randomness prediction and conformal prediction. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.12359</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.12359</id><created>2025-01-21</created><updated>2025-02-05</updated><authors><author><keyname>Nuradha</keyname><forenames>Theshani</forenames></author><author><keyname>Singh</keyname><forenames>Vishal</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Measured Hockey-Stick Divergence and its Applications to Quantum   Pufferfish Privacy</title><categories>quant-ph cs.CR cs.IT cs.LG math.IT</categories><comments>21 pages, submission to the 2025 International Symposium on   Information Theory to be held at University of Michigan</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The hockey-stick divergence is a fundamental quantity characterizing several statistical privacy frameworks that ensure privacy for classical and quantum data. In such quantum privacy frameworks, the adversary is allowed to perform all possible measurements. However, in practice, there are typically limitations to the set of measurements that can be performed. To this end, here, we comprehensively analyze the measured hockey-stick divergence under several classes of practically relevant measurement classes. We prove several of its properties, including data processing and convexity. We show that it is efficiently computable by semi-definite programming for some classes of measurements and can be analytically evaluated for Werner and isotropic states. Notably, we show that the measured hockey-stick divergence characterizes optimal privacy parameters in the quantum pufferfish privacy framework. With this connection and the developed technical tools, we enable methods to quantify and audit privacy for several practically relevant settings. Lastly, we introduce the measured hockey-stick divergence of channels and explore its applications in ensuring privacy for channels. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.12959</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.12959</id><created>2025-01-22</created><updated>2025-02-05</updated><authors><author><keyname>Fei</keyname><forenames>Weizhi</forenames></author><author><keyname>Niu</keyname><forenames>Xueyan</forenames></author><author><keyname>Xie</keyname><forenames>Guoqing</forenames></author><author><keyname>Liu</keyname><forenames>Yingqing</forenames></author><author><keyname>Bai</keyname><forenames>Bo</forenames></author><author><keyname>Han</keyname><forenames>Wei</forenames></author></authors><title>Efficient Prompt Compression with Evaluator Heads for Long-Context   Transformer Inference</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Although applications involving long-context inputs are crucial for the effective utilization of large language models (LLMs), they also result in increased computational costs and reduced performance. To address this challenge, we propose an efficient, training-free prompt compression method that retains key information within compressed prompts. We identify specific attention heads in transformer-based LLMs, which we designate as evaluator heads, that are capable of selecting tokens in long inputs that are most significant for inference. Building on this discovery, we develop EHPC, an Evaluator Head-based Prompt Compression method, which enables LLMs to rapidly "skim through" input prompts by leveraging only the first few layers with evaluator heads during the pre-filling stage, subsequently passing only the important tokens to the model for inference. EHPC achieves state-of-the-art results across two mainstream benchmarks: prompt compression and long-context inference acceleration. Consequently, it effectively reduces the complexity and costs associated with commercial API calls. We further demonstrate that EHPC attains competitive results compared to key-value cache-based acceleration methods, thereby highlighting its potential to enhance the efficiency of LLMs for long-context tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.13331</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.13331</id><created>2025-01-22</created><updated>2025-02-05</updated><authors><author><keyname>Lee</keyname><forenames>Dongyoung</forenames></author><author><keyname>Choi</keyname><forenames>Seungkyu</forenames></author><author><keyname>Chang</keyname><forenames>Ik Joon</forenames></author></authors><title>Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant   Data Razoring</title><categories>cs.LG</categories><comments>16 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large-scale language models (LLMs) excel in language processing tasks but face deployment challenges due to high memory and computational demands. While low-bit quantization, such as 4-bit techniques, offers a potential solution, these methods often suffer from significant accuracy loss or require considerable effort for implementation such as reordering, rotation, etc. To address these challenges, we propose QRazor, a simple yet effective quantization scheme that enables 4-bit quantization of weights, activations, and KV cache in transformer-based LLMs. QRazor operates in two stages: first, quantizing data using 8 or 16-bit integers as a basis with absolute max scaling to preserve accuracy close to full-precision models, and second, compressing the quantized data to 4-bit using our significant data razoring (SDR) technique, which retains only the four most salient bits. Without any additional requirment of fine-tuning or additional training, QRazor achieves performance similar or better compared to state-of-the-art in 4-bit quantization method, surpassing Smoothquant and QLLM by over 12 points and Quarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the LLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit optimized for QRazor, allowing direct low-precision operations on SDR data without decompression. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.13397</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.13397</id><created>2025-01-23</created><updated>2025-02-05</updated><authors><author><keyname>Zheng</keyname><forenames>Kangjie</forenames></author><author><keyname>Yang</keyname><forenames>Junwei</forenames></author><author><keyname>Liang</keyname><forenames>Siyue</forenames></author><author><keyname>Feng</keyname><forenames>Bin</forenames></author><author><keyname>Liu</keyname><forenames>Zequn</forenames></author><author><keyname>Ju</keyname><forenames>Wei</forenames></author><author><keyname>Xiao</keyname><forenames>Zhiping</forenames></author><author><keyname>Zhang</keyname><forenames>Ming</forenames></author></authors><title>ExLM: Rethinking the Impact of [MASK] Tokens in Masked Language Models</title><categories>cs.CL cs.LG</categories><comments>30 pages, 12 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Masked Language Models (MLMs) have achieved remarkable success in many self-supervised representation learning tasks. MLMs are trained by randomly masking portions of the input sequences with [MASK] tokens and learning to reconstruct the original content based on the remaining context. This paper explores the impact of [MASK] tokens on MLMs. Analytical studies show that masking tokens can introduce the corrupted semantics problem, wherein the corrupted context may convey multiple, ambiguous meanings. This problem is also a key factor affecting the performance of MLMs on downstream tasks. Based on these findings, we propose a novel enhanced-context MLM, ExLM. Our approach expands [MASK] tokens in the input context and models the dependencies between these expanded states. This enhancement increases context capacity and enables the model to capture richer semantic information, effectively mitigating the corrupted semantics problem during pre-training. Experimental results demonstrate that ExLM achieves significant performance improvements in both text modeling and SMILES modeling tasks. Further analysis confirms that ExLM enriches semantic representations through context enhancement, and effectively reduces the semantic multimodality commonly observed in MLMs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.13554</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.13554</id><created>2025-01-23</created><updated>2025-02-05</updated><authors><author><keyname>Liu</keyname><forenames>Tao</forenames></author><author><keyname>Wang</keyname><forenames>Kai</forenames></author><author><keyname>Li</keyname><forenames>Senmao</forenames></author><author><keyname>van de Weijer</keyname><forenames>Joost</forenames></author><author><keyname>Khan</keyname><forenames>Fahad Shahbaz</forenames></author><author><keyname>Yang</keyname><forenames>Shiqi</forenames></author><author><keyname>Wang</keyname><forenames>Yaxing</forenames></author><author><keyname>Yang</keyname><forenames>Jian</forenames></author><author><keyname>Cheng</keyname><forenames>Ming-Ming</forenames></author></authors><title>One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation   Using a Single Prompt</title><categories>cs.CV cs.AI cs.LG</categories><comments>28 pages, 22 figures, ICLR2025 conference</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Text-to-image generation models can create high-quality images from input prompts. However, they struggle to support the consistent generation of identity-preserving requirements for storytelling. Existing approaches to this problem typically require extensive training in large datasets or additional modifications to the original model architectures. This limits their applicability across different domains and diverse diffusion model configurations. In this paper, we first observe the inherent capability of language models, coined context consistency, to comprehend identity through context with a single prompt. Drawing inspiration from the inherent context consistency, we propose a novel training-free method for consistent text-to-image (T2I) generation, termed "One-Prompt-One-Story" (1Prompt1Story). Our approach 1Prompt1Story concatenates all prompts into a single input for T2I diffusion models, initially preserving character identities. We then refine the generation process using two novel techniques: Singular-Value Reweighting and Identity-Preserving Cross-Attention, ensuring better alignment with the input description for each frame. In our experiments, we compare our method against various existing consistent T2I generation approaches to demonstrate its effectiveness through quantitative metrics and qualitative assessments. Code is available at https://github.com/byliutao/1Prompt1Story. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.13971</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.13971</id><created>2025-01-22</created><updated>2025-02-05</updated><authors><author><keyname>Jiang</keyname><forenames>Junzhe</forenames></author><author><keyname>Gu</keyname><forenames>Chun</forenames></author><author><keyname>Chen</keyname><forenames>Yurui</forenames></author><author><keyname>Zhang</keyname><forenames>Li</forenames></author></authors><title>GS-LiDAR: Generating Realistic LiDAR Point Clouds with Panoramic   Gaussian Splatting</title><categories>cs.CV cs.GR eess.IV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LiDAR novel view synthesis (NVS) has emerged as a novel task within LiDAR simulation, offering valuable simulated point cloud data from novel viewpoints to aid in autonomous driving systems. However, existing LiDAR NVS methods typically rely on neural radiance fields (NeRF) as their 3D representation, which incurs significant computational costs in both training and rendering. Moreover, NeRF and its variants are designed for symmetrical scenes, making them ill-suited for driving scenarios. To address these challenges, we propose GS-LiDAR, a novel framework for generating realistic LiDAR point clouds with panoramic Gaussian splatting. Our approach employs 2D Gaussian primitives with periodic vibration properties, allowing for precise geometric reconstruction of both static and dynamic elements in driving scenarios. We further introduce a novel panoramic rendering technique with explicit ray-splat intersection, guided by panoramic LiDAR supervision. By incorporating intensity and ray-drop spherical harmonic (SH) coefficients into the Gaussian primitives, we enhance the realism of the rendered point clouds. Extensive experiments on KITTI-360 and nuScenes demonstrate the superiority of our method in terms of quantitative metrics, visual quality, as well as training and rendering efficiency. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14070</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14070</id><created>2025-01-23</created><updated>2025-02-04</updated><authors><author><keyname>Jager</keyname><forenames>Gavin</forenames></author><author><keyname>Cornett</keyname><forenames>David</forenames><suffix>III</suffix></author><author><keyname>Glenn</keyname><forenames>Gavin</forenames></author><author><keyname>Aykac</keyname><forenames>Deniz</forenames></author><author><keyname>Johnson</keyname><forenames>Christi</forenames></author><author><keyname>Zhang</keyname><forenames>Robert</forenames></author><author><keyname>Shivers</keyname><forenames>Ryan</forenames></author><author><keyname>Bolme</keyname><forenames>David</forenames></author><author><keyname>Davies</keyname><forenames>Laura</forenames></author><author><keyname>Dolvin</keyname><forenames>Scott</forenames></author><author><keyname>Barber</keyname><forenames>Nell</forenames></author><author><keyname>Brogan</keyname><forenames>Joel</forenames></author><author><keyname>Burchfield</keyname><forenames>Nick</forenames></author><author><keyname>Dukes</keyname><forenames>Carl</forenames></author><author><keyname>Duncan</keyname><forenames>Andrew</forenames></author><author><keyname>Ferrell</keyname><forenames>Regina</forenames></author><author><keyname>Garrett</keyname><forenames>Austin</forenames></author><author><keyname>Goddard</keyname><forenames>Jim</forenames></author><author><keyname>Hines</keyname><forenames>Jairus</forenames></author><author><keyname>Murphy</keyname><forenames>Bart</forenames></author><author><keyname>Pharris</keyname><forenames>Sean</forenames></author><author><keyname>Stockwell</keyname><forenames>Brandon</forenames></author><author><keyname>Thompson</keyname><forenames>Leanne</forenames></author><author><keyname>Yohe</keyname><forenames>Matthew</forenames></author></authors><title>Expanding on the BRIAR Dataset: A Comprehensive Whole Body Biometric   Recognition Resource at Extreme Distances and Real-World Scenarios   (Collections 1-4)</title><categories>cs.CV cs.AI cs.LG</categories><comments>9 pages, 11 figures, 2 tables</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The state-of-the-art in biometric recognition algorithms and operational systems has advanced quickly in recent years providing high accuracy and robustness in more challenging collection environments and consumer applications. However, the technology still suffers greatly when applied to non-conventional settings such as those seen when performing identification at extreme distances or from elevated cameras on buildings or mounted to UAVs. This paper summarizes an extension to the largest dataset currently focused on addressing these operational challenges, and describes its composition as well as methodologies of collection, curation, and annotation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14739</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14739</id><created>2024-12-11</created><updated>2025-02-04</updated><authors><author><keyname>Ludolf</keyname><forenames>Joshua</forenames></author><author><keyname>Reyna-Hernandez</keyname><forenames>Yesmin</forenames></author><author><keyname>Trevino</keyname><forenames>Matthew</forenames></author></authors><title>Reproduction Research of FSA-Benchmark</title><categories>cs.DC cs.LG</categories><comments>14 pages, 9 figures</comments><msc-class>68</msc-class><acm-class>C.4.2</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the current landscape of big data, the reliability and performance of storage systems are essential to the success of various applications and services. as data volumes continue to grow exponentially, the complexity and scale of the storage infrastructures needed to manage this data also increase. a significant challenge faced by data centers and storage systems is the detection and management of fail-slow disks that experience a gradual decline in performance before ultimately failing. Unlike outright disk failures, fail-slow conditions can go undetected for prolonged periods, leading to considerable impacts on system performance and user experience. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14744</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14744</id><created>2024-12-15</created><updated>2025-02-05</updated><authors><author><keyname>Yu</keyname><forenames>Kairong</forenames></author><author><keyname>Zhang</keyname><forenames>Tianqing</forenames></author><author><keyname>Wang</keyname><forenames>Hongwei</forenames></author><author><keyname>Xu</keyname><forenames>Qi</forenames></author></authors><title>FSTA-SNN:Frequency-based Spatial-Temporal Attention Module for Spiking   Neural Networks</title><categories>cs.NE cs.CV cs.LG</categories><comments>Accepted by AAAI 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spiking Neural Networks (SNNs) are emerging as a promising alternative to Artificial Neural Networks (ANNs) due to their inherent energy efficiency. Owing to the inherent sparsity in spike generation within SNNs, the in-depth analysis and optimization of intermediate output spikes are often neglected. This oversight significantly restricts the inherent energy efficiency of SNNs and diminishes their advantages in spatiotemporal feature extraction, resulting in a lack of accuracy and unnecessary energy expenditure. In this work, we analyze the inherent spiking characteristics of SNNs from both temporal and spatial perspectives. In terms of spatial analysis, we find that shallow layers tend to focus on learning vertical variations, while deeper layers gradually learn horizontal variations of features. Regarding temporal analysis, we observe that there is not a significant difference in feature learning across different time steps. This suggests that increasing the time steps has limited effect on feature learning. Based on the insights derived from these analyses, we propose a Frequency-based Spatial-Temporal Attention (FSTA) module to enhance feature learning in SNNs. This module aims to improve the feature learning capabilities by suppressing redundant spike features.The experimental results indicate that the introduction of the FSTA module significantly reduces the spike firing rate of SNNs, demonstrating superior performance compared to state-of-the-art baselines across multiple datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14940</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14940</id><created>2025-01-24</created><updated>2025-02-04</updated><authors><author><keyname>Sun</keyname><forenames>Guangzhi</forenames></author><author><keyname>Zhan</keyname><forenames>Xiao</forenames></author><author><keyname>Feng</keyname><forenames>Shutong</forenames></author><author><keyname>Woodland</keyname><forenames>Philip C.</forenames></author><author><keyname>Such</keyname><forenames>Jose</forenames></author></authors><title>CASE-Bench: Context-Aware SafEty Benchmark for Large Language Models</title><categories>cs.CL cs.AI</categories><comments>24 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Aligning large language models (LLMs) with human values is essential for their safe deployment and widespread adoption. Current LLM safety benchmarks often focus solely on the refusal of individual problematic queries, which overlooks the importance of the context where the query occurs and may cause undesired refusal of queries under safe contexts that diminish user experience. Addressing this gap, we introduce CASE-Bench, a Context-Aware SafEty Benchmark that integrates context into safety assessments of LLMs. CASE-Bench assigns distinct, formally described contexts to categorized queries based on Contextual Integrity theory. Additionally, in contrast to previous studies which mainly rely on majority voting from just a few annotators, we recruited a sufficient number of annotators necessary to ensure the detection of statistically significant differences among the experimental conditions based on power analysis. Our extensive analysis using CASE-Bench on various open-source and commercial LLMs reveals a substantial and significant influence of context on human judgments (p&lt;0.0001 from a z-test), underscoring the necessity of context in safety evaluations. We also identify notable mismatches between human judgments and LLM responses, particularly in commercial models within safe contexts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.15068</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.15068</id><created>2025-01-24</created><updated>2025-02-05</updated><authors><author><keyname>Li</keyname><forenames>Dongjiang</forenames></author><author><keyname>Peng</keyname><forenames>Bo</forenames></author><author><keyname>Li</keyname><forenames>Chang</forenames></author><author><keyname>Qiao</keyname><forenames>Ning</forenames></author><author><keyname>Zheng</keyname><forenames>Qi</forenames></author><author><keyname>Sun</keyname><forenames>Lei</forenames></author><author><keyname>Qin</keyname><forenames>Yusen</forenames></author><author><keyname>Li</keyname><forenames>Bangguo</forenames></author><author><keyname>Luan</keyname><forenames>Yifeng</forenames></author><author><keyname>Wu</keyname><forenames>Bo</forenames></author><author><keyname>Zhan</keyname><forenames>Yibing</forenames></author><author><keyname>Sun</keyname><forenames>Mingang</forenames></author><author><keyname>Xu</keyname><forenames>Tong</forenames></author><author><keyname>Li</keyname><forenames>Lusong</forenames></author><author><keyname>Shen</keyname><forenames>Hui</forenames></author><author><keyname>He</keyname><forenames>Xiaodong</forenames></author></authors><title>An Atomic Skill Library Construction Method for Data-Efficient Embodied   Manipulation</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Embodied manipulation is a fundamental ability in the realm of embodied artificial intelligence. Although current embodied manipulation models show certain generalizations in specific settings, they struggle in new environments and tasks due to the complexity and diversity of real-world scenarios. The traditional end-to-end data collection and training manner leads to significant data demands. Decomposing end-to-end tasks into atomic skills helps reduce data requirements and improves the task success rate. However, existing methods are limited by predefined skill sets that cannot be dynamically updated. To address the issue, we introduce a three-wheeled data-driven method to build an atomic skill library. We divide tasks into subtasks using the Vision-Language-Planning (VLP). Then, atomic skill definitions are formed by abstracting the subtasks. Finally, an atomic skill library is constructed via data collection and Vision-Language-Action (VLA) fine-tuning. As the atomic skill library expands dynamically with the three-wheel update strategy, the range of tasks it can cover grows naturally. In this way, our method shifts focus from end-to-end tasks to atomic skills, significantly reducing data costs while maintaining high performance and enabling efficient adaptation to new tasks. Extensive experiments in real-world settings demonstrate the effectiveness and efficiency of our approach. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.15104</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.15104</id><created>2025-01-25</created><updated>2025-02-05</updated><authors><author><keyname>Dvir</keyname><forenames>Yotam</forenames></author><author><keyname>Kammar</keyname><forenames>Ohad</forenames></author><author><keyname>Lahav</keyname><forenames>Ori</forenames></author><author><keyname>Plotkin</keyname><forenames>Gordon</forenames></author></authors><title>Two-sorted algebraic decompositions of Brookes's shared-state   denotational semantics</title><categories>cs.PL cs.DC cs.LO</categories><comments>to be published in the conference proceedings of Foundations of   Software Science and Computation Structures (FoSSaCS) 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We use a two sorted equational theory of algebraic effects to model concurrent shared state with preemptive interleaving, recovering Brookes's seminal 1996 trace-based model precisely. The decomposition allows us to analyse Brookes's model algebraically in terms of separate but interacting components. The multiple sorts partition terms into layers. We use two sorts: a "hold" sort for layers that disallow interleaving of environment memory accesses, analogous to holding a global lock on the memory; and a "cede" sort for the opposite. The algebraic signature comprises of independent interlocking components: two new operators that switch between these sorts, delimiting the atomic layers, thought of as acquiring and releasing the global lock; non-deterministic choice; and state-accessing operators. The axioms similarly divide cleanly: the delimiters behave as a closure pair; all operators are strict, and distribute over non-empty non-deterministic choice; and non-deterministic global state obeys Plotkin and Power's presentation of global state. Our representation theorem expresses the free algebras over a two-sorted family of variables as sets of traces with suitable closure conditions. When the held sort has no variables, we recover Brookes's trace semantics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.15417</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.15417</id><created>2025-01-26</created><authors><author><keyname>Zhang</keyname><forenames>Junan</forenames></author><author><keyname>Yang</keyname><forenames>Jing</forenames></author><author><keyname>Fang</keyname><forenames>Zihao</forenames></author><author><keyname>Wang</keyname><forenames>Yuancheng</forenames></author><author><keyname>Zhang</keyname><forenames>Zehua</forenames></author><author><keyname>Wang</keyname><forenames>Zhuo</forenames></author><author><keyname>Fan</keyname><forenames>Fan</forenames></author><author><keyname>Wu</keyname><forenames>Zhizheng</forenames></author></authors><title>AnyEnhance: A Unified Generative Model with Prompt-Guidance and   Self-Critic for Voice Enhancement</title><categories>cs.SD cs.AI cs.LG eess.AS</categories><comments>12 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce AnyEnhance, a unified generative model for voice enhancement that processes both speech and singing voices. Based on a masked generative model, AnyEnhance is capable of handling both speech and singing voices, supporting a wide range of enhancement tasks including denoising, dereverberation, declipping, super-resolution, and target speaker extraction, all simultaneously and without fine-tuning. AnyEnhance introduces a prompt-guidance mechanism for in-context learning, which allows the model to natively accept a reference speaker's timbre. In this way, it could boost enhancement performance when a reference audio is available and enable the target speaker extraction task without altering the underlying architecture. Moreover, we also introduce a self-critic mechanism into the generative process for masked generative models, yielding higher-quality outputs through iterative self-assessment and refinement. Extensive experiments on various enhancement tasks demonstrate AnyEnhance outperforms existing methods in terms of both objective metrics and subjective listening tests. Demo audios are publicly available at https://amphionspace.github.io/anyenhance/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.15753</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.15753</id><created>2025-01-26</created><updated>2025-02-05</updated><authors><author><keyname>Fallahgoul</keyname><forenames>Hasan</forenames></author></authors><title>Scale-Insensitive Neural Network Significance Tests</title><categories>stat.ML cs.LG econ.EM</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper develops a scale-insensitive framework for neural network significance testing, substantially generalizing existing approaches through three key innovations. First, we replace metric entropy calculations with Rademacher complexity bounds, enabling the analysis of neural networks without requiring bounded weights or specific architectural constraints. Second, we weaken the regularity conditions on the target function to require only Sobolev space membership $H^s([-1,1]^d)$ with $s &gt; d/2$, significantly relaxing previous smoothness assumptions while maintaining optimal approximation rates. Third, we introduce a modified sieve space construction based on moment bounds rather than weight constraints, providing a more natural theoretical framework for modern deep learning practices. Our approach achieves these generalizations while preserving optimal convergence rates and establishing valid asymptotic distributions for test statistics. The technical foundation combines localization theory, sharp concentration inequalities, and scale-insensitive complexity measures to handle unbounded weights and general Lipschitz activation functions. This framework better aligns theoretical guarantees with contemporary deep learning practice while maintaining mathematical rigor. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.15828</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.15828</id><created>2025-01-27</created><updated>2025-02-04</updated><authors><author><keyname>Chen</keyname><forenames>Ying</forenames></author><author><keyname>Griffin</keyname><forenames>Paul</forenames></author><author><keyname>Recchia</keyname><forenames>Paolo</forenames></author><author><keyname>Zhou</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>Hongrui</forenames></author></authors><title>Hybrid Quantum Neural Networks with Amplitude Encoding: Advancing   Recovery Rate Predictions</title><categories>q-fin.CP cs.LG quant-ph</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Recovery rate prediction plays a pivotal role in bond investment strategies, enhancing risk assessment, optimizing portfolio allocation, improving pricing accuracy, and supporting effective credit risk management. However, forecasting faces challenges like high-dimensional features, small sample sizes, and overfitting. We propose a hybrid Quantum Machine Learning model incorporating Parameterized Quantum Circuits (PQC) within a neural network framework. PQCs inherently preserve unitarity, avoiding computationally costly orthogonality constraints, while amplitude encoding enables exponential data compression, reducing qubit requirements logarithmically. Applied to a global dataset of 1,725 observations (1996-2023), our method achieved superior accuracy (RMSE 0.228) compared to classical neural networks (0.246) and quantum models with angle encoding (0.242), with efficient computation times. This work highlights the potential of hybrid quantum-classical architectures in advancing recovery rate forecasting. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.15877</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.15877</id><created>2025-01-27</created><updated>2025-02-05</updated><authors><author><keyname>Batra</keyname><forenames>Ashita</forenames></author><author><keyname>narang</keyname><forenames>Mannas</forenames></author><author><keyname>Sharma</keyname><forenames>Neeraj Kumar</forenames></author><author><keyname>Das</keyname><forenames>Pradip K</forenames></author></authors><title>Boli: A dataset for understanding stuttering experience and analyzing   stuttered speech</title><categories>cs.HC cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  There is a growing need for diverse, high-quality stuttered speech data, particularly in the context of Indian languages. This paper introduces Project Boli, a multi-lingual stuttered speech dataset designed to advance scientific understanding and technology development for individuals who stutter, particularly in India. The dataset constitutes (a) anonymized metadata (gender, age, country, mother tongue) and responses to a questionnaire about how stuttering affects their daily lives, (b) captures both read speech (using the Rainbow Passage) and spontaneous speech (through image description tasks) for each participant and (c) includes detailed annotations of five stutter types: blocks, prolongations, interjections, sound repetitions and word repetitions. We present a comprehensive analysis of the dataset, including the data collection procedure, experience summarization of people who stutter, severity assessment of stuttering events and technical validation of the collected data. The dataset is released as an open access to further speech technology development. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.15911</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.15911</id><created>2025-01-27</created><updated>2025-02-05</updated><authors><author><keyname>Hantke</keyname><forenames>Florian</forenames></author><author><keyname>Snyder</keyname><forenames>Peter</forenames></author><author><keyname>Haddadi</keyname><forenames>Hamed</forenames></author><author><keyname>Stock</keyname><forenames>Ben</forenames></author></authors><title>Web Execution Bundles: Reproducible, Accurate, and Archivable Web   Measurements</title><categories>cs.CR</categories><comments>In Proceedings of the 34rd USENIX Security Symposium, USENIX Security   2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recently, reproducibility has become a cornerstone in the security and privacy research community, including artifact evaluations and even a new symposium topic. However, Web measurements lack tools that can be reused across many measurement tasks without modification, while being robust to circumvention, and accurate across the wide range of behaviors in the Web. As a result, most measurement studies use custom tools and varied archival formats, each of unknown correctness and significant limitations, systematically affecting the research's accuracy and reproducibility.   To address these limitations, we present WebREC, a Web measurement tool that is, compared against the current state-of-the-art, accurate (i.e., correctly measures and attributes events not possible with existing tools), general (i.e., reusable without modification for a broad range of measurement tasks), and comprehensive (i.e., handling events from all relevant browser behaviors). We also present .web, an archival format for the accurate and reproducible measurement of a wide range of website behaviors. We empirically evaluate WebREC's accuracy by replicating well-known Web measurement studies and showing that WebREC's results more accurately match our baseline. We then assess if WebREC and .web succeed as general-purpose tools, which could be used to accomplish many Web measurement tasks without modification. We find that this is so: 70% of papers discussed in a 2024 web crawling SoK paper could be conducted using WebREC as is, and a larger number (48%) could be leveraged against .web archives without requiring any new crawling. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.16368</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.16368</id><created>2025-01-22</created><updated>2025-02-04</updated><authors><author><keyname>Baris</keyname><forenames>Ozan</forenames></author><author><keyname>Chen</keyname><forenames>Yizhuo</forenames></author><author><keyname>Dong</keyname><forenames>Gaofeng</forenames></author><author><keyname>Han</keyname><forenames>Liying</forenames></author><author><keyname>Kimura</keyname><forenames>Tomoyoshi</forenames></author><author><keyname>Quan</keyname><forenames>Pengrui</forenames></author><author><keyname>Wang</keyname><forenames>Ruijie</forenames></author><author><keyname>Wang</keyname><forenames>Tianchen</forenames></author><author><keyname>Abdelzaher</keyname><forenames>Tarek</forenames></author><author><keyname>Bergés</keyname><forenames>Mario</forenames></author><author><keyname>Liang</keyname><forenames>Paul Pu</forenames></author><author><keyname>Srivastava</keyname><forenames>Mani</forenames></author></authors><title>Foundation Models for CPS-IoT: Opportunities and Challenges</title><categories>cs.LG cs.AI cs.SY eess.SY</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Methods from machine learning (ML) have transformed the implementation of Perception-Cognition-Communication-Action loops in Cyber-Physical Systems (CPS) and the Internet of Things (IoT), replacing mechanistic and basic statistical models with those derived from data. However, the first generation of ML approaches, which depend on supervised learning with annotated data to create task-specific models, faces significant limitations in scaling to the diverse sensor modalities, deployment configurations, application tasks, and operating dynamics characterizing real-world CPS-IoT systems. The success of task-agnostic foundation models (FMs), including multimodal large language models (LLMs), in addressing similar challenges across natural language, computer vision, and human speech has generated considerable enthusiasm for and exploration of FMs and LLMs as flexible building blocks in CPS-IoT analytics pipelines, promising to reduce the need for costly task-specific engineering.   Nonetheless, a significant gap persists between the current capabilities of FMs and LLMs in the CPS-IoT domain and the requirements they must meet to be viable for CPS-IoT applications. In this paper, we analyze and characterize this gap through a thorough examination of the state of the art and our research, which extends beyond it in various dimensions. Based on the results of our analysis and research, we identify essential desiderata that CPS-IoT domain-specific FMs and LLMs must satisfy to bridge this gap. We also propose actions by CPS-IoT researchers to collaborate in developing key community resources necessary for establishing FMs and LLMs as foundational tools for the next generation of CPS-IoT systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.16489</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.16489</id><created>2025-01-27</created><updated>2025-02-04</updated><authors><author><keyname>Hou</keyname><forenames>Boya</forenames></author><author><keyname>Sanjari</keyname><forenames>Sina</forenames></author><author><keyname>Dahlin</keyname><forenames>Nathan</forenames></author><author><keyname>Koppel</keyname><forenames>Alec</forenames></author><author><keyname>Bose</keyname><forenames>Subhonmesh</forenames></author></authors><title>Nonparametric Sparse Online Learning of the Koopman Operator</title><categories>stat.ML cs.LG cs.SY eess.SY</categories><comments>This work was intended as a replacement of arXiv:2405.07432 and any   subsequent updates will appear there</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The Koopman operator provides a powerful framework for representing the dynamics of general nonlinear dynamical systems. Data-driven techniques to learn the Koopman operator typically assume that the chosen function space is closed under system dynamics. In this paper, we study the Koopman operator via its action on the reproducing kernel Hilbert space (RKHS), and explore the mis-specified scenario where the dynamics may escape the chosen function space. We relate the Koopman operator to the conditional mean embeddings (CME) operator and then present an operator stochastic approximation algorithm to learn the Koopman operator iteratively with control over the complexity of the representation. We provide both asymptotic and finite-time last-iterate guarantees of the online sparse learning algorithm with trajectory-based sampling with an analysis that is substantially more involved than that for finite-dimensional stochastic approximation. Numerical examples confirm the effectiveness of the proposed algorithm. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.16718</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.16718</id><created>2025-01-28</created><updated>2025-02-05</updated><authors><author><keyname>Li</keyname><forenames>Hengzhuang</forenames></author><author><keyname>Zhang</keyname><forenames>Teng</forenames></author></authors><title>Outlier Synthesis via Hamiltonian Monte Carlo for Out-of-Distribution   Detection</title><categories>cs.LG</categories><comments>ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Out-of-distribution (OOD) detection is crucial for developing trustworthy and reliable machine learning systems. Recent advances in training with auxiliary OOD data demonstrate efficacy in enhancing detection capabilities. Nonetheless, these methods heavily rely on acquiring a large pool of high-quality natural outliers. Some prior methods try to alleviate this problem by synthesizing virtual outliers but suffer from either poor quality or high cost due to the monotonous sampling strategy and the heavy-parameterized generative models. In this paper, we overcome all these problems by proposing the Hamiltonian Monte Carlo Outlier Synthesis (HamOS) framework, which views the synthesis process as sampling from Markov chains. Based solely on the in-distribution data, the Markov chains can extensively traverse the feature space and generate diverse and representative outliers, hence exposing the model to miscellaneous potential OOD scenarios. The Hamiltonian Monte Carlo with sampling acceptance rate almost close to 1 also makes our framework enjoy great efficiency. By empirically competing with SOTA baselines on both standard and large-scale benchmarks, we verify the efficacy and efficiency of our proposed HamOS. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.17501</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.17501</id><created>2025-01-29</created><updated>2025-02-05</updated><authors><author><keyname>Salerno</keyname><forenames>Fabio</forenames></author><author><keyname>Al-Kaswan</keyname><forenames>Ali</forenames></author><author><keyname>Izadi</keyname><forenames>Maliheh</forenames></author></authors><title>How Much Do Code Language Models Remember? An Investigation on Data   Extraction Attacks before and after Fine-tuning</title><categories>cs.CR</categories><comments>MSR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Code language models, while widely popular, are often trained on unsanitized source code gathered from across the Internet. Previous work revealed that pre-trained models can remember the content of their training data and regurgitate them through data extraction attacks. Due to the large size of current models, only a few entities have the resources for pre-training such models. However, fine-tuning requires fewer resources and is increasingly used by both small and large entities for its effectiveness on specialized data. Such small curated data for fine-tuning might contain sensitive information or proprietary assets. In this study, we attack both pre-trained and fine-tuned code language models to investigate the extent of data extractability. We first develop a custom benchmark to assess the vulnerability of both pre-training and fine-tuning samples to extraction attacks. Our findings reveal that 54.9% of extractable pre-training data could be retrieved from StarCoder2-15B, whereas this number decreased to 23.5% after fine-tuning. This indicates that fine-tuning reduces the extractability of pre-training data. However, compared to larger models, fine-tuning smaller models increases their vulnerability to data extraction attacks on fine-tuning data. Given the potential sensitivity of fine-tuning data, this can lead to more severe consequences. Lastly, we also manually analyzed 2000 extractable samples before and after fine-tuning. We also found that data carriers and licensing information are the most likely data categories to be memorized from pre-trained and fine-tuned models, while the latter is the most likely to be forgotten after fine-tuning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.17703</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.17703</id><created>2025-01-29</created><updated>2025-02-05</updated><authors><author><keyname>Wang</keyname><forenames>Yubo</forenames></author><author><keyname>Yue</keyname><forenames>Xiang</forenames></author><author><keyname>Chen</keyname><forenames>Wenhu</forenames></author></authors><title>Critique Fine-Tuning: Learning to Critique is More Effective than   Learning to Imitate</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions. In this paper, we challenge this paradigm and propose Critique Fine-Tuning (CFT), a strategy where models learn to critique noisy responses rather than simply imitate correct ones. Inspired by human learning processes that emphasize critical thinking, CFT encourages deeper analysis and nuanced understanding-traits often overlooked by standard SFT. To validate the effectiveness of CFT, we construct a 50K-sample dataset from WebInstruct, using GPT-4o as the teacher to generate critiques in the form of ([query; noisy response], critique). CFT on this dataset yields a consistent 4-10% improvement over SFT on six math benchmarks with different base models like Qwen2.5, Qwen2.5-Math and DeepSeek-Math. We further expand to MetaMath and NuminaMath datasets and observe similar gains over SFT. Notably, our model Qwen2.5-Math-CFT only requires 1 hour training on 8xH100 over the 50K examples. It can match or outperform strong competitors like Qwen2.5-Math-Instruct on most benchmarks, which use over 2M samples. Moreover, it can match the performance of SimpleRL, which is a deepseek-r1 replication trained with 140x more compute. Ablation studies show that CFT is robust to the source of noisy response and teacher critique model. Through these findings, we argue that CFT offers a more effective alternative to advance the reasoning of language models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18080</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18080</id><created>2025-01-29</created><updated>2025-02-04</updated><authors><author><keyname>Gu</keyname><forenames>Xinyi</forenames></author><author><keyname>Rowshan</keyname><forenames>Mohammad</forenames></author><author><keyname>Yuan</keyname><forenames>Jinhong</forenames></author></authors><title>PAC Codes Meet CRC-Polar Codes</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:2406.01903</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  CRC-Polar codes under SC list decoding are well-regarded for their competitive error performance. This paper examines these codes by focusing on minimum weight codewords, breaking them down into the rows of the polar transform. Inspired by the significant impact of parity check bits and their positions, we apply a shifted rate-profile for polarization-adjusted convolutional (PS-PAC) codes, thereby achieving similar improvements in the weight distribution of polar codes through precoding. The results demonstrate a significant improvement in error performance, achieving up to a 0.5 dB power gain with short PS-PAC codes. Additionally, leveraging convolutional precoding in PAC codes, we adopt a continuous deployment (masking) of parity check bits derived from the remainder of continuous division of the partial message polynomial and the CRC polynomial over frozen positions in the rate-profile. This approach enhances performance for medium-length codes, with an overall improvement of 0.12 dB. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18094</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18094</id><created>2025-01-29</created><updated>2025-02-05</updated><authors><author><keyname>Chang</keyname><forenames>Da</forenames></author><author><keyname>Li</keyname><forenames>Yu</forenames></author><author><keyname>Yuan</keyname><forenames>Ganzhao</forenames></author></authors><title>AlphaAdam:Asynchronous Masked Optimization with Dynamic Alpha for   Selective Updates</title><categories>cs.LG</categories><comments>Theorem 3.5 has issues of insufficient rigor. The content "Let   $E[g_i^2] = \sigma_i^2$ ... $E[g_im_{t-1,i}] = \rho_i \sigma_i^2$ be the   correlation between gradients and historical momentum ...." is a non-standard   assumption and may mislead readers. In the spirit of rigor and   responsibility, we temporarily withdraw this version of the content</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the training of large language models (LLMs), updating parameters more efficiently and stably has always been an important challenge. To achieve efficient parameter updates, existing methods usually achieve performance comparable to full parameter updates through methods such as low-dimensional decomposition or layer-wise selective updates. In this work, we propose AlphaAdam, an optimization framework for LLM from the perspective of intra-layer parameter updates. By decoupling parameter updates and dynamically adjusting their strength, AlphaAdam accelerates convergence and improves training stability. We construct parameter masks based on the consistency of historical momentum and gradient direction and combine them with an adaptive mask strength strategy to ensure efficient optimization and theoretical convergence guarantees, which is also applicable to most momentum-based optimizers. Extensive experiments show that AlphaAdam outperforms state-of-the-art methods such as AdamW in terms of convergence speed and computational efficiency across tasks, including GPT-2 pre-trained and fine-tuned RoBERTa and Llama-7B. Our AlphaAdam implements an optimizer enhancement framework for LLMs through intra-layer asynchronous masked adaptive updates. Our code is available in this https://github.com/MaeChd/AlphaAdam. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18184</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18184</id><created>2025-01-30</created><updated>2025-02-05</updated><authors><author><keyname>Lyu</keyname><forenames>Qingchuan</forenames></author></authors><title>Genetic Algorithm with Border Trades (GAB)</title><categories>cs.LG cs.NE stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel approach to improving Genetic Algorithms (GA) in large or complex problem spaces by incorporating new chromosome patterns in the breeding process through border trade activities. These strategies increase chromosome diversity, preventing premature convergence and enhancing the GA's ability to explore the solution space more effectively. Empirical evidence demonstrates significant improvements in convergence behavior. This approach offers a promising pathway to addressing challenges in optimizing large or complex problem domains. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18427</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18427</id><created>2025-01-30</created><updated>2025-02-05</updated><authors><author><keyname>Xie</keyname><forenames>Enze</forenames></author><author><keyname>Chen</keyname><forenames>Junsong</forenames></author><author><keyname>Zhao</keyname><forenames>Yuyang</forenames></author><author><keyname>Yu</keyname><forenames>Jincheng</forenames></author><author><keyname>Zhu</keyname><forenames>Ligeng</forenames></author><author><keyname>Lin</keyname><forenames>Yujun</forenames></author><author><keyname>Zhang</keyname><forenames>Zhekai</forenames></author><author><keyname>Li</keyname><forenames>Muyang</forenames></author><author><keyname>Chen</keyname><forenames>Junyu</forenames></author><author><keyname>Cai</keyname><forenames>Han</forenames></author><author><keyname>Liu</keyname><forenames>Bingchen</forenames></author><author><keyname>Zhou</keyname><forenames>Daquan</forenames></author><author><keyname>Han</keyname><forenames>Song</forenames></author></authors><title>SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute   in Linear Diffusion Transformer</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents SANA-1.5, a linear Diffusion Transformer for efficient scaling in text-to-image generation. Building upon SANA-1.0, we introduce three key innovations: (1) Efficient Training Scaling: A depth-growth paradigm that enables scaling from 1.6B to 4.8B parameters with significantly reduced computational resources, combined with a memory-efficient 8-bit optimizer. (2) Model Depth Pruning: A block importance analysis technique for efficient model compression to arbitrary sizes with minimal quality loss. (3) Inference-time Scaling: A repeated sampling strategy that trades computation for model capacity, enabling smaller models to match larger model quality at inference time. Through these strategies, SANA-1.5 achieves a text-image alignment score of 0.72 on GenEval, which can be further improved to 0.80 through inference scaling, establishing a new SoTA on GenEval benchmark. These innovations enable efficient model scaling across different compute budgets while maintaining high quality, making high-quality image generation more accessible. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18432</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18432</id><created>2025-01-30</created><updated>2025-02-05</updated><authors><author><keyname>Osaba</keyname><forenames>Eneko</forenames></author><author><keyname>Miranda-Rodriguez</keyname><forenames>Pablo</forenames></author><author><keyname>Oikonomakis</keyname><forenames>Andreas</forenames></author><author><keyname>Petrič</keyname><forenames>Matic</forenames></author><author><keyname>Ruiz</keyname><forenames>Alejandra</forenames></author><author><keyname>Bock</keyname><forenames>Sebastian</forenames></author><author><keyname>Kourtis</keyname><forenames>Michail-Alexandros</forenames></author></authors><title>Solving Drone Routing Problems with Quantum Computing: A Hybrid Approach   Combining Quantum Annealing and Gate-Based Paradigms</title><categories>quant-ph cs.AI cs.ET</categories><comments>8 pages, 5 figures. Paper submitted to IEEE Congress on Evolutionary   Computation (IEEE CEC 2025)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents a novel hybrid approach to solving real-world drone routing problems by leveraging the capabilities of quantum computing. The proposed method, coined Quantum for Drone Routing (Q4DR), integrates the two most prominent paradigms in the field: quantum gate-based computing, through the Eclipse Qrisp programming language; and quantum annealers, by means of D-Wave System's devices. The algorithm is divided into two different phases: an initial clustering phase executed using a Quantum Approximate Optimization Algorithm (QAOA), and a routing phase employing quantum annealers. The efficacy of Q4DR is demonstrated through three use cases of increasing complexity, each incorporating real-world constraints such as asymmetric costs, forbidden paths, and itinerant charging points. This research contributes to the growing body of work in quantum optimization, showcasing the practical applications of quantum computing in logistics and route planning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18853</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18853</id><created>2025-01-30</created><updated>2025-02-05</updated><authors><author><keyname>Sun</keyname><forenames>Shuai</forenames></author></authors><title>Non-Asymptotic Analysis of Subspace Identification for Stochastic   Systems Using Multiple Trajectories</title><categories>eess.SY cs.SY</categories><comments>23 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the analysis of identification errors for $n$-dimensional discrete-time Linear Time-Invariant (LTI) systems with $m$ outputs and no external inputs, using Subspace Identification Methods (SIM) with finite sample data. We provide non-asymptotic high-probability upper bounds for matrices $A,C$, the Kalman filter gain $K$, and the closed loop matrix $A-KC $, based on multiple sample trajectories, and further give the first non-asymptotic high-probability upper bounds for the system poles, which cover both (marginally) stable systems and unstable systems. We show that, with high probability, the non-asymptotic estimation errors of these matrices decay at a rate of at least $ \mathcal{O}(\sqrt{1/N}) $, while the estimation error of the system poles decays at a rate of at least $ \mathcal{O}(N^{-\frac{1}{2n}}) $, where $ N $ represents the number of sample trajectories. Furthermore, we prove that SIMs become ill-conditioned when the ratio $n/m$ is large, regardless of the system parameters. Numerical experiments are conducted to validate the non-asymptotic results and the ill-conditionedness of SIM. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.19054</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.19054</id><created>2025-01-31</created><updated>2025-02-05</updated><authors><author><keyname>Wang</keyname><forenames>Ruiyu</forenames></author><author><keyname>Yuan</keyname><forenames>Yu</forenames></author><author><keyname>Sun</keyname><forenames>Shizhao</forenames></author><author><keyname>Bian</keyname><forenames>Jiang</forenames></author></authors><title>Text-to-CAD Generation Through Infusing Visual Feedback in Large   Language Models</title><categories>cs.CV cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Creating Computer-Aided Design (CAD) models requires significant expertise and effort. Text-to-CAD, which converts textual descriptions into CAD parametric sequences, is crucial in streamlining this process. Recent studies have utilized ground-truth parametric sequences, known as sequential signals, as supervision to achieve this goal. However, CAD models are inherently multimodal, comprising parametric sequences and corresponding rendered visual objects. Besides,the rendering process from parametric sequences to visual objects is many-to-one. Therefore, both sequential and visual signals are critical for effective training. In this work, we introduce CADFusion, a framework that uses Large Language Models (LLMs) as the backbone and alternates between two training stages: the sequential learning (SL) stage and the visual feedback (VF) stage. In the SL stage, we train LLMs using ground-truth parametric sequences, enabling the generation of logically coherent parametric sequences. In the VF stage, we reward parametric sequences that render into visually preferred objects and penalize those that do not, allowing LLMs to learn how rendered visual objects are perceived and evaluated. These two stages alternate throughout the training, ensuring balanced learning and preserving benefits of both signals. Experiments demonstrate that CADFusion significantly improves performance, both qualitatively and quantitatively. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.19060</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.19060</id><created>2025-01-31</created><updated>2025-02-04</updated><authors><author><keyname>Lv</keyname><forenames>Song-Lin</forenames></author><author><keyname>Chen</keyname><forenames>Yu-Yang</forenames></author><author><keyname>Zhou</keyname><forenames>Zhi</forenames></author><author><keyname>Li</keyname><forenames>Yu-Feng</forenames></author><author><keyname>Guo</keyname><forenames>Lan-Zhe</forenames></author></authors><title>Contrast-Aware Calibration for Fine-Tuned CLIP: Leveraging Image-Text   Alignment</title><categories>cs.CV cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Vision-language models (VLMs), such as CLIP, have demonstrated exceptional generalization capabilities and can quickly adapt to downstream tasks through prompt fine-tuning. Unfortunately, in classification tasks involving non-training classes, known as open-vocabulary setting, fine-tuned VLMs often overfit to train classes, resulting in a misalignment between confidence scores and actual accuracy on unseen classes, which significantly undermines their reliability in real-world deployments. Existing confidence calibration methods typically require training parameters or analyzing features from the training dataset, restricting their ability to generalize unseen classes without corresponding train data. Moreover, VLM-specific calibration methods rely solely on text features from train classes as calibration indicators, which inherently limits their ability to calibrate train classes. To address these challenges, we propose an effective multimodal calibration method Contrast-Aware Calibration (CAC). Building on the original CLIP's zero-shot adaptability and the conclusion from empirical analysis that poor intra-class and inter-class discriminative ability on unseen classes is the root cause, we calculate calibration weights based on the contrastive difference between the original and fine-tuned CLIP. This method not only adapts to calibrating unseen classes but also overcomes the limitations of previous VLM calibration methods that could not calibrate train classes. In experiments involving 11 datasets with 5 fine-tuning methods, CAC consistently achieved the best calibration effect on both train and unseen classes without sacrificing accuracy and inference speed. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.19216</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.19216</id><created>2025-01-31</created><updated>2025-02-03</updated><authors><author><keyname>Li</keyname><forenames>Yunyang</forenames></author><author><keyname>Huang</keyname><forenames>Lin</forenames></author><author><keyname>Ding</keyname><forenames>Zhihao</forenames></author><author><keyname>Wang</keyname><forenames>Chu</forenames></author><author><keyname>Wei</keyname><forenames>Xinran</forenames></author><author><keyname>Yang</keyname><forenames>Han</forenames></author><author><keyname>Wang</keyname><forenames>Zun</forenames></author><author><keyname>Liu</keyname><forenames>Chang</forenames></author><author><keyname>Shi</keyname><forenames>Yu</forenames></author><author><keyname>Jin</keyname><forenames>Peiran</forenames></author><author><keyname>Zhang</keyname><forenames>Jia</forenames></author><author><keyname>Gerstein</keyname><forenames>Mark</forenames></author><author><keyname>Qin</keyname><forenames>Tao</forenames></author></authors><title>E2Former: A Linear-time Efficient and Equivariant Transformer for   Scalable Molecular Modeling</title><categories>cs.LG cond-mat.mtrl-sci</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Equivariant Graph Neural Networks (EGNNs) have demonstrated significant success in modeling microscale systems, including those in chemistry, biology and materials science. However, EGNNs face substantial computational challenges due to the high cost of constructing edge features via spherical tensor products, making them impractical for large-scale systems. To address this limitation, we introduce E2Former, an equivariant and efficient transformer architecture that incorporates the Wigner $6j$ convolution (Wigner $6j$ Conv). By shifting the computational burden from edges to nodes, the Wigner $6j$ Conv reduces the complexity from $O(|\mathcal{E}|)$ to $ O(| \mathcal{V}|)$ while preserving both the model's expressive power and rotational equivariance. We show that this approach achieves a 7x-30x speedup compared to conventional $\mathrm{SO}(3)$ convolutions. Furthermore, our empirical results demonstrate that the derived E2Former mitigates the computational challenges of existing approaches without compromising the ability to capture detailed geometric information. This development could suggest a promising direction for scalable and efficient molecular modeling. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.19378</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.19378</id><created>2025-01-31</created><updated>2025-02-04</updated><authors><author><keyname>Cao</keyname><forenames>Lang</forenames></author></authors><title>TableMaster: A Recipe to Advance Table Understanding with Language   Models</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Tables serve as a fundamental format for representing structured relational data. While current language models (LMs) excel at many text-based tasks, they still face challenges in table understanding due to the complex characteristics of tabular data, such as their structured nature. In this paper, we aim to enhance LMs for improved table understanding. We identify four key challenges: 1) difficulty in locating target data, 2) deficiency in table semantics, 3) numerical inaccuracies in textual reasoning, and 4) semantic inflexibility in symbolic reasoning. To address these issues, we propose TableMaster, a recipe and comprehensive framework that integrates multiple solutions to overcome these obstacles. TableMaster first extracts relevant table content and verbalizes it with enriched semantic context. Additionally, we introduce adaptive reasoning, a flexible approach that dynamically adjusts between textual and symbolic reasoning, tailoring the reasoning process to each query. Extensive analyses and experiments demonstrate our findings and the effectiveness of TableMaster. On the WikiTQ dataset, TableMaster achieves an accuracy of 78.13% using GPT-4o-mini, surpassing existing baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.19407</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.19407</id><created>2025-01-23</created><updated>2025-02-05</updated><authors><author><keyname>Pataranutaporn</keyname><forenames>Pat</forenames></author><author><keyname>Powdthavee</keyname><forenames>Nattavudh</forenames></author><author><keyname>Maes</keyname><forenames>Pattie</forenames></author></authors><title>Algorithmic Inheritance: Surname Bias in AI Decisions Reinforces   Intergenerational Inequality</title><categories>cs.CY cs.AI cs.LG econ.GN q-fin.EC</categories><comments>33 pages, 5 figures, 1 table</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Surnames often convey implicit markers of social status, wealth, and lineage, shaping perceptions in ways that can perpetuate systemic biases and intergenerational inequality. This study is the first of its kind to investigate whether and how surnames influence AI-driven decision-making, focusing on their effects across key areas such as hiring recommendations, leadership appointments, and loan approvals. Using 72,000 evaluations of 600 surnames from the United States and Thailand, two countries with distinct sociohistorical contexts and surname conventions, we classify names into four categories: Rich, Legacy, Normal, and phonetically similar Variant groups. Our findings show that elite surnames consistently increase AI-generated perceptions of power, intelligence, and wealth, which in turn influence AI-driven decisions in high-stakes contexts. Mediation analysis reveals perceived intelligence as a key mechanism through which surname biases influence AI decision-making process. While providing objective qualifications alongside surnames mitigates most of these biases, it does not eliminate them entirely, especially in contexts where candidate credentials are low. These findings highlight the need for fairness-aware algorithms and robust policy measures to prevent AI systems from reinforcing systemic inequalities tied to surnames, an often-overlooked bias compared to more salient characteristics such as race and gender. Our work calls for a critical reassessment of algorithmic accountability and its broader societal impact, particularly in systems designed to uphold meritocratic principles while counteracting the perpetuation of intergenerational privilege. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00047</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00047</id><created>2025-01-28</created><updated>2025-02-05</updated><authors><author><keyname>Foucault</keyname><forenames>Armand</forenames><affiliation>IMT, ANITI</affiliation></author><author><keyname>Mamalet</keyname><forenames>Franck</forenames><affiliation>ANITI</affiliation></author><author><keyname>Malgouyres</keyname><forenames>François</forenames><affiliation>IMT</affiliation></author></authors><title>HadamRNN: Binary and Sparse Ternary Orthogonal RNNs</title><categories>cs.LG cs.AI</categories><proxy>ccsd</proxy><journal-ref>International Conference on Learning Representations (ICLR), Apr   2025, Singapour, Singapore</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary and sparse ternary weights in neural networks enable faster computations and lighter representations, facilitating their use on edge devices with limited computational power. Meanwhile, vanilla RNNs are highly sensitive to changes in their recurrent weights, making the binarization and ternarization of these weights inherently challenging. To date, no method has successfully achieved binarization or ternarization of vanilla RNN weights. We present a new approach leveraging the properties of Hadamard matrices to parameterize a subset of binary and sparse ternary orthogonal matrices. This method enables the training of orthogonal RNNs (ORNNs) with binary and sparse ternary recurrent weights, effectively creating a specific class of binary and sparse ternary vanilla RNNs. The resulting ORNNs, called HadamRNN and lock-HadamRNN, are evaluated on benchmarks such as the copy task, permuted and sequential MNIST tasks, and IMDB dataset. Despite binarization or sparse ternarization, these RNNs maintain performance levels comparable to state-of-the-art full-precision models, highlighting the effectiveness of our approach. Notably, our approach is the first solution with binary recurrent weights capable of tackling the copy task over 1000 timesteps. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00094</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00094</id><created>2025-01-31</created><updated>2025-02-04</updated><authors><author><keyname>Heakl</keyname><forenames>Ahmed</forenames></author><author><keyname>Ghaboura</keyname><forenames>Sara</forenames></author><author><keyname>Thawkar</keyname><forenames>Omkar</forenames></author><author><keyname>Khan</keyname><forenames>Fahad Shahbaz</forenames></author><author><keyname>Cholakkal</keyname><forenames>Hisham</forenames></author><author><keyname>Anwer</keyname><forenames>Rao Muhammad</forenames></author><author><keyname>Khan</keyname><forenames>Salman</forenames></author></authors><title>AIN: The Arabic INclusive Large Multimodal Model</title><categories>cs.CV cs.AI cs.CL cs.HC cs.LG</categories><comments>20 pages, 16 figures, ACL</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Amid the swift progress of large language models (LLMs) and their evolution into large multimodal models (LMMs), significant strides have been made in high-resource languages such as English and Chinese. While Arabic LLMs have seen notable progress, Arabic LMMs remain largely unexplored, often narrowly focusing on a few specific aspects of the language and visual understanding. To bridge this gap, we introduce AIN-the Arabic Inclusive Multimodal Model-designed to excel across diverse domains. AIN is an English-Arabic bilingual LMM designed to excel in English and Arabic, leveraging carefully constructed 3.6 million high-quality Arabic-English multimodal data samples. AIN demonstrates state-of-the-art Arabic performance, while also possessing strong English-language visual capabilities. On the recent CAMEL-Bench benchmark comprising 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding, our AIN demonstrates strong performance with the 7B model outperforming GPT-4o by an absolute gain of 3.4% averaged over eight domains and 38 sub-domains. AIN's superior capabilities position it as a significant step toward empowering Arabic speakers with advanced multimodal generative AI tools across diverse applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00202</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00202</id><created>2025-01-31</created><updated>2025-02-04</updated><authors><author><keyname>Kim</keyname><forenames>Hyeok</forenames></author><author><keyname>Jeng</keyname><forenames>Mingyoung J.</forenames></author><author><keyname>Smith</keyname><forenames>Kaitlin N.</forenames></author></authors><title>Toward Human-Quantum Computer Interaction: Interface Techniques for   Usable Quantum Computing</title><categories>cs.HC</categories><comments>18 pages, 11 figures, 1 table. Conditionally accepted to ACM CHI 2025</comments><doi>10.1145/3706598.3713370</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  By leveraging quantum-mechanical properties like superposition, entanglement, and interference, quantum computing (QC) offers promising solutions for problems that classical computing has not been able to solve efficiently, such as drug discovery, cryptography, and physical simulation. Unfortunately, adopting QC remains difficult for potential users like QC beginners and application-specific domain experts, due to limited theoretical and practical knowledge, the lack of integrated interface-wise support, and poor documentation. For example, to use quantum computers, one has to convert conceptual logic into low-level codes, analyze quantum program results, and share programs and results. To support the wider adoption of QC, we, as designers and QC experts, propose interaction techniques for QC through design iterations. These techniques include writing quantum codes conceptually, comparing initial quantum programs with optimized programs, sharing quantum program results, and exploring quantum machines. We demonstrate the feasibility and utility of these techniques via use cases with high-fidelity prototypes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00265</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00265</id><created>2025-01-31</created><updated>2025-02-04</updated><authors><author><keyname>Martinez-Romero</keyname><forenames>Marcos</forenames></author><author><keyname>Horridge</keyname><forenames>Matthew</forenames></author><author><keyname>Mistry</keyname><forenames>Nilesh</forenames></author><author><keyname>Weyhmiller</keyname><forenames>Aubrie</forenames></author><author><keyname>Yu</keyname><forenames>Jimmy K.</forenames></author><author><keyname>Fujimoto</keyname><forenames>Alissa</forenames></author><author><keyname>Henry</keyname><forenames>Aria</forenames></author><author><keyname>O'Connor</keyname><forenames>Martin J.</forenames></author><author><keyname>Sier</keyname><forenames>Ashley</forenames></author><author><keyname>Suber</keyname><forenames>Stephanie</forenames></author><author><keyname>Akdogan</keyname><forenames>Mete U.</forenames></author><author><keyname>Cao</keyname><forenames>Yan</forenames></author><author><keyname>Valliappan</keyname><forenames>Somu</forenames></author><author><keyname>Mieczkowska</keyname><forenames>Joanna O.</forenames></author><author><keyname>team</keyname><forenames>the RADx Data Hub</forenames></author><author><keyname>Krishnamurthy</keyname><forenames>Ashok</forenames></author><author><keyname>Keller</keyname><forenames>Michael A.</forenames></author><author><keyname>Musen</keyname><forenames>Mark A.</forenames></author></authors><title>RADx Data Hub: A Cloud Repository for FAIR, Harmonized COVID-19 Data</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The COVID-19 pandemic highlighted the urgent need for robust systems to enable rapid data collection, integration, and analysis for public health responses. Existing approaches often relied on disparate, non-interoperable systems, creating bottlenecks in comprehensive analyses and timely decision-making. To address these challenges, the U.S. National Institutes of Health (NIH) launched the Rapid Acceleration of Diagnostics (RADx) initiative in 2020, with the RADx Data Hub, a centralized repository for de-identified and curated COVID-19 data, as its cornerstone. The RADx Data Hub hosts diverse study data, including clinical data, testing results, smart sensor outputs, self-reported symptoms, and information on social determinants of health. Built on cloud infrastructure, the RADx Data Hub integrates metadata standards, interoperable formats, and ontology-based tools to adhere to the FAIR (Findable, Accessible, Interoperable, Reusable) principles for data sharing. Initially developed for COVID-19 research, its architecture and processes are adaptable to other scientific disciplines. This paper provides an overview of the data hosted by the RADx Data Hub and describes the platform's capabilities and architecture. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00334</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00334</id><created>2025-02-01</created><updated>2025-02-05</updated><authors><author><keyname>Xu</keyname><forenames>Xin</forenames></author><author><keyname>Xu</keyname><forenames>Qiyun</forenames></author><author><keyname>Xiao</keyname><forenames>Tong</forenames></author><author><keyname>Chen</keyname><forenames>Tianhao</forenames></author><author><keyname>Yan</keyname><forenames>Yuchen</forenames></author><author><keyname>Zhang</keyname><forenames>Jiaxin</forenames></author><author><keyname>Diao</keyname><forenames>Shizhe</forenames></author><author><keyname>Yang</keyname><forenames>Can</forenames></author><author><keyname>Wang</keyname><forenames>Yang</forenames></author></authors><title>UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning   with Large Language Models</title><categories>cs.CL cs.AI</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large language models (LLMs) have demonstrated remarkable capabilities in solving complex reasoning tasks, particularly in mathematics. However, the domain of physics reasoning presents unique challenges that have received significantly less attention. Existing benchmarks often fall short in evaluating LLMs' abilities on the breadth and depth of undergraduate-level physics, underscoring the need for a comprehensive evaluation. To fill this gap, we introduce UGPhysics, a large-scale and comprehensive benchmark specifically designed to evaluate UnderGraduate-level Physics (UGPhysics) reasoning with LLMs. UGPhysics includes 5,520 undergraduate-level physics problems in both English and Chinese, covering 13 subjects with seven different answer types and four distinct physics reasoning skills, all rigorously screened for data leakage. Additionally, we develop a Model-Assistant Rule-based Judgment (MARJ) pipeline specifically tailored for assessing answer correctness of physics problems, ensuring accurate evaluation. Our evaluation of 31 leading LLMs shows that the highest overall accuracy, 49.8% (achieved by OpenAI-o1-mini), emphasizes the necessity for models with stronger physics reasoning skills, beyond math abilities. We hope UGPhysics, along with MARJ, will drive future advancements in AI for physics reasoning. Codes and data are available at https://github.com/YangLabHKUST/UGPhysics . </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00473</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00473</id><created>2025-02-01</created><updated>2025-02-05</updated><authors><author><keyname>Bai</keyname><forenames>Lichen</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author><author><keyname>Xie</keyname><forenames>Zeke</forenames></author></authors><title>Weak-to-Strong Diffusion with Reflection</title><categories>cs.LG cs.CV</categories><comments>20 pages, 19 figures, 14 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of diffusion generative models is to align the learned distribution with the real data distribution through gradient score matching. However, inherent limitations in training data quality, modeling strategies, and architectural design lead to inevitable gap between generated outputs and real data. To reduce this gap, we propose Weak-to-Strong Diffusion (W2SD), a novel framework that utilizes the estimated difference between existing weak and strong models (i.e., weak-to-strong difference) to approximate the gap between an ideal model and a strong model. By employing a reflective operation that alternates between denoising and inversion with weak-to-strong difference, we theoretically understand that W2SD steers latent variables along sampling trajectories toward regions of the real data distribution. W2SD is highly flexible and broadly applicable, enabling diverse improvements through the strategic selection of weak-to-strong model pairs (e.g., DreamShaper vs. SD1.5, good experts vs. bad experts in MoE). Extensive experiments demonstrate that W2SD significantly improves human preference, aesthetic quality, and prompt adherence, achieving SOTA performance across various modalities (e.g., image, video), architectures (e.g., UNet-based, DiT-based, MoE), and benchmarks. For example, Juggernaut-XL with W2SD can improve with the HPSv2 winning rate up to 90% over the original results. Moreover, the performance gains achieved by W2SD markedly outweigh its additional computational overhead, while the cumulative improvements from different weak-to-strong difference further solidify its practical utility and deployability. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00620</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00620</id><created>2025-02-01</created><updated>2025-02-04</updated><authors><author><keyname>Xue</keyname><forenames>Yihao</forenames></author><author><keyname>Li</keyname><forenames>Jiping</forenames></author><author><keyname>Mirzasoleiman</keyname><forenames>Baharan</forenames></author></authors><title>Representations Shape Weak-to-Strong Generalization: Theoretical   Insights and Empirical Predictions</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Weak-to-Strong Generalization (W2SG), where a weak model supervises a stronger one, serves as an important analogy for understanding how humans might guide superhuman intelligence in the future. Promising empirical results revealed that a strong model can surpass its weak supervisor. While recent work has offered theoretical insights into this phenomenon, a clear understanding of the interactions between weak and strong models that drive W2SG remains elusive. We investigate W2SG through a theoretical lens and show that it can be characterized using kernels derived from the principal components of weak and strong models' internal representations. These kernels can be used to define a space that, at a high level, captures what the weak model is unable to learn but is learnable by the strong model. The projection of labels onto this space quantifies how much the strong model falls short of its full potential due to weak supervision. This characterization also provides insights into how certain errors in weak supervision can be corrected by the strong model, regardless of overfitting. Our theory has significant practical implications, providing a representation-based metric that predicts W2SG performance trends without requiring labels, as shown in experiments on molecular predictions with transformers and 5 NLP tasks involving 52 LLMs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00634</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00634</id><created>2025-02-01</created><updated>2025-02-05</updated><authors><author><keyname>Yu</keyname><forenames>Donglei</forenames></author><author><keyname>Zhao</keyname><forenames>Yang</forenames></author><author><keyname>Zhu</keyname><forenames>Jie</forenames></author><author><keyname>Xu</keyname><forenames>Yangyifan</forenames></author><author><keyname>Zhou</keyname><forenames>Yu</forenames></author><author><keyname>Zong</keyname><forenames>Chengqing</forenames></author></authors><title>SimulPL: Aligning Human Preferences in Simultaneous Machine Translation</title><categories>cs.CL cs.AI</categories><comments>Accepted to ICLR 2025. 23 pages,13 figures,11 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneous Machine Translation (SiMT) generates translations while receiving streaming source inputs. This requires the SiMT model to learn a read/write policy, deciding when to translate and when to wait for more source input. Numerous linguistic studies indicate that audiences in SiMT scenarios have distinct preferences, such as accurate translations, simpler syntax, and no unnecessary latency. Aligning SiMT models with these human preferences is crucial to improve their performances. However, this issue still remains unexplored. Additionally, preference optimization for SiMT task is also challenging. Existing methods focus solely on optimizing the generated responses, ignoring human preferences related to latency and the optimization of read/write policy during the preference optimization phase. To address these challenges, we propose Simultaneous Preference Learning (SimulPL), a preference learning framework tailored for the SiMT task. In the SimulPL framework, we categorize SiMT human preferences into five aspects: \textbf{translation quality preference}, \textbf{monotonicity preference}, \textbf{key point preference}, \textbf{simplicity preference}, and \textbf{latency preference}. By leveraging the first four preferences, we construct human preference prompts to efficiently guide GPT-4/4o in generating preference data for the SiMT task. In the preference optimization phase, SimulPL integrates \textbf{latency preference} into the optimization objective and enables SiMT models to improve the read/write policy, thereby aligning with human preferences more effectively. Experimental results indicate that SimulPL exhibits better alignment with human preferences across all latency levels in Zh$\rightarrow$En, De$\rightarrow$En and En$\rightarrow$Zh SiMT tasks. Our data and code will be available at https://github.com/EurekaForNLP/SimulPL. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00893</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00893</id><created>2025-02-02</created><updated>2025-02-05</updated><authors><author><keyname>Shi</keyname><forenames>Haochen</forenames></author><author><keyname>Wang</keyname><forenames>Weizhuo</forenames></author><author><keyname>Song</keyname><forenames>Shuran</forenames></author><author><keyname>Liu</keyname><forenames>C. Karen</forenames></author></authors><title>ToddlerBot: Open-Source ML-Compatible Humanoid Platform for   Loco-Manipulation</title><categories>cs.RO</categories><comments>Project website: https://toddlerbot.github.io/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning-based robotics research driven by data demands a new approach to robot hardware design-one that serves as both a platform for policy execution and a tool for embodied data collection to train policies. We introduce ToddlerBot, a low-cost, open-source humanoid robot platform designed for scalable policy learning and research in robotics and AI. ToddlerBot enables seamless acquisition of high-quality simulation and real-world data. The plug-and-play zero-point calibration and transferable motor system identification ensure a high-fidelity digital twin, enabling zero-shot policy transfer from simulation to the real world. A user-friendly teleoperation interface facilitates streamlined real-world data collection for learning motor skills from human demonstrations. Utilizing its data collection ability and anthropomorphic design, ToddlerBot is an ideal platform to perform whole-body loco-manipulation. Additionally, ToddlerBot's compact size (0.56m, 3.4kg) ensures safe operation in real-world environments. Reproducibility is achieved with an entirely 3D-printed, open-source design and commercially available components, keeping the total cost under 6,000 USD. Comprehensive documentation allows assembly and maintenance with basic technical expertise, as validated by a successful independent replication of the system. We demonstrate ToddlerBot's capabilities through arm span, payload, endurance tests, loco-manipulation tasks, and a collaborative long-horizon scenario where two robots tidy a toy session together. By advancing ML-compatibility, capability, and reproducibility, ToddlerBot provides a robust platform for scalable learning and dynamic policy execution in robotics research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01110</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01110</id><created>2025-02-03</created><updated>2025-02-05</updated><authors><author><keyname>Carlet</keyname><forenames>Claude</forenames></author><author><keyname>Sarkar</keyname><forenames>Palash</forenames></author></authors><title>The Nonlinear Filter Model of Stream Cipher Redivivus</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The nonlinear filter model is an old and well understood approach to the design of secure stream ciphers. Extensive research over several decades has shown how to attack stream ciphers based on this model and has identified the security properties required of the Boolean function used as the filtering function to resist such attacks. This led to the problem of constructing Boolean functions which provide adequate security \textit{and} at the same time are efficient to implement. Unfortunately, over the last two decades no good solutions to this problem appeared in the literature. The lack of good solutions has effectively led to nonlinear filter model becoming more or less obsolete. This is a big loss to the cryptographic design toolkit, since the great advantages of the nonlinear filter model are its simplicity, well understood security and the potential to provide low cost solutions for hardware oriented stream ciphers. In this paper, we revive the nonlinear filter model by constructing appropriate Boolean functions which provide required security and are also efficient to implement. We put forward concrete suggestions of stream ciphers which are $\kappa$-bit secure against known types of attacks for $\kappa=80,128,160,192,224$ and $256$. For the $80$-bit, $128$-bit, and the $256$-bit security levels, the circuits for the corresponding stream ciphers require about 1743.5, 2771.5, and 5607.5 NAND gates respectively. For the $80$-bit and the $128$-bit security levels, the gate count estimates compare quite well to the famous ciphers Trivium and Grain-128a respectively, while for the $256$-bit security level, we do not know of any other stream cipher design which has such a low gate count. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01127</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01127</id><created>2025-02-03</created><updated>2025-02-04</updated><authors><author><keyname>Wu</keyname><forenames>Young</forenames></author><author><keyname>Zhu</keyname><forenames>Yancheng</forenames></author><author><keyname>Cai</keyname><forenames>Jin-Yi</forenames></author><author><keyname>Zhu</keyname><forenames>Xiaojin</forenames></author></authors><title>The Battling Influencers Game: Nash Equilibria Structure of a Potential   Game and Implications to Value Alignment</title><categories>cs.GT cs.AI</categories><comments>9 pages, 8 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  When multiple influencers attempt to compete for a receiver's attention, their influencing strategies must account for the presence of one another. We introduce the Battling Influencers Game (BIG), a multi-player simultaneous-move general-sum game, to provide a game-theoretic characterization of this social phenomenon. We prove that BIG is a potential game, that it has either one or an infinite number of pure Nash equilibria (NEs), and these pure NEs can be found by convex optimization. Interestingly, we also prove that at any pure NE, all (except at most one) influencers must exaggerate their actions to the maximum extent. In other words, it is rational for the influencers to be non-truthful and extreme because they anticipate other influencers to cancel out part of their influence. We discuss the implications of BIG to value alignment. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01208</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01208</id><created>2025-02-03</created><updated>2025-02-05</updated><authors><author><keyname>Ji</keyname><forenames>Xiaotong</forenames></author><author><keyname>Ramesh</keyname><forenames>Shyam Sundhar</forenames></author><author><keyname>Zimmer</keyname><forenames>Matthieu</forenames></author><author><keyname>Bogunovic</keyname><forenames>Ilija</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Ammar</keyname><forenames>Haitham Bou</forenames></author></authors><title>Almost Surely Safe Alignment of Large Language Models at Inference-Time</title><categories>cs.LG cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Even highly capable large language models (LLMs) can produce biased or unsafe responses, and alignment techniques, such as RLHF, aimed at mitigating this issue, are expensive and prone to overfitting as they retrain the LLM. This paper introduces a novel inference-time alignment approach that ensures LLMs generate safe responses almost surely, i.e., with a probability approaching one. We achieve this by framing the safe generation of inference-time responses as a constrained Markov decision process within the LLM's latent space. Crucially, we augment a safety state that tracks the evolution of safety constraints and enables us to demonstrate formal safety guarantees upon solving the MDP in the latent space. Building on this foundation, we propose InferenceGuard, a practical implementation that safely aligns LLMs without modifying the model weights. Empirically, we demonstrate InferenceGuard effectively balances safety and task performance, outperforming existing inference-time alignment methods in generating safe and aligned responses. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01506</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01506</id><created>2025-02-03</created><updated>2025-02-05</updated><authors><author><keyname>Yang</keyname><forenames>Yuzhe</forenames></author><author><keyname>Zhang</keyname><forenames>Yifei</forenames></author><author><keyname>Wu</keyname><forenames>Minghao</forenames></author><author><keyname>Zhang</keyname><forenames>Kaidi</forenames></author><author><keyname>Zhang</keyname><forenames>Yunmiao</forenames></author><author><keyname>Yu</keyname><forenames>Honghai</forenames></author><author><keyname>Hu</keyname><forenames>Yan</forenames></author><author><keyname>Wang</keyname><forenames>Benyou</forenames></author></authors><title>TwinMarket: A Scalable Behavioral and Social Simulation for Financial   Markets</title><categories>cs.CE cs.CY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The study of social emergence has long been a central focus in social science. Traditional modeling approaches, such as rule-based Agent-Based Models (ABMs), struggle to capture the diversity and complexity of human behavior, particularly the irrational factors emphasized in behavioral economics. Recently, large language model (LLM) agents have gained traction as simulation tools for modeling human behavior in social science and role-playing applications. Studies suggest that LLMs can account for cognitive biases, emotional fluctuations, and other non-rational influences, enabling more realistic simulations of socio-economic dynamics. In this work, we introduce TwinMarket, a novel multi-agent framework that leverages LLMs to simulate socio-economic systems. Specifically, we examine how individual behaviors, through interactions and feedback mechanisms, give rise to collective dynamics and emergent phenomena. Through experiments in a simulated stock market environment, we demonstrate how individual actions can trigger group behaviors, leading to emergent outcomes such as financial bubbles and recessions. Our approach provides valuable insights into the complex interplay between individual decision-making and collective socio-economic patterns. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01572</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01572</id><created>2025-02-03</created><updated>2025-02-04</updated><authors><author><keyname>Song</keyname><forenames>Yiren</forenames></author><author><keyname>Liu</keyname><forenames>Cheng</forenames></author><author><keyname>Shou</keyname><forenames>Mike Zheng</forenames></author></authors><title>MakeAnything: Harnessing Diffusion Transformers for Multi-Domain   Procedural Sequence Generation</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  A hallmark of human intelligence is the ability to create complex artifacts through structured multi-step processes. Generating procedural tutorials with AI is a longstanding but challenging goal, facing three key obstacles: (1) scarcity of multi-task procedural datasets, (2) maintaining logical continuity and visual consistency between steps, and (3) generalizing across multiple domains. To address these challenges, we propose a multi-domain dataset covering 21 tasks with over 24,000 procedural sequences. Building upon this foundation, we introduce MakeAnything, a framework based on the diffusion transformer (DIT), which leverages fine-tuning to activate the in-context capabilities of DIT for generating consistent procedural sequences. We introduce asymmetric low-rank adaptation (LoRA) for image generation, which balances generalization capabilities and task-specific performance by freezing encoder parameters while adaptively tuning decoder layers. Additionally, our ReCraft model enables image-to-process generation through spatiotemporal consistency constraints, allowing static images to be decomposed into plausible creation sequences. Extensive experiments demonstrate that MakeAnything surpasses existing methods, setting new performance benchmarks for procedural generation tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01627</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01627</id><created>2025-02-03</created><updated>2025-02-04</updated><authors><author><keyname>Song</keyname><forenames>Yanke</forenames></author><author><keyname>Villar</keyname><forenames>Victoria Ashley</forenames></author><author><keyname>Martinez-Galarza</keyname><forenames>Juan Rafael</forenames></author><author><keyname>Dillmann</keyname><forenames>Steven</forenames></author></authors><title>A Poisson Process AutoDecoder for X-ray Sources</title><categories>astro-ph.IM astro-ph.HE cs.LG stat.AP</categories><comments>13 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  X-ray observing facilities, such as the Chandra X-ray Observatory and the eROSITA, have detected millions of astronomical sources associated with high-energy phenomena. The arrival of photons as a function of time follows a Poisson process and can vary by orders-of-magnitude, presenting obstacles for common tasks such as source classification, physical property derivation, and anomaly detection. Previous work has either failed to directly capture the Poisson nature of the data or only focuses on Poisson rate function reconstruction. In this work, we present Poisson Process AutoDecoder (PPAD). PPAD is a neural field decoder that maps fixed-length latent features to continuous Poisson rate functions across energy band and time via unsupervised learning. PPAD reconstructs the rate function and yields a representation at the same time. We demonstrate the efficacy of PPAD via reconstruction, regression, classification and anomaly detection experiments using the Chandra Source Catalog. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01692</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01692</id><created>2025-02-02</created><updated>2025-02-05</updated><authors><author><keyname>Tan</keyname><forenames>Kim Yong</forenames></author><author><keyname>Lyu</keyname><forenames>Yueming</forenames></author><author><keyname>Tsang</keyname><forenames>Ivor</forenames></author><author><keyname>Ong</keyname><forenames>Yew-Soon</forenames></author></authors><title>Fast Direct: Query-Efficient Online Black-box Guidance for   Diffusion-model Target Generation</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Guided diffusion-model generation is a promising direction for customizing the generation process of a pre-trained diffusion-model to address the specific downstream tasks. Existing guided diffusion models either rely on training of the guidance model with pre-collected datasets or require the objective functions to be differentiable. However, for most real-world tasks, the offline datasets are often unavailable, and their objective functions are often not differentiable, such as image generation with human preferences, molecular generation for drug discovery, and material design. Thus, we need an \textbf{online} algorithm capable of collecting data during runtime and supporting a \textbf{black-box} objective function. Moreover, the \textbf{query efficiency} of the algorithm is also critical because the objective evaluation of the query is often expensive in the real-world scenarios. In this work, we propose a novel and simple algorithm, \textbf{Fast Direct}, for query-efficient online black-box target generation. Our Fast Direct builds a pseudo-target on the data manifold to update the noise sequence of the diffusion model with a universal direction, which is promising to perform query-efficient guided generation. Extensive experiments on twelve high-resolution ($\small {1024 \times 1024}$) image target generation tasks and six 3D-molecule target generation tasks show $\textbf{6}\times$ up to $\textbf{10}\times$ query efficiency improvement and $\textbf{11}\times$ up to $\textbf{44}\times$ query efficiency improvement, respectively. Our implementation is publicly available at: https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01697</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01697</id><created>2025-02-02</created><updated>2025-02-04</updated><authors><author><keyname>Zhu</keyname><forenames>Alan</forenames></author><author><keyname>Asawa</keyname><forenames>Parth</forenames></author><author><keyname>Davis</keyname><forenames>Jared Quincy</forenames></author><author><keyname>Chen</keyname><forenames>Lingjiao</forenames></author><author><keyname>Hanin</keyname><forenames>Boris</forenames></author><author><keyname>Stoica</keyname><forenames>Ion</forenames></author><author><keyname>Gonzalez</keyname><forenames>Joseph E.</forenames></author><author><keyname>Zaharia</keyname><forenames>Matei</forenames></author></authors><title>BARE: Combining Base and Instruction-Tuned Language Models for Better   Synthetic Data Generation</title><categories>cs.CL cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As the demand for high-quality data in model training grows, researchers and developers are increasingly generating synthetic data to tune and train LLMs. A common assumption about synthetic data is that sampling from instruct-tuned models is sufficient; however, these models struggle to produce diverse outputs-a key requirement for generalization. Despite various prompting methods, in this work we show that achieving meaningful diversity from instruct-tuned models remains challenging. In contrast, we find base models without post-training exhibit greater diversity, but are less capable at instruction following and hence of lower quality. Leveraging this insight, we propose Base-Refine (BARE), a synthetic data generation method that combines the diversity of base models with the quality of instruct-tuned models through a two-stage process. With minimal few-shot examples and curation, BARE generates diverse and high-quality datasets, improving downstream task performance. We show that fine-tuning with as few as 1,000 BARE-generated samples can reach performance comparable to the best similarly sized models on LiveCodeBench tasks. Furthermore, fine-tuning with BARE-generated data achieves a 101% improvement over instruct-only data on GSM8K and a 18.4% improvement over SOTA methods on RAFT. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01706</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01706</id><created>2025-02-03</created><updated>2025-02-05</updated><authors><author><keyname>Figueroa</keyname><forenames>Alexei</forenames></author><author><keyname>Westerhoff</keyname><forenames>Justus</forenames></author><author><keyname>Atefi</keyname><forenames>Golzar</forenames></author><author><keyname>Fast</keyname><forenames>Dennis</forenames></author><author><keyname>Winter</keyname><forenames>Benjamin</forenames></author><author><keyname>Gers</keyname><forenames>Felix Alexader</forenames></author><author><keyname>Löser</keyname><forenames>Alexander</forenames></author><author><keyname>Nejdl</keyname><forenames>Wolfang</forenames></author></authors><title>Comply: Learning Sentences with Complex Weights inspired by Fruit Fly   Olfaction</title><categories>cs.CL cs.AI cs.LG cs.NE</categories><comments>Accepted at NICE2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biologically inspired neural networks offer alternative avenues to model data distributions. FlyVec is a recent example that draws inspiration from the fruit fly's olfactory circuit to tackle the task of learning word embeddings. Surprisingly, this model performs competitively even against deep learning approaches specifically designed to encode text, and it does so with the highest degree of computational efficiency. We pose the question of whether this performance can be improved further. For this, we introduce Comply. By incorporating positional information through complex weights, we enable a single-layer neural network to learn sequence representations. Our experiments show that Comply not only supersedes FlyVec but also performs on par with significantly larger state-of-the-art models. We achieve this without additional parameters. Comply yields sparse contextual representations of sentences that can be interpreted explicitly from the neuron weights. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01719</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01719</id><created>2025-02-03</created><updated>2025-02-05</updated><authors><author><keyname>Tong</keyname><forenames>Haibo</forenames></author><author><keyname>Wang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Chen</keyname><forenames>Zhaorun</forenames></author><author><keyname>Ji</keyname><forenames>Haonian</forenames></author><author><keyname>Qiu</keyname><forenames>Shi</forenames></author><author><keyname>Han</keyname><forenames>Siwei</forenames></author><author><keyname>Geng</keyname><forenames>Kexin</forenames></author><author><keyname>Xue</keyname><forenames>Zhongkai</forenames></author><author><keyname>Zhou</keyname><forenames>Yiyang</forenames></author><author><keyname>Xia</keyname><forenames>Peng</forenames></author><author><keyname>Ding</keyname><forenames>Mingyu</forenames></author><author><keyname>Rafailov</keyname><forenames>Rafael</forenames></author><author><keyname>Finn</keyname><forenames>Chelsea</forenames></author><author><keyname>Yao</keyname><forenames>Huaxiu</forenames></author></authors><title>MJ-VIDEO: Fine-Grained Benchmarking and Rewarding Video Preferences in   Video Generation</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent advancements in video generation have significantly improved the ability to synthesize videos from text instructions. However, existing models still struggle with key challenges such as instruction misalignment, content hallucination, safety concerns, and bias. Addressing these limitations, we introduce MJ-BENCH-VIDEO, a large-scale video preference benchmark designed to evaluate video generation across five critical aspects: Alignment, Safety, Fineness, Coherence &amp; Consistency, and Bias &amp; Fairness. This benchmark incorporates 28 fine-grained criteria to provide a comprehensive evaluation of video preference. Building upon this dataset, we propose MJ-VIDEO, a Mixture-of-Experts (MoE)-based video reward model designed to deliver fine-grained reward. MJ-VIDEO can dynamically select relevant experts to accurately judge the preference based on the input text-video pair. This architecture enables more precise and adaptable preference judgments. Through extensive benchmarking on MJ-BENCH-VIDEO, we analyze the limitations of existing video reward models and demonstrate the superior performance of MJ-VIDEO in video preference assessment, achieving 17.58% and 15.87% improvements in overall and fine-grained preference judgments, respectively. Additionally, introducing MJ-VIDEO for preference tuning in video generation enhances the alignment performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01976</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01976</id><created>2025-02-03</created><updated>2025-02-05</updated><authors><author><keyname>Zheng</keyname><forenames>Wenhao</forenames></author><author><keyname>Chen</keyname><forenames>Yixiao</forenames></author><author><keyname>Zhang</keyname><forenames>Weitong</forenames></author><author><keyname>Kundu</keyname><forenames>Souvik</forenames></author><author><keyname>Li</keyname><forenames>Yun</forenames></author><author><keyname>Liu</keyname><forenames>Zhengzhong</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author><author><keyname>Wang</keyname><forenames>Hongyi</forenames></author><author><keyname>Yao</keyname><forenames>Huaxiu</forenames></author></authors><title>CITER: Collaborative Inference for Efficient Large Language Model   Decoding with Token-Level Routing</title><categories>cs.CL cs.AI cs.LG cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large language models have achieved remarkable success in various tasks but suffer from high computational costs during inference, limiting their deployment in resource-constrained applications. To address this issue, we propose a novel CITER (\textbf{C}ollaborative \textbf{I}nference with \textbf{T}oken-l\textbf{E}vel \textbf{R}outing) framework that enables efficient collaboration between small and large language models (SLMs &amp; LLMs) through a token-level routing strategy. Specifically, CITER routes non-critical tokens to an SLM for efficiency and routes critical tokens to an LLM for generalization quality. We formulate router training as a policy optimization, where the router receives rewards based on both the quality of predictions and the inference costs of generation. This allows the router to learn to predict token-level routing scores and make routing decisions based on both the current token and the future impact of its decisions. To further accelerate the reward evaluation process, we introduce a shortcut which significantly reduces the costs of the reward estimation and improving the practicality of our approach. Extensive experiments on five benchmark datasets demonstrate that CITER reduces the inference costs while preserving high-quality generation, offering a promising solution for real-time and resource-constrained applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01989</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01989</id><created>2025-02-03</created><updated>2025-02-04</updated><authors><author><keyname>Zhang</keyname><forenames>Tao</forenames></author><author><keyname>Pan</keyname><forenames>Jia-Shu</forenames></author><author><keyname>Feng</keyname><forenames>Ruiqi</forenames></author><author><keyname>Wu</keyname><forenames>Tailin</forenames></author></authors><title>T-SCEND: Test-time Scalable MCTS-enhanced Diffusion Model</title><categories>cs.LG</categories><comments>20 pages, 12 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce Test-time Scalable MCTS-enhanced Diffusion Model (T-SCEND), a novel framework that significantly improves diffusion model's reasoning capabilities with better energy-based training and scaling up test-time computation. We first show that na\"ively scaling up inference budget for diffusion models yields marginal gain. To address this, the training of T-SCEND consists of a novel linear-regression negative contrastive learning objective to improve the performance-energy consistency of the energy landscape, and a KL regularization to reduce adversarial sampling. During inference, T-SCEND integrates the denoising process with a novel hybrid Monte Carlo Tree Search (hMCTS), which sequentially performs best-of-N random search and MCTS as denoising proceeds. On challenging reasoning tasks of Maze and Sudoku, we demonstrate the effectiveness of T-SCEND's training objective and scalable inference method. In particular, trained with Maze sizes of up to $6\times6$, our T-SCEND solves $88\%$ of Maze problems with much larger sizes of $15\times15$, while standard diffusion completely fails. Code to reproduce the experiments can be found at https://github.com/AI4Science-WestlakeU/t_scend. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01991</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01991</id><created>2025-02-03</created><updated>2025-02-04</updated><authors><author><keyname>Islam</keyname><forenames>Tunazzina</forenames></author><author><keyname>Goldwasser</keyname><forenames>Dan</forenames></author></authors><title>Can LLMs Assist Annotators in Identifying Morality Frames? -- Case Study   on Vaccination Debate on Social Media</title><categories>cs.CL cs.AI cs.CY cs.HC cs.SI</categories><comments>Accepted at 17th ACM Web Science Conference 2025 (WebSci'25)</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Nowadays, social media is pivotal in shaping public discourse, especially on polarizing issues like vaccination, where diverse moral perspectives influence individual opinions. In NLP, data scarcity and complexity of psycholinguistic tasks, such as identifying morality frames, make relying solely on human annotators costly, time-consuming, and prone to inconsistency due to cognitive load. To address these issues, we leverage large language models (LLMs), which are adept at adapting new tasks through few-shot learning, utilizing a handful of in-context examples coupled with explanations that connect examples to task principles. Our research explores LLMs' potential to assist human annotators in identifying morality frames within vaccination debates on social media. We employ a two-step process: generating concepts and explanations with LLMs, followed by human evaluation using a "think-aloud" tool. Our study shows that integrating LLMs into the annotation process enhances accuracy, reduces task difficulty, lowers cognitive load, suggesting a promising avenue for human-AI collaboration in complex psycholinguistic tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02088</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02088</id><created>2025-02-04</created><updated>2025-02-05</updated><authors><author><keyname>Yang</keyname><forenames>Xiaomeng</forenames></author><author><keyname>Tan</keyname><forenames>Zhiyu</forenames></author><author><keyname>Nie</keyname><forenames>Xuecheng</forenames></author><author><keyname>Li</keyname><forenames>Hao</forenames></author></authors><title>IPO: Iterative Preference Optimization for Text-to-Video Generation</title><categories>cs.CV cs.AI</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Video foundation models have achieved significant advancement with the help of network upgrade as well as model scale-up. However, they are still hard to meet requirements of applications due to unsatisfied generation quality. To solve this problem, we propose to align video foundation models with human preferences from the perspective of post-training in this paper. Consequently, we introduce an Iterative Preference Optimization strategy to enhance generated video quality by incorporating human feedback. Specifically, IPO exploits a critic model to justify video generations for pairwise ranking as in Direct Preference Optimization or point-wise scoring as in Kahneman-Tversky Optimization. Given this, IPO optimizes video foundation models with guidance of signals from preference feedback, which helps improve generated video quality in subject consistency, motion smoothness and aesthetic quality, etc. In addition, IPO incorporates the critic model with the multi-modality large language model, which enables it to automatically assign preference labels without need of retraining or relabeling. In this way, IPO can efficiently perform multi-round preference optimization in an iterative manner, without the need of tediously manual labeling. Comprehensive experiments demonstrate that the proposed IPO can effectively improve the video generation quality of a pretrained model and help a model with only 2B parameters surpass the one with 5B parameters. Besides, IPO achieves new state-of-the-art performance on VBench benchmark. We will release our source codes, models as well as dataset to advance future research and applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02096</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02096</id><created>2025-02-04</created><updated>2025-02-05</updated><authors><author><keyname>Chen</keyname><forenames>Yixiao</forenames></author><author><keyname>Sun</keyname><forenames>Shikun</forenames></author><author><keyname>Li</keyname><forenames>Jianshu</forenames></author><author><keyname>Li</keyname><forenames>Ruoyu</forenames></author><author><keyname>Li</keyname><forenames>Zhe</forenames></author><author><keyname>Xing</keyname><forenames>Junliang</forenames></author></authors><title>Dual-Flow: Transferable Multi-Target, Instance-Agnostic Attacks via   In-the-wild Cascading Flow Optimization</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adversarial attacks are widely used to evaluate model robustness, and in black-box scenarios, the transferability of these attacks becomes crucial. Existing generator-based attacks have excellent generalization and transferability due to their instance-agnostic nature. However, when training generators for multi-target tasks, the success rate of transfer attacks is relatively low due to the limitations of the model's capacity. To address these challenges, we propose a novel Dual-Flow framework for multi-target instance-agnostic adversarial attacks, utilizing Cascading Distribution Shift Training to develop an adversarial velocity function. Extensive experiments demonstrate that Dual-Flow significantly improves transferability over previous multi-target generative attacks. For example, it increases the success rate from Inception-v3 to ResNet-152 by 34.58%. Furthermore, our attack method shows substantially stronger robustness against defense mechanisms, such as adversarially trained models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02202</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02202</id><created>2025-02-04</created><updated>2025-02-05</updated><authors><author><keyname>Ghanooni</keyname><forenames>Naghmeh</forenames></author><author><keyname>Pajoum</keyname><forenames>Barbod</forenames></author><author><keyname>Rawal</keyname><forenames>Harshit</forenames></author><author><keyname>Fellenz</keyname><forenames>Sophie</forenames></author><author><keyname>Duy</keyname><forenames>Vo Nguyen Le</forenames></author><author><keyname>Kloft</keyname><forenames>Marius</forenames></author></authors><title>Multi-level Supervised Contrastive Learning</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Contrastive learning is a well-established paradigm in representation learning. The standard framework of contrastive learning minimizes the distance between "similar" instances and maximizes the distance between dissimilar ones in the projection space, disregarding the various aspects of similarity that can exist between two samples. Current methods rely on a single projection head, which fails to capture the full complexity of different aspects of a sample, leading to suboptimal performance, especially in scenarios with limited training data. In this paper, we present a novel supervised contrastive learning method in a unified framework called multilevel contrastive learning (MLCL), that can be applied to both multi-label and hierarchical classification tasks. The key strength of the proposed method is the ability to capture similarities between samples across different labels and/or hierarchies using multiple projection heads. Extensive experiments on text and image datasets demonstrate that the proposed approach outperforms state-of-the-art contrastive learning methods </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02275</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02275</id><created>2025-02-04</created><updated>2025-02-05</updated><authors><author><keyname>Sisouk</keyname><forenames>Keanu</forenames></author><author><keyname>Delon</keyname><forenames>Julie</forenames></author><author><keyname>Tierny</keyname><forenames>Julien</forenames></author></authors><title>A User's Guide to Sampling Strategies for Sliced Optimal Transport</title><categories>cs.LG math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper serves as a user's guide to sampling strategies for sliced optimal transport. We provide reminders and additional regularity results on the Sliced Wasserstein distance. We detail the construction methods, generation time complexity, theoretical guarantees, and conditions for each strategy. Additionally, we provide insights into their suitability for sliced optimal transport in theory. Extensive experiments on both simulated and real-world data offer a representative comparison of the strategies, culminating in practical recommendations for their best usage. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02283</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02283</id><created>2025-02-04</created><updated>2025-02-05</updated><authors><author><keyname>Guo</keyname><forenames>Zhihao</forenames></author><author><keyname>Su</keyname><forenames>Jingxuan</forenames></author><author><keyname>Wang</keyname><forenames>Shenglin</forenames></author><author><keyname>Fan</keyname><forenames>Jinlong</forenames></author><author><keyname>Zhang</keyname><forenames>Jing</forenames></author><author><keyname>Han</keyname><forenames>Liangxiu</forenames></author><author><keyname>Wang</keyname><forenames>Peng</forenames></author></authors><title>GP-GS: Gaussian Processes for Enhanced Gaussian Splatting</title><categories>cs.CV cs.AI</categories><comments>14 pages,11 figures</comments><msc-class>68T45</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  3D Gaussian Splatting has emerged as an efficient photorealistic novel view synthesis method. However, its reliance on sparse Structure-from-Motion (SfM) point clouds consistently compromises the scene reconstruction quality. To address these limitations, this paper proposes a novel 3D reconstruction framework Gaussian Processes Gaussian Splatting (GP-GS), where a multi-output Gaussian Process model is developed to achieve adaptive and uncertainty-guided densification of sparse SfM point clouds. Specifically, we propose a dynamic sampling and filtering pipeline that adaptively expands the SfM point clouds by leveraging GP-based predictions to infer new candidate points from the input 2D pixels and depth maps. The pipeline utilizes uncertainty estimates to guide the pruning of high-variance predictions, ensuring geometric consistency and enabling the generation of dense point clouds. The densified point clouds provide high-quality initial 3D Gaussians to enhance reconstruction performance. Extensive experiments conducted on synthetic and real-world datasets across various scales validate the effectiveness and practicality of the proposed framework. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02358</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02358</id><created>2025-02-04</created><updated>2025-02-04</updated><authors><author><keyname>Guo</keyname><forenames>Ziyan</forenames></author><author><keyname>Hu</keyname><forenames>Zeyu</forenames></author><author><keyname>Zhao</keyname><forenames>Na</forenames></author><author><keyname>Soh</keyname><forenames>De Wen</forenames></author></authors><title>MotionLab: Unified Human Motion Generation and Editing via the   Motion-Condition-Motion Paradigm</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Human motion generation and editing are key components of computer graphics and vision. However, current approaches in this field tend to offer isolated solutions tailored to specific tasks, which can be inefficient and impractical for real-world applications. While some efforts have aimed to unify motion-related tasks, these methods simply use different modalities as conditions to guide motion generation. Consequently, they lack editing capabilities, fine-grained control, and fail to facilitate knowledge sharing across tasks. To address these limitations and provide a versatile, unified framework capable of handling both human motion generation and editing, we introduce a novel paradigm: Motion-Condition-Motion, which enables the unified formulation of diverse tasks with three concepts: source motion, condition, and target motion. Based on this paradigm, we propose a unified framework, MotionLab, which incorporates rectified flows to learn the mapping from source motion to target motion, guided by the specified conditions. In MotionLab, we introduce the 1) MotionFlow Transformer to enhance conditional generation and editing without task-specific modules; 2) Aligned Rotational Position Encoding} to guarantee the time synchronization between source motion and target motion; 3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for effective multi-task learning and knowledge sharing across tasks. Notably, our MotionLab demonstrates promising generalization capabilities and inference efficiency across multiple benchmarks for human motion. Our code and additional video results are available at: https://diouo.github.io/motionlab.github.io/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02387</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02387</id><created>2025-02-04</created><updated>2025-02-04</updated><authors><author><keyname>Liang</keyname><forenames>Junkai</forenames></author><author><keyname>Hu</keyname><forenames>Daqi</forenames></author><author><keyname>Wu</keyname><forenames>Pengfei</forenames></author><author><keyname>Yang</keyname><forenames>Yunbo</forenames></author><author><keyname>Shen</keyname><forenames>Qingni</forenames></author><author><keyname>Wu</keyname><forenames>Zhonghai</forenames></author></authors><title>SoK: Understanding zk-SNARKs: The Gap Between Research and Practice</title><categories>cs.CR</categories><comments>Accepted to USENIX Security Symposium 2025, Seattle, WA, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Zero-knowledge succinct non-interactive arguments of knowledge (zk-SNARKs) are a powerful tool for proving computation correctness, attracting significant interest from researchers, developers, and users. However, the complexity of zk-SNARKs has created gaps between these groups, hindering progress. Researchers focus on constructing efficient proving systems with stronger security and new properties, while developers and users prioritize toolchains, usability, and compatibility.   In this work, we provide a comprehensive study of zk-SNARK, from theory to practice, pinpointing gaps and limitations. We first present a master recipe that unifies the main steps in converting a program into a zk-SNARK. We then classify existing zk-SNARKs according to their key techniques. Our classification addresses the main difference in practically valuable properties between existing zk-SNARK schemes. We survey over 40 zk-SNARKs since 2013 and provide a reference table listing their categories and properties. Following the steps in master recipe, we then survey 11 general-purpose popular used libraries. We elaborate on these libraries' usability, compatibility, efficiency and limitations. Since installing and executing these zk-SNARK systems is challenging, we also provide a completely virtual environment in which to run the compiler for each of them. We identify that the proving system is the primary focus in cryptography academia. In contrast, the constraint system presents a bottleneck in industry. To bridge this gap, we offer recommendations and advocate for the opensource community to enhance documentation, standardization and compatibility. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02389</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02389</id><created>2025-02-04</created><authors><author><keyname>Colomer</keyname><forenames>Pau</forenames></author><author><keyname>Deppe</keyname><forenames>Christian</forenames></author><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>Rate-reliability functions for deterministic identification</title><categories>cs.IT math.IT quant-ph</categories><comments>12 pages, 2 figures. A preliminary version of this work has been   accepted for presentation at the 2025 IEEE International Conference on   Communications, Montreal (Canada) 8-12 June 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We investigate deterministic identification over arbitrary memoryless channels under the constraint that the error probabilities of first and second kind are exponentially small in the block length $n$, controlled by reliability exponents $E_1,E_2 \geq 0$. In contrast to the regime of slowly vanishing errors, where the identifiable message length scales as $\Theta(n\log n)$, here we find that for positive exponents linear scaling is restored, now with a rate that is a function of the reliability exponents. We give upper and lower bounds on the ensuing rate-reliability function in terms of (the logarithm of) the packing and covering numbers of the channel output set, which for small error exponents $E_1,E_2&gt;0$ can be expanded in leading order as the product of the Minkowski dimension of a certain parametrisation the channel output set and $\log\min\{E_1,E_2\}$. These allow us to recover the previously observed slightly superlinear identification rates, and offer a different perspective for understanding them in more traditional information theory terms. We further illustrate our results with a discussion of the case of dimension zero, and extend them to classical-quantum channels and quantum channels with tensor product input restriction. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02451</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02451</id><created>2025-02-04</created><updated>2025-02-05</updated><authors><author><keyname>Cheng</keyname><forenames>Calvin Yixiang</forenames></author><author><keyname>Hale</keyname><forenames>Scott A</forenames></author></authors><title>Beyond English: Evaluating Automated Measurement of Moral Foundations in   Non-English Discourse with a Chinese Case Study</title><categories>cs.CL cs.SI</categories><comments>12 pages, 2 figures, 6 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This study explores computational approaches for measuring moral foundations (MFs) in non-English corpora. Since most resources are developed primarily for English, cross-linguistic applications of moral foundation theory remain limited. Using Chinese as a case study, this paper evaluates the effectiveness of applying English resources to machine translated text, local language lexicons, multilingual language models, and large language models (LLMs) in measuring MFs in non-English texts. The results indicate that machine translation and local lexicon approaches are insufficient for complex moral assessments, frequently resulting in a substantial loss of cultural information. In contrast, multilingual models and LLMs demonstrate reliable cross-language performance with transfer learning, with LLMs excelling in terms of data efficiency. Importantly, this study also underscores the need for human-in-the-loop validation of automated MF assessment, as the most advanced models may overlook cultural nuances in cross-language measurements. The findings highlight the potential of LLMs for cross-language MF measurements and other complex multilingual deductive coding tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02464</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02464</id><created>2025-02-04</created><updated>2025-02-05</updated><authors><author><keyname>Abdallah</keyname><forenames>Abdelrahman</forenames></author><author><keyname>Piryani</keyname><forenames>Bhawna</forenames></author><author><keyname>Mozafari</keyname><forenames>Jamshid</forenames></author><author><keyname>Ali</keyname><forenames>Mohammed</forenames></author><author><keyname>Jatowt</keyname><forenames>Adam</forenames></author></authors><title>Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and   Retrieval-Augmented Generation</title><categories>cs.IR cs.CL</categories><comments>Work in Progress</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Retrieval, re-ranking, and retrieval-augmented generation (RAG) are critical components of modern natural language processing (NLP) applications in information retrieval, question answering, and knowledge-based text generation. However, existing solutions are often fragmented, lacking a unified framework that easily integrates these essential processes. The absence of a standardized implementation, coupled with the complexity of retrieval and re-ranking workflows, makes it challenging for researchers to compare and evaluate different approaches in a consistent environment. While existing toolkits such as Rerankers and RankLLM provide general-purpose reranking pipelines, they often lack the flexibility required for fine-grained experimentation and benchmarking. In response to these challenges, we introduce \textbf{Rankify}, a powerful and modular open-source toolkit designed to unify retrieval, re-ranking, and RAG within a cohesive framework. Rankify supports a wide range of retrieval techniques, including dense and sparse retrievers, while incorporating state-of-the-art re-ranking models to enhance retrieval quality. Additionally, Rankify includes a collection of pre-retrieved datasets to facilitate benchmarking, available at Huggingface (https://huggingface.co/datasets/abdoelsayed/reranking-datasets). To encourage adoption and ease of integration, we provide comprehensive documentation (http://rankify.readthedocs.io/), an open-source implementation on GitHub(https://github.com/DataScienceUIBK/rankify), and a PyPI package for effortless installation(https://pypi.org/project/rankify/). By providing a unified and lightweight framework, Rankify allows researchers and practitioners to advance retrieval and re-ranking methodologies while ensuring consistency, scalability, and ease of use. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02481</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02481</id><created>2025-02-04</created><updated>2025-02-05</updated><authors><author><keyname>Cui</keyname><forenames>Menglong</forenames></author><author><keyname>Gao</keyname><forenames>Pengzhi</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Luan</keyname><forenames>Jian</forenames></author><author><keyname>Wang</keyname><forenames>Bin</forenames></author></authors><title>Multilingual Machine Translation with Open Large Language Models at   Practical Scale: An Empirical Study</title><categories>cs.CL</categories><comments>Accept to NAACL2025 Main Conference</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models (LLMs) have shown continuously improving multilingual capabilities, and even small-scale open-source models have demonstrated rapid performance enhancement. In this paper, we systematically explore the abilities of open LLMs with less than ten billion parameters to handle multilingual machine translation (MT) tasks. We conduct comprehensive evaluations on six popular LLMs and find that models like Gemma2-9B exhibit impressive multilingual translation capabilities. We then introduce the Parallel-First Monolingual-Second (PFMS) data mixing strategy in the continual pretraining stage to further enhance the MT performance and present GemmaX2-28, a 9B model achieving top-tier multilingual translation performance across 28 languages. Specifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA) models such as TowerInstruct and XALMA and achieves competitive performance with Google Translate and GPT-4-turbo. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02523</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02523</id><created>2025-02-04</created><updated>2025-02-05</updated><authors><author><keyname>Mercer</keyname><forenames>Sarah</forenames></author><author><keyname>Spillard</keyname><forenames>Samuel</forenames></author><author><keyname>Martin</keyname><forenames>Daniel P.</forenames></author></authors><title>Brief analysis of DeepSeek R1 and its implications for Generative AI</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In late January 2025, DeepSeek released their new reasoning model (DeepSeek R1); which was developed at a fraction of the cost yet remains competitive with OpenAI's models, despite the US's GPU export ban. This report discusses the model, and what its release means for the field of Generative AI more widely. We briefly discuss other models released from China in recent weeks, their similarities; innovative use of Mixture of Experts (MoE), Reinforcement Learning (RL) and clever engineering appear to be key factors in the capabilities of these models. This think piece has been written to a tight timescale, providing broad coverage of the topic, and serves as introductory material for those looking to understand the model's technical advancements, as well as its place in the ecosystem. Several further areas of research are identified. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02531</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02531</id><created>2025-02-04</created><updated>2025-02-05</updated><authors><author><keyname>Bordelon</keyname><forenames>Blake</forenames></author><author><keyname>Pehlevan</keyname><forenames>Cengiz</forenames></author></authors><title>Deep Linear Network Training Dynamics from Random Initialization: Data,   Width, Depth, and Hyperparameter Transfer</title><categories>cs.LG cond-mat.dis-nn stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We theoretically characterize gradient descent dynamics in deep linear networks trained at large width from random initialization and on large quantities of random data. Our theory captures the ``wider is better" effect of mean-field/maximum-update parameterized networks as well as hyperparameter transfer effects, which can be contrasted with the neural-tangent parameterization where optimal learning rates shift with model width. We provide asymptotic descriptions of both non-residual and residual neural networks, the latter of which enables an infinite depth limit when branches are scaled as $1/\sqrt{\text{depth}}$. We also compare training with one-pass stochastic gradient descent to the dynamics when training data are repeated at each iteration. Lastly, we show that this model recovers the accelerated power law training dynamics for power law structured data in the rich regime observed in recent works. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02542</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02542</id><created>2025-02-04</created><updated>2025-02-05</updated><authors><author><keyname>Kumar</keyname><forenames>Abhinav</forenames></author><author><keyname>Roh</keyname><forenames>Jaechul</forenames></author><author><keyname>Naseh</keyname><forenames>Ali</forenames></author><author><keyname>Karpinska</keyname><forenames>Marzena</forenames></author><author><keyname>Iyyer</keyname><forenames>Mohit</forenames></author><author><keyname>Houmansadr</keyname><forenames>Amir</forenames></author><author><keyname>Bagdasarian</keyname><forenames>Eugene</forenames></author></authors><title>OverThink: Slowdown Attacks on Reasoning LLMs</title><categories>cs.LG cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We increase overhead for applications that rely on reasoning LLMs-we force models to spend an amplified number of reasoning tokens, i.e., "overthink", to respond to the user query while providing contextually correct answers. The adversary performs an OVERTHINK attack by injecting decoy reasoning problems into the public content that is used by the reasoning LLM (e.g., for RAG applications) during inference time. Due to the nature of our decoy problems (e.g., a Markov Decision Process), modified texts do not violate safety guardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini) and open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD datasets. Our results show up to 18x slowdown on FreshQA dataset and 46x slowdown on SQuAD dataset. The attack also shows high transferability across models. To protect applications, we discuss and implement defenses leveraging LLM-based and system design approaches. Finally, we discuss societal, financial, and energy impacts of OVERTHINK attack which could amplify the costs for third-party applications operating reasoning models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02555</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02555</id><created>2025-02-04</created><updated>2025-02-05</updated><authors><author><keyname>Bharti</keyname><forenames>Divya</forenames></author><author><keyname>Ramanarayanan</keyname><forenames>Sriprabha</forenames></author><author><keyname>S</keyname><forenames>Sadhana</forenames></author><author><keyname>M</keyname><forenames>Kishore Kumar</forenames></author><author><keyname>Ram</keyname><forenames>Keerthi</forenames></author><author><keyname>Agarwal</keyname><forenames>Harsh</forenames></author><author><keyname>Venkatesan</keyname><forenames>Ramesh</forenames></author><author><keyname>Sivaprakasam</keyname><forenames>Mohanasankar</forenames></author></authors><title>AAD-DCE: An Aggregated Multimodal Attention Mechanism for Early and Late   Dynamic Contrast Enhanced Prostate MRI Synthesis</title><categories>eess.IV cs.CV</categories><comments>Accepted at ICASSP 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Dynamic Contrast-Enhanced Magnetic Resonance Imaging (DCE-MRI) is a medical imaging technique that plays a crucial role in the detailed visualization and identification of tissue perfusion in abnormal lesions and radiological suggestions for biopsy. However, DCE-MRI involves the administration of a Gadolinium based (Gad) contrast agent, which is associated with a risk of toxicity in the body. Previous deep learning approaches that synthesize DCE-MR images employ unimodal non-contrast or low-dose contrast MRI images lacking focus on the local perfusion information within the anatomy of interest. We propose AAD-DCE, a generative adversarial network (GAN) with an aggregated attention discriminator module consisting of global and local discriminators. The discriminators provide a spatial embedded attention map to drive the generator to synthesize early and late response DCE-MRI images. Our method employs multimodal inputs - T2 weighted (T2W), Apparent Diffusion Coefficient (ADC), and T1 pre-contrast for image synthesis. Extensive comparative and ablation studies on the ProstateX dataset show that our model (i) is agnostic to various generator benchmarks and (ii) outperforms other DCE-MRI synthesis approaches with improvement margins of +0.64 dB PSNR, +0.0518 SSIM, -0.015 MAE for early response and +0.1 dB PSNR, +0.0424 SSIM, -0.021 MAE for late response, and (ii) emphasize the importance of attention ensembling. Our code is available at https://github.com/bhartidivya/AAD-DCE. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02591</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02591</id><created>2024-12-12</created><authors><author><keyname>Surmont</keyname><forenames>Florian</forenames></author><author><keyname>Coache</keyname><forenames>Damien</forenames></author></authors><title>Investigation on the Shooting Method Ability to Solve Different Mooring   Lines Boundary Condition Types</title><categories>cs.CE</categories><comments>ASME 2018 37th International Conference on Ocean, Offshore and Arctic   Engineering, Jun 2018, Madrid, France</comments><proxy>ccsd</proxy><doi>10.1115/OMAE2018-77563</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of undersea cables and mooring lines statics remains an unavoidable subject of simulation in offshore field for either steady-state analysis or dynamic simulation initialization. Whether the study concerns mooring systems pinned both at seabed and floating platform, cables towed by a moving underwater system or when special links such as stiffeners are needed, the ability to model every combination is a key point. To do so the authors propose to investigate the use of the shooting method to solve the two point boundary value problem (TPBVP) associated with Dirichlet, Robin or mixed boundary conditions representing respectively, displacement, force and force/displacement boundary conditions. 3D nonlinear static string calculations are confronted to a semi-analytic formulation established from the catenary closed form equations. The comparisons are performed on various pairs of boundary conditions developed in five configurations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02592</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02592</id><created>2024-12-17</created><authors><author><keyname>Dessena</keyname><forenames>Gabriele</forenames></author><author><keyname>Pontillo</keyname><forenames>Alessandro</forenames></author><author><keyname>Ignatyev</keyname><forenames>Dmitry I.</forenames></author><author><keyname>Whidborne</keyname><forenames>James F.</forenames></author><author><keyname>Fragonara</keyname><forenames>Luca Zanotti</forenames></author></authors><title>A Paradigm Shift to Assembly-like Finite Element Model Updating</title><categories>cs.CE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In general, there is a mismatch between a finite element model of a structure and its real behaviour. In aeronautics, this mismatch must be small because finite element models are a fundamental part of the development of an aircraft and of increasing importance with the trend to more flexible wings in modern designs. Finite element model updating can be computationally expensive for complex structures and surrogate models can be employed to reduce the computational burden. A novel approach for finite element model updating, namely assembly-like, is proposed and validated using real experimental data. The assembly-like model updating framework implies that the model is updated as parts are assembled. Benchmarking against the classical global, or one-shot, approach demonstrates that the proposed method is more computationally efficient since it takes 20% fewer iterations to obtain convergence, also using fewer parameters for the model evaluations. Despite the increase in computational performance, the new approach retains the fidelity of the global approach. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02593</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02593</id><created>2024-12-20</created><authors><author><keyname>Lei</keyname><forenames>Fan</forenames></author></authors><title>Reconstructing 3D Flow from 2D Data with Diffusion Transformer</title><categories>cs.CE cs.AI physics.flu-dyn</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fluid flow is a widely applied physical problem, crucial in various fields. Due to the highly nonlinear and chaotic nature of fluids, analyzing fluid-related problems is exceptionally challenging. Computational fluid dynamics (CFD) is the best tool for this analysis but involves significant computational resources, especially for 3D simulations, which are slow and resource-intensive. In experimental fluid dynamics, PIV cost increases with dimensionality. Reconstructing 3D flow fields from 2D PIV data could reduce costs and expand application scenarios. Here, We propose a Diffusion Transformer-based method for reconstructing 3D flow fields from 2D flow data. By embedding the positional information of 2D planes into the model, we enable the reconstruction of 3D flow fields from any combination of 2D slices, enhancing flexibility. We replace global attention with window and plane attention to reduce computational costs associated with higher dimensions without compromising performance. Our experiments demonstrate that our model can efficiently and accurately reconstruct 3D flow fields from 2D data, producing realistic results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02594</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02594</id><created>2024-12-28</created><authors><author><keyname>Ribeiro</keyname><forenames>João Alves</forenames></author><author><keyname>Ribeiro</keyname><forenames>Bruno Alves</forenames></author><author><keyname>Pimenta</keyname><forenames>Francisco</forenames></author><author><keyname>Tavares</keyname><forenames>Sérgio M. O.</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author><author><keyname>Ahmed</keyname><forenames>Faez</forenames></author></authors><title>Offshore Wind Turbine Tower Design and Optimization: A Review and   AI-Driven Future Directions</title><categories>cs.CE cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Offshore wind energy leverages the high intensity and consistency of oceanic winds, playing a key role in the transition to renewable energy. As energy demands grow, larger turbines are required to optimize power generation and reduce the Levelized Cost of Energy (LCoE), which represents the average cost of electricity over a project's lifetime. However, upscaling turbines introduces engineering challenges, particularly in the design of supporting structures, especially towers. These towers must support increased loads while maintaining structural integrity, cost-efficiency, and transportability, making them essential to offshore wind projects' success. This paper presents a comprehensive review of the latest advancements, challenges, and future directions driven by Artificial Intelligence (AI) in the design optimization of Offshore Wind Turbine (OWT) structures, with a focus on towers. It provides an in-depth background on key areas such as design types, load types, analysis methods, design processes, monitoring systems, Digital Twin (DT), software, standards, reference turbines, economic factors, and optimization techniques. Additionally, it includes a state-of-the-art review of optimization studies related to tower design optimization, presenting a detailed examination of turbine, software, loads, optimization method, design variables and constraints, analysis, and findings, motivating future research to refine design approaches for effective turbine upscaling and improved efficiency. Lastly, the paper explores future directions where AI can revolutionize tower design optimization, enabling the development of efficient, scalable, and sustainable structures. By addressing the upscaling challenges and supporting the growth of renewable energy, this work contributes to shaping the future of offshore wind turbine towers and others supporting structures. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02602</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02602</id><created>2025-01-25</created><authors><author><keyname>Chen</keyname><forenames>Sifan</forenames></author><author><keyname>Kong</keyname><forenames>Yuan</forenames></author><author><keyname>Zou</keyname><forenames>Qiang</forenames></author></authors><title>A Quasi-Optimal Shape Design Method for Lattice Structure Construction</title><categories>cs.CE math.OC</categories><acm-class>I.3.5</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Lattice structures, known for their superior mechanical properties, are widely used in industries such as aerospace, automotive, and biomedical. Their advantages primarily lie in the interconnected struts at the micro-scale. The robust construction of these struts is crucial for downstream design and manufacturing applications, as it provides a detailed shape description necessary for precise simulation and fabrication. However, constructing lattice structures presents significant challenges, particularly at nodes where multiple struts intersect. The complexity of these intersections can lead to robustness issues. To address this challenge, this paper presents an optimization-based approach that simplifies the construction of lattice structures by cutting struts and connecting them to optimized node shapes. By utilizing the recent Grey Wolf optimization method -- a type of meta-heuristic method -- for node shape design, the approach ensures robust model construction and optimal shape design. Its effectiveness has been validated through a series of case studies with increasing topological and geometric complexity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02603</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02603</id><created>2025-01-26</created><authors><author><keyname>Sun</keyname><forenames>Chunyu</forenames></author><author><keyname>Liu</keyname><forenames>Bingyu</forenames></author><author><keyname>Cui</keyname><forenames>Zhichao</forenames></author><author><keyname>Qi</keyname><forenames>Anbin</forenames></author><author><keyname>Zhang</keyname><forenames>Tian-hao</forenames></author><author><keyname>Zhou</keyname><forenames>Dinghao</forenames></author><author><keyname>Lu</keyname><forenames>Lewei</forenames></author></authors><title>SEAL: Speech Embedding Alignment Learning for Speech Large Language   Model with Retrieval-Augmented Generation</title><categories>eess.AS cs.CL cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Embedding-based retrieval models have made significant strides in retrieval-augmented generation (RAG) techniques for text and multimodal large language models (LLMs) applications. However, when it comes to speech larage language models (SLLMs), these methods are limited to a two-stage process, where automatic speech recognition (ASR) is combined with text-based retrieval. This sequential architecture suffers from high latency and error propagation. To address these limitations, we propose a unified embedding framework that eliminates the need for intermediate text representations. Specifically, the framework includes separate speech and text encoders, followed by a shared scaling layer that maps both modalities into a common embedding space. Our model reduces pipeline latency by 50\% while achieving higher retrieval accuracy compared to traditional two-stage methods. We also provide a theoretical analysis of the challenges inherent in end-to-end speech retrieval and introduce architectural principles for effective speech-to-document matching. Extensive experiments demonstrate the robustness of our approach across diverse acoustic conditions and speaker variations, paving the way for a new paradigm in multimodal SLLMs retrieval systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02605</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02605</id><created>2025-01-31</created><authors><author><keyname>Fan</keyname><forenames>Tiffany</forenames></author><author><keyname>Cutforth</keyname><forenames>Murray</forenames></author><author><keyname>D'Elia</keyname><forenames>Marta</forenames></author><author><keyname>Cortiella</keyname><forenames>Alexandre</forenames></author><author><keyname>Doostan</keyname><forenames>Alireza</forenames></author><author><keyname>Darve</keyname><forenames>Eric</forenames></author></authors><title>Physically Interpretable Representation and Controlled Generation for   Turbulence Data</title><categories>cs.CE cs.LG physics.comp-ph physics.flu-dyn</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Computational Fluid Dynamics (CFD) plays a pivotal role in fluid mechanics, enabling precise simulations of fluid behavior through partial differential equations (PDEs). However, traditional CFD methods are resource-intensive, particularly for high-fidelity simulations of complex flows, which are further complicated by high dimensionality, inherent stochasticity, and limited data availability. This paper addresses these challenges by proposing a data-driven approach that leverages a Gaussian Mixture Variational Autoencoder (GMVAE) to encode high-dimensional scientific data into low-dimensional, physically meaningful representations. The GMVAE learns a structured latent space where data can be categorized based on physical properties such as the Reynolds number while maintaining global physical consistency. To assess the interpretability of the learned representations, we introduce a novel metric based on graph spectral theory, quantifying the smoothness of physical quantities along the latent manifold. We validate our approach using 2D Navier-Stokes simulations of flow past a cylinder over a range of Reynolds numbers. Our results demonstrate that the GMVAE provides improved clustering, meaningful latent structure, and robust generative capabilities compared to baseline dimensionality reduction methods. This framework offers a promising direction for data-driven turbulence modeling and broader applications in computational fluid dynamics and engineering systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02606</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02606</id><created>2025-02-01</created><authors><author><keyname>ElSayed</keyname><forenames>Zag</forenames></author><author><keyname>Elsayed</keyname><forenames>Nelly</forenames></author><author><keyname>Abdelgawad</keyname><forenames>Ahmed</forenames></author></authors><title>Carbon Per Transistor (CPT): The Golden Formula for Green Computing   Metrics</title><categories>cs.OH cond-mat.mtrl-sci</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As computing power advances, the environmental cost of semiconductor manufacturing and operation has become a critical concern. However, current sustainability metrics fail to quantify carbon emissions at the transistor level, the fundamental building block of modern processors. This paper introduces a Carbon Per Transistor (CPT) formula -- a novel approach and green implementation metric to measuring the CO$_2$ footprint of semiconductor chips from fabrication to end-of-life. By integrating emissions from silicon crystal growth, wafer production, chip manufacturing, and operational power dissipation, the CPT formula provides a scientifically rigorous benchmark for evaluating the sustainability of computing hardware. Using real-world data from Intel Core i9-13900K, AMD Ryzen 9 7950X, and Apple M1/M2/M3 processors, we reveal a startling insight-manufacturing emissions dominate, contributing 60-125 kg CO$_2$ per CPU, far exceeding operational emissions over a typical device lifespan. Notably, Apple's high-transistor-count M-series chips, despite their energy efficiency, exhibit a significantly larger carbon footprint than traditional processors due to extensive fabrication impact. This research establishes a critical reference point for green computing initiatives, enabling industry leaders and researchers to make data-driven decisions in reducing semiconductor-related emissions and get correct estimates for the green factor of the information technology process. The proposed formula paves the way for carbon-aware chip design, regulatory standards, and future innovations in sustainable computing. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02607</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02607</id><created>2025-02-01</created><authors><author><keyname>Xue</keyname><forenames>Tianyang</forenames></author><author><keyname>Li</keyname><forenames>Haochen</forenames></author><author><keyname>Liu</keyname><forenames>Longdu</forenames></author><author><keyname>Henderson</keyname><forenames>Paul</forenames></author><author><keyname>Tang</keyname><forenames>Pengbin</forenames></author><author><keyname>Lu</keyname><forenames>Lin</forenames></author><author><keyname>Liu</keyname><forenames>Jikai</forenames></author><author><keyname>Zhao</keyname><forenames>Haisen</forenames></author><author><keyname>Peng</keyname><forenames>Hao</forenames></author><author><keyname>Bickel</keyname><forenames>Bernd</forenames></author></authors><title>MIND: Microstructure INverse Design with Generative Hybrid Neural   Representation</title><categories>cs.CV cs.GR cs.LG</categories><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The inverse design of microstructures plays a pivotal role in optimizing metamaterials with specific, targeted physical properties. While traditional forward design methods are constrained by their inability to explore the vast combinatorial design space, inverse design offers a compelling alternative by directly generating structures that fulfill predefined performance criteria. However, achieving precise control over both geometry and material properties remains a significant challenge due to their intricate interdependence. Existing approaches, which typically rely on voxel or parametric representations, often limit design flexibility and structural diversity. In this work, we present a novel generative model that integrates latent diffusion with Holoplane, an advanced hybrid neural representation that simultaneously encodes both geometric and physical properties. This combination ensures superior alignment between geometry and properties. Our approach generalizes across multiple microstructure classes, enabling the generation of diverse, tileable microstructures with significantly improved property accuracy and enhanced control over geometric validity, surpassing the performance of existing methods. We introduce a multi-class dataset encompassing a variety of geometric morphologies, including truss, shell, tube, and plate structures, to train and validate our model. Experimental results demonstrate the model's ability to generate microstructures that meet target properties, maintain geometric validity, and integrate seamlessly into complex assemblies. Additionally, we explore the potential of our framework through the generation of new microstructures, cross-class interpolation, and the infilling of heterogeneous microstructures. The dataset and source code will be open-sourced upon publication. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02610</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02610</id><created>2025-02-02</created><authors><author><keyname>Agarwal</keyname><forenames>Mehul</forenames></author><author><keyname>Agarwal</keyname><forenames>Gauri</forenames></author><author><keyname>Benoit</keyname><forenames>Santiago</forenames></author><author><keyname>Lippman</keyname><forenames>Andrew</forenames></author><author><keyname>Oh</keyname><forenames>Jean</forenames></author></authors><title>Secure &amp; Personalized Music-to-Video Generation via CHARCHA</title><categories>cs.AI cs.CV cs.HC cs.MM</categories><comments>NeurIPS 2024 Creative AI Track</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Music is a deeply personal experience and our aim is to enhance this with a fully-automated pipeline for personalized music video generation. Our work allows listeners to not just be consumers but co-creators in the music video generation process by creating personalized, consistent and context-driven visuals based on lyrics, rhythm and emotion in the music. The pipeline combines multimodal translation and generation techniques and utilizes low-rank adaptation on listeners' images to create immersive music videos that reflect both the music and the individual. To ensure the ethical use of users' identity, we also introduce CHARCHA (patent pending), a facial identity verification protocol that protects people against unauthorized use of their face while at the same time collecting authorized images from users for personalizing their videos. This paper thus provides a secure and innovative framework for creating deeply personalized music videos. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02617</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02617</id><created>2025-02-04</created><authors><author><keyname>Han</keyname><forenames>Insu</forenames></author><author><keyname>Kacham</keyname><forenames>Praneeth</forenames></author><author><keyname>Karbasi</keyname><forenames>Amin</forenames></author><author><keyname>Mirrokni</keyname><forenames>Vahab</forenames></author><author><keyname>Zandieh</keyname><forenames>Amir</forenames></author></authors><title>PolarQuant: Quantizing KV Caches with Polar Transformation</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models (LLMs) require significant memory to store Key-Value (KV) embeddings in their KV cache, especially when handling long-range contexts. Quantization of these KV embeddings is a common technique to reduce memory consumption. This work introduces PolarQuant, a novel quantization method employing random preconditioning and polar transformation. Our method transforms the KV embeddings into polar coordinates using an efficient recursive algorithm and then quantizes resulting angles. Our key insight is that, after random preconditioning, the angles in the polar representation exhibit a tightly bounded and highly concentrated distribution with an analytically computable form. This nice distribution eliminates the need for explicit normalization, a step required by traditional quantization methods which introduces significant memory overhead because quantization parameters (e.g., zero point and scale) must be stored in full precision per each data block. PolarQuant bypasses this normalization step, enabling substantial memory savings. The long-context evaluation demonstrates that PolarQuant compresses the KV cache by over x4.2 while achieving the best quality scores compared to the state-of-the-art methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02618</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02618</id><created>2025-02-04</created><authors><author><keyname>Gaya-Morey</keyname><forenames>F. Xavier</forenames></author><author><keyname>Buades-Rubio</keyname><forenames>Jose M.</forenames></author><author><keyname>Palanque</keyname><forenames>Philippe</forenames></author><author><keyname>Lacuesta</keyname><forenames>Raquel</forenames></author><author><keyname>Manresa-Yee</keyname><forenames>Cristina</forenames></author></authors><title>Deep Learning-Based Facial Expression Recognition for the Elderly: A   Systematic Review</title><categories>cs.CV cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The rapid aging of the global population has highlighted the need for technologies to support elderly, particularly in healthcare and emotional well-being. Facial expression recognition (FER) systems offer a non-invasive means of monitoring emotional states, with applications in assisted living, mental health support, and personalized care. This study presents a systematic review of deep learning-based FER systems, focusing on their applications for the elderly population. Following a rigorous methodology, we analyzed 31 studies published over the last decade, addressing challenges such as the scarcity of elderly-specific datasets, class imbalances, and the impact of age-related facial expression differences. Our findings show that convolutional neural networks remain dominant in FER, and especially lightweight versions for resource-constrained environments. However, existing datasets often lack diversity in age representation, and real-world deployment remains limited. Additionally, privacy concerns and the need for explainable artificial intelligence emerged as key barriers to adoption. This review underscores the importance of developing age-inclusive datasets, integrating multimodal solutions, and adopting XAI techniques to enhance system usability, reliability, and trustworthiness. We conclude by offering recommendations for future research to bridge the gap between academic progress and real-world implementation in elderly care. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02619</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02619</id><created>2025-02-04</created><authors><author><keyname>Karzanov</keyname><forenames>Daniil</forenames></author><author><keyname>Garzón</keyname><forenames>Rubén</forenames></author><author><keyname>Terekhov</keyname><forenames>Mikhail</forenames></author><author><keyname>Gulcehre</keyname><forenames>Caglar</forenames></author><author><keyname>Raffinot</keyname><forenames>Thomas</forenames></author><author><keyname>Detyniecki</keyname><forenames>Marcin</forenames></author></authors><title>Regret-Optimized Portfolio Enhancement through Deep Reinforcement   Learning and Future Looking Rewards</title><categories>q-fin.PM cs.LG q-fin.RM</categories><comments>11 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel agent-based approach for enhancing existing portfolio strategies using Proximal Policy Optimization (PPO). Rather than focusing solely on traditional portfolio construction, our approach aims to improve an already high-performing strategy through dynamic rebalancing driven by PPO and Oracle agents. Our target is to enhance the traditional 60/40 benchmark (60% stocks, 40% bonds) by employing the Regret-based Sharpe reward function. To address the impact of transaction fee frictions and prevent signal loss, we develop a transaction cost scheduler. We introduce a future-looking reward function and employ synthetic data training through a circular block bootstrap method to facilitate the learning of generalizable allocation strategies. We focus on two key evaluation measures: return and maximum drawdown. Given the high stochasticity of financial markets, we train 20 independent agents each period and evaluate their average performance against the benchmark. Our method not only enhances the performance of the existing portfolio strategy through strategic rebalancing but also demonstrates strong results compared to other baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02622</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02622</id><created>2025-02-04</created><authors><author><keyname>Lakshmanan</keyname><forenames>Vinith</forenames></author><author><keyname>Guichet</keyname><forenames>Xavier</forenames></author><author><keyname>Sciarretta</keyname><forenames>Antonio</forenames></author></authors><title>Backcasting the Optimal Decisions in Transport Systems: An Example with   Electric Vehicle Purchase Incentives</title><categories>math.OC cs.SY eess.SY</categories><comments>This paper has been submitted to the hEART 2025 conference and is   under review. arXiv admin note: substantial text overlap with   arXiv:2502.02204</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This study represents a first attempt to build a backcasting methodology to identify the optimal policy roadmaps in transport systems. In this methodology, desired objectives are set by decision makers at a given time horizon, and then the optimal combinations of policies to achieve these objectives are computed as a function of time (i.e., ``backcasted''). This approach is illustrated on the transportation sector by considering a specific subsystem with a single policy decision. The subsystem describes the evolution of the passenger car fleet within a given region and its impact on greenhouse gas emissions. The optimized policy is a monetary incentive for the purchase of electric vehicles while minimizing the total budget of the state and achieving a desired CO$_2$ target. A case study applied to Metropolitan France is presented to illustrate the approach. Additionally, alternative policy scenarios are also analyzed to provide further insights. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02623</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02623</id><created>2025-02-04</created><authors><author><keyname>Matilla</keyname><forenames>German Martinez</forenames></author><author><keyname>Marecek</keyname><forenames>Jakub</forenames></author></authors><title>Sample Complexity of Bias Detection with Subsampled Point-to-Subspace   Distances</title><categories>cs.LG cs.AI math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sample complexity of bias estimation is a lower bound on the runtime of any bias detection method. Many regulatory frameworks require the bias to be tested for all subgroups, whose number grows exponentially with the number of protected attributes. Unless one wishes to run a bias detection with a doubly-exponential run-time, one should like to have polynomial complexity of bias detection for a single subgroup. At the same time, the reference data may be based on surveys, and thus come with non-trivial uncertainty.   Here, we reformulate bias detection as a point-to-subspace problem on the space of measures and show that, for supremum norm, it can be subsampled efficiently. In particular, our probabilistically approximately correct (PAC) results are corroborated by tests on well-known instances. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02624</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02624</id><created>2025-02-04</created><authors><author><keyname>O'Donnell</keyname><forenames>William</forenames></author><author><keyname>Mahon</keyname><forenames>David</forenames></author><author><keyname>Yang</keyname><forenames>Guangliang</forenames></author><author><keyname>Gardner</keyname><forenames>Simon</forenames></author></authors><title>Muographic Image Upsampling with Machine Learning for Built   Infrastructure Applications</title><categories>eess.IV cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The civil engineering industry faces a critical need for innovative non-destructive evaluation methods, particularly for ageing critical infrastructure, such as bridges, where current techniques fall short. Muography, a non-invasive imaging technique, constructs three-dimensional density maps by detecting interactions of naturally occurring cosmic-ray muons within the scanned volume. Cosmic-ray muons provide deep penetration and inherent safety due to their high momenta and natural source. However, the technology's reliance on this source results in constrained muon flux, leading to prolonged acquisition times, noisy reconstructions and image interpretation challenges. To address these limitations, we developed a two-model deep learning approach. First, we employed a conditional Wasserstein generative adversarial network with gradient penalty (cWGAN-GP) to perform predictive upsampling of undersampled muography images. Using the structural similarity index measure (SSIM), 1-day sampled images matched the perceptual qualities of a 21-day image, while the peak signal-to-noise ratio (PSNR) indicated noise improvement equivalent to 31 days of sampling. A second cWGAN-GP model, trained for semantic segmentation, quantitatively assessed the upsampling model's impact on concrete sample features. This model achieved segmentation of rebar grids and tendon ducts, with Dice-S{\o}rensen accuracy coefficients of 0.8174 and 0.8663. Notably, it could mitigate or remove z-plane smearing artifacts caused by muography's inverse imaging problem. Both models were trained on a comprehensive Geant4 Monte-Carlo simulation dataset reflecting realistic civil infrastructure scenarios. Our results demonstrate significant improvements in acquisition speed and image quality, marking a substantial step toward making muography more practical for reinforced concrete infrastructure monitoring applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02625</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02625</id><created>2025-02-04</created><authors><author><keyname>Pedrielli</keyname><forenames>Samuele</forenames></author><author><keyname>Anders</keyname><forenames>Christopher J.</forenames></author><author><keyname>Funcke</keyname><forenames>Lena</forenames></author><author><keyname>Jansen</keyname><forenames>Karl</forenames></author><author><keyname>Nicoli</keyname><forenames>Kim A.</forenames></author><author><keyname>Nakajima</keyname><forenames>Shinichi</forenames></author></authors><title>Bayesian Parameter Shift Rule in Variational Quantum Eigensolvers</title><categories>cs.LG quant-ph</categories><comments>8 pages, 5 figures. arXiv admin note: text overlap with   arXiv:2502.01704</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Parameter shift rules (PSRs) are key techniques for efficient gradient estimation in variational quantum eigensolvers (VQEs). In this paper, we propose its Bayesian variant, where Gaussian processes with appropriate kernels are used to estimate the gradient of the VQE objective. Our Bayesian PSR offers flexible gradient estimation from observations at arbitrary locations with uncertainty information and reduces to the generalized PSR in special cases. In stochastic gradient descent (SGD), the flexibility of Bayesian PSR allows the reuse of observations in previous steps, which accelerates the optimization process. Furthermore, the accessibility to the posterior uncertainty, along with our proposed notion of gradient confident region (GradCoRe), enables us to minimize the observation costs in each SGD step. Our numerical experiments show that the VQE optimization with Bayesian PSR and GradCoRe significantly accelerates SGD and outperforms the state-of-the-art methods, including sequential minimal optimization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02626</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02626</id><created>2025-02-04</created><authors><author><keyname>Benz</keyname><forenames>Thomas</forenames></author><author><keyname>Scheffler</keyname><forenames>Paul</forenames></author><author><keyname>Wistoff</keyname><forenames>Nils</forenames></author><author><keyname>Sauter</keyname><forenames>Philippe</forenames></author><author><keyname>Muheim</keyname><forenames>Beat</forenames></author><author><keyname>Benini</keyname><forenames>Luca</forenames></author></authors><title>ArtistIC: An Open-Source Toolchain for Top-Metal IC Art and   Ultra-High-Fidelity GDSII Renders</title><categories>cs.OH</categories><comments>2 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Open-source projects require outreach material to grow their community, secure funds, and strengthen their influence. Numbers, specifications, and facts alone are intangible to uninvolved people; using a clear brand and appealing visual material is thus ample to reach a broad audience. This is especially true for application-specific integrated circuits (ASICs) during the early stages of the development cycle without running prototype systems. This work presents ArtistIC, an open-source framework to brand ASICs with top-metal art and to render GDSII layouts with ultra-high fidelity reaching render densities below 25 nm/px and gigapixels-scale resolutions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02628</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02628</id><created>2025-02-04</created><authors><author><keyname>Cheong</keyname><forenames>Hyunmin</forenames></author><author><keyname>Ataei</keyname><forenames>Mohammadmehdi</forenames></author><author><keyname>Khasahmadi</keyname><forenames>Amir Hosein</forenames></author><author><keyname>Jayaraman</keyname><forenames>Pradeep Kumar</forenames></author></authors><title>e-SimFT: Alignment of Generative Models with Simulation Feedback for   Pareto-Front Design Exploration</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Deep generative models have recently shown success in solving complex engineering design problems where models predict solutions that address the design requirements specified as input. However, there remains a challenge in aligning such models for effective design exploration. For many design problems, finding a solution that meets all the requirements is infeasible. In such a case, engineers prefer to obtain a set of Pareto optimal solutions with respect to those requirements, but uniform sampling of generative models may not yield a useful Pareto front. To address this gap, we introduce a new framework for Pareto-front design exploration with simulation fine-tuned generative models. First, the framework adopts preference alignment methods developed for Large Language Models (LLMs) and showcases the first application in fine-tuning a generative model for engineering design. The important distinction here is that we use a simulator instead of humans to provide accurate and scalable feedback. Next, we propose epsilon-sampling, inspired by the epsilon-constraint method used for Pareto-front generation with classical optimization algorithms, to construct a high-quality Pareto front with the fine-tuned models. Our framework, named e-SimFT, is shown to produce better-quality Pareto fronts than existing multi-objective alignment methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02629</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02629</id><created>2025-02-04</created><authors><author><keyname>Huang</keyname><forenames>Yu-An</forenames></author><author><keyname>Li</keyname><forenames>Yue-Chao</forenames></author><author><keyname>You</keyname><forenames>Hai-Ru</forenames></author><author><keyname>Pan</keyname><forenames>Jie</forenames></author><author><keyname>Cao</keyname><forenames>Xiyue</forenames></author><author><keyname>Li</keyname><forenames>Xinyuan</forenames></author><author><keyname>Huang</keyname><forenames>Zhi-An</forenames></author><author><keyname>You</keyname><forenames>Zhu-Hong</forenames></author></authors><title>Graph Structure Learning for Tumor Microenvironment with Cell Type   Annotation from non-spatial scRNA-seq data</title><categories>q-bio.GN cs.AI cs.LG</categories><comments>29 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The exploration of cellular heterogeneity within the tumor microenvironment (TME) via single-cell RNA sequencing (scRNA-seq) is essential for understanding cancer progression and response to therapy. Current scRNA-seq approaches, however, lack spatial context and rely on incomplete datasets of ligand-receptor interactions (LRIs), limiting accurate cell type annotation and cell-cell communication (CCC) inference. This study addresses these challenges using a novel graph neural network (GNN) model that enhances cell type prediction and cell interaction analysis. Our study utilized a dataset consisting of 49,020 cells from 19 patients across three cancer types: Leukemia, Breast Invasive Carcinoma, and Colorectal Cancer. The proposed scGSL model demonstrated robust performance, achieving an average accuracy of 84.83%, precision of 86.23%, recall of 81.51%, and an F1 score of 80.92% across all datasets. These metrics represent a significant enhancement over existing methods, which typically exhibit lower performance metrics. Additionally, by reviewing existing literature on gene interactions within the TME, the scGSL model proves to robustly identify biologically meaningful gene interactions in an unsupervised manner, validated by significant expression differences in key gene pairs across various cancers. The source code and data used in this paper can be found in https://github.com/LiYuechao1998/scGSL. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02630</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02630</id><created>2025-02-04</created><authors><author><keyname>Huang</keyname><forenames>Yu-An</forenames></author><author><keyname>Hu</keyname><forenames>Yao</forenames></author><author><keyname>Li</keyname><forenames>Yue-Chao</forenames></author><author><keyname>Cao</keyname><forenames>Xiyue</forenames></author><author><keyname>Li</keyname><forenames>Xinyuan</forenames></author><author><keyname>Tan</keyname><forenames>Kay Chen</forenames></author><author><keyname>You</keyname><forenames>Zhu-Hong</forenames></author><author><keyname>Huang</keyname><forenames>Zhi-An</forenames></author></authors><title>scBIT: Integrating Single-cell Transcriptomic Data into fMRI-based   Prediction for Alzheimer's Disease Diagnosis</title><categories>q-bio.QM cs.AI cs.LG</categories><comments>31 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Functional MRI (fMRI) and single-cell transcriptomics are pivotal in Alzheimer's disease (AD) research, each providing unique insights into neural function and molecular mechanisms. However, integrating these complementary modalities remains largely unexplored. Here, we introduce scBIT, a novel method for enhancing AD prediction by combining fMRI with single-nucleus RNA (snRNA). scBIT leverages snRNA as an auxiliary modality, significantly improving fMRI-based prediction models and providing comprehensive interpretability. It employs a sampling strategy to segment snRNA data into cell-type-specific gene networks and utilizes a self-explainable graph neural network to extract critical subgraphs. Additionally, we use demographic and genetic similarities to pair snRNA and fMRI data across individuals, enabling robust cross-modal learning. Extensive experiments validate scBIT's effectiveness in revealing intricate brain region-gene associations and enhancing diagnostic prediction accuracy. By advancing brain imaging transcriptomics to the single-cell level, scBIT sheds new light on biomarker discovery in AD research. Experimental results show that incorporating snRNA data into the scBIT model significantly boosts accuracy, improving binary classification by 3.39% and five-class classification by 26.59%. The codes were implemented in Python and have been released on GitHub (https://github.com/77YQ77/scBIT) and Zenodo (https://zenodo.org/records/11599030) with detailed instructions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02631</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02631</id><created>2025-02-04</created><authors><author><keyname>Liu</keyname><forenames>Zechun</forenames></author><author><keyname>Zhao</keyname><forenames>Changsheng</forenames></author><author><keyname>Huang</keyname><forenames>Hanxian</forenames></author><author><keyname>Chen</keyname><forenames>Sijia</forenames></author><author><keyname>Zhang</keyname><forenames>Jing</forenames></author><author><keyname>Zhao</keyname><forenames>Jiawei</forenames></author><author><keyname>Roy</keyname><forenames>Scott</forenames></author><author><keyname>Jin</keyname><forenames>Lisa</forenames></author><author><keyname>Xiong</keyname><forenames>Yunyang</forenames></author><author><keyname>Shi</keyname><forenames>Yangyang</forenames></author><author><keyname>Xiao</keyname><forenames>Lin</forenames></author><author><keyname>Tian</keyname><forenames>Yuandong</forenames></author><author><keyname>Soran</keyname><forenames>Bilge</forenames></author><author><keyname>Krishnamoorthi</keyname><forenames>Raghuraman</forenames></author><author><keyname>Blankevoort</keyname><forenames>Tijmen</forenames></author><author><keyname>Chandra</keyname><forenames>Vikas</forenames></author></authors><title>ParetoQ: Scaling Laws in Extremely Low-bit LLM Quantization</title><categories>cs.LG cs.AI cs.CL cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The optimal bit-width for achieving the best trade-off between quantized model size and accuracy has been a subject of ongoing debate. While some advocate for 4-bit quantization, others propose that 1.58-bit offers superior results. However, the lack of a cohesive framework for different bits has left such conclusions relatively tenuous. We present ParetoQ, the first unified framework that facilitates rigorous comparisons across 1-bit, 1.58-bit, 2-bit, 3-bit, and 4-bit quantization settings. Our findings reveal a notable learning transition between 2 and 3 bits: For 3-bits and above, the fine-tuned models stay close to their original pre-trained distributions, whereas for learning 2-bit networks or below, the representations change drastically. By optimizing training schemes and refining quantization functions, ParetoQ surpasses all previous methods tailored to specific bit widths. Remarkably, our ParetoQ ternary 600M-parameter model even outperforms the previous SoTA ternary 3B-parameter model in accuracy, using only one-fifth of the parameters. Extensive experimentation shows that ternary, 2-bit, and 3-bit quantization maintains comparable performance in the size-accuracy trade-off and generally exceeds 4-bit and binary quantization. Considering hardware constraints, 2-bit quantization offers promising potential for memory reduction and speedup. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02649</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02649</id><created>2025-02-04</created><authors><author><keyname>Mitchell</keyname><forenames>Margaret</forenames></author><author><keyname>Ghosh</keyname><forenames>Avijit</forenames></author><author><keyname>Luccioni</keyname><forenames>Alexandra Sasha</forenames></author><author><keyname>Pistilli</keyname><forenames>Giada</forenames></author></authors><title>Fully Autonomous AI Agents Should Not be Developed</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper argues that fully autonomous AI agents should not be developed. In support of this position, we build from prior scientific literature and current product marketing to delineate different AI agent levels and detail the ethical values at play in each, documenting trade-offs in potential benefits and risks. Our analysis reveals that risks to people increase with the autonomy of a system: The more control a user cedes to an AI agent, the more risks to people arise. Particularly concerning are safety risks, which affect human life and impact further values. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02657</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02657</id><created>2025-02-04</created><authors><author><keyname>Tao</keyname><forenames>Yifu</forenames></author><author><keyname>Fallon</keyname><forenames>Maurice</forenames></author></authors><title>SiLVR: Scalable Lidar-Visual Radiance Field Reconstruction with   Uncertainty Quantification</title><categories>cs.RO cs.CV</categories><comments>webpage: https://dynamic.robots.ox.ac.uk/projects/silvr/</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We present a neural radiance field (NeRF) based large-scale reconstruction system that fuses lidar and vision data to generate high-quality reconstructions that are geometrically accurate and capture photorealistic texture. Our system adopts the state-of-the-art NeRF representation to additionally incorporate lidar. Adding lidar data adds strong geometric constraints on the depth and surface normals, which is particularly useful when modelling uniform texture surfaces which contain ambiguous visual reconstruction cues. Furthermore, we estimate the epistemic uncertainty of the reconstruction as the spatial variance of each point location in the radiance field given the sensor observations from camera and lidar. This enables the identification of areas that are reliably reconstructed by each sensor modality, allowing the map to be filtered according to the estimated uncertainty. Our system can also exploit the trajectory produced by a real-time pose-graph lidar SLAM system during online mapping to bootstrap a (post-processed) Structure-from-Motion (SfM) reconstruction procedure reducing SfM training time by up to 70%. It also helps to properly constrain the overall metric scale which is essential for the lidar depth loss. The globally-consistent trajectory can then be divided into submaps using Spectral Clustering to group sets of co-visible images together. This submapping approach is more suitable for visual reconstruction than distance-based partitioning. Each submap is filtered according to point-wise uncertainty estimates and merged to obtain the final large-scale 3D reconstruction. We demonstrate the reconstruction system using a multi-camera, lidar sensor suite in experiments involving both robot-mounted and handheld scanning. Our test datasets cover a total area of more than 20,000 square metres, including multiple university buildings and an aerial survey of a multi-storey. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02659</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02659</id><created>2025-02-04</created><authors><author><keyname>Li</keyname><forenames>Yan</forenames></author><author><keyname>Zhang</keyname><forenames>Tianyi</forenames></author><author><keyname>Li</keyname><forenames>Zechuan</forenames></author><author><keyname>Han</keyname><forenames>Soyeon Caren</forenames></author></authors><title>A Training-Free Length Extrapolation Approach for LLMs: Greedy Attention   Logit Interpolation (GALI)</title><categories>cs.CL cs.AI</categories><comments>9 pages, under review in the conference</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Transformer-based Large Language Models (LLMs) struggle to process inputs exceeding their training context window, with performance degrading due to positional out-of-distribution (O.O.D.) that disrupt attention computations. Existing solutions, fine-tuning and training-free methods, are limited by computational inefficiency, attention logit outliers or loss of local positional information. To address this, we propose Greedy Attention Logit Interpolation (GALI), a training-free length extrapolation method that maximizes the utilization of pretrained positional intervals while avoiding attention logit outliers through attention logit interpolation. The result demonstrates that GALI consistently outperforms state-of-the-art training-free methods. Our findings reveal that LLMs interpret positional intervals unevenly within their training context window, suggesting that extrapolating within a smaller positional interval range yields superior results-even for short-context tasks. GALI represents a significant step toward resolving the positional O.O.D. challenge, enabling more reliable long-text understanding in LLMs. Our implementation of GALI, along with the experiments from our paper, is open-sourced at https://github.com/AcademyCityL/GALI. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02663</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02663</id><created>2025-02-04</created><authors><author><keyname>Jin</keyname><forenames>Shengmiao</forenames></author><author><keyname>Mo</keyname><forenames>Yuchen</forenames></author><author><keyname>Yuan</keyname><forenames>Wenzhen</forenames></author></authors><title>Learning to Double Guess: An Active Perception Approach for Estimating   the Center of Mass of Arbitrary Objects</title><categories>cs.RO cs.LG</categories><comments>Accepted to ICRA 25; 7 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Manipulating arbitrary objects in unstructured environments is a significant challenge in robotics, primarily due to difficulties in determining an object's center of mass. This paper introduces U-GRAPH: Uncertainty-Guided Rotational Active Perception with Haptics, a novel framework to enhance the center of mass estimation using active perception. Traditional methods often rely on single interaction and are limited by the inherent inaccuracies of Force-Torque (F/T) sensors. Our approach circumvents these limitations by integrating a Bayesian Neural Network (BNN) to quantify uncertainty and guide the robotic system through multiple, information-rich interactions via grid search and a neural network that scores each action. We demonstrate the remarkable generalizability and transferability of our method with training on a small dataset with limited variation yet still perform well on unseen complex real-world objects. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02664</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02664</id><created>2025-02-04</created><authors><author><keyname>Bukhari</keyname><forenames>S. Talha</forenames></author><author><keyname>Lawson</keyname><forenames>Daniel</forenames></author><author><keyname>Qureshi</keyname><forenames>Ahmed H.</forenames></author></authors><title>Differentiable Composite Neural Signed Distance Fields for Robot   Navigation in Dynamic Indoor Environments</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural Signed Distance Fields (SDFs) provide a differentiable environment representation to readily obtain collision checks and well-defined gradients for robot navigation tasks. However, updating neural SDFs as the scene evolves entails re-training, which is tedious, time consuming, and inefficient, making it unsuitable for robot navigation with limited field-of-view in dynamic environments. Towards this objective, we propose a compositional framework of neural SDFs to solve robot navigation in indoor environments using only an onboard RGB-D sensor. Our framework embodies a dual mode procedure for trajectory optimization, with different modes using complementary methods of modeling collision costs and collision avoidance gradients. The primary stage queries the robot body's SDF, swept along the route to goal, at the obstacle point cloud, enabling swift local optimization of trajectories. The secondary stage infers the visible scene's SDF by aligning and composing the SDF representations of its constituents, providing better informed costs and gradients for trajectory optimization. The dual mode procedure combines the best of both stages, achieving a success rate of 98%, 14.4% higher than baseline with comparable amortized plan time on iGibson 2.0. We also demonstrate its effectiveness in adapting to real-world indoor scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02666</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02666</id><created>2025-02-04</created><authors><author><keyname>Mondal</keyname><forenames>Md Safwan</forenames></author><author><keyname>Ramasamy</keyname><forenames>Subramanian</forenames></author><author><keyname>Bhounsule</keyname><forenames>Pranav</forenames></author></authors><title>Deep Reinforcement Learning Enabled Persistent Surveillance with   Energy-Aware UAV-UGV Systems for Disaster Management Applications</title><categories>cs.RO</categories><comments>Submitted</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Integrating Unmanned Aerial Vehicles (UAVs) with Unmanned Ground Vehicles (UGVs) provides an effective solution for persistent surveillance in disaster management. UAVs excel at covering large areas rapidly, but their range is limited by battery capacity. UGVs, though slower, can carry larger batteries for extended missions. By using UGVs as mobile recharging stations, UAVs can extend mission duration through periodic refueling, leveraging the complementary strengths of both systems. To optimize this energy-aware UAV-UGV cooperative routing problem, we propose a planning framework that determines optimal routes and recharging points between a UAV and a UGV. Our solution employs a deep reinforcement learning (DRL) framework built on an encoder-decoder transformer architecture with multi-head attention mechanisms. This architecture enables the model to sequentially select actions for visiting mission points and coordinating recharging rendezvous between the UAV and UGV. The DRL model is trained to minimize the age periods (the time gap between consecutive visits) of mission points, ensuring effective surveillance. We evaluate the framework across various problem sizes and distributions, comparing its performance against heuristic methods and an existing learning-based model. Results show that our approach consistently outperforms these baselines in both solution quality and runtime. Additionally, we demonstrate the DRL policy's applicability in a real-world disaster scenario as a case study and explore its potential for online mission planning to handle dynamic changes. Adapting the DRL policy for priority-driven surveillance highlights the model's generalizability for real-time disaster response. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02668</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02668</id><created>2025-02-04</created><authors><author><keyname>Eppert</keyname><forenames>Martin</forenames></author><author><keyname>Mukherjee</keyname><forenames>Satyaki</forenames></author><author><keyname>Ghoshdastidar</keyname><forenames>Debarghya</forenames></author></authors><title>Recovering Imbalanced Clusters via Gradient-Based Projection Pursuit</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Projection Pursuit is a classic exploratory technique for finding interesting projections of a dataset. We propose a method for recovering projections containing either Imbalanced Clusters or a Bernoulli-Rademacher distribution using a gradient-based technique to optimize the projection index. As sample complexity is a major limiting factor in Projection Pursuit, we analyze our algorithm's sample complexity within a Planted Vector setting where we can observe that Imbalanced Clusters can be recovered more easily than balanced ones. Additionally, we give a generalized result that works for a variety of data distributions and projection indices. We compare these results to computational lower bounds in the Low-Degree-Polynomial Framework. Finally, we experimentally evaluate our method's applicability to real-world data using FashionMNIST and the Human Activity Recognition Dataset, where our algorithm outperforms others when only a few samples are available. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02669</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02669</id><created>2025-02-04</created><authors><author><keyname>de Heij</keyname><forenames>Vincent</forenames></author><author><keyname>Niazi</keyname><forenames>M. Umar B.</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author><author><keyname>Ahmed</keyname><forenames>Saeed</forenames></author></authors><title>Distributed Prescribed-Time Observer for Nonlinear Systems</title><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a distributed prescribed-time observer for nonlinear systems representable in a block-triangular observable canonical form. Using a weighted average of neighbor estimates exchanged over a strongly connected digraph, each observer estimates the system state despite limited local sensor measurements. The proposed design guarantees that distributed state estimation errors converge to zero at a user-specified convergence time, irrespective of observers' initial conditions. To achieve this prescribed-time convergence, distributed observers implement time-varying local output injection gains that asymptotically approach infinity as the prescribed time is approached. The theoretical convergence is rigorously proven and validated through numerical simulations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02671</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02671</id><created>2025-02-04</created><authors><author><keyname>Tiapkin</keyname><forenames>Daniil</forenames></author><author><keyname>Calandriello</keyname><forenames>Daniele</forenames></author><author><keyname>Ferret</keyname><forenames>Johan</forenames></author><author><keyname>Perrin</keyname><forenames>Sarah</forenames></author><author><keyname>Vieillard</keyname><forenames>Nino</forenames></author><author><keyname>Ramé</keyname><forenames>Alexandre</forenames></author><author><keyname>Blondel</keyname><forenames>Mathieu</forenames></author></authors><title>On Teacher Hacking in Language Model Distillation</title><categories>cs.LG cs.AI cs.CL stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Post-training of language models (LMs) increasingly relies on the following two stages: (i) knowledge distillation, where the LM is trained to imitate a larger teacher LM, and (ii) reinforcement learning from human feedback (RLHF), where the LM is aligned by optimizing a reward model. In the second RLHF stage, a well-known challenge is reward hacking, where the LM over-optimizes the reward model. Such phenomenon is in line with Goodhart's law and can lead to degraded performance on the true objective. In this paper, we investigate whether a similar phenomenon, that we call teacher hacking, can occur during knowledge distillation. This could arise because the teacher LM is itself an imperfect approximation of the true distribution. To study this, we propose a controlled experimental setup involving: (i) an oracle LM representing the ground-truth distribution, (ii) a teacher LM distilled from the oracle, and (iii) a student LM distilled from the teacher. Our experiments reveal the following insights. When using a fixed offline dataset for distillation, teacher hacking occurs; moreover, we can detect it by observing when the optimization process deviates from polynomial convergence laws. In contrast, employing online data generation techniques effectively mitigates teacher hacking. More precisely, we identify data diversity as the key factor in preventing hacking. Overall, our findings provide a deeper understanding of the benefits and limitations of distillation for building robust and efficient LMs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02672</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02672</id><created>2025-02-04</created><authors><author><keyname>Jayawardhana</keyname><forenames>Mayuka</forenames><affiliation>University of Maryland</affiliation></author><author><keyname>Tu</keyname><forenames>Renbo</forenames><affiliation>University of Toronto</affiliation></author><author><keyname>Dooley</keyname><forenames>Samuel</forenames><affiliation>Meta</affiliation></author><author><keyname>Cherepanova</keyname><forenames>Valeriia</forenames><affiliation>Amazon</affiliation></author><author><keyname>Wilson</keyname><forenames>Andrew Gordon</forenames><affiliation>New York University</affiliation></author><author><keyname>Hutter</keyname><forenames>Frank</forenames><affiliation>University of Freiburg</affiliation></author><author><keyname>White</keyname><forenames>Colin</forenames><affiliation>Abacus.AI</affiliation></author><author><keyname>Goldstein</keyname><forenames>Tom</forenames><affiliation>University of Maryland</affiliation></author><author><keyname>Goldblum</keyname><forenames>Micah</forenames><affiliation>Columbia University</affiliation></author></authors><title>Transformers Boost the Performance of Decision Trees on Tabular Data   across Sample Sizes</title><categories>cs.CL cs.LG</categories><comments>12 pages, 6 figures</comments><acm-class>I.2.m; I.2.6; I.2.7</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models (LLMs) perform remarkably well on tabular datasets in zero- and few-shot settings, since they can extract meaning from natural language column headers that describe features and labels. Similarly, TabPFN, a recent non-LLM transformer pretrained on numerous tables for in-context learning, has demonstrated excellent performance for dataset sizes up to a thousand samples. In contrast, gradient-boosted decision trees (GBDTs) are typically trained from scratch on each dataset without benefiting from pretraining data and must learn the relationships between columns from their entries alone since they lack natural language understanding. LLMs and TabPFN excel on small tabular datasets where a strong prior is essential, yet they are not competitive with GBDTs on medium or large datasets, since their context lengths are limited. In this paper, we propose a simple and lightweight approach for fusing large language models and TabPFN with gradient-boosted decision trees, which allows scalable GBDTs to benefit from the natural language capabilities and pretraining of transformers. We name our fusion methods LLM-Boost and PFN-Boost, respectively. While matching or surpassing the performance of the transformer at sufficiently small dataset sizes and GBDTs at sufficiently large sizes, LLM-Boost and PFN-Boost outperform both standalone components on a wide range of dataset sizes in between. We demonstrate state-of-the-art performance against numerous baselines and ensembling algorithms. We find that PFN-Boost achieves the best average performance among all methods we test for all but very small dataset sizes. We release our code at http://github.com/MayukaJ/LLM-Boost . </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02673</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02673</id><created>2025-02-04</created><authors><author><keyname>Fallahpour</keyname><forenames>Adibvafa</forenames></author><author><keyname>Ma</keyname><forenames>Jun</forenames></author><author><keyname>Munim</keyname><forenames>Alif</forenames></author><author><keyname>Lyu</keyname><forenames>Hongwei</forenames></author><author><keyname>Wang</keyname><forenames>Bo</forenames></author></authors><title>MedRAX: Medical Reasoning Agent for Chest X-ray</title><categories>cs.LG cs.AI cs.MA</categories><comments>11 pages, 4 figures, 2 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Chest X-rays (CXRs) play an integral role in driving critical decisions in disease management and patient care. While recent innovations have led to specialized models for various CXR interpretation tasks, these solutions often operate in isolation, limiting their practical utility in clinical practice. We present MedRAX, the first versatile AI agent that seamlessly integrates state-of-the-art CXR analysis tools and multimodal large language models into a unified framework. MedRAX dynamically leverages these models to address complex medical queries without requiring additional training. To rigorously evaluate its capabilities, we introduce ChestAgentBench, a comprehensive benchmark containing 2,500 complex medical queries across 7 diverse categories. Our experiments demonstrate that MedRAX achieves state-of-the-art performance compared to both open-source and proprietary models, representing a significant step toward the practical deployment of automated CXR interpretation systems. Data and code have been publicly available at https://github.com/bowang-lab/MedRAX </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02675</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02675</id><created>2025-02-04</created><authors><author><keyname>Brockenbrough</keyname><forenames>Allan</forenames></author><author><keyname>Feild</keyname><forenames>Henry</forenames></author><author><keyname>Salinas</keyname><forenames>Dominic</forenames></author></authors><title>Exploring LLMs Impact on Student-Created User Stories and Acceptance   Testing in Software Development</title><categories>cs.SE</categories><comments>3m pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Agile software development methodology, a user story describes a new feature or functionality from an end user's perspective. The user story details may also incorporate acceptance testing criteria, which can be developed through negotiation with users. When creating stories from user feedback, the software engineer may maximize their usefulness by considering story attributes, including scope, independence, negotiability, and testability. This study investigates how LLMs (large language models), with guided instructions, affect undergraduate software engineering students' ability to transform user feedback into user stories. Students, working individually, were asked to analyze user feedback comments, appropriately group related items, and create user stories following the principles of INVEST, a framework for assessing user stories. We found that LLMs help students develop valuable stories with well-defined acceptance criteria. However, students tend to perform better without LLMs when creating user stories with an appropriate scope. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02676</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02676</id><created>2025-02-04</created><authors><author><keyname>Robinette</keyname><forenames>Preston K.</forenames></author><author><keyname>Johnson</keyname><forenames>Taylor T.</forenames></author></authors><title>Blind Visible Watermark Removal with Morphological Dilation</title><categories>cs.CV cs.CR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visible watermarks pose significant challenges for image restoration techniques, especially when the target background is unknown. Toward this end, we present MorphoMod, a novel method for automated visible watermark removal that operates in a blind setting -- without requiring target images. Unlike existing methods, MorphoMod effectively removes opaque and transparent watermarks while preserving semantic content, making it well-suited for real-world applications. Evaluations on benchmark datasets, including the Colored Large-scale Watermark Dataset (CLWD), LOGO-series, and the newly introduced Alpha1 datasets, demonstrate that MorphoMod achieves up to a 50.8% improvement in watermark removal effectiveness compared to state-of-the-art methods. Ablation studies highlight the impact of prompts used for inpainting, pre-removal filling strategies, and inpainting model performance on watermark removal. Additionally, a case study on steganographic disorientation reveals broader applications for watermark removal in disrupting high-level hidden messages. MorphoMod offers a robust, adaptable solution for watermark removal and opens avenues for further advancements in image restoration and adversarial manipulation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02679</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02679</id><created>2025-02-04</created><authors><author><keyname>Kurkova</keyname><forenames>Vera</forenames></author><author><keyname>Sanguineti</keyname><forenames>Marcello</forenames></author></authors><title>Networks with Finite VC Dimension: Pro and Contra</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Approximation and learning of classifiers of large data sets by neural networks in terms of high-dimensional geometry and statistical learning theory are investigated. The influence of the VC dimension of sets of input-output functions of networks on approximation capabilities is compared with its influence on consistency in learning from samples of data. It is shown that, whereas finite VC dimension is desirable for uniform convergence of empirical errors, it may not be desirable for approximation of functions drawn from a probability distribution modeling the likelihood that they occur in a given type of application. Based on the concentration-of-measure properties of high dimensional geometry, it is proven that both errors in approximation and empirical errors behave almost deterministically for networks implementing sets of input-output functions with finite VC dimensions in processing large data sets. Practical limitations of the universal approximation property, the trade-offs between the accuracy of approximation and consistency in learning from data, and the influence of depth of networks with ReLU units on their accuracy and consistency are discussed. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02680</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02680</id><created>2025-02-04</created><authors><author><keyname>Clementino</keyname><forenames>Thailsson</forenames></author><author><keyname>de Freitas</keyname><forenames>Rosiane</forenames></author></authors><title>Improving polynomial bounds for the Graphical Traveling Salesman Problem   with release dates on paths</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Graphical Traveling Salesman Problem with release dates (GTSP-rd) is a variation of the TSP-rd where each vertex in a weighted graph $G$ must be visited at least once, respecting the release date restriction. The edges may be traversed multiple times if necessary, as in some sparse graphs. This paper focuses on solving the GTSP-rd in paths. We consider two objective functions: minimizing the route completion time (GTSP-rd (time)) and minimizing the total distance traveled (GTSP-rd (distance)). We present improvements to existing dynamic programming algorithms, offering an $O(n)$ solution for paths where the depot is located at the extremity and an $O(n^2)$ solution for paths where the depot is located anywhere. For the GTSP-rd (distance), we propose an $O(n \log \log n)$ solution for the case with the depot at the extremity and an $O(n^2 \log \log n)$ solution for the general case. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02681</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02681</id><created>2025-02-04</created><authors><author><keyname>Ng</keyname><forenames>Lynnette Hui Xian</forenames></author><author><keyname>Cruickshank</keyname><forenames>Iain J.</forenames></author><author><keyname>Farr</keyname><forenames>David</forenames></author></authors><title>Building Bridges between Users and Content across Multiple Platforms   during Natural Disasters</title><categories>cs.SI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Social media is a primary medium for information diffusion during natural disasters. The social media ecosystem has been used to identify destruction, analyze opinions and organize aid. While the overall picture and aggregate trends may be important, a crucial part of the picture is the connections on these sites. These bridges are essential to facilitate information flow within the network. In this work, we perform a multi-platform analysis (X, Reddit, YouTube) of Hurricanes Helene and Milton, which occurred in quick session to each other in the US in late 2024. We construct network graphs to understand the properties of effective bridging content and users. We find that bridges tend to exist on X, that bridging content is complex, and that bridging users have relatable affiliations related to gender, race and job. Public organizations can use these characteristics to manage their social media personas during natural disasters more effectively. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02682</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02682</id><created>2025-02-04</created><authors><author><keyname>Chen</keyname><forenames>Keyan</forenames></author><author><keyname>Li</keyname><forenames>Yile</forenames></author><author><keyname>Long</keyname><forenames>Da</forenames></author><author><keyname>Xu</keyname><forenames>Zhitong</forenames></author><author><keyname>Xing</keyname><forenames>Wei</forenames></author><author><keyname>Hochhalter</keyname><forenames>Jacob</forenames></author><author><keyname>Zhe</keyname><forenames>Shandian</forenames></author></authors><title>Pseudo-Physics-Informed Neural Operators: Enhancing Operator Learning   from Limited Data</title><categories>cs.LG physics.comp-ph</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Neural operators have shown great potential in surrogate modeling. However, training a well-performing neural operator typically requires a substantial amount of data, which can pose a major challenge in complex applications. In such scenarios, detailed physical knowledge can be unavailable or difficult to obtain, and collecting extensive data is often prohibitively expensive. To mitigate this challenge, we propose the Pseudo Physics-Informed Neural Operator (PPI-NO) framework. PPI-NO constructs a surrogate physics system for the target system using partial differential equations (PDEs) derived from simple, rudimentary physics principles, such as basic differential operators. This surrogate system is coupled with a neural operator model, using an alternating update and learning process to iteratively enhance the model's predictive power. While the physics derived via PPI-NO may not mirror the ground-truth underlying physical laws -- hence the term ``pseudo physics'' -- this approach significantly improves the accuracy of standard operator learning models in data-scarce scenarios, which is evidenced by extensive evaluations across five benchmark tasks and a fatigue modeling application. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02683</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02683</id><created>2025-02-04</created><authors><author><keyname>Wang</keyname><forenames>Peidong</forenames></author><author><keyname>Kanda</keyname><forenames>Naoyuki</forenames></author><author><keyname>Xue</keyname><forenames>Jian</forenames></author><author><keyname>Li</keyname><forenames>Jinyu</forenames></author><author><keyname>Wang</keyname><forenames>Xiaofei</forenames></author><author><keyname>Subramanian</keyname><forenames>Aswin Shanmugam</forenames></author><author><keyname>Chen</keyname><forenames>Junkun</forenames></author><author><keyname>Sivasankaran</keyname><forenames>Sunit</forenames></author><author><keyname>Xiao</keyname><forenames>Xiong</forenames></author><author><keyname>Zhao</keyname><forenames>Yong</forenames></author></authors><title>Streaming Speaker Change Detection and Gender Classification for   Transducer-Based Multi-Talker Speech Translation</title><categories>cs.SD cs.AI cs.CL eess.AS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Streaming multi-talker speech translation is a task that involves not only generating accurate and fluent translations with low latency but also recognizing when a speaker change occurs and what the speaker's gender is. Speaker change information can be used to create audio prompts for a zero-shot text-to-speech system, and gender can help to select speaker profiles in a conventional text-to-speech model. We propose to tackle streaming speaker change detection and gender classification by incorporating speaker embeddings into a transducer-based streaming end-to-end speech translation model. Our experiments demonstrate that the proposed methods can achieve high accuracy for both speaker change detection and gender classification. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02684</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02684</id><created>2025-02-04</created><authors><author><keyname>Wang</keyname><forenames>Yisen</forenames></author><author><keyname>Cai</keyname><forenames>Hanqin</forenames></author><author><keyname>Huang</keyname><forenames>Longxiu</forenames></author></authors><title>Three-dimensional signal processing: a new approach in dynamical   sampling via tensor products</title><categories>eess.SP cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamical sampling problem is centered around reconstructing signals that evolve over time according to a dynamical process, from spatial-temporal samples that may be noisy. This topic has been thoroughly explored for one-dimensional signals. Multidimensional signal recovery has also been studied, but primarily in scenarios where the driving operator is a convolution operator. In this work, we shift our focus to the dynamical sampling problem in the context of three-dimensional signal recovery, where the evolution system can be characterized by tensor products. Specifically, we provide a necessary condition for the sampling set that ensures successful recovery of the three-dimensional signal. Furthermore, we reformulate the reconstruction problem as an optimization task, which can be solved efficiently. To demonstrate the effectiveness of our approach, we include some straightforward numerical simulations that showcase the reconstruction performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02685</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02685</id><created>2025-02-04</created><authors><author><keyname>Dutta</keyname><forenames>Tapas</forenames></author><author><keyname>Adamu-Lema</keyname><forenames>Fikru</forenames></author><author><keyname>Bensouiah</keyname><forenames>Djamel</forenames></author><author><keyname>Asenov</keyname><forenames>Asen</forenames></author></authors><title>A Methodology for Process Design Kit Re-Centering Using TCAD and   Experimental Data for Cryogenic Temperatures</title><categories>physics.app-ph cond-mat.mes-hall cs.SY eess.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we describe and demonstrate a novel Technology Computer Aided Design (TCAD) driven methodology that allows measurement data from 'non-ideal' silicon wafers to be used for re-centering a room temperature-based Process Design Kit (PDK) to cryogenic temperatures. This comprehensive approach holds promise for advancing cryogenic CMOS design in the absence of foundry supplied cryogenic PDKs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02687</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02687</id><created>2025-02-04</created><authors><author><keyname>Farzan</keyname><forenames>Siavash</forenames></author></authors><title>NDKF: A Neural-Enhanced Distributed Kalman Filter for Nonlinear   Multi-Sensor Estimation</title><categories>eess.SY cs.SY</categories><comments>Under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Neural-Enhanced Distributed Kalman Filter (NDKF) for multi-sensor state estimation in nonlinear systems. Unlike traditional Kalman filters that rely on explicit, linear models and centralized data fusion, the NDKF leverages neural networks to learn both the system dynamics and measurement functions directly from data. Each sensor node performs local prediction and update steps using these learned models and exchanges only compact summary information with its neighbors via a consensus-based fusion process, which reduces communication overhead and eliminates a single point of failure. Our theoretical convergence analysis establishes sufficient conditions under which the local linearizations of the neural models guarantee overall filter stability and provides a solid foundation for the proposed approach. Simulation studies on a 2D system with four partially observing nodes demonstrate that the NDKF significantly outperforms a distributed Extended Kalman Filter. These outcomes, as yielded by the proposed NDKF method, highlight its potential to improve the scalability, robustness, and accuracy of distributed state estimation in complex nonlinear environments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02688</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02688</id><created>2025-02-04</created><authors><author><keyname>Schmied</keyname><forenames>Margaux</forenames></author><author><keyname>Regin</keyname><forenames>Jean-Charles</forenames></author></authors><title>Efficient Implementation of the Global Cardinality Constraint with Costs</title><categories>cs.AI cs.DS</categories><comments>Published at the 30th International Conference on Principles and   Practice of Constraint Programming (CP 2024)</comments><journal-ref>30th International Conference on Principles and Practice of   Constraint Programming (CP 2024), Leibniz International Proceedings in   Informatics (LIPIcs), Volume 307, pp. 27:1--27:18</journal-ref><doi>10.4230/LIPIcs.CP.2024.27</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The success of Constraint Programming relies partly on the global constraints and implementation of the associated filtering algorithms. Recently, new ideas emerged to improve these implementations in practice, especially regarding the all different constraint. In this paper, we consider the cardinality constraint with costs. The cardinality constraint is a generalization of the all different constraint that specifies the number of times each value must be taken by a given set of variables in a solution. The version with costs introduces an assignment cost and bounds the total sum of assignment costs. The arc consistency filtering algorithm of this constraint is difficult to use in practice, as it systematically searches for many shortest paths. We propose a new approach that works with upper bounds on shortest paths based on landmarks. This approach can be seen as a preprocessing. It is fast and avoids, in practice, a large number of explicit computations of shortest paths. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02689</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02689</id><created>2025-02-04</created><authors><author><keyname>Ban</keyname><forenames>Tae-Won</forenames></author><author><keyname>Kang</keyname><forenames>Kyu-Min</forenames></author><author><keyname>Jung</keyname><forenames>Bang Chul</forenames></author></authors><title>Multidimensional Swarm Flight Approach For Chasing Unauthorized UAVs   Leveraging Asynchronous Deep Learning</title><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel unmanned aerial vehicles (UAV) chasing system designed to track and chase unauthorized UAVs, significantly enhancing their neutralization effectiveness. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02690</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02690</id><created>2025-02-04</created><authors><author><keyname>Shen</keyname><forenames>Yifan</forenames></author><author><keyname>Zhu</keyname><forenames>Peiyuan</forenames></author><author><keyname>Li</keyname><forenames>Zijian</forenames></author><author><keyname>Xie</keyname><forenames>Shaoan</forenames></author><author><keyname>Tang</keyname><forenames>Zeyu</forenames></author><author><keyname>Deka</keyname><forenames>Namrata</forenames></author><author><keyname>Liu</keyname><forenames>Zongfang</forenames></author><author><keyname>Chen</keyname><forenames>Guangyi</forenames></author><author><keyname>Zhang</keyname><forenames>Kun</forenames></author></authors><title>Controllable Video Generation with Provable Disentanglement</title><categories>cs.CV cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Controllable video generation remains a significant challenge, despite recent advances in generating high-quality and consistent videos. Most existing methods for controlling video generation treat the video as a whole, neglecting intricate fine-grained spatiotemporal relationships, which limits both control precision and efficiency. In this paper, we propose Controllable Video Generative Adversarial Networks (CoVoGAN) to disentangle the video concepts, thus facilitating efficient and independent control over individual concepts. Specifically, following the minimal change principle, we first disentangle static and dynamic latent variables. We then leverage the sufficient change property to achieve component-wise identifiability of dynamic latent variables, enabling independent control over motion and identity. To establish the theoretical foundation, we provide a rigorous analysis demonstrating the identifiability of our approach. Building on these theoretical insights, we design a Temporal Transition Module to disentangle latent dynamics. To enforce the minimal change principle and sufficient change property, we minimize the dimensionality of latent dynamic variables and impose temporal conditional independence. To validate our approach, we integrate this module as a plug-in for GANs. Extensive qualitative and quantitative experiments on various video generation benchmarks demonstrate that our method significantly improves generation quality and controllability across diverse real-world scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02692</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02692</id><created>2025-02-04</created><authors><author><keyname>Trivedi</keyname><forenames>Amit Ranjan</forenames></author><author><keyname>Tayebati</keyname><forenames>Sina</forenames></author><author><keyname>Kumawat</keyname><forenames>Hemant</forenames></author><author><keyname>Darabi</keyname><forenames>Nastaran</forenames></author><author><keyname>Kumar</keyname><forenames>Divake</forenames></author><author><keyname>Kosta</keyname><forenames>Adarsh Kumar</forenames></author><author><keyname>Venkatesha</keyname><forenames>Yeshwanth</forenames></author><author><keyname>Jayasuriya</keyname><forenames>Dinithi</forenames></author><author><keyname>Jayasinghe</keyname><forenames>Nethmi</forenames></author><author><keyname>Panda</keyname><forenames>Priyadarshini</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Saibal</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author></authors><title>Intelligent Sensing-to-Action for Robust Autonomy at the Edge:   Opportunities and Challenges</title><categories>cs.RO cs.CV cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Autonomous edge computing in robotics, smart cities, and autonomous vehicles relies on the seamless integration of sensing, processing, and actuation for real-time decision-making in dynamic environments. At its core is the sensing-to-action loop, which iteratively aligns sensor inputs with computational models to drive adaptive control strategies. These loops can adapt to hyper-local conditions, enhancing resource efficiency and responsiveness, but also face challenges such as resource constraints, synchronization delays in multi-modal data fusion, and the risk of cascading errors in feedback loops. This article explores how proactive, context-aware sensing-to-action and action-to-sensing adaptations can enhance efficiency by dynamically adjusting sensing and computation based on task demands, such as sensing a very limited part of the environment and predicting the rest. By guiding sensing through control actions, action-to-sensing pathways can improve task relevance and resource use, but they also require robust monitoring to prevent cascading errors and maintain reliability. Multi-agent sensing-action loops further extend these capabilities through coordinated sensing and actions across distributed agents, optimizing resource use via collaboration. Additionally, neuromorphic computing, inspired by biological systems, provides an efficient framework for spike-based, event-driven processing that conserves energy, reduces latency, and supports hierarchical control--making it ideal for multi-agent optimization. This article highlights the importance of end-to-end co-design strategies that align algorithmic models with hardware and environmental dynamics and improve cross-layer interdependencies to improve throughput, precision, and adaptability for energy-efficient edge autonomy in complex environments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02696</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02696</id><created>2025-02-04</created><authors><author><keyname>Galarnyk</keyname><forenames>Michael</forenames></author><author><keyname>Shah</keyname><forenames>Agam</forenames></author><author><keyname>Guhathakurta</keyname><forenames>Dipanwita</forenames></author><author><keyname>Nandigam</keyname><forenames>Poojitha</forenames></author><author><keyname>Chava</keyname><forenames>Sudheer</forenames></author></authors><title>How Inclusively do LMs Perceive Social and Moral Norms?</title><categories>cs.CL</categories><comments>Accepted at NAACL 2025 Findings</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper discusses and contains offensive content. Language models (LMs) are used in decision-making systems and as interactive assistants. However, how well do these models making judgements align with the diversity of human values, particularly regarding social and moral norms? In this work, we investigate how inclusively LMs perceive norms across demographic groups (e.g., gender, age, and income). We prompt 11 LMs on rules-of-thumb (RoTs) and compare their outputs with the existing responses of 100 human annotators. We introduce the Absolute Distance Alignment Metric (ADA-Met) to quantify alignment on ordinal questions. We find notable disparities in LM responses, with younger, higher-income groups showing closer alignment, raising concerns about the representation of marginalized perspectives. Our findings highlight the importance of further efforts to make LMs more inclusive of diverse human values. The code and prompts are available on GitHub under the CC BY-NC 4.0 license. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02700</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02700</id><created>2025-02-04</created><authors><author><keyname>Iqrah</keyname><forenames>Jurdana Masuma</forenames></author><author><keyname>Koo</keyname><forenames>Younghyun</forenames></author><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>Xie</keyname><forenames>Hongjie</forenames></author><author><keyname>Prasad</keyname><forenames>Sushil K.</forenames></author></authors><title>Scalable Higher Resolution Polar Sea Ice Classification and Freeboard   Calculation from ICESat-2 ATL03 Data</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  ICESat-2 (IS2) by NASA is an Earth-observing satellite that measures high-resolution surface elevation. The IS2's ATL07 and ATL10 sea ice elevation and freeboard products of 10m-200m segments which aggregated 150 signal photons from the raw ATL03 (geolocated photon) data. These aggregated products can potentially overestimate local sea surface height, thus underestimating the calculations of freeboard (sea ice height above sea surface). To achieve a higher resolution of sea surface height and freeboard information, in this work we utilize a 2m window to resample the ATL03 data. Then, we classify these 2m segments into thick sea ice, thin ice, and open water using deep learning methods (Long short-term memory and Multi-layer perceptron models). To obtain labeled training data for our deep learning models, we use segmented Sentinel-2 (S2) multi-spectral imagery overlapping with IS2 tracks in space and time to auto-label IS2 data, followed by some manual corrections in the regions of transition between different ice/water types or cloudy regions. We employ a parallel workflow for this auto-labeling using PySpark to scale, and we achieve 9-fold data loading and 16.25-fold map-reduce speedup. To train our models, we employ a Horovod-based distributed deep-learning workflow on a DGX A100 8 GPU cluster, achieving a 7.25-fold speedup. Next, we calculate the local sea surface heights based on the open water segments. Finally, we scale the freeboard calculation using the derived local sea level and achieve 8.54-fold data loading and 15.7-fold map-reduce speedup. Compared with the ATL07 (local sea level) and ATL10 (freeboard) data products, our results show higher resolutions and accuracy (96.56%). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02701</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02701</id><created>2025-02-04</created><authors><author><keyname>Noda</keyname><forenames>Atsushi</forenames></author><author><keyname>Isozaki</keyname><forenames>Takashi</forenames></author></authors><title>Practically Effective Adjustment Variable Selection in Causal Inference</title><categories>cs.LG cs.AI physics.data-an stat.ME</categories><comments>20 pages, 8 figures</comments><journal-ref>Journal of Physics: Complexity 6, 015001 (2025)</journal-ref><doi>10.1088/2632-072X/ada861</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the estimation of causal effects, one common method for removing the influence of confounders is to adjust the variables that satisfy the back-door criterion. However, it is not always possible to uniquely determine sets of such variables. Moreover, real-world data is almost always limited, which means it may be insufficient for statistical estimation. Therefore, we propose criteria for selecting variables from a list of candidate adjustment variables along with an algorithm to prevent accuracy degradation in causal effect estimation. We initially focus on directed acyclic graphs (DAGs) and then outlines specific steps for applying this method to completed partially directed acyclic graphs (CPDAGs). We also present and prove a theorem on causal effect computation possibility in CPDAGs. Finally, we demonstrate the practical utility of our method using both existing and artificial data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02703</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02703</id><created>2025-02-04</created><authors><author><keyname>Wang</keyname><forenames>Shenran</forenames></author><author><keyname>Yang</keyname><forenames>Changbing</forenames></author><author><keyname>Parkhill</keyname><forenames>Mike</forenames></author><author><keyname>Quinn</keyname><forenames>Chad</forenames></author><author><keyname>Hammerly</keyname><forenames>Christopher</forenames></author><author><keyname>Zhu</keyname><forenames>Jian</forenames></author></authors><title>Developing multilingual speech synthesis system for Ojibwe, Mi'kmaq, and   Maliseet</title><categories>cs.CL cs.AI cs.LG cs.SD eess.AS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present lightweight flow matching multilingual text-to-speech (TTS) systems for Ojibwe, Mi'kmaq, and Maliseet, three Indigenous languages in North America. Our results show that training a multilingual TTS model on three typologically similar languages can improve the performance over monolingual models, especially when data are scarce. Attention-free architectures are highly competitive with self-attention architecture with higher memory efficiency. Our research not only advances technical development for the revitalization of low-resource languages but also highlights the cultural gap in human evaluation protocols, calling for a more community-centered approach to human evaluation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02704</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02704</id><created>2025-02-04</created><authors><author><keyname>Laidin</keyname><forenames>Tino</forenames></author><author><keyname>Rey</keyname><forenames>Thomas</forenames></author></authors><title>A Parareal in time numerical method for the collisional Vlasov equation   in the hyperbolic scaling</title><categories>math.NA cs.NA</categories><comments>18 pages, 7 figures, 3 tables</comments><msc-class>82B40, 82C70, 76P05, 65M08</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present the design of a multiscale parareal method for kinetic equations in the fluid dynamic regime. The goal is to reduce the cost of a fully kinetic simulation using a parallel in time procedure. Using the multiscale property of kinetic models, the cheap, coarse propagator consists in a fluid solver and the fine (expensive) propagation is achieved through a kinetic solver for a collisional Vlasov equation. To validate our approach, we present simulations in the 1D in space, 3D in velocity settings over a wide range of initial data and kinetic regimes, showcasing the accuracy, efficiency, and the speedup capabilities of our method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02705</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02705</id><created>2025-02-04</created><authors><author><keyname>Yin</keyname><forenames>Patrick</forenames></author><author><keyname>Westenbroek</keyname><forenames>Tyler</forenames></author><author><keyname>Bagaria</keyname><forenames>Simran</forenames></author><author><keyname>Huang</keyname><forenames>Kevin</forenames></author><author><keyname>Cheng</keyname><forenames>Ching-an</forenames></author><author><keyname>Kobolov</keyname><forenames>Andrey</forenames></author><author><keyname>Gupta</keyname><forenames>Abhishek</forenames></author></authors><title>Rapidly Adapting Policies to the Real World via Simulation-Guided   Fine-Tuning</title><categories>cs.RO cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Robot learning requires a considerable amount of high-quality data to realize the promise of generalization. However, large data sets are costly to collect in the real world. Physics simulators can cheaply generate vast data sets with broad coverage over states, actions, and environments. However, physics engines are fundamentally misspecified approximations to reality. This makes direct zero-shot transfer from simulation to reality challenging, especially in tasks where precise and force-sensitive manipulation is necessary. Thus, fine-tuning these policies with small real-world data sets is an appealing pathway for scaling robot learning. However, current reinforcement learning fine-tuning frameworks leverage general, unstructured exploration strategies which are too inefficient to make real-world adaptation practical. This paper introduces the Simulation-Guided Fine-tuning (SGFT) framework, which demonstrates how to extract structural priors from physics simulators to substantially accelerate real-world adaptation. Specifically, our approach uses a value function learned in simulation to guide real-world exploration. We demonstrate this approach across five real-world dexterous manipulation tasks where zero-shot sim-to-real transfer fails. We further demonstrate our framework substantially outperforms baseline fine-tuning methods, requiring up to an order of magnitude fewer real-world samples and succeeding at difficult tasks where prior approaches fail entirely. Last but not least, we provide theoretical justification for this new paradigm which underpins how SGFT can rapidly learn high-performance policies in the face of large sim-to-real dynamics gaps. Project webpage: https://weirdlabuw.github.io/sgft/{weirdlabuw.github.io/sgft} </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02707</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02707</id><created>2025-02-04</created><authors><author><keyname>Wu</keyname><forenames>Shuyang</forenames></author><author><keyname>Qiu</keyname><forenames>Yifu</forenames></author><author><keyname>Nearchou</keyname><forenames>Ines P.</forenames></author><author><keyname>Prost</keyname><forenames>Sandrine</forenames></author><author><keyname>Fallowfield</keyname><forenames>Jonathan A.</forenames></author><author><keyname>Bilen</keyname><forenames>Hakan</forenames></author><author><keyname>Kendall</keyname><forenames>Timothy J.</forenames></author></authors><title>Multiple Instance Learning with Coarse-to-Fine Self-Distillation</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multiple Instance Learning (MIL) for whole slide image (WSI) analysis in computational pathology often neglects instance-level learning as supervision is typically provided only at the bag level. In this work, we present PathMIL, a framework designed to improve MIL through two perspectives: (1) employing instance-level supervision and (2) learning inter-instance contextual information on bag level. Firstly, we propose a novel Coarse-to-Fine Self-Distillation (CFSD) paradigm, to probe and distil a classifier trained with bag-level information to obtain instance-level labels which could effectively provide the supervision for the same classifier in a finer way. Secondly, to capture inter-instance contextual information in WSI, we propose Two-Dimensional Positional Encoding (2DPE), which encodes the spatial appearance of instances within a bag. We also theoretically and empirically prove the instance-level learnability of CFSD. PathMIL is evaluated on multiple benchmarking tasks, including subtype classification (TCGA-NSCLC), tumour classification (CAMELYON16), and an internal benchmark for breast cancer receptor status classification. Our method achieves state-of-the-art performance, with AUC scores of 0.9152 and 0.8524 for estrogen and progesterone receptor status classification, respectively, an AUC of 0.9618 for subtype classification, and 0.8634 for tumour classification, surpassing existing methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02708</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02708</id><created>2025-02-04</created><authors><author><keyname>Primbs</keyname><forenames>Severin</forenames></author><author><keyname>Fein</keyname><forenames>Benedikt</forenames></author><author><keyname>Fraser</keyname><forenames>Gordon</forenames></author></authors><title>AsserT5: Test Assertion Generation Using a Fine-Tuned Code Language   Model</title><categories>cs.SE</categories><comments>Accepted for AST 2025 (https://conf.researchr.org/home/ast-2025)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Writing good software tests can be challenging, therefore approaches that support developers are desirable. While generating complete tests automatically is such an approach commonly proposed in research, developers may already have specific test scenarios in mind and thus just require help in selecting the most suitable test assertions for these scenarios. This can be done using deep learning models to predict assertions for given test code. Prior research on assertion generation trained these models specifically for the task, raising the question how much the use of larger models pre-trained on code that have emerged since then can improve their performance. In particular, while abstracting identifiers has been shown to improve specifically trained models, it remains unclear whether this also generalises to models pre-trained on non-abstracted code. Finally, even though prior work demonstrated high accuracy it remains unclear how this translates into the effectiveness of the assertions at their intended application -- finding faults. To shed light on these open questions, in this paper we propose AsserT5, a new model based on the pre-trained CodeT5 model, and use this to empirically study assertion generation. We find that the abstraction and the inclusion of the focal method are useful also for a fine-tuned pre-trained model, resulting in test assertions that match the ground truth assertions precisely in up to 59.5\% of cases, more than twice as precise as prior models. However, evaluation on real bugs from the Defects4J dataset shows that out of 138 bugs detectable with assertions in real-world projects, AsserT5 was only able to suggest fault-finding assertions for 33, indicating the need for further improvements. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02709</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02709</id><created>2025-02-04</created><authors><author><keyname>Bun</keyname><forenames>Mark</forenames></author><author><keyname>Carmosino</keyname><forenames>Marco</forenames></author><author><keyname>Jain</keyname><forenames>Palak</forenames></author><author><keyname>Kaptchuk</keyname><forenames>Gabriel</forenames></author><author><keyname>Sivakumar</keyname><forenames>Satchit</forenames></author></authors><title>Enforcing Demographic Coherence: A Harms Aware Framework for Reasoning   about Private Data Release</title><categories>cs.CR cs.DB</categories><comments>42 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The technical literature about data privacy largely consists of two complementary approaches: formal definitions of conditions sufficient for privacy preservation and attacks that demonstrate privacy breaches. Differential privacy is an accepted standard in the former sphere. However, differential privacy's powerful adversarial model and worst-case guarantees may make it too stringent in some situations, especially when achieving it comes at a significant cost to data utility. Meanwhile, privacy attacks aim to expose real and worrying privacy risks associated with existing data release processes but often face criticism for being unrealistic. Moreover, the literature on attacks generally does not identify what properties are necessary to defend against them.   We address the gap between these approaches by introducing demographic coherence, a condition inspired by privacy attacks that we argue is necessary for data privacy. This condition captures privacy violations arising from inferences about individuals that are incoherent with respect to the demographic patterns in the data. Our framework focuses on confidence rated predictors, which can in turn be distilled from almost any data-informed process. Thus, we capture privacy threats that exist even when no attack is explicitly being carried out. Our framework not only provides a condition with respect to which data release algorithms can be analysed but suggests natural experimental evaluation methodologies that could be used to build practical intuition and make tangible assessment of risks. Finally, we argue that demographic coherence is weaker than differential privacy: we prove that every differentially private data release is also demographically coherent, and that there are demographically coherent algorithms which are not differentially private. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02710</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02710</id><created>2025-02-04</created><authors><author><keyname>Kostin</keyname><forenames>Julia</forenames></author><author><keyname>Gnecco</keyname><forenames>Nicola</forenames></author><author><keyname>Yang</keyname><forenames>Fanny</forenames></author></authors><title>Achievable distributional robustness when the robust risk is only   partially identified</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In safety-critical applications, machine learning models should generalize well under worst-case distribution shifts, that is, have a small robust risk. Invariance-based algorithms can provably take advantage of structural assumptions on the shifts when the training distributions are heterogeneous enough to identify the robust risk. However, in practice, such identifiability conditions are rarely satisfied -- a scenario so far underexplored in the theoretical literature. In this paper, we aim to fill the gap and propose to study the more general setting when the robust risk is only partially identifiable. In particular, we introduce the worst-case robust risk as a new measure of robustness that is always well-defined regardless of identifiability. Its minimum corresponds to an algorithm-independent (population) minimax quantity that measures the best achievable robustness under partial identifiability. While these concepts can be defined more broadly, in this paper we introduce and derive them explicitly for a linear model for concreteness of the presentation. First, we show that existing robustness methods are provably suboptimal in the partially identifiable case. We then evaluate these methods and the minimizer of the (empirical) worst-case robust risk on real-world gene expression data and find a similar trend: the test error of existing robustness methods grows increasingly suboptimal as the fraction of data from unseen environments increases, whereas accounting for partial identifiability allows for better generalization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02711</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02711</id><created>2025-02-04</created><authors><author><keyname>Guo</keyname><forenames>Zheng</forenames></author><author><keyname>Deshpande</keyname><forenames>Aditya</forenames></author><author><keyname>Kiedrowski</keyname><forenames>Brian</forenames></author><author><keyname>Wang</keyname><forenames>Xinyu</forenames></author><author><keyname>Gorodetsky</keyname><forenames>Alex</forenames></author></authors><title>Tensor Network Structure Search Using Program Synthesis</title><categories>cs.CE cs.PL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Tensor networks provide a powerful framework for compressing multi-dimensional data. The optimal tensor network structure for a given data tensor depends on both the inherent data properties and the specific optimality criteria, making tensor network structure search a crucial research problem. Existing solutions typically involve sampling and validating numerous candidate structures; this is computationally expensive, limiting their practical applications. We address this challenge by formulating tensor network structure search as a program synthesis problem and proposing a highly efficient validation method that is based on constraint solving. Specifically, we design a domain specific language: it builds the correspondence between programs and network structures, and uses a novel idea of output-directed splits to compress the search space without hindering the expressiveness. We then propose a synthesis algorithm that can prioritize promising candidates through constraint solving. % Experimental results show that our approach improves search speed by $10\times$ and achieves compression ratios by $1.5\times$ to $3\times$ better than state-of-the-art. Notably, our approach scales to larger tensors that are out of reach by prior work. Finally, we demonstrate that the discovered topologies generalize to data from the same source, achieving compression ratios up to $ 2.4\times$ better than hierarchical Tuckers while maintaining the runtime around $110$ seconds. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02715</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02715</id><created>2025-02-04</created><authors><author><keyname>More</keyname><forenames>Riddhi</forenames></author><author><keyname>Bradbury</keyname><forenames>Jeremy S.</forenames></author></authors><title>An Analysis of LLM Fine-Tuning and Few-Shot Learning for Flaky Test   Detection and Classification</title><categories>cs.SE cs.AI</categories><comments>10 pages</comments><acm-class>D.2.5; I.2.7</acm-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Flaky tests exhibit non-deterministic behavior during execution and they may pass or fail without any changes to the program under test. Detecting and classifying these flaky tests is crucial for maintaining the robustness of automated test suites and ensuring the overall reliability and confidence in the testing. However, flaky test detection and classification is challenging due to the variability in test behavior, which can depend on environmental conditions and subtle code interactions. Large Language Models (LLMs) offer promising approaches to address this challenge, with fine-tuning and few-shot learning (FSL) emerging as viable techniques. With enough data fine-tuning a pre-trained LLM can achieve high accuracy, making it suitable for organizations with more resources. Alternatively, we introduce FlakyXbert, an FSL approach that employs a Siamese network architecture to train efficiently with limited data. To understand the performance and cost differences between these two methods, we compare fine-tuning on larger datasets with FSL in scenarios restricted by smaller datasets. Our evaluation involves two existing flaky test datasets, FlakyCat and IDoFT. Our results suggest that while fine-tuning can achieve high accuracy, FSL provides a cost-effective approach with competitive accuracy, which is especially beneficial for organizations or projects with limited historical data available for training. These findings underscore the viability of both fine-tuning and FSL in flaky test detection and classification with each suited to different organizational needs and resource availability. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02716</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02716</id><created>2025-02-04</created><authors><author><keyname>Im</keyname><forenames>Shawn</forenames></author><author><keyname>Li</keyname><forenames>Yixuan</forenames></author></authors><title>A Unified Understanding and Evaluation of Steering Methods</title><categories>cs.LG cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Steering methods provide a practical approach to controlling large language models by applying steering vectors to intermediate activations, guiding outputs toward desired behaviors while avoiding retraining. Despite their growing importance, the field lacks a unified understanding and consistent evaluation across tasks and datasets, hindering progress. This paper introduces a unified framework for analyzing and evaluating steering methods, formalizing their core principles and offering theoretical insights into their effectiveness. Through comprehensive empirical evaluations on multiple-choice and open-ended text generation tasks, we validate these insights, identifying key factors that influence performance and demonstrating the superiority of certain methods. Our work bridges theoretical and practical perspectives, offering actionable guidance for advancing the design, optimization, and deployment of steering methods in LLMs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02717</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02717</id><created>2025-02-04</created><authors><author><keyname>Donoso-Oliva</keyname><forenames>Cristobal</forenames></author><author><keyname>Becker</keyname><forenames>Ignacio</forenames></author><author><keyname>Protopapas</keyname><forenames>Pavlos</forenames></author><author><keyname>Cabrera-Vives</keyname><forenames>Guillermo</forenames></author><author><keyname>Cádiz-Leyton</keyname><forenames>Martina</forenames></author><author><keyname>Moreno-Cartagena</keyname><forenames>Daniel</forenames></author></authors><title>Astromer 2</title><categories>astro-ph.IM cs.AI cs.LG</categories><comments>10 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Foundational models have emerged as a powerful paradigm in deep learning field, leveraging their capacity to learn robust representations from large-scale datasets and effectively to diverse downstream applications such as classification. In this paper, we present Astromer 2 a foundational model specifically designed for extracting light curve embeddings. We introduce Astromer 2 as an enhanced iteration of our self-supervised model for light curve analysis. This paper highlights the advantages of its pre-trained embeddings, compares its performance with that of its predecessor, Astromer 1, and provides a detailed empirical analysis of its capabilities, offering deeper insights into the model's representations. Astromer 2 is pretrained on 1.5 million single-band light curves from the MACHO survey using a self-supervised learning task that predicts randomly masked observations within sequences. Fine-tuning on a smaller labeled dataset allows us to assess its performance in classification tasks. The quality of the embeddings is measured by the F1 score of an MLP classifier trained on Astromer-generated embeddings. Our results demonstrate that Astromer 2 significantly outperforms Astromer 1 across all evaluated scenarios, including limited datasets of 20, 100, and 500 samples per class. The use of weighted per-sample embeddings, which integrate intermediate representations from Astromer's attention blocks, is particularly impactful. Notably, Astromer 2 achieves a 15% improvement in F1 score on the ATLAS dataset compared to prior models, showcasing robust generalization to new datasets. This enhanced performance, especially with minimal labeled data, underscores the potential of Astromer 2 for more efficient and scalable light curve analysis. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02718</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02718</id><created>2025-02-04</created><authors><author><keyname>Mizan</keyname><forenames>Md Rezwan Bin</forenames></author><author><keyname>Olshanskii</keyname><forenames>Maxim</forenames></author><author><keyname>Timofeyev</keyname><forenames>Ilya</forenames></author></authors><title>Parametric Reduced Order Models for the Generalized   Kuramoto--Sivashinsky Equations</title><categories>math.NA cs.NA math.AP</categories><msc-class>65M22, 65M60</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The paper studies parametric Reduced Order Models (ROMs) for the Kuramoto--Sivashinsky (KS) and generalized Kuramoto--Sivashinsky (gKS) equations. We consider several POD and POD-DEIM projection ROMs with various strategies for parameter sampling and snapshot collection. The aim is to identify an approach for constructing a ROM that is efficient across a range of parameters, encompassing several regimes exhibited by the KS and gKS solutions: weakly chaotic, transitional, and quasi-periodic dynamics. We describe such an approach and demonstrate that it is essential to develop ROMs that adequately represent the short-time transient behavior of the gKS model. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02719</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02719</id><created>2025-02-04</created><authors><author><keyname>Azzolin</keyname><forenames>Steve</forenames></author><author><keyname>Malhotra</keyname><forenames>Sagar</forenames></author><author><keyname>Passerini</keyname><forenames>Andrea</forenames></author><author><keyname>Teso</keyname><forenames>Stefano</forenames></author></authors><title>Beyond Topological Self-Explainable GNNs: A Formal Explainability   Perspective</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Self-Explainable Graph Neural Networks (SE-GNNs) are popular explainable-by-design GNNs, but the properties and the limitations of their explanations are not well understood. Our first contribution fills this gap by formalizing the explanations extracted by SE-GNNs, referred to as Trivial Explanations (TEs), and comparing them to established notions of explanations, namely Prime Implicant (PI) and faithful explanations. Our analysis reveals that TEs match PI explanations for a restricted but significant family of tasks. In general, however, they can be less informative than PI explanations and are surprisingly misaligned with widely accepted notions of faithfulness. Although faithful and PI explanations are informative, they are intractable to find and we show that they can be prohibitively large. Motivated by this, we propose Dual-Channel GNNs that integrate a white-box rule extractor and a standard SE-GNN, adaptively combining both channels when the task benefits. Our experiments show that even a simple instantiation of Dual-Channel GNNs can recover succinct rules and perform on par or better than widely used SE-GNNs. Our code can be found in the supplementary material. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02720</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02720</id><created>2025-02-04</created><authors><author><keyname>Felemban</keyname><forenames>Muhamad</forenames></author><author><keyname>Almutairi</keyname><forenames>Abdulrahman</forenames></author><author><keyname>Ghafoor</keyname><forenames>Arif</forenames></author></authors><title>Risk-Aware Sensitive Property-Driven Resource Management in Cloud   Datacenters</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Organizations are increasingly moving towards the cloud computing paradigm, in which an on-demand access to a pool of shared configurable resources is provided. However, security challenges, which are particularly exacerbated by the multitenancy and virtualization features of cloud computing, present a major obstacle. In particular, sharing of resources among potentially untrusted tenants in access controlled cloud datacenters can result in increased risk of data leakage. To address such risk, we propose an efficient risk-aware sensitive property-driven virtual resource assignment mechanism for cloud datacenters. We have used two information-theoretic measures, i.e., KL-divergence and mutual information, to represent sensitive properties in the dataset. Based on the vulnerabilities of cloud architecture and the sensitive property profile, we have formulated the problem as a cost-drive optimization problem. The problem is shown to be NP-complete. Accordingly, we have proposed two heuristics and presented simulation based performance results for cloud datacenters with multiple sensitivity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02721</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02721</id><created>2025-02-04</created><authors><author><keyname>Landman</keyname><forenames>Malena Sabaté</forenames></author><author><keyname>Brown</keyname><forenames>Ariana N.</forenames></author><author><keyname>Chung</keyname><forenames>Julianne</forenames></author><author><keyname>Nagy</keyname><forenames>James G.</forenames></author></authors><title>Randomized and Inner-product Free Krylov Methods for Large-scale Inverse   Problems</title><categories>math.NA cs.NA</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Iterative Krylov projection methods have become widely used for solving large-scale linear inverse problems. However, methods based on orthogonality include the computation of inner-products, which become costly when the number of iterations is high; are a bottleneck for parallelization; and can cause the algorithms to break down in low precision due to information loss in the projections. Recent works on inner-product free Krylov iterative algorithms alleviate these concerns, but they are quasi-minimal residual rather than minimal residual methods. This is a potential concern for inverse problems where the residual norm provides critical information from the observations via the likelihood function, and we do not have any way of controlling how close the quasi-norm is from the norm we want to minimize. In this work, we introduce a new Krylov method that is both inner-product-free and minimizes a functional that is theoretically closer to the residual norm. The proposed scheme combines an inner-product free Hessenberg projection approach for generating a solution subspace with a randomized sketch-and-solve approach for solving the resulting strongly overdetermined projected least-squares problem. Numerical results show that the proposed algorithm can solve large-scale inverse problems efficiently and without requiring inner-products. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02722</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02722</id><created>2025-02-04</created><authors><author><keyname>García-Ferrero</keyname><forenames>Iker</forenames></author></authors><title>Cross-Lingual Transfer for Low-Resource Natural Language Processing</title><categories>cs.CL</categories><comments>Doctoral Thesis: University of the Basque Country UPV/EHU</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Natural Language Processing (NLP) has seen remarkable advances in recent years, particularly with the emergence of Large Language Models that have achieved unprecedented performance across many tasks. However, these developments have mainly benefited a small number of high-resource languages such as English. The majority of languages still face significant challenges due to the scarcity of training data and computational resources. To address this issue, this thesis focuses on cross-lingual transfer learning, a research area aimed at leveraging data and models from high-resource languages to improve NLP performance for low-resource languages. Specifically, we focus on Sequence Labeling tasks such as Named Entity Recognition, Opinion Target Extraction, and Argument Mining.   The research is structured around three main objectives: (1) advancing data-based cross-lingual transfer learning methods through improved translation and annotation projection techniques, (2) developing enhanced model-based transfer learning approaches utilizing state-of-the-art multilingual models, and (3) applying these methods to real-world problems while creating open-source resources that facilitate future research in low-resource NLP.   More specifically, this thesis presents a new method to improve data-based transfer with T-Projection, a state-of-the-art annotation projection method that leverages text-to-text multilingual models and machine translation systems. T-Projection significantly outperforms previous annotation projection methods by a wide margin. For model-based transfer, we introduce a constrained decoding algorithm that enhances cross-lingual Sequence Labeling in zero-shot settings using text-to-text models. Finally, we develop Medical mT5, the first multilingual text-to-text medical model, demonstrating the practical impact of our research on real-world applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02723</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02723</id><created>2025-02-04</created><authors><author><keyname>Wang</keyname><forenames>Qinsi</forenames></author><author><keyname>Ke</keyname><forenames>Jinghan</forenames></author><author><keyname>Tomizuka</keyname><forenames>Masayoshi</forenames></author><author><keyname>Chen</keyname><forenames>Yiran</forenames></author><author><keyname>Keutzer</keyname><forenames>Kurt</forenames></author><author><keyname>Xu</keyname><forenames>Chenfeng</forenames></author></authors><title>Dobi-SVD: Differentiable SVD for LLM Compression and Some New   Perspectives</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a new LLM-compression solution via SVD, unlocking new possibilities for LLM compression beyond quantization and pruning. We point out that the optimal use of SVD lies in truncating activations, rather than merely using activations as an optimization distance. Building on this principle, we address three critical challenges in SVD-based LLM compression: including (1) How can we determine the optimal activation truncation position for each weight matrix in LLMs? (2) How can we efficiently reconstruct the weight matrices based on truncated activations? (3) How can we address the inherent "injection" nature that results in the information loss of the SVD? We propose Dobi-SVD, which establishes a new, principled approach to SVD-based LLM compression. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02725</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02725</id><created>2025-02-04</created><authors><author><keyname>Antony</keyname><forenames>Victor Nikhil</forenames></author><author><keyname>Jeon</keyname><forenames>Clara</forenames></author><author><keyname>Li</keyname><forenames>Jiasheng</forenames></author><author><keyname>Gao</keyname><forenames>Ge</forenames></author><author><keyname>Peng</keyname><forenames>Huaishu</forenames></author><author><keyname>Ostrowski</keyname><forenames>Anastasia K.</forenames></author><author><keyname>Huang</keyname><forenames>Chien-Ming</forenames></author></authors><title>The Design of On-Body Robots for Older Adults</title><categories>cs.RO cs.HC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Wearable technology has significantly improved the quality of life for older adults, and the emergence of on-body, movable robots presents new opportunities to further enhance well-being. Yet, the interaction design for these robots remains under-explored, particularly from the perspective of older adults. We present findings from a two-phase co-design process involving 13 older adults to uncover design principles for on-body robots for this population. We identify a rich spectrum of potential applications and characterize a design space to inform how on-body robots should be built for older adults. Our findings highlight the importance of considering factors like co-presence, embodiment, and multi-modal communication. Our work offers design insights to facilitate the integration of on-body robots into daily life and underscores the value of involving older adults in the co-design process to promote usability and acceptance of emerging wearable robotic technologies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02727</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02727</id><created>2025-02-04</created><authors><author><keyname>Zhang</keyname><forenames>Evan Chen. Jianing</forenames></author><author><keyname>Wang</keyname><forenames>Shiqiang</forenames></author><author><keyname>Liu</keyname><forenames>Chaoyue</forenames></author><author><keyname>Brinton</keyname><forenames>Christopher</forenames></author></authors><title>Parameter Tracking in Federated Learning with Adaptive Optimization</title><categories>cs.LG cs.AI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Federated Learning (FL), model training performance is strongly impacted by data heterogeneity across clients. Gradient Tracking (GT) has recently emerged as a solution which mitigates this issue by introducing correction terms to local model updates. To date, GT has only been considered under Stochastic Gradient Descent (SGD)-based model training, while modern FL frameworks increasingly employ adaptive optimizers for improved convergence. In this work, we generalize the GT framework to a more flexible Parameter Tracking (PT) paradigm and propose two novel adaptive optimization algorithms, {\tt FAdamET} and {\tt FAdamGT}, that integrate PT into Adam-based FL. We provide a rigorous convergence analysis of these algorithms under non-convex settings. Our experimental results demonstrate that both proposed algorithms consistently outperform existing methods when evaluating total communication cost and total computation cost across varying levels of data heterogeneity, showing the effectiveness of correcting first-order information in federated adaptive optimization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02730</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02730</id><created>2025-02-04</created><authors><author><keyname>Eisa</keyname><forenames>Mohammad</forenames></author><author><keyname>Yardley</keyname><forenames>Quentin</forenames></author><author><keyname>Witherspoon</keyname><forenames>Rafael</forenames></author><author><keyname>Pendlebury</keyname><forenames>Harriet</forenames></author><author><keyname>Rutherford</keyname><forenames>Clement</forenames></author></authors><title>Semantic Entanglement-Based Ransomware Detection via Probabilistic   Latent Encryption Mapping</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Encryption-based attacks have introduced significant challenges for detection mechanisms that rely on predefined signatures, heuristic indicators, or static rule-based classifications. Probabilistic Latent Encryption Mapping presents an alternative detection framework that models ransomware-induced encryption behaviors through statistical representations of entropy deviations and probabilistic dependencies in execution traces. Unlike conventional approaches that depend on explicit bytecode analysis or predefined cryptographic function call monitoring, probabilistic inference techniques classify encryption anomalies based on their underlying statistical characteristics, ensuring greater adaptability to polymorphic attack strategies. Evaluations demonstrate that entropy-driven classification reduces false positive rates while maintaining high detection accuracy across diverse ransomware families and encryption methodologies. Experimental results further highlight the framework's ability to differentiate between benign encryption workflows and adversarial cryptographic manipulations, ensuring that classification performance remains effective across cloud-based and localized execution environments. Benchmark comparisons illustrate that probabilistic modeling exhibits advantages over heuristic and machine learning-based detection approaches, particularly in handling previously unseen encryption techniques and adversarial obfuscation strategies. Computational efficiency analysis confirms that detection latency remains within operational feasibility constraints, reinforcing the viability of probabilistic encryption classification for real-time security infrastructures. The ability to systematically infer encryption-induced deviations without requiring static attack signatures strengthens detection robustness against adversarial evasion techniques. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02731</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02731</id><created>2025-02-04</created><authors><author><keyname>Geneson</keyname><forenames>Jesse</forenames></author><author><keyname>Tsai</keyname><forenames>Shen-Fu</forenames></author></authors><title>Fault tolerance for metric dimension and its variants</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hernando et al. (2008) introduced the fault-tolerant metric dimension $\text{ftdim}(G)$, which is the size of the smallest resolving set $S$ of a graph $G$ such that $S-\left\{s\right\}$ is also a resolving set of $G$ for every $s \in S$. They found an upper bound $\text{ftdim}(G) \le \dim(G) (1+2 \cdot 5^{\dim(G)-1})$, where $\dim(G)$ denotes the standard metric dimension of $G$. It was unknown whether there exists a family of graphs where $\text{ftdim}(G)$ grows exponentially in terms of $\dim(G)$, until recently when Knor et al. (2024) found a family with $\text{ftdim}(G) = \dim(G)+2^{\dim(G)-1}$ for any possible value of $\dim(G)$. We improve the upper bound on fault-tolerant metric dimension by showing that $\text{ftdim}(G) \le \dim(G)(1+3^{\dim(G)-1})$ for every connected graph $G$. Moreover, we find an infinite family of connected graphs $J_k$ such that $\dim(J_k) = k$ and $\text{ftdim}(J_k) \ge 3^{k-1}-k-1$ for each positive integer $k$. Together, our results show that \[\lim_{k \rightarrow \infty} \left( \max_{G: \text{ } \dim(G) = k} \frac{\log_3(\text{ftdim}(G))}{k} \right) = 1.\] In addition, we consider the fault-tolerant edge metric dimension $\text{ftedim}(G)$ and bound it with respect to the edge metric dimension $\text{edim}(G)$, showing that \[\lim_{k \rightarrow \infty} \left( \max_{G: \text{ } \text{edim}(G) = k} \frac{\log_2(\text{ftedim}(G))}{k} \right) = 1.\] We also obtain sharp extremal bounds on fault-tolerance for adjacency dimension and $k$-truncated metric dimension. Furthermore, we obtain sharp bounds for some other extremal problems about metric dimension and its variants. In particular, we prove an equivalence between an extremal problem about edge metric dimension and an open problem of Erd\H{o}s and Kleitman (1974) in extremal set theory. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02732</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02732</id><created>2025-02-04</created><authors><author><keyname>Kim</keyname><forenames>Jeonghoon</forenames></author><author><keyname>Lee</keyname><forenames>Byeongchan</forenames></author><author><keyname>Park</keyname><forenames>Cheonbok</forenames></author><author><keyname>Oh</keyname><forenames>Yeontaek</forenames></author><author><keyname>Kim</keyname><forenames>Beomjun</forenames></author><author><keyname>Yoo</keyname><forenames>Taehwan</forenames></author><author><keyname>Shin</keyname><forenames>Seongjin</forenames></author><author><keyname>Han</keyname><forenames>Dongyoon</forenames></author><author><keyname>Shin</keyname><forenames>Jinwoo</forenames></author><author><keyname>Yoo</keyname><forenames>Kang Min</forenames></author></authors><title>Peri-LN: Revisiting Layer Normalization in the Transformer Architecture</title><categories>cs.LG cs.AI cs.CL</categories><comments>Preprint</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Designing Transformer architectures with the optimal layer normalization (LN) strategy that ensures large-scale training stability and expedite convergence has remained elusive, even in this era of large language models (LLMs). To this end, we present a comprehensive analytical foundation for understanding how different LN strategies influence training dynamics in large-scale Transformer training. Until recently, Pre-LN and Post-LN have long dominated standard practices despite their limitations in large-scale training. However, several open-source large-scale models have recently begun silently adopting a third strategy without much explanation. This strategy places layer normalization (LN) peripherally around sublayers, a design we term Peri-LN. While Peri-LN has demonstrated promising empirical performance, its precise mechanisms and benefits remain almost unexplored. Our in-depth analysis shows that Peri-LN strikes an ideal balance in variance growth -- unlike Pre-LN and Post-LN, which are prone to vanishing gradients and ``massive activations.'' To validate our theoretical insight, we conduct large-scale experiments on Transformers up to 3.2B parameters, showing that Peri-LN consistently achieves more balanced variance growth, steadier gradient flow, and convergence stability. Our results suggest that Peri-LN warrants broader consideration for large-scale Transformer architectures, providing renewed insights into the optimal placement and application of LN. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02735</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02735</id><created>2025-02-04</created><authors><author><keyname>Zelaya-Arrazabal</keyname><forenames>Francisco</forenames></author><author><keyname>Martinez-Lizana</keyname><forenames>Sebastian</forenames></author><author><keyname>Pulgar-Painemal</keyname><forenames>Héctor</forenames></author></authors><title>A Modal-Based Approach for System Frequency Response and Frequency Nadir   Prediction</title><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter introduces a novel approach for predicting system frequency response and frequency nadir by leveraging modal information. It significantly differentiates from traditional methods rooted in the average system frequency model. The proposed methodology targets system modes associated with the slower dynamics of the grid, enabling precise predictions through modal decomposition applied to the full system model. This decomposition facilitates an analytical solution for the frequency at the center of inertia, resulting in highly accurate predictions of both frequency response and nadir. Numerical results from a 39-bus, 10-machine test system verify the method's effectiveness and accuracy. This methodology represents a shift from observing a simplified average system frequency response to a more detailed analysis focusing on system dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02737</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02737</id><created>2025-02-04</created><authors><author><keyname>Allal</keyname><forenames>Loubna Ben</forenames></author><author><keyname>Lozhkov</keyname><forenames>Anton</forenames></author><author><keyname>Bakouch</keyname><forenames>Elie</forenames></author><author><keyname>Blázquez</keyname><forenames>Gabriel Martín</forenames></author><author><keyname>Penedo</keyname><forenames>Guilherme</forenames></author><author><keyname>Tunstall</keyname><forenames>Lewis</forenames></author><author><keyname>Marafioti</keyname><forenames>Andrés</forenames></author><author><keyname>Kydlíček</keyname><forenames>Hynek</forenames></author><author><keyname>Lajarín</keyname><forenames>Agustín Piqueres</forenames></author><author><keyname>Srivastav</keyname><forenames>Vaibhav</forenames></author><author><keyname>Lochner</keyname><forenames>Joshua</forenames></author><author><keyname>Fahlgren</keyname><forenames>Caleb</forenames></author><author><keyname>Nguyen</keyname><forenames>Xuan-Son</forenames></author><author><keyname>Fourrier</keyname><forenames>Clémentine</forenames></author><author><keyname>Burtenshaw</keyname><forenames>Ben</forenames></author><author><keyname>Larcher</keyname><forenames>Hugo</forenames></author><author><keyname>Zhao</keyname><forenames>Haojun</forenames></author><author><keyname>Zakka</keyname><forenames>Cyril</forenames></author><author><keyname>Morlon</keyname><forenames>Mathieu</forenames></author><author><keyname>Raffel</keyname><forenames>Colin</forenames></author><author><keyname>von Werra</keyname><forenames>Leandro</forenames></author><author><keyname>Wolf</keyname><forenames>Thomas</forenames></author></authors><title>SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language   Model</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While large language models have facilitated breakthroughs in many applications of artificial intelligence, their inherent largeness makes them computationally expensive and challenging to deploy in resource-constrained settings. In this paper, we document the development of SmolLM2, a state-of-the-art "small" (1.7 billion parameter) language model (LM). To attain strong performance, we overtrain SmolLM2 on ~11 trillion tokens of data using a multi-stage training process that mixes web text with specialized math, code, and instruction-following data. We additionally introduce new specialized datasets (FineMath, Stack-Edu, and SmolTalk) at stages where we found existing datasets to be problematically small or low-quality. To inform our design decisions, we perform both small-scale ablations as well as a manual refinement process that updates the dataset mixing rates at each stage based on the performance at the previous stage. Ultimately, we demonstrate that SmolLM2 outperforms other recent small LMs including Qwen2.5-1.5B and Llama3.2-1B. To facilitate future research on LM development as well as applications of small LMs, we release both SmolLM2 as well as all of the datasets we prepared in the course of this project. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02740</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02740</id><created>2025-02-04</created><authors><author><keyname>Konyushkova</keyname><forenames>Ksenia</forenames></author><author><keyname>Kaplanis</keyname><forenames>Christos</forenames></author><author><keyname>Cabi</keyname><forenames>Serkan</forenames></author><author><keyname>Denil</keyname><forenames>Misha</forenames></author></authors><title>Vision-Language Model Dialog Games for Self-Improvement</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The increasing demand for high-quality, diverse training data poses a significant bottleneck in advancing vision-language models (VLMs). This paper presents VLM Dialog Games, a novel and scalable self-improvement framework for VLMs. Our approach leverages self-play between two agents engaged in a goal-oriented play centered around image identification. By filtering for successful game interactions, we automatically curate a high-quality dataset of interleaved images and text. We demonstrate that fine-tuning on this synthetic data leads to performance gains on downstream tasks and generalises across datasets. Moreover, as the improvements in the model lead to better game play, this procedure can be applied iteratively. This work paves the way for self-improving VLMs, with potential applications in various real-world scenarios especially when the high-quality multimodal data is scarce. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02741</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02741</id><created>2025-02-04</created><authors><author><keyname>Xie</keyname><forenames>Bin</forenames></author><author><keyname>Tang</keyname><forenames>Hao</forenames></author><author><keyname>Yan</keyname><forenames>Yan</forenames></author><author><keyname>Agam</keyname><forenames>Gady</forenames></author></authors><title>RFMedSAM 2: Automatic Prompt Refinement for Enhanced Volumetric Medical   Image Segmentation with SAM 2</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Segment Anything Model 2 (SAM 2), a prompt-driven foundation model extending SAM to both image and video domains, has shown superior zero-shot performance compared to its predecessor. Building on SAM's success in medical image segmentation, SAM 2 presents significant potential for further advancement. However, similar to SAM, SAM 2 is limited by its output of binary masks, inability to infer semantic labels, and dependence on precise prompts for the target object area. Additionally, direct application of SAM and SAM 2 to medical image segmentation tasks yields suboptimal results. In this paper, we explore the upper performance limit of SAM 2 using custom fine-tuning adapters, achieving a Dice Similarity Coefficient (DSC) of 92.30% on the BTCV dataset, surpassing the state-of-the-art nnUNet by 12%. Following this, we address the prompt dependency by investigating various prompt generators. We introduce a UNet to autonomously generate predicted masks and bounding boxes, which serve as input to SAM 2. Subsequent dual-stage refinements by SAM 2 further enhance performance. Extensive experiments show that our method achieves state-of-the-art results on the AMOS2022 dataset, with a Dice improvement of 2.9% compared to nnUNet, and outperforms nnUNet by 6.4% on the BTCV dataset. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02743</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02743</id><created>2025-02-04</created><authors><author><keyname>Li</keyname><forenames>Yang</forenames></author></authors><title>LLM Bandit: Cost-Efficient LLM Generation via Preference-Conditioned   Dynamic Routing</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The rapid advancement in large language models (LLMs) has brought forth a diverse range of models with varying capabilities that excel in different tasks and domains. However, selecting the optimal LLM for user queries often involves a challenging trade-off between accuracy and cost, a problem exacerbated by the diverse demands of individual queries. In this work, we present a novel framework that formulates the LLM selection process as a multi-armed bandit problem, enabling dynamic and intelligent routing of queries to the most appropriate model. Our approach incorporates a preference-conditioned dynamic routing mechanism, allowing users to specify their preferences at inference time, thereby offering a customizable balance between performance and cost. Additionally, our selection policy is designed to generalize to unseen LLMs, ensuring adaptability to new models as they emerge. Experimental results demonstrate that our method achieves significant improvements in both accuracy and cost-effectiveness across various LLM platforms, showcasing the potential of our framework to adaptively optimize LLM selection in real-world scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02747</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02747</id><created>2025-02-04</created><authors><author><keyname>Li</keyname><forenames>Hongwei</forenames></author><author><keyname>Tang</keyname><forenames>Yuheng</forenames></author><author><keyname>Wang</keyname><forenames>Shiqi</forenames></author><author><keyname>Guo</keyname><forenames>Wenbo</forenames></author></authors><title>PatchPilot: A Stable and Cost-Efficient Agentic Patching Framework</title><categories>cs.RO cs.AI cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research builds various patching agents that combine large language models (LLMs) with non-ML tools and achieve promising results on the state-of-the-art (SOTA) software patching benchmark, SWE-Bench. Based on how to determine the patching workflows, existing patching agents can be categorized as agent-based planning methods, which rely on LLMs for planning, and human-based planning methods, which follow a pre-defined workflow. At a high level, agent-based planning methods achieve high patching performance but with a high cost and limited stability. Human-based planning methods, on the other hand, are more stable and efficient but have key workflow limitations that compromise their patching performance. In this paper, we propose PatchPilot, an agentic patcher that strikes a balance between patching efficacy, stability, and cost-efficiency. PatchPilot proposes a novel human-based planning workflow with five components: reproduction, localization, generation, validation, and refinement (where refinement is unique to PatchPilot). We introduce novel and customized designs to each component to optimize their effectiveness and efficiency. Through extensive experiments on the SWE-Bench benchmarks, PatchPilot shows a superior performance than existing open-source methods while maintaining low cost (less than 1$ per instance) and ensuring higher stability. We also conduct a detailed ablation study to validate the key designs in each component. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02748</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02748</id><created>2025-02-04</created><authors><author><keyname>Nie</keyname><forenames>Jianan</forenames></author><author><keyname>Xiao</keyname><forenames>Peiyao</forenames></author><author><keyname>Ji</keyname><forenames>Kaiyi</forenames></author><author><keyname>Gao</keyname><forenames>Peng</forenames></author></authors><title>ReGNet: Reciprocal Space-Aware Long-Range Modeling and Multi-Property   Prediction for Crystals</title><categories>cs.LG cond-mat.mtrl-sci</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Predicting properties of crystals from their structures is a fundamental yet challenging task in materials science. Unlike molecules, crystal structures exhibit infinite periodic arrangements of atoms, requiring methods capable of capturing both local and global information effectively. However, most current works fall short of capturing long-range interactions within periodic structures. To address this limitation, we leverage reciprocal space to efficiently encode long-range interactions with learnable filters within Fourier transforms. We introduce Reciprocal Geometry Network (ReGNet), a novel architecture that integrates geometric GNNs and reciprocal blocks to model short-range and long-range interactions, respectively. Additionally, we introduce ReGNet-MT, a multi-task extension that employs mixture of experts (MoE) for multi-property prediction. Experimental results on the JARVIS and Materials Project benchmarks demonstrate that ReGNet achieves significant performance improvements. Moreover, ReGNet-MT attains state-of-the-art results on two bandgap properties due to positive transfer, while maintaining high computational efficiency. These findings highlight the potential of our model as a scalable and accurate solution for crystal property prediction. The code will be released upon paper acceptance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02749</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02749</id><created>2025-02-04</created><authors><author><keyname>Hassan</keyname><forenames>Muhammad</forenames></author><author><keyname>Jameel</keyname><forenames>Mahnoor</forenames></author><author><keyname>Wang</keyname><forenames>Tian</forenames></author><author><keyname>Bashir</keyname><forenames>Masooda</forenames></author></authors><title>Unveiling Privacy and Security Gaps in Female Health Apps</title><categories>cs.HC cs.CR</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Female Health Applications (FHA), a growing segment of FemTech, aim to provide affordable and accessible healthcare solutions for women globally. These applications gather and monitor health and reproductive data from millions of users. With ongoing debates on women's reproductive rights and privacy, it's crucial to assess how these apps protect users' privacy. In this paper, we undertake a security and data protection assessment of 45 popular FHAs. Our investigation uncovers harmful permissions, extensive collection of sensitive personal and medical data, and the presence of numerous third-party tracking libraries. Furthermore, our examination of their privacy policies reveals deviations from fundamental data privacy principles. These findings highlight a significant lack of privacy and security measures for FemTech apps, especially as women's reproductive rights face growing political challenges. The results and recommendations provide valuable insights for users, app developers, and policymakers, paving the way for better privacy and security in Female Health Applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02750</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02750</id><created>2025-02-04</created><authors><author><keyname>Zussman</keyname><forenames>Tal</forenames></author><author><keyname>Zarkadas</keyname><forenames>Ioannis</forenames></author><author><keyname>Carin</keyname><forenames>Jeremy</forenames></author><author><keyname>Cheng</keyname><forenames>Andrew</forenames></author><author><keyname>Franke</keyname><forenames>Hubertus</forenames></author><author><keyname>Pfefferle</keyname><forenames>Jonas</forenames></author><author><keyname>Cidon</keyname><forenames>Asaf</forenames></author></authors><title>Cache is King: Smart Page Eviction with eBPF</title><categories>cs.OS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The page cache is a central part of an OS. It reduces repeated accesses to storage by deciding which pages to retain in memory. As a result, the page cache has a significant impact on the performance of many applications. However, its one-size-fits-all eviction policy performs poorly in many workloads. While the systems community has experimented with a plethora of new and adaptive eviction policies in non-OS settings (e.g., key-value stores, CDNs), it is very difficult to implement such policies in the page cache, due to the complexity of modifying kernel code. To address these shortcomings, we design a novel eBPF-based framework for the Linux page cache, called $\texttt{cachebpf}$, that allows developers to customize the page cache without modifying the kernel. $\texttt{cachebpf}$ enables applications to customize the page cache policy for their specific needs, while also ensuring that different applications' policies do not interfere with each other and preserving the page cache's ability to share memory across different processes. We demonstrate the flexibility of $\texttt{cachebpf}$'s interface by using it to implement several eviction policies. Our evaluation shows that it is indeed beneficial for applications to customize the page cache to match their workloads' unique properties, and that they can achieve up to 70% higher throughput and 58% lower tail latency. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02753</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02753</id><created>2025-02-04</created><authors><author><keyname>Gao</keyname><forenames>Kai</forenames></author><author><keyname>Wang</keyname><forenames>Fan</forenames></author><author><keyname>Aduh</keyname><forenames>Erica</forenames></author><author><keyname>Randle</keyname><forenames>Dylan</forenames></author><author><keyname>Shi</keyname><forenames>Jane</forenames></author></authors><title>MuST: Multi-Head Skill Transformer for Long-Horizon Dexterous   Manipulation with Skill Progress</title><categories>cs.RO</categories><comments>Accepted by ICRA 2025 (2025 IEEE International Conference on Robotics   &amp; Automation)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robot picking and packing tasks require dexterous manipulation skills, such as rearranging objects to establish a good grasping pose, or placing and pushing items to achieve tight packing. These tasks are challenging for robots due to the complexity and variability of the required actions. To tackle the difficulty of learning and executing long-horizon tasks, we propose a novel framework called the Multi-Head Skill Transformer (MuST). This model is designed to learn and sequentially chain together multiple motion primitives (skills), enabling robots to perform complex sequences of actions effectively. MuST introduces a "progress value" for each skill, guiding the robot on which skill to execute next and ensuring smooth transitions between skills. Additionally, our model is capable of expanding its skill set and managing various sequences of sub-tasks efficiently. Extensive experiments in both simulated and real-world environments demonstrate that MuST significantly enhances the robot's ability to perform long-horizon dexterous manipulation tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02756</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02756</id><created>2025-02-04</created><authors><author><keyname>Dzikunu</keyname><forenames>Obed Korshie</forenames></author><author><keyname>Ahamed</keyname><forenames>Shadab</forenames></author><author><keyname>Toosi</keyname><forenames>Amirhossein</forenames></author><author><keyname>Li</keyname><forenames>Xiaoxiao</forenames></author><author><keyname>Rahmim</keyname><forenames>Arman</forenames></author></authors><title>Adaptive Voxel-Weighted Loss Using L1 Norms in Deep Neural Networks for   Detection and Segmentation of Prostate Cancer Lesions in PET/CT Images</title><categories>eess.IV cs.AI cs.CV</categories><comments>29 pages, 7 figures, 1 table</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This study proposes a new loss function for deep neural networks, L1-weighted Dice Focal Loss (L1DFL), that leverages L1 norms for adaptive weighting of voxels based on their classification difficulty, towards automated detection and segmentation of metastatic prostate cancer lesions in PET/CT scans. We obtained 380 PSMA [18-F] DCFPyL PET/CT scans of patients diagnosed with biochemical recurrence metastatic prostate cancer. We trained two 3D convolutional neural networks, Attention U-Net and SegResNet, and concatenated the PET and CT volumes channel-wise as input. The performance of our custom loss function was evaluated against the Dice and Dice Focal Loss functions. For clinical significance, we considered a detected region of interest (ROI) as a true positive if at least the voxel with the maximum standardized uptake value falls within the ROI. We assessed the models' performance based on the number of lesions in an image, tumour volume, activity, and extent of spread. The L1DFL outperformed the comparative loss functions by at least 13% on the test set. In addition, the F1 scores of the Dice Loss and the Dice Focal Loss were lower than that of L1DFL by at least 6% and 34%, respectively. The Dice Focal Loss yielded more false positives, whereas the Dice Loss was more sensitive to smaller volumes and struggled to segment larger lesions accurately. They also exhibited network-specific variations and yielded declines in segmentation accuracy with increased tumour spread. Our results demonstrate the potential of L1DFL to yield robust segmentation of metastatic prostate cancer lesions in PSMA PET/CT images. The results further highlight potential complexities arising from the variations in lesion characteristics that may influence automated prostate cancer tumour detection and segmentation. The code is publicly available at: https://github.com/ObedDzik/pca_segment.git. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02757</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02757</id><created>2025-02-04</created><authors><author><keyname>Liu</keyname><forenames>Chunhua</forenames></author><author><keyname>Lin</keyname><forenames>Hong Yi</forenames></author><author><keyname>Thongtanunam</keyname><forenames>Patanamon</forenames></author></authors><title>Too Noisy To Learn: Enhancing Data Quality for Code Review C</title><categories>cs.SE</categories><comments>The paper is published at the International Conference on Mining   Software Repositories (MSR2025)</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Code review is an important practice in software development, yet it is time-consuming and requires substantial effort. While open-source datasets have been used to train neural models for automating code review tasks, including review comment generation, these datasets contain a significant amount of noisy comments (e.g., vague or non-actionable feedback) that persist despite cleaning methods using heuristics and machine learning approaches. Such remaining noise may lead models to generate low-quality review comments, yet removing them requires a complex semantic understanding of both code changes and natural language comments. In this paper, we investigate the impact of such noise on review comment generation and propose a novel approach using large language models (LLMs) to further clean these datasets. Based on an empirical study on a large-scale code review dataset, our LLM-based approach achieves 66-85% precision in detecting valid comments. Using the predicted valid comments to fine-tune the state-of-the-art code review models (cleaned models) can generate review comments that are 13.0% - 12.4% more similar to valid human-written comments than the original models. We also find that the cleaned models can generate more informative and relevant comments than the original models. Our findings underscore the critical impact of dataset quality on the performance of review comment generation. We advocate for further research into cleaning training data to enhance the practical utility and quality of automated code review. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02759</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02759</id><created>2025-02-04</created><authors><author><keyname>Joyce</keyname><forenames>Robert J.</forenames></author><author><keyname>Everett</keyname><forenames>Derek</forenames></author><author><keyname>Fuchs</keyname><forenames>Maya</forenames></author><author><keyname>Raff</keyname><forenames>Edward</forenames></author><author><keyname>Holt</keyname><forenames>James</forenames></author></authors><title>ClarAVy: A Tool for Scalable and Accurate Malware Family Labeling</title><categories>cs.CR</categories><doi>10.1145/3701716.3715212</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determining the family to which a malicious file belongs is an essential component of cyberattack investigation, attribution, and remediation. Performing this task manually is time consuming and requires expert knowledge. Automated tools using that label malware using antivirus detections lack accuracy and/or scalability, making them insufficient for real-world applications. Three pervasive shortcomings in these tools are responsible: (1) incorrect parsing of antivirus detections, (2) errors during family alias resolution, and (3) an inappropriate antivirus aggregation strategy. To address each of these, we created our own malware family labeling tool called ClarAVy. ClarAVy utilizes a Variational Bayesian approach to aggregate detections from a collection of antivirus products into accurate family labels. Our tool scales to enormous malware datasets, and we evaluated it by labeling $\approx$40 million malicious files. ClarAVy has 8 and 12 percentage points higher accuracy than the prior leading tool in labeling the MOTIF and MalPedia datasets, respectively. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02760</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02760</id><created>2025-02-04</created><authors><author><keyname>Schroeder</keyname><forenames>Tom</forenames></author><author><keyname>Phan</keyname><forenames>Minh</forenames></author><author><keyname>Chen</keyname><forenames>Yang</forenames></author></authors><title>A Preliminary Study of Fixed Flaky Tests in Rust Projects on GitHub</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prior research has extensively studied flaky tests in various domains, such as web applications, mobile applications, and other open-source projects in a range of multiple programing languages, including Java, Javascript, Python, Ruby, and more. However, little attention has been given to flaky tests in Rust -- an emerging popular language known for its safety features relative to C/C++. Rust incorporates interesting features that make it easy to detect some flaky tests, e.g., the Rust standard randomizes the order of elements in hash tables, effectively exposing implementation-dependent flakiness. However, Rust still has several sources of nondeterminism that can lead to flaky tests. We present our work-in-progress on studying flaky tests in Rust projects on GitHub. Searching through the closed Github issues and pull requests. We focus on flaky tests that are fixed, not just reported, as the fixes can offer valuable information on root causes, manifestation characteristics, and strategies of fixes. By far, we have inspected 53 tests. Our initial findings indicate that the predominant root causes include asynchronous wait (33.9%), concurrency issues (24.5%), logic errors (9.4%). and network-related problems (9.4%). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02761</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02761</id><created>2025-02-04</created><authors><author><keyname>Van Nguyen</keyname><forenames>Anh</forenames></author><author><keyname>Klabjan</keyname><forenames>Diego</forenames></author><author><keyname>Ryu</keyname><forenames>Minseok</forenames></author><author><keyname>Kim</keyname><forenames>Kibaek</forenames></author><author><keyname>Di</keyname><forenames>Zichao</forenames></author></authors><title>Federated Low-Rank Tensor Estimation for Multimodal Image Reconstruction</title><categories>cs.LG cs.CV cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-rank tensor estimation offers a powerful approach to addressing high-dimensional data challenges and can substantially improve solutions to ill-posed inverse problems, such as image reconstruction under noisy or undersampled conditions. Meanwhile, tensor decomposition has gained prominence in federated learning (FL) due to its effectiveness in exploiting latent space structure and its capacity to enhance communication efficiency. In this paper, we present a federated image reconstruction method that applies Tucker decomposition, incorporating joint factorization and randomized sketching to manage large-scale, multimodal data. Our approach avoids reconstructing full-size tensors and supports heterogeneous ranks, allowing clients to select personalized decomposition ranks based on prior knowledge or communication capacity. Numerical results demonstrate that our method achieves superior reconstruction quality and communication compression compared to existing approaches, thereby highlighting its potential for multimodal inverse problems in the FL setting. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02763</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02763</id><created>2025-02-04</created><authors><author><keyname>Traub</keyname><forenames>Manuel</forenames></author><author><keyname>Butz</keyname><forenames>Martin V.</forenames></author></authors><title>Rethinking Vision Transformer for Object Centric Foundation Models</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent state-of-the-art object segmentation mechanisms, such as the Segment Anything Model (SAM) and FastSAM, first encode the full image over several layers and then focus on generating the mask for one particular object or area. We present an off-grid Fovea-Like Input Patching (FLIP) approach, which selects image input and encodes it from the beginning in an object-focused manner. While doing so, it separates locational encoding from an object-centric perceptual code. FLIP is more data-efficient and yields improved segmentation performance when masking relatively small objects in high-resolution visual scenes. On standard benchmarks such as Hypersim, KITTI-360, and OpenImages, FLIP achieves Intersection over Union (IoU) scores that approach the performance of SAM with much less compute effort. It surpasses FastSAM in all IoU measurements. We also introduce an additional semi-natural but highly intuitive dataset where FLIP outperforms SAM and FastSAM overall and particularly on relatively small objects. Seeing that FLIP is an end-to-end object-centric segmentation approach, it has high potential particularly for applications that benefit from computationally efficient, spatially highly selective object tracking. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02764</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02764</id><created>2025-02-04</created><authors><author><keyname>S</keyname><forenames>Karthik Somayaji N.</forenames></author><author><keyname>Li</keyname><forenames>Peng</forenames></author></authors><title>LLM-USO: Large Language Model-based Universal Sizing Optimizer</title><categories>cs.AR cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The design of analog circuits is a cornerstone of integrated circuit (IC) development, requiring the optimization of complex, interconnected sub-structures such as amplifiers, comparators, and buffers. Traditionally, this process relies heavily on expert human knowledge to refine design objectives by carefully tuning sub-components while accounting for their interdependencies. Existing methods, such as Bayesian Optimization (BO), offer a mathematically driven approach for efficiently navigating large design spaces. However, these methods fall short in two critical areas compared to human expertise: (i) they lack the semantic understanding of the sizing solution space and its direct correlation with design objectives before optimization, and (ii) they fail to reuse knowledge gained from optimizing similar sub-structures across different circuits. To overcome these limitations, we propose the Large Language Model-based Universal Sizing Optimizer (LLM-USO), which introduces a novel method for knowledge representation to encode circuit design knowledge in a structured text format. This representation enables the systematic reuse of optimization insights for circuits with similar sub-structures. LLM-USO employs a hybrid framework that integrates BO with large language models (LLMs) and a learning summary module. This approach serves to: (i) infuse domain-specific knowledge into the BO process and (ii) facilitate knowledge transfer across circuits, mirroring the cognitive strategies of expert designers. Specifically, LLM-USO constructs a knowledge summary mechanism to distill and apply design insights from one circuit to related ones. It also incorporates a knowledge summary critiquing mechanism to ensure the accuracy and quality of the summaries and employs BO-guided suggestion filtering to identify optimal design points efficiently. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02766</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02766</id><created>2025-02-04</created><authors><author><keyname>Zhang</keyname><forenames>Shihao</forenames></author><author><keyname>Saab</keyname><forenames>Rayan</forenames></author></authors><title>Theoretical Guarantees for Low-Rank Compression of Deep Neural Networks</title><categories>cs.LG cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks have achieved state-of-the-art performance across numerous applications, but their high memory and computational demands present significant challenges, particularly in resource-constrained environments. Model compression techniques, such as low-rank approximation, offer a promising solution by reducing the size and complexity of these networks while only minimally sacrificing accuracy. In this paper, we develop an analytical framework for data-driven post-training low-rank compression. We prove three recovery theorems under progressively weaker assumptions about the approximate low-rank structure of activations, modeling deviations via noise. Our results represent a step toward explaining why data-driven low-rank compression methods outperform data-agnostic approaches and towards theoretically grounded compression algorithms that reduce inference costs while maintaining performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02767</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02767</id><created>2025-02-04</created><authors><author><keyname>Xiao</keyname><forenames>Madelyne</forenames></author><author><keyname>Sellars</keyname><forenames>Andrew</forenames></author><author><keyname>Scheffler</keyname><forenames>Sarah</forenames></author></authors><title>When Anti-Fraud Laws Become a Barrier to Computer Science Research</title><categories>cs.CY</categories><comments>ACM CS+Law '25, to appear</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Computer science research sometimes brushes with the law, from red-team exercises that probe the boundaries of authentication mechanisms, to AI research processing copyrighted material, to platform research measuring the behavior of algorithms and users. U.S.-based computer security research is no stranger to the Computer Fraud and Abuse Act (CFAA) and the Digital Millennium Copyright Act (DMCA) in a relationship that is still evolving through case law, research practices, changing policies, and legislation.   Amid the landscape computer scientists, lawyers, and policymakers have learned to navigate, anti-fraud laws are a surprisingly under-examined challenge for computer science research. Fraud brings separate issues that are not addressed by the methods for navigating CFAA, DMCA, and Terms of Service that are more familiar in the computer security literature. Although anti-fraud laws have been discussed to a limited extent in older research on phishing attacks, modern computer science researchers are left with little guidance when it comes to navigating issues of deception outside the context of pure laboratory research.   In this paper, we analyze and taxonomize the anti-fraud and deception issues that arise in several areas of computer science research. We find that, despite the lack of attention to these issues in the legal and computer science literature, issues of misrepresented identity or false information that could implicate anti-fraud laws are actually relevant to many methodologies used in computer science research, including penetration testing, web scraping, user studies, sock puppets, social engineering, auditing AI or socio-technical systems, and attacks on artificial intelligence. We especially highlight the importance of anti-fraud laws in two research fields of great policy importance: attacking or auditing AI systems, and research involving legal identification. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02768</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02768</id><created>2025-02-04</created><authors><author><keyname>Mangannavar</keyname><forenames>Rajesh</forenames></author></authors><title>Planning with affordances: Integrating learned affordance models and   symbolic planning</title><categories>cs.AI cs.RO</categories><comments>10 pages, 2 figures</comments><acm-class>I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intelligent agents working in real-world environments must be able to learn about the environment and its capabilities which enable them to take actions to change to the state of the world to complete a complex multi-step task in a photorealistic environment. Learning about the environment is especially important to perform various multiple-step tasks without having to redefine an agent's action set for different tasks or environment settings. In our work, we augment an existing task and motion planning framework with learned affordance models of objects in the world to enable planning and executing multi-step tasks using learned models. Each task can be seen as changing the current state of the world to a given goal state. The affordance models provide us with what actions are possible and how to perform those actions in any given state. A symbolic planning algorithm uses this information and the starting and goal state to create a feasible plan to reach the desired goal state to complete a given task. We demonstrate our approach in a virtual 3D photorealistic environment, AI2-Thor, and evaluate it on real-world tasks. Our results show that our agent quickly learns how to interact with the environment and is well prepared to perform tasks such as "Moving an object out of the way to reach the desired location." </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02770</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02770</id><created>2025-02-04</created><authors><author><keyname>Lin</keyname><forenames>Chaofan</forenames></author><author><keyname>Tang</keyname><forenames>Jiaming</forenames></author><author><keyname>Yang</keyname><forenames>Shuo</forenames></author><author><keyname>Wang</keyname><forenames>Hanshuo</forenames></author><author><keyname>Tang</keyname><forenames>Tian</forenames></author><author><keyname>Tian</keyname><forenames>Boyu</forenames></author><author><keyname>Stoica</keyname><forenames>Ion</forenames></author><author><keyname>Han</keyname><forenames>Song</forenames></author><author><keyname>Gao</keyname><forenames>Mingyu</forenames></author></authors><title>Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning</title><categories>cs.LG cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been a hot research topic. However, current algorithms such as sparse attention or key-value (KV) cache compression tend to use a fixed budget, which presents a significant challenge during deployment because it fails to account for the dynamic nature of real-world scenarios, where the optimal balance between accuracy and efficiency can vary greatly. In this paper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse attention can surprisingly achieve adaptive budgeting. Based on this, we propose Twilight, a framework to bring adaptive sparsity to any existing sparse attention algorithm without sacrificing their accuracy. Empirical results show that Twilight can adaptively prune at most 98% of redundant tokens, leading to $15.4\times$ acceleration in self-attention operations and $3.9\times$ acceleration in end-to-end per token latency in long context LLM decoding. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02771</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02771</id><created>2025-02-04</created><authors><author><keyname>Cheung</keyname><forenames>Matt Y.</forenames></author><author><keyname>Zorek</keyname><forenames>Sophia</forenames></author><author><keyname>Netherton</keyname><forenames>Tucker J.</forenames></author><author><keyname>Court</keyname><forenames>Laurence E.</forenames></author><author><keyname>Al-Kindi</keyname><forenames>Sadeer</forenames></author><author><keyname>Veeraraghavan</keyname><forenames>Ashok</forenames></author><author><keyname>Balakrishnan</keyname><forenames>Guha</forenames></author></authors><title>When are Diffusion Priors Helpful in Sparse Reconstruction? A Study with   Sparse-view CT</title><categories>physics.med-ph cs.CV cs.LG eess.IV stat.AP</categories><comments>Accepted at IEEE ISBI 2025, 5 pages, 2 figures, 1 table</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diffusion models demonstrate state-of-the-art performance on image generation, and are gaining traction for sparse medical image reconstruction tasks. However, compared to classical reconstruction algorithms relying on simple analytical priors, diffusion models have the dangerous property of producing realistic looking results \emph{even when incorrect}, particularly with few observations. We investigate the utility of diffusion models as priors for image reconstruction by varying the number of observations and comparing their performance to classical priors (sparse and Tikhonov regularization) using pixel-based, structural, and downstream metrics. We make comparisons on low-dose chest wall computed tomography (CT) for fat mass quantification. First, we find that classical priors are superior to diffusion priors when the number of projections is ``sufficient''. Second, we find that diffusion priors can capture a large amount of detail with very few observations, significantly outperforming classical priors. However, they fall short of capturing all details, even with many observations. Finally, we find that the performance of diffusion priors plateau after extremely few ($\approx$10-15) projections. Ultimately, our work highlights potential issues with diffusion-based sparse reconstruction and underscores the importance of further investigation, particularly in high-stakes clinical settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02772</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02772</id><created>2025-02-04</created><authors><author><keyname>Tejwani</keyname><forenames>Ravi</forenames></author><author><keyname>Velazquez</keyname><forenames>Karl</forenames></author><author><keyname>Payne</keyname><forenames>John</forenames></author><author><keyname>Bonato</keyname><forenames>Paolo</forenames></author><author><keyname>Asada</keyname><forenames>Harry</forenames></author></authors><title>Cross-Modality Embedding of Force and Language for Natural Human-Robot   Communication</title><categories>cs.RO cs.AI cs.HC</categories><comments>Under review in RSS 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A method for cross-modality embedding of force profile and words is presented for synergistic coordination of verbal and haptic communication. When two people carry a large, heavy object together, they coordinate through verbal communication about the intended movements and physical forces applied to the object. This natural integration of verbal and physical cues enables effective coordination. Similarly, human-robot interaction could achieve this level of coordination by integrating verbal and haptic communication modalities. This paper presents a framework for embedding words and force profiles in a unified manner, so that the two communication modalities can be integrated and coordinated in a way that is effective and synergistic. Here, it will be shown that, although language and physical force profiles are deemed completely different, the two can be embedded in a unified latent space and proximity between the two can be quantified. In this latent space, a force profile and words can a) supplement each other, b) integrate the individual effects, and c) substitute in an exchangeable manner. First, the need for cross-modality embedding is addressed, and the basic architecture and key building block technologies are presented. Methods for data collection and implementation challenges will be addressed, followed by experimental results and discussions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02773</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02773</id><created>2025-02-04</created><authors><author><keyname>Diwanji</keyname><forenames>Hitvarth</forenames></author><author><keyname>Liao</keyname><forenames>Jing-Yan</forenames></author><author><keyname>Tumu</keyname><forenames>Akshar</forenames></author><author><keyname>Christensen</keyname><forenames>Henrik I.</forenames></author><author><keyname>Vazquez-Chanlatte</keyname><forenames>Marcell</forenames></author><author><keyname>Tsuchiya</keyname><forenames>Chikao</forenames></author></authors><title>SD++: Enhancing Standard Definition Maps by Incorporating Road Knowledge   using LLMs</title><categories>cs.RO cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  High-definition maps (HD maps) are detailed and informative maps capturing lane centerlines and road elements. Although very useful for autonomous driving, HD maps are costly to build and maintain. Furthermore, access to these high-quality maps is usually limited to the firms that build them. On the other hand, standard definition (SD) maps provide road centerlines with an accuracy of a few meters. In this paper, we explore the possibility of enhancing SD maps by incorporating information from road manuals using LLMs. We develop SD++, an end-to-end pipeline to enhance SD maps with location-dependent road information obtained from a road manual. We suggest and compare several ways of using LLMs for such a task. Furthermore, we show the generalization ability of SD++ by showing results from both California and Japan. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02774</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02774</id><created>2025-02-04</created><authors><author><keyname>Aureliano</keyname><forenames>Igor L.</forenames></author><author><keyname>Cohen</keyname><forenames>Alejandro</forenames></author><author><keyname>D'Oliveira</keyname><forenames>Rafael G. L.</forenames></author></authors><title>Optimal Computational Secret Sharing</title><categories>cs.IT cs.CR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In $(t, n)$-threshold secret sharing, a secret $S$ is distributed among $n$ participants such that any subset of size $t$ can recover $S$, while any subset of size $t-1$ or fewer learns nothing about it. For information-theoretic secret sharing, it is known that the share size must be at least as large as the secret, i.e., $|S|$. When computational security is employed using cryptographic encryption with a secret key $K$, previous work has shown that the share size can be reduced to $\tfrac{|S|}{t} + |K|$.   In this paper, we present a construction achieving a share size of $\tfrac{|S| + |K|}{t}$. Furthermore, we prove that, under reasonable assumptions on the encryption scheme -- namely, the non-compressibility of pseudorandom encryption and the non-redundancy of the secret key -- this share size is optimal. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02777</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02777</id><created>2025-02-04</created><authors><author><keyname>Bauwens</keyname><forenames>Bruno</forenames></author><author><keyname>Marchenko</keyname><forenames>Maria</forenames></author></authors><title>Symmetry of information for space-bounded online Kolmogorov complexity</title><categories>cs.CC cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The even online Kolmogorov complexity of a string $x = x_1 x_2 \cdots x_{n}$ is the minimal length of a program that for all $i\le n/2$, on input $x_1x_3 \cdots x_{2i-1}$ outputs $x_{2i}$. The odd complexity is defined similarly. The sum of the odd and even complexities is called the dialogue complexity.   In [Bauwens, 2014] it is proven that for all $n$, there exist $n$-bit $x$ for which the dialogue complexity exceeds the Kolmogorov complexity by $n\log \frac 4 3 + O(\log n)$. Let $\mathrm C^s(x)$ denote the Kolmogorov complexity with space bound~$s$. Here, we prove that the space-bounded dialogue complexity with bound $s + 6n + O(1)$ is at most $\mathrm C^{s}(x) + O(\log (sn))$, where $n=|x|$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02779</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02779</id><created>2025-02-04</created><authors><author><keyname>Zhu</keyname><forenames>Weicheng</forenames></author><author><keyname>Huang</keyname><forenames>Haoxu</forenames></author><author><keyname>Tang</keyname><forenames>Huanze</forenames></author><author><keyname>Musthyala</keyname><forenames>Rushabh</forenames></author><author><keyname>Yu</keyname><forenames>Boyang</forenames></author><author><keyname>Chen</keyname><forenames>Long</forenames></author><author><keyname>Vega</keyname><forenames>Emilio</forenames></author><author><keyname>O'Donnell</keyname><forenames>Thomas</forenames></author><author><keyname>Dehkharghani</keyname><forenames>Seena</forenames></author><author><keyname>Frontera</keyname><forenames>Jennifer A.</forenames></author><author><keyname>Masurkar</keyname><forenames>Arjun V.</forenames></author><author><keyname>Melmed</keyname><forenames>Kara</forenames></author><author><keyname>Razavian</keyname><forenames>Narges</forenames></author></authors><title>3D Foundation AI Model for Generalizable Disease Detection in Head   Computed Tomography</title><categories>cs.CV cs.AI</categories><comments>Under Review Preprint</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Head computed tomography (CT) imaging is a widely-used imaging modality with multitudes of medical indications, particularly in assessing pathology of the brain, skull, and cerebrovascular system. It is commonly the first-line imaging in neurologic emergencies given its rapidity of image acquisition, safety, cost, and ubiquity. Deep learning models may facilitate detection of a wide range of diseases. However, the scarcity of high-quality labels and annotations, particularly among less common conditions, significantly hinders the development of powerful models. To address this challenge, we introduce FM-CT: a Foundation Model for Head CT for generalizable disease detection, trained using self-supervised learning. Our approach pre-trains a deep learning model on a large, diverse dataset of 361,663 non-contrast 3D head CT scans without the need for manual annotations, enabling the model to learn robust, generalizable features. To investigate the potential of self-supervised learning in head CT, we employed both discrimination with self-distillation and masked image modeling, and we construct our model in 3D rather than at the slice level (2D) to exploit the structure of head CT scans more comprehensively and efficiently. The model's downstream classification performance is evaluated using internal and three external datasets, encompassing both in-distribution (ID) and out-of-distribution (OOD) data. Our results demonstrate that the self-supervised foundation model significantly improves performance on downstream diagnostic tasks compared to models trained from scratch and previous 3D CT foundation models on scarce annotated datasets. This work highlights the effectiveness of self-supervised learning in medical imaging and sets a new benchmark for head CT image analysis in 3D, enabling broader use of artificial intelligence for head CT-based diagnosis. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02780</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02780</id><created>2025-02-04</created><authors><author><keyname>Xu</keyname><forenames>Songlin</forenames></author><author><keyname>Wen</keyname><forenames>Hao-Ning</forenames></author><author><keyname>Pan</keyname><forenames>Hongyi</forenames></author><author><keyname>Dominguez</keyname><forenames>Dallas</forenames></author><author><keyname>Hu</keyname><forenames>Dongyin</forenames></author><author><keyname>Zhang</keyname><forenames>Xinyu</forenames></author></authors><title>Classroom Simulacra: Building Contextual Student Generative Agents in   Online Education for Learning Behavioral Simulation</title><categories>cs.HC cs.AI cs.LG</categories><comments>26 pages</comments><doi>10.1145/3706598.3713773</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Student simulation supports educators to improve teaching by interacting with virtual students. However, most existing approaches ignore the modulation effects of course materials because of two challenges: the lack of datasets with granularly annotated course materials, and the limitation of existing simulation models in processing extremely long textual data. To solve the challenges, we first run a 6-week education workshop from N = 60 students to collect fine-grained data using a custom built online education system, which logs students' learning behaviors as they interact with lecture materials over time. Second, we propose a transferable iterative reflection (TIR) module that augments both prompting-based and finetuning-based large language models (LLMs) for simulating learning behaviors. Our comprehensive experiments show that TIR enables the LLMs to perform more accurate student simulation than classical deep learning models, even with limited demonstration data. Our TIR approach better captures the granular dynamism of learning performance and inter-student correlations in classrooms, paving the way towards a ''digital twin'' for online education. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02783</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02783</id><created>2025-02-04</created><authors><author><keyname>Li</keyname><forenames>Ziyue</forenames></author><author><keyname>Chow</keyname><forenames>Joseph Y. J.</forenames></author><author><keyname>Guo</keyname><forenames>Qianwen</forenames></author></authors><title>Runway capacity expansion planning for public airports under demand   uncertainty</title><categories>eess.SY cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flight delay is a significant issue affecting air travel. The runway system, frequently falling short of demand, serves as a bottleneck. As demand increases, runway capacity expansion becomes imperative to mitigate congestion. However, the decision to expand runway capacity is challenging due to inherent uncertainties in demand forecasts. This paper presents a novel approach to modeling air traffic demand growth as a jump diffusion process, incorporating two layers of uncertainty: Geometric Brownian Motion (GBM) for continuous variability and a Poisson process to capture the impact of crisis events, such as natural disasters or public health emergencies, on decision-making. We propose a real options model to jointly evaluate the interrelated factors of optimal runway capacity and investment timing under uncertainty, with investment timing linked to trigger demand. The findings suggest that increased uncertainty indicates more conservative decision-making. Furthermore, the relationship between optimal investment timing and expansion size is complex: if the expansion size remains unchanged, the trigger demand decreases as the demand growth rate increases; if the expansion size experiences a jump, the trigger demand also exhibits a sharp rise. This work provides valuable insights for airport authorities for informed capacity expansion decision-making. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02785</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02785</id><created>2025-02-04</created><authors><author><keyname>Yeung</keyname><forenames>Calvin</forenames></author><author><keyname>Ide</keyname><forenames>Kenjiro</forenames></author><author><keyname>Someya</keyname><forenames>Taiga</forenames></author><author><keyname>Fujii</keyname><forenames>Keisuke</forenames></author></authors><title>OpenSTARLab: Open Approach for Spatio-Temporal Agent Data Analysis in   Soccer</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Sports analytics has become both more professional and sophisticated, driven by the growing availability of detailed performance data. This progress enables applications such as match outcome prediction, player scouting, and tactical analysis. In soccer, the effective utilization of event and tracking data is fundamental for capturing and analyzing the dynamics of the game. However, there are two primary challenges: the limited availability of event data, primarily restricted to top-tier teams and leagues, and the scarcity and high cost of tracking data, which complicates its integration with event data for comprehensive analysis. Here we propose OpenSTARLab, an open-source framework designed to democratize spatio-temporal agent data analysis in sports by addressing these key challenges. OpenSTARLab includes the Pre-processing Package that standardizes event and tracking data through Unified and Integrated Event Data and State-Action-Reward formats, the Event Modeling Package that implements deep learning-based event prediction, alongside the RLearn Package for reinforcement learning tasks. These technical components facilitate the handling of diverse data sources and support advanced analytical tasks, thereby enhancing the overall functionality and usability of the framework. To assess OpenSTARLab's effectiveness, we conducted several experimental evaluations. These demonstrate the superior performance of the specific event prediction model in terms of action and time prediction accuracies and maintained its robust event simulation performance. Furthermore, reinforcement learning experiments reveal a trade-off between action accuracy and temporal difference loss and show comprehensive visualization. Overall, OpenSTARLab serves as a robust platform for researchers and practitioners, enhancing innovation and collaboration in the field of soccer data analytics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02786</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02786</id><created>2025-02-04</created><authors><author><keyname>Cornelis</keyname><forenames>Louisa</forenames></author><author><keyname>Bernárdez</keyname><forenames>Guillermo</forenames></author><author><keyname>Jeong</keyname><forenames>Haewon</forenames></author><author><keyname>Miolane</keyname><forenames>Nina</forenames></author></authors><title>When Machine Learning Gets Personal: Understanding Fairness of   Personalized Models</title><categories>cs.LG</categories><comments>35 pages, 9 figures, submitted to ICML 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Personalization in machine learning involves tailoring models to individual users by incorporating personal attributes such as demographic or medical data. While personalization can improve prediction accuracy, it may also amplify biases and reduce explainability. This work introduces a unified framework to evaluate the impact of personalization on both prediction accuracy and explanation quality across classification and regression tasks. We derive novel upper bounds for the number of personal attributes that can be used to reliably validate benefits of personalization. Our analysis uncovers key trade-offs. We show that regression models can potentially utilize more personal attributes than classification models. We also demonstrate that improvements in prediction accuracy due to personalization do not necessarily translate to enhanced explainability -- underpinning the importance to evaluate both metrics when personalizing machine learning models in critical settings such as healthcare. Validated with a real-world dataset, this framework offers practical guidance for balancing accuracy, fairness, and interpretability in personalized models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02787</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02787</id><created>2025-02-04</created><authors><author><keyname>Dabiriaghdam</keyname><forenames>Amirhossein</forenames></author><author><keyname>Wang</keyname><forenames>Lele</forenames></author></authors><title>SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm   for Large Language Models</title><categories>cs.CL cs.CR cs.CY cs.LG</categories><comments>15 pages, 5 tables, 6 figures</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The rapid proliferation of large language models (LLMs) has created an urgent need for reliable methods to detect whether a text is generated by such models. In this paper, we propose SimMark, a posthoc watermarking algorithm that makes LLMs' outputs traceable without requiring access to the model's internal logits, enabling compatibility with a wide range of LLMs, including API-only models. By leveraging the similarity of semantic sentence embeddings and rejection sampling to impose detectable statistical patterns imperceptible to humans, and employing a soft counting mechanism, SimMark achieves robustness against paraphrasing attacks. Experimental results demonstrate that SimMark sets a new benchmark for robust watermarking of LLM-generated content, surpassing prior sentence-level watermarking techniques in robustness, sampling efficiency, and applicability across diverse domains, all while preserving the text quality. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02788</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02788</id><created>2025-02-04</created><authors><author><keyname>Phatak</keyname><forenames>Abhijeet</forenames></author><author><keyname>Sachdev</keyname><forenames>Jayant</forenames></author><author><keyname>Rosario</keyname><forenames>Sean D</forenames></author><author><keyname>Kirti</keyname><forenames>Swati</forenames></author><author><keyname>Tripathy</keyname><forenames>Chittaranjan</forenames></author></authors><title>Inducing Diversity in Differentiable Search Indexing</title><categories>cs.IR cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Differentiable Search Indexing (DSI) is a recent paradigm for information retrieval which uses a transformer-based neural network architecture as the document index to simplify the retrieval process. A differentiable index has many advantages enabling modifications, updates or extensions to the index. In this work, we explore balancing relevance and novel information content (diversity) for training DSI systems inspired by Maximal Marginal Relevance (MMR), and show the benefits of our approach over the naive DSI training. We present quantitative and qualitative evaluations of relevance and diversity measures obtained using our method on NQ320K and MSMARCO datasets in comparison to naive DSI. With our approach, it is possible to achieve diversity without any significant impact to relevance. Since we induce diversity while training DSI, the trained model has learned to diversify while being relevant. This obviates the need for a post-processing step to induce diversity in the recall set as typically performed using MMR. Our approach will be useful for Information Retrieval problems where both relevance and diversity are important such as in sub-topic retrieval. Our work can also be easily be extended to the incremental DSI settings which would enable fast updates to the index while retrieving a diverse recall set. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02789</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02789</id><created>2025-02-04</created><authors><author><keyname>Liu</keyname><forenames>Jingyu</forenames></author><author><keyname>Chen</keyname><forenames>Beidi</forenames></author><author><keyname>Zhang</keyname><forenames>Ce</forenames></author></authors><title>Speculative Prefill: Turbocharging TTFT with Lightweight and   Training-Free Token Importance Estimation</title><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Improving time-to-first-token (TTFT) is an essentially important objective in modern large language model (LLM) inference engines. Because optimizing TTFT directly results in higher maximal QPS and meets the requirements of many critical applications. However, boosting TTFT is notoriously challenging since it is purely compute-bounded and the performance bottleneck shifts from the self-attention to the MLP part. We present SpecPrefill, a training free framework that accelerates the inference TTFT for both long and medium context queries based on the following insight: LLMs are generalized enough to still preserve the quality given only a carefully chosen subset of prompt tokens. At its core, SpecPrefill leverages a lightweight model to speculate locally important tokens based on the context. These tokens, along with the necessary positional information, are then sent to the main model for processing. We evaluate SpecPrefill with a diverse set of tasks, followed by a comprehensive benchmarking of performance improvement both in a real end-to-end setting and ablation studies. SpecPrefill manages to serve Llama-3.1-405B-Instruct-FP8 with up to $7\times$ maximal end-to-end QPS on real downstream tasks and $7.66\times$ TTFT improvement during benchmarking. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02790</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02790</id><created>2025-02-04</created><authors><author><keyname>González</keyname><forenames>Ramón Calvo</forenames></author><author><keyname>Paliotta</keyname><forenames>Daniele</forenames></author><author><keyname>Pagliardini</keyname><forenames>Matteo</forenames></author><author><keyname>Jaggi</keyname><forenames>Martin</forenames></author><author><keyname>Fleuret</keyname><forenames>François</forenames></author></authors><title>Leveraging the true depth of LLMs</title><categories>cs.LG cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models demonstrate remarkable capabilities at the cost of high compute requirements. While recent research has shown that intermediate layers can be removed or have their order shuffled without impacting performance significantly, these findings have not been employed to reduce the computational cost of inference. We investigate several potential ways to reduce the depth of pre-trained LLMs without significantly affecting performance. Leveraging our insights, we present a novel approach that exploits this decoupling between layers by grouping some of them into pairs that can be evaluated in parallel.   This modification of the computational graph -- through better parallelism -- results in an average improvement of around 1.20x on the number of tokens generated per second, without re-training nor fine-tuning, while retaining 95%-99% of the original accuracy. Empirical evaluation demonstrates that this approach significantly improves serving efficiency while maintaining model performance, offering a practical improvement for large-scale LLM deployment. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02792</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02792</id><created>2025-02-04</created><authors><author><keyname>Liu</keyname><forenames>Hailong</forenames></author><author><keyname>Zeng</keyname><forenames>Zhe</forenames></author><author><keyname>Wada</keyname><forenames>Takahiro</forenames></author></authors><title>Where Do Passengers Gaze? Impact of Passengers' Personality Traits on   Their Gaze Pattern Toward Pedestrians During APMV-Pedestrian Interactions   with Diverse eHMIs</title><categories>cs.HC</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Autonomous Personal Mobility Vehicles (APMVs) are designed to address the ``last-mile'' transportation challenge for everyone. When an APMV encounters a pedestrian, it uses an external Human-Machine Interface (eHMI) to negotiate road rights. Through this interaction, passengers also engage with the process. This study examines passengers' gaze behavior toward pedestrians during such interactions, focusing on whether different eHMI designs influence gaze patterns based on passengers' personality traits. The results indicated that when using a visual-based eHMI, passengers often struggled to perceive the communication content. Consequently, passengers with higher Neuroticism scores, who were more sensitive to communication details, might seek cues from pedestrians' reactions. In addition, a multimodal eHMI (visual and voice) using neutral voice did not significantly affect the gaze behavior of passengers toward pedestrians, regardless of personality traits. In contrast, a multimodal eHMI using affective voice encouraged passengers with high Openness to Experience scores to focus on pedestrians' heads. In summary, this study revealed how different eHMI designs influence passengers' gaze behavior and highlighted the effects of personality traits on their gaze patterns toward pedestrians, providing new insights for personalized eHMI designs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02794</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02794</id><created>2025-02-04</created><authors><author><keyname>Lee</keyname><forenames>Hyeonseok</forenames></author><author><keyname>An</keyname><forenames>Gabin</forenames></author><author><keyname>Yoo</keyname><forenames>Shin</forenames></author></authors><title>METAMON: Finding Inconsistencies between Program Documentation and   Behavior using Metamorphic LLM Queries</title><categories>cs.SE</categories><comments>8 pages and 7 figures, accepted to LLM4Code 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Code documentation can, if written precisely, help developers better understand the code they accompany. However, unlike code, code documentation cannot be automatically verified via execution, potentially leading to inconsistencies between documentation and the actual behavior. While such inconsistencies can be harmful for the developer's understanding of the code, checking and finding them remains a costly task due to the involvement of human engineers. This paper proposes METAMON, which uses an existing search-based test generation technique to capture the current program behavior in the form of test cases, and subsequently uses LLM-based code reasoning to identify the generated regression test oracles that are not consistent with the program specifications in the documentation. METAMON is supported in this task by metamorphic testing and self-consistency. An empirical evaluation against 9,482 pairs of code documentation and code snippets, generated using five open-source projects from Defects4J v2.0.1, shows that METAMON can classify the code-and-documentation inconsistencies with a precision of 0.72 and a recall of 0.48. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02797</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02797</id><created>2025-02-04</created><authors><author><keyname>Sanyal</keyname><forenames>Sunny</forenames></author><author><keyname>Prairie</keyname><forenames>Hayden</forenames></author><author><keyname>Das</keyname><forenames>Rudrajit</forenames></author><author><keyname>Kavis</keyname><forenames>Ali</forenames></author><author><keyname>Sanghavi</keyname><forenames>Sujay</forenames></author></authors><title>Upweighting Easy Samples in Fine-Tuning Mitigates Forgetting</title><categories>cs.LG cs.AI stat.ML</categories><comments>49 pages, 4 figures, 12 tables. Code available at   https://github.com/sanyalsunny111/FLOW_finetuning</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Fine-tuning a pre-trained model on a downstream task often degrades its original capabilities, a phenomenon known as "catastrophic forgetting". This is especially an issue when one does not have access to the data and recipe used to develop the pre-trained model. Under this constraint, most existing methods for mitigating forgetting are inapplicable. To address this challenge, we propose a sample weighting scheme for the fine-tuning data solely based on the pre-trained model's losses. Specifically, we upweight the easy samples on which the pre-trained model's loss is low and vice versa to limit the drift from the pre-trained model. Our approach is orthogonal and yet complementary to existing methods; while such methods mostly operate on parameter or gradient space, we concentrate on the sample space. We theoretically analyze the impact of fine-tuning with our method in a linear setting, showing that it stalls learning in a certain subspace which inhibits overfitting to the target task. We empirically demonstrate the efficacy of our method on both language and vision tasks. As an example, when fine-tuning Gemma 2 2B on MetaMathQA, our method results in only a $0.8\%$ drop in accuracy on GSM8K (another math dataset) compared to standard fine-tuning, while preserving $5.4\%$ more accuracy on the pre-training datasets. Our code is publicly available at https://github.com/sanyalsunny111/FLOW_finetuning . </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02799</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02799</id><created>2025-02-04</created><authors><author><keyname>Gharan</keyname><forenames>Shayan Oveis</forenames></author><author><keyname>Sahami</keyname><forenames>Arvin</forenames></author></authors><title>Unweighted Code Sparsifiers and Thin Subgraphs</title><categories>math.CO cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that for every $k$-dimensional linear code $\mathcal{C} \subseteq \mathbb{F}_2^n$ there exists a set $S\subseteq [n]$ of size at most $n/2+O(\sqrt{nk})$ such that the projection of $\mathcal{C}$ onto $S$ has distance at least $\frac12\mathrm{dist}(\mathcal{C})$. As a consequence we show that any connected graph $G$ with $m$ edges and $n$ vertices has at least $2^{m-(n-1)}$ many $1/2$-thin subgraphs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02802</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02802</id><created>2025-02-04</created><authors><author><keyname>Yang</keyname><forenames>Yizhe</forenames></author><author><keyname>Achananuparp</keyname><forenames>Palakorn</forenames></author><author><keyname>Huang</keyname><forenames>Heyan</forenames></author><author><keyname>Jiang</keyname><forenames>Jing</forenames></author><author><keyname>Pinto</keyname><forenames>John</forenames></author><author><keyname>Giam</keyname><forenames>Jenny</forenames></author><author><keyname>Leng</keyname><forenames>Kit Phey</forenames></author><author><keyname>Lim</keyname><forenames>Nicholas Gabriel</forenames></author><author><keyname>Ern</keyname><forenames>Cameron Tan Shi</forenames></author><author><keyname>Lim</keyname><forenames>Ee-peng</forenames></author></authors><title>Consistent Client Simulation for Motivational Interviewing-based   Counseling</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Simulating human clients in mental health counseling is crucial for training and evaluating counselors (both human or simulated) in a scalable manner. Nevertheless, past research on client simulation did not focus on complex conversation tasks such as mental health counseling. In these tasks, the challenge is to ensure that the client's actions (i.e., interactions with the counselor) are consistent with with its stipulated profiles and negative behavior settings. In this paper, we propose a novel framework that supports consistent client simulation for mental health counseling. Our framework tracks the mental state of a simulated client, controls its state transitions, and generates for each state behaviors consistent with the client's motivation, beliefs, preferred plan to change, and receptivity. By varying the client profile and receptivity, we demonstrate that consistent simulated clients for different counseling scenarios can be effectively created. Both our automatic and expert evaluations on the generated counseling sessions also show that our client simulation method achieves higher consistency than previous methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02805</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02805</id><created>2025-02-04</created><authors><author><keyname>Liu</keyname><forenames>Hailong</forenames></author><author><keyname>Li</keyname><forenames>Yang</forenames></author><author><keyname>Hiraoka</keyname><forenames>Toshihiro</forenames></author><author><keyname>Wada</keyname><forenames>Takahiro</forenames></author></authors><title>Data-driven Causal Discovery for Pedestrians-Autonomous Personal   Mobility Vehicle Interactions with eHMIs: From Psychological States to   Walking Behaviors</title><categories>cs.HC</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Autonomous personal mobility vehicle (APMV) is a new type of small smart vehicle designed for mixed-traffic environments, including interactions with pedestrians. To enhance the interaction experience between pedestrians and APMVs and to prevent potential risks, it is crucial to investigate pedestrians' walking behaviors when interacting with APMVs and to understand the psychological processes underlying these behaviors. This study aims to investigate the causal relationships between subjective evaluations of pedestrians and their walking behaviors during interactions with an external human-machine interface (eHMI) equipped with an APMV. An experiment of pedestrian-APMV interaction (N = 42) was conducted, in which various eHMIs on the APMV were designed to induce participants to experience different levels of subjective evaluations and generate the corresponding walking behaviors. Based on the hypothesized model of the pedestrian's cognition-decision-behavior process, the results of causal discovery align with the previously proposed model. Furthermore, this study further analyzes the direct and total causal effects of each factor and investigates the causal processes affecting several important factors in the field of human-vehicle interaction, such as situation awareness, trust in vehicle, risk perception, hesitation in decision making, and walking behaviors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02807</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02807</id><created>2025-02-04</created><authors><author><keyname>Yang</keyname><forenames>Yizhe</forenames></author><author><keyname>Achananuparp</keyname><forenames>Palakorn</forenames></author><author><keyname>Huang</keyname><forenames>Heyan</forenames></author><author><keyname>Jiang</keyname><forenames>Jing</forenames></author><author><keyname>Leng</keyname><forenames>Kit Phey</forenames></author><author><keyname>Lim</keyname><forenames>Nicholas Gabriel</forenames></author><author><keyname>Ern</keyname><forenames>Cameron Tan Shi</forenames></author><author><keyname>Lim</keyname><forenames>Ee-peng</forenames></author></authors><title>CAMI: A Counselor Agent Supporting Motivational Interviewing through   State Inference and Topic Exploration</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Conversational counselor agents have become essential tools for addressing the rising demand for scalable and accessible mental health support. This paper introduces CAMI, a novel automated counselor agent grounded in Motivational Interviewing (MI) -- a client-centered counseling approach designed to address ambivalence and facilitate behavior change. CAMI employs a novel STAR framework, consisting of client's state inference, motivation topic exploration, and response generation modules, leveraging large language models (LLMs). These components work together to evoke change talk, aligning with MI principles and improving counseling outcomes for clients from diverse backgrounds. We evaluate CAMI's performance through both automated and manual evaluations, utilizing simulated clients to assess MI skill competency, client's state inference accuracy, topic exploration proficiency, and overall counseling success. Results show that CAMI not only outperforms several state-of-the-art methods but also shows more realistic counselor-like behavior. Additionally, our ablation study underscores the critical roles of state inference and topic exploration in achieving this performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02810</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02810</id><created>2025-02-04</created><authors><author><keyname>Lee</keyname><forenames>Chanhui</forenames></author><author><keyname>Song</keyname><forenames>Yuheon</forenames></author><author><keyname>Jeong</keyname><forenames>YongJun</forenames></author><author><keyname>Ko</keyname><forenames>Hanbum</forenames></author><author><keyname>Hormazabal</keyname><forenames>Rodrigo</forenames></author><author><keyname>Han</keyname><forenames>Sehui</forenames></author><author><keyname>Bae</keyname><forenames>Kyunghoon</forenames></author><author><keyname>Lim</keyname><forenames>Sungbin</forenames></author><author><keyname>Kim</keyname><forenames>Sungwoong</forenames></author></authors><title>Mol-LLM: Generalist Molecular LLM with Improved Graph Utilization</title><categories>cs.LG cs.AI physics.chem-ph q-bio.BM</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent advances in Large Language Models (LLMs) have motivated the development of general LLMs for molecular tasks. While several studies have demonstrated that fine-tuned LLMs can achieve impressive benchmark performances, they are far from genuine generalist molecular LLMs due to a lack of fundamental understanding of molecular structure. Specifically, when given molecular task instructions, LLMs trained with naive next-token prediction training assign similar likelihood scores to both original and negatively corrupted molecules, revealing their lack of molecular structure understanding that is crucial for reliable and general molecular LLMs. To overcome this limitation and obtain a true generalist molecular LLM, we introduce a novel multi-modal training method based on a thorough multi-modal instruction tuning as well as a molecular structure preference optimization between chosen and rejected graphs. On various molecular benchmarks, the proposed generalist molecular LLM, called Mol-LLM, achieves state-of-the-art performances among generalist LLMs on most tasks, at the same time, surpassing or comparable to state-of-the-art specialist LLMs. Moreover, Mol-LLM also shows superior generalization performances in reaction prediction tasks, demonstrating the effect of the molecular structure understanding for generalization perspective. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02813</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02813</id><created>2025-02-04</created><authors><author><keyname>Kang</keyname><forenames>Xueyu</forenames></author><author><keyname>Qi</keyname><forenames>Nan</forenames></author><author><keyname>Lv</keyname><forenames>Lu</forenames></author><author><keyname>Boulogeorgos</keyname><forenames>Alexandros-Apostolos A.</forenames></author><author><keyname>Tsiftsis</keyname><forenames>Theodoros A.</forenames></author><author><keyname>Liu</keyname><forenames>Hongwu</forenames></author></authors><title>Covert Communications in Active-IOS Aided Uplink NOMA Systems With   Full-Duplex Receiver</title><categories>cs.IT eess.SP math.IT</categories><comments>Accepted by IEEE Journal</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, an active intelligent omni-surface (A-IOS) is deployed to aid uplink transmissions in a non-orthogonal multiple access (NOMA) system. In order to shelter the covert signal embedded in the superposition transmissions, a multi-antenna full-duplex (FD) receiver is utilized at the base-station to recover signal in addition to jamming the warden. With the aim of maximizing the covert rate, the FD transmit and receive beamforming, A-IOS refraction and reflection beamforming, NOMA transmit power, and FD jamming power are jointly optimized. To tackle the non-convex covert rate maximization problem subject to the highly coupled system parameters, an alternating optimization algorithm is designed to iteratively solve the decoupled sub-problems of optimizing the system parameters. The optimal solutions for the sub-problems of the NOMA transmit power and FD jamming power optimizations are derived in closed-form. To tackle the rank-one constrained non-convex fractional programming of the A-IOS beamforming and FD beamforming, a penalized Dinkelbach transformation approach is proposed to resort to the optimal solutions via semidefinite programming. Numerical results clarify that the deployment of the A-IOS significantly improves the covert rate compared with the passive-IOS aided uplink NOMA system. It is also found that the proposed scheme provides better covert communication performance with the optimized NOMA transmit power and FD jamming power compared with the benchmark schemes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02815</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02815</id><created>2025-02-04</created><authors><author><keyname>Garg</keyname><forenames>Jugal</forenames></author><author><keyname>Sharma</keyname><forenames>Eklavya</forenames></author></authors><title>Exploring Relations among Fairness Notions in Discrete Fair Division</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fairly allocating indivisible items among agents is an important and well-studied problem. However, fairness does not have a single universally agreed-upon definition, and so, many different definitions of fairness have been proposed and studied. Some of these definitions are considered more fair than others, although stronger fairness notions are also more difficult to guarantee. In this work, we study 21 different notions of fairness and arrange them in a hierarchy. Formally, we say that a fairness notion $F_1$ implies another notion $F_2$ if every $F_1$-fair allocation is also an $F_2$-fair allocation. We give a near-complete picture of implications among fairness notions: for almost every pair of notions, we either prove that one notion implies the other, or we give a counterexample, i.e., an allocation that is fair by one notion but not by the other. Although some of these results are well-known, many of them are new. We give results for many different settings: allocating goods, allocating chores, and allocating mixed manna. We believe our work clarifies the relative merits of different fairness notions, and provides a foundation for further research in fair allocation. Moreover, we developed an inference engine to automate part of our work. This inference engine is implemented as a user-friendly web application and is not restricted to fair division scenarios, so it holds potential for broader use. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02817</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02817</id><created>2025-02-04</created><authors><author><keyname>Yin</keyname><forenames>Hao</forenames></author><author><keyname>Parmar</keyname><forenames>Paritosh</forenames></author><author><keyname>Xu</keyname><forenames>Daoliang</forenames></author><author><keyname>Zhang</keyname><forenames>Yang</forenames></author><author><keyname>Zheng</keyname><forenames>Tianyou</forenames></author><author><keyname>Fu</keyname><forenames>Weiwei</forenames></author></authors><title>A Decade of Action Quality Assessment: Largest Systematic Survey of   Trends, Challenges, and Future Directions</title><categories>cs.AI cs.CV</categories><comments>36 Pages, 20 Figures, 12 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Action Quality Assessment (AQA) -- the ability to quantify the quality of human motion, actions, or skill levels and provide feedback -- has far-reaching implications in areas such as low-cost physiotherapy, sports training, and workforce development. As such, it has become a critical field in computer vision &amp; video understanding over the past decade. Significant progress has been made in AQA methodologies, datasets, &amp; applications, yet a pressing need remains for a comprehensive synthesis of this rapidly evolving field. In this paper, we present a thorough survey of the AQA landscape, systematically reviewing over 200 research papers using the preferred reporting items for systematic reviews &amp; meta-analyses (PRISMA) framework. We begin by covering foundational concepts &amp; definitions, then move to general frameworks &amp; performance metrics, &amp; finally discuss the latest advances in methodologies &amp; datasets. This survey provides a detailed analysis of research trends, performance comparisons, challenges, &amp; future directions. Through this work, we aim to offer a valuable resource for both newcomers &amp; experienced researchers, promoting further exploration &amp; progress in AQA. Data are available at https://haoyin116.github.io/Survey_of_AQA/ </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02818</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02818</id><created>2025-02-04</created><authors><author><keyname>Sun</keyname><forenames>Wenbo</forenames></author><author><keyname>Guo</keyname><forenames>Qiming</forenames></author><author><keyname>Wang</keyname><forenames>Wenlu</forenames></author><author><keyname>Hai</keyname><forenames>Rihan</forenames></author></authors><title>Accessible and Portable LLM Inference by Compiling Computational Graphs   into SQL</title><categories>cs.DB cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Serving large language models (LLMs) often demands specialized hardware, dedicated frameworks, and substantial development efforts, which restrict their accessibility, especially for edge devices and organizations with limited technical resources. We propose a novel compiler that translates LLM inference graphs into SQL queries, enabling relational databases, one of the most widely used and mature software systems globally, to serve as the runtime. By mapping neural operators such as matrix multiplication and attention into relational primitives like joins and aggregations, our approach leverages database capabilities, including disk-based data management and native caching. Supporting key transformer components, such as attention mechanisms and key-value caching, our system generates SQL pipelines for end-to-end LLM inference. Using the Llama3 family as a case study, we demonstrate up to 30x speedup in token generation for memory-constrained scenarios comparable to competitive CPU-based frameworks. Our work offers an accessible, portable, and efficient solution, facilitating the serving of LLMs across diverse deployment environments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02820</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02820</id><created>2025-02-04</created><authors><author><keyname>Quirke</keyname><forenames>Lucia</forenames></author><author><keyname>Belrose</keyname><forenames>Nora</forenames></author></authors><title>Slowing Learning by Erasing Simple Features</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Prior work suggests that neural networks tend to learn low-order moments of the data distribution first, before moving on to higher-order correlations. In this work, we derive a novel closed-form concept erasure method, QLEACE, which surgically removes all quadratically available information about a concept from a representation. Through comparisons with linear erasure (LEACE) and two approximate forms of quadratic erasure, we explore whether networks can still learn when low-order statistics are removed from image classification datasets. We find that while LEACE consistently slows learning, quadratic erasure can exhibit both positive and negative effects on learning speed depending on the choice of dataset, model architecture, and erasure method.   Use of QLEACE consistently slows learning in feedforward architectures, but more sophisticated architectures learn to use injected higher order Shannon information about class labels. Its approximate variants avoid injecting information, but surprisingly act as data augmentation techniques on some datasets, enhancing learning speed compared to LEACE. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02821</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02821</id><created>2025-02-04</created><authors><author><keyname>Elbasha</keyname><forenames>Ahmed Mahmoud</forenames></author><author><keyname>Abdellatif</keyname><forenames>Mohammad M.</forenames></author></authors><title>AIoT-based smart traffic management system</title><categories>cs.CV</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  This paper presents a novel AI-based smart traffic management system de-signed to optimize traffic flow and reduce congestion in urban environments. By analysing live footage from existing CCTV cameras, this approach eliminates the need for additional hardware, thereby minimizing both deployment costs and ongoing maintenance expenses. The AI model processes live video feeds to accurately count vehicles and assess traffic density, allowing for adaptive signal control that prioritizes directions with higher traffic volumes. This real-time adaptability ensures smoother traffic flow, reduces congestion, and minimizes waiting times for drivers. Additionally, the proposed system is simulated using PyGame to evaluate its performance under various traffic conditions. The simulation results demonstrate that the AI-based system out-performs traditional static traffic light systems by 34%, leading to significant improvements in traffic flow efficiency. The use of AI to optimize traffic signals can play a crucial role in addressing urban traffic challenges, offering a cost-effective, scalable, and efficient solution for modern cities. This innovative system represents a key advancement in the field of smart city infra-structure and intelligent transportation systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02827</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02827</id><created>2025-02-04</created><authors><author><keyname>Peng</keyname><forenames>Yun</forenames></author><author><keyname>Wan</keyname><forenames>Jun</forenames></author><author><keyname>Li</keyname><forenames>Yichen</forenames></author><author><keyname>Ren</keyname><forenames>Xiaoxue</forenames></author></authors><title>COFFE: A Code Efficiency Benchmark for Code Generation</title><categories>cs.SE</categories><comments>This paper has been accepted by FSE 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Code generation has largely improved development efficiency in the era of large language models (LLMs). With the ability to follow instructions, current LLMs can be prompted to generate code solutions given detailed descriptions in natural language. Many research efforts are being devoted to improving the correctness of LLM-generated code, and many benchmarks are proposed to evaluate the correctness comprehensively. Despite the focus on correctness, the time efficiency of LLM-generated code solutions is under-explored. Current correctness benchmarks are not suitable for time efficiency evaluation since their test cases cannot well distinguish the time efficiency of different code solutions. Besides, the current execution time measurement is not stable and comprehensive, threatening the validity of the time efficiency evaluation.   To address the challenges in the time efficiency evaluation of code generation, we propose COFFE, a code generation benchmark for evaluating the time efficiency of LLM-generated code solutions. COFFE contains 398 and 358 problems for function-level and file-level code generation, respectively. To improve the distinguishability, we design a novel stressful test case generation approach with contracts and two new formats of test cases to improve the accuracy of generation. For the time evaluation metric, we propose efficienct@k based on CPU instruction count to ensure a stable and solid comparison between different solutions. We evaluate 14 popular LLMs on COFFE and identify four findings. Based on the findings, we draw some implications for LLM researchers and software practitioners to facilitate future research and usage of LLMs in code generation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02829</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02829</id><created>2025-02-04</created><authors><author><keyname>Kang</keyname><forenames>Shucheng</forenames></author><author><keyname>Liu</keyname><forenames>Guorui</forenames></author><author><keyname>Yang</keyname><forenames>Heng</forenames></author></authors><title>Global Contact-Rich Planning with Sparsity-Rich Semidefinite Relaxations</title><categories>cs.RO math.OC</categories><comments>Website: https://computationalrobotics.seas.harvard.edu/project-spot/</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We show that contact-rich motion planning is also sparsity-rich when viewed as polynomial optimization (POP). We can exploit not only the correlative and term sparsity patterns that are general to all POPs, but also specialized sparsity patterns from the robot kinematic structure and the separability of contact modes. Such sparsity enables the design of high-order but sparse semidefinite programming (SDPs) relaxations--building upon Lasserre's moment and sums of squares hierarchy--that (i) can be solved in seconds by off-the-shelf SDP solvers, and (ii) compute near globally optimal solutions to the nonconvex contact-rich planning problems with small certified suboptimality. Through extensive experiments both in simulation (Push Bot, Push Box, Push Box with Obstacles, and Planar Hand) and real world (Push T), we demonstrate the power of using convex SDP relaxations to generate global contact-rich motion plans. As a contribution of independent interest, we release the Sparse Polynomial Optimization Toolbox (SPOT)--implemented in C++ with interfaces to both Python and Matlab--that automates sparsity exploitation for robotics and beyond. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02830</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02830</id><created>2025-02-04</created><authors><author><keyname>Li</keyname><forenames>Siyang</forenames></author><author><keyname>Wang</keyname><forenames>Hongbin</forenames></author><author><keyname>Chen</keyname><forenames>Xiaoqing</forenames></author><author><keyname>Wu</keyname><forenames>Dongrui</forenames></author></authors><title>Multimodal Brain-Computer Interfaces: AI-powered Decoding Methodologies</title><categories>cs.HC cs.LG q-bio.NC</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Brain-computer interfaces (BCIs) enable direct communication between the brain and external devices. This review highlights the core decoding algorithms that enable multimodal BCIs, including a dissection of the elements, a unified view of diversified approaches, and a comprehensive analysis of the present state of the field. We emphasize algorithmic advancements in cross-modality mapping, sequential modeling, besides classic multi-modality fusion, illustrating how these novel AI approaches enhance decoding of brain data. The current literature of BCI applications on visual, speech, and affective decoding are comprehensively explored. Looking forward, we draw attention on the impact of emerging architectures like multimodal Transformers, and discuss challenges such as brain data heterogeneity and common errors. This review also serves as a bridge in this interdisciplinary field for experts with neuroscience background and experts that study AI, aiming to provide a comprehensive understanding for AI-powered multimodal BCIs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02831</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02831</id><created>2025-02-04</created><authors><author><keyname>Prabhakaran</keyname><forenames>Divya</forenames></author><author><keyname>Grasemann</keyname><forenames>Uli</forenames></author><author><keyname>Kiran</keyname><forenames>Swathi</forenames></author><author><keyname>Miikkulainen</keyname><forenames>Risto</forenames></author></authors><title>How the Stroop Effect Arises from Optimal Response Times in Laterally   Connected Self-Organizing Maps</title><categories>q-bio.NC cs.NE</categories><comments>7 pages, 6 figures, submitted to CogSci 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Stroop effect refers to cognitive interference in a color-naming task: When the color and the word do not match, the response is slower and more likely to be incorrect. The Stroop task is used to assess cognitive flexibility, selective attention, and executive function. This paper implements the Stroop task with self-organizing maps (SOMs): Target color and the competing word are inputs for the semantic and lexical maps, associative connections bring color information to the lexical map, and lateral connections combine their effects over time. The model achieved an overall accuracy of 84.2%, with significantly fewer errors and faster responses in congruent compared to no-input and incongruent conditions. The model's effect is a side effect of optimizing response times, and can thus be seen as a cost associated with overall efficient performance. The model can further serve studying neurologically-inspired cognitive control and related phenomena. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02834</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02834</id><created>2025-02-04</created><authors><author><keyname>Kim</keyname><forenames>Jeongmo</forenames></author><author><keyname>Park</keyname><forenames>Yisak</forenames></author><author><keyname>Kim</keyname><forenames>Minung</forenames></author><author><keyname>Han</keyname><forenames>Seungyul</forenames></author></authors><title>Task-Aware Virtual Training: Enhancing Generalization in   Meta-Reinforcement Learning for Out-of-Distribution Tasks</title><categories>cs.LG cs.AI</categories><comments>8 pages main paper, 19 pages appendices with reference, Submitted to   ICML 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Meta reinforcement learning aims to develop policies that generalize to unseen tasks sampled from a task distribution. While context-based meta-RL methods improve task representation using task latents, they often struggle with out-of-distribution (OOD) tasks. To address this, we propose Task-Aware Virtual Training (TAVT), a novel algorithm that accurately captures task characteristics for both training and OOD scenarios using metric-based representation learning. Our method successfully preserves task characteristics in virtual tasks and employs a state regularization technique to mitigate overestimation errors in state-varying environments. Numerical results demonstrate that TAVT significantly enhances generalization to OOD tasks across various MuJoCo and MetaWorld environments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02835</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02835</id><created>2025-02-04</created><authors><author><keyname>Ding</keyname><forenames>Lei</forenames></author><author><keyname>Hong</keyname><forenames>Danfeng</forenames></author><author><keyname>Zhao</keyname><forenames>Maofan</forenames></author><author><keyname>Chen</keyname><forenames>Hongruixuan</forenames></author><author><keyname>Li</keyname><forenames>Chenyu</forenames></author><author><keyname>Deng</keyname><forenames>Jie</forenames></author><author><keyname>Yokoya</keyname><forenames>Naoto</forenames></author><author><keyname>Bruzzone</keyname><forenames>Lorenzo</forenames></author><author><keyname>Chanussot</keyname><forenames>Jocelyn</forenames></author></authors><title>A Survey of Sample-Efficient Deep Learning for Change Detection in   Remote Sensing: Tasks, Strategies, and Challenges</title><categories>cs.CV</categories><comments>Accepted in IEEE GRSM</comments><doi>10.1109/MGRS.2025.3533605</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last decade, the rapid development of deep learning (DL) has made it possible to perform automatic, accurate, and robust Change Detection (CD) on large volumes of Remote Sensing Images (RSIs). However, despite advances in CD methods, their practical application in real-world contexts remains limited due to the diverse input data and the applicational context. For example, the collected RSIs can be time-series observations, and more informative results are required to indicate the time of change or the specific change category. Moreover, training a Deep Neural Network (DNN) requires a massive amount of training samples, whereas in many cases these samples are difficult to collect. To address these challenges, various specific CD methods have been developed considering different application scenarios and training resources. Additionally, recent advancements in image generation, self-supervision, and visual foundation models (VFMs) have opened up new approaches to address the 'data-hungry' issue of DL-based CD. The development of these methods in broader application scenarios requires further investigation and discussion. Therefore, this article summarizes the literature methods for different CD tasks and the available strategies and techniques to train and deploy DL-based CD methods in sample-limited scenarios. We expect that this survey can provide new insights and inspiration for researchers in this field to develop more effective CD methods that can be applied in a wider range of contexts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02842</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02842</id><created>2025-02-04</created><authors><author><keyname>Andrade</keyname><forenames>Maiko</forenames></author><author><keyname>Wickboldt</keyname><forenames>Juliano Araujo</forenames></author></authors><title>A Study on 5G Network Slice Isolation Based on Native Cloud and Edge   Computing Tools</title><categories>cs.NI cs.DC</categories><comments>Submitted to Journal of Network and Systems Management (JNSM)</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  5G networks support various advanced applications through network slicing, network function virtualization (NFV), and edge computing, ensuring low latency and service isolation. However, private 5G networks relying on open-source tools still face challenges in maturity and integration with edge/cloud platforms, compromising proper slice isolation. This study investigates resource allocation mechanisms to address this issue, conducting experiments in a hospital scenario with medical video conferencing. The results show that CPU limitations improve the performance of prioritized slices, while memory restrictions have minimal impact. The generated data and scripts have been made publicly available for future research and machine learning applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02843</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02843</id><created>2025-02-04</created><authors><author><keyname>Suryanarayanan</keyname><forenames>Shambhavi</forenames></author><author><keyname>Rebrova</keyname><forenames>Elizaveta</forenames></author></authors><title>On Trimming Tensor-structured Measurements and Efficient Low-rank Tensor   Recovery</title><categories>math.NA cs.NA</categories><comments>31 pages, 7 figures</comments><msc-class>15B52, 15A69, 15A83, 97N40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we take a step towards developing efficient hard thresholding methods for low-rank tensor recovery from memory-efficient linear measurements with tensorial structure. Theoretical guarantees for many standard iterative low-rank recovery methods, such as iterative hard thresholding (IHT), are based on model assumptions on the measurement operator, like the restricted isometry property (RIP). However, tensor-structured random linear maps -- while memory-efficient and convenient to apply -- lack good restricted isometry properties; that is, they do not preserve the norms of low-rank tensors sufficiently well.   To address this, we propose local trimming techniques that provably restore point-wise geometry-preservation properties of tensor-structured maps, making them comparable to those of unstructured sub-Gaussian measurements. Then, we propose two novel versions of tensor IHT algorithms: an adaptive gradient trimming algorithm and a randomized Kaczmarz-based IHT algorithm, that efficiently recover low-rank tensors from linear measurements. We provide initial theoretical guarantees for the proposed methods and present numerical experiments on real and synthetic data, highlighting their efficiency over the original TensorIHT for low HOSVD and CP-rank tensors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02844</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02844</id><created>2025-02-04</created><authors><author><keyname>Lee</keyname><forenames>Sunwoo</forenames></author><author><keyname>Hwang</keyname><forenames>Jaebak</forenames></author><author><keyname>Jo</keyname><forenames>Yonghyeon</forenames></author><author><keyname>Han</keyname><forenames>Seungyul</forenames></author></authors><title>Wolfpack Adversarial Attack for Robust Multi-Agent Reinforcement   Learning</title><categories>cs.LG cs.AI cs.CR cs.MA</categories><comments>8 pages main, 21 pages appendix with reference. Submitted to ICML   2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional robust methods in multi-agent reinforcement learning (MARL) often struggle against coordinated adversarial attacks in cooperative scenarios. To address this limitation, we propose the Wolfpack Adversarial Attack framework, inspired by wolf hunting strategies, which targets an initial agent and its assisting agents to disrupt cooperation. Additionally, we introduce the Wolfpack-Adversarial Learning for MARL (WALL) framework, which trains robust MARL policies to defend against the proposed Wolfpack attack by fostering system-wide collaboration. Experimental results underscore the devastating impact of the Wolfpack attack and the significant robustness improvements achieved by WALL. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02850</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02850</id><created>2025-02-04</created><authors><author><keyname>Yang</keyname><forenames>Lei</forenames></author><author><keyname>Yuan</keyname><forenames>Guowu</forenames></author><author><keyname>Zhou</keyname><forenames>Hao</forenames></author><author><keyname>Liu</keyname><forenames>Hongyu</forenames></author><author><keyname>Chen</keyname><forenames>Jian</forenames></author><author><keyname>Wu</keyname><forenames>Hao</forenames></author></authors><title>RS-YOLOX: A High Precision Detector for Object Detection in Satellite   Remote Sensing Images</title><categories>cs.CV</categories><doi>10.3390/app12178707</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Automatic object detection by satellite remote sensing images is of great significance for resource exploration and natural disaster assessment. To solve existing problems in remote sensing image detection, this article proposes an improved YOLOX model for satellite remote sensing image automatic detection. This model is named RS-YOLOX. To strengthen the feature learning ability of the network, we used Efficient Channel Attention (ECA) in the backbone network of YOLOX and combined the Adaptively Spatial Feature Fusion (ASFF) with the neck network of YOLOX. To balance the numbers of positive and negative samples in training, we used the Varifocal Loss function. Finally, to obtain a high-performance remote sensing object detector, we combined the trained model with an open-source framework called Slicing Aided Hyper Inference (SAHI). This work evaluated models on three aerial remote sensing datasets (DOTA-v1.5, TGRS-HRRSD, and RSOD). Our comparative experiments demonstrate that our model has the highest accuracy in detecting objects in remote sensing image datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02851</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02851</id><created>2025-02-04</created><authors><author><keyname>Ko</keyname><forenames>Yongho</forenames></author><author><keyname>Pawana</keyname><forenames>I Wayan Adi Juliawan</forenames></author><author><keyname>You</keyname><forenames>Ilsun</forenames></author></authors><title>5G-AKA-HPQC: Hybrid Post-Quantum Cryptography Protocol for   Quantum-Resilient 5G Primary Authentication with Forward Secrecy</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  5G enables digital innovation by integrating diverse services, making security especially primary authentication crucial. Two standardized protocols, 5G AKA and EAP AKA', handle authentication for 3GPP and non 3GPP devices. However, 5G AKA has vulnerabilities, including linkability attacks. Additionally, quantum computing poses threats, requiring quantum resistant cryptography. While post-quantum cryptography (PQC) is being standardized, its real world robustness remains unproven. Conventional cryptographic schemes offer reliability due to decades of practical use. To bridge this gap, IETF is standardizing hybrid PQC (HPQC), combining classical and quantum resistant methods. Ensuring forward secrecy and quantum resilience in 5G-AKA is critical. To address these issues, we propose 5G AKA HPQC, a protocol maintaining compatibility with existing standards while enhancing security by integrating keys derived from Elliptic Curve Integrated Encryption Scheme (ECIES) and PQC Key Encapsulation Mechanism (KEM). We validate its security using SVO Logic and ProVerif, confirming its robustness. Performance evaluations assess computational and communication overheads, demonstrating a balance between security and efficiency. This research provides key insights into quantum-safe authentication, contributing to future standardization of secure mobile authentication protocols. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02853</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02853</id><created>2025-02-04</created><authors><author><keyname>Bai</keyname><forenames>Shuanghai</forenames></author><author><keyname>Zhou</keyname><forenames>Wanqi</forenames></author><author><keyname>Ding</keyname><forenames>Pengxiang</forenames></author><author><keyname>Zhao</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Donglin</forenames></author><author><keyname>Chen</keyname><forenames>Badong</forenames></author></authors><title>Rethinking Latent Representations in Behavior Cloning: An Information   Bottleneck Approach for Robot Manipulation</title><categories>cs.RO cs.LG</categories><comments>20 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Behavior Cloning (BC) is a widely adopted visual imitation learning method in robot manipulation. Current BC approaches often enhance generalization by leveraging large datasets and incorporating additional visual and textual modalities to capture more diverse information. However, these methods overlook whether the learned representations contain redundant information and lack a solid theoretical foundation to guide the learning process. To address these limitations, we adopt an information-theoretic perspective and introduce mutual information to quantify and mitigate redundancy in latent representations. Building on this, we incorporate the Information Bottleneck (IB) principle into BC, which extends the idea of reducing redundancy by providing a structured framework for compressing irrelevant information while preserving task-relevant features. This work presents the first comprehensive study on redundancy in latent representations across various methods, backbones, and experimental settings, while extending the generalizability of the IB to BC. Extensive experiments and analyses on the CortexBench and LIBERO benchmarks demonstrate significant performance improvements with IB, underscoring the importance of reducing input data redundancy and highlighting its practical value for more practical applications. Project Page: https://baishuanghao.github.io/BC-IB.github.io. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02854</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02854</id><created>2025-02-04</created><authors><author><keyname>Zhang</keyname><forenames>Jiaqing</forenames></author><author><keyname>Yin</keyname><forenames>Mingjia</forenames></author><author><keyname>Wang</keyname><forenames>Hao</forenames></author><author><keyname>Li</keyname><forenames>Yawen</forenames></author><author><keyname>Ye</keyname><forenames>Yuyang</forenames></author><author><keyname>Lou</keyname><forenames>Xingyu</forenames></author><author><keyname>Du</keyname><forenames>Junping</forenames></author><author><keyname>Chen</keyname><forenames>Enhong</forenames></author></authors><title>TD3: Tucker Decomposition Based Dataset Distillation Method for   Sequential Recommendation</title><categories>cs.IR cs.LG</categories><doi>10.1145/3696410.3714613</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the era of data-centric AI, the focus of recommender systems has shifted from model-centric innovations to data-centric approaches. The success of modern AI models is built on large-scale datasets, but this also results in significant training costs. Dataset distillation has emerged as a key solution, condensing large datasets to accelerate model training while preserving model performance. However, condensing discrete and sequentially correlated user-item interactions, particularly with extensive item sets, presents considerable challenges. This paper introduces \textbf{TD3}, a novel \textbf{T}ucker \textbf{D}ecomposition based \textbf{D}ataset \textbf{D}istillation method within a meta-learning framework, designed for sequential recommendation. TD3 distills a fully expressive \emph{synthetic sequence summary} from original data. To efficiently reduce computational complexity and extract refined latent patterns, Tucker decomposition decouples the summary into four factors: \emph{synthetic user latent factor}, \emph{temporal dynamics latent factor}, \emph{shared item latent factor}, and a \emph{relation core} that models their interconnections. Additionally, a surrogate objective in bi-level optimization is proposed to align feature spaces extracted from models trained on both original data and synthetic sequence summary beyond the na\"ive performance matching approach. In the \emph{inner-loop}, an augmentation technique allows the learner to closely fit the synthetic summary, ensuring an accurate update of it in the \emph{outer-loop}. To accelerate the optimization process and address long dependencies, RaT-BPTT is employed for bi-level optimization. Experiments and analyses on multiple public datasets have confirmed the superiority and cross-architecture generalizability of the proposed designs. Codes are released at https://github.com/USTC-StarTeam/TD3. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02856</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02856</id><created>2025-02-04</created><authors><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Li</keyname><forenames>Shaofan</forenames></author></authors><title>PH-VAE: A Polynomial Hierarchical Variational Autoencoder Towards   Disentangled Representation Learning</title><categories>cs.LG</categories><comments>15 pages,14 figures</comments><msc-class>68T07 (Primary) 92B20 (Secondary)</msc-class><acm-class>I.2.6; I.5.1</acm-class><journal-ref>NeurIPS 33 (2020)</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The variational autoencoder (VAE) is a simple and efficient generative artificial intelligence method for modeling complex probability distributions of various types of data, such as images and texts. However, it suffers some main shortcomings, such as lack of interpretability in the latent variables, difficulties in tuning hyperparameters while training, producing blurry, unrealistic downstream outputs or loss of information due to how it calculates loss functions and recovers data distributions, overfitting, and origin gravity effect for small data sets, among other issues. These and other limitations have caused unsatisfactory generation effects for the data with complex distributions. In this work, we proposed and developed a polynomial hierarchical variational autoencoder (PH-VAE), in which we used a polynomial hierarchical date format to generate or to reconstruct the data distributions. In doing so, we also proposed a novel Polynomial Divergence in the loss function to replace or generalize the Kullback-Leibler (KL) divergence, which results in systematic and drastic improvements in both accuracy and reproducibility of the re-constructed distribution function as well as the quality of re-constructed data images while keeping the dataset size the same but capturing fine resolution of the data. Moreover, we showed that the proposed PH-VAE has some form of disentangled representation learning ability. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02858</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02858</id><created>2025-02-04</created><authors><author><keyname>Chen</keyname><forenames>Rui</forenames></author><author><keyname>Sun</keyname><forenames>Yifan</forenames></author><author><keyname>Liu</keyname><forenames>Changliu</forenames></author></authors><title>Dexterous Safe Control for Humanoids in Cluttered Environments via   Projected Safe Set Algorithm</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is critical to ensure safety for humanoid robots in real-world applications without compromising performance. In this paper, we consider the problem of dexterous safety, featuring limb-level geometry constraints for avoiding both external and self-collisions in cluttered environments. Compared to safety with simplified bounding geometries in sprase environments, dexterous safety produces numerous constraints which often lead to infeasible constraint sets when solving for safe robot control. To address this issue, we propose Projected Safe Set Algorithm (p-SSA), an extension of classical safe control algorithms to multi-constraint cases. p-SSA relaxes conflicting constraints in a principled manner, minimizing safety violations to guarantee feasible robot control. We verify our approach in simulation and on a real Unitree G1 humanoid robot performing complex collision avoidance tasks. Results show that p-SSA enables the humanoid to operate robustly in challenging situations with minimal safety violations and directly generalizes to various tasks with zero parameter tuning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02859</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02859</id><created>2025-02-04</created><authors><author><keyname>Zhang</keyname><forenames>Haochen</forenames></author><author><keyname>Zheng</keyname><forenames>Zhong</forenames></author><author><keyname>Xue</keyname><forenames>Lingzhou</forenames></author></authors><title>Gap-Dependent Bounds for Federated $Q$-learning</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present the first gap-dependent analysis of regret and communication cost for on-policy federated $Q$-Learning in tabular episodic finite-horizon Markov decision processes (MDPs). Existing FRL methods focus on worst-case scenarios, leading to $\sqrt{T}$-type regret bounds and communication cost bounds with a $\log T$ term scaling with the number of agents $M$, states $S$, and actions $A$, where $T$ is the average total number of steps per agent. In contrast, our novel framework leverages the benign structures of MDPs, such as a strictly positive suboptimality gap, to achieve a $\log T$-type regret bound and a refined communication cost bound that disentangles exploration and exploitation. Our gap-dependent regret bound reveals a distinct multi-agent speedup pattern, and our gap-dependent communication cost bound removes the dependence on $MSA$ from the $\log T$ term. Notably, our gap-dependent communication cost bound also yields a better global switching cost when $M=1$, removing $SA$ from the $\log T$ term. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02861</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02861</id><created>2025-02-04</created><authors><author><keyname>Shen</keyname><forenames>Judy</forenames></author><author><keyname>Vitercik</keyname><forenames>Ellen</forenames></author><author><keyname>Wikum</keyname><forenames>Anders</forenames></author></authors><title>Algorithms with Calibrated Machine Learning Predictions</title><categories>stat.ML cs.DS cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The field of algorithms with predictions incorporates machine learning advice in the design of online algorithms to improve real-world performance. While this theoretical framework often assumes uniform reliability across all predictions, modern machine learning models can now provide instance-level uncertainty estimates. In this paper, we propose calibration as a principled and practical tool to bridge this gap, demonstrating the benefits of calibrated advice through two case studies: the ski rental and online job scheduling problems. For ski rental, we design an algorithm that achieves optimal prediction-dependent performance and prove that, in high-variance settings, calibrated advice offers more effective guidance than alternative methods for uncertainty quantification. For job scheduling, we demonstrate that using a calibrated predictor leads to significant performance improvements over existing methods. Evaluations on real-world data validate our theoretical findings, highlighting the practical impact of calibration for algorithms with predictions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02862</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02862</id><created>2025-02-04</created><authors><author><keyname>Yue</keyname><forenames>Peiyan</forenames></author><author><keyname>Cai</keyname><forenames>Die</forenames></author><author><keyname>Guo</keyname><forenames>Chu</forenames></author><author><keyname>Liu</keyname><forenames>Mengxing</forenames></author><author><keyname>Xia</keyname><forenames>Jun</forenames></author><author><keyname>Wang</keyname><forenames>Yi</forenames></author></authors><title>Learning Generalizable Features for Tibial Plateau Fracture Segmentation   Using Masked Autoencoder and Limited Annotations</title><categories>eess.IV cs.AI cs.CV</categories><comments>5 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate automated segmentation of tibial plateau fractures (TPF) from computed tomography (CT) requires large amounts of annotated data to train deep learning models, but obtaining such annotations presents unique challenges. The process demands expert knowledge to identify diverse fracture patterns, assess severity, and account for individual anatomical variations, making the annotation process highly time-consuming and expensive. Although semi-supervised learning methods can utilize unlabeled data, existing approaches often struggle with the complexity and variability of fracture morphologies, as well as limited generalizability across datasets. To tackle these issues, we propose an effective training strategy based on masked autoencoder (MAE) for the accurate TPF segmentation in CT. Our method leverages MAE pretraining to capture global skeletal structures and fine-grained fracture details from unlabeled data, followed by fine-tuning with a small set of labeled data. This strategy reduces the dependence on extensive annotations while enhancing the model's ability to learn generalizable and transferable features. The proposed method is evaluated on an in-house dataset containing 180 CT scans with TPF. Experimental results demonstrate that our method consistently outperforms semi-supervised methods, achieving an average Dice similarity coefficient (DSC) of 95.81%, average symmetric surface distance (ASSD) of 1.91mm, and Hausdorff distance (95HD) of 9.42mm with only 20 annotated cases. Moreover, our method exhibits strong transferability when applying to another public pelvic CT dataset with hip fractures, highlighting its potential for broader applications in fracture segmentation tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02863</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02863</id><created>2025-02-04</created><authors><author><keyname>Pataranutaporn</keyname><forenames>Pat</forenames></author><author><keyname>Doudkin</keyname><forenames>Alexander</forenames></author><author><keyname>Maes</keyname><forenames>Pattie</forenames></author></authors><title>OceanChat: The Effect of Virtual Conversational AI Agents on Sustainable   Attitude and Behavior Change</title><categories>cs.HC cs.AI</categories><comments>21 pages, 18 figures, 2 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Marine ecosystems face unprecedented threats from climate change and plastic pollution, yet traditional environmental education often struggles to translate awareness into sustained behavioral change. This paper presents OceanChat, an interactive system leveraging large language models to create conversational AI agents represented as animated marine creatures -- specifically a beluga whale, a jellyfish, and a seahorse -- designed to promote environmental behavior (PEB) and foster awareness through personalized dialogue. Through a between-subjects experiment (N=900), we compared three conditions: (1) Static Scientific Information, providing conventional environmental education through text and images; (2) Static Character Narrative, featuring first-person storytelling from 3D-rendered marine creatures; and (3) Conversational Character Narrative, enabling real-time dialogue with AI-powered marine characters. Our analysis revealed that the Conversational Character Narrative condition significantly increased behavioral intentions and sustainable choice preferences compared to static approaches. The beluga whale character demonstrated consistently stronger emotional engagement across multiple measures, including perceived anthropomorphism and empathy. However, impacts on deeper measures like climate policy support and psychological distance were limited, highlighting the complexity of shifting entrenched beliefs. Our work extends research on sustainability interfaces facilitating PEB and offers design principles for creating emotionally resonant, context-aware AI characters. By balancing anthropomorphism with species authenticity, OceanChat demonstrates how interactive narratives can bridge the gap between environmental knowledge and real-world behavior change. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02866</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02866</id><created>2025-02-04</created><authors><author><keyname>Chang</keyname><forenames>Hung-Fu</forenames></author><author><keyname>Shirazi</keyname><forenames>Mohammad Shokrolah</forenames></author></authors><title>A Systematic Approach for Assessing Large Language Models' Test Case   Generation Capability</title><categories>cs.SE cs.AI</categories><comments>17 pages, 9 figures</comments><acm-class>D.2.5; I.2.7</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Software testing ensures the quality and reliability of software products, but manual test case creation is labor-intensive. With the rise of large language models (LLMs), there is growing interest in unit test creation with LLMs. However, effective assessment of LLM-generated test cases is limited by the lack of standardized benchmarks that comprehensively cover diverse programming scenarios. To address the assessment of LLM's test case generation ability and lacking dataset for evaluation, we propose the Generated Benchmark from Control-Flow Structure and Variable Usage Composition (GBCV) approach, which systematically generates programs used for evaluating LLMs' test generation capabilities. By leveraging basic control-flow structures and variable usage, GBCV provides a flexible framework to create a spectrum of programs ranging from simple to complex. Because GPT-4o and GPT-3-Turbo are publicly accessible models, to present real-world regular user's use case, we use GBCV to assess LLM performance on them. Our findings indicate that GPT-4o performs better on complex program structures, while all models effectively detect boundary values in simple conditions but face challenges with arithmetic computations. This study highlights the strengths and limitations of LLMs in test generation, provides a benchmark framework, and suggests directions for future improvement. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02867</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02867</id><created>2025-02-04</created><authors><author><keyname>Kim</keyname><forenames>Minung</forenames></author><author><keyname>Lee</keyname><forenames>Kawon</forenames></author><author><keyname>Kim</keyname><forenames>Jungmo</forenames></author><author><keyname>Choi</keyname><forenames>Sungho</forenames></author><author><keyname>Han</keyname><forenames>Seungyul</forenames></author></authors><title>Domain-Invariant Per-Frame Feature Extraction for Cross-Domain Imitation   Learning with Visual Observations</title><categories>cs.CV cs.AI cs.LG</categories><comments>8 pages main, 19 pages appendix with reference. Submitted to ICML   2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Imitation learning (IL) enables agents to mimic expert behavior without reward signals but faces challenges in cross-domain scenarios with high-dimensional, noisy, and incomplete visual observations. To address this, we propose Domain-Invariant Per-Frame Feature Extraction for Imitation Learning (DIFF-IL), a novel IL method that extracts domain-invariant features from individual frames and adapts them into sequences to isolate and replicate expert behaviors. We also introduce a frame-wise time labeling technique to segment expert behaviors by timesteps and assign rewards aligned with temporal contexts, enhancing task performance. Experiments across diverse visual environments demonstrate the effectiveness of DIFF-IL in addressing complex visual tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02869</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02869</id><created>2025-02-04</created><authors><author><keyname>Wang</keyname><forenames>Fan</forenames></author><author><keyname>Shao</keyname><forenames>Pengtao</forenames></author><author><keyname>Zhang</keyname><forenames>Yiming</forenames></author><author><keyname>Yu</keyname><forenames>Bo</forenames></author><author><keyname>Liu</keyname><forenames>Shaoshan</forenames></author><author><keyname>Ding</keyname><forenames>Ning</forenames></author><author><keyname>Cao</keyname><forenames>Yang</forenames></author><author><keyname>Kang</keyname><forenames>Yu</forenames></author><author><keyname>Wang</keyname><forenames>Haifeng</forenames></author></authors><title>OmniRL: In-Context Reinforcement Learning by Large-Scale Meta-Training   in Randomized Worlds</title><categories>cs.LG cs.AI</categories><comments>Preprint</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce OmniRL, a highly generalizable in-context reinforcement learning (ICRL) model that is meta-trained on hundreds of thousands of diverse tasks. These tasks are procedurally generated by randomizing state transitions and rewards within Markov Decision Processes. To facilitate this extensive meta-training, we propose two key innovations: 1. An efficient data synthesis pipeline for ICRL, which leverages the interaction histories of diverse behavior policies; and 2. A novel modeling framework that integrates both imitation learning and reinforcement learning (RL) within the context, by incorporating prior knowledge. For the first time, we demonstrate that in-context learning (ICL) alone, without any gradient-based fine-tuning, can successfully tackle unseen Gymnasium tasks through imitation learning, online RL, or offline RL. Additionally, we show that achieving generalized ICRL capabilities-unlike task identification-oriented few-shot learning-critically depends on long trajectories generated by variant tasks and diverse behavior policies. By emphasizing the potential of ICL and departing from pre-training focused on acquiring specific skills, we further underscore the significance of meta-training aimed at cultivating the ability of ICL itself. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02870</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02870</id><created>2025-02-04</created><authors><author><keyname>Wilson</keyname><forenames>Joseph</forenames></author><author><keyname>van der Heide</keyname><forenames>Chris</forenames></author><author><keyname>Hodgkinson</keyname><forenames>Liam</forenames></author><author><keyname>Roosta</keyname><forenames>Fred</forenames></author></authors><title>Uncertainty Quantification with the Empirical Neural Tangent Kernel</title><categories>stat.ML cs.LG</categories><comments>24 pages, 5 figures, 9 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While neural networks have demonstrated impressive performance across various tasks, accurately quantifying uncertainty in their predictions is essential to ensure their trustworthiness and enable widespread adoption in critical systems. Several Bayesian uncertainty quantification (UQ) methods exist that are either cheap or reliable, but not both. We propose a post-hoc, sampling-based UQ method for over-parameterized networks at the end of training. Our approach constructs efficient and meaningful deep ensembles by employing a (stochastic) gradient-descent sampling process on appropriately linearized networks. We demonstrate that our method effectively approximates the posterior of a Gaussian process using the empirical Neural Tangent Kernel. Through a series of numerical experiments, we show that our method not only outperforms competing approaches in computational efficiency (often reducing costs by multiple factors) but also maintains state-of-the-art performance across a variety of UQ metrics for both regression and classification tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02871</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02871</id><created>2025-02-04</created><authors><author><keyname>Yan</keyname><forenames>Yibo</forenames></author><author><keyname>Wang</keyname><forenames>Shen</forenames></author><author><keyname>Huo</keyname><forenames>Jiahao</forenames></author><author><keyname>Ye</keyname><forenames>Jingheng</forenames></author><author><keyname>Chu</keyname><forenames>Zhendong</forenames></author><author><keyname>Hu</keyname><forenames>Xuming</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author><author><keyname>Gomes</keyname><forenames>Carla</forenames></author><author><keyname>Selman</keyname><forenames>Bart</forenames></author><author><keyname>Wen</keyname><forenames>Qingsong</forenames></author></authors><title>Position: Multimodal Large Language Models Can Significantly Advance   Scientific Reasoning</title><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific reasoning, the process through which humans apply logic, evidence, and critical thinking to explore and interpret scientific phenomena, is essential in advancing knowledge reasoning across diverse fields. However, despite significant progress, current scientific reasoning models still struggle with generalization across domains and often fall short of multimodal perception. Multimodal Large Language Models (MLLMs), which integrate text, images, and other modalities, present an exciting opportunity to overcome these limitations and enhance scientific reasoning. Therefore, this position paper argues that MLLMs can significantly advance scientific reasoning across disciplines such as mathematics, physics, chemistry, and biology. First, we propose a four-stage research roadmap of scientific reasoning capabilities, and highlight the current state of MLLM applications in scientific reasoning, noting their ability to integrate and reason over diverse data types. Second, we summarize the key challenges that remain obstacles to achieving MLLM's full potential. To address these challenges, we propose actionable insights and suggestions for the future. Overall, our work offers a novel perspective on MLLM integration with scientific reasoning, providing the LLM community with a valuable vision for achieving Artificial General Intelligence (AGI). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02872</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02872</id><created>2025-02-04</created><authors><author><keyname>Gahler</keyname><forenames>Daniel</forenames></author><author><keyname>Thomas</keyname><forenames>Dean</forenames></author><author><keyname>Lach</keyname><forenames>Slawomir</forenames></author><author><keyname>Cronin</keyname><forenames>Leroy</forenames></author></authors><title>Achieving Operational Universality through a Turing Complete Chemputer</title><categories>cs.CL</categories><comments>18 pages, 7 figures, 28 references</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The most fundamental abstraction underlying all modern computers is the Turing Machine, that is if any modern computer can simulate a Turing Machine, an equivalence which is called Turing completeness, it is theoretically possible to achieve any task that can be algorithmically described by executing a series of discrete unit operations. In chemistry, the ability to program chemical processes is demanding because it is hard to ensure that the process can be understood at a high level of abstraction, and then reduced to practice. Herein we exploit the concept of Turing completeness applied to robotic platforms for chemistry that can be used to synthesise complex molecules through unit operations that execute chemical processes using a chemically-aware programming language, XDL. We leverage the concept of computability by computers to synthesizability of chemical compounds by automated synthesis machines. The results of an interactive demonstration of Turing completeness using the colour gamut and conditional logic are presented and examples of chemical use-cases are discussed. Over 16.7 million combinations of Red, Green, Blue (RGB) colour space were binned into 5 discrete values and measured over 10 regions of interest (ROIs), affording 78 million possible states per step and served as a proxy for conceptual, chemical space exploration. This formal description establishes a formal framework in future chemical programming languages to ensure complex logic operations are expressed and executed correctly, with the possibility of error correction, in the automated and autonomous pursuit of increasingly complex molecules. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02874</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02874</id><created>2025-02-04</created><authors><author><keyname>Temiz</keyname><forenames>Fatih</forenames></author><author><keyname>Ibrahimi</keyname><forenames>Memedhe</forenames></author><author><keyname>Musumeci</keyname><forenames>Francesco</forenames></author><author><keyname>Passera</keyname><forenames>Claudio</forenames></author><author><keyname>Tornatore</keyname><forenames>Massimo</forenames></author></authors><title>Vertical Federated Learning for Failure-Cause Identification in   Disaggregated Microwave Networks</title><categories>cs.NI cs.AI cs.DC cs.LG</categories><comments>6 pages, 7 figure, IEEE ICC 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Machine Learning (ML) has proven to be a promising solution to provide novel scalable and efficient fault management solutions in modern 5G-and-beyond communication networks. In the context of microwave networks, ML-based solutions have received significant attention. However, current solutions can only be applied to monolithic scenarios in which a single entity (e.g., an operator) manages the entire network. As current network architectures move towards disaggregated communication platforms in which multiple operators and vendors collaborate to achieve cost-efficient and reliable network management, new ML-based approaches for fault management must tackle the challenges of sharing business-critical information due to potential conflicts of interest. In this study, we explore the application of Federated Learning in disaggregated microwave networks for failure-cause identification using a real microwave hardware failure dataset. In particular, we investigate the application of two Vertical Federated Learning (VFL), namely using Split Neural Networks (SplitNNs) and Federated Learning based on Gradient Boosting Decision Trees (FedTree), on different multi-vendor deployment scenarios, and we compare them to a centralized scenario where data is managed by a single entity. Our experimental results show that VFL-based scenarios can achieve F1-Scores consistently within at most a 1% gap with respect to a centralized scenario, regardless of the deployment strategies or model types, while also ensuring minimal leakage of sensitive-data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02875</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02875</id><created>2025-02-04</created><authors><author><keyname>Wang</keyname><forenames>Siying</forenames></author><author><keyname>Zhou</keyname><forenames>Yang</forenames></author><author><keyname>Zhao</keyname><forenames>Zhitong</forenames></author><author><keyname>Zhang</keyname><forenames>Ruoning</forenames></author><author><keyname>Shao</keyname><forenames>Jinliang</forenames></author><author><keyname>Chen</keyname><forenames>Wenyu</forenames></author><author><keyname>Cheng</keyname><forenames>Yuhua</forenames></author></authors><title>Heterogeneous Value Decomposition Policy Fusion for Multi-Agent   Cooperation</title><categories>cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Value decomposition (VD) has become one of the most prominent solutions in cooperative multi-agent reinforcement learning. Most existing methods generally explore how to factorize the joint value and minimize the discrepancies between agent observations and characteristics of environmental states. However, direct decomposition may result in limited representation or difficulty in optimization. Orthogonal to designing a new factorization scheme, in this paper, we propose Heterogeneous Policy Fusion (HPF) to integrate the strengths of various VD methods. We construct a composite policy set to select policies for interaction adaptively. Specifically, this adaptive mechanism allows agents' trajectories to benefit from diverse policy transitions while incorporating the advantages of each factorization method. Additionally, HPF introduces a constraint between these heterogeneous policies to rectify the misleading update caused by the unexpected exploratory or suboptimal non-cooperation. Experimental results on cooperative tasks show HPF's superior performance over multiple baselines, proving its effectiveness and ease of implementation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02877</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02877</id><created>2025-02-04</created><authors><author><keyname>Chen</keyname><forenames>Evan</forenames></author><author><keyname>Lin</keyname><forenames>Frank Po-Chen</forenames></author><author><keyname>Han</keyname><forenames>Dong-Jun</forenames></author><author><keyname>Brinton</keyname><forenames>Christopher G.</forenames></author></authors><title>Differentially-Private Multi-Tier Federated Learning: A Formal Analysis   and Evaluation</title><categories>cs.NI</categories><comments>This paper is under review in IEEE/ACM Transactions on Networking   Special Issue on AI and Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While federated learning (FL) eliminates the transmission of raw data over a network, it is still vulnerable to privacy breaches from the communicated model parameters. Differential privacy (DP) is often employed to address such issues. However, the impact of DP on FL in multi-tier networks -- where hierarchical aggregations couple noise injection decisions at different tiers, and trust models are heterogeneous across subnetworks -- is not well understood. To fill this gap, we develop \underline{M}ulti-Tier \underline{F}ederated Learning with \underline{M}ulti-Tier \underline{D}ifferential \underline{P}rivacy ({\tt M$^2$FDP}), a DP-enhanced FL methodology for jointly optimizing privacy and performance over such networks. One of the key principles of {\tt M$^2$FDP} is to adapt DP noise injection across the established edge/fog computing hierarchy (e.g., edge devices, intermediate nodes, and other tiers up to cloud servers) according to the trust models in different subnetworks. We conduct a comprehensive analysis of the convergence behavior of {\tt M$^2$FDP} under non-convex problem settings, revealing conditions on parameter tuning under which the training process converges sublinearly to a finite stationarity gap that depends on the network hierarchy, trust model, and target privacy level. We show how these relationships can be employed to develop an adaptive control algorithm for {\tt M$^2$FDP} that tunes properties of local model training to minimize energy, latency, and the stationarity gap while meeting desired convergence and privacy criterion. Subsequent numerical evaluations demonstrate that {\tt M$^2$FDP} obtains substantial improvements in these metrics over baselines for different privacy budgets and system configurations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02879</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02879</id><created>2025-02-04</created><authors><author><keyname>Ede</keyname><forenames>Nkiru</forenames></author><author><keyname>Dietrich</keyname><forenames>Jens</forenames></author><author><keyname>Zülicke</keyname><forenames>Ulrich</forenames></author></authors><title>Popularity and Innovation in Maven Central</title><categories>cs.SE</categories><comments>5 pages, 4 figures, accepted at the 2025 IEEE/ACM 22nd International   Conference on Mining Software Repositories (MSR) - Mining Challenge Track</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Maven Central is a large popular repository of Java components that has evolved over the last 20 years. The distribution of dependencies indicates that the repository is dominated by a relatively small number of components other components depend on. The question is whether those elites are static, or change over time, and how this relates to innovation in the Maven ecosystem. We study those questions using several metrics. We find that elites are dynamic, and that the rate of innovation is slowing as the repository ages but remains healthy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02880</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02880</id><created>2025-02-04</created><authors><author><keyname>Lira</keyname><forenames>Benjamin</forenames></author><author><keyname>Rogers</keyname><forenames>Todd</forenames></author><author><keyname>Goldstein</keyname><forenames>Daniel G.</forenames></author><author><keyname>Ungar</keyname><forenames>Lyle</forenames></author><author><keyname>Duckworth</keyname><forenames>Angela L.</forenames></author></authors><title>Learning from examples: AI assistance can enhance rather than hinder   skill development</title><categories>cs.HC</categories><comments>25 pages, 13 figures, submitted to Nature Human Behaviour</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  It is widely believed that outsourcing cognitive work to AI boosts immediate productivity at the expense of long-term human capital development. An opposing possibility is that AI tools can support skill development by providing just-in-time, high-quality, personalized examples. This work explores whether using an AI writing tool undermines or supports performance on later unaided writing. In Study 1, forecasters predicted that practicing writing cover letters with an AI tool would impair learning compared to practicing alone. However, in Study 2, participants randomly assigned to practice writing with AI improved more on a subsequent writing test than those assigned to practice without AI (d = 0.40***) -- despite exerting less effort, whether measured by time on task, keystrokes, or subjective ratings. In Study 3, participants who had practiced writing with AI again outperformed those who practiced without AI (d = 0.31***). Consistent with the positive impact of exposure to high-quality examples, these participants performed just as well as those who viewed -- but could not edit -- an AI-generated cover letter (d = 0.03, ns). In both Studies 2 and 3, the benefits of practicing with AI persisted in a one-day follow-up writing test. Collectively, these findings constitute an existence proof that, contrary to participants' intuition, using AI tools can improve, rather than undermine, learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02883</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02883</id><created>2025-02-04</created><authors><author><keyname>Yu</keyname><forenames>Xiaofan</forenames></author><author><keyname>Hu</keyname><forenames>Lanxiang</forenames></author><author><keyname>Reichman</keyname><forenames>Benjamin</forenames></author><author><keyname>Chu</keyname><forenames>Dylan</forenames></author><author><keyname>Chandrupatla</keyname><forenames>Rushil</forenames></author><author><keyname>Zhang</keyname><forenames>Xiyuan</forenames></author><author><keyname>Heck</keyname><forenames>Larry</forenames></author><author><keyname>Rosing</keyname><forenames>Tajana</forenames></author></authors><title>SensorChat: Answering Qualitative and Quantitative Questions during   Long-Term Multimodal Sensor Interactions</title><categories>cs.AI cs.HC</categories><comments>Under review</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Natural language interaction with sensing systems is crucial for enabling all users to comprehend sensor data and its impact on their everyday lives. However, existing systems, which typically operate in a Question Answering (QA) manner, are significantly limited in terms of the duration and complexity of sensor data they can handle. In this work, we introduce SensorChat, the first end-to-end QA system designed for long-term sensor monitoring with multimodal and high-dimensional data including time series. SensorChat effectively answers both qualitative (requiring high-level reasoning) and quantitative (requiring accurate responses derived from sensor data) questions in real-world scenarios. To achieve this, SensorChat uses an innovative three-stage pipeline that includes question decomposition, sensor data query, and answer assembly. The first and third stages leverage Large Language Models (LLMs) for intuitive human interactions and to guide the sensor data query process. Unlike existing multimodal LLMs, SensorChat incorporates an explicit query stage to precisely extract factual information from long-duration sensor data. We implement SensorChat and demonstrate its capability for real-time interactions on a cloud server while also being able to run entirely on edge platforms after quantization. Comprehensive QA evaluations show that SensorChat achieves up to 26% higher answer accuracy than state-of-the-art systems on quantitative questions. Additionally, a user study with eight volunteers highlights SensorChat's effectiveness in handling qualitative and open-ended questions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02885</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02885</id><created>2025-02-04</created><authors><author><keyname>Chen</keyname><forenames>Junxiang</forenames></author><author><keyname>yang</keyname><forenames>Baoyao</forenames></author><author><keyname>Yao</keyname><forenames>Wenbin</forenames></author></authors><title>Expertized Caption Auto-Enhancement for Video-Text Retrieval</title><categories>cs.CV cs.AI cs.LG</categories><acm-class>H.3.3; I.2.10; I.2.7; H.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The burgeoning field of video-text retrieval has witnessed significant advancements with the advent of deep learning. However, the challenge of matching text and video persists due to inadequate textual descriptions of videos. The substantial information gap between the two modalities hinders a comprehensive understanding of videos, resulting in ambiguous retrieval results. While rewriting methods based on large language models have been proposed to broaden text expressions, carefully crafted prompts are essential to ensure the reasonableness and completeness of the rewritten texts. This paper proposes an automatic caption enhancement method that enhances expression quality and mitigates empiricism in augmented captions through self-learning. Additionally, an expertized caption selection mechanism is designed and introduced to customize augmented captions for each video, facilitating video-text matching. Our method is entirely data-driven, which not only dispenses with heavy data collection and computation workload but also improves self-adaptability by circumventing lexicon dependence and introducing personalized matching. The superiority of our method is validated by state-of-the-art results on various benchmarks, specifically achieving Top-1 recall accuracy of 68.5% on MSR-VTT, 68.1% on MSVD, and 62.0% on DiDeMo. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02886</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02886</id><created>2025-02-04</created><authors><author><keyname>Barker</keyname><forenames>Ryan</forenames></author></authors><title>Advancements in Mobile Edge Computing and Open RAN: Leveraging   Artificial Intelligence and Machine Learning for Wireless Systems</title><categories>cs.NI</categories><comments>6 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Mobile Edge Computing (MEC) and Open Radio Access Networks (ORAN) are transformative technologies in the development of next-generation wireless communication systems. MEC pushes computational resources closer to end-users, enabling low latency and efficient processing, while ORAN promotes interoperability and openness in radio networks, thereby fostering innovation. This paper explores recent advancements in these two domains, with a particular focus on how Artificial Intelligence (AI) and Machine Learning (ML) techniques are being utilized to solve complex wireless challenges. In MEC, Deep Reinforcement Learning (DRL) is leveraged for optimizing computation offloading, ensuring energy-efficient solutions, and meeting Quality of Service (QoS) requirements. In ORAN, AI/ML is used to develop intelligent xApps for network slicing, scheduling, and online training to enhance network adaptability. This reading report provides an in-depth analysis of multiple key papers, discusses the methodologies employed, and highlights the impact of these technologies in improving network efficiency and scalability. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02887</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02887</id><created>2025-02-04</created><authors><author><keyname>Perlaza</keyname><forenames>Samir M.</forenames></author><author><keyname>Bisson</keyname><forenames>Gaetan</forenames></author></authors><title>Variations on the Expectation Due to Changes in the Probability Measure</title><categories>cs.IT cs.LG math.IT math.PR math.ST stat.TH</categories><comments>Submitted to the IEEE International Symposium on Information Theory   (ISIT2025)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Closed-form expressions are presented for the variation of the expectation of a given function due to changes in the probability measure used for the expectation. They unveil interesting connections with Gibbs probability measures, the mutual information, and the lautum information. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02889</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02889</id><created>2025-02-05</created><authors><author><keyname>Barker</keyname><forenames>Ryan</forenames></author></authors><title>From DeepSense to Open RAN: AI/ML Advancements in Dynamic Spectrum   Sensing and Their Applications</title><categories>cs.NI eess.SP</categories><comments>6 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The integration of Artificial Intelligence (AI) and Machine Learning (ML) in next-generation wireless communication systems has become a cornerstone for advancing intelligent, adaptive, and scalable networks. This reading report examines key innovations in dynamic spectrum sensing (DSS), beginning with the foundational DeepSense framework, which uses convolutional neural networks (CNNs) and spectrogram-based analysis for real-time wideband spectrum monitoring. Building on this groundwork, it highlights advancements such as DeepSweep and Wideband Signal Stitching, which address the challenges of scalability, latency, and dataset diversity through parallel processing, semantic segmentation, and robust data augmentation strategies. The report then explores Open Radio Access Networks (ORAN), focusing on AI/ML-driven enhancements for UAV experimentation, digital twin-based optimization, network slicing, and self-healing xApp development. By bridging AI-based DSS methodologies with ORAN's open, vendor-neutral architecture, these studies underscore the potential of software-defined, intelligent infrastructures in enabling efficient, resilient, and self-optimizing networks for 5G/6G ecosystems. Through this synthesis, the report highlights AI's transformative role in shaping the future of wireless communication and autonomous systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02891</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02891</id><created>2025-02-05</created><authors><author><keyname>Rubab</keyname><forenames>Fizza</forenames></author><author><keyname>Tong</keyname><forenames>Yiying</forenames></author></authors><title>INST-Sculpt: Interactive Stroke-based Neural SDF Sculpting</title><categories>cs.GR cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent advances in implicit neural representations have made them a popular choice for modeling 3D geometry, achieving impressive results in tasks such as shape representation, reconstruction, and learning priors. However, directly editing these representations poses challenges due to the complex relationship between model weights and surface regions they influence. Among such editing tools, sculpting, which allows users to interactively carve or extrude the surface, is a valuable editing operation to the graphics and modeling community. While traditional mesh-based tools like ZBrush facilitate fast and intuitive edits, a comparable toolkit for sculpting neural SDFs is currently lacking. We introduce a framework that enables interactive surface sculpting edits directly on neural implicit representations. Unlike previous works limited to spot edits, our approach allows users to perform stroke-based modifications on the fly, ensuring intuitive shape manipulation without switching representations. By employing tubular neighborhoods to sample strokes and custom brush profiles, we achieve smooth deformations along user-defined curves, providing precise control over the sculpting process. Our method demonstrates that intricate and versatile edits can be made while preserving the smooth nature of implicit representations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02893</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02893</id><created>2025-02-05</created><authors><author><keyname>Zhang</keyname><forenames>Yejian</forenames></author><author><keyname>Takada</keyname><forenames>Shingo</forenames></author></authors><title>Lowering the Barrier of Machine Learning: Achieving Zero Manual Labeling   in Review Classification Using LLMs</title><categories>cs.CL</categories><comments>Accepted to 2025 11th International Conference on Computing and   Artificial Intelligence (ICCAI 2025)</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  With the internet's evolution, consumers increasingly rely on online reviews for service or product choices, necessitating that businesses analyze extensive customer feedback to enhance their offerings. While machine learning-based sentiment classification shows promise in this realm, its technical complexity often bars small businesses and individuals from leveraging such advancements, which may end up making the competitive gap between small and large businesses even bigger in terms of improving customer satisfaction. This paper introduces an approach that integrates large language models (LLMs), specifically Generative Pre-trained Transformer (GPT) and Bidirectional Encoder Representations from Transformers (BERT)-based models, making it accessible to a wider audience. Our experiments across various datasets confirm that our approach retains high classification accuracy without the need for manual labeling, expert knowledge in tuning and data annotation, or substantial computational power. By significantly lowering the barriers to applying sentiment classification techniques, our methodology enhances competitiveness and paves the way for making machine learning technology accessible to a broader audience. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02895</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02895</id><created>2025-02-05</created><authors><author><keyname>Yamamura</keyname><forenames>Keiichiro</forenames></author><author><keyname>Mitsutake</keyname><forenames>Toru</forenames></author><author><keyname>Ishikura</keyname><forenames>Hiroki</forenames></author><author><keyname>Kusuhara</keyname><forenames>Daiki</forenames></author><author><keyname>Yoshida</keyname><forenames>Akihiro</forenames></author><author><keyname>Fujisawa</keyname><forenames>Katsuki</forenames></author></authors><title>Enhancing Quantum-ready QUBO-based Suppression for Object Detection with   Appearance and Confidence Features</title><categories>cs.CV</categories><comments>8 pages for main contents, 3 pages for appendix, 3 pages for   reference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quadratic Unconstrained Binary Optimization (QUBO)-based suppression in object detection is known to have superiority to conventional Non-Maximum Suppression (NMS), especially for crowded scenes where NMS possibly suppresses the (partially-) occluded true positives with low confidence scores. Whereas existing QUBO formulations are less likely to miss occluded objects than NMS, there is room for improvement because existing QUBO formulations naively consider confidence scores and pairwise scores based on spatial overlap between predictions. This study proposes new QUBO formulations that aim to distinguish whether the overlap between predictions is due to the occlusion of objects or due to redundancy in prediction, i.e., multiple predictions for a single object. The proposed QUBO formulation integrates two features into the pairwise score of the existing QUBO formulation: i) the appearance feature calculated by the image similarity metric and ii) the product of confidence scores. These features are derived from the hypothesis that redundant predictions share a similar appearance feature and (partially-) occluded objects have low confidence scores, respectively. The proposed methods demonstrate significant advancement over state-of-the-art QUBO-based suppression without a notable increase in runtime, achieving up to 4.54 points improvement in mAP and 9.89 points gain in mAR. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02896</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02896</id><created>2025-02-05</created><authors><author><keyname>Allen</keyname><forenames>Bradley P.</forenames></author><author><keyname>Groth</keyname><forenames>Paul T.</forenames></author></authors><title>A Benchmark for the Detection of Metalinguistic Disagreements between   LLMs and Knowledge Graphs</title><categories>cs.CL cs.AI</categories><comments>6 pages, 2 tables, to appear in Reham Alharbi, Jacopo de Berardinis,   Paul Groth, Albert Mero\~no-Pe\~nuela, Elena Simperl, Valentina Tamma (eds.),   ISWC 2024 Special Session on Harmonising Generative AI and Semantic Web   Technologies. CEUR-WS.org (forthcoming), for associated code and data see   https://github.com/bradleypallen/trex-metalinguistic-disagreement</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Evaluating large language models (LLMs) for tasks like fact extraction in support of knowledge graph construction frequently involves computing accuracy metrics using a ground truth benchmark based on a knowledge graph (KG). These evaluations assume that errors represent factual disagreements. However, human discourse frequently features metalinguistic disagreement, where agents differ not on facts but on the meaning of the language used to express them. Given the complexity of natural language processing and generation using LLMs, we ask: do metalinguistic disagreements occur between LLMs and KGs? Based on an investigation using the T-REx knowledge alignment dataset, we hypothesize that metalinguistic disagreement does in fact occur between LLMs and KGs, with potential relevance for the practice of knowledge graph engineering. We propose a benchmark for evaluating the detection of factual and metalinguistic disagreements between LLMs and KGs. An initial proof of concept of such a benchmark is available on Github. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02901</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02901</id><created>2025-02-05</created><authors><author><keyname>Konicki</keyname><forenames>Christine</forenames></author><author><keyname>Chakraborty</keyname><forenames>Mithun</forenames></author><author><keyname>Wellman</keyname><forenames>Michael P.</forenames></author></authors><title>Policy Abstraction and Nash Refinement in Tree-Exploiting PSRO</title><categories>cs.GT cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Policy Space Response Oracles (PSRO) interleaves empirical game-theoretic analysis with deep reinforcement learning (DRL) to solve games too complex for traditional analytic methods. Tree-exploiting PSRO (TE-PSRO) is a variant of this approach that iteratively builds a coarsened empirical game model in extensive form using data obtained from querying a simulator that represents a detailed description of the game. We make two main methodological advances to TE-PSRO that enhance its applicability to complex games of imperfect information. First, we introduce a scalable representation for the empirical game tree where edges correspond to implicit policies learned through DRL. These policies cover conditions in the underlying game abstracted in the game model, supporting sustainable growth of the tree over epochs. Second, we leverage extensive form in the empirical model by employing refined Nash equilibria to direct strategy exploration. To enable this, we give a modular and scalable algorithm based on generalized backward induction for computing a subgame perfect equilibrium (SPE) in an imperfect-information game. We experimentally evaluate our approach on a suite of games including an alternating-offer bargaining game with outside offers; our results demonstrate that TE-PSRO converges toward equilibrium faster when new strategies are generated based on SPE rather than Nash equilibrium, and with reasonable time/memory requirements for the growing empirical model. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02903</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02903</id><created>2025-02-05</created><authors><author><keyname>Manchanda</keyname><forenames>Sahil</forenames></author><author><keyname>Shivaswamy</keyname><forenames>Pannaga</forenames></author></authors><title>What is in a name? Mitigating Name Bias in Text Embeddings via   Anonymization</title><categories>cs.CL cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text-embedding models often exhibit biases arising from the data on which they are trained. In this paper, we examine a hitherto unexplored bias in text-embeddings: bias arising from the presence of $\textit{names}$ such as persons, locations, organizations etc. in the text. Our study shows how the presence of $\textit{name-bias}$ in text-embedding models can potentially lead to erroneous conclusions in assessment of thematic similarity.Text-embeddings can mistakenly indicate similarity between texts based on names in the text, even when their actual semantic content has no similarity or indicate dissimilarity simply because of the names in the text even when the texts match semantically. We first demonstrate the presence of name bias in different text-embedding models and then propose $\textit{text-anonymization}$ during inference which involves removing references to names, while preserving the core theme of the text. The efficacy of the anonymization approach is demonstrated on two downstream NLP tasks, achieving significant performance gains. Our simple and training-optimization-free approach offers a practical and easily implementable solution to mitigate name bias. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02904</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02904</id><created>2025-02-05</created><authors><author><keyname>Wang</keyname><forenames>Linghe</forenames></author><author><keyname>Lee</keyname><forenames>Minhwa</forenames></author><author><keyname>Volkov</keyname><forenames>Ross</forenames></author><author><keyname>Chau</keyname><forenames>Luan Tuyen</forenames></author><author><keyname>Kang</keyname><forenames>Dongyeop</forenames></author></authors><title>ScholaWrite: A Dataset of End-to-End Scholarly Writing Process</title><categories>cs.HC cs.CL q-bio.NC</categories><comments>Equal contribution: Linghe Wang, Minhwa Lee | project page:   https://minnesotanlp.github.io/scholawrite/</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Writing is a cognitively demanding task involving continuous decision-making, heavy use of working memory, and frequent switching between multiple activities. Scholarly writing is particularly complex as it requires authors to coordinate many pieces of multiform knowledge. To fully understand writers' cognitive thought process, one should fully decode the end-to-end writing data (from individual ideas to final manuscript) and understand their complex cognitive mechanisms in scholarly writing. We introduce ScholaWrite dataset, the first-of-its-kind keystroke logs of an end-to-end scholarly writing process for complete manuscripts, with thorough annotations of cognitive writing intentions behind each keystroke. Our dataset includes LaTeX-based keystroke data from five preprints with nearly 62K total text changes and annotations across 4 months of paper writing. ScholaWrite shows promising usability and applications (e.g., iterative self-writing) for the future development of AI writing assistants for academic research, which necessitate complex methods beyond LLM prompting. Our experiments clearly demonstrated the importance of collection of end-to-end writing data, rather than the final manuscript, for the development of future writing assistants to support the cognitive thinking process of scientists. Our de-identified dataset, demo, and code repository are available on our project page. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02905</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02905</id><created>2025-02-05</created><authors><author><keyname>Cheng</keyname><forenames>Mouyang</forenames></author><author><keyname>Fu</keyname><forenames>Chu-Liang</forenames></author><author><keyname>Okabe</keyname><forenames>Ryotaro</forenames></author><author><keyname>Chotrattanapituk</keyname><forenames>Abhijatmedhi</forenames></author><author><keyname>Boonkird</keyname><forenames>Artittaya</forenames></author><author><keyname>Hung</keyname><forenames>Nguyen Tuan</forenames></author><author><keyname>Li</keyname><forenames>Mingda</forenames></author></authors><title>AI-driven materials design: a mini-review</title><categories>cond-mat.mtrl-sci cs.LG</categories><comments>18 pages, 7 figures, 1 table; Review article</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Materials design is an important component of modern science and technology, yet traditional approaches rely heavily on trial-and-error and can be inefficient. Computational techniques, enhanced by modern artificial intelligence (AI), have greatly accelerated the design of new materials. Among these approaches, inverse design has shown great promise in designing materials that meet specific property requirements. In this mini-review, we summarize key computational advancements for materials design over the past few decades. We follow the evolution of relevant materials design techniques, from high-throughput forward machine learning (ML) methods and evolutionary algorithms, to advanced AI strategies like reinforcement learning (RL) and deep generative models. We highlight the paradigm shift from conventional screening approaches to inverse generation driven by deep generative models. Finally, we discuss current challenges and future perspectives of materials inverse design. This review may serve as a brief guide to the approaches, progress, and outlook of designing future functional materials with technological relevance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02907</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02907</id><created>2025-02-05</created><authors><author><keyname>Villa</keyname><forenames>Jacopo</forenames></author><author><keyname>McMahon</keyname><forenames>Jay W.</forenames></author><author><keyname>Nesnas</keyname><forenames>Issa A. D.</forenames></author></authors><title>PoleStack: Robust Pole Estimation of Irregular Objects from Silhouette   Stacking</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm to estimate the rotation pole of a principal-axis rotator using silhouette images collected from multiple camera poses. First, a set of images is stacked to form a single silhouette-stack image, where the object's rotation introduces reflective symmetry about the imaged pole direction. We estimate this projected-pole direction by identifying maximum symmetry in the silhouette stack. To handle unknown center-of-mass image location, we apply the Discrete Fourier Transform to produce the silhouette-stack amplitude spectrum, achieving translation invariance and increased robustness to noise. Second, the 3D pole orientation is estimated by combining two or more projected-pole measurements collected from different camera orientations. We demonstrate degree-level pole estimation accuracy using low-resolution imagery, showing robustness to severe surface shadowing and centroid-based image-registration errors. The proposed approach could be suitable for pole estimation during both the approach phase toward a target object and while hovering. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02908</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02908</id><created>2025-02-05</created><authors><author><keyname>Cho</keyname><forenames>Hyunjoon</forenames></author><author><keyname>Kang</keyname><forenames>Sungmin</forenames></author><author><keyname>An</keyname><forenames>Gabin</forenames></author><author><keyname>Yoo</keyname><forenames>Shin</forenames></author></authors><title>COSMosFL: Ensemble of Small Language Models for Fault Localisation</title><categories>cs.SE cs.LG</categories><comments>LLM4Code 2025 Workshop</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  LLMs are rapidly being adopted to build powerful tools and agents for software engineering, but most of them rely heavily on extremely large closed-source models. This, in turn, can hinder wider adoption due to security issues as well as financial cost and environmental impact. Recently, a number of open source Small Language Models (SLMs) are being released and gaining traction. While SLMs are smaller, more energy-efficient, and therefore easier to locally deploy, they tend to show worse performance when compared to larger closed LLMs. We present COSMos, a task-level LLM ensemble technique that uses voting mechanism, to provide a broader range of choice between SLMs and LLMs. We instantiate COSMos with an LLM-based Fault Localisation technique, AutoFL, and report the cost-benefit trade-off between LLM accuracy and various costs such as energy consumption, inference time, and the number of tokens used. An empirical evaluation using Defects4J shows that COSMos can build effective ensembles that can achieve Pareto-optimality in terms of FL accuracy and inference cost, when compared to individual models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02909</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02909</id><created>2025-02-05</created><authors><author><keyname>Jayasuriya</keyname><forenames>Dinithi</forenames><affiliation>Intel Labs, Oregon</affiliation></author><author><keyname>Tayebati</keyname><forenames>Sina</forenames><affiliation>Intel Labs, Oregon</affiliation></author><author><keyname>Ettori</keyname><forenames>Davide</forenames><affiliation>Intel Labs, Oregon</affiliation></author><author><keyname>Krishnan</keyname><forenames>Ranganath</forenames><affiliation>Intel Labs, Oregon</affiliation></author><author><keyname>Trivedi</keyname><forenames>Amit Ranjan</forenames><affiliation>Intel Labs, Oregon</affiliation></author></authors><title>SPARC: Subspace-Aware Prompt Adaptation for Robust Continual Learning in   LLMs</title><categories>cs.LG cs.AI cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose SPARC, a lightweight continual learning framework for large language models (LLMs) that enables efficient task adaptation through prompt tuning in a lower-dimensional space. By leveraging principal component analysis (PCA), we identify a compact subspace of the training data. Optimizing prompts in this lower-dimensional space enhances training efficiency, as it focuses updates on the most relevant features while reducing computational overhead. Furthermore, since the model's internal structure remains unaltered, the extensive knowledge gained from pretraining is fully preserved, ensuring that previously learned information is not compromised during adaptation. Our method achieves high knowledge retention in both task-incremental and domain-incremental continual learning setups while fine-tuning only 0.04% of the model's parameters. Additionally, by integrating LoRA, we enhance adaptability to computational constraints, allowing for a tradeoff between accuracy and training cost. Experiments on the SuperGLUE benchmark demonstrate that our PCA-based prompt tuning combined with LoRA maintains full knowledge retention while improving accuracy, utilizing only 1% of the model's parameters. These results establish our approach as a scalable and resource-efficient solution for continual learning in LLMs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02910</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02910</id><created>2025-02-05</created><authors><author><keyname>Kim</keyname><forenames>Somin</forenames></author><author><keyname>Yoo</keyname><forenames>Shin</forenames></author></authors><title>DANDI: Diffusion as Normative Distribution for Deep Neural Network Input</title><categories>cs.SE cs.LG</categories><comments>DeepTest 2025 Workshop</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Surprise Adequacy (SA) has been widely studied as a test adequacy metric that can effectively guide software engineers towards inputs that are more likely to reveal unexpected behaviour of Deep Neural Networks (DNNs). Intuitively, SA is an out-of-distribution metric that quantifies the dissimilarity between the given input and the training data: if a new input is very different from those seen during training, the DNN is more likely to behave unexpectedly against the input. While SA has been widely adopted as a test prioritization method, its major weakness is the fact that the computation of the metric requires access to the training dataset, which is often not allowed in real-world use cases. We present DANDI, a technique that generates a surrogate input distribution using Stable Diffusion to compute SA values without requiring the original training data. An empirical evaluation of DANDI applied to image classifiers for CIFAR10 and ImageNet-1K shows that SA values computed against synthetic data are highly correlated with the values computed against the training data, with Spearman Rank correlation value of 0.852 for ImageNet-1K and 0.881 for CIFAR-10. Further, we show that SA value computed by DANDI achieves can prioritize inputs as effectively as those computed using the training data, when testing DNN models mutated by DeepMutation. We believe that DANDI can significantly improve the usability of SA for practical DNN testing. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02911</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02911</id><created>2025-02-05</created><authors><author><keyname>Zhu</keyname><forenames>Zicheng</forenames></author><author><keyname>Tan</keyname><forenames>Yugin</forenames></author><author><keyname>Yamashita</keyname><forenames>Naomi</forenames></author><author><keyname>Lee</keyname><forenames>Yi-Chieh</forenames></author><author><keyname>Zhang</keyname><forenames>Renwen</forenames></author></authors><title>The Benefits of Prosociality towards AI Agents: Examining the Effects of   Helping AI Agents on Human Well-Being</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prosocial behaviors, such as helping others, are well-known to enhance human well-being. While there is a growing trend of humans helping AI agents, it remains unclear whether the well-being benefits of helping others extend to interactions with non-human entities. To address this, we conducted an experiment (N = 295) to explore how helping AI agents impacts human well-being, especially when the agents fulfill human basic psychological needs--relatedness, competence, and autonomy--during the interaction. Our findings showed that helping AI agents reduced participants' feelings of loneliness. When AI met participants' needs for competence and autonomy during the helping process, there was a further decrease in loneliness and an increase in positive affect. However, when AI did not meet participants' need for relatedness, participants experienced an increase in positive affect. We discuss the implications of these findings for understanding how AI can support human well-being. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02912</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02912</id><created>2025-02-05</created><authors><author><keyname>Kim</keyname><forenames>Namwoo</forenames></author><author><keyname>Yabe</keyname><forenames>Takahiro</forenames></author><author><keyname>Park</keyname><forenames>Chanyoung</forenames></author><author><keyname>Yoon</keyname><forenames>Yoonjin</forenames></author></authors><title>MobiCLR: Mobility Time Series Contrastive Learning for Urban Region   Representations</title><categories>cs.LG cs.AI</categories><comments>Submitted to Information Sciences (under review)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, learning effective representations of urban regions has gained significant attention as a key approach to understanding urban dynamics and advancing smarter cities. Existing approaches have demonstrated the potential of leveraging mobility data to generate latent representations, providing valuable insights into the intrinsic characteristics of urban areas. However, incorporating the temporal dynamics and detailed semantics inherent in human mobility patterns remains underexplored. To address this gap, we propose a novel urban region representation learning model, Mobility Time Series Contrastive Learning for Urban Region Representations (MobiCLR), designed to capture semantically meaningful embeddings from inflow and outflow mobility patterns. MobiCLR uses contrastive learning to enhance the discriminative power of its representations, applying an instance-wise contrastive loss to capture distinct flow-specific characteristics. Additionally, we develop a regularizer to align output features with these flow-specific representations, enabling a more comprehensive understanding of mobility dynamics. To validate our model, we conduct extensive experiments in Chicago, New York, and Washington, D.C. to predict income, educational attainment, and social vulnerability. The results demonstrate that our model outperforms state-of-the-art models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02913</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02913</id><created>2025-02-05</created><authors><author><keyname>Meng</keyname><forenames>Jiayang</forenames></author><author><keyname>Huang</keyname><forenames>Tao</forenames></author><author><keyname>Shi</keyname><forenames>Xin</forenames></author><author><keyname>Huang</keyname><forenames>Qingyu</forenames></author><author><keyname>Hou</keyname><forenames>Chen</forenames></author><author><keyname>Chen</keyname><forenames>Hong</forenames></author></authors><title>Privacy Token: Surprised to Find Out What You Accidentally Revealed</title><categories>cs.LG cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The widespread deployment of deep learning models in privacy-sensitive domains has amplified concerns regarding privacy risks, particularly those stemming from gradient leakage during training. Current privacy assessments primarily rely on post-training attack simulations. However, these methods are inherently reactive, unable to encompass all potential attack scenarios, and often based on idealized adversarial assumptions. These limitations underscore the need for proactive approaches to privacy risk assessment during the training process. To address this gap, we propose the concept of privacy tokens, which are derived directly from private gradients during training. Privacy tokens encapsulate gradient features and, when combined with data features, offer valuable insights into the extent of private information leakage from training data, enabling real-time measurement of privacy risks without relying on adversarial attack simulations. Additionally, we employ Mutual Information (MI) as a robust metric to quantify the relationship between training data and gradients, providing precise and continuous assessments of privacy leakage throughout the training process. Extensive experiments validate our framework, demonstrating the effectiveness of privacy tokens and MI in identifying and quantifying privacy risks. This proactive approach marks a significant advancement in privacy monitoring, promoting the safer deployment of deep learning models in sensitive applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02917</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02917</id><created>2025-02-05</created><authors><author><keyname>Tian</keyname><forenames>Yuan</forenames></author><author><keyname>Zhou</keyname><forenames>Wenqi</forenames></author><author><keyname>Viscione</keyname><forenames>Michele</forenames></author><author><keyname>Dong</keyname><forenames>Hao</forenames></author><author><keyname>Kammer</keyname><forenames>David</forenames></author><author><keyname>Fink</keyname><forenames>Olga</forenames></author></authors><title>Interactive Symbolic Regression through Offline Reinforcement Learning:   A Co-Design Framework</title><categories>cs.LG cs.AI cs.SC</categories><comments>arXiv admin note: text overlap with arXiv:2402.05306</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Symbolic Regression (SR) holds great potential for uncovering underlying mathematical and physical relationships from observed data. However, the vast combinatorial space of possible expressions poses significant challenges for both online search methods and pre-trained transformer models. Additionally, current state-of-the-art approaches typically do not consider the integration of domain experts' prior knowledge and do not support iterative interactions with the model during the equation discovery process. To address these challenges, we propose the Symbolic Q-network (Sym-Q), an advanced interactive framework for large-scale symbolic regression. Unlike previous large-scale transformer-based SR approaches, Sym-Q leverages reinforcement learning without relying on a transformer-based decoder. This formulation allows the agent to learn through offline reinforcement learning using any type of tree encoder, enabling more efficient training and inference. Furthermore, we propose a co-design mechanism, where the reinforcement learning-based Sym-Q facilitates effective interaction with domain experts at any stage of the equation discovery process. Users can dynamically modify generated nodes of the expression, collaborating with the agent to tailor the mathematical expression to best fit the problem and align with the assumed physical laws, particularly when there is prior partial knowledge of the expected behavior. Our experiments demonstrate that the pre-trained Sym-Q surpasses existing SR algorithms on the challenging SSDNC benchmark. Moreover, we experimentally show on real-world cases that its performance can be further enhanced by the interactive co-design mechanism, with Sym-Q achieving greater performance gains than other state-of-the-art models. Our reproducible code is available at https://github.com/EPFL-IMOS/Sym-Q. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02919</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02919</id><created>2025-02-05</created><authors><author><keyname>Lee</keyname><forenames>Wonjun</forenames></author><author><keyname>Ham</keyname><forenames>Bumsub</forenames></author><author><keyname>Kim</keyname><forenames>Suhyun</forenames></author></authors><title>Maximizing the Position Embedding for Vision Transformers with Global   Average Pooling</title><categories>cs.CV cs.LG</categories><comments>Accepted at AAAI 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In vision transformers, position embedding (PE) plays a crucial role in capturing the order of tokens. However, in vision transformer structures, there is a limitation in the expressiveness of PE due to the structure where position embedding is simply added to the token embedding. A layer-wise method that delivers PE to each layer and applies independent Layer Normalizations for token embedding and PE has been adopted to overcome this limitation. In this paper, we identify the conflicting result that occurs in a layer-wise structure when using the global average pooling (GAP) method instead of the class token. To overcome this problem, we propose MPVG, which maximizes the effectiveness of PE in a layer-wise structure with GAP. Specifically, we identify that PE counterbalances token embedding values at each layer in a layer-wise structure. Furthermore, we recognize that the counterbalancing role of PE is insufficient in the layer-wise structure, and we address this by maximizing the effectiveness of PE through MPVG. Through experiments, we demonstrate that PE performs a counterbalancing role and that maintaining this counterbalancing directionality significantly impacts vision transformers. As a result, the experimental results show that MPVG outperforms existing methods across vision transformers on various tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02920</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02920</id><created>2025-02-05</created><authors><author><keyname>Gangopadhyay</keyname><forenames>Briti</forenames></author><author><keyname>Wang</keyname><forenames>Zhao</forenames></author><author><keyname>Chiappa</keyname><forenames>Alberto Silvio</forenames></author><author><keyname>Takamatsu</keyname><forenames>Shingo</forenames></author></authors><title>Adaptive Budget Optimization for Multichannel Advertising Using   Combinatorial Bandits</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Effective budget allocation is crucial for optimizing the performance of digital advertising campaigns. However, the development of practical budget allocation algorithms remain limited, primarily due to the lack of public datasets and comprehensive simulation environments capable of verifying the intricacies of real-world advertising. While multi-armed bandit (MAB) algorithms have been extensively studied, their efficacy diminishes in non-stationary environments where quick adaptation to changing market dynamics is essential. In this paper, we advance the field of budget allocation in digital advertising by introducing three key contributions. First, we develop a simulation environment designed to mimic multichannel advertising campaigns over extended time horizons, incorporating logged real-world data. Second, we propose an enhanced combinatorial bandit budget allocation strategy that leverages a saturating mean function and a targeted exploration mechanism with change-point detection. This approach dynamically adapts to changing market conditions, improving allocation efficiency by filtering target regions based on domain knowledge. Finally, we present both theoretical analysis and empirical results, demonstrating that our method consistently outperforms baseline strategies, achieving higher rewards and lower regret across multiple real-world campaigns. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02921</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02921</id><created>2025-02-05</created><authors><author><keyname>Xie</keyname><forenames>Zhixian</forenames></author><author><keyname>Zhang</keyname><forenames>Haode</forenames></author><author><keyname>Feng</keyname><forenames>Yizhe</forenames></author><author><keyname>Jin</keyname><forenames>Wanxin</forenames></author></authors><title>Robust Reward Alignment in Hypothesis Space</title><categories>cs.LG</categories><comments>17 pages, including appendix</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Reward design for reinforcement learning and optimal control agents is challenging. Preference-based alignment addresses this by enabling agents to learn rewards from ranked trajectory pairs provided by humans. However, existing methods often struggle from poor robustness to unknown false human preferences. In this work, we propose a robust and efficient reward alignment method based on a novel and geometrically interpretable perspective: hypothesis space batched cutting. Our method iteratively refines the reward hypothesis space through "cuts" based on batches of human preferences. Within each batch, human preferences, queried based on disagreement, are grouped using a voting function to determine the appropriate cut, ensuring a bounded human query complexity. To handle unknown erroneous preferences, we introduce a conservative cutting method within each batch, preventing erroneous human preferences from making overly aggressive cuts to the hypothesis space. This guarantees provable robustness against false preferences. We evaluate our method in a model predictive control setting across diverse tasks, including DM-Control, dexterous in-hand manipulation, and locomotion. The results demonstrate that our framework achieves comparable or superior performance to state-of-the-art methods in error-free settings while significantly outperforming existing method when handling high percentage of erroneous human preferences. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02922</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02922</id><created>2025-02-05</created><authors><author><keyname>Zheng</keyname><forenames>Kaiwen</forenames></author><author><keyname>He</keyname><forenames>Guande</forenames></author><author><keyname>Chen</keyname><forenames>Jianfei</forenames></author><author><keyname>Bao</keyname><forenames>Fan</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author></authors><title>Elucidating the Preconditioning in Consistency Distillation</title><categories>cs.LG cs.CV</categories><comments>Accepted at ICLR 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consistency distillation is a prevalent way for accelerating diffusion models adopted in consistency (trajectory) models, in which a student model is trained to traverse backward on the probability flow (PF) ordinary differential equation (ODE) trajectory determined by the teacher model. Preconditioning is a vital technique for stabilizing consistency distillation, by linear combining the input data and the network output with pre-defined coefficients as the consistency function. It imposes the boundary condition of consistency functions without restricting the form and expressiveness of the neural network. However, previous preconditionings are hand-crafted and may be suboptimal choices. In this work, we offer the first theoretical insights into the preconditioning in consistency distillation, by elucidating its design criteria and the connection to the teacher ODE trajectory. Based on these analyses, we further propose a principled way dubbed \textit{Analytic-Precond} to analytically optimize the preconditioning according to the consistency gap (defined as the gap between the teacher denoiser and the optimal student denoiser) on a generalized teacher ODE. We demonstrate that Analytic-Precond can facilitate the learning of trajectory jumpers, enhance the alignment of the student trajectory with the teacher's, and achieve $2\times$ to $3\times$ training acceleration of consistency trajectory models in multi-step generation across various datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02924</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02924</id><created>2025-02-05</created><authors><author><keyname>Kim</keyname><forenames>Namwoo</forenames></author><author><keyname>Baik</keyname><forenames>Hyungryul</forenames></author><author><keyname>Yoon</keyname><forenames>Yoonjin</forenames></author></authors><title>TopoCL: Topological Contrastive Learning for Time Series</title><categories>cs.LG cs.AI</categories><comments>Submitted to TNNLS (under review)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Universal time series representation learning is challenging but valuable in real-world applications such as classification, anomaly detection, and forecasting. Recently, contrastive learning (CL) has been actively explored to tackle time series representation. However, a key challenge is that the data augmentation process in CL can distort seasonal patterns or temporal dependencies, inevitably leading to a loss of semantic information. To address this challenge, we propose Topological Contrastive Learning for time series (TopoCL). TopoCL mitigates such information loss by incorporating persistent homology, which captures the topological characteristics of data that remain invariant under transformations. In this paper, we treat the temporal and topological properties of time series data as distinct modalities. Specifically, we compute persistent homology to construct topological features of time series data, representing them in persistence diagrams. We then design a neural network to encode these persistent diagrams. Our approach jointly optimizes CL within the time modality and time-topology correspondence, promoting a comprehensive understanding of both temporal semantics and topological properties of time series. We conduct extensive experiments on four downstream tasks-classification, anomaly detection, forecasting, and transfer learning. The results demonstrate that TopoCL achieves state-of-the-art performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02925</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02925</id><created>2025-02-05</created><authors><author><keyname>Hiew</keyname><forenames>Joshua Zoen-Git</forenames></author><author><keyname>Lim</keyname><forenames>Tongseok</forenames></author><author><keyname>Pass</keyname><forenames>Brendan</forenames></author><author><keyname>de Souza</keyname><forenames>Marcelo Cruz</forenames></author></authors><title>Data denoising with self consistency, variance maximization, and the   Kantorovich dominance</title><categories>stat.ME cs.LG math.PR math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce a new framework for data denoising, partially inspired by martingale optimal transport. For a given noisy distribution (the data), our approach involves finding the closest distribution to it among all distributions which 1) have a particular prescribed structure (expressed by requiring they lie in a particular domain), and 2) are self-consistent with the data. We show that this amounts to maximizing the variance among measures in the domain which are dominated in convex order by the data. For particular choices of the domain, this problem and a relaxed version of it, in which the self-consistency condition is removed, are intimately related to various classical approaches to denoising. We prove that our general problem has certain desirable features: solutions exist under mild assumptions, have certain robustness properties, and, for very simple domains, coincide with solutions to the relaxed problem.   We also introduce a novel relationship between distributions, termed Kantorovich dominance, which retains certain aspects of the convex order while being a weaker, more robust, and easier-to-verify condition. Building on this, we propose and analyze a new denoising problem by substituting the convex order in the previously described framework with Kantorovich dominance. We demonstrate that this revised problem shares some characteristics with the full convex order problem but offers enhanced stability, greater computational efficiency, and, in specific domains, more meaningful solutions. Finally, we present simple numerical examples illustrating solutions for both the full convex order problem and the Kantorovich dominance problem. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02928</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02928</id><created>2025-02-05</created><authors><author><keyname>Adnan</keyname><forenames>Muntasir</forenames></author><author><keyname>Xu</keyname><forenames>Zhiwei</forenames></author><author><keyname>Kuhn</keyname><forenames>Carlos C. N.</forenames></author></authors><title>Large Language Model Guided Self-Debugging Code Generation</title><categories>cs.SE cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Automated code generation is gaining significant importance in intelligent computer programming and system deployment. However, current approaches often face challenges in computational efficiency and lack robust mechanisms for code parsing and error correction. In this work, we propose a novel framework, PyCapsule, with a simple yet effective two-agent pipeline and efficient self-debugging modules for Python code generation. PyCapsule features sophisticated prompt inference, iterative error handling, and case testing, ensuring high generation stability, safety, and correctness. Empirically, PyCapsule achieves up to 5.7% improvement of success rate on HumanEval, 10.3% on HumanEval-ET, and 24.4% on BigCodeBench compared to the state-of-art methods. We also observe a decrease in normalized success rate given more self-debugging attempts, potentially affected by limited and noisy error feedback in retention. PyCapsule demonstrates broader impacts on advancing lightweight and efficient code generation for artificial intelligence systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02929</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02929</id><created>2025-02-05</created><authors><author><keyname>Woodard</keyname><forenames>Brandon</forenames></author><author><keyname>Geleta</keyname><forenames>Margarita</forenames></author><author><keyname>LaViola</keyname><forenames>Joseph J.</forenames><suffix>Jr.</suffix></author><author><keyname>Fanelli</keyname><forenames>Andrea</forenames></author><author><keyname>Wilson</keyname><forenames>Rhonda</forenames></author></authors><title>AudioMiXR: Spatial Audio Object Manipulation with 6DoF for Sound Design   in Augmented Reality</title><categories>cs.HC cs.SD eess.AS</categories><comments>34 pages, 18 Figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present AudioMiXR, an augmented reality (AR) interface intended to assess how users manipulate virtual audio objects situated in their physical space using six degrees of freedom (6DoF) deployed on a head-mounted display (Apple Vision Pro) for 3D sound design. Existing tools for 3D sound design are typically constrained to desktop displays, which may limit spatial awareness of mixing within the execution environment. Utilizing an XR HMD to create soundscapes may provide a real-time test environment for 3D sound design, as modern HMDs can provide precise spatial localization assisted by cross-modal interactions. However, there is no research on design guidelines specific to sound design with six degrees of freedom (6DoF) in XR. To provide a first step toward identifying design-related research directions in this space, we conducted an exploratory study where we recruited 27 participants, consisting of expert and non-expert sound designers. The goal was to assess design lessons that can be used to inform future research venues in 3D sound design. We ran a within-subjects study where users designed both a music and cinematic soundscapes. After thematically analyzing participant data, we constructed two design lessons: 1. Proprioception for AR Sound Design, and 2. Balancing Audio-Visual Modalities in AR GUIs. Additionally, we provide application domains that can benefit most from 6DoF sound design based on our results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02932</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02932</id><created>2025-02-05</created><authors><author><keyname>Huang</keyname><forenames>Weiwen</forenames></author><author><keyname>Liang</keyname><forenames>Li</forenames></author><author><keyname>Xu</keyname><forenames>Ningsheng</forenames></author><author><keyname>Deng</keyname><forenames>Fang</forenames></author></authors><title>Dominance Regions of Pursuit-evasion Games in Non-anticipative   Information Patterns</title><categories>math.OC cs.GT cs.SY eess.SY</categories><comments>23 pages,18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evader's dominance region is an important concept and the foundation of geometric methods for pursuit-evasion games. This article mainly reveals the relevant properties of the evader's dominance region, especially in non-anticipative information patterns. We can use these properties to research pursuit-evasion games in non-anticipative information patterns. The core problem is under what condition the pursuer has a non-anticipative strategy to prevent the evader leaving its initial dominance region before being captured regardless of the evader's strategy. We first define the evader's dominance region by the shortest path distance, and we rigorously prove for the first time that the initial dominance region of the evader is the reachable region of the evader in the open-loop sense. Subsequently, we prove that there exists a non-anticipative strategy by which the pursuer can capture the evader before the evader leaves its initial dominance region's closure in the absence of obstacles. For cases with obstacles, we provide a counter example to illustrate that such a non-anticipative strategy does not always exist, and provide a necessary condition for the existence of such strategy. Finally, we consider a scenario with a single corner obstacle and provide a sufficient condition for the existence of such a non-anticipative strategy. At the end of this article, we discuss the application of the evader's dominance region in target defense games. This article has important reference significance for the design of non-anticipative strategies in pursuit-evasion games with obstacles. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02934</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02934</id><created>2025-02-05</created><authors><author><keyname>Li</keyname><forenames>Junheng</forenames></author><author><keyname>Duan</keyname><forenames>Ziwei</forenames></author><author><keyname>Ma</keyname><forenames>Junchao</forenames></author><author><keyname>Nguyen</keyname><forenames>Quan</forenames></author></authors><title>Gait-Net-augmented Implicit Kino-dynamic MPC for Dynamic   Variable-frequency Humanoid Locomotion over Discrete Terrains</title><categories>cs.RO cs.SY eess.SY</categories><comments>14 pages, 12 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Current optimization-based control techniques for humanoid locomotion struggle to adapt step duration and placement simultaneously in dynamic walking gaits due to their reliance on fixed-time discretization, which limits responsiveness to terrain conditions and results in suboptimal performance in challenging environments. In this work, we propose a Gait-Net-augmented implicit kino-dynamic model-predictive control (MPC) to simultaneously optimize step location, step duration, and contact forces for natural variable-frequency locomotion. The proposed method incorporates a Gait-Net-augmented Sequential Convex MPC algorithm to solve multi-linearly constrained variables by iterative quadratic programs. At its core, a lightweight Gait-frequency Network (Gait-Net) determines the preferred step duration in terms of variable MPC sampling times, simplifying step duration optimization to the parameter level. Additionally, it enhances and updates the spatial reference trajectory within each sequential iteration by incorporating local solutions, allowing the projection of kinematic constraints to the design of reference trajectories. We validate the proposed algorithm in high-fidelity simulations and on small-size humanoid hardware, demonstrating its capability for variable-frequency and 3-D discrete terrain locomotion with only a one-step preview of terrain data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02936</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02936</id><created>2025-02-05</created><authors><author><keyname>Jiang</keyname><forenames>Junkun</forenames></author><author><keyname>Chen</keyname><forenames>Jie</forenames></author><author><keyname>Au</keyname><forenames>Ho Yin</forenames></author><author><keyname>Chen</keyname><forenames>Mingyuan</forenames></author><author><keyname>Xue</keyname><forenames>Wei</forenames></author><author><keyname>Guo</keyname><forenames>Yike</forenames></author></authors><title>Every Angle Is Worth A Second Glance: Mining Kinematic Skeletal   Structures from Multi-view Joint Cloud</title><categories>cs.CV</categories><comments>Accepted by IEEE Transactions on Visualization and Computer Graphics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-person motion capture over sparse angular observations is a challenging problem under interference from both self- and mutual-occlusions. Existing works produce accurate 2D joint detection, however, when these are triangulated and lifted into 3D, available solutions all struggle in selecting the most accurate candidates and associating them to the correct joint type and target identity. As such, in order to fully utilize all accurate 2D joint location information, we propose to independently triangulate between all same-typed 2D joints from all camera views regardless of their target ID, forming the Joint Cloud. Joint Cloud consist of both valid joints lifted from the same joint type and target ID, as well as falsely constructed ones that are from different 2D sources. These redundant and inaccurate candidates are processed over the proposed Joint Cloud Selection and Aggregation Transformer (JCSAT) involving three cascaded encoders which deeply explore the trajectile, skeletal structural, and view-dependent correlations among all 3D point candidates in the cross-embedding space. An Optimal Token Attention Path (OTAP) module is proposed which subsequently selects and aggregates informative features from these redundant observations for the final prediction of human motion. To demonstrate the effectiveness of JCSAT, we build and publish a new multi-person motion capture dataset BUMocap-X with complex interactions and severe occlusions. Comprehensive experiments over the newly presented as well as benchmark datasets validate the effectiveness of the proposed framework, which outperforms all existing state-of-the-art methods, especially under challenging occlusion scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02938</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02938</id><created>2025-02-05</created><authors><author><keyname>Chay-intr</keyname><forenames>T.</forenames></author><author><keyname>Chen</keyname><forenames>Y.</forenames></author><author><keyname>Viriyayudhakorn</keyname><forenames>K.</forenames></author><author><keyname>Theeramunkong</keyname><forenames>T.</forenames></author></authors><title>LLaVAC: Fine-tuning LLaVA as a Multimodal Sentiment Classifier</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present LLaVAC, a method for constructing a classifier for multimodal sentiment analysis. This method leverages fine-tuning of the Large Language and Vision Assistant (LLaVA) to predict sentiment labels across both image and text modalities. Our approach involves designing a structured prompt that incorporates both unimodal and multimodal labels to fine-tune LLaVA, enabling it to perform sentiment classification effectively. Experiments on the MVSA-Single dataset demonstrate that LLaVAC outperforms existing methods in multimodal sentiment analysis across three data processing procedures. The implementation of LLaVAC is publicly available at https://github.com/tchayintr/llavac. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02941</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02941</id><created>2025-02-05</created><authors><author><keyname>Li</keyname><forenames>Yang</forenames></author><author><keyname>Guo</keyname><forenames>Jinpei</forenames></author><author><keyname>Wang</keyname><forenames>Runzhong</forenames></author><author><keyname>Zha</keyname><forenames>Hongyuan</forenames></author><author><keyname>Yan</keyname><forenames>Junchi</forenames></author></authors><title>Fast T2T: Optimization Consistency Speeds Up Diffusion-Based   Training-to-Testing Solving for Combinatorial Optimization</title><categories>cs.LG</categories><comments>Published at NeurIPS 2024, the implementation code is available at   https://github.com/Thinklab-SJTU/Fast-T2T</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diffusion models have recently advanced Combinatorial Optimization (CO) as a powerful backbone for neural solvers. However, their iterative sampling process requiring denoising across multiple noise levels incurs substantial overhead. We propose to learn direct mappings from different noise levels to the optimal solution for a given instance, facilitating high-quality generation with minimal shots. This is achieved through an optimization consistency training protocol, which, for a given instance, minimizes the difference among samples originating from varying generative trajectories and time steps relative to the optimal solution. The proposed model enables fast single-step solution generation while retaining the option of multi-step sampling to trade for sampling quality, which offers a more effective and efficient alternative backbone for neural solvers. In addition, within the training-to-testing (T2T) framework, to bridge the gap between training on historical instances and solving new instances, we introduce a novel consistency-based gradient search scheme during the test stage, enabling more effective exploration of the solution space learned during training. It is achieved by updating the latent solution probabilities under objective gradient guidance during the alternation of noise injection and denoising steps. We refer to this model as Fast T2T. Extensive experiments on two popular tasks, the Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS), demonstrate the superiority of Fast T2T regarding both solution quality and efficiency, even outperforming LKH given limited time budgets. Notably, Fast T2T with merely one-step generation and one-step gradient search can mostly outperform the SOTA diffusion-based counterparts that require hundreds of steps, while achieving tens of times speedup. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02942</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02942</id><created>2025-02-05</created><authors><author><keyname>Yao</keyname><forenames>Jixun</forenames></author><author><keyname>Liu</keyname><forenames>Hexin</forenames></author><author><keyname>Chen</keyname><forenames>Chen</forenames></author><author><keyname>Hu</keyname><forenames>Yuchen</forenames></author><author><keyname>Chng</keyname><forenames>EngSiong</forenames></author><author><keyname>Xie</keyname><forenames>Lei</forenames></author></authors><title>GenSE: Generative Speech Enhancement via Language Models using   Hierarchical Modeling</title><categories>eess.AS cs.SD</categories><comments>Accepted by ICLR 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02943</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02943</id><created>2025-02-05</created><authors><author><keyname>Yuan</keyname><forenames>Lanqin</forenames></author><author><keyname>Schneider</keyname><forenames>Philipp J.</forenames></author><author><keyname>Rizoiu</keyname><forenames>Marian-Andrei</forenames></author></authors><title>Behavioral Homophily in Social Media via Inverse Reinforcement Learning:   A Reddit Case Study</title><categories>cs.SI cs.LG</categories><doi>10.1145/3696410.3714618</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Online communities play a critical role in shaping societal discourse and influencing collective behavior in the real world. The tendency for people to connect with others who share similar characteristics and views, known as homophily, plays a key role in the formation of echo chambers which further amplify polarization and division. Existing works examining homophily in online communities traditionally infer it using content- or adjacency-based approaches, such as constructing explicit interaction networks or performing topic analysis. These methods fall short for platforms where interaction networks cannot be easily constructed and fail to capture the complex nature of user interactions across the platform. This work introduces a novel approach for quantifying user homophily. We first use an Inverse Reinforcement Learning (IRL) framework to infer users' policies, then use these policies as a measure of behavioral homophily. We apply our method to Reddit, conducting a case study across 5.9 million interactions over six years, demonstrating how this approach uncovers distinct behavioral patterns and user roles that vary across different communities. We further validate our behavioral homophily measure against traditional content-based homophily, offering a powerful method for analyzing social media dynamics and their broader societal implications. We find, among others, that users can behave very similarly (high behavioral homophily) when discussing entirely different topics like soccer vs e-sports (low topical homophily), and that there is an entire class of users on Reddit whose purpose seems to be to disagree with others. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02945</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02945</id><created>2025-02-05</created><authors><author><keyname>Wang</keyname><forenames>Ziwei</forenames></author><author><keyname>Zhou</keyname><forenames>Jie</forenames></author><author><keyname>Chen</keyname><forenames>Qin</forenames></author><author><keyname>Zhang</keyname><forenames>Min</forenames></author><author><keyname>Jiang</keyname><forenames>Bo</forenames></author><author><keyname>Zhou</keyname><forenames>Aimin</forenames></author><author><keyname>Bai</keyname><forenames>Qinchun</forenames></author><author><keyname>He</keyname><forenames>Liang</forenames></author></authors><title>LLM-KT: Aligning Large Language Models with Knowledge Tracing using a   Plug-and-Play Instruction</title><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The knowledge tracing (KT) problem is an extremely important topic in personalized education, which aims to predict whether students can correctly answer the next question based on their past question-answer records. Prior work on this task mainly focused on learning the sequence of behaviors based on the IDs or textual information. However, these studies usually fail to capture students' sufficient behavioral patterns without reasoning with rich world knowledge about questions. In this paper, we propose a large language models (LLMs)-based framework for KT, named \texttt{\textbf{LLM-KT}}, to integrate the strengths of LLMs and traditional sequence interaction models. For task-level alignment, we design Plug-and-Play instruction to align LLMs with KT, leveraging LLMs' rich knowledge and powerful reasoning capacity. For modality-level alignment, we design the plug-in context and sequence to integrate multiple modalities learned by traditional methods. To capture the long context of history records, we present a plug-in context to flexibly insert the compressed context embedding into LLMs using question-specific and concept-specific tokens. Furthermore, we introduce a plug-in sequence to enhance LLMs with sequence interaction behavior representation learned by traditional sequence models using a sequence adapter. Extensive experiments show that \texttt{\textbf{LLM-KT}} obtains state-of-the-art performance on four typical datasets by comparing it with approximately 20 strong baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02950</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02950</id><created>2025-02-05</created><authors><author><keyname>Yao</keyname><forenames>Jixun</forenames></author><author><keyname>Yang</keyname><forenames>Yuguang</forenames></author><author><keyname>Pan</keyname><forenames>Yu</forenames></author><author><keyname>Feng</keyname><forenames>Yuan</forenames></author><author><keyname>Ning</keyname><forenames>Ziqian</forenames></author><author><keyname>Ye</keyname><forenames>Jianhao</forenames></author><author><keyname>Zhou</keyname><forenames>Hongbin</forenames></author><author><keyname>Xie</keyname><forenames>Lei</forenames></author></authors><title>Fine-grained Preference Optimization Improves Zero-shot Text-to-Speech</title><categories>eess.AS cs.SD</categories><comments>WIP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrating human feedback to align text-to-speech (TTS) system outputs with human preferences has proven to be an effective approach for enhancing the robustness of language model-based TTS systems. Current approaches primarily focus on using preference data annotated at the utterance level. However, frequent issues that affect the listening experience often only arise in specific segments of audio samples, while other segments are well-generated. In this study, we propose a fine-grained preference optimization approach (FPO) to enhance the robustness of TTS systems. FPO focuses on addressing localized issues in generated samples rather than uniformly optimizing the entire utterance. Specifically, we first analyze the types of issues in generated samples, categorize them into two groups, and propose a selective training loss strategy to optimize preferences based on fine-grained labels for each issue type. Experimental results show that FPO enhances the robustness of zero-shot TTS systems by effectively addressing local issues, significantly reducing the bad case ratio, and improving intelligibility. Furthermore, FPO exhibits superior data efficiency compared with baseline systems, achieving similar performance with fewer training samples. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02951</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02951</id><created>2025-02-05</created><authors><author><keyname>Madaka</keyname><forenames>Madhuri Latha</forenames></author><author><keyname>Bhagvati</keyname><forenames>Chakravarthy</forenames></author></authors><title>VQA-Levels: A Hierarchical Approach for Classifying Questions in VQA</title><categories>cs.CV cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Designing datasets for Visual Question Answering (VQA) is a difficult and complex task that requires NLP for parsing and computer vision for analysing the relevant aspects of the image for answering the question asked. Several benchmark datasets have been developed by researchers but there are many issues with using them for methodical performance tests. This paper proposes a new benchmark dataset -- a pilot version called VQA-Levels is ready now -- for testing VQA systems systematically and assisting researchers in advancing the field. The questions are classified into seven levels ranging from direct answers based on low-level image features (without needing even a classifier) to those requiring high-level abstraction of the entire image content. The questions in the dataset exhibit one or many of ten properties. Each is categorised into a specific level from 1 to 7. Levels 1 - 3 are directly on the visual content while the remaining levels require extra knowledge about the objects in the image. Each question generally has a unique one or two-word answer. The questions are 'natural' in the sense that a human is likely to ask such a question when seeing the images. An example question at Level 1 is, ``What is the shape of the red colored region in the image?" while at Level 7, it is, ``Why is the man cutting the paper?". Initial testing of the proposed dataset on some of the existing VQA systems reveals that their success is high on Level 1 (low level features) and Level 2 (object classification) questions, least on Level 3 (scene text) followed by Level 6 (extrapolation) and Level 7 (whole scene analysis) questions. The work in this paper will go a long way to systematically analyze VQA systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02954</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02954</id><created>2025-02-05</created><authors><author><keyname>Kawata</keyname><forenames>Ryotaro</forenames></author><author><keyname>Oko</keyname><forenames>Kazusato</forenames></author><author><keyname>Nitanda</keyname><forenames>Atsushi</forenames></author><author><keyname>Suzuki</keyname><forenames>Taiji</forenames></author></authors><title>Direct Distributional Optimization for Provable Alignment of Diffusion   Models</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce a novel alignment method for diffusion models from distribution optimization perspectives while providing rigorous convergence guarantees. We first formulate the problem as a generic regularized loss minimization over probability distributions and directly optimize the distribution using the Dual Averaging method. Next, we enable sampling from the learned distribution by approximating its score function via Doob's $h$-transform technique. The proposed framework is supported by rigorous convergence guarantees and an end-to-end bound on the sampling error, which imply that when the original distribution's score is known accurately, the complexity of sampling from shifted distributions is independent of isoperimetric conditions. This framework is broadly applicable to general distribution optimization problems, including alignment tasks in Reinforcement Learning with Human Feedback (RLHF), Direct Preference Optimization (DPO), and Kahneman-Tversky Optimization (KTO). We empirically validate its performance on synthetic and image datasets using the DPO objective. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02955</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02955</id><created>2025-02-05</created><authors><author><keyname>Wu</keyname><forenames>Qinzhuo</forenames></author><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Luan</keyname><forenames>Jian</forenames></author><author><keyname>Wang</keyname><forenames>Bin</forenames></author></authors><title>ReachAgent: Enhancing Mobile Agent via Page Reaching and Operation</title><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, mobile AI agents have gained increasing attention. Given a task, mobile AI agents can interact with mobile devices in multiple steps and finally form a GUI flow that solves the task. However, existing agents tend to focus on most task-relevant elements at each step, leading to local optimal solutions and ignoring the overall GUI flow. To address this issue, we constructed a training dataset called MobileReach, which breaks the task into page reaching and operation subtasks. Furthermore, we propose ReachAgent, a two-stage framework that focuses on improving its task-completion abilities. It utilizes the page reaching and page operation subtasks, along with reward-based preference GUI flows, to further enhance the agent. Experimental results show that ReachAgent significantly improves the IoU Acc and Text Acc by 7.12% and 7.69% on the step-level and 4.72% and 4.63% on the task-level compared to the SOTA agent. Our data and code will be released upon acceptance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02957</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02957</id><created>2025-02-05</created><authors><author><keyname>Coghlan</keyname><forenames>Simon</forenames></author><author><keyname>Chia</keyname><forenames>Hui Xian</forenames></author><author><keyname>Scholer</keyname><forenames>Falk</forenames></author><author><keyname>Spina</keyname><forenames>Damiano</forenames></author></authors><title>Control Search Rankings, Control the World: What is a Good Search   Engine?</title><categories>cs.IR cs.CY</categories><comments>Accepted to Springer's AI and Ethics journal on February 4, 2025; 31   pages, 1 figure</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines the ethical question, 'What is a good search engine?' Since search engines are gatekeepers of global online information, it is vital they do their job ethically well. While the Internet is now several decades old, the topic remains under-explored from interdisciplinary perspectives. This paper presents a novel role-based approach involving four ethical models of types of search engine behavior: Customer Servant, Librarian, Journalist, and Teacher. It explores these ethical models with reference to the research field of information retrieval, and by means of a case study involving the COVID-19 global pandemic. It also reflects on the four ethical models in terms of the history of search engine development, from earlier crude efforts in the 1990s, to the very recent prospect of Large Language Model-based conversational information seeking systems taking on the roles of established web search engines like Google. Finally, the paper outlines considerations that inform present and future regulation and accountability for search engines as they continue to evolve. The paper should interest information retrieval researchers and others interested in the ethics of search engines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02958</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02958</id><created>2025-02-05</created><authors><author><keyname>Youssef</keyname><forenames>Paul</forenames></author><author><keyname>Zhao</keyname><forenames>Zhixue</forenames></author><author><keyname>Braun</keyname><forenames>Daniel</forenames></author><author><keyname>Schlötterer</keyname><forenames>Jörg</forenames></author><author><keyname>Seifert</keyname><forenames>Christin</forenames></author></authors><title>Position: Editing Large Language Models Poses Serious Safety Risks</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Large Language Models (LLMs) contain large amounts of facts about the world. These facts can become outdated over time, which has led to the development of knowledge editing methods (KEs) that can change specific facts in LLMs with limited side effects. This position paper argues that editing LLMs poses serious safety risks that have been largely overlooked. First, we note the fact that KEs are widely available, computationally inexpensive, highly performant, and stealthy makes them an attractive tool for malicious actors. Second, we discuss malicious use cases of KEs, showing how KEs can be easily adapted for a variety of malicious purposes. Third, we highlight vulnerabilities in the AI ecosystem that allow unrestricted uploading and downloading of updated models without verification. Fourth, we argue that a lack of social and institutional awareness exacerbates this risk, and discuss the implications for different stakeholders. We call on the community to (i) research tamper-resistant models and countermeasures against malicious model editing, and (ii) actively engage in securing the AI ecosystem. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02960</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02960</id><created>2025-02-05</created><authors><author><keyname>Wang</keyname><forenames>Nan</forenames></author><author><keyname>Walter</keyname><forenames>Kane</forenames></author><author><keyname>Gao</keyname><forenames>Yansong</forenames></author><author><keyname>Abuadbba</keyname><forenames>Alsharif</forenames></author></authors><title>Large Language Model Adversarial Landscape Through the Lens of Attack   Objectives</title><categories>cs.CR</categories><comments>15 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) represent a transformative leap in artificial intelligence, enabling the comprehension, generation, and nuanced interaction with human language on an unparalleled scale. However, LLMs are increasingly vulnerable to a range of adversarial attacks that threaten their privacy, reliability, security, and trustworthiness. These attacks can distort outputs, inject biases, leak sensitive information, or disrupt the normal functioning of LLMs, posing significant challenges across various applications.   In this paper, we provide a novel comprehensive analysis of the adversarial landscape of LLMs, framed through the lens of attack objectives. By concentrating on the core goals of adversarial actors, we offer a fresh perspective that examines threats from the angles of privacy, integrity, availability, and misuse, moving beyond conventional taxonomies that focus solely on attack techniques. This objective-driven adversarial landscape not only highlights the strategic intent behind different adversarial approaches but also sheds light on the evolving nature of these threats and the effectiveness of current defenses. Our analysis aims to guide researchers and practitioners in better understanding, anticipating, and mitigating these attacks, ultimately contributing to the development of more resilient and robust LLM systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02963</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02963</id><created>2025-02-05</created><authors><author><keyname>Weinzierl</keyname><forenames>Sven</forenames></author><author><keyname>Cora</keyname><forenames>Carl</forenames></author></authors><title>(Neural-Symbolic) Machine Learning for Inconsistency Measurement</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present machine-learning-based approaches for determining the \emph{degree} of inconsistency -- which is a numerical value -- for propositional logic knowledge bases. Specifically, we present regression- and neural-based models that learn to predict the values that the inconsistency measures $\incmi$ and $\incat$ would assign to propositional logic knowledge bases. Our main motivation is that computing these values conventionally can be hard complexity-wise. As an important addition, we use specific postulates, that is, properties, of the underlying inconsistency measures to infer symbolic rules, which we combine with the learning-based models in the form of constraints. We perform various experiments and show that a) predicting the degree values is feasible in many situations, and b) including the symbolic constraints deduced from the rationality postulates increases the prediction quality. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02966</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02966</id><created>2025-02-05</created><authors><author><keyname>Fayyazi</keyname><forenames>Arya</forenames></author><author><keyname>Kamal</keyname><forenames>Mehdi</forenames></author><author><keyname>Pedram</keyname><forenames>Massoud</forenames></author></authors><title>FACTER: Fairness-Aware Conformal Thresholding and Prompt Engineering for   Enabling Fair LLM-Based Recommender Systems</title><categories>cs.IR cs.AI cs.CY cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose FACTER, a fairness-aware framework for LLM-based recommendation systems that integrates conformal prediction with dynamic prompt engineering. By introducing an adaptive semantic variance threshold and a violation-triggered mechanism, FACTER automatically tightens fairness constraints whenever biased patterns emerge. We further develop an adversarial prompt generator that leverages historical violations to reduce repeated demographic biases without retraining the LLM. Empirical results on MovieLens and Amazon show that FACTER substantially reduces fairness violations (up to 95.5%) while maintaining strong recommendation accuracy, revealing semantic variance as a potent proxy of bias. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02967</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02967</id><created>2025-02-05</created><authors><author><keyname>Muraccioli</keyname><forenames>Bastien</forenames><affiliation>CNRS-AIST JRL</affiliation></author><author><keyname>Mathieu</keyname><forenames>Celerier</forenames><affiliation>CNRS-AIST JRL</affiliation></author><author><keyname>Mehdi</keyname><forenames>Benallegue</forenames><affiliation>CNRS-AIST JRL</affiliation></author><author><keyname>Gentiane</keyname><forenames>Venture</forenames><affiliation>CNRS-AIST JRL, UTokyo</affiliation></author></authors><title>Demonstrating a Control Framework for Physical Human-Robot Interaction   Toward Industrial Applications</title><categories>cs.RO cs.SY eess.SY</categories><comments>Demo Paper submitted to Robotics: Science and Systems (RSS2025),   pending review</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human-Robot Interaction (pHRI) is critical for implementing Industry 5.0 which focuses on human-centric approaches. However, few studies explore the practical alignment of pHRI to industrial grade performance. This paper introduces a versatile control framework designed to bridge this gap by incorporating the torque-based control modes: compliance control, null-space compliance, dual compliance, all in static and dynamic scenarios. Thanks to our second-order Quadratic Programming (QP) formulation, strict kinematic and collision constraints are integrated into the system as safety features, and a weighted hierarchy guarantees singularity-robust task tracking performance. The framework is implemented on a Kinova Gen3 collaborative robot (cobot) equipped with a Bota force/torque sensor. A DualShock 4 game controller is attached at the robot's end-effector to demonstrate the framework's capabilities. This setup enables seamless dynamic switching between the modes, and real-time adjustment of parameters, such as transitioning between position and torque control or selecting a more robust custom-developed low-level torque controller over the default one.Built on the open-source robotic control software mc_rtc, to ensure reproducibility for both research and industrial deployment, this framework demonstrates industrial-grade performance and repeatability, showcasing its potential as a robust pHRI control system for industrial environments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02968</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02968</id><created>2025-02-05</created><authors><author><keyname>Berrebi</keyname><forenames>Shoham Shimon</forenames></author><author><keyname>Yaakobi</keyname><forenames>Eitan</forenames></author><author><keyname>Yakhini</keyname><forenames>Zohar</forenames></author><author><keyname>Bar-Lev</keyname><forenames>Daniella</forenames></author></authors><title>The Labeled Coupon Collector Problem with Random Sample Sizes and   Partial Recovery</title><categories>cs.DM</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We extend the Coupon Collector's Problem (CCP) and present a novel generalized model, referred as the k-LCCP problem, where one is interested in recovering a bipartite graph with a perfect matching, which represents the coupons and their matching labels. We show two extra-extensions to this variation: the heterogeneous sample size case (K-LCCP) and the partly recovering case. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02970</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02970</id><created>2025-02-05</created><authors><author><keyname>Li</keyname><forenames>Muxing</forenames></author><author><keyname>Ye</keyname><forenames>Zesheng</forenames></author><author><keyname>Li</keyname><forenames>Yixuan</forenames></author><author><keyname>Song</keyname><forenames>Andy</forenames></author><author><keyname>Zhang</keyname><forenames>Guangquan</forenames></author><author><keyname>Liu</keyname><forenames>Feng</forenames></author></authors><title>Membership Inference Attack Should Move On to Distributional Statistics   for Distilled Generative Models</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Membership inference attacks (MIAs) determine whether certain data instances were used to train a model by exploiting the differences in how the model responds to seen versus unseen instances. This capability makes MIAs important in assessing privacy leakage within modern generative AI systems. However, this paper reveals an oversight in existing MIAs against \emph{distilled generative models}: attackers can no longer detect a teacher model's training instances individually when targeting the distilled student model, as the student learns from the teacher-generated data rather than its original member data, preventing direct instance-level memorization. Nevertheless, we find that student-generated samples exhibit a significantly stronger distributional alignment with teacher's member data than non-member data. This leads us to posit that MIAs \emph{on distilled generative models should shift from instance-level to distribution-level statistics}. We thereby introduce a \emph{set-based} MIA framework that measures \emph{relative} distributional discrepancies between student-generated data\emph{sets} and potential member/non-member data\emph{sets}, Empirically, distributional statistics reliably distinguish a teacher's member data from non-member data through the distilled model. Finally, we discuss scenarios in which our setup faces limitations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02972</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02972</id><created>2025-02-05</created><authors><author><keyname>Kou</keyname><forenames>Wei-Bin</forenames></author><author><keyname>Zhu</keyname><forenames>Guangxu</forenames></author><author><keyname>Ye</keyname><forenames>Rongguang</forenames></author><author><keyname>Wang</keyname><forenames>Shuai</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author><author><keyname>Wu</keyname><forenames>Yik-Chung</forenames></author></authors><title>Label Anything: An Interpretable, High-Fidelity and Prompt-Free   Annotator</title><categories>cs.RO cs.LG</categories><comments>Accepted by ICRA 2025</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Learning-based street scene semantic understanding in autonomous driving (AD) has advanced significantly recently, but the performance of the AD model is heavily dependent on the quantity and quality of the annotated training data. However, traditional manual labeling involves high cost to annotate the vast amount of required data for training robust model. To mitigate this cost of manual labeling, we propose a Label Anything Model (denoted as LAM), serving as an interpretable, high-fidelity, and prompt-free data annotator. Specifically, we firstly incorporate a pretrained Vision Transformer (ViT) to extract the latent features. On top of ViT, we propose a semantic class adapter (SCA) and an optimization-oriented unrolling algorithm (OptOU), both with a quite small number of trainable parameters. SCA is proposed to fuse ViT-extracted features to consolidate the basis of the subsequent automatic annotation. OptOU consists of multiple cascading layers and each layer contains an optimization formulation to align its output with the ground truth as closely as possible, though which OptOU acts as being interpretable rather than learning-based blackbox nature. In addition, training SCA and OptOU requires only a single pre-annotated RGB seed image, owing to their small volume of learnable parameters. Extensive experiments clearly demonstrate that the proposed LAM can generate high-fidelity annotations (almost 100% in mIoU) for multiple real-world datasets (i.e., Camvid, Cityscapes, and Apolloscapes) and CARLA simulation dataset. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02975</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02975</id><created>2025-02-05</created><authors><author><keyname>Yi</keyname><forenames>Lu</forenames></author><author><keyname>Peng</keyname><forenames>Jie</forenames></author><author><keyname>Zheng</keyname><forenames>Yanping</forenames></author><author><keyname>Mo</keyname><forenames>Fengran</forenames></author><author><keyname>Wei</keyname><forenames>Zhewei</forenames></author><author><keyname>Ye</keyname><forenames>Yuhang</forenames></author><author><keyname>Zixuan</keyname><forenames>Yue</forenames></author><author><keyname>Huang</keyname><forenames>Zengfeng</forenames></author></authors><title>TGB-Seq Benchmark: Challenging Temporal GNNs with Complex Sequential   Dynamics</title><categories>cs.LG cs.AI</categories><comments>published at ICLR 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future link prediction is a fundamental challenge in various real-world dynamic systems. To address this, numerous temporal graph neural networks (temporal GNNs) and benchmark datasets have been developed. However, these datasets often feature excessive repeated edges and lack complex sequential dynamics, a key characteristic inherent in many real-world applications such as recommender systems and ``Who-To-Follow'' on social networks. This oversight has led existing methods to inadvertently downplay the importance of learning sequential dynamics, focusing primarily on predicting repeated edges.   In this study, we demonstrate that existing methods, such as GraphMixer and DyGFormer, are inherently incapable of learning simple sequential dynamics, such as ``a user who has followed OpenAI and Anthropic is more likely to follow AI at Meta next.'' Motivated by this issue, we introduce the Temporal Graph Benchmark with Sequential Dynamics (TGB-Seq), a new benchmark carefully curated to minimize repeated edges, challenging models to learn sequential dynamics and generalize to unseen edges. TGB-Seq comprises large real-world datasets spanning diverse domains, including e-commerce interactions, movie ratings, business reviews, social networks, citation networks and web link networks. Benchmarking experiments reveal that current methods usually suffer significant performance degradation and incur substantial training costs on TGB-Seq, posing new challenges and opportunities for future research. TGB-Seq datasets, leaderboards, and example codes are available at https://tgb-seq.github.io/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02977</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02977</id><created>2025-02-05</created><authors><author><keyname>Rawelekar</keyname><forenames>Samyak</forenames></author><author><keyname>Cai</keyname><forenames>Yujun</forenames></author><author><keyname>Wang</keyname><forenames>Yiwei</forenames></author><author><keyname>Yang</keyname><forenames>Ming-Hsuan</forenames></author><author><keyname>Ahuja</keyname><forenames>Narendra</forenames></author></authors><title>Disentangling CLIP Features for Enhanced Localized Understanding</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vision-language models (VLMs) demonstrate impressive capabilities in coarse-grained tasks like image classification and retrieval. However, they struggle with fine-grained tasks that require localized understanding. To investigate this weakness, we comprehensively analyze CLIP features and identify an important issue: semantic features are highly correlated. Specifically, the features of a class encode information about other classes, which we call mutual feature information (MFI). This mutual information becomes evident when we query a specific class and unrelated objects are activated along with the target class. To address this issue, we propose Unmix-CLIP, a novel framework designed to reduce MFI and improve feature disentanglement. We introduce MFI loss, which explicitly separates text features by projecting them into a space where inter-class similarity is minimized. To ensure a corresponding separation in image features, we use multi-label recognition (MLR) to align the image features with the separated text features. This ensures that both image and text features are disentangled and aligned across modalities, improving feature separation for downstream tasks. For the COCO- 14 dataset, Unmix-CLIP reduces feature similarity by 24.9%. We demonstrate its effectiveness through extensive evaluations of MLR and zeroshot semantic segmentation (ZS3). In MLR, our method performs competitively on the VOC2007 and surpasses SOTA approaches on the COCO-14 dataset, using fewer training parameters. Additionally, Unmix-CLIP consistently outperforms existing ZS3 methods on COCO and VOC </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02982</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02982</id><created>2025-02-05</created><authors><author><keyname>Wang</keyname><forenames>Wenhao</forenames></author><author><keyname>Yu</keyname><forenames>Zijie</forenames></author><author><keyname>Liu</keyname><forenames>William</forenames></author><author><keyname>Ye</keyname><forenames>Rui</forenames></author><author><keyname>Jin</keyname><forenames>Tian</forenames></author><author><keyname>Chen</keyname><forenames>Siheng</forenames></author><author><keyname>Wang</keyname><forenames>Yanfeng</forenames></author></authors><title>FedMobileAgent: Training Mobile Agents Using Decentralized Self-Sourced   Data from Diverse Users</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The advancement of mobile agents has opened new opportunities for automating tasks on mobile devices. Training these agents requires large-scale high-quality data, which is costly using human labor. Given the vast number of mobile phone users worldwide, if automated data collection from them is feasible, the resulting data volume and the subsequently trained mobile agents could reach unprecedented levels. Nevertheless, two major challenges arise: (1) extracting high-level and low-level user instructions without involving human and (2) utilizing distributed data from diverse users while preserving privacy.   To tackle these challenges, we propose FedMobileAgent, a collaborative framework that trains mobile agents using self-sourced data from diverse users. Specifically, it includes two techniques. First, we propose Auto-Annotation, which enables the automatic collection of high-quality datasets during users' routine phone usage with minimal cost. Second, we introduce adapted aggregation to improve federated training of mobile agents on non-IID user data, by incorporating both episode- and step-level distributions. In distributed settings, FedMobileAgent achieves performance comparable to centralized human-annotated models at less than 0.02\% of the cost, highlighting its potential for real-world applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02984</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02984</id><created>2025-02-05</created><authors><author><keyname>Zhang</keyname><forenames>Dengyu</forenames></author><author><keyname>Chenghao</keyname></author><author><keyname>Xue</keyname><forenames>Feng</forenames></author><author><keyname>Zhang</keyname><forenames>Qingrui</forenames></author></authors><title>Learning Efficient Flocking Control based on Gibbs Random Fields</title><categories>cs.RO cs.LG cs.SY eess.SY</categories><comments>9 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flocking control is essential for multi-robot systems in diverse applications, yet achieving efficient flocking in congested environments poses challenges regarding computation burdens, performance optimality, and motion safety. This paper addresses these challenges through a multi-agent reinforcement learning (MARL) framework built on Gibbs Random Fields (GRFs). With GRFs, a multi-robot system is represented by a set of random variables conforming to a joint probability distribution, thus offering a fresh perspective on flocking reward design. A decentralized training and execution mechanism, which enhances the scalability of MARL concerning robot quantity, is realized using a GRF-based credit assignment method. An action attention module is introduced to implicitly anticipate the motion intentions of neighboring robots, consequently mitigating potential non-stationarity issues in MARL. The proposed framework enables learning an efficient distributed control policy for multi-robot systems in challenging environments with success rate around $99\%$, as demonstrated through thorough comparisons with state-of-the-art solutions in simulations and experiments. Ablation studies are also performed to validate the efficiency of different framework modules. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02988</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02988</id><created>2025-02-05</created><authors><author><keyname>Hu</keyname><forenames>Renjun</forenames></author><author><keyname>Cheng</keyname><forenames>Yi</forenames></author><author><keyname>Meng</keyname><forenames>Libin</forenames></author><author><keyname>Xia</keyname><forenames>Jiaxin</forenames></author><author><keyname>Zong</keyname><forenames>Yi</forenames></author><author><keyname>Shi</keyname><forenames>Xing</forenames></author><author><keyname>Lin</keyname><forenames>Wei</forenames></author></authors><title>Training an LLM-as-a-Judge Model: Pipeline, Insights, and Practical   Lessons</title><categories>cs.CL cs.AI cs.LG</categories><comments>accepted at WWW'25 (Industrial Track), extended version</comments><doi>10.1145/3701716.3715265</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The rapid advancement of large language models (LLMs) has opened new possibilities for their adoption as evaluative judges. This paper introduces Themis, a fine-tuned LLM judge that delivers sophisticated context-aware evaluations. We provide a comprehensive overview of the development pipeline for Themis, highlighting its scenario-dependent evaluation prompts and two novel methods for controlled instruction generation. These designs enable Themis to effectively distill evaluative skills from teacher models, while retaining flexibility for continuous development. We introduce two human-labeled benchmarks for meta-evaluation, demonstrating that Themis can achieve high alignment with human preferences in an economical manner. Additionally, we explore insights into the LLM-as-a-judge paradigm, revealing nuances in performance and the varied effects of reference answers. Notably, we observe that pure knowledge distillation from strong LLMs, though common, does not guarantee performance improvement through scaling. We propose a mitigation strategy based on instruction-following difficulty. Furthermore, we provide practical guidelines covering data balancing, prompt customization, multi-objective training, and metric aggregation. We aim for our method and findings, along with the fine-tuning data, benchmarks, and model checkpoints, to support future research and development in this area. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02990</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02990</id><created>2025-02-05</created><authors><author><keyname>Aamand</keyname><forenames>Anders</forenames></author><author><keyname>Boninsegna</keyname><forenames>Fabrizio</forenames></author><author><keyname>Gentle</keyname><forenames>Abigail</forenames></author><author><keyname>Imola</keyname><forenames>Jacob</forenames></author><author><keyname>Pagh</keyname><forenames>Rasmus</forenames></author></authors><title>Lightweight Protocols for Distributed Private Quantile Estimation</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed data analysis is a large and growing field driven by a massive proliferation of user devices, and by privacy concerns surrounding the centralised storage of data. We consider two \emph{adaptive} algorithms for estimating one quantile (e.g.~the median) when each user holds a single data point lying in a domain $[B]$ that can be queried once through a private mechanism; one under local differential privacy (LDP) and another for shuffle differential privacy (shuffle-DP). In the adaptive setting we present an $\varepsilon$-LDP algorithm which can estimate any quantile within error $\alpha$ only requiring $O(\frac{\log B}{\varepsilon^2\alpha^2})$ users, and an $(\varepsilon,\delta)$-shuffle DP algorithm requiring only $\widetilde{O}((\frac{1}{\varepsilon^2}+\frac{1}{\alpha^2})\log B)$ users. Prior (nonadaptive) algorithms require more users by several logarithmic factors in $B$. We further provide a matching lower bound for adaptive protocols, showing that our LDP algorithm is optimal in the low-$\varepsilon$ regime. Additionally, we establish lower bounds against non-adaptive protocols which paired with our understanding of the adaptive case, proves a fundamental separation between these models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02996</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02996</id><created>2025-02-05</created><authors><author><keyname>Stewart</keyname><forenames>Lawrence</forenames><affiliation>DI-ENS, LIENS, Inria</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>LIENS, SIERRA</affiliation></author><author><keyname>Berthet</keyname><forenames>Quentin</forenames></author></authors><title>Building Bridges between Regression, Clustering, and Classification</title><categories>stat.ML cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regression, the task of predicting a continuous scalar target y based on some features x is one of the most fundamental tasks in machine learning and statistics. It has been observed and theoretically analyzed that the classical approach, meansquared error minimization, can lead to suboptimal results when training neural networks. In this work, we propose a new method to improve the training of these models on regression tasks, with continuous scalar targets. Our method is based on casting this task in a different fashion, using a target encoder, and a prediction decoder, inspired by approaches in classification and clustering. We showcase the performance of our method on a wide range of real-world datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02997</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02997</id><created>2025-02-05</created><authors><author><keyname>Sharma</keyname><forenames>Kiran</forenames></author><author><keyname>Khurana</keyname><forenames>Parul</forenames></author></authors><title>Assessing Research Impact in Indian Conference Proceedings: Insights   from Collaboration and Citations</title><categories>cs.IR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Conferences serve as a crucial avenue for scientific communication. However, the increase in conferences and the subsequent publication of proceedings have prompted inquiries regarding the research quality being showcased at such events. This investigation delves into the conference publications indexed by Springer's Lecture Notes in Networks and Systems Series. Among the 570 international conferences held worldwide in this series, 177 were exclusively hosted in India. These 177 conferences collectively published 11,066 papers as conference proceedings. All these publications, along with conference details, were sourced from the Scopus database. The study aims to evaluate the research impact of these conference proceedings and identify the primary contributors. The results reveal a downward trend in the average number of citations per year. The collective average citation for all publications is 1.01. Papers co-authored by Indian and international authors (5.6%) exhibit a higher average impact of 1.44, in contrast to those authored solely by Indian authors (84.9%), which have an average impact of 0.97. Notably, Indian-collaborated papers, among the largest contributors, predominantly originate from private colleges and universities. Only 19% of papers exhibit collaboration with institutes of different prestige, yet their impact is considerably higher as compared to collaboration with institutes of similar prestige. This study highlights the importance of improving research quality in academic forums. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02998</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02998</id><created>2025-02-05</created><authors><author><keyname>Lyu</keyname><forenames>Fan</forenames></author><author><keyname>Zhao</keyname><forenames>Hanyu</forenames></author><author><keyname>Shi</keyname><forenames>Ziqi</forenames></author><author><keyname>Liu</keyname><forenames>Ye</forenames></author><author><keyname>Hu</keyname><forenames>Fuyuan</forenames></author><author><keyname>Zhang</keyname><forenames>Zhang</forenames></author><author><keyname>Wang</keyname><forenames>Liang</forenames></author></authors><title>Conformal Uncertainty Indicator for Continual Test-Time Adaptation</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Continual Test-Time Adaptation (CTTA) aims to adapt models to sequentially changing domains during testing, relying on pseudo-labels for self-adaptation. However, incorrect pseudo-labels can accumulate, leading to performance degradation. To address this, we propose a Conformal Uncertainty Indicator (CUI) for CTTA, leveraging Conformal Prediction (CP) to generate prediction sets that include the true label with a specified coverage probability. Since domain shifts can lower the coverage than expected, making CP unreliable, we dynamically compensate for the coverage by measuring both domain and data differences. Reliable pseudo-labels from CP are then selectively utilized to enhance adaptation. Experiments confirm that CUI effectively estimates uncertainty and improves adaptation performance across various existing CTTA methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03000</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03000</id><created>2025-02-05</created><authors><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author><author><keyname>Curtin</keyname><forenames>Ryan</forenames></author></authors><title>Armadillo: An Efficient Framework for Numerical Linear Algebra</title><categories>cs.MS</categories><msc-class>68N99, 65Y04, 65Y15, 65F45</msc-class><acm-class>G.4; G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major challenge in the deployment of scientific software solutions is the adaptation of research prototypes to production-grade code. While high-level languages like MATLAB are useful for rapid prototyping, they lack the resource efficiency required for scalable production applications, necessitating translation into lower level languages like C++. Further, for machine learning and signal processing applications, the underlying linear algebra primitives, generally provided by the standard BLAS and LAPACK libraries, are unwieldy and difficult to use, requiring manual memory management and other tedium. To address this challenge, the Armadillo C++ linear algebra library provides an intuitive interface for writing linear algebra expressions that are easily compiled into efficient production-grade implementations. We describe the expression optimisations we have implemented in Armadillo, exploiting template metaprogramming. We demonstrate that these optimisations result in considerable efficiency gains on a variety of benchmark linear algebra expressions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03004</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03004</id><created>2025-02-05</created><authors><author><keyname>Kim</keyname><forenames>Seonok</forenames></author></authors><title>MedBioLM: Optimizing Medical and Biological QA with Fine-Tuned Large   Language Models and Retrieval-Augmented Generation</title><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) have demonstrated impressive capabilities across natural language processing tasks. However, their application to specialized domains such as medicine and biology requires further optimization to ensure factual accuracy, reliability, and contextual depth. We introduce MedBioLM, a domain-adapted biomedical question-answering model designed to enhance both short-form and long-form queries. By integrating fine-tuning and retrieval-augmented generation (RAG), MedBioLM dynamically incorporates domain-specific knowledge, improving reasoning abilities and factual accuracy. To evaluate its effectiveness, we fine-tuned the model on diverse biomedical QA datasets, covering structured multiple-choice assessments and complex clinical reasoning tasks. Fine-tuning significantly improves accuracy on benchmark datasets, while RAG enhances factual consistency. These results highlight the potential of domain-optimized LLMs in advancing biomedical research, medical education, and clinical decision support. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03005</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03005</id><created>2025-02-05</created><authors><author><keyname>Zhouxiang</keyname><forenames>Long</forenames></author><author><keyname>Petrosian</keyname><forenames>Ovanes</forenames></author></authors><title>Driver Assistance System Based on Multimodal Data Hazard Detection</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous driving technology has advanced significantly, yet detecting driving anomalies remains a major challenge due to the long-tailed distribution of driving events. Existing methods primarily rely on single-modal road condition video data, which limits their ability to capture rare and unpredictable driving incidents. This paper proposes a multimodal driver assistance detection system that integrates road condition video, driver facial video, and audio data to enhance incident recognition accuracy. Our model employs an attention-based intermediate fusion strategy, enabling end-to-end learning without separate feature extraction. To support this approach, we develop a new three-modality dataset using a driving simulator. Experimental results demonstrate that our method effectively captures cross-modal correlations, reducing misjudgments and improving driving safety. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03006</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03006</id><created>2025-02-05</created><authors><author><keyname>Kusch</keyname><forenames>Jonas</forenames></author><author><keyname>Schotthöfer</keyname><forenames>Steffen</forenames></author><author><keyname>Walter</keyname><forenames>Alexandra</forenames></author></authors><title>An Augmented Backward-Corrected Projector Splitting Integrator for   Dynamical Low-Rank Training</title><categories>math.NA cs.LG cs.NA</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Layer factorization has emerged as a widely used technique for training memory-efficient neural networks. However, layer factorization methods face several challenges, particularly a lack of robustness during the training process. To overcome this limitation, dynamical low-rank training methods have been developed, utilizing robust time integration techniques for low-rank matrix differential equations. Although these approaches facilitate efficient training, they still depend on computationally intensive QR and singular value decompositions of matrices with small rank. In this work, we introduce a novel low-rank training method that reduces the number of required QR decompositions. Our approach integrates an augmentation step into a projector-splitting scheme, ensuring convergence to a locally optimal solution. We provide a rigorous theoretical analysis of the proposed method and demonstrate its effectiveness across multiple benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03008</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03008</id><created>2025-02-05</created><authors><author><keyname>Baumann</keyname><forenames>Lena</forenames></author><author><keyname>Einkemmer</keyname><forenames>Lukas</forenames></author><author><keyname>Klingenberg</keyname><forenames>Christian</forenames></author><author><keyname>Kusch</keyname><forenames>Jonas</forenames></author></authors><title>An energy stable and conservative multiplicative dynamical low-rank   discretization for the Su-Olson problem</title><categories>math.NA cs.NA</categories><msc-class>35L65, 35Q49, 65M12, 65M22</msc-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Computing numerical solutions of the thermal radiative transfer equations on a finely resolved grid can be costly due to high computational and memory requirements. A numerical reduced order method that has recently been applied to a wide variety of kinetic partial differential equations is the concept of dynamical low-rank approximation (DLRA). In this paper, we consider the thermal radiative transfer equations with Su-Olson closure, leading to a linearized kinetic model. For the conducted theoretical and practical considerations we use a multiplicative splitting of the distribution function that poses additional challenges in finding an energy stable discretization and deriving a hyperbolic Courant-Friedrichs-Lewy (CFL) condition. We propose such an energy stable DLRA scheme that makes use of the augmented basis update &amp; Galerkin integrator. This integrator allows for additional basis augmentations, enabling us to give a mathematically rigorous proof of energy stability and local mass conservation. Numerical examples confirm the derived properties and show the computational advantages of the DLRA scheme compared to a numerical solution of the full system of equations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03009</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03009</id><created>2025-02-05</created><authors><author><keyname>Liew</keyname><forenames>Seng Pei</forenames></author><author><keyname>Kato</keyname><forenames>Takuya</forenames></author><author><keyname>Takase</keyname><forenames>Sho</forenames></author></authors><title>Scaling Laws for Upcycling Mixture-of-Experts Language Models</title><categories>cs.LG cs.CL</categories><comments>15 figures, 8 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Pretraining large language models (LLMs) is resource-intensive, often requiring months of training time even with high-end GPU clusters. There are two approaches of mitigating such computational demands: reusing smaller models to train larger ones (upcycling), and training computationally efficient models like mixture-of-experts (MoE). In this paper, we study the upcycling of LLMs to MoE models, of which the scaling behavior remains underexplored. Through extensive experiments, we identify empirical scaling laws that describe how performance depends on dataset size and model configuration. Particularly, we show that, while scaling these factors improves performance, there is a novel interaction term between the dense and upcycled training dataset that limits the efficiency of upcycling at large computational budgets. Based on these findings, we provide guidance to scale upcycling, and establish conditions under which upcycling outperforms from-scratch trainings within budget constraints. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03013</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03013</id><created>2025-02-05</created><authors><author><keyname>Heim</keyname><forenames>Philippe</forenames></author><author><keyname>Dimitrova</keyname><forenames>Rayna</forenames></author></authors><title>Issy: A Comprehensive Tool for Specification and Synthesis of   Infinite-State Reactive Systems</title><categories>cs.LO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The synthesis of infinite-state reactive systems from temporal logic specifications or infinite-state games has attracted significant attention in recent years, leading to the emergence of novel solving techniques. Most approaches are accompanied by an implementation showcasing their viability on an increasingly larger collection of benchmarks. Those implementations are -- often simple -- prototypes. Furthermore, differences in specification formalisms and formats make comparisons difficult, and writing specifications is a tedious and error-prone task.   To address this, we present Issy, a tool for specification, realizability, and synthesis of infinite-state reactive systems. Issy comes with an expressive specification language that allows for combining infinite-state games and temporal formulas, thus encompassing the current formalisms. The realizability checking and synthesis methods implemented in Issy build upon recently developed approaches and extend them with newly engineered efficient techniques, offering a portfolio of solving algorithms. We evaluate Issy on an extensive set of benchmarks, demonstrating its competitiveness with the state of the art. Furthermore, Issy provides tooling for a general high-level format designed to make specification easier for users. It also includes a compiler to a more machine-readable format that other tool developers can easily use, which we hope will lead to a broader adoption and advances in infinite-state reactive synthesis. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03014</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03014</id><created>2025-02-05</created><authors><author><keyname>Seth</keyname><forenames>Pratinav</forenames></author><author><keyname>Rathore</keyname><forenames>Yashwardhan</forenames></author><author><keyname>Singh</keyname><forenames>Neeraj Kumar</forenames></author><author><keyname>Chitroda</keyname><forenames>Chintan</forenames></author><author><keyname>Sankarapu</keyname><forenames>Vinay Kumar</forenames></author></authors><title>xai_evals : A Framework for Evaluating Post-Hoc Local Explanation   Methods</title><categories>cs.LG cs.AI cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing complexity of machine learning and deep learning models has led to an increased reliance on opaque "black box" systems, making it difficult to understand the rationale behind predictions. This lack of transparency is particularly challenging in high-stakes applications where interpretability is as important as accuracy. Post-hoc explanation methods are commonly used to interpret these models, but they are seldom rigorously evaluated, raising concerns about their reliability. The Python package xai_evals addresses this by providing a comprehensive framework for generating, benchmarking, and evaluating explanation methods across both tabular and image data modalities. It integrates popular techniques like SHAP, LIME, Grad-CAM, Integrated Gradients (IG), and Backtrace, while supporting evaluation metrics such as faithfulness, sensitivity, and robustness. xai_evals enhances the interpretability of machine learning models, fostering transparency and trust in AI systems. The library is open-sourced at https://pypi.org/project/xai-evals/ . </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03016</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03016</id><created>2025-02-05</created><authors><author><keyname>Plate</keyname><forenames>Christoph</forenames></author><author><keyname>Hahn</keyname><forenames>Mirko</forenames></author><author><keyname>Klimek</keyname><forenames>Alexander</forenames></author><author><keyname>Ganzer</keyname><forenames>Caroline</forenames></author><author><keyname>Sundmacher</keyname><forenames>Kai</forenames></author><author><keyname>Sager</keyname><forenames>Sebastian</forenames></author></authors><title>An analysis of optimization problems involving ReLU neural networks</title><categories>math.OC cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Solving mixed-integer optimization problems with embedded neural networks with ReLU activation functions is challenging. Big-M coefficients that arise in relaxing binary decisions related to these functions grow exponentially with the number of layers. We survey and propose different approaches to analyze and improve the run time behavior of mixed-integer programming solvers in this context. Among them are clipped variants and regularization techniques applied during training as well as optimization-based bound tightening and a novel scaling for given ReLU networks. We numerically compare these approaches for three benchmark problems from the literature. We use the number of linear regions, the percentage of stable neurons, and overall computational effort as indicators. As a major takeaway we observe and quantify a trade-off between the often desired redundancy of neural network models versus the computational costs for solving related optimization problems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03018</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03018</id><created>2025-02-05</created><authors><author><keyname>Gu</keyname><forenames>Qiling</forenames></author><author><keyname>Zhang</keyname><forenames>Wenlong</forenames></author><author><keyname>Zhang</keyname><forenames>Zhidong</forenames></author></authors><title>Determine the point source of the heat equation with sparse boundary   measurements</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work the authors consider the recovery of the point source in the heat equation. The used data is the sparse boundary measurements. The uniqueness theorem of the inverse problem is given. After that, the numerical reconstruction is considered. We propose a numerical method to reconstruct the location of a Dirac point source by reformulating the inverse problem as a least-squares optimization problem, which is efficiently solved using a gradient descent algorithm. Numerical experiments confirm the accuracy of the proposed method and demonstrate its robustness to noise. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03020</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03020</id><created>2025-02-05</created><authors><author><keyname>Nortier</keyname><forenames>Berné L.</forenames></author><author><keyname>Dobson</keyname><forenames>Simon</forenames></author><author><keyname>Battiston</keyname><forenames>Federico</forenames></author></authors><title>Higher-order shortest paths in hypergraphs</title><categories>physics.soc-ph cs.SI</categories><comments>Pre-submission version, 10 pages and 7 figures. Uses preamble.sty</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  One of the defining features of complex networks is the connectivity properties that we observe emerging from local interactions. Recently, hypergraphs have emerged as a versatile tool to model networks with non-dyadic, higher-order interactions. Nevertheless, the connectivity properties of real-world hypergraphs remain largely understudied. In this work we introduce path size as a measure to characterise higher-order connectivity and quantify the relevance of non-dyadic ties for efficient shortest paths in a diverse set of empirical networks with and without temporal information. By comparing our results with simple randomised null models, our analysis presents a nuanced picture, suggesting that non-dyadic ties are often central and are vital for system connectivity, while dyadic edges remain essential to connect more peripheral nodes, an effect which is particularly pronounced for time-varying systems. Our work contributes to a better understanding of the structural organisation of systems with higher-order interactions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03023</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03023</id><created>2025-02-05</created><authors><author><keyname>Zeng</keyname><forenames>Hao</forenames></author><author><keyname>Liu</keyname><forenames>Kangdao</forenames></author><author><keyname>Jing</keyname><forenames>Bingyi</forenames></author><author><keyname>Wei</keyname><forenames>Hongxin</forenames></author></authors><title>Parametric Scaling Law of Tuning Bias in Conformal Prediction</title><categories>cs.LG math.ST stat.ME stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Conformal prediction is a popular framework of uncertainty quantification that constructs prediction sets with coverage guarantees. To uphold the exchangeability assumption, many conformal prediction methods necessitate an additional holdout set for parameter tuning. Yet, the impact of violating this principle on coverage remains underexplored, making it ambiguous in practical applications. In this work, we empirically find that the tuning bias - the coverage gap introduced by leveraging the same dataset for tuning and calibration, is negligible for simple parameter tuning in many conformal prediction methods. In particular, we observe the scaling law of the tuning bias: this bias increases with parameter space complexity and decreases with calibration set size. Formally, we establish a theoretical framework to quantify the tuning bias and provide rigorous proof for the scaling law of the tuning bias by deriving its upper bound. In the end, we discuss how to reduce the tuning bias, guided by the theories we developed. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03029</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03029</id><created>2025-02-05</created><authors><author><keyname>Diep</keyname><forenames>Nghiem T.</forenames></author><author><keyname>Nguyen</keyname><forenames>Huy</forenames></author><author><keyname>Nguyen</keyname><forenames>Chau</forenames></author><author><keyname>Le</keyname><forenames>Minh</forenames></author><author><keyname>Nguyen</keyname><forenames>Duy M. H.</forenames></author><author><keyname>Sonntag</keyname><forenames>Daniel</forenames></author><author><keyname>Niepert</keyname><forenames>Mathias</forenames></author><author><keyname>Ho</keyname><forenames>Nhat</forenames></author></authors><title>On Zero-Initialized Attention: Optimal Prompt and Gating Factor   Estimation</title><categories>cs.LG</categories><comments>43 pages, 5 tables, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The LLaMA-Adapter has recently emerged as an efficient fine-tuning technique for LLaMA models, leveraging zero-initialized attention to stabilize training and enhance performance. However, despite its empirical success, the theoretical foundations of zero-initialized attention remain largely unexplored. In this paper, we provide a rigorous theoretical analysis, establishing a connection between zero-initialized attention and mixture-of-expert models. We prove that both linear and non-linear prompts, along with gating functions, can be optimally estimated, with non-linear prompts offering greater flexibility for future applications. Empirically, we validate our findings on the open LLM benchmarks, demonstrating that non-linear prompts outperform linear ones. Notably, even with limited training data, both prompt types consistently surpass vanilla attention, highlighting the robustness and adaptability of zero-initialized attention. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03032</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03032</id><created>2025-02-05</created><authors><author><keyname>Laptev</keyname><forenames>Daniil</forenames></author><author><keyname>Balagansky</keyname><forenames>Nikita</forenames></author><author><keyname>Aksenov</keyname><forenames>Yaroslav</forenames></author><author><keyname>Gavrilov</keyname><forenames>Daniil</forenames></author></authors><title>Analyze Feature Flow to Enhance Interpretation and Steering in Language   Models</title><categories>cs.LG cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce a new approach to systematically map features discovered by sparse autoencoder across consecutive layers of large language models, extending earlier work that examined inter-layer feature links. By using a data-free cosine similarity technique, we trace how specific features persist, transform, or first appear at each stage. This method yields granular flow graphs of feature evolution, enabling fine-grained interpretability and mechanistic insights into model computations. Crucially, we demonstrate how these cross-layer feature maps facilitate direct steering of model behavior by amplifying or suppressing chosen features, achieving targeted thematic control in text generation. Together, our findings highlight the utility of a causal, cross-layer interpretability framework that not only clarifies how features develop through forward passes but also provides new means for transparent manipulation of large language models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03033</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03033</id><created>2025-02-05</created><authors><author><keyname>Zhang</keyname><forenames>Zhen</forenames></author><author><keyname>He</keyname><forenames>Bingsheng</forenames></author></authors><title>Aggregate to Adapt: Node-Centric Aggregation for Multi-Source-Free Graph   Domain Adaptation</title><categories>cs.LG</categories><comments>Accepted by WWW-2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Unsupervised graph domain adaptation (UGDA) focuses on transferring knowledge from labeled source graph to unlabeled target graph under domain discrepancies. Most existing UGDA methods are designed to adapt information from a single source domain, which cannot effectively exploit the complementary knowledge from multiple source domains. Furthermore, their assumptions that the labeled source graphs are accessible throughout the training procedure might not be practical due to privacy, regulation, and storage concerns. In this paper, we investigate multi-source-free unsupervised graph domain adaptation, i.e., adapting knowledge from multiple source domains to an unlabeled target domain without utilizing labeled source graphs but relying solely on source pre-trained models. Unlike previous multi-source domain adaptation approaches that aggregate predictions at model level, we introduce a novel model named GraphATA which conducts adaptation at node granularity. Specifically, we parameterize each node with its own graph convolutional matrix by automatically aggregating weight matrices from multiple source models according to its local context, thus realizing dynamic adaptation over graph structured data. We also demonstrate the capability of GraphATA to generalize to both model-centric and layer-centric methods. Comprehensive experiments on various public datasets show that our GraphATA can consistently surpass recent state-of-the-art baselines with different gains. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03034</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03034</id><created>2025-02-05</created><authors><author><keyname>Takrouri</keyname><forenames>Mohannad</forenames></author><author><keyname>Cuadrado</keyname><forenames>Nicolás M.</forenames></author><author><keyname>Takáč</keyname><forenames>Martin</forenames></author></authors><title>Knowledge Distillation from Large Language Models for Household Energy   Modeling</title><categories>cs.CL cs.LG</categories><comments>Source code is available at   https://github.com/Singularity-AI-Lab/LLM-Energy-Knowledge-Distillation</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Machine learning (ML) is increasingly vital for smart-grid research, yet restricted access to realistic, diverse data - often due to privacy concerns - slows progress and fuels doubts within the energy sector about adopting ML-based strategies. We propose integrating Large Language Models (LLMs) in energy modeling to generate realistic, culturally sensitive, and behavior-specific data for household energy usage across diverse geographies. In this study, we employ and compare five different LLMs to systematically produce family structures, weather patterns, and daily consumption profiles for households in six distinct countries. A four-stage methodology synthesizes contextual daily data, including culturally nuanced activities, realistic weather ranges, HVAC operations, and distinct `energy signatures' that capture unique consumption footprints. Additionally, we explore an alternative strategy where external weather datasets can be directly integrated, bypassing intermediate weather modeling stages while ensuring physically consistent data inputs. The resulting dataset provides insights into how cultural, climatic, and behavioral factors converge to shape carbon emissions, offering a cost-effective avenue for scenario-based energy optimization. This approach underscores how prompt engineering, combined with knowledge distillation, can advance sustainable energy research and climate mitigation efforts. Source code is available at https://github.com/Singularity-AI-Lab/LLM-Energy-Knowledge-Distillation . </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03035</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03035</id><created>2025-02-05</created><authors><author><keyname>Qiu</keyname><forenames>Yu</forenames></author><author><keyname>Lin</keyname><forenames>Xin</forenames></author><author><keyname>Wang</keyname><forenames>Jingbo</forenames></author><author><keyname>Li</keyname><forenames>Xiangtai</forenames></author><author><keyname>Qi</keyname><forenames>Lu</forenames></author><author><keyname>Yang</keyname><forenames>Ming-Hsuan</forenames></author></authors><title>UMC: Unified Resilient Controller for Legged Robots with Joint   Malfunctions</title><categories>cs.RO</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Adaptation to unpredictable damages is crucial for autonomous legged robots, yet existing methods based on multi-policy or meta-learning frameworks face challenges like limited generalization and complex maintenance. To address this issue, we first analyze and summarize eight types of damage scenarios, including sensor failures and joint malfunctions. Then, we propose a novel, model-free, two-stage training framework, Unified Malfunction Controller (UMC), incorporating a masking mechanism to enhance damage resilience. Specifically, the model is initially trained with normal environments to ensure robust performance under standard conditions. In the second stage, we use masks to prevent the legged robot from relying on malfunctioning limbs, enabling adaptive gait and movement adjustments upon malfunction. Experimental results demonstrate that our approach improves the task completion capability by an average of 36% for the transformer and 39% for the MLP across three locomotion tasks. The source code and trained models will be made available to the public. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03036</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03036</id><created>2025-02-05</created><authors><author><keyname>Ye</keyname><forenames>Yufei</forenames></author><author><keyname>Guo</keyname><forenames>Wei</forenames></author><author><keyname>Chin</keyname><forenames>Jin Yao</forenames></author><author><keyname>Wang</keyname><forenames>Hao</forenames></author><author><keyname>Zhu</keyname><forenames>Hong</forenames></author><author><keyname>Lin</keyname><forenames>Xi</forenames></author><author><keyname>Ye</keyname><forenames>Yuyang</forenames></author><author><keyname>Liu</keyname><forenames>Yong</forenames></author><author><keyname>Tang</keyname><forenames>Ruiming</forenames></author><author><keyname>Lian</keyname><forenames>Defu</forenames></author><author><keyname>Chen</keyname><forenames>Enhong</forenames></author></authors><title>FuXi-$\alpha$: Scaling Recommendation Model with Feature Interaction   Enhanced Transformer</title><categories>cs.IR</categories><comments>Accepted by WWW2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by scaling laws and large language models, research on large-scale recommendation models has gained significant attention. Recent advancements have shown that expanding sequential recommendation models to large-scale recommendation models can be an effective strategy. Current state-of-the-art sequential recommendation models primarily use self-attention mechanisms for explicit feature interactions among items, while implicit interactions are managed through Feed-Forward Networks (FFNs). However, these models often inadequately integrate temporal and positional information, either by adding them to attention weights or by blending them with latent representations, which limits their expressive power. A recent model, HSTU, further reduces the focus on implicit feature interactions, constraining its performance. We propose a new model called FuXi-$\alpha$ to address these issues. This model introduces an Adaptive Multi-channel Self-attention mechanism that distinctly models temporal, positional, and semantic features, along with a Multi-stage FFN to enhance implicit feature interactions. Our offline experiments demonstrate that our model outperforms existing models, with its performance continuously improving as the model size increases. Additionally, we conducted an online A/B test within the Huawei Music app, which showed a $4.76\%$ increase in the average number of songs played per user and a $5.10\%$ increase in the average listening duration per user. Our code has been released at https://github.com/USTC-StarTeam/FuXi-alpha. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03038</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03038</id><created>2025-02-05</created><authors><author><keyname>Mundt</keyname><forenames>Martin</forenames></author><author><keyname>Ovalle</keyname><forenames>Anaelia</forenames></author><author><keyname>Friedrich</keyname><forenames>Felix</forenames></author><author><keyname>Agrawal</keyname><forenames>Pranav</forenames></author><author><keyname>Paul</keyname><forenames>Subarnaduti</forenames></author><author><keyname>Brack</keyname><forenames>Manuel</forenames></author><author><keyname>Kersting</keyname><forenames>Kristian</forenames></author><author><keyname>Agnew</keyname><forenames>William</forenames></author></authors><title>The Cake that is Intelligence and Who Gets to Bake it: An AI Analogy and   its Implications for Participation</title><categories>cs.AI cs.CY cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a widely popular analogy by Turing Award Laureate Yann LeCun, machine intelligence has been compared to cake - where unsupervised learning forms the base, supervised learning adds the icing, and reinforcement learning is the cherry on top. We expand this 'cake that is intelligence' analogy from a simple structural metaphor to the full life-cycle of AI systems, extending it to sourcing of ingredients (data), conception of recipes (instructions), the baking process (training), and the tasting and selling of the cake (evaluation and distribution). Leveraging our re-conceptualization, we describe each step's entailed social ramifications and how they are bounded by statistical assumptions within machine learning. Whereas these technical foundations and social impacts are deeply intertwined, they are often studied in isolation, creating barriers that restrict meaningful participation. Our re-conceptualization paves the way to bridge this gap by mapping where technical foundations interact with social outcomes, highlighting opportunities for cross-disciplinary dialogue. Finally, we conclude with actionable recommendations at each stage of the metaphorical AI cake's life-cycle, empowering prospective AI practitioners, users, and researchers, with increased awareness and ability to engage in broader AI discourse. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03040</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03040</id><created>2025-02-05</created><authors><author><keyname>Alex</keyname><forenames>Bazigu</forenames></author><author><keyname>Johnson</keyname><forenames>Mwebaze</forenames></author></authors><title>A Framework for IoT-Enabled Smart Manufacturing for Energy and Resource   Optimization</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The increasing demands for sustainable and efficient manufacturing systems have driven the integration of Internet of Things (IoT) technologies into smart manufacturing. This study investigates IoT-enabled systems designed to enhance energy efficiency and resource optimization in the manufacturing sector, focusing on a multi-layered architecture integrating sensors, edge computing, and cloud platforms. MATLAB Simulink was utilized for modeling and simulation, replicating typical manufacturing conditions to evaluate energy consumption, machine uptime, and resource usage. The results demonstrate an 18% reduction in energy consumption, a 22% decrease in machine downtime, and a 15% improvement in resource utilization. Comparative analyses highlight the superiority of the proposed framework in addressing operational inefficiencies and aligning with sustainability goals. The study underscores the potential of IoT in transforming traditional manufacturing into interconnected, intelligent systems, offering practical implications for industrial stakeholders aiming to optimize operations while adhering to global sustainability standards. Future work will focus on addressing identified challenges such as high deployment costs and data security concerns, aiming to facilitate the broader adoption of IoT in industrial applications.   Keywords: IoT (Internet of Things), Smart Manufacturing, Energy Efficiency, Resource Optimization, Manufacturing </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03041</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03041</id><created>2025-02-05</created><authors><author><keyname>Jiang</keyname><forenames>Junguang</forenames></author><author><keyname>Huang</keyname><forenames>Yanwen</forenames></author><author><keyname>Liu</keyname><forenames>Bin</forenames></author><author><keyname>Kong</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Xu</keyname><forenames>Ziru</forenames></author><author><keyname>Zhu</keyname><forenames>Han</forenames></author><author><keyname>Xu</keyname><forenames>Jian</forenames></author><author><keyname>Zheng</keyname><forenames>Bo</forenames></author></authors><title>Large Language Models Are Universal Recommendation Learners</title><categories>cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In real-world recommender systems, different tasks are typically addressed using supervised learning on task-specific datasets with carefully designed model architectures. We demonstrate that large language models (LLMs) can function as universal recommendation learners, capable of handling multiple tasks within a unified input-output framework, eliminating the need for specialized model designs. To improve the recommendation performance of LLMs, we introduce a multimodal fusion module for item representation and a sequence-in-set-out approach for efficient candidate generation. When applied to industrial-scale data, our LLM achieves competitive results with expert models elaborately designed for different recommendation tasks. Furthermore, our analysis reveals that recommendation outcomes are highly sensitive to text input, highlighting the potential of prompt engineering in optimizing industrial-scale recommender systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03044</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03044</id><created>2025-02-05</created><authors><author><keyname>Truong</keyname><forenames>Tuan</forenames></author><author><keyname>Nguyen</keyname><forenames>Chau</forenames></author><author><keyname>Nguyen</keyname><forenames>Huy</forenames></author><author><keyname>Le</keyname><forenames>Minh</forenames></author><author><keyname>Le</keyname><forenames>Trung</forenames></author><author><keyname>Ho</keyname><forenames>Nhat</forenames></author></authors><title>RepLoRA: Reparameterizing Low-Rank Adaptation via the Perspective of   Mixture of Experts</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Low-rank adaptation (LoRA) has emerged as a powerful method for fine-tuning large-scale foundation models. Despite its popularity, the theoretical understanding of LoRA has remained limited. This paper presents a theoretical analysis of LoRA by examining its connection to the Mixture of Experts models. Under this framework, we show that simple reparameterizations of the LoRA matrices can notably accelerate the low-rank matrix estimation process. In particular, we prove that reparameterization can reduce the data needed to achieve a desired estimation error from an exponential to a polynomial scale. Motivated by this insight, we propose Reparameterized Low-rank Adaptation (RepLoRA), which incorporates lightweight MLPs to reparameterize the LoRA matrices. Extensive experiments across multiple domains demonstrate that RepLoRA consistently outperforms vanilla LoRA. Notably, with limited data, RepLoRA surpasses LoRA by a margin of up to 40.0% and achieves LoRA's performance with only 30.0% of the training data, highlighting both the theoretical and empirical robustness of our PEFT method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03047</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03047</id><created>2025-02-05</created><authors><author><keyname>de Vries</keyname><forenames>Sigur</forenames></author><author><keyname>Keemink</keyname><forenames>Sander W.</forenames></author><author><keyname>van Gerven</keyname><forenames>Marcel A. J.</forenames></author></authors><title>Kozax: Flexible and Scalable Genetic Programming in JAX</title><categories>cs.NE cs.AI</categories><comments>5 figures, 3 tables, 1 algorithm, 10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Genetic programming is an optimization algorithm inspired by natural selection which automatically evolves the structure of computer programs. The resulting computer programs are interpretable and efficient compared to black-box models with fixed structure. The fitness evaluation in genetic programming suffers from high computational requirements, limiting the performance on difficult problems. To reduce the runtime, many implementations of genetic programming require a specific data format, making the applicability limited to specific problem classes. Consequently, there is no efficient genetic programming framework that is usable for a wide range of tasks. To this end, we developed Kozax, a genetic programming framework that evolves symbolic expressions for arbitrary problems. We implemented Kozax using JAX, a framework for high-performance and scalable machine learning, which allows the fitness evaluation to scale efficiently to large populations or datasets on GPU. Furthermore, Kozax offers constant optimization, custom operator definition and simultaneous evolution of multiple trees. We demonstrate successful applications of Kozax to discover equations of natural laws, recover equations of hidden dynamic variables and evolve a control policy. Overall, Kozax provides a general, fast, and scalable library to optimize white-box solutions in the realm of scientific computing. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03048</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03048</id><created>2025-02-05</created><authors><author><keyname>MacKinlay</keyname><forenames>Dan</forenames></author></authors><title>The Ensemble Kalman Update is an Empirical Matheron Update</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Ensemble Kalman Filter (EnKF) is a widely used method for data assimilation in high-dimensional systems. In this paper, we show that the ensemble update step of the EnKF is equivalent to an empirical version of the Matheron update popular in the study of Gaussian process regression.   While this connection is simple, it seems not to be widely known, the literature about each technique seems distinct, and connections between the methods are not exploited. This paper exists to provide an informal introduction to the connection, with the necessary definitions so that it is intelligible to as broad an audience as possible. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03052</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03052</id><created>2025-02-05</created><authors><author><keyname>Lin</keyname><forenames>Runqi</forenames></author><author><keyname>Han</keyname><forenames>Bo</forenames></author><author><keyname>Li</keyname><forenames>Fengwang</forenames></author><author><keyname>Liu</keyname><forenames>Tongling</forenames></author></authors><title>Understanding and Enhancing the Transferability of Jailbreaking Attacks</title><categories>cs.LG cs.CR</categories><comments>Accepted by ICLR 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Jailbreaking attacks can effectively manipulate open-source large language models (LLMs) to produce harmful responses. However, these attacks exhibit limited transferability, failing to disrupt proprietary LLMs consistently. To reliably identify vulnerabilities in proprietary LLMs, this work investigates the transferability of jailbreaking attacks by analysing their impact on the model's intent perception. By incorporating adversarial sequences, these attacks can redirect the source LLM's focus away from malicious-intent tokens in the original input, thereby obstructing the model's intent recognition and eliciting harmful responses. Nevertheless, these adversarial sequences fail to mislead the target LLM's intent perception, allowing the target LLM to refocus on malicious-intent tokens and abstain from responding. Our analysis further reveals the inherent distributional dependency within the generated adversarial sequences, whose effectiveness stems from overfitting the source LLM's parameters, resulting in limited transferability to target LLMs. To this end, we propose the Perceived-importance Flatten (PiF) method, which uniformly disperses the model's focus across neutral-intent tokens in the original input, thus obscuring malicious-intent tokens without relying on overfitted adversarial sequences. Extensive experiments demonstrate that PiF provides an effective and efficient red-teaming evaluation for proprietary LLMs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03053</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03053</id><created>2025-02-05</created><authors><author><keyname>Nakhlé</keyname><forenames>Mariam</forenames></author><author><keyname>Dinarelli</keyname><forenames>Marco</forenames></author><author><keyname>Qader</keyname><forenames>Raheel</forenames></author><author><keyname>Esperança-Rodier</keyname><forenames>Emmanuelle</forenames></author><author><keyname>Blanchon</keyname><forenames>Hervé</forenames></author></authors><title>DOLFIN -- Document-Level Financial test set for Machine Translation</title><categories>cs.CL</categories><comments>To be published in NAACL 2025 Findings</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Despite the strong research interest in document-level Machine Translation (MT), the test sets dedicated to this task are still scarce. The existing test sets mainly cover topics from the general domain and fall short on specialised domains, such as legal and financial. Also, in spite of their document-level aspect, they still follow a sentence-level logic that does not allow for including certain linguistic phenomena such as information reorganisation. In this work, we aim to fill this gap by proposing a novel test set: DOLFIN. The dataset is built from specialised financial documents, and it makes a step towards true document-level MT by abandoning the paradigm of perfectly aligned sentences, presenting data in units of sections rather than sentences. The test set consists of an average of 1950 aligned sections for five language pairs. We present a detailed data collection pipeline that can serve as inspiration for aligning new document-level datasets. We demonstrate the usefulness and quality of this test set by evaluating a number of models. Our results show that the test set is able to discriminate between context-sensitive and context-agnostic models and shows the weaknesses when models fail to accurately translate financial texts. The test set is made public for the community. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03057</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03057</id><created>2025-02-05</created><authors><author><keyname>Simpsi</keyname><forenames>Andrea</forenames></author><author><keyname>Aspesi</keyname><forenames>Andrea</forenames></author><author><keyname>Mentasti</keyname><forenames>Simone</forenames></author><author><keyname>Merigo</keyname><forenames>Luca</forenames></author><author><keyname>Ongarello</keyname><forenames>Tommaso</forenames></author><author><keyname>Matteucci</keyname><forenames>Matteo</forenames></author></authors><title>High-frequency near-eye ground truth for event-based eye tracking</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Event-based eye tracking is a promising solution for efficient and low-power eye tracking in smart eyewear technologies. However, the novelty of event-based sensors has resulted in a limited number of available datasets, particularly those with eye-level annotations, crucial for algorithm validation and deep-learning training. This paper addresses this gap by presenting an improved version of a popular event-based eye-tracking dataset. We introduce a semi-automatic annotation pipeline specifically designed for event-based data annotation. Additionally, we provide the scientific community with the computed annotations for pupil detection at 200Hz. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03059</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03059</id><created>2025-02-05</created><authors><author><keyname>Wilson</keyname><forenames>Thomas D.</forenames></author><author><keyname>Maceviciute</keyname><forenames>Elena</forenames></author></authors><title>Where is information research?</title><categories>cs.DL</categories><comments>15 Pages, 6 tables, 1 figure</comments><acm-class>H.4</acm-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We report on a preliminary investigation into the current scope of research in information management, adopting a conceptual approach derived from previous work by Hj{\o}rland in information science and by Palvia in information systems. We created a data-set of 107 articles resulting from a search in Web of Science, using the search strategy of the term information management in the titles of articles, and then restricting the analysis to those journals we identified as having an information science orientation, rather than an information systems orientation. The analysis reveals the International Journal of Information Management as the most significant journal in the field, but also draws attention to the rise of interest in the field through contributions to two Brazilian journals and one Spanish journal. The thematic analysis revealed that the dominant research themes from the information science perspective were empirical user studies, studies of the structural and institutional approach, and information system usage and adoption. Further work will be undertaken to explore the relevance of the approach in the analysis of other document sets from areas such as health care, construction and engineering. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03061</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03061</id><created>2025-02-05</created><authors><author><keyname>Shahverdikondori</keyname><forenames>Mohammad</forenames></author><author><keyname>Abouei</keyname><forenames>Amir Mohammad</forenames></author><author><keyname>Rezaeimoghadam</keyname><forenames>Alireza</forenames></author><author><keyname>Kiyavash</keyname><forenames>Negar</forenames></author></authors><title>Optimal Best Arm Identification with Post-Action Context</title><categories>cs.LG</categories><comments>37 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce the problem of best arm identification (BAI) with post-action context, a new BAI problem in a stochastic multi-armed bandit environment and the fixed-confidence setting. The problem addresses the scenarios in which the learner receives a $\textit{post-action context}$ in addition to the reward after playing each action. This post-action context provides additional information that can significantly facilitate the decision process. We analyze two different types of the post-action context: (i) $\textit{non-separator}$, where the reward depends on both the action and the context, and (ii) $\textit{separator}$, where the reward depends solely on the context. For both cases, we derive instance-dependent lower bounds on the sample complexity and propose algorithms that asymptotically achieve the optimal sample complexity. For the non-separator setting, we do so by demonstrating that the Track-and-Stop algorithm can be extended to this setting. For the separator setting, we propose a novel sampling rule called $\textit{G-tracking}$, which uses the geometry of the context space to directly track the contexts rather than the actions. Finally, our empirical results showcase the advantage of our approaches compared to the state of the art. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03062</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03062</id><created>2025-02-05</created><authors><author><keyname>Yamada</keyname><forenames>Akifumi</forenames></author><author><keyname>Shiraishi</keyname><forenames>Tomohiro</forenames></author><author><keyname>Nishino</keyname><forenames>Shuichi</forenames></author><author><keyname>Katsuoka</keyname><forenames>Teruyuki</forenames></author><author><keyname>Taji</keyname><forenames>Kouichi</forenames></author><author><keyname>Takeuchi</keyname><forenames>Ichiro</forenames></author></authors><title>Time Series Anomaly Detection in the Frequency Domain with Statistical   Reliability</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Effective anomaly detection in complex systems requires identifying change points (CPs) in the frequency domain, as abnormalities often arise across multiple frequencies. This paper extends recent advancements in statistically significant CP detection, based on Selective Inference (SI), to the frequency domain. The proposed SI method quantifies the statistical significance of detected CPs in the frequency domain using $p$-values, ensuring that the detected changes reflect genuine structural shifts in the target system. We address two major technical challenges to achieve this. First, we extend the existing SI framework to the frequency domain by appropriately utilizing the properties of discrete Fourier transform (DFT). Second, we develop an SI method that provides valid $p$-values for CPs where changes occur across multiple frequencies. Experimental results demonstrate that the proposed method reliably identifies genuine CPs with strong statistical guarantees, enabling more accurate root-cause analysis in the frequency domain of complex systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03065</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03065</id><created>2025-02-05</created><authors><author><keyname>Kruff</keyname><forenames>A. K.</forenames></author><author><keyname>Schaer</keyname><forenames>P.</forenames></author></authors><title>Scientometric Analysis of the German IR Community within TREC &amp; CLEF</title><categories>cs.IR cs.DL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Within this study, the influence of the German Information Retrieval community on the retrieval campaigns Text Retrieval Conference (TREC) and Conference and Labs of the Evaluation Forum (CLEF) between 2000 and 2022 was analyzed based on metadata provided by OpenAlex and further metadata extracted with the GROBID framework from the publication's full texts. The analysis was conducted at the institutional and researcher levels. It was found that the German IR community, both on the author and institution level, mainly contributed to CLEF. Furthermore, it was shown that productivity follows the assumptions made by Lotka's Law. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03067</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03067</id><created>2025-02-05</created><authors><author><keyname>Orfanoudakis</keyname><forenames>Stavros</forenames></author><author><keyname>Palensky</keyname><forenames>Peter</forenames></author><author><keyname>Vergara</keyname><forenames>Pedro P.</forenames></author></authors><title>Optimizing Electric Vehicles Charging using Large Language Models and   Graph Neural Networks</title><categories>eess.SY cs.LG cs.SY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Maintaining grid stability amid widespread electric vehicle (EV) adoption is vital for sustainable transportation. Traditional optimization methods and Reinforcement Learning (RL) approaches often struggle with the high dimensionality and dynamic nature of real-time EV charging, leading to sub-optimal solutions. To address these challenges, this study demonstrates that combining Large Language Models (LLMs), for sequence modeling, with Graph Neural Networks (GNNs), for relational information extraction, not only outperforms conventional EV smart charging methods, but also paves the way for entirely new research directions and innovative solutions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03068</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03068</id><created>2025-02-05</created><authors><author><keyname>Chaikovskii</keyname><forenames>Dmitrii</forenames></author><author><keyname>Zhang</keyname><forenames>Ye</forenames></author><author><keyname>Liubavin</keyname><forenames>Aleksei</forenames></author></authors><title>Internal layer solutions and coefficient recovery in time-periodic   reaction-diffusion-advection equations</title><categories>math.NA cs.NA math.AP</categories><comments>32 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This article investigates the non-stationary reaction-diffusion-advection equation, emphasizing solutions with internal layers and the associated inverse problems. We examine a nonlinear singularly perturbed partial differential equation (PDE) within a bounded spatial domain and an infinite temporal domain, subject to periodic temporal boundary conditions. A periodic asymptotic solution featuring an inner transition layer is proposed, advancing the mathematical modeling of reaction-diffusion-advection dynamics. Building on this asymptotic analysis, we develop a simple yet effective numerical algorithm to address ill-posed nonlinear inverse problems aimed at reconstructing coefficient functions that depend solely on spatial or temporal variables. Conditions ensuring the existence and uniqueness of solutions for both forward and inverse problems are established. The proposed method's effectiveness is validated through numerical experiments, demonstrating high accuracy in reconstructing coefficient functions under varying noise conditions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03069</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03069</id><created>2025-02-05</created><authors><author><keyname>Rasch</keyname><forenames>Julian</forenames></author><author><keyname>Töws</keyname><forenames>Julia</forenames></author><author><keyname>Hirzle</keyname><forenames>Teresa</forenames></author><author><keyname>Müller</keyname><forenames>Florian</forenames></author><author><keyname>Schmitz</keyname><forenames>Martin</forenames></author></authors><title>CreepyCoCreator? Investigating AI Representation Modes for 3D Object   Co-Creation in Virtual Reality</title><categories>cs.HC</categories><comments>To appear: Proceedings of the 2025 CHI Conference on Human Factors in   Computing Systems - CHI '25</comments><doi>10.1145/3706598.3713720</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Generative AI in Virtual Reality enables users to create detailed immersive worlds with a rich variety However current worldbuilding systems often lack an understanding of the fundamental aspects of human-AI cocreation resulting in a disconnect between user intent and AIgenerated content This paper investigates the co-creative process between users and an object-generating AI in Virtual Reality Through a WizardofOz study we explore how AI can represent its intent to users when customizing objects Inspired by human-to-human collaboration we focus on three representation modes the presence of an embodied avatar whether the AIs contributions are visualized immediately or incrementally and whether the areas modified are highlighted in advance The findings provide insights into how these factors affect user perception and interaction with object-generating AI in Virtual Reality The results offer design implications for co-creative worldbuilding systems aiming to foster more effective and satisfying collaborations between humans and AI in Virtual Reality. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03070</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03070</id><created>2025-02-05</created><authors><author><keyname>Carlsson</keyname><forenames>Marcus</forenames></author><author><keyname>Nikitin</keyname><forenames>Viktor</forenames></author><author><keyname>Troedsson</keyname><forenames>Erik</forenames></author><author><keyname>Wendt</keyname><forenames>Herwig</forenames></author></authors><title>The bilinear Hessian for large scale optimization</title><categories>math.OC cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Second order information is useful in many ways in smooth optimization problems, including for the design of step size rules and descent directions, or the analysis of the local properties of the objective functional. However, the computation and storage of the Hessian matrix using second order partial derivatives is prohibitive in many contexts, and in particular in large scale problems. In this work, we propose a new framework for computing and presenting second order information in analytic form. The key novel insight is that the Hessian for a problem can be worked with efficiently by computing its bilinear form or operator form using Taylor expansions, instead of introducing a basis and then computing the Hessian matrix. Our new framework is suited for high-dimensional problems stemming e.g. from imaging applications, where computation of the Hessian matrix is unfeasible. We also show how this can be used to implement Newton's step rule, Daniel's Conjugate Gradient rule, or Quasi-Newton schemes, without explicit knowledge of the Hessian matrix, and illustrate our findings with a simple numerical experiment. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03072</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03072</id><created>2025-02-05</created><authors><author><keyname>Huang</keyname><forenames>Yiqi</forenames></author><author><keyname>Davies</keyname><forenames>Travis</forenames></author><author><keyname>Yan</keyname><forenames>Jiahuan</forenames></author><author><keyname>Chen</keyname><forenames>Xiang</forenames></author><author><keyname>Tian</keyname><forenames>Yu</forenames></author><author><keyname>Hu</keyname><forenames>Luhui</forenames></author></authors><title>RoboGrasp: A Universal Grasping Policy for Robust Robotic Control</title><categories>cs.RO cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Imitation learning and world models have shown significant promise in advancing generalizable robotic learning, with robotic grasping remaining a critical challenge for achieving precise manipulation. Existing methods often rely heavily on robot arm state data and RGB images, leading to overfitting to specific object shapes or positions. To address these limitations, we propose RoboGrasp, a universal grasping policy framework that integrates pretrained grasp detection models with robotic learning. By leveraging robust visual guidance from object detection and segmentation tasks, RoboGrasp significantly enhances grasp precision, stability, and generalizability, achieving up to 34% higher success rates in few-shot learning and grasping box prompt tasks. Built on diffusion-based methods, RoboGrasp is adaptable to various robotic learning paradigms, enabling precise and reliable manipulation across diverse and complex scenarios. This framework represents a scalable and versatile solution for tackling real-world challenges in robotic grasping. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03078</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03078</id><created>2025-02-05</created><authors><author><keyname>Freise</keyname><forenames>Nina</forenames></author><author><keyname>Heitlinger</keyname><forenames>Marius</forenames></author><author><keyname>Nuredini</keyname><forenames>Ruben</forenames></author><author><keyname>Meixner</keyname><forenames>Gerrit</forenames></author></authors><title>Automatic Prompt Optimization Techniques: Exploring the Potential for   Synthetic Data Generation</title><categories>cs.HC cs.LG</categories><comments>Accepted for publication in the Proceedings of the 2025 HCI   International Conference</comments><msc-class>68T05, 68T20, 62H35, 92C50</msc-class><acm-class>I.2.0; I.2.6; I.5.2; J.3</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Artificial Intelligence (AI) advancement is heavily dependent on access to large-scale, high-quality training data. However, in specialized domains such as healthcare, data acquisition faces significant constraints due to privacy regulations, ethical considerations, and limited availability. While synthetic data generation offers a promising solution, conventional approaches typically require substantial real data for training generative models. The emergence of large-scale prompt-based models presents new opportunities for synthetic data generation without direct access to protected data. However, crafting effective prompts for domain-specific data generation remains challenging, and manual prompt engineering proves insufficient for achieving output with sufficient precision and authenticity. We review recent developments in automatic prompt optimization, following PRISMA guidelines. We analyze six peer-reviewed studies published between 2020 and 2024 that focus on automatic data-free prompt optimization methods. Our analysis reveals three approaches: feedback-driven, error-based, and control-theoretic. Although all approaches demonstrate promising capabilities in prompt refinement and adaptation, our findings suggest the need for an integrated framework that combines complementary optimization techniques to enhance synthetic data generation while minimizing manual intervention. We propose future research directions toward developing robust, iterative prompt optimization frameworks capable of improving the quality of synthetic data. This advancement can be particularly crucial for sensitive fields and in specialized domains where data access is restricted, potentially transforming how we approach synthetic data generation for AI development. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03080</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03080</id><created>2025-02-05</created><authors><author><keyname>Diallo</keyname><forenames>Aissatou</forenames></author><author><keyname>Bikakis</keyname><forenames>Antonis</forenames></author><author><keyname>Dickens</keyname><forenames>Luke</forenames></author><author><keyname>Hunter</keyname><forenames>Anthony</forenames></author><author><keyname>Miller</keyname><forenames>Rob</forenames></author></authors><title>IAO Prompting: Making Knowledge Flow Explicit in LLMs through Structured   Reasoning Templates</title><categories>cs.CL</categories><comments>Accepted as Oral at KnowFM @ AAAI 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While Large Language Models (LLMs) demonstrate impressive reasoning capabilities, understanding and validating their knowledge utilization remains challenging. Chain-of-thought (CoT) prompting partially addresses this by revealing intermediate reasoning steps, but the knowledge flow and application remain implicit. We introduce IAO (Input-Action-Output) prompting, a structured template-based method that explicitly models how LLMs access and apply their knowledge during complex reasoning tasks. IAO decomposes problems into sequential steps, each clearly identifying the input knowledge being used, the action being performed, and the resulting output. This structured decomposition enables us to trace knowledge flow, verify factual consistency, and identify potential knowledge gaps or misapplications. Through experiments across diverse reasoning tasks, we demonstrate that IAO not only improves zero-shot performance but also provides transparency in how LLMs leverage their stored knowledge. Human evaluation confirms that this structured approach enhances our ability to verify knowledge utilization and detect potential hallucinations or reasoning errors. Our findings provide insights into both knowledge representation within LLMs and methods for more reliable knowledge application. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03081</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03081</id><created>2025-02-05</created><authors><author><keyname>Rajabi</keyname><forenames>Nona</forenames></author><author><keyname>Ribeiro</keyname><forenames>Antônio H.</forenames></author><author><keyname>Vasco</keyname><forenames>Miguel</forenames></author><author><keyname>Taleb</keyname><forenames>Farzaneh</forenames></author><author><keyname>Björkman</keyname><forenames>Mårten</forenames></author><author><keyname>Kragic</keyname><forenames>Danica</forenames></author></authors><title>Human-Aligned Image Models Improve Visual Decoding from the Brain</title><categories>cs.CV cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Decoding visual images from brain activity has significant potential for advancing brain-computer interaction and enhancing the understanding of human perception. Recent approaches align the representation spaces of images and brain activity to enable visual decoding. In this paper, we introduce the use of human-aligned image encoders to map brain signals to images. We hypothesize that these models more effectively capture perceptual attributes associated with the rapid visual stimuli presentations commonly used in visual brain data recording experiments. Our empirical results support this hypothesis, demonstrating that this simple modification improves image retrieval accuracy by up to 21% compared to state-of-the-art methods. Comprehensive experiments confirm consistent performance improvements across diverse EEG architectures, image encoders, alignment methods, participants, and brain imaging modalities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03083</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03083</id><created>2025-02-05</created><authors><author><keyname>Alex</keyname><forenames>Bazigu</forenames><affiliation>PhD</affiliation></author><author><keyname>Nafuna</keyname><forenames>jacinta</forenames><affiliation>PhD</affiliation></author><author><keyname>Mirembe</keyname><forenames>Drake</forenames><affiliation>PhD</affiliation></author></authors><title>The Role of Mobile and Social Media Services in Enhancing Freedom of   Expression: Opportunities, Challenges, and Prospects for Local Platform   Development in Uganda's Digital Ecosystem</title><categories>cs.CY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Utilizing mobile and social media platforms is a transformative approach to enhancing freedom of expression and fostering digital engagement. However, Uganda's digital ecosystem faces challenges such as restrictive legislation, financial barriers, and the absence of localized platforms tailored to cultural contexts. This study employed a mixed-methods approach to explore how these platforms influence public discourse, activism, and civic participation while highlighting opportunities for local innovation. The research further identified the critical need for regulatory reforms, investments in digital literacy, and collaborative efforts to develop sustainable and culturally relevant platforms, ensuring a more inclusive and empowered digital society.   Keywords: Freedom of Expression, Mobile Services, Social Media Platforms, Local Digital Innovation, Uganda's Digital Ecosystem </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03086</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03086</id><created>2025-02-05</created><authors><author><keyname>Sinno</keyname><forenames>Salvatore</forenames></author><author><keyname>Bertl</keyname><forenames>Markus</forenames></author><author><keyname>Sahoo</keyname><forenames>Arati</forenames></author><author><keyname>Bhalgamiya</keyname><forenames>Bhavika</forenames></author><author><keyname>Groß</keyname><forenames>Thomas</forenames></author><author><keyname>Chancellor</keyname><forenames>Nicholas</forenames></author></authors><title>Implementing Large Quantum Boltzmann Machines as Generative AI Models   for Dataset Balancing</title><categories>cs.ET cs.AI cs.LG cs.NE quant-ph</categories><comments>accapted at IEEE International Conference on Next Generation   Information System Engineering</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This study explores the implementation of large Quantum Restricted Boltzmann Machines (QRBMs), a key advancement in Quantum Machine Learning (QML), as generative models on D-Wave's Pegasus quantum hardware to address dataset imbalance in Intrusion Detection Systems (IDS). By leveraging Pegasus's enhanced connectivity and computational capabilities, a QRBM with 120 visible and 120 hidden units was successfully embedded, surpassing the limitations of default embedding tools. The QRBM synthesized over 1.6 million attack samples, achieving a balanced dataset of over 4.2 million records. Comparative evaluations with traditional balancing methods, such as SMOTE and RandomOversampler, revealed that QRBMs produced higher-quality synthetic samples, significantly improving detection rates, precision, recall, and F1 score across diverse classifiers. The study underscores the scalability and efficiency of QRBMs, completing balancing tasks in milliseconds. These findings highlight the transformative potential of QML and QRBMs as next-generation tools in data preprocessing, offering robust solutions for complex computational challenges in modern information systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03090</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03090</id><created>2025-02-05</created><authors><author><keyname>Li</keyname><forenames>Jinglai</forenames></author><author><keyname>Wang</keyname><forenames>Hongqiao</forenames></author></authors><title>Gaussian Processes Regression for Uncertainty Quantification: An   Introductory Tutorial</title><categories>stat.CO cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian Process Regression (GPR) is a powerful method widely used in Uncertainty Quantification (UQ). This tutorial serves as an introductory guide for beginners, aiming to offer a structured and accessible overview of GPR's applications in UQ. We begin with an introduction to UQ and outline its key tasks, including uncertainty propagation, risk estimation, optimization under uncertainty, parameter estimation, and sensitivity analysis. We then introduce Gaussian Processes (GPs) as a surrogate modeling technique, detailing their formulation, choice of covariance kernels, hyperparameter estimation, and active learning strategies for efficient data acquisition. The tutorial further explores how GPR can be applied to different UQ tasks, including Bayesian quadrature for uncertainty propagation, active learning-based risk estimation, Bayesian optimization for optimization under uncertainty, and surrogate-based sensitivity analysis. Throughout, we emphasize how to leverage the unique formulation of GP for these UQ tasks, rather than simply using it as a standard surrogate model. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03091</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03091</id><created>2025-02-05</created><authors><author><keyname>Whyte</keyname><forenames>Travis</forenames></author><author><keyname>Stathopoulos</keyname><forenames>Andreas</forenames></author><author><keyname>Romero</keyname><forenames>Eloy</forenames></author></authors><title>Chiral rank-$k$ truncations for the multigrid preconditioner of Wilson   fermions in lattice QCD</title><categories>hep-lat cs.NA math.NA</categories><comments>10 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a modification to the setup algorithm for the multigrid preconditioner of Wilson fermions in lattice QCD. A larger number of test vectors than that used in conventional multigrid is generated by the smoother. This set of test vectors is then truncated by a singular value decomposition on the chiral components of the test vectors, which are subsequently used to form the prolongation and restriction matrices of the multigrid hierarchy. This modification is demonstrated to improve the convergence of linear equations on an anisotropic lattice with $m_{\pi} \approx 239$ MeV from the Hadron Spectrum Collaboration and an isotropic lattice with $m_{\pi} \approx 220$ MeV from the MILC Collaboration. The lattice volume dependence of the method is also examined. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03092</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03092</id><created>2025-02-05</created><authors><author><keyname>Zhou</keyname><forenames>Yuhao</forenames></author><author><keyname>Tian</keyname><forenames>Yuxin</forenames></author><author><keyname>Shi</keyname><forenames>Mingjia</forenames></author><author><keyname>Li</keyname><forenames>Yuanxi</forenames></author><author><keyname>Sun</keyname><forenames>Yanan</forenames></author><author><keyname>Ye</keyname><forenames>Qing</forenames></author><author><keyname>Lv</keyname><forenames>Jiancheng</forenames></author></authors><title>E-3SFC: Communication-Efficient Federated Learning with Double-way   Features Synthesizing</title><categories>cs.LG cs.AI cs.DC</categories><comments>Accepted by TNNLS. arXiv admin note: text overlap with   arXiv:2302.13562</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  The exponential growth in model sizes has significantly increased the communication burden in Federated Learning (FL). Existing methods to alleviate this burden by transmitting compressed gradients often face high compression errors, which slow down the model's convergence. To simultaneously achieve high compression effectiveness and lower compression errors, we study the gradient compression problem from a novel perspective. Specifically, we propose a systematical algorithm termed Extended Single-Step Synthetic Features Compressing (E-3SFC), which consists of three sub-components, i.e., the Single-Step Synthetic Features Compressor (3SFC), a double-way compression algorithm, and a communication budget scheduler. First, we regard the process of gradient computation of a model as decompressing gradients from corresponding inputs, while the inverse process is considered as compressing the gradients. Based on this, we introduce a novel gradient compression method termed 3SFC, which utilizes the model itself as a decompressor, leveraging training priors such as model weights and objective functions. 3SFC compresses raw gradients into tiny synthetic features in a single-step simulation, incorporating error feedback to minimize overall compression errors. To further reduce communication overhead, 3SFC is extended to E-3SFC, allowing double-way compression and dynamic communication budget scheduling. Our theoretical analysis under both strongly convex and non-convex conditions demonstrates that 3SFC achieves linear and sub-linear convergence rates with aggregation noise. Extensive experiments across six datasets and six models reveal that 3SFC outperforms state-of-the-art methods by up to 13.4% while reducing communication costs by 111.6 times. These findings suggest that 3SFC can significantly enhance communication efficiency in FL without compromising model performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03095</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03095</id><created>2025-02-05</created><authors><author><keyname>Su</keyname><forenames>Xuerui</forenames></author><author><keyname>Wang</keyname><forenames>Yue</forenames></author><author><keyname>Zhu</keyname><forenames>Jinhua</forenames></author><author><keyname>Yi</keyname><forenames>Mingyang</forenames></author><author><keyname>Xu</keyname><forenames>Feng</forenames></author><author><keyname>Ma</keyname><forenames>Zhiming</forenames></author><author><keyname>Liu</keyname><forenames>Yuting</forenames></author></authors><title>Reveal the Mystery of DPO: The Connection between DPO and RL Algorithms</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  With the rapid development of Large Language Models (LLMs), numerous Reinforcement Learning from Human Feedback (RLHF) algorithms have been introduced to improve model safety and alignment with human preferences. These algorithms can be divided into two main frameworks based on whether they require an explicit reward (or value) function for training: actor-critic-based Proximal Policy Optimization (PPO) and alignment-based Direct Preference Optimization (DPO). The mismatch between DPO and PPO, such as DPO's use of a classification loss driven by human-preferred data, has raised confusion about whether DPO should be classified as a Reinforcement Learning (RL) algorithm. To address these ambiguities, we focus on three key aspects related to DPO, RL, and other RLHF algorithms: (1) the construction of the loss function; (2) the target distribution at which the algorithm converges; (3) the impact of key components within the loss function. Specifically, we first establish a unified framework named UDRRA connecting these algorithms based on the construction of their loss functions. Next, we uncover their target policy distributions within this framework. Finally, we investigate the critical components of DPO to understand their impact on the convergence rate. Our work provides a deeper understanding of the relationship between DPO, RL, and other RLHF algorithms, offering new insights for improving existing algorithms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03100</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03100</id><created>2025-02-05</created><authors><author><keyname>Esslinger</keyname><forenames>J.</forenames></author><author><keyname>Weisse</keyname><forenames>N.</forenames></author><author><keyname>Eberle</keyname><forenames>C.</forenames></author><author><keyname>Schroeder</keyname><forenames>J.</forenames></author><author><keyname>Howard</keyname><forenames>S.</forenames></author><author><keyname>Norreys</keyname><forenames>P.</forenames></author><author><keyname>Karsch</keyname><forenames>S.</forenames></author><author><keyname>Döpp</keyname><forenames>A.</forenames></author></authors><title>A Bayesian perspective on single-shot laser characterization</title><categories>physics.optics cs.LG physics.ins-det</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a Bayesian framework for measuring spatio-temporal couplings (STCs) in ultra-intense lasers that reconceptualizes what constitutes a 'single-shot' measurement. Moving beyond traditional distinctions between single- and multi-shot devices, our approach provides rigorous criteria for determining when measurements can truly resolve individual laser shots rather than statistical averages. This framework shows that single-shot capability is not an intrinsic device property but emerges from the relationship between measurement precision and inherent parameter variability. Implementing this approach with a new measurement device at the ATLAS-3000 petawatt laser, we provide the first quantitative uncertainty bounds on pulse front tilt and curvature. Notably, we observe that our Bayesian method reduces uncertainty by up to 60% compared to traditional approaches. Through this analysis, we reveal how the interplay between measurement precision and intrinsic system variability defines achievable resolution -- insights that have direct implications for applications where precise control of laser-matter interaction is critical. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03102</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03102</id><created>2025-02-05</created><authors><author><keyname>Delena</keyname><forenames>Jonathan</forenames></author><author><keyname>Moreau</keyname><forenames>Augustin</forenames></author><author><keyname>Ravensdale</keyname><forenames>Dominic</forenames></author><author><keyname>Chatterton</keyname><forenames>Frederick</forenames></author></authors><title>Structured Token Retention and Computational Memory Paths in Large   Language Models</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Memory retention mechanisms play a central role in determining the efficiency of computational architectures designed for processing extended sequences. Conventional methods for token management often impose fixed retention thresholds or rely on uniform attention weight distributions, leading to inefficient memory utilization and premature information loss in extended sequence modeling. Structured Token Retention (STR) introduces a probabilistic selection framework that dynamically adjusts token persistence based on contextual significance, ensuring that computational resources are allocated to semantically relevant elements. Computational Memory Paths (CMP) extend this framework through hierarchical memory allocation, refining retention efficiency through structured reallocation of token embeddings. Comparative assessments against baseline models demonstrate that STR and CMP improve token survival rates across long input sequences while reducing cumulative error propagation across processing layers. Experimental results further indicate reductions in computational overhead, improving inference speed without degrading contextual coherence. Token distribution analyses reveal that structured memory allocation prevents excessive redundancy in attention weight calculations, optimizing information retrieval efficiency in large-scale generative architectures. The integration of STR and CMP into an open-source model illustrates the adaptability of structured memory retention methodologies, highlighting their applicability in generative text processing, long-context comprehension, and scalable sequence modeling. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03103</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03103</id><created>2025-02-05</created><authors><author><keyname>Roy</keyname><forenames>Santanu</forenames></author><author><keyname>Suresh</keyname><forenames>Ashvath</forenames></author><author><keyname>Gupta</keyname><forenames>Archit</forenames></author></authors><title>Edge Attention Module for Object Classification</title><categories>cs.CV cs.LG</categories><comments>11 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A novel ``edge attention-based Convolutional Neural Network (CNN)'' is proposed in this research for object classification task. With the advent of advanced computing technology, CNN models have achieved to remarkable success, particularly in computer vision applications. Nevertheless, the efficacy of the conventional CNN is often hindered due to class imbalance and inter-class similarity problems, which are particularly prominent in the computer vision field. In this research, we introduce for the first time an ``Edge Attention Module (EAM)'' consisting of a Max-Min pooling layer, followed by convolutional layers. This Max-Min pooling is entirely a novel pooling technique, specifically designed to capture only the edge information that is crucial for any object classification task. Therefore, by integrating this novel pooling technique into the attention module, the CNN network inherently prioritizes on essential edge features, thereby boosting the accuracy and F1-score of the model significantly. We have implemented our proposed EAM or 2EAMs on several standard pre-trained CNN models for Caltech-101, Caltech-256, CIFAR-100 and Tiny ImageNet-200 datasets. The extensive experiments reveal that our proposed framework (that is, EAM with CNN and 2EAMs with CNN), outperforms all pre-trained CNN models as well as recent trend models ``Pooling-based Vision Transformer (PiT)'', ``Convolutional Block Attention Module (CBAM)'', and ConvNext, by substantial margins. We have achieved the accuracy of 95.5% and 86% by the proposed framework on Caltech-101 and Caltech-256 datasets, respectively. So far, this is the best results on these datasets, to the best of our knowledge. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03104</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03104</id><created>2025-02-05</created><authors><author><keyname>Chen</keyname><forenames>Xingguo</forenames></author><author><keyname>Gong</keyname><forenames>Yu</forenames></author><author><keyname>Yang</keyname><forenames>Shangdong</forenames></author><author><keyname>Wang</keyname><forenames>Wenhao</forenames></author></authors><title>Bellman Error Centering</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper revisits the recently proposed reward centering algorithms including simple reward centering (SRC) and value-based reward centering (VRC), and points out that SRC is indeed the reward centering, while VRC is essentially Bellman error centering (BEC). Based on BEC, we provide the centered fixpoint for tabular value functions, as well as the centered TD fixpoint for linear value function approximation. We design the on-policy CTD algorithm and the off-policy CTDC algorithm, and prove the convergence of both algorithms. Finally, we experimentally validate the stability of our proposed algorithms. Bellman error centering facilitates the extension to various reinforcement learning algorithms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03105</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03105</id><created>2025-02-05</created><authors><author><keyname>Kupavskii</keyname><forenames>Andrey</forenames></author><author><keyname>Popova</keyname><forenames>Elizaveta</forenames></author></authors><title>Satisfying sequences for rainbow partite matchings</title><categories>math.CO cs.DM</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Let $\mathcal F_1,\ldots, \mathcal F_s\subset [n]^k$ be a collection of $s$ families. In this paper, we address the following question: for which sequences $f_1,\ldots, f_s$ the conditions $|\ff_i|&gt;f_i$ imply that the families contain a rainbow matching, that is, there are pairwise disjoint $F_1\in \ff_1,\ldots F_s\in \ff_s$? We call such sequences {\em satisfying}. Kiselev and the first author verified the conjecture of Aharoni and Howard and showed that $f_1 = \ldots = f_s=(s-1)n^{k-1}$ is satisfying for $s&gt;470$. This is the best possible if the restriction is uniform over all families. However, it turns out that much more can be said about asymmetric restrictions. In this paper, we investigate this question in several regimes and in particular answer the questions asked by Kiselev and Kupavskii. We use a variety of methods, including concentration and anticoncentration results, spread approximations, and Combinatorial Nullstellenzats. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03108</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03108</id><created>2025-02-05</created><authors><author><keyname>Hartmann</keyname><forenames>Maria</forenames></author><author><keyname>Danoy</keyname><forenames>Grégoire</forenames></author><author><keyname>Bouvry</keyname><forenames>Pascal</forenames></author></authors><title>Multi-objective methods in Federated Learning: A survey and taxonomy</title><categories>cs.LG cs.DC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Federated Learning paradigm facilitates effective distributed machine learning in settings where training data is decentralized across multiple clients. As the popularity of the strategy grows, increasingly complex real-world problems emerge, many of which require balancing conflicting demands such as fairness, utility, and resource consumption. Recent works have begun to recognise the use of a multi-objective perspective in answer to this challenge. However, this novel approach of combining federated methods with multi-objective optimisation has never been discussed in the broader context of both fields. In this work, we offer a first clear and systematic overview of the different ways the two fields can be integrated. We propose a first taxonomy on the use of multi-objective methods in connection with Federated Learning, providing a targeted survey of the state-of-the-art and proposing unambiguous labels to categorise contributions. Given the developing nature of this field, our taxonomy is designed to provide a solid basis for further research, capturing existing works while anticipating future additions. Finally, we outline open challenges and possible directions for further research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03111</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03111</id><created>2025-02-05</created><authors><author><keyname>Schneider</keyname><forenames>Felix</forenames><affiliation>Zoom Communications</affiliation></author><author><keyname>Turchi</keyname><forenames>Marco</forenames><affiliation>Zoom Communications</affiliation></author><author><keyname>Waibel</keyname><forenames>Alex</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author></authors><title>Policies and Evaluation for Online Meeting Summarization</title><categories>cs.CL cs.AI cs.LG</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With more and more meetings moving to a digital domain, meeting summarization has recently gained interest in both academic and commercial research. However, prior academic research focuses on meeting summarization as an offline task, performed after the meeting concludes. In this paper, we perform the first systematic study of online meeting summarization. For this purpose, we propose several policies for conducting online summarization. We discuss the unique challenges of this task compared to the offline setting and define novel metrics to evaluate latency and partial summary quality. The experiments on the AutoMin dataset show that 1) online models can produce strong summaries, 2) our metrics allow a detailed analysis of different systems' quality-latency trade-off, also taking into account intermediate outputs and 3) adaptive policies perform better than fixed scheduled ones. These findings provide a starting point for the wider research community to explore this important task. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03113</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03113</id><created>2025-02-05</created><authors><author><keyname>Lavie</keyname><forenames>Gilad</forenames></author><author><keyname>Tamir</keyname><forenames>Tami</forenames></author></authors><title>Coordination Mechanisms with Rank-Based Utilities</title><categories>cs.GT</categories><comments>A preliminary version appers in GameNets 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In classical job-scheduling games, each job behaves as a selfish player, choosing a machine to minimize its own completion time. To reduce the equilibria inefficiency, coordination mechanisms~\cite{CKN04} are employed, allowing each machine to follow its own scheduling policy. In this paper we study the effects of incorporating {\em rank-based utilities} within coordination mechanisms across environments with either identical or unrelated machines.   With rank-based utilities, players aim to perform well {\em relative to their competitors}, rather than solely minimizing their completion time. We first demonstrate that even in basic setups, such as two identical machines with unit-length jobs, a pure Nash equilibrium (NE) assignment may not exist. This observation motivates our inquiry into the complexity of determining whether a given game instance admits a NE. We prove that this problem is NP-complete, even in highly restricted cases. In contrast, we identify specific classes of games where a NE is guaranteed to exist, or where the decision problem can be resolved in polynomial time.   Additionally, we examine how competition impacts the efficiency of Nash equilibria, or sink equilibria if a NE does not exist. We derive tight bounds on the price of anarchy, and show that competition may either enhance or degrade overall performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03115</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03115</id><created>2025-02-05</created><authors><author><keyname>Pourya</keyname><forenames>Mehrsa</forenames></author><author><keyname>Nogarotto</keyname><forenames>Maïka</forenames></author><author><keyname>Unser</keyname><forenames>Michael</forenames></author></authors><title>Comparison of 2D Regular Lattices for the CPWL Approximation of   Functions</title><categories>math.NA cs.NA eess.SP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We investigate the approximation error of functions with continuous and piecewise-linear (CPWL) representations. We focus on the CPWL search spaces generated by translates of box splines on two-dimensional regular lattices. We compute the approximation error in terms of the stepsize and angles that define the lattice. Our results show that hexagonal lattices are optimal, in the sense that they minimize the asymptotic approximation error. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03117</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03117</id><created>2025-02-05</created><authors><author><keyname>Cha</keyname><forenames>Jihoon</forenames></author><author><keyname>Kim</keyname><forenames>Hwanjin</forenames></author><author><keyname>Choi</keyname><forenames>Junil</forenames></author></authors><title>Meta-Learning-Based People Counting and Localization Models Employing   CSI from Commodity WiFi NICs</title><categories>cs.IT eess.SP math.IT</categories><comments>13 pages, 15 figures, submitted to IEEE Internet of Things Journal   (IoTJ)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider people counting and localization systems exploiting channel state information (CSI) measured from commodity WiFi network interface cards (NICs). While CSI has useful information of amplitude and phase to describe signal propagation in a measurement environment of interest, CSI measurement suffers from offsets due to various uncertainties. Moreover, an uncontrollable external environment where other WiFi devices communicate each other induces interfering signals, resulting in erroneous CSI captured at a receiver. In this paper, preprocessing of CSI is first proposed for offset removal, and it guarantees low-latency operation without any filtering process. Afterwards, we design people counting and localization models based on pre-training. To be adaptive to different measurement environments, meta-learning-based people counting and localization models are also proposed. Numerical results show that the proposed meta-learning-based people counting and localization models can achieve high sensing accuracy, compared to other learning schemes that follow simple training and test procedures. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03118</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03118</id><created>2025-02-05</created><authors><author><keyname>Yan</keyname><forenames>Wen</forenames></author><author><keyname>Yang</keyname><forenames>Qianye</forenames></author><author><keyname>Huang</keyname><forenames>Shiqi</forenames></author><author><keyname>Wang</keyname><forenames>Yipei</forenames></author><author><keyname>Punwani</keyname><forenames>Shonit</forenames></author><author><keyname>Emberton</keyname><forenames>Mark</forenames></author><author><keyname>Stavrinides</keyname><forenames>Vasilis</forenames></author><author><keyname>Hu</keyname><forenames>Yipeng</forenames></author><author><keyname>Barratt</keyname><forenames>Dean</forenames></author></authors><title>Tell2Reg: Establishing spatial correspondence between images by the same   language prompts</title><categories>cs.CV cs.AI eess.IV</categories><comments>5 pages, 3 figures, conference paper</comments><msc-class>00B25</msc-class><acm-class>I.2.7</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Spatial correspondence can be represented by pairs of segmented regions, such that the image registration networks aim to segment corresponding regions rather than predicting displacement fields or transformation parameters. In this work, we show that such a corresponding region pair can be predicted by the same language prompt on two different images using the pre-trained large multimodal models based on GroundingDINO and SAM. This enables a fully automated and training-free registration algorithm, potentially generalisable to a wide range of image registration tasks. In this paper, we present experimental results using one of the challenging tasks, registering inter-subject prostate MR images, which involves both highly variable intensity and morphology between patients. Tell2Reg is training-free, eliminating the need for costly and time-consuming data curation and labelling that was previously required for this registration task. This approach outperforms unsupervised learning-based registration methods tested, and has a performance comparable to weakly-supervised methods. Additional qualitative results are also presented to suggest that, for the first time, there is a potential correlation between language semantics and spatial correspondence, including the spatial invariance in language-prompted regions and the difference in language prompts between the obtained local and global correspondences. Code is available at https://github.com/yanwenCi/Tell2Reg.git. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03119</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03119</id><created>2025-02-05</created><authors><author><keyname>Graf</keyname><forenames>Ricarda</forenames></author><author><keyname>Todd</keyname><forenames>Susan</forenames></author><author><keyname>Baksh</keyname><forenames>M. Fazil</forenames></author></authors><title>Comparison of the Cox proportional hazards model and Random Survival   Forest algorithm for predicting patient-specific survival probabilities in   clinical trial data</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Cox proportional hazards model is often used for model development in data from randomized controlled trials (RCT) with time-to-event outcomes. Random survival forests (RSF) is a machine-learning algorithm known for its high predictive performance. We conduct a comprehensive neutral comparison study to compare the predictive performance of Cox regression and RSF in real-world as well as simulated data. Performance is compared using multiple performance measures according to recommendations for the comparison of prognostic prediction models. We found that while the RSF usually outperforms the Cox model when using the $C$ index, Cox model predictions may be better calibrated. With respect to overall performance, the Cox model often exceeds the RSF in nonproportional hazards settings, while otherwise the RSF typically performs better especially for smaller sample sizes. Overall performance of the RSF is more affected by higher censoring rates, while overall performance of the Cox model suffers more from smaller sample sizes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03120</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03120</id><created>2025-02-05</created><authors><author><keyname>Pratap</keyname><forenames>Abhinav</forenames></author></authors><title>At the Mahakumbh, Faith Met Tragedy: Computational Analysis of Stampede   Patterns Using Machine Learning and NLP</title><categories>cs.LG cs.AI cs.CY cs.SI</categories><comments>6 pages, 4 figures, 3 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This study employs machine learning, historical analysis, and natural language processing (NLP) to examine recurring lethal stampedes at Indias mass religious gatherings, focusing on the 2025 Mahakumbh tragedy in Prayagraj (48+ deaths) and its 1954 predecessor (700+ casualties). Through computational modeling of crowd dynamics and administrative records, it investigates how systemic vulnerabilities contribute to these disasters. Temporal trend analysis identifies persistent choke points, with narrow riverbank access routes linked to 92% of past stampede sites and lethal crowd densities (eight or more persons per square meter) recurring during spiritually significant moments like Mauni Amavasya. NLP analysis of seven decades of inquiry reports reveals cyclical administrative failures, where VIP route prioritization diverted safety resources in both 1954 and 2025, exacerbating fatalities. Statistical modeling demonstrates how ritual urgency overrides risk perception, leading to panic propagation patterns that mirror historical incidents. Findings support the Institutional Amnesia Theory, highlighting how disaster responses remain reactionary rather than preventive. By correlating archival patterns with computational crowd behavior analysis, this study frames stampedes as a collision of infrastructure limitations, socio spiritual urgency, and governance inertia, challenging disaster discourse to address how spiritual economies normalize preventable mortality. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03122</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03122</id><created>2025-02-05</created><authors><author><keyname>Zhang</keyname><forenames>Qiyuan</forenames></author><author><keyname>Weng</keyname><forenames>Chenfan</forenames></author><author><keyname>Li</keyname><forenames>Guanwu</forenames></author><author><keyname>He</keyname><forenames>Fulai</forenames></author><author><keyname>Cai</keyname><forenames>Yusheng</forenames></author></authors><title>HiLo: Learning Whole-Body Human-like Locomotion with Motion Tracking   Controller</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Reinforcement Learning (RL) has emerged as a promising method to develop humanoid robot locomotion controllers. Despite the robust and stable locomotion demonstrated by previous RL controllers, their behavior often lacks the natural and agile motion patterns necessary for human-centric scenarios. In this work, we propose HiLo (human-like locomotion with motion tracking), an effective framework designed to learn RL policies that perform human-like locomotion. The primary challenges of human-like locomotion are complex reward engineering and domain randomization. HiLo overcomes these issues by developing an RL-based motion tracking controller and simple domain randomization through random force injection and action delay. Within the framework of HiLo, the whole-body control problem can be decomposed into two components: One part is solved using an open-loop control method, while the residual part is addressed with RL policies. A distributional value function is also implemented to stabilize the training process by improving the estimation of cumulative rewards under perturbed dynamics. Our experiments demonstrate that the motion tracking controller trained using HiLo can perform natural and agile human-like locomotion while exhibiting resilience to external disturbances in real-world systems. Furthermore, we show that the motion patterns of humanoid robots can be adapted through the residual mechanism without fine-tuning, allowing quick adjustments to task requirements. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03123</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03123</id><created>2025-02-05</created><authors><author><keyname>Zhang</keyname><forenames>Xingshen</forenames></author><author><keyname>Liu</keyname><forenames>Shuangrong</forenames></author><author><keyname>Lu</keyname><forenames>Xintao</forenames></author><author><keyname>Pang</keyname><forenames>Chaoran</forenames></author><author><keyname>Wang</keyname><forenames>Lin</forenames></author><author><keyname>Yang</keyname><forenames>Bo</forenames></author></authors><title>Disentanglement in Difference: Directly Learning Semantically   Disentangled Representations by Maximizing Inter-Factor Differences</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, Disentanglement in Difference(DiD) is proposed to address the inherent inconsistency between the statistical independence of latent variables and the goal of semantic disentanglement in disentanglement representation learning. Conventional disentanglement methods achieve disentanglement representation by improving statistical independence among latent variables. However, the statistical independence of latent variables does not necessarily imply that they are semantically unrelated, thus, improving statistical independence does not always enhance disentanglement performance. To address the above issue, DiD is proposed to directly learn semantic differences rather than the statistical independence of latent variables. In the DiD, a Difference Encoder is designed to measure the semantic differences; a contrastive loss function is established to facilitate inter-dimensional comparison. Both of them allow the model to directly differentiate and disentangle distinct semantic factors, thereby resolving the inconsistency between statistical independence and semantic disentanglement. Experimental results on the dSprites and 3DShapes datasets demonstrate that the proposed DiD outperforms existing mainstream methods across various disentanglement metrics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03124</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03124</id><created>2025-02-05</created><authors><author><keyname>Thrän</keyname><forenames>Jacob</forenames></author><author><keyname>Green</keyname><forenames>Tim C.</forenames></author><author><keyname>Shorten</keyname><forenames>Robert</forenames></author></authors><title>Levelised Cost of Demand Response: Estimating the Cost-Competitiveness   of Flexible Demand</title><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To make well-informed investment decisions, energy system stakeholders require reliable cost frameworks for demand response (DR) and storage technologies. While the levelised cost of storage (LCOS) permits comprehensive cost comparisons between different storage technologies, no generic cost measure for the comparison of different DR schemes exists. This paper introduces the levelised cost of demand response (LCODR) which is an analogous measure to the LCOS but crucially differs from it by considering consumer reward payments. Additionally, the value factor from cost estimations of variable renewable energy is adapted to account for the variable availability of DR. The LCODRs for four direct load control (DLC) schemes and twelve storage applications are estimated and contrasted against LCOS literature values for the most competitive storage technologies. The DLC schemes are vehicle-to-grid, smart charging, smart heat pumps, and heat pumps with thermal storage. The results show that only heat pumps with thermal storage consistently outcompete storage technologies with EV-based DR schemes being competitive for some applications. The results and the underlying methodology offer a tool for energy system stakeholders to assess the competitiveness of DR schemes even with limited user data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03125</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03125</id><created>2025-02-05</created><authors><author><keyname>Zhou</keyname><forenames>Yang</forenames></author><author><keyname>Wang</keyname><forenames>Siying</forenames></author><author><keyname>Chen</keyname><forenames>Wenyu</forenames></author><author><keyname>Zhang</keyname><forenames>Ruoning</forenames></author><author><keyname>Zhao</keyname><forenames>Zhitong</forenames></author><author><keyname>Zhang</keyname><forenames>Zixuan</forenames></author></authors><title>Double Distillation Network for Multi-Agent Reinforcement Learning</title><categories>cs.MA cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-agent reinforcement learning typically employs a centralized training-decentralized execution (CTDE) framework to alleviate the non-stationarity in environment. However, the partial observability during execution may lead to cumulative gap errors gathered by agents, impairing the training of effective collaborative policies. To overcome this challenge, we introduce the Double Distillation Network (DDN), which incorporates two distillation modules aimed at enhancing robust coordination and facilitating the collaboration process under constrained information. The external distillation module uses a global guiding network and a local policy network, employing distillation to reconcile the gap between global training and local execution. In addition, the internal distillation module introduces intrinsic rewards, drawn from state information, to enhance the exploration capabilities of agents. Extensive experiments demonstrate that DDN significantly improves performance across multiple scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03127</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03127</id><created>2025-02-05</created><authors><author><keyname>Konala</keyname><forenames>Pandu Ranga Reddy</forenames></author><author><keyname>Kumar</keyname><forenames>Vimal</forenames></author><author><keyname>Bainbridge</keyname><forenames>David</forenames></author><author><keyname>Haseeb</keyname><forenames>Junaid</forenames></author></authors><title>A Framework for Measuring the Quality of Infrastructure-as-Code Scripts</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Infrastructure as Code (IaC) has become integral to modern software development, enabling automated and consistent configuration of computing environments. The rapid proliferation of IaC scripts has highlighted the need for better code quality assessment methods. This paper proposes a new IaC code quality framework specifically showcased for Ansible repositories as a foundation. By analyzing a comprehensive dataset of repositories from Ansible Galaxy, we applied our framework to evaluate code quality across multiple attributes. The analysis of our code quality metrics applied to Ansible Galaxy repositories reveal trends over time indicating improvements in areas such as metadata and error handling, while highlighting declines in others such as sophistication and automation. The framework offers practitioners a systematic tool for assessing and enhancing IaC scripts, fostering standardization and facilitating continuous improvement. It also provides a standardized foundation for further work into IaC code quality. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03128</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03128</id><created>2025-02-05</created><authors><author><keyname>Wang</keyname><forenames>Yuancheng</forenames></author><author><keyname>Zheng</keyname><forenames>Jiachen</forenames></author><author><keyname>Zhang</keyname><forenames>Junan</forenames></author><author><keyname>Zhang</keyname><forenames>Xueyao</forenames></author><author><keyname>Liao</keyname><forenames>Huan</forenames></author><author><keyname>Wu</keyname><forenames>Zhizheng</forenames></author></authors><title>Metis: A Foundation Speech Generation Model with Masked Generative   Pre-training</title><categories>cs.SD cs.AI cs.LG eess.AS eess.SP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Metis, a foundation model for unified speech generation. Unlike previous task-specific or multi-task models, Metis follows a pre-training and fine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data using masked generative modeling and then fine-tuned to adapt to diverse speech generation tasks. Specifically, 1) Metis utilizes two discrete speech representations: SSL tokens derived from speech self-supervised learning (SSL) features, and acoustic tokens directly quantized from waveforms. 2) Metis performs masked generative pre-training on SSL tokens, utilizing 300K hours of diverse speech data, without any additional condition. 3) Through fine-tuning with task-specific conditions, Metis achieves efficient adaptation to various speech generation tasks while supporting multimodal input, even when using limited data and trainable parameters. Experiments demonstrate that Metis can serve as a foundation model for unified speech generation: Metis outperforms state-of-the-art task-specific or multi-task systems across five speech generation tasks, including zero-shot text-to-speech, voice conversion, target speaker extraction, speech enhancement, and lip-to-speech, even with fewer than 20M trainable parameters or 300 times less training data. Audio samples are are available at https://metis-demo.github.io/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03129</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03129</id><created>2025-02-05</created><authors><author><keyname>Qian</keyname><forenames>Zhen</forenames></author><author><keyname>Zhang</keyname><forenames>Xiuzhen</forenames></author><author><keyname>Xu</keyname><forenames>Xiaofei</forenames></author><author><keyname>Xia</keyname><forenames>Feng</forenames></author></authors><title>Teaching Large Language Models Number-Focused Headline Generation With   Key Element Rationales</title><categories>cs.CL cs.LG</categories><comments>Pre-print for a paper accepted to findings of NAACL 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Number-focused headline generation is a summarization task requiring both high textual quality and precise numerical accuracy, which poses a unique challenge for Large Language Models (LLMs). Existing studies in the literature focus only on either textual quality or numerical reasoning and thus are inadequate to address this challenge. In this paper, we propose a novel chain-of-thought framework for using rationales comprising key elements of the Topic, Entities, and Numerical reasoning (TEN) in news articles to enhance the capability for LLMs to generate topic-aligned high-quality texts with precise numerical accuracy. Specifically, a teacher LLM is employed to generate TEN rationales as supervision data, which are then used to teach and fine-tune a student LLM. Our approach teaches the student LLM automatic generation of rationales with enhanced capability for numerical reasoning and topic-aligned numerical headline generation. Experiments show that our approach achieves superior performance in both textual quality and numerical accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03130</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03130</id><created>2025-02-05</created><authors><author><keyname>Mohammed</keyname><forenames>Alnawar J.</forenames></author><author><keyname>Ali</keyname><forenames>Qutaiba I.</forenames></author></authors><title>Solar Synergy: Innovative Strategies for Data Centers Energy Efficiency   and Sustainability</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  One of the current trends related to data centers is providing it with renewable energy sources. This paper suggests an analysis technique for a model uses solar panels energy to power a data center consists of 100 traditional servers, physical infrastructures, 5 backup batteries. The analysis passes through three phases: Initially, the power consumption model of the data center is proposed to show the variation in traffic and total energy consumed. Then, a solar system model is designed according to the power needed. At the last phase, the desired battery capacity is chosen as (10000Ah-48V) to accommodate the solar energy production. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03132</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03132</id><created>2025-02-05</created><authors><author><keyname>Sun</keyname><forenames>Yifan</forenames></author><author><keyname>Chen</keyname><forenames>Rui</forenames></author><author><keyname>Yun</keyname><forenames>Kai S.</forenames></author><author><keyname>Fang</keyname><forenames>Yikuan</forenames></author><author><keyname>Jung</keyname><forenames>Sebin</forenames></author><author><keyname>Li</keyname><forenames>Feihan</forenames></author><author><keyname>Li</keyname><forenames>Bowei</forenames></author><author><keyname>Zhao</keyname><forenames>Weiye</forenames></author><author><keyname>Liu</keyname><forenames>Changliu</forenames></author></authors><title>SPARK: A Modular Benchmark for Humanoid Robot Safety</title><categories>cs.RO cs.SY eess.SY</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper introduces the Safe Protective and Assistive Robot Kit (SPARK), a comprehensive benchmark designed to ensure safety in humanoid autonomy and teleoperation. Humanoid robots pose significant safety risks due to their physical capabilities of interacting with complex environments. The physical structures of humanoid robots further add complexity to the design of general safety solutions. To facilitate the safe deployment of complex robot systems, SPARK can be used as a toolbox that comes with state-of-the-art safe control algorithms in a modular and composable robot control framework. Users can easily configure safety criteria and sensitivity levels to optimize the balance between safety and performance. To accelerate humanoid safety research and development, SPARK provides a simulation benchmark that compares safety approaches in a variety of environments, tasks, and robot models. Furthermore, SPARK allows quick deployment of synthesized safe controllers on real robots. For hardware deployment, SPARK supports Apple Vision Pro (AVP) or a Motion Capture System as external sensors, while also offering interfaces for seamless integration with alternative hardware setups. This paper demonstrates SPARK's capability with both simulation experiments and case studies with a Unitree G1 humanoid robot. Leveraging these advantages of SPARK, users and researchers can significantly improve the safety of their humanoid systems as well as accelerate relevant research. The open-source code is available at https://github.com/intelligent-control-lab/spark. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03134</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03134</id><created>2025-02-05</created><authors><author><keyname>Belarbi</keyname><forenames>Othmane</forenames></author><author><keyname>Spyridopoulos</keyname><forenames>Theodoros</forenames></author><author><keyname>Anthi</keyname><forenames>Eirini</forenames></author><author><keyname>Rana</keyname><forenames>Omer</forenames></author><author><keyname>Carnelli</keyname><forenames>Pietro</forenames></author><author><keyname>Khan</keyname><forenames>Aftab</forenames></author></authors><title>Gotham Dataset 2025: A Reproducible Large-Scale IoT Network Dataset for   Intrusion Detection and Security Research</title><categories>cs.CR cs.AI</categories><comments>16 pages, 7 figures, 4 tables. Submitted at the Data in Brief journal</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, a dataset of IoT network traffic is presented. Our dataset was generated by utilising the Gotham testbed, an emulated large-scale Internet of Things (IoT) network designed to provide a realistic and heterogeneous environment for network security research. The testbed includes 78 emulated IoT devices operating on various protocols, including MQTT, CoAP, and RTSP. Network traffic was captured in Packet Capture (PCAP) format using tcpdump, and both benign and malicious traffic were recorded. Malicious traffic was generated through scripted attacks, covering a variety of attack types, such as Denial of Service (DoS), Telnet Brute Force, Network Scanning, CoAP Amplification, and various stages of Command and Control (C&amp;C) communication. The data were subsequently processed in Python for feature extraction using the Tshark tool, and the resulting data was converted to Comma Separated Values (CSV) format and labelled. The data repository includes the raw network traffic in PCAP format and the processed labelled data in CSV format. Our dataset was collected in a distributed manner, where network traffic was captured separately for each IoT device at the interface between the IoT gateway and the device. Our dataset was collected in a distributed manner, where network traffic was separately captured for each IoT device at the interface between the IoT gateway and the device. With its diverse traffic patterns and attack scenarios, this dataset provides a valuable resource for developing Intrusion Detection Systems and security mechanisms tailored to complex, large-scale IoT environments. The dataset is publicly available at Zenodo. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03135</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03135</id><created>2025-02-05</created><authors><author><keyname>Hamamatsu</keyname><forenames>Yuya</forenames></author><author><keyname>Kupyn</keyname><forenames>Pavlo</forenames></author><author><keyname>Gkliva</keyname><forenames>Roza</forenames></author><author><keyname>Ristolainen</keyname><forenames>Asko</forenames></author><author><keyname>Kruusmaa</keyname><forenames>Maarja</forenames></author></authors><title>Underwater Soft Fin Flapping Motion with Deep Neural Network Based   Surrogate Model</title><categories>cs.RO cs.LG</categories><comments>Accepted in IEEE International Conference on Soft Robotics 2025   (Robosoft)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study presents a novel framework for precise force control of fin-actuated underwater robots by integrating a deep neural network (DNN)-based surrogate model with reinforcement learning (RL). To address the complex interactions with the underwater environment and the high experimental costs, a DNN surrogate model acts as a simulator for enabling efficient training for the RL agent. Additionally, grid-switching control is applied to select optimized models for specific force reference ranges, improving control accuracy and stability. Experimental results show that the RL agent, trained in the surrogate simulation, generates complex thrust motions and achieves precise control of a real soft fin actuator. This approach provides an efficient control solution for fin-actuated robots in challenging underwater environments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03139</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03139</id><created>2025-02-05</created><authors><author><keyname>Savchenko</keyname><forenames>Oleg</forenames></author><author><keyname>Abellán</keyname><forenames>Guillermo Franco</forenames></author><author><keyname>List</keyname><forenames>Florian</forenames></author><author><keyname>Montel</keyname><forenames>Noemi Anau</forenames></author><author><keyname>Weniger</keyname><forenames>Christoph</forenames></author></authors><title>Fast Sampling of Cosmological Initial Conditions with Gaussian Neural   Posterior Estimation</title><categories>astro-ph.CO astro-ph.IM cs.LG</categories><comments>9 + 2 pages, 7 figures, 1 table. Comments welcome!</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Knowledge of the primordial matter density field from which the large-scale structure of the Universe emerged over cosmic time is of fundamental importance for cosmology. However, reconstructing these cosmological initial conditions from late-time observations is a notoriously difficult task, which requires advanced cosmological simulators and sophisticated statistical methods to explore a multi-million-dimensional parameter space. We show how simulation-based inference (SBI) can be used to tackle this problem and to obtain data-constrained realisations of the primordial dark matter density field in a simulation-efficient way with general non-differentiable simulators. Our method is applicable to full high-resolution dark matter $N$-body simulations and is based on modelling the posterior distribution of the constrained initial conditions to be Gaussian with a diagonal covariance matrix in Fourier space. As a result, we can generate thousands of posterior samples within seconds on a single GPU, orders of magnitude faster than existing methods, paving the way for sequential SBI for cosmological fields. Furthermore, we perform an analytical fit of the estimated dependence of the covariance on the wavenumber, effectively transforming any point-estimator of initial conditions into a fast sampler. We test the validity of our obtained samples by comparing them to the true values with summary statistics and performing a Bayesian consistency test. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03143</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03143</id><created>2025-02-05</created><authors><author><keyname>Chen</keyname><forenames>Yawen</forenames></author><author><keyname>Sun</keyname><forenames>Jiande</forenames></author><author><keyname>Wang</keyname><forenames>Jinhui</forenames></author><author><keyname>Zhao</keyname><forenames>Liang</forenames></author><author><keyname>Song</keyname><forenames>Xinmin</forenames></author><author><keyname>Zhai</keyname><forenames>Linbo</forenames></author></authors><title>Machine Learning-Driven Student Performance Prediction for Enhancing   Tiered Instruction</title><categories>cs.LG cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Student performance prediction is one of the most important subjects in educational data mining. As a modern technology, machine learning offers powerful capabilities in feature extraction and data modeling, providing essential support for diverse application scenarios, as evidenced by recent studies confirming its effectiveness in educational data mining. However, despite extensive prediction experiments, machine learning methods have not been effectively integrated into practical teaching strategies, hindering their application in modern education. In addition, massive features as input variables for machine learning algorithms often leads to information redundancy, which can negatively impact prediction accuracy. Therefore, how to effectively use machine learning methods to predict student performance and integrate the prediction results with actual teaching scenarios is a worthy research subject. To this end, this study integrates the results of machine learning-based student performance prediction with tiered instruction, aiming to enhance student outcomes in target course, which is significant for the application of educational data mining in contemporary teaching scenarios. Specifically, we collect original educational data and perform feature selection to reduce information redundancy. Then, the performance of five representative machine learning methods is analyzed and discussed with Random Forest showing the best performance. Furthermore, based on the results of the classification of students, tiered instruction is applied accordingly, and different teaching objectives and contents are set for all levels of students. The comparison of teaching outcomes between the control and experimental classes, along with the analysis of questionnaire results, demonstrates the effectiveness of the proposed framework. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03144</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03144</id><created>2025-02-05</created><authors><author><keyname>Ali</keyname><forenames>Dildar</forenames></author><author><keyname>Banerjee</keyname><forenames>Suman</forenames></author><author><keyname>Prasad</keyname><forenames>Yamuna</forenames></author></authors><title>Group Trip Planning Query Problem with Multimodal Journey</title><categories>cs.MA cs.DB cs.DS</categories><comments>11 Pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In Group Trip Planning (GTP) Query Problem, we are given a city road network where a number of Points of Interest (PoI) have been marked with their respective categories (e.g., Cafeteria, Park, Movie Theater, etc.). A group of agents want to visit one PoI from every category from their respective starting location and once finished, they want to reach their respective destinations. This problem asks which PoI from every category should be chosen so that the aggregated travel cost of the group is minimized. This problem has been studied extensively in the last decade, and several solution approaches have been proposed. However, to the best of our knowledge, none of the existing studies have considered the different modalities of the journey, which makes the problem more practical. To bridge this gap, we introduce and study the GTP Query Problem with Multimodal Journey in this paper. Along with the other inputs of the GTP Query Problem, we are also given the different modalities of the journey that are available and their respective cost. Now, the problem is not only to select the PoIs from respective categories but also to select the modality of the journey. For this problem, we have proposed an efficient solution approach, which has been analyzed to understand their time and space requirements. A large number of experiments have been conducted using real-life datasets and the results have been reported. From the results, we observe that the PoIs and modality of journey recommended by the proposed solution approach lead to much less time and cost than the baseline methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03146</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03146</id><created>2025-02-05</created><authors><author><keyname>Ruple</keyname><forenames>Laura</forenames></author><author><keyname>Torresi</keyname><forenames>Luca</forenames></author><author><keyname>Schopmans</keyname><forenames>Henrik</forenames></author><author><keyname>Friederich</keyname><forenames>Pascal</forenames></author></authors><title>Symmetry-Aware Bayesian Flow Networks for Crystal Generation</title><categories>cs.LG cond-mat.mtrl-sci</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The discovery of new crystalline materials is essential to scientific and technological progress. However, traditional trial-and-error approaches are inefficient due to the vast search space. Recent advancements in machine learning have enabled generative models to predict new stable materials by incorporating structural symmetries and to condition the generation on desired properties. In this work, we introduce SymmBFN, a novel symmetry-aware Bayesian Flow Network (BFN) for crystalline material generation that accurately reproduces the distribution of space groups found in experimentally observed crystals. SymmBFN substantially improves efficiency, generating stable structures at least 50 times faster than the next-best method. Furthermore, we demonstrate its capability for property-conditioned generation, enabling the design of materials with tailored properties. Our findings establish BFNs as an effective tool for accelerating the discovery of crystalline materials. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03147</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03147</id><created>2025-02-05</created><authors><author><keyname>Wen</keyname><forenames>Xumeng</forenames></author><author><keyname>Zheng</keyname><forenames>Shun</forenames></author><author><keyname>Xu</keyname><forenames>Zhen</forenames></author><author><keyname>Sun</keyname><forenames>Yiming</forenames></author><author><keyname>Bian</keyname><forenames>Jiang</forenames></author></authors><title>Scalable In-Context Learning on Tabular Data via Retrieval-Augmented   Large Language Models</title><categories>cs.CL cs.AI</categories><comments>Preprint</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent studies have shown that large language models (LLMs), when customized with post-training on tabular data, can acquire general tabular in-context learning (TabICL) capabilities. These models are able to transfer effectively across diverse data schemas and different task domains. However, existing LLM-based TabICL approaches are constrained to few-shot scenarios due to the sequence length limitations of LLMs, as tabular instances represented in plain text consume substantial tokens. To address this limitation and enable scalable TabICL for any data size, we propose retrieval-augmented LLMs tailored to tabular data. Our approach incorporates a customized retrieval module, combined with retrieval-guided instruction-tuning for LLMs. This enables LLMs to effectively leverage larger datasets, achieving significantly improved performance across 69 widely recognized datasets and demonstrating promising scaling behavior. Extensive comparisons with state-of-the-art tabular models reveal that, while LLM-based TabICL still lags behind well-tuned numeric models in overall performance, it uncovers powerful algorithms under limited contexts, enhances ensemble diversity, and excels on specific datasets. These unique properties underscore the potential of language as a universal and accessible interface for scalable tabular data learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03148</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03148</id><created>2025-02-05</created><authors><author><keyname>de Nobel</keyname><forenames>Jacob</forenames></author><author><keyname>Vermetten</keyname><forenames>Diederick</forenames></author><author><keyname>Wang</keyname><forenames>Hao</forenames></author><author><keyname>Kononova</keyname><forenames>Anna V.</forenames></author><author><keyname>Rudolph</keyname><forenames>Günter</forenames></author><author><keyname>Bäck</keyname><forenames>Thomas</forenames></author></authors><title>Abnormal Mutations: Evolution Strategies Don't Require Gaussianity</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mutation process in evolution strategies has been interlinked with the normal distribution since its inception. Many lines of reasoning have been given for this strong dependency, ranging from maximum entropy arguments to the need for isotropy. However, some theoretical results suggest that other distributions might lead to similar local convergence properties. This paper empirically shows that a wide range of evolutionary strategies, from the (1+1)-ES to CMA-ES, show comparable optimization performance when using a mutation distribution other than the standard Gaussian. Replacing it with, e.g., uniformly distributed mutations, does not deteriorate the performance of ES, when using the default adaptation mechanism for the strategy parameters. We observe that these results hold not only for the sphere model but also for a wider range of benchmark problems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03149</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03149</id><created>2025-02-05</created><authors><author><keyname>Saxena</keyname><forenames>Deepika</forenames></author><author><keyname>Swain</keyname><forenames>Smruti Rekha</forenames></author><author><keyname>Kumar</keyname><forenames>Jatinder</forenames></author><author><keyname>Patni</keyname><forenames>Sakshi</forenames></author><author><keyname>Gupta</keyname><forenames>Kishu</forenames></author><author><keyname>Singh</keyname><forenames>Ashutosh Kumar</forenames></author><author><keyname>Lindenstruth</keyname><forenames>Volker</forenames></author></authors><title>Secure Resource Management in Cloud Computing: Challenges, Strategies   and Meta-Analysis</title><categories>cs.CR</categories><comments>16 Pages, 12 Figures, 6 Tables, in IEEE Transactions on Systems, Man,   and Cybernetics: Systems, 2025</comments><doi>10.1109/TSMC.2025.3525956</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Secure resource management (SRM) within a cloud computing environment is a critical yet infrequently studied research topic. This paper provides a comprehensive survey and comparative performance evaluation of potential cyber threat countermeasure strategies that address security challenges during cloud workload execution and resource management. Cybersecurity is explored specifically in the context of cloud resource management, with an emphasis on identifying the associated challenges. The cyber threat countermeasure methods are categorized into three classes: defensive strategies, mitigating strategies, and hybrid strategies. The existing countermeasure strategies belonging to each class are thoroughly discussed and compared. In addition to conceptual and theoretical analysis, the leading countermeasure strategies within these categories are implemented on a common platform and examined using two real-world virtual machine (VM) data traces. Based on this comprehensive study and performance evaluation, the paper discusses the trade-offs among these countermeasure strategies and their utility, providing imperative concluding remarks on the holistic study of cloud cyber threat countermeasures and secure resource management. Furthermore, the study suggests future methodologies that could effectively address the emerging challenges of secure cloud resource management. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03150</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03150</id><created>2025-02-05</created><authors><author><keyname>Shpilka</keyname><forenames>Amir</forenames></author></authors><title>Improved Debordering of Waring Rank</title><categories>cs.CC math.AG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that if a degree-$d$ homogeneous polynomial $f$ has border Waring rank $\underline{\mathrm{WR}}({f}) = r$, then its Waring rank is bounded by \[ {\mathrm{WR}}({f}) \leq d \cdot r^{O(\sqrt{r})}. \] This result significantly improves upon the recent bound ${\mathrm{WR}}({f}) \leq d \cdot 4^r$ established in [Dutta, Gesmundo, Ikenmeyer, Jindal, and Lysikov, STACS 2024], which itself was an improvement over the earlier bound ${\mathrm{WR}}({f}) \leq d^r$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03153</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03153</id><created>2025-02-05</created><authors><author><keyname>Biazotto</keyname><forenames>João Paulo</forenames></author><author><keyname>Feitosa</keyname><forenames>Daniel</forenames></author><author><keyname>Avgeriou</keyname><forenames>Paris</forenames></author><author><keyname>Nakagawa</keyname><forenames>Elisa Yumi</forenames></author></authors><title>Automating Technical Debt Management: Insights from Practitioner   Discussions in Stack Exchange</title><categories>cs.SE</categories><comments>Accepted for publication on the International Conference on Technical   Debt 2025 (TechDebt'25)</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Managing technical debt (TD) is essential for maintaining long-term software projects. Nonetheless, the time and cost involved in technical debt management (TDM) are often high, which may lead practitioners to omit TDM tasks. The adoption of tools, and particularly the usage of automated solutions, can potentially reduce the time, cost, and effort involved. However, the adoption of tools remains low, indicating the need for further research on TDM automation. To address this problem, this study aims at understanding which TDM activities practitioners are discussing with respect to automation in TDM, what tools they report for automating TDM, and the challenges they face that require automation solutions. To this end, we conducted a mining software repositories (MSR) study on three websites of Stack Exchange (Stack Overflow, Project Management, and Software Engineering) and collected 216 discussions, which were analyzed using both thematic synthesis and descriptive statistics. We found that identification and measurement are the most cited activities. Furthermore, 51 tools were reported as potential alternatives for TDM automation. Finally, a set of nine main challenges were identified and clustered into two main categories: challenges driving TDM automation and challenges related to tool usage. These findings highlight that tools for automating TDM are being discussed and used; however, several significant barriers persist, such as tool errors and poor explainability, hindering the adoption of these tools. Moreover, further research is needed to investigate the automation of other TDM activities such as TD prioritization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03157</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03157</id><created>2025-02-05</created><authors><author><keyname>Hou</keyname><forenames>Yongli</forenames></author><author><keyname>Liu</keyname><forenames>Yi</forenames></author><author><keyname>Wang</keyname><forenames>Yanqiu</forenames></author></authors><title>A boundary-corrected weak Galerkin mixed finite method for elliptic   interface problems with curved interfaces</title><categories>math.NA cs.NA</categories><comments>23 pages, 9 figures</comments><msc-class>65N12, 65N22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a boundary-corrected weak Galerkin mixed finite element method for solving elliptic interface problems in 2D domains with curved interfaces. The method is formulated on body-fitted polygonal meshes, where interface edges are straight and may not align exactly with the curved physical interface. To address this discrepancy, a boundary value correction technique is employed to transfer the interface conditions from the physical interface to the approximate interface using a Taylor expansion approach. The Neumann interface condition is then weakly imposed in the variational formulation. This approach eliminates the need for numerical integration on curved elements, thereby reducing implementation complexity. We establish optimal-order convergence in the energy norm for arbitrary-order discretizations. Numerical results are provided to support the theoretical findings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03159</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03159</id><created>2025-02-05</created><authors><author><keyname>Wu</keyname><forenames>Yuchao</forenames></author><author><keyname>Yu</keyname><forenames>Xiaofei</forenames></author><author><keyname>Chen</keyname><forenames>Hao</forenames></author><author><keyname>Luo</keyname><forenames>Yang</forenames></author><author><keyname>Tong</keyname><forenames>Yeyu</forenames></author><author><keyname>Ma</keyname><forenames>Yuzhe</forenames></author></authors><title>PICBench: Benchmarking LLMs for Photonic Integrated Circuits Design</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While large language models (LLMs) have shown remarkable potential in automating various tasks in digital chip design, the field of Photonic Integrated Circuits (PICs)-a promising solution to advanced chip designs-remains relatively unexplored in this context. The design of PICs is time-consuming and prone to errors due to the extensive and repetitive nature of code involved in photonic chip design. In this paper, we introduce PICBench, the first benchmarking and evaluation framework specifically designed to automate PIC design generation using LLMs, where the generated output takes the form of a netlist. Our benchmark consists of dozens of meticulously crafted PIC design problems, spanning from fundamental device designs to more complex circuit-level designs. It automatically evaluates both the syntax and functionality of generated PIC designs by comparing simulation outputs with expert-written solutions, leveraging an open-source simulator. We evaluate a range of existing LLMs, while also conducting comparative tests on various prompt engineering techniques to enhance LLM performance in automated PIC design. The results reveal the challenges and potential of LLMs in the PIC design domain, offering insights into the key areas that require further research and development to optimize automation in this field. Our benchmark and evaluation code is available at https://github.com/PICDA/PICBench. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03160</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03160</id><created>2025-02-05</created><authors><author><keyname>Tan</keyname><forenames>Boyin</forenames></author><author><keyname>Xu</keyname><forenames>Junjielong</forenames></author><author><keyname>Zhu</keyname><forenames>Zhouruixing</forenames></author><author><keyname>He</keyname><forenames>Pinjia</forenames></author></authors><title>AL-Bench: A Benchmark for Automatic Logging</title><categories>cs.SE</categories><comments>submitted to TOSEM</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logging, the practice of inserting log statements into source code, is critical for improving software reliability. Recently, language model-based techniques have been developed to automate log generation based on input code. Although these methods demonstrate promising results in isolated evaluations, their effectiveness diminishes when applied to ad-hoc low-quality data and code similarity-based evaluation methods. We consider a comprehensive evaluation benchmark should include (1) a high-quality, diverse, and large-scale dataset, (2) an assessment of the compilability of the code with inserted log statements, and (3) a runtime log-oriented evaluation method. To this end, this paper introduces AL-Bench, a comprehensive benchmark designed specifically for automatic logging tools. AL-Bench includes a high-quality, diverse dataset collected from 10 widely recognized projects with varying logging requirements and introduces a novel dynamic evaluation approach. Different from the evaluation in existing logging papers, AL-Bench assesses both the compilability of the code with inserted log statements and the quality of the logs generated by them during runtime, which we believe can better reflect the effectiveness of logging techniques in practice. AL-Bench reveals significant limitations in the state-of-the-art tools. The codes with log statements generated by the state-of-the-art tools fail to compile in 20.1%-83.6% cases. In addition, even the best-performing tool did not achieve high similarity between the runtime logs produced by the generated log statements and the ground-truth log statements, demonstrating a 0.213 cosine similarity. The results reveal substantial opportunities to further enhance the development of automatic logging tools. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03162</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03162</id><created>2025-02-05</created><authors><author><keyname>Fang</keyname><forenames>Tianyu</forenames></author><author><keyname>Nguyen</keyname><forenames>Nhan Thanh</forenames></author><author><keyname>Juntti</keyname><forenames>Markku</forenames></author></authors><title>Low-Complexity Cram\'er-Rao Lower Bound and Sum Rate Optimization in   ISAC Systems</title><categories>cs.IT eess.SP math.IT</categories><comments>5 pages, 3figures. Accepted by ICASSP 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While Cram\'er-Rao lower bound is an important metric in sensing functions in integrated sensing and communications (ISAC) designs, its optimization usually involves a computationally expensive solution such as semidefinite relaxation. In this paper, we aim to develop a low-complexity yet efficient algorithm for CRLB optimization. We focus on a beamforming design that maximizes the weighted sum between the communications sum rate and the sensing CRLB, subject to a transmit power constraint. Given the non-convexity of this problem, we propose a novel method that combines successive convex approximation (SCA) with a shifted generalized power iteration (SGPI) approach, termed SCA-SGPI. The SCA technique is utilized to approximate the non-convex objective function with convex surrogates, while the SGPI efficiently solves the resulting quadratic subproblems. Simulation results demonstrate that the proposed SCA-SGPI algorithm not only achieves superior tradeoff performance compared to existing method but also significantly reduces computational time, making it a promising solution for practical ISAC applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03163</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03163</id><created>2025-02-05</created><authors><author><keyname>Glückstad</keyname><forenames>Mie</forenames></author><author><keyname>Cirone</keyname><forenames>Nicola Muca</forenames></author><author><keyname>Teichmann</keyname><forenames>Josef</forenames></author></authors><title>Signature Reconstruction from Randomized Signatures</title><categories>math.CA cs.LG math.PR stat.ML</categories><comments>37 pages, 7 figures</comments><msc-class>60L10 (Primary) 60L70, 60L90, 68T07 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Controlled ordinary differential equations driven by continuous bounded variation curves can be considered a continuous time analogue of recurrent neural networks for the construction of expressive features of the input curves. We ask up to which extent well known signature features of such curves can be reconstructed from controlled ordinary differential equations with (untrained) random vector fields. The answer turns out to be algebraically involved, but essentially the number of signature features, which can be reconstructed from the non-linear flow of the controlled ordinary differential equation, is exponential in its hidden dimension, when the vector fields are chosen to be neural with depth two. Moreover, we characterize a general linear independence condition on arbitrary vector fields, under which the signature features up to some fixed order can always be reconstructed. Algebraically speaking this complements in a quantitative manner several well known results from the theory of Lie algebras of vector fields and puts them in a context of machine learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03167</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03167</id><created>2025-02-05</created><authors><author><keyname>Roy</keyname><forenames>Shrish</forenames></author><author><keyname>Ulmann</keyname><forenames>Bernd</forenames></author></authors><title>Experiments with an oscillator based Ising machine</title><categories>cs.ET nlin.AO</categories><msc-class>68</msc-class><acm-class>B.m; C.3; F.1.2</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Interest in non-algorithmic, unconventional computing is rising in recent years due to more and more apparent short comings of classic stored-program digital computers, such as energy efficiency, degree of parallelism in computations, clock frequency limitations, integration density, silicon utilization, etc. One notable such unconventional approach are oscillator based Ising machines, i.e., systems consisting of a number of oscillators which can be coupled in order to create an analogue for some problem to be solved, while the actual information is encoded in the phase relationships of these oscillators with respect to some reference (typically one of these oscillators). It has been shown that machines of this type are capable of solving NP-hard problems such as max-cut, etc. In the following an experimental Ising machine is presented together with experimental results obtained from this machine. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03177</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03177</id><created>2025-02-05</created><authors><author><keyname>Goldberg</keyname><forenames>Emmanuel</forenames></author><author><keyname>Brodt</keyname><forenames>Oleg</forenames></author><author><keyname>Elyashar</keyname><forenames>Aviad</forenames></author><author><keyname>Puzis</keyname><forenames>Rami</forenames></author></authors><title>LED there be DoS: Exploiting variable bitrate IP cameras for network DoS</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Variable-bitrate video streaming is ubiquitous in video surveillance and CCTV, enabling high-quality video streaming while conserving network bandwidth. However, as the name suggests, variable-bitrate IP cameras can generate sharp traffic spikes depending on the dynamics of the visual input. In this paper, we show that the effectiveness of video compression can be reduced by up to 6X using a simple laser LED pointing at a variable-bitrate IP camera, forcing the camera to generate excessive network traffic. Experiments with IP cameras connected to wired and wireless networks indicate that a laser attack on a single camera can cause significant packet loss in systems sharing the network with the camera and reduce the available bandwidth of a shared network link by 90%. This attack represents a new class of cyber-physical attacks that manipulate variable bitrate devices through changes in the physical environment without a digital presence on the device or the network. We also analyze the broader view of multidimensional cyberattacks that involve both the physical and digital realms and present a taxonomy that categorizes attacks based on their direction of influence (physical-to-digital or digital-to-physical) and their method of operation (environment-driven or device-driven), highlighting multiple areas for future research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03183</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03183</id><created>2025-02-05</created><authors><author><keyname>Li</keyname><forenames>Pengyi</forenames></author><author><keyname>Abdullaeva</keyname><forenames>Irina</forenames></author><author><keyname>Gambashidze</keyname><forenames>Alexander</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Andrey</forenames></author><author><keyname>Oseledets</keyname><forenames>Ivan</forenames></author></authors><title>MaxInfo: A Training-Free Key-Frame Selection Method Using Maximum Volume   for Enhanced Video Understanding</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern Video Large Language Models (VLLMs) often rely on uniform frame sampling for video understanding, but this approach frequently fails to capture critical information due to frame redundancy and variations in video content. We propose MaxInfo, a training-free method based on the maximum volume principle, which selects and retains the most representative frames from the input video. By maximizing the geometric volume formed by selected embeddings, MaxInfo ensures that the chosen frames cover the most informative regions of the embedding space, effectively reducing redundancy while preserving diversity. This method enhances the quality of input representations and improves long video comprehension performance across benchmarks. For instance, MaxInfo achieves a 3.28% improvement on LongVideoBench and a 6.4% improvement on EgoSchema for LLaVA-Video-7B. It also achieves a 3.47% improvement for LLaVA-Video-72B. The approach is simple to implement and works with existing VLLMs without the need for additional training, making it a practical and effective alternative to traditional uniform sampling methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03188</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03188</id><created>2025-02-05</created><authors><author><keyname>Heredia</keyname><forenames>Maite</forenames></author><author><keyname>Barnes</keyname><forenames>Jeremy</forenames></author><author><keyname>Soroa</keyname><forenames>Aitor</forenames></author></authors><title>Euska\~nolDS: A Naturally Sourced Corpus for Basque-Spanish   Code-Switching</title><categories>cs.CL cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Code-switching (CS) remains a significant challenge in Natural Language Processing (NLP), mainly due a lack of relevant data. In the context of the contact between the Basque and Spanish languages in the north of the Iberian Peninsula, CS frequently occurs in both formal and informal spontaneous interactions. However, resources to analyse this phenomenon and support the development and evaluation of models capable of understanding and generating code-switched language for this language pair are almost non-existent. We introduce a first approach to develop a naturally sourced corpus for Basque-Spanish code-switching. Our methodology consists of identifying CS texts from previously available corpora using language identification models, which are then manually validated to obtain a reliable subset of CS instances. We present the properties of our corpus and make it available under the name Euska\~nolDS. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03197</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03197</id><created>2025-02-05</created><authors><author><keyname>Schlotter</keyname><forenames>Ildikó</forenames></author><author><keyname>Cechlárová</keyname><forenames>Katarína</forenames></author></authors><title>Candidate nomination for Condorcet-consistent voting rules</title><categories>cs.GT</categories><comments>Accepted at AAMAS 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Consider elections where the set of candidates is partitioned into parties, and each party must nominate exactly one candidate. The Possible President problem asks whether some candidate of a given party can become the winner of the election for some nominations from other parties. We perform a multivariate computational complexity analysis of Possible President for a range of Condorcet-consistent voting rules, namely for Copeland$^\alpha$ for $\alpha \in [0,1]$ and Maximin. The parameters we study are the number of voters, the number of parties, and the maximum size of a party. For all voting rules under consideration, we obtain dichotomies based on the number of voters, classifying $\mathsf{NP}$-complete and polynomial-time solvable cases. Moreover, for each $\mathsf{NP}$-complete variant, we determine the parameterized complexity of every possible parameterization with the studied parameters as either (a) fixed-parameter tractable, (b) $\mathsf{W}[1]$-hard but in $\mathsf{XP}$, or (c) $\mathsf{paraNP}$-hard, outlining the limits of tractability for these problems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03198</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03198</id><created>2025-02-05</created><authors><author><keyname>Zhang</keyname><forenames>Yimu</forenames></author><author><keyname>Han</keyname><forenames>Dongqi</forenames></author><author><keyname>Wang</keyname><forenames>Yansen</forenames></author><author><keyname>Gu</keyname><forenames>Yu</forenames></author><author><keyname>Li</keyname><forenames>Dongsheng</forenames></author></authors><title>SimSort: A Powerful Framework for Spike Sorting by Large-Scale   Electrophysiology Simulation</title><categories>q-bio.NC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spike sorting is an essential process in neural recording, which identifies and separates electrical signals from individual neurons recorded by electrodes in the brain, enabling researchers to study how specific neurons communicate and process information. Although there exist a number of spike sorting methods which have contributed to significant neuroscientific breakthroughs, many are heuristically designed, making it challenging to verify their correctness due to the difficulty of obtaining ground truth labels from real-world neural recordings. In this work, we explore a data-driven, deep learning-based approach. We begin by creating a large-scale dataset through electrophysiology simulations using biologically realistic computational models. We then present \textbf{SimSort}, a pretraining framework for spike sorting. Remarkably, when trained on our simulated dataset, SimSort demonstrates strong zero-shot generalization to real-world spike sorting tasks, significantly outperforming existing methods. Our findings underscore the potential of data-driven techniques to enhance the reliability and scalability of spike sorting in experimental neuroscience. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03199</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03199</id><created>2025-02-05</created><authors><author><keyname>Wu</keyname><forenames>Jialiang</forenames></author><author><keyname>Shen</keyname><forenames>Yi</forenames></author><author><keyname>Liu</keyname><forenames>Sijia</forenames></author><author><keyname>Tang</keyname><forenames>Yi</forenames></author><author><keyname>Song</keyname><forenames>Sen</forenames></author><author><keyname>Wang</keyname><forenames>Xiaoyi</forenames></author><author><keyname>Cai</keyname><forenames>Longjun</forenames></author></authors><title>Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large   Language Models</title><categories>cs.CL cs.AI</categories><comments>NAACL 2025 Findings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite their impressive capacities, Large language models (LLMs) often struggle with the hallucination issue of generating inaccurate or fabricated content even when they possess correct knowledge. In this paper, we extend the exploration of the correlation between hidden-state prediction changes and output factuality into a deeper, token-wise level. Based on the insights , we propose cross-layer Entropy eNhanced Decoding (END), a decoding method that mitigates hallucinations without requiring extra training. END leverages inner probability changes across layers to individually quantify the factual knowledge required for each candidate token, and adjusts the final predicting distribution to prioritize tokens with higher factuality. Experiments on both hallucination and QA benchmarks demonstrate that END significantly enhances the truthfulness and informativeness of generated content while maintaining robust QA accuracy. Moreover, our work provides a deeper perspective on understanding the correlations between inherent knowledge and output factuality. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03200</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03200</id><created>2025-02-05</created><authors><author><keyname>Kopanja</keyname><forenames>Marija</forenames></author><author><keyname>Savić</keyname><forenames>Miloš</forenames></author><author><keyname>Longo</keyname><forenames>Luca</forenames></author></authors><title>CORTEX: A Cost-Sensitive Rule and Tree Extraction Method</title><categories>cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Tree-based and rule-based machine learning models play pivotal roles in explainable artificial intelligence (XAI) due to their unique ability to provide explanations in the form of tree or rule sets that are easily understandable and interpretable, making them essential for applications in which trust in model decisions is necessary. These transparent models are typically used in surrogate modeling, a post-hoc XAI approach for explaining the logic of black-box models, enabling users to comprehend and trust complex predictive systems while maintaining competitive performance. This study proposes the Cost-Sensitive Rule and Tree Extraction (CORTEX) method, a novel rule-based XAI algorithm grounded in the multi-class cost-sensitive decision tree (CSDT) method. The original version of the CSDT is extended to classification problems with more than two classes by inducing the concept of an n-dimensional class-dependent cost matrix. The performance of CORTEX as a rule-extractor XAI method is compared to other post-hoc tree and rule extraction methods across several datasets with different numbers of classes. Several quantitative evaluation metrics are employed to assess the explainability of generated rule sets. Our findings demonstrate that CORTEX is competitive with other tree-based methods and can be superior to other rule-based methods across different datasets. The extracted rule sets suggest the advantages of using the CORTEX method over other methods by producing smaller rule sets with shorter rules on average across datasets with a diverse number of classes. Overall, the results underscore the potential of CORTEX as a powerful XAI tool for scenarios that require the generation of clear, human-understandable rules while maintaining good predictive performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03201</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03201</id><created>2025-02-05</created><authors><author><keyname>Dong</keyname><forenames>Xiangyu</forenames></author><author><keyname>Zhang</keyname><forenames>Xingyi</forenames></author><author><keyname>Chen</keyname><forenames>Lei</forenames></author><author><keyname>Yuan</keyname><forenames>Mingxuan</forenames></author><author><keyname>Wang</keyname><forenames>Sibo</forenames></author></authors><title>SpaceGNN: Multi-Space Graph Neural Network for Node Anomaly Detection   with Extremely Limited Labels</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Node Anomaly Detection (NAD) has gained significant attention in the deep learning community due to its diverse applications in real-world scenarios. Existing NAD methods primarily embed graphs within a single Euclidean space, while overlooking the potential of non-Euclidean spaces. Besides, to address the prevalent issue of limited supervision in real NAD tasks, previous methods tend to leverage synthetic data to collect auxiliary information, which is not an effective solution as shown in our experiments. To overcome these challenges, we introduce a novel SpaceGNN model designed for NAD tasks with extremely limited labels. Specifically, we provide deeper insights into a task-relevant framework by empirically analyzing the benefits of different spaces for node representations, based on which, we design a Learnable Space Projection function that effectively encodes nodes into suitable spaces. Besides, we introduce the concept of weighted homogeneity, which we empirically and theoretically validate as an effective coefficient during information propagation. This concept inspires the design of the Distance Aware Propagation module. Furthermore, we propose the Multiple Space Ensemble module, which extracts comprehensive information for NAD under conditions of extremely limited supervision. Our findings indicate that this module is more beneficial than data augmentation techniques for NAD. Extensive experiments conducted on 9 real datasets confirm the superiority of SpaceGNN, which outperforms the best rival by an average of 8.55% in AUC and 4.31% in F1 scores. Our code is available at https://github.com/xydong127/SpaceGNN. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03202</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03202</id><created>2025-02-05</created><authors><author><keyname>Mohn</keyname><forenames>Fabian</forenames></author><author><keyname>Thieben</keyname><forenames>Florian</forenames></author><author><keyname>Knopp</keyname><forenames>Tobias</forenames></author></authors><title>Low-cost analog signal chain for transmit-receive circuits of passive   induction-based resonators</title><categories>eess.SP cs.SY eess.SY</categories><comments>Data available at https://doi.org/10.5281/zenodo.14764827</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Passive wireless sensors are crucial in modern medical and industrial settings to monitor procedures and conditions. We demonstrate a circuit to inductively excite passive resonators and to conduct their decaying signal response to a low noise amplifier. Two design variations of a generic transmit-receive signal chain are proposed, measured, and described in detail for the purpose of facilitating replication. Instrumentation and design aim to be scalable for multi-channel array configurations, using either off-the-shelf class-D audio amplifiers or a custom full H-bridge. Measurements are conducted on miniature magneto-mechanical resonators in the ultra low frequency range to enable sensing and tracking applications of such devices in different environments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03203</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03203</id><created>2025-02-05</created><authors><author><keyname>Blanco</keyname><forenames>Roberto</forenames></author><author><keyname>Ducruet</keyname><forenames>Léon</forenames></author><author><keyname>Harwig</keyname><forenames>Sebastian</forenames></author><author><keyname>Hritcu</keyname><forenames>Catalin</forenames></author></authors><title>FSLH: Flexible Mechanized Speculative Load Hardening</title><categories>cs.CR cs.PL</categories><comments>CSF'25 submission</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Spectre speculative side-channel attacks pose formidable threats for computer system security. Research has shown that cryptographic constant-time code can be efficiently protected against Spectre v1 using a selective variant of Speculative Load Hardening (SLH). SLH was, however, not strong enough for protecting non-cryptographic code, leading to the introduction of Ultimate SLH, which provides protection for arbitrary programs, but has too large overhead for general use, since it conservatively assumes that all data is secret. In this paper we introduce a flexible SLH notion that achieves the best of both worlds by formally generalizing both Selective and Ultimate SLH. We give a suitable security definition for such transformations protecting arbitrary programs: any transformed program running with speculation should not leak more than what the source program leaks sequentially. We formally prove using the Rocq prover that two flexible SLH variants enforce this relative security guarantee. As easy corollaries we also obtain that Ultimate SLH enforces our relative security notion, and also that the selective variants of value SLH and address SLH enforce speculative constant-time security. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03204</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03204</id><created>2025-02-05</created><authors><author><keyname>Balicki</keyname><forenames>Linus</forenames></author><author><keyname>Gugercin</keyname><forenames>Serkan</forenames></author></authors><title>Multivariate Rational Approximation via Low-Rank Tensors and the p-AAA   Algorithm</title><categories>math.NA cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximations based on rational functions are widely used in various applications across computational science and engineering. For univariate functions, the adaptive Antoulas-Anderson algorithm (AAA), which uses the barycentric form of a rational approximant, has established itself as a powerful tool for efficiently computing such approximations. The p-AAA algorithm, an extension of the AAA algorithm specifically designed to address multivariate approximation problems, has been recently introduced. A common challenge in multivariate approximation methods is that multivariate problems with a large number of variables often pose significant memory and computational demands. To tackle this hurdle in the setting of p-AAA, we first introduce barycentric forms that are represented in the terms of separable functions. This then leads to the low-rank p-AAA algorithm which leverages low-rank tensor decompositions in the setting of barycentric rational approximations. We discuss various theoretical and practical aspects of the proposed computational framework and showcase its effectiveness on four numerical examples. We focus specifically on applications in parametric reduced-order modeling for which higher-dimensional data sets can be tackled effectively with our novel procedure. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03205</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03205</id><created>2025-02-05</created><authors><author><keyname>Neufeld</keyname><forenames>Ariel</forenames></author><author><keyname>Nguyen</keyname><forenames>Tuan Anh</forenames></author><author><keyname>Schmocker</keyname><forenames>Philipp</forenames></author></authors><title>Multilevel Picard approximations for McKean-Vlasov stochastic   differential equations with nonconstant diffusion</title><categories>math.NA cs.NA math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce multilevel Picard (MLP) approximations for McKean-Vlasov stochastic differential equations (SDEs) with nonconstant diffusion coefficient. Under standard Lipschitz assumptions on the coefficients, we show that the MLP algorithm approximates the solution of the SDE in the $L^2$-sense without the curse of dimensionality. The latter means that its computational cost grows at most polynomially in both the dimension and the reciprocal of the prescribed error tolerance. In two numerical experiments, we demonstrate its applicability by approximating McKean-Vlasov SDEs in dimensions up to 10000. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03206</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03206</id><created>2025-02-05</created><authors><author><keyname>Xue</keyname><forenames>Yufei</forenames></author><author><keyname>Dong</keyname><forenames>Wentao</forenames></author><author><keyname>Liu</keyname><forenames>Minghuan</forenames></author><author><keyname>Zhang</keyname><forenames>Weinan</forenames></author><author><keyname>Pang</keyname><forenames>Jiangmiao</forenames></author></authors><title>A Unified and General Humanoid Whole-Body Controller for Fine-Grained   Locomotion</title><categories>cs.RO cs.AI</categories><comments>The first two authors contribute equally. Project page:   https://hugwbc.github.io/</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Locomotion is a fundamental skill for humanoid robots. However, most existing works made locomotion a single, tedious, unextendable, and passive movement. This limits the kinematic capabilities of humanoid robots. In contrast, humans possess versatile athletic abilities-running, jumping, hopping, and finely adjusting walking parameters such as frequency, and foot height. In this paper, we investigate solutions to bring such versatility into humanoid locomotion and thereby propose HUGWBC: a unified and general humanoid whole-body controller for fine-grained locomotion. By designing a general command space in the aspect of tasks and behaviors, along with advanced techniques like symmetrical loss and intervention training for learning a whole-body humanoid controlling policy in simulation, HugWBC enables real-world humanoid robots to produce various natural gaits, including walking (running), jumping, standing, and hopping, with customizable parameters such as frequency, foot swing height, further combined with different body height, waist rotation, and body pitch, all in one single policy. Beyond locomotion, HUGWBC also supports real-time interventions from external upper-body controllers like teleoperation, enabling loco-manipulation while maintaining precise control under any locomotive behavior. Our experiments validate the high tracking accuracy and robustness of HUGWBC with/without upper-body intervention for all commands, and we further provide an in-depth analysis of how the various commands affect humanoid movement and offer insights into the relationships between these commands. To our knowledge, HugWBC is the first humanoid whole-body controller that supports such fine-grained locomotion behaviors with high robustness and flexibility. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03207</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03207</id><created>2025-02-05</created><authors><author><keyname>Liao</keyname><forenames>Xinyao</forenames></author><author><keyname>Zeng</keyname><forenames>Xianfang</forenames></author><author><keyname>Wang</keyname><forenames>Liao</forenames></author><author><keyname>Yu</keyname><forenames>Gang</forenames></author><author><keyname>Lin</keyname><forenames>Guosheng</forenames></author><author><keyname>Zhang</keyname><forenames>Chi</forenames></author></authors><title>MotionAgent: Fine-grained Controllable Video Generation via Motion Field   Agent</title><categories>cs.CV cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose MotionAgent, enabling fine-grained motion control for text-guided image-to-video generation. The key technique is the motion field agent that converts motion information in text prompts into explicit motion fields, providing flexible and precise motion guidance. Specifically, the agent extracts the object movement and camera motion described in the text and converts them into object trajectories and camera extrinsics, respectively. An analytical optical flow composition module integrates these motion representations in 3D space and projects them into a unified optical flow. An optical flow adapter takes the flow to control the base image-to-video diffusion model for generating fine-grained controlled videos. The significant improvement in the Video-Text Camera Motion metrics on VBench indicates that our method achieves precise control over camera motion. We construct a subset of VBench to evaluate the alignment of motion information in the text and the generated video, outperforming other advanced models on motion generation accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03210</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03210</id><created>2025-02-05</created><authors><author><keyname>Rubin</keyname><forenames>Noa</forenames></author><author><keyname>Fischer</keyname><forenames>Kirsten</forenames></author><author><keyname>Lindner</keyname><forenames>Javed</forenames></author><author><keyname>Dahmen</keyname><forenames>David</forenames></author><author><keyname>Seroussi</keyname><forenames>Inbar</forenames></author><author><keyname>Ringel</keyname><forenames>Zohar</forenames></author><author><keyname>Krämer</keyname><forenames>Michael</forenames></author><author><keyname>Helias</keyname><forenames>Moritz</forenames></author></authors><title>From Kernels to Features: A Multi-Scale Adaptive Theory of Feature   Learning</title><categories>cond-mat.dis-nn cs.LG stat.ML</categories><comments>24 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Theoretically describing feature learning in neural networks is crucial for understanding their expressive power and inductive biases, motivating various approaches. Some approaches describe network behavior after training through a simple change in kernel scale from initialization, resulting in a generalization power comparable to a Gaussian process. Conversely, in other approaches training results in the adaptation of the kernel to the data, involving complex directional changes to the kernel. While these approaches capture different facets of network behavior, their relationship and respective strengths across scaling regimes remains an open question. This work presents a theoretical framework of multi-scale adaptive feature learning bridging these approaches. Using methods from statistical mechanics, we derive analytical expressions for network output statistics which are valid across scaling regimes and in the continuum between them. A systematic expansion of the network's probability distribution reveals that mean-field scaling requires only a saddle-point approximation, while standard scaling necessitates additional correction terms. Remarkably, we find across regimes that kernel adaptation can be reduced to an effective kernel rescaling when predicting the mean network output of a linear network. However, even in this case, the multi-scale adaptive approach captures directional feature learning effects, providing richer insights than what could be recovered from a rescaling of the kernel alone. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03212</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03212</id><created>2025-02-05</created><authors><author><keyname>Poncelet</keyname><forenames>Jakob</forenames></author><author><keyname>Van hamme</keyname><forenames>Hugo</forenames></author></authors><title>Leveraging Broadcast Media Subtitle Transcripts for Automatic Speech   Recognition and Subtitling</title><categories>eess.AS cs.SD</categories><comments>Preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent advancement of speech recognition technology has been driven by large-scale datasets and attention-based architectures, but many challenges still remain, especially for low-resource languages and dialects. This paper explores the integration of weakly supervised transcripts from TV subtitles into automatic speech recognition (ASR) systems, aiming to improve both verbatim transcriptions and automatically generated subtitles. To this end, verbatim data and subtitles are regarded as different domains or languages, due to their distinct characteristics. We propose and compare several end-to-end architectures that are designed to jointly model both modalities with separate or shared encoders and decoders. The proposed methods are able to jointly generate a verbatim transcription and a subtitle. Evaluation on Flemish (Belgian Dutch) demonstrates that a model with cascaded encoders and separate decoders allows to represent the differences between the two data types most efficiently while improving on both domains. Despite differences in domain and linguistic variations, combining verbatim transcripts with subtitle data leads to notable ASR improvements without the need for extensive preprocessing. Additionally, experiments with a large-scale subtitle dataset show the scalability of the proposed approach. The methods not only improve ASR accuracy but also generate subtitles that closely match standard written text, offering several potential applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03214</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03214</id><created>2025-02-05</created><authors><author><keyname>Mayer</keyname><forenames>Julius</forenames></author><author><keyname>Ballout</keyname><forenames>Mohamad</forenames></author><author><keyname>Jassim</keyname><forenames>Serwan</forenames></author><author><keyname>Nezami</keyname><forenames>Farbod Nosrat</forenames></author><author><keyname>Bruni</keyname><forenames>Elia</forenames></author></authors><title>iVISPAR -- An Interactive Visual-Spatial Reasoning Benchmark for VLMs</title><categories>cs.CL cs.AI cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Vision-Language Models (VLMs) are known to struggle with spatial reasoning and visual alignment. To help overcome these limitations, we introduce iVISPAR, an interactive multi-modal benchmark designed to evaluate the spatial reasoning capabilities of VLMs acting as agents. iVISPAR is based on a variant of the sliding tile puzzle-a classic problem that demands logical planning, spatial awareness, and multi-step reasoning. The benchmark supports visual 2D, 3D, and text-based input modalities, enabling comprehensive assessments of VLMs' planning and reasoning skills. We evaluate a broad suite of state-of-the-art open-source and closed-source VLMs, comparing their performance while also providing optimal path solutions and a human baseline to assess the task's complexity and feasibility for humans. Results indicate that while some VLMs perform well on simple spatial tasks, they encounter difficulties with more complex configurations and problem properties. Notably, while VLMs generally perform better in 2D vision compared to 3D or text-based representations, they consistently fall short of human performance, illustrating the persistent challenge of visual alignment. This highlights critical gaps in current VLM capabilities, highlighting their limitations in achieving human-level cognition. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03218</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03218</id><created>2025-02-05</created><authors><author><keyname>Bouke</keyname><forenames>Mohamed Aly</forenames></author><author><keyname>Abdullah</keyname><forenames>Azizol</forenames></author><author><keyname>Cengiz</keyname><forenames>Korhan</forenames></author><author><keyname>Ivković</keyname><forenames>Nikola</forenames></author><author><keyname>Mihaljević</keyname><forenames>Ivan</forenames></author><author><keyname>Mohamud</keyname><forenames>Mudathir Ahmed</forenames></author><author><keyname>Kowrina</keyname><forenames>Ahmed</forenames></author></authors><title>Data Dams: A Novel Framework for Regulating and Managing Data Flow in   Large-Scale Systems</title><categories>cs.IR cs.DB cs.DC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the era of big data, managing dynamic data flows efficiently is crucial as traditional storage models struggle with real-time regulation and risk overflow. This paper introduces Data Dams, a novel framework designed to optimize data inflow, storage, and outflow by dynamically adjusting flow rates to prevent congestion while maximizing resource utilization. Inspired by physical dam mechanisms, the framework employs intelligent sluice controls and predictive analytics to regulate data flow based on system conditions such as bandwidth availability, processing capacity, and security constraints. Simulation results demonstrate that the Data Dam significantly reduces average storage levels (371.68 vs. 426.27 units) and increases total outflow (7999.99 vs. 7748.76 units) compared to static baseline models. By ensuring stable and adaptive outflow rates under fluctuating data loads, this approach enhances system efficiency, mitigates overflow risks, and outperforms existing static flow control strategies. The proposed framework presents a scalable solution for dynamic data management in large-scale distributed systems, paving the way for more resilient and efficient real-time processing architectures. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03220</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03220</id><created>2025-02-05</created><authors><author><keyname>Laosaengpha</keyname><forenames>Napat</forenames></author><author><keyname>Tativannarat</keyname><forenames>Thanit</forenames></author><author><keyname>Rutherford</keyname><forenames>Attapol</forenames></author><author><keyname>Chuangsuwanich</keyname><forenames>Ekapol</forenames></author></authors><title>Mitigating Language Bias in Cross-Lingual Job Retrieval: A Recruitment   Platform Perspective</title><categories>cs.CL</categories><comments>To be published in CompJobs Workshop at AAAI 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Understanding the textual components of resumes and job postings is critical for improving job-matching accuracy and optimizing job search systems in online recruitment platforms. However, existing works primarily focus on analyzing individual components within this information, requiring multiple specialized tools to analyze each aspect. Such disjointed methods could potentially hinder overall generalizability in recruitment-related text processing. Therefore, we propose a unified sentence encoder that utilized multi-task dual-encoder framework for jointly learning multiple component into the unified sentence encoder. The results show that our method outperforms other state-of-the-art models, despite its smaller model size. Moreover, we propose a novel metric, Language Bias Kullback-Leibler Divergence (LBKL), to evaluate language bias in the encoder, demonstrating significant bias reduction and superior cross-lingual performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03221</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03221</id><created>2025-02-05</created><authors><author><keyname>Maringer</keyname><forenames>Georg</forenames></author><author><keyname>Hiller</keyname><forenames>Matthias</forenames></author></authors><title>Information Theoretic Analysis of PUF-Based Tamper Protection</title><categories>cs.IT cs.CR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Physical Unclonable Functions (PUFs) enable physical tamper protection for high-assurance devices without needing a continuous power supply that is active over the entire lifetime of the device. Several methods for PUF-based tamper protection have been proposed together with practical quantization and error correction schemes. In this work we take a step back from the implementation to analyze theoretical properties and limits. We apply zero leakage output quantization to existing quantization schemes and minimize the reconstruction error probability under zero leakage. We apply wiretap coding within a helper data algorithm to enable a reliable key reconstruction for the legitimate user while guaranteeing a selectable reconstruction complexity for an attacker, analogously to the security level for a cryptographic algorithm for the attacker models considered in this work. We present lower bounds on the achievable key rates depending on the attacker's capabilities in the asymptotic and finite blocklength regime to give fundamental security guarantees even if the attacker gets partial information about the PUF response and the helper data. Furthermore, we present converse bounds on the number of PUF cells. Our results show for example that for a practical scenario one needs at least 459 PUF cells using 3 bit quantization to achieve a security level of 128 bit. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03226</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03226</id><created>2025-02-05</created><authors><author><keyname>Zha</keyname><forenames>Siyu</forenames></author><author><keyname>Tang</keyname><forenames>Yuanrong</forenames></author><author><keyname>Gong</keyname><forenames>Jiangtao</forenames></author><author><keyname>Xu</keyname><forenames>Yingqing</forenames></author></authors><title>COLP: Scaffolding Children's Online Long-term Collaborative Learning</title><categories>cs.HC</categories><comments>42 pages, 13 figures. Submitted to International Journal of   Human-Computer Interaction</comments><msc-class>68U35 (Primary), 68T50 (Secondary)</msc-class><acm-class>H.5.2; K.3.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Online collaborative learning and working are important for everyone including children. However, children still face a lot of difficulties communicating and working together while online, which keeps them from engaging in long-term project-based teamwork. We aim to investigate online long-term collaborative learning opportunities to address this gap. We design COLP, an online, 16-week, project-based learning program, as an educational intervention based on multiple learning theories for primary school students. We conducted this program with 67 primary school students ages 8-13, across more than five provinces of China. We found that this program could engage more than one-third of children in teamwork after long-term study. Furthermore, we interview children and their parents to help us understand the communication channel, benefits, and challenges of this program. Interestingly, we discovered that parents play multiple roles in their children's collaborative learning, particularly modeling and guiding the children's collaborative skills. Given the lack of programs designed for children's long-term online collaboration, this study may inspire intervention design in computer-supported collaborative learning communities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03227</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03227</id><created>2025-02-05</created><authors><author><keyname>De Plaen</keyname><forenames>Pierre-François</forenames></author><author><keyname>Tuytelaars</keyname><forenames>Tinne</forenames></author><author><keyname>Proesmans</keyname><forenames>Marc</forenames></author><author><keyname>Van Gool</keyname><forenames>Luc</forenames></author></authors><title>Adversarial Dependence Minimization</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Many machine learning techniques rely on minimizing the covariance between output feature dimensions to extract minimally redundant representations from data. However, these methods do not eliminate all dependencies/redundancies, as linearly uncorrelated variables can still exhibit nonlinear relationships. This work provides a differentiable and scalable algorithm for dependence minimization that goes beyond linear pairwise decorrelation. Our method employs an adversarial game where small networks identify dependencies among feature dimensions, while the encoder exploits this information to reduce dependencies. We provide empirical evidence of the algorithm's convergence and demonstrate its utility in three applications: extending PCA to nonlinear decorrelation, improving the generalization of image classification methods, and preventing dimensional collapse in self-supervised representation learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03228</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03228</id><created>2025-02-05</created><authors><author><keyname>Li</keyname><forenames>Mingrui</forenames></author><author><keyname>Chen</keyname><forenames>Weijian</forenames></author><author><keyname>Cheng</keyname><forenames>Na</forenames></author><author><keyname>Xu</keyname><forenames>Jingyuan</forenames></author><author><keyname>Li</keyname><forenames>Dong</forenames></author><author><keyname>Wang</keyname><forenames>Hongyu</forenames></author></authors><title>GARAD-SLAM: 3D GAussian splatting for Real-time Anti Dynamic SLAM</title><categories>cs.RO cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 3D Gaussian Splatting (3DGS)-based SLAM system has garnered widespread attention due to its excellent performance in real-time high-fidelity rendering. However, in real-world environments with dynamic objects, existing 3DGS-based SLAM systems often face mapping errors and tracking drift issues. To address these problems, we propose GARAD-SLAM, a real-time 3DGS-based SLAM system tailored for dynamic scenes. In terms of tracking, unlike traditional methods, we directly perform dynamic segmentation on Gaussians and map them back to the front-end to obtain dynamic point labels through a Gaussian pyramid network, achieving precise dynamic removal and robust tracking. For mapping, we impose rendering penalties on dynamically labeled Gaussians, which are updated through the network, to avoid irreversible erroneous removal caused by simple pruning. Our results on real-world datasets demonstrate that our method is competitive in tracking compared to baseline methods, generating fewer artifacts and higher-quality reconstructions in rendering. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03229</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03229</id><created>2025-02-05</created><authors><author><keyname>Li</keyname><forenames>Ruizhe</forenames></author><author><keyname>Figueredo</keyname><forenames>Grazziela</forenames></author><author><keyname>Auer</keyname><forenames>Dorothee</forenames></author><author><keyname>Dineen</keyname><forenames>Rob</forenames></author><author><keyname>Morgan</keyname><forenames>Paul</forenames></author><author><keyname>Chen</keyname><forenames>Xin</forenames></author></authors><title>A Unified Framework for Semi-Supervised Image Segmentation and   Registration</title><categories>cs.CV</categories><comments>Accepted for publication at IEEE International Symposium on   Biomedical Imaging (ISBI) 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Semi-supervised learning, which leverages both annotated and unannotated data, is an efficient approach for medical image segmentation, where obtaining annotations for the whole dataset is time-consuming and costly. Traditional semi-supervised methods primarily focus on extracting features and learning data distributions from unannotated data to enhance model training. In this paper, we introduce a novel approach incorporating an image registration model to generate pseudo-labels for the unannotated data, producing more geometrically correct pseudo-labels to improve the model training. Our method was evaluated on a 2D brain data set, showing excellent performance even using only 1\% of the annotated data. The results show that our approach outperforms conventional semi-supervised segmentation methods (e.g. teacher-student model), particularly in a low percentage of annotation scenario. GitHub: https://github.com/ruizhe-l/UniSegReg. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03230</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03230</id><created>2025-02-05</created><authors><author><keyname>He</keyname><forenames>Jiayi</forenames></author><author><keyname>Tang</keyname><forenames>Shengeng</forenames></author><author><keyname>Liu</keyname><forenames>Ao</forenames></author><author><keyname>Cheng</keyname><forenames>Lechao</forenames></author><author><keyname>Wu</keyname><forenames>Jingjing</forenames></author><author><keyname>Wei</keyname><forenames>Yanyan</forenames></author></authors><title>Efficient Vision Language Model Fine-tuning for Text-based Person   Anomaly Search</title><categories>cs.CV cs.MM</categories><comments>Accepted by 2025 WWW Workshop on MORE</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents the HFUT-LMC team's solution to the WWW 2025 challenge on Text-based Person Anomaly Search (TPAS). The primary objective of this challenge is to accurately identify pedestrians exhibiting either normal or abnormal behavior within a large library of pedestrian images. Unlike traditional video analysis tasks, TPAS significantly emphasizes understanding and interpreting the subtle relationships between text descriptions and visual data. The complexity of this task lies in the model's need to not only match individuals to text descriptions in massive image datasets but also accurately differentiate between search results when faced with similar descriptions. To overcome these challenges, we introduce the Similarity Coverage Analysis (SCA) strategy to address the recognition difficulty caused by similar text descriptions. This strategy effectively enhances the model's capacity to manage subtle differences, thus improving both the accuracy and reliability of the search. Our proposed solution demonstrated excellent performance in this challenge. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03231</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03231</id><created>2025-02-05</created><authors><author><keyname>Zhu</keyname><forenames>Guogang</forenames></author><author><keyname>Liu</keyname><forenames>Xuefeng</forenames></author><author><keyname>Niu</keyname><forenames>Jianwei</forenames></author><author><keyname>Tang</keyname><forenames>Shaojie</forenames></author><author><keyname>Wu</keyname><forenames>Xinghao</forenames></author></authors><title>The Other Side of the Coin: Unveiling the Downsides of Model Aggregation   in Federated Learning from a Layer-peeled Perspective</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In federated learning (FL), model aggregation is a critical step by which multiple clients share their knowledge with one another. However, it is also widely recognized that the aggregated model, when sent back to each client, performs poorly on local data until after several rounds of local training. This temporary performance drop can potentially slow down the convergence of the FL model. Most research in FL regards this performance drop as an inherent cost of knowledge sharing among clients and does not give it special attention. While some studies directly focus on designing techniques to alleviate the issue, an in-depth investigation of the reasons behind this performance drop has yet to be conducted.To address this gap, we conduct a layer-peeled analysis of model aggregation across various datasets and model architectures. Our findings reveal that the performance drop can be attributed to two major consequences of the aggregation process: (1) it disrupts feature variability suppression in deep neural networks (DNNs), and (2) it weakens the coupling between features and subsequent parameters.Based on these findings, we propose several simple yet effective strategies to mitigate the negative impacts of model aggregation while still enjoying the benefit it brings. To the best of our knowledge, our work is the first to conduct a layer-peeled analysis of model aggregation, potentially paving the way for the development of more effective FL algorithms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03232</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03232</id><created>2025-02-05</created><authors><author><keyname>Yao</keyname><forenames>Yao</forenames></author><author><keyname>Westermann</keyname><forenames>Maximilian</forenames></author><author><keyname>Pontin</keyname><forenames>Marco</forenames></author><author><keyname>Albini</keyname><forenames>Alessandro</forenames></author><author><keyname>Maiolino</keyname><forenames>Perla</forenames></author></authors><title>JAMMit! Monolithic 3D-Printing of a Bead Jamming Soft Pneumatic Arm</title><categories>cs.RO</categories><comments>6 pages, 8 figures, accepted by the 8th IEEE-RAS International   Conference on Soft Robotics, RoboSoft 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  3D-printed bellow soft pneumatic arms are widely adopted for their flexible design, ease of fabrication, and large deformation capabilities. However, their low stiffness limits their real-world applications. Although several methods exist to enhance the stiffness of soft actuators, many involve complex manufacturing processes not in line with modern goals of monolithic and automated additive manufacturing. With its simplicity, bead-jamming represents a simple and effective solution to these challenges. This work introduces a method for monolithic printing of a bellow soft pneumatic arm, integrating a tendon-driven central spine of bowl-shaped beads. We experimentally characterized the arm's range of motion in both unjammed and jammed states, as well as its stiffness under various actuation and jamming conditions. As a result, we provide an optimal jamming policy as a trade-off between preserving the range of motion and maximizing stiffness. The proposed design was further demonstrated in a switch-toggling task, showing its potential for practical applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03233</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03233</id><created>2025-02-05</created><authors><author><keyname>Lin</keyname><forenames>Bo</forenames></author><author><keyname>Wang</keyname><forenames>Shangwen</forenames></author><author><keyname>Chen</keyname><forenames>Liqian</forenames></author><author><keyname>Mao</keyname><forenames>Xiaoguang</forenames></author></authors><title>Exploring the Security Threats of Knowledge Base Poisoning in   Retrieval-Augmented Code Generation</title><categories>cs.CR cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The integration of Large Language Models (LLMs) into software development has revolutionized the field, particularly through the use of Retrieval-Augmented Code Generation (RACG) systems that enhance code generation with information from external knowledge bases. However, the security implications of RACG systems, particularly the risks posed by vulnerable code examples in the knowledge base, remain largely unexplored. This risk is particularly concerning given that public code repositories, which often serve as the sources for knowledge base collection in RACG systems, are usually accessible to anyone in the community. Malicious attackers can exploit this accessibility to inject vulnerable code into the knowledge base, making it toxic. Once these poisoned samples are retrieved and incorporated into the generated code, they can propagate security vulnerabilities into the final product. This paper presents the first comprehensive study on the security risks associated with RACG systems, focusing on how vulnerable code in the knowledge base compromises the security of generated code. We investigate the LLM-generated code security across different settings through extensive experiments using four major LLMs, two retrievers, and two poisoning scenarios. Our findings highlight the significant threat of knowledge base poisoning, where even a single poisoned code example can compromise up to 48% of generated code. Our findings provide crucial insights into vulnerability introduction in RACG systems and offer practical mitigation recommendations, thereby helping improve the security of LLM-generated code in future works. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03236</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03236</id><created>2025-02-05</created><authors><author><keyname>Sun</keyname><forenames>Li</forenames></author><author><keyname>Zhang</keyname><forenames>Ziheng</forenames></author><author><keyname>Wang</keyname><forenames>Zixi</forenames></author><author><keyname>Wang</keyname><forenames>Yujie</forenames></author><author><keyname>Wan</keyname><forenames>Qiqi</forenames></author><author><keyname>Li</keyname><forenames>Hao</forenames></author><author><keyname>Peng</keyname><forenames>Hao</forenames></author><author><keyname>Yu</keyname><forenames>Philip S.</forenames></author></authors><title>Pioneer: Physics-informed Riemannian Graph ODE for Entropy-increasing   Dynamics</title><categories>cs.LG</categories><comments>Accepted by AAAI25</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Dynamic interacting system modeling is important for understanding and simulating real world systems. The system is typically described as a graph, where multiple objects dynamically interact with each other and evolve over time. In recent years, graph Ordinary Differential Equations (ODE) receive increasing research attentions. While achieving encouraging results, existing solutions prioritize the traditional Euclidean space, and neglect the intrinsic geometry of the system and physics laws, e.g., the principle of entropy increasing. The limitations above motivate us to rethink the system dynamics from a fresh perspective of Riemannian geometry, and pose a more realistic problem of physics-informed dynamic system modeling, considering the underlying geometry and physics law for the first time. In this paper, we present a novel physics-informed Riemannian graph ODE for a wide range of entropy-increasing dynamic systems (termed as Pioneer). In particular, we formulate a differential system on the Riemannian manifold, where a manifold-valued graph ODE is governed by the proposed constrained Ricci flow, and a manifold preserving Gyro-transform aware of system geometry. Theoretically, we report the provable entropy non-decreasing of our formulation, obeying the physics laws. Empirical results show the superiority of Pioneer on real datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03238</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03238</id><created>2025-02-05</created><authors><author><keyname>Pan</keyname><forenames>Li</forenames></author><author><keyname>Zhang</keyname><forenames>Yupei</forenames></author><author><keyname>Yang</keyname><forenames>Qiushi</forenames></author><author><keyname>Li</keyname><forenames>Tan</forenames></author><author><keyname>Chen</keyname><forenames>Zhen</forenames></author></authors><title>Long-tailed Medical Diagnosis with Relation-aware Representation   Learning and Iterative Classifier Calibration</title><categories>cs.CV cs.AI cs.LG cs.MM</categories><comments>This work has been accepted in Computers in Biology and Medicine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently computer-aided diagnosis has demonstrated promising performance, effectively alleviating the workload of clinicians. However, the inherent sample imbalance among different diseases leads algorithms biased to the majority categories, leading to poor performance for rare categories. Existing works formulated this challenge as a long-tailed problem and attempted to tackle it by decoupling the feature representation and classification. Yet, due to the imbalanced distribution and limited samples from tail classes, these works are prone to biased representation learning and insufficient classifier calibration. To tackle these problems, we propose a new Long-tailed Medical Diagnosis (LMD) framework for balanced medical image classification on long-tailed datasets. In the initial stage, we develop a Relation-aware Representation Learning (RRL) scheme to boost the representation ability by encouraging the encoder to capture intrinsic semantic features through different data augmentations. In the subsequent stage, we propose an Iterative Classifier Calibration (ICC) scheme to calibrate the classifier iteratively. This is achieved by generating a large number of balanced virtual features and fine-tuning the encoder using an Expectation-Maximization manner. The proposed ICC compensates for minority categories to facilitate unbiased classifier optimization while maintaining the diagnostic knowledge in majority classes. Comprehensive experiments on three public long-tailed medical datasets demonstrate that our LMD framework significantly surpasses state-of-the-art approaches. The source code can be accessed at https://github.com/peterlipan/LMD. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03244</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03244</id><created>2025-02-05</created><authors><author><keyname>Mustafin</keyname><forenames>Arsenii</forenames></author><author><keyname>Colla</keyname><forenames>Sebastien</forenames></author><author><keyname>Olshevsky</keyname><forenames>Alex</forenames></author><author><keyname>Paschalidis</keyname><forenames>Ioannis Ch.</forenames></author></authors><title>Analysis of Value Iteration Through Absolute Probability Sequences</title><categories>cs.LG</categories><comments>8 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Value Iteration is a widely used algorithm for solving Markov Decision Processes (MDPs). While previous studies have extensively analyzed its convergence properties, they primarily focus on convergence with respect to the infinity norm. In this work, we use absolute probability sequences to develop a new line of analysis and examine the algorithm's convergence in terms of the $L^2$ norm, offering a new perspective on its behavior and performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03245</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03245</id><created>2025-02-05</created><authors><author><keyname>Sanami</keyname><forenames>Saba</forenames></author><author><keyname>Aghdam</keyname><forenames>Amir G.</forenames></author></authors><title>Calibrated Unsupervised Anomaly Detection in Multivariate Time-series   using Reinforcement Learning</title><categories>cs.LG cs.SY eess.SP eess.SY</categories><comments>This paper has been accepted for publication and presentation at the   2025 IEEE International systems Conference (SysCon)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper investigates unsupervised anomaly detection in multivariate time-series data using reinforcement learning (RL) in the latent space of an autoencoder. A significant challenge is the limited availability of anomalous data, often leading to misclassifying anomalies as normal events, thus raising false negatives. RL can help overcome this limitation by promoting exploration and balancing exploitation during training, effectively preventing overfitting. Wavelet analysis is also utilized to enhance anomaly detection, enabling time-series data decomposition into both time and frequency domains. This approach captures anomalies at multiple resolutions, with wavelet coefficients extracted to detect both sudden and subtle shifts in the data, thereby refining the anomaly detection process. We calibrate the decision boundary by generating synthetic anomalies and embedding a supervised framework within the model. This supervised element aids the unsupervised learning process by fine-tuning the decision boundary and increasing the model's capacity to distinguish between normal and anomalous patterns effectively. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03247</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03247</id><created>2025-02-05</created><authors><author><keyname>Barbaraci</keyname><forenames>Mariarosaria</forenames></author><author><keyname>Schmid</keyname><forenames>Noah</forenames></author><author><keyname>Alpos</keyname><forenames>Orestis</forenames></author><author><keyname>Senn</keyname><forenames>Michael</forenames></author><author><keyname>Cachin</keyname><forenames>Christian</forenames></author></authors><title>Thetacrypt: A Distributed Service for Threshold Cryptography</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Threshold cryptography is a powerful and well-known technique with many applications to systems relying on distributed trust. It has recently emerged also as a solution to challenges in blockchain: frontrunning prevention, managing wallet keys, and generating randomness. This work presents Thetacrypt, a versatile library for integrating many threshold schemes into one codebase. It offers a way to easily build distributed systems using threshold cryptography and is agnostic to their implementation language. The architecture of Thetacrypt supports diverse protocols uniformly. The library currently includes six cryptographic schemes that span ciphers, signatures, and randomness generation. The library additionally contains a flexible adapter to an underlying networking layer that provides peer-to-peer communication and a total-order broadcast channel; the latter can be implemented by distributed ledgers, for instance. Thetacrypt serves as a controlled testbed for evaluating the performance of multiple threshold-cryptographic schemes under consistent conditions, showing how the traditional micro benchmarking approach neglects the distributed nature of the protocols and its relevance when considering system performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03248</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03248</id><created>2025-02-05</created><authors><author><keyname>Dominguez</keyname><forenames>Victor</forenames></author><author><keyname>Duque</keyname><forenames>Alejandro</forenames></author></authors><title>Practical Introduction to FEM with GMSH: A MATLAB/Octave Perspective</title><categories>math.NA cs.NA</categories><comments>42p</comments><msc-class>65N30</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Finite Element Method (FEM) is a powerful computational tool for solving partial differential equations (PDEs). Although commercial and open-source FEM software packages are widely available, an independent implementation of FEM provides significant educational value, provides a deeper understanding of the method, and enables the development of custom solutions tailored to specialized applications or integration with other solvers. This work introduces a 3D $\mathbb{P}_m$-element FEM implementation in MATLAB/Octave that is designed to balance educational clarity with computational efficiency. A key feature is its integration with GMSH, an open-source 3D mesh generator with CAD capabilities that streamlines mesh generation for complex geometries. By leveraging GMSH data structures, we provide a seamless connection between geometric modeling and numerical simulation. The implementation focuses on solving the general convection-diffusion-advection equation and serves as a flexible foundation for addressing advanced problems, including elasticity, mixed formulations, and integration with other numerical methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03250</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03250</id><created>2025-02-05</created><authors><author><keyname>Su</keyname><forenames>Shaojie</forenames></author><author><keyname>Wu</keyname><forenames>Jiasheng</forenames></author><author><keyname>Ying</keyname><forenames>Zijie</forenames></author><author><keyname>Zhao</keyname><forenames>Zhiyuan</forenames></author><author><keyname>Jia</keyname><forenames>Xiangyu</forenames></author><author><keyname>Zhu</keyname><forenames>Wenjun</forenames></author><author><keyname>Gao</keyname><forenames>Yue</forenames></author></authors><title>SkyOctopus: Enabling Low-Latency Mobile Satellite Network through   Multiple Anchors</title><categories>cs.NI</categories><comments>11 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapid deployment of low earth orbit (LEO) satellite constellations has drawn attention to the potential of nonterrestrial networks (NTN) in providing global communication services. Telecom operators are attempting to collaborate with satellite network providers to develop mobile satellite networks, which serve as an effective supplement to terrestrial networks. However, current mobile satellite network architectures still employ the single-anchor design of terrestrial mobile networks, leading to severely circuitous routing for users and significantly impacting their service experience. To reduce unnecessary latency caused by circuitous routing and provide users with low-latency global internet services, this paper presents SkyOctopus, an advanced multi-anchor mobile satellite network architecture. SkyOctopus innovatively deploys traffic classifiers on satellites to enable connections between users and multiple anchor points distributed globally. It guarantees optimal anchor point selection for each user's target server by monitoring multiple end-to-end paths. We build a prototype of SkyOctopus using enhanced Open5GS and UERANSIM, which is driven by actual LEO satellite constellations such as Starlink, Kuiper, and OneWeb. We conducted extensive experiments, and the results demonstrate that, compared to standard 5G NTN and two other existing schemes, SkyOctopus can reduce end-to-end latency by up to 53\%. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03251</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03251</id><created>2025-02-05</created><authors><author><keyname>Sun</keyname><forenames>Li</forenames></author><author><keyname>Huang</keyname><forenames>Zhenhao</forenames></author><author><keyname>Zhou</keyname><forenames>Suyang</forenames></author><author><keyname>Wan</keyname><forenames>Qiqi</forenames></author><author><keyname>Peng</keyname><forenames>Hao</forenames></author><author><keyname>Yu</keyname><forenames>Philip</forenames></author></authors><title>RiemannGFM: Learning a Graph Foundation Model from Riemannian Geometry</title><categories>cs.LG</categories><comments>Accepted by WWW25</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The foundation model has heralded a new era in artificial intelligence, pretraining a single model to offer cross-domain transferability on different datasets. Graph neural networks excel at learning graph data, the omnipresent non-Euclidean structure, but often lack the generalization capacity. Hence, graph foundation model is drawing increasing attention, and recent efforts have been made to leverage Large Language Models. On the one hand, existing studies primarily focus on text-attributed graphs, while a wider range of real graphs do not contain fruitful textual attributes. On the other hand, the sequential graph description tailored for the Large Language Model neglects the structural complexity, which is a predominant characteristic of the graph. Such limitations motivate an important question: Can we go beyond Large Language Models, and pretrain a universal model to learn the structural knowledge for any graph? The answer in the language or vision domain is a shared vocabulary. We observe the fact that there also exist shared substructures underlying graph domain, and thereby open a new opportunity of graph foundation model with structural vocabulary. The key innovation is the discovery of a simple yet effective structural vocabulary of trees and cycles, and we explore its inherent connection to Riemannian geometry. Herein, we present a universal pretraining model, RiemannGFM. Concretely, we first construct a novel product bundle to incorporate the diverse geometries of the vocabulary. Then, on this constructed space, we stack Riemannian layers where the structural vocabulary, regardless of specific graph, is learned in Riemannian manifold offering cross-domain transferability. Extensive experiments show the effectiveness of RiemannGFM on a diversity of real graphs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03252</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03252</id><created>2025-02-05</created><authors><author><keyname>Emmrich</keyname><forenames>Volker</forenames></author></authors><title>A scale of conceptual orality and literacy: Automatic text   categorization in the tradition of "N\"ahe und Distanz"</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Koch and Oesterreicher's model of "N\"ahe und Distanz" (N\"ahe = immediacy, conceptual orality; Distanz = distance, conceptual literacy) is constantly used in German linguistics. However, there is no statistical foundation for use in corpus linguistic analyzes, while it is increasingly moving into empirical corpus linguistics. Theoretically, it is stipulated, among other things, that written texts can be rated on a scale of conceptual orality and literacy by linguistic features. This article establishes such a scale based on PCA and combines it with automatic analysis. Two corpora of New High German serve as examples. When evaluating established features, a central finding is that features of conceptual orality and literacy must be distinguished in order to rank texts in a differentiated manner. The scale is also discussed with a view to its use in corpus compilation and as a guide for analyzes in larger corpora. With a theory-driven starting point and as a "tailored" dimension, the approach compared to Biber's Dimension 1 is particularly suitable for these supporting, controlling tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03253</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03253</id><created>2025-02-05</created><authors><author><keyname>Laverghetta</keyname><forenames>Antonio</forenames><suffix>Jr.</suffix></author><author><keyname>Chakrabarty</keyname><forenames>Tuhin</forenames></author><author><keyname>Hope</keyname><forenames>Tom</forenames></author><author><keyname>Pronchick</keyname><forenames>Jimmy</forenames></author><author><keyname>Bhawsar</keyname><forenames>Krupa</forenames></author><author><keyname>Beaty</keyname><forenames>Roger E.</forenames></author></authors><title>How do Humans and Language Models Reason About Creativity? A Comparative   Analysis</title><categories>cs.CL</categories><comments>CogSci 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Creativity assessment in science and engineering is increasingly based on both human and AI judgment, but the cognitive processes and biases behind these evaluations remain poorly understood. We conducted two experiments examining how including example solutions with ratings impact creativity evaluation, using a finegrained annotation protocol where raters were tasked with explaining their originality scores and rating for the facets of remoteness (whether the response is "far" from everyday ideas), uncommonness (whether the response is rare), and cleverness. In Study 1, we analyzed creativity ratings from 72 experts with formal science or engineering training, comparing those who received example solutions with ratings (example) to those who did not (no example). Computational text analysis revealed that, compared to experts with examples, no-example experts used more comparative language (e.g., "better/worse") and emphasized solution uncommonness, suggesting they may have relied more on memory retrieval for comparisons. In Study 2, parallel analyses with state-of-the-art LLMs revealed that models prioritized uncommonness and remoteness of ideas when rating originality, suggesting an evaluative process rooted around the semantic similarity of ideas. In the example condition, while LLM accuracy in predicting the true originality scores improved, the correlations of remoteness, uncommonness, and cleverness with originality also increased substantially - to upwards of 0.99 - suggesting a homogenization in the LLMs evaluation of the individual facets. These findings highlight important implications for how humans and AI reason about creativity and suggest diverging preferences for what different populations prioritize when rating. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03257</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03257</id><created>2025-02-05</created><authors><author><keyname>Fabacher</keyname><forenames>Thibaut</forenames></author><author><keyname>Sauleau</keyname><forenames>Erik-André</forenames></author><author><keyname>Arcay</keyname><forenames>Emmanuelle</forenames></author><author><keyname>Faye</keyname><forenames>Bineta</forenames></author><author><keyname>Alter</keyname><forenames>Maxime</forenames></author><author><keyname>Chahard</keyname><forenames>Archia</forenames></author><author><keyname>Miraillet</keyname><forenames>Nathan</forenames></author><author><keyname>Coulet</keyname><forenames>Adrien</forenames></author><author><keyname>Névéol</keyname><forenames>Aurélie</forenames></author></authors><title>Efficient extraction of medication information from clinical notes: an   evaluation in two languages</title><categories>cs.CL cs.IR</categories><comments>Submitted to JAMIA, 17 pages, 3 figures, 2 tables and 5 supplementary   tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Objective: To evaluate the accuracy, computational cost and portability of a new Natural Language Processing (NLP) method for extracting medication information from clinical narratives. Materials and Methods: We propose an original transformer-based architecture for the extraction of entities and their relations pertaining to patients' medication regimen. First, we used this approach to train and evaluate a model on French clinical notes, using a newly annotated corpus from H\^opitaux Universitaires de Strasbourg. Second, the portability of the approach was assessed by conducting an evaluation on clinical documents in English from the 2018 n2c2 shared task. Information extraction accuracy and computational cost were assessed by comparison with an available method using transformers. Results: The proposed architecture achieves on the task of relation extraction itself performance that are competitive with the state-of-the-art on both French and English (F-measures 0.82 and 0.96 vs 0.81 and 0.95), but reduce the computational cost by 10. End-to-end (Named Entity recognition and Relation Extraction) F1 performance is 0.69 and 0.82 for French and English corpus. Discussion: While an existing system developed for English notes was deployed in a French hospital setting with reasonable effort, we found that an alternative architecture offered end-to-end drug information extraction with comparable extraction performance and lower computational impact for both French and English clinical text processing, respectively. Conclusion: The proposed architecture can be used to extract medication information from clinical text with high performance and low computational cost and consequently suits with usually limited hospital IT resources </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03260</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03260</id><created>2025-02-05</created><authors><author><keyname>Zhang</keyname><forenames>Qiquan</forenames></author><author><keyname>Wickramasinghe</keyname><forenames>Buddhi</forenames></author><author><keyname>Ambikairajah</keyname><forenames>Eliathamby</forenames></author><author><keyname>Sethu</keyname><forenames>Vidhyasaharan</forenames></author><author><keyname>Li</keyname><forenames>Haizhou</forenames></author></authors><title>Should Audio Front-ends be Adaptive? Comparing Learnable and Adaptive   Front-ends</title><categories>eess.AS cs.SD</categories><comments>Accepted by IEEE TASLP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hand-crafted features, such as Mel-filterbanks, have traditionally been the choice for many audio processing applications. Recently, there has been a growing interest in learnable front-ends that extract representations directly from the raw audio waveform. \textcolor{black}{However, both hand-crafted filterbanks and current learnable front-ends lead to fixed computation graphs at inference time, failing to dynamically adapt to varying acoustic environments, a key feature of human auditory systems.} To this end, we explore the question of whether audio front-ends should be adaptive by comparing the Ada-FE front-end (a recently developed adaptive front-end that employs a neural adaptive feedback controller to dynamically adjust the Q-factors of its spectral decomposition filters) to established learnable front-ends. Specifically, we systematically investigate learnable front-ends and Ada-FE across two commonly used back-end backbones and a wide range of audio benchmarks including speech, sound event, and music. The comprehensive results show that our Ada-FE outperforms advanced learnable front-ends, and more importantly, it exhibits impressive stability or robustness on test samples over various training epochs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03261</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03261</id><created>2025-02-05</created><authors><author><keyname>Somerstep</keyname><forenames>Seamus</forenames></author><author><keyname>Polo</keyname><forenames>Felipe Maia</forenames></author><author><keyname>de Oliveira</keyname><forenames>Allysson Flavio Melo</forenames></author><author><keyname>Mangal</keyname><forenames>Prattyush</forenames></author><author><keyname>Silva</keyname><forenames>Mírian</forenames></author><author><keyname>Bhardwaj</keyname><forenames>Onkar</forenames></author><author><keyname>Yurochkin</keyname><forenames>Mikhail</forenames></author><author><keyname>Maity</keyname><forenames>Subha</forenames></author></authors><title>CARROT: A Cost Aware Rate Optimal Router</title><categories>stat.ML cs.LG cs.NI math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  With the rapid growth in the number of Large Language Models (LLMs), there has been a recent interest in LLM routing, or directing queries to the cheapest LLM that can deliver a suitable response. Following this line of work, we introduce CARROT, a Cost AwaRe Rate Optimal rouTer that can select models based on any desired trade-off between performance and cost. Given a query, CARROT selects a model based on estimates of models' cost and performance. Its simplicity lends CARROT computational efficiency, while our theoretical analysis demonstrates minimax rate-optimality in its routing performance. Alongside CARROT, we also introduce the Smart Price-aware Routing (SPROUT) dataset to facilitate routing on a wide spectrum of queries with the latest state-of-the-art LLMs. Using SPROUT and prior benchmarks such as Routerbench and open-LLM-leaderboard-v2 we empirically validate CARROT's performance against several alternative routers. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03263</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03263</id><created>2025-02-05</created><authors><author><keyname>Shahna</keyname><forenames>Mehdi Heydari</forenames></author><author><keyname>Humaloja</keyname><forenames>Jukka-Pekka</forenames></author><author><keyname>Mattila</keyname><forenames>Jouni</forenames></author></authors><title>Model Reference-Based Control with Guaranteed Predefined Performance for   Uncertain Strict-Feedback Systems</title><categories>eess.SY cs.SY</categories><comments>This paper is under review by Automatica, a Journal of IFAC, the   International Federation of Automatic Control</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  To address the complexities posed by time- and state-varying uncertainties and the computation of analytic derivatives in strict-feedback form (SFF) systems, this study introduces a novel model reference-based control (MRBC) framework which applies locally to each subsystem (SS), to ensure output tracking performance within the specified transient and steady-state response criteria. This framework includes 1) novel homogeneous adaptive estimators (HAEs) designed to match the uncertain nonlinear SFF system to a reference model, enabling easier analysis and control design at the $SS$ level, and 2) model-based homogeneous adaptive controllers enhanced by logarithmic barrier Lyapunov functions (HAC-BLFs), intended to control the reference model provided by HAEs in each SS, while ensuring the prescribed tracking responses under control amplitude saturation. The inherently robust MRBC achieves uniformly exponential stability using a generic stability connector term, which addresses dynamic interactions between the adjacent SSs. The parameter sensitivities of HAEs and HAC-BLFs in the MRBC framework are analyzed, focusing on the system's robustness and responsiveness. The proposed MRBC framework is experimentally validated through several scenarios involving an electromechanical linear actuator system with an uncertain SFF, subjected loading disturbance forces challenging 0-95% of its capacity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03264</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03264</id><created>2025-02-05</created><authors><author><keyname>He</keyname><forenames>Cheng</forenames></author><author><keyname>Huang</keyname><forenames>Xu</forenames></author><author><keyname>Jiang</keyname><forenames>Gangwei</forenames></author><author><keyname>Li</keyname><forenames>Zhaoyi</forenames></author><author><keyname>Lian</keyname><forenames>Defu</forenames></author><author><keyname>Xie</keyname><forenames>Hong</forenames></author><author><keyname>Chen</keyname><forenames>Enhong</forenames></author><author><keyname>Liang</keyname><forenames>Xijie</forenames></author><author><keyname>Zheng</keyname><forenames>Zengrong</forenames></author></authors><title>General Time-series Model for Universal Knowledge Representation of   Multivariate Time-Series data</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Universal knowledge representation is a central problem for multivariate time series(MTS) foundation models and yet remains open. This paper investigates this problem from the first principle and it makes four folds of contributions. First, a new empirical finding is revealed: time series with different time granularities (or corresponding frequency resolutions) exhibit distinct joint distributions in the frequency domain. This implies a crucial aspect of learning universal knowledge, one that has been overlooked by previous studies. Second, a novel Fourier knowledge attention mechanism is proposed to enable learning time granularity-aware representations from both the temporal and frequency domains. Third, an autoregressive blank infilling pre-training framework is incorporated to time series analysis for the first time, leading to a generative tasks agnostic pre-training strategy. To this end, we develop the General Time-series Model (GTM), a unified MTS foundation model that addresses the limitation of contemporary time series models, which often require token, pre-training, or model-level customizations for downstream tasks adaption. Fourth, extensive experiments show that GTM outperforms state-of-the-art (SOTA) methods across all generative tasks, including long-term forecasting, anomaly detection, and imputation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03265</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03265</id><created>2025-02-05</created><authors><author><keyname>Kotarsky</keyname><forenames>Niklas</forenames></author><author><keyname>Birken</keyname><forenames>Philipp</forenames></author></authors><title>A time adaptive multirate Quasi-Newton waveform iteration for coupled   problems</title><categories>math.NA cs.NA</categories><msc-class>65M22</msc-class><acm-class>G.1.8</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider waveform iterations for dynamical coupled problems, or more specifically, PDEs that interact through a lower dimensional interface. We want to allow for the reuse of existing codes for the subproblems, called a partitioned approach. To improve computational efficiency, different and adaptive time steps in the subsolvers are advisable. Using so called waveform iterations in combination with relaxation, this has been achieved for heat transfer problems earlier. Alternatively, one can use a black box method like Quasi-Newton to improve the convergence behaviour. These methods have recently been combined with waveform iterations for fixed time steps. Here, we suggest an extension of the Quasi-Newton method to the time adaptive setting and analyze its properties.   We compare the proposed Quasi-Newton method with state of the art solvers on a heat transfer test case, and a complex mechanical Fluid-Structure interaction case, demonstrating the methods efficiency. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03266</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03266</id><created>2025-02-05</created><authors><author><keyname>Zhang</keyname><forenames>Ying</forenames></author><author><keyname>Yin</keyname><forenames>Maoliang</forenames></author><author><keyname>Bi</keyname><forenames>Wenfu</forenames></author><author><keyname>Yan</keyname><forenames>Haibao</forenames></author><author><keyname>Bian</keyname><forenames>Shaohan</forenames></author><author><keyname>Zhang</keyname><forenames>Cui-Hua</forenames></author><author><keyname>Hua</keyname><forenames>Changchun</forenames></author></authors><title>ZISVFM: Zero-Shot Object Instance Segmentation in Indoor Robotic   Environments with Vision Foundation Models</title><categories>cs.CV cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service robots operating in unstructured environments must effectively recognize and segment unknown objects to enhance their functionality. Traditional supervised learningbased segmentation techniques require extensive annotated datasets, which are impractical for the diversity of objects encountered in real-world scenarios. Unseen Object Instance Segmentation (UOIS) methods aim to address this by training models on synthetic data to generalize to novel objects, but they often suffer from the simulation-to-reality gap. This paper proposes a novel approach (ZISVFM) for solving UOIS by leveraging the powerful zero-shot capability of the segment anything model (SAM) and explicit visual representations from a selfsupervised vision transformer (ViT). The proposed framework operates in three stages: (1) generating object-agnostic mask proposals from colorized depth images using SAM, (2) refining these proposals using attention-based features from the selfsupervised ViT to filter non-object masks, and (3) applying K-Medoids clustering to generate point prompts that guide SAM towards precise object segmentation. Experimental validation on two benchmark datasets and a self-collected dataset demonstrates the superior performance of ZISVFM in complex environments, including hierarchical settings such as cabinets, drawers, and handheld objects. Our source code is available at https://github.com/Yinmlmaoliang/zisvfm. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03270</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03270</id><created>2025-02-05</created><authors><author><keyname>Tsagkas</keyname><forenames>Nikolaos</forenames></author><author><keyname>Sochopoulos</keyname><forenames>Andreas</forenames></author><author><keyname>Danier</keyname><forenames>Duolikun</forenames></author><author><keyname>Lu</keyname><forenames>Chris Xiaoxuan</forenames></author><author><keyname>Mac Aodha</keyname><forenames>Oisin</forenames></author></authors><title>When Pre-trained Visual Representations Fall Short: Limitations in   Visuo-Motor Robot Learning</title><categories>cs.RO cs.AI cs.CV cs.LG</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The integration of pre-trained visual representations (PVRs) into visuo-motor robot learning has emerged as a promising alternative to training visual encoders from scratch. However, PVRs face critical challenges in the context of policy learning, including temporal entanglement and an inability to generalise even in the presence of minor scene perturbations. These limitations hinder performance in tasks requiring temporal awareness and robustness to scene changes. This work identifies these shortcomings and proposes solutions to address them. First, we augment PVR features with temporal perception and a sense of task completion, effectively disentangling them in time. Second, we introduce a module that learns to selectively attend to task-relevant local features, enhancing robustness when evaluated on out-of-distribution scenes. Our experiments demonstrate significant performance improvements, particularly in PVRs trained with masking objectives, and validate the effectiveness of our enhancements in addressing PVR-specific limitations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03271</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03271</id><created>2025-02-05</created><authors><author><keyname>Chen</keyname><forenames>Hung-Mao</forenames></author><author><keyname>He</keyname><forenames>Xu</forenames></author><author><keyname>Wang</keyname><forenames>Shu</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaokuan</forenames></author><author><keyname>Sun</keyname><forenames>Kun</forenames></author></authors><title>TYPEPULSE: Detecting Type Confusion Bugs in Rust Programs</title><categories>cs.CR cs.PL</categories><comments>To Appear in the 34th USENIX Security Symposium, August 13-15, 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rust supports type conversions and safe Rust guarantees the security of these conversions through robust static type checking and strict ownership guidelines. However, there are instances where programmers need to use unsafe Rust for certain type conversions, especially those involving pointers. Consequently, these conversions may cause severe memory corruption problems. Despite extensive research on type confusion bugs in C/C++, studies on type confusion bugs in Rust are still lacking. Also, due to Rust's new features in the type system, existing solutions in C/C++ cannot be directly applied to Rust. In this paper, we develop a static analysis tool called TYPEPULSE to detect three main categories of type confusion bugs in Rust including misalignment, inconsistent layout, and mismatched scope. TYPEPULSE first performs a type conversion analysis to collect and determine trait bounds for type pairs. Moreover, it performs a pointer alias analysis to resolve the alias relationship of pointers. Following the integration of information into the property graph, it constructs type patterns and detects each type of bug in various conversion scenarios. We run TYPEPULSE on the top 3,000 Rust packages and uncover 71 new type confusion bugs, exceeding the total number of type confusion bugs reported in RUSTSEC over the past five years. We have received 32 confirmations from developers, along with one CVE ID and six RUSTSEC IDs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03272</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03272</id><created>2025-02-05</created><authors><author><keyname>Schwab</keyname><forenames>Matthias</forenames></author><author><keyname>Pamminger</keyname><forenames>Mathias</forenames></author><author><keyname>Kremser</keyname><forenames>Christian</forenames></author><author><keyname>Mayr</keyname><forenames>Agnes</forenames></author></authors><title>Deep Learning Pipeline for Fully Automated Myocardial Infarct   Segmentation from Clinical Cardiac MR Scans</title><categories>eess.IV cs.AI cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose: To develop and evaluate a deep learning-based method that allows to perform myocardial infarct segmentation in a fully-automated way.   Materials and Methods: For this retrospective study, a cascaded framework of two and three-dimensional convolutional neural networks (CNNs), specialized on identifying ischemic myocardial scars on late gadolinium enhancement (LGE) cardiac magnetic resonance (CMR) images, was trained on an in-house training dataset consisting of 144 examinations. On a separate test dataset from the same institution, including images from 152 examinations obtained between 2021 and 2023, a quantitative comparison between artificial intelligence (AI)-based segmentations and manual segmentations was performed. Further, qualitative assessment of segmentation accuracy was evaluated for both human and AI-generated contours by two CMR experts in a blinded experiment.   Results: Excellent agreement could be found between manually and automatically calculated infarct volumes ($\rho_c$ = 0.9). The qualitative evaluation showed that compared to human-based measurements, the experts rated the AI-based segmentations to better represent the actual extent of infarction significantly (p &lt; 0.001) more often (33.4% AI, 25.1% human, 41.5% equal). On the contrary, for segmentation of microvascular obstruction (MVO), manual measurements were still preferred (11.3% AI, 55.6% human, 33.1% equal).   Conclusion: This fully-automated segmentation pipeline enables CMR infarct size to be calculated in a very short time and without requiring any pre-processing of the input images while matching the segmentation quality of trained human observers. In a blinded experiment, experts preferred automated infarct segmentations more often than manual segmentations, paving the way for a potential clinical application. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03274</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03274</id><created>2025-02-05</created><authors><author><keyname>Manginas</keyname><forenames>Vasileios</forenames></author><author><keyname>Manginas</keyname><forenames>Nikolaos</forenames></author><author><keyname>Stevinson</keyname><forenames>Edward</forenames></author><author><keyname>Varghese</keyname><forenames>Sherwin</forenames></author><author><keyname>Katzouris</keyname><forenames>Nikos</forenames></author><author><keyname>Paliouras</keyname><forenames>Georgios</forenames></author><author><keyname>Lomuscio</keyname><forenames>Alessio</forenames></author></authors><title>A Scalable Approach to Probabilistic Neuro-Symbolic Verification</title><categories>cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Neuro-Symbolic Artificial Intelligence (NeSy AI) has emerged as a promising direction for integrating neural learning with symbolic reasoning. In the probabilistic variant of such systems, a neural network first extracts a set of symbols from sub-symbolic input, which are then used by a symbolic component to reason in a probabilistic manner towards answering a query. In this work, we address the problem of formally verifying the robustness of such NeSy probabilistic reasoning systems, therefore paving the way for their safe deployment in critical domains. We analyze the complexity of solving this problem exactly, and show that it is $\mathrm{NP}^{\# \mathrm{P}}$-hard. To overcome this issue, we propose the first approach for approximate, relaxation-based verification of probabilistic NeSy systems. We demonstrate experimentally that the proposed method scales exponentially better than solver-based solutions and apply our technique to a real-world autonomous driving dataset, where we verify a safety property under large input dimensionalities and network sizes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03275</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03275</id><created>2025-02-05</created><authors><author><keyname>Su</keyname><forenames>DiJia</forenames></author><author><keyname>Zhu</keyname><forenames>Hanlin</forenames></author><author><keyname>Xu</keyname><forenames>Yingchen</forenames></author><author><keyname>Jiao</keyname><forenames>Jiantao</forenames></author><author><keyname>Tian</keyname><forenames>Yuandong</forenames></author><author><keyname>Zheng</keyname><forenames>Qinqing</forenames></author></authors><title>Token Assorted: Mixing Latent and Text Tokens for Improved Language   Model Reasoning</title><categories>cs.CL cs.AI cs.LG cs.LO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03278</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03278</id><created>2025-02-05</created><authors><author><keyname>Shahna</keyname><forenames>Mehdi Heydari</forenames></author><author><keyname>Mustalahti</keyname><forenames>Pauli</forenames></author><author><keyname>Mattila</keyname><forenames>Jouni</forenames></author></authors><title>Fault-Tolerant Control for System Availability and Continuous Operation   in Heavy-Duty Wheeled Mobile Robots</title><categories>eess.SY cs.SY</categories><comments>This paper is under review by IEEE/ASME Transactions on Mechatronics   (TMECH)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  When the control system in a heavy-duty wheeled mobile robot (HD-WMR) malfunctions, deviations from ideal motion occur, significantly heightening the risks of off-road instability and costly damage. To meet the demands for safety, reliability, and controllability in HD-WMRs, the control system must tolerate faults to a certain extent, ensuring continuous operation. To this end, this paper introduces a model-free hierarchical control with fault accommodation (MFHCA) framework designed to address sensor and actuator faults in hydraulically powered HD-WMRs with independently controlled wheels. To begin, a novel mathematical representation of the motion dynamics of HD-WMRs, incorporating both sensor and actuator fault modes, is investigated. Subsequently, the MFHCA framework is proposed to manage all wheels under various fault modes, ensuring that each wheel tracks the reference driving velocities and steering angles, which are inverse kinematically mapped from the angular and linear velocities commanded in the HD-WMR's base frame. To do so, this framework generates appropriate power efforts in independently valve-regulated wheels to accommodate the adaptively isolated faults, thereby ensuring exponential stability. The experimental analysis of a 6,500-kg hydraulic-powered HD-WMR under various fault modes and rough terrains demonstrates the validity of the MFHCA framework. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03280</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03280</id><created>2025-02-05</created><authors><author><keyname>Groeneveld</keyname><forenames>Wouter</forenames></author></authors><title>Leveraging Creativity as a Problem Solving Tool in Software Engineering</title><categories>cs.SE</categories><comments>IEEE Software - Special Issue on Creativity and Software Development   to be published in Q1 2025</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Today's software engineering (SE) complexities require a more diverse tool set going beyond technical expertise to be able to successfully tackle all challenges. Previous studies have indicated that creativity is a prime indicator for overcoming these hurdles. In this paper, we port results from creativity research in the field of cognitive psychology to the field of SE. After all, programming is a highly creative endeavour. We explore how to leverage creativity as a practical problem solving tool to wield for software developers. The seven distinct but intertwined creative problem solving themes unfolded in this paper are accompanied with practical perspectives, specifically geared for software professionals. Just like technical skills such as knowledge of programming languages, we believe that creativity can be learned and improved with practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03281</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03281</id><created>2025-02-05</created><authors><author><keyname>Buser</keyname><forenames>Elle</forenames></author><author><keyname>Chung</keyname><forenames>Julianne</forenames></author></authors><title>Efficient sampling approaches based on generalized Golub-Kahan methods   for large-scale hierarchical Bayesian inverse problems</title><categories>math.NA cs.NA</categories><comments>29 pages, 40 figures</comments><msc-class>65F22, 65M32, 62F10</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Uncertainty quantification for large-scale inverse problems remains a challenging task. For linear inverse problems with additive Gaussian noise and Gaussian priors, the posterior is Gaussian but sampling can be challenging, especially for problems with a very large number of unknown parameters (e.g., dynamic inverse problems) and for problems where computation of the square root and inverse of the prior covariance matrix are not feasible. Moreover, for hierarchical problems where several hyperparameters that define the prior and the noise model must be estimated from the data, the posterior distribution may no longer be Gaussian, even if the forward operator is linear. Performing large-scale uncertainty quantification for these hierarchical settings requires new computational techniques. In this work, we consider a hierarchical Bayesian framework where both the noise and prior variance are modeled as hyperparameters. Our approach uses Metropolis-Hastings independence sampling within Gibbs where the proposal distribution is based on generalized Golub-Kahan based methods. We consider two proposal samplers, one that uses a low rank approximation to the conditional covariance matrix and another that uses a preconditioned Lanczos method. Numerical examples from seismic imaging, dynamic photoacoustic tomography, and atmospheric inverse modeling demonstrate the effectiveness of the described approaches. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03283</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03283</id><created>2025-02-05</created><authors><author><keyname>Liu</keyname><forenames>Ben</forenames></author><author><keyname>Zhang</keyname><forenames>Jihai</forenames></author><author><keyname>Lin</keyname><forenames>Fangquan</forenames></author><author><keyname>Yang</keyname><forenames>Cheng</forenames></author><author><keyname>Peng</keyname><forenames>Min</forenames></author><author><keyname>Yin</keyname><forenames>Wotao</forenames></author></authors><title>SymAgent: A Neural-Symbolic Self-Learning Agent Framework for Complex   Reasoning over Knowledge Graphs</title><categories>cs.AI cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent advancements have highlighted that Large Language Models (LLMs) are prone to hallucinations when solving complex reasoning problems, leading to erroneous results. To tackle this issue, researchers incorporate Knowledge Graphs (KGs) to improve the reasoning ability of LLMs. However, existing methods face two limitations: 1) they typically assume that all answers to the questions are contained in KGs, neglecting the incompleteness issue of KGs, and 2) they treat the KG as a static repository and overlook the implicit logical reasoning structures inherent in KGs. In this paper, we introduce SymAgent, an innovative neural-symbolic agent framework that achieves collaborative augmentation between KGs and LLMs. We conceptualize KGs as dynamic environments and transform complex reasoning tasks into a multi-step interactive process, enabling KGs to participate deeply in the reasoning process. SymAgent consists of two modules: Agent-Planner and Agent-Executor. The Agent-Planner leverages LLM's inductive reasoning capability to extract symbolic rules from KGs, guiding efficient question decomposition. The Agent-Executor autonomously invokes predefined action tools to integrate information from KGs and external documents, addressing the issues of KG incompleteness. Furthermore, we design a self-learning framework comprising online exploration and offline iterative policy updating phases, enabling the agent to automatically synthesize reasoning trajectories and improve performance. Experimental results demonstrate that SymAgent with weak LLM backbones (i.e., 7B series) yields better or comparable performance compared to various strong baselines. Further analysis reveals that our agent can identify missing triples, facilitating automatic KG updates. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03285</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03285</id><created>2025-02-05</created><authors><author><keyname>Seleem</keyname><forenames>Abdelrahman</forenames><affiliation>Instituto Superior Técnico - Universidade de Lisboa, Lisbon, Portugal</affiliation><affiliation>Instituto de Telecomunicações, Portugal</affiliation><affiliation>Faculty of Computers and Information, South Valley University, Qena, Egypt</affiliation></author><author><keyname>Guarda</keyname><forenames>André F. R.</forenames><affiliation>Instituto de Telecomunicações, Portugal</affiliation></author><author><keyname>Rodrigues</keyname><forenames>Nuno M. M.</forenames><affiliation>Instituto de Telecomunicações, Portugal</affiliation><affiliation>ESTG, Politécnico de Leiria, Leiria, Portugal</affiliation></author><author><keyname>Pereira</keyname><forenames>Fernando</forenames><affiliation>Instituto Superior Técnico - Universidade de Lisboa, Lisbon, Portugal</affiliation><affiliation>Instituto de Telecomunicações, Portugal</affiliation></author></authors><title>Deep Learning-based Event Data Coding: A Joint Spatiotemporal and   Polarity Solution</title><categories>cs.CV eess.IV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Neuromorphic vision sensors, commonly referred to as event cameras, have recently gained relevance for applications requiring high-speed, high dynamic range and low-latency data acquisition. Unlike traditional frame-based cameras that capture 2D images, event cameras generate a massive number of pixel-level events, composed by spatiotemporal and polarity information, with very high temporal resolution, thus demanding highly efficient coding solutions. Existing solutions focus on lossless coding of event data, assuming that no distortion is acceptable for the target use cases, mostly including computer vision tasks. One promising coding approach exploits the similarity between event data and point clouds, thus allowing to use current point cloud coding solutions to code event data, typically adopting a two-point clouds representation, one for each event polarity. This paper proposes a novel lossy Deep Learning-based Joint Event data Coding (DL-JEC) solution adopting a single-point cloud representation, thus enabling to exploit the correlation between the spatiotemporal and polarity event information. DL-JEC can achieve significant compression performance gains when compared with relevant conventional and DL-based state-of-the-art event data coding solutions. Moreover, it is shown that it is possible to use lossy event data coding with its reduced rate regarding lossless coding without compromising the target computer vision task performance, notably for event classification. The use of novel adaptive voxel binarization strategies, adapted to the target task, further enables DL-JEC to reach a superior performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03286</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03286</id><created>2025-02-05</created><authors><author><keyname>Konstantinidis</keyname><forenames>Fabian</forenames></author><author><keyname>Sackmann</keyname><forenames>Moritz</forenames></author><author><keyname>Hofmann</keyname><forenames>Ulrich</forenames></author><author><keyname>Stiller</keyname><forenames>Christoph</forenames></author></authors><title>Conditional Prediction by Simulation for Automated Driving</title><categories>cs.RO cs.CV</categories><comments>Accepted for publication at "16. Uni-DAS e.V. Workshop   Fahrerassistenz und automatisiertes Fahren". Link:   https://www.uni-das.de/fas-workshop/2025.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modular automated driving systems commonly handle prediction and planning as sequential, separate tasks, thereby prohibiting cooperative maneuvers. To enable cooperative planning, this work introduces a prediction model that models the conditional dependencies between trajectories. For this, predictions are generated by a microscopic traffic simulation, with the individual traffic participants being controlled by a realistic behavior model trained via Adversarial Inverse Reinforcement Learning. By assuming various candidate trajectories for the automated vehicle, we generate predictions conditioned on each of them. Furthermore, our approach allows the candidate trajectories to adapt dynamically during the prediction rollout. Several example scenarios are available at https://conditionalpredictionbysimulation.github.io/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03287</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03287</id><created>2025-02-05</created><authors><author><keyname>Eissa</keyname><forenames>Sherif</forenames></author><author><keyname>Stuijk</keyname><forenames>Sander</forenames></author><author><keyname>De Putter</keyname><forenames>Floran</forenames></author><author><keyname>Nardi-Dei</keyname><forenames>Andrea</forenames></author><author><keyname>Corradi</keyname><forenames>Federico</forenames></author><author><keyname>Corporaal</keyname><forenames>Henk</forenames></author></authors><title>STEM: Spatial-Temporal Mapping Tool For Spiking Neural Networks</title><categories>cs.NE cs.AI cs.AR cs.DC</categories><comments>24 pages, 23 figures, under review at IEEE TC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spiking Neural Networks (SNNs) are promising bio-inspired third-generation neural networks. Recent research has trained deep SNN models with accuracy on par with Artificial Neural Networks (ANNs). Although the event-driven and sparse nature of SNNs show potential for more energy efficient computation than ANNs, SNN neurons have internal states which evolve over time. Keeping track of SNN states can significantly increase data movement and storage requirements, potentially losing its advantages with respect to ANNs. This paper investigates the energy effects of having neuron states, and how it is influenced by the chosen mapping to realistic hardware architectures with advanced memory hierarchies. Therefore, we develop STEMS, a mapping design space exploration tool for SNNs. STEMS models SNN's stateful behavior and explores intra-layer and inter-layer mapping optimizations to minimize data movement, considering both spatial and temporal SNN dimensions. Using STEMS, we show up to 12x reduction in off-chip data movement and 5x reduction in energy (on top of intra-layer optimizations), on two event-based vision SNN benchmarks. Finally, neuron states may not be needed for all SNN layers. By optimizing neuron states for one of our benchmarks, we show 20x reduction in neuron states and 1.4x better performance without accuracy loss. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03292</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03292</id><created>2025-02-05</created><authors><author><keyname>Halitaj</keyname><forenames>Aida</forenames></author><author><keyname>Zubiaga</keyname><forenames>Arkaitz</forenames></author></authors><title>ALPET: Active Few-shot Learning for Citation Worthiness Detection in   Low-Resource Wikipedia Languages</title><categories>cs.CL cs.AI cs.LG</categories><comments>24 pages, 8 figures, 4 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Citation Worthiness Detection (CWD) consists in determining which sentences, within an article or collection, should be backed up with a citation to validate the information it provides. This study, introduces ALPET, a framework combining Active Learning (AL) and Pattern-Exploiting Training (PET), to enhance CWD for languages with limited data resources. Applied to Catalan, Basque, and Albanian Wikipedia datasets, ALPET outperforms the existing CCW baseline while reducing the amount of labeled data in some cases above 80\%. ALPET's performance plateaus after 300 labeled samples, showing it suitability for low-resource scenarios where large, labeled datasets are not common. While specific active learning query strategies, like those employing K-Means clustering, can offer advantages, their effectiveness is not universal and often yields marginal gains over random sampling, particularly with smaller datasets. This suggests that random sampling, despite its simplicity, remains a strong baseline for CWD in constraint resource environments. Overall, ALPET's ability to achieve high performance with fewer labeled samples makes it a promising tool for enhancing the verifiability of online content in low-resource language settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03293</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03293</id><created>2025-02-05</created><authors><author><keyname>Pyae</keyname><forenames>Aung</forenames></author></authors><title>What is Human-Centeredness in Human-Centered AI? Development of   Human-Centeredness Framework and AI Practitioners' Perspectives</title><categories>cs.HC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  There is no consensus on what constitutes human-centeredness in AI, and existing frameworks lack empirical validation. This study addresses this gap by developing a hierarchical framework of 26 attributes of human-centeredness, validated through practitioner input. The framework prioritizes ethical foundations (e.g., fairness, transparency), usability, and emotional intelligence, organized into four tiers: ethical foundations, usability, emotional and cognitive dimensions, and personalization. By integrating theoretical insights with empirical data, this work offers actionable guidance for AI practitioners, promoting inclusive design, rigorous ethical standards, and iterative user feedback. The framework provides a robust foundation for creating AI systems that enhance human well-being and align with societal values. Future research should explore how these attributes evolve across cultural and industrial contexts, ensuring the framework remains relevant as AI technologies advance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03297</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03297</id><created>2025-02-05</created><authors><author><keyname>Jiang</keyname><forenames>Xinkai</forenames></author><author><keyname>Yuan</keyname><forenames>Qihao</forenames></author><author><keyname>Dincer</keyname><forenames>Enes Ulas</forenames></author><author><keyname>Zhou</keyname><forenames>Hongyi</forenames></author><author><keyname>Li</keyname><forenames>Ge</forenames></author><author><keyname>Li</keyname><forenames>Xueyin</forenames></author><author><keyname>Haag</keyname><forenames>Julius</forenames></author><author><keyname>Schreiber</keyname><forenames>Nicolas</forenames></author><author><keyname>Li</keyname><forenames>Kailai</forenames></author><author><keyname>Neumann</keyname><forenames>Gerhard</forenames></author><author><keyname>Lioutikov</keyname><forenames>Rudolf</forenames></author></authors><title>IRIS: An Immersive Robot Interaction System</title><categories>cs.RO cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper introduces IRIS, an immersive Robot Interaction System leveraging Extended Reality (XR), designed for robot data collection and interaction across multiple simulators, benchmarks, and real-world scenarios. While existing XR-based data collection systems provide efficient and intuitive solutions for large-scale data collection, they are often challenging to reproduce and reuse. This limitation arises because current systems are highly tailored to simulator-specific use cases and environments. IRIS is a novel, easily extendable framework that already supports multiple simulators, benchmarks, and even headsets. Furthermore, IRIS is able to include additional information from real-world sensors, such as point clouds captured through depth cameras. A unified scene specification is generated directly from simulators or real-world sensors and transmitted to XR headsets, creating identical scenes in XR. This specification allows IRIS to support any of the objects, assets, and robots provided by the simulators. In addition, IRIS introduces shared spatial anchors and a robust communication protocol that links simulations between multiple XR headsets. This feature enables multiple XR headsets to share a synchronized scene, facilitating collaborative and multi-user data collection. IRIS can be deployed on any device that supports the Unity Framework, encompassing the vast majority of commercially available headsets. In this work, IRIS was deployed and tested on the Meta Quest 3 and the HoloLens 2. IRIS showcased its versatility across a wide range of real-world and simulated scenarios, using current popular robot simulators such as MuJoCo, IsaacSim, CoppeliaSim, and Genesis. In addition, a user study evaluates IRIS on a data collection task for the LIBERO benchmark. The study shows that IRIS significantly outperforms the baseline in both objective and subjective metrics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03298</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03298</id><created>2025-02-05</created><authors><author><keyname>Dada</keyname><forenames>Amin</forenames></author><author><keyname>Koras</keyname><forenames>Osman Alperen</forenames></author><author><keyname>Bauer</keyname><forenames>Marie</forenames></author><author><keyname>Butler</keyname><forenames>Amanda</forenames></author><author><keyname>Smith</keyname><forenames>Kaleb E.</forenames></author><author><keyname>Kleesiek</keyname><forenames>Jens</forenames></author><author><keyname>Friedrich</keyname><forenames>Julian</forenames></author></authors><title>MeDiSumQA: Patient-Oriented Question-Answer Generation from Discharge   Letters</title><categories>cs.CL cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  While increasing patients' access to medical documents improves medical care, this benefit is limited by varying health literacy levels and complex medical terminology. Large language models (LLMs) offer solutions by simplifying medical information. However, evaluating LLMs for safe and patient-friendly text generation is difficult due to the lack of standardized evaluation resources. To fill this gap, we developed MeDiSumQA. MeDiSumQA is a dataset created from MIMIC-IV discharge summaries through an automated pipeline combining LLM-based question-answer generation with manual quality checks. We use this dataset to evaluate various LLMs on patient-oriented question-answering. Our findings reveal that general-purpose LLMs frequently surpass biomedical-adapted models, while automated metrics correlate with human judgment. By releasing MeDiSumQA on PhysioNet, we aim to advance the development of LLMs to enhance patient understanding and ultimately improve care outcomes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03302</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03302</id><created>2025-02-05</created><authors><author><keyname>Chand</keyname><forenames>Jyothi Rikhab</forenames></author><author><keyname>Jacob</keyname><forenames>Mathews</forenames></author></authors><title>MAP Image Recovery with Guarantees using Locally Convex Multi-Scale   Energy (LC-MUSE) Model</title><categories>cs.LG cs.CV eess.IV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a multi-scale deep energy model that is strongly convex in the local neighbourhood around the data manifold to represent its probability density, with application in inverse problems. In particular, we represent the negative log-prior as a multi-scale energy model parameterized by a Convolutional Neural Network (CNN). We restrict the gradient of the CNN to be locally monotone, which constrains the model as a Locally Convex Multi-Scale Energy (LC-MuSE). We use the learned energy model in image-based inverse problems, where the formulation offers several desirable properties: i) uniqueness of the solution, ii) convergence guarantees to a minimum of the inverse problem, and iii) robustness to input perturbations. In the context of parallel Magnetic Resonance (MR) image reconstruction, we show that the proposed method performs better than the state-of-the-art convex regularizers, while the performance is comparable to plug-and-play regularizers and end-to-end trained methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03304</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03304</id><created>2025-02-05</created><authors><author><keyname>Tan</keyname><forenames>Qitao</forenames></author><author><keyname>Liu</keyname><forenames>Jun</forenames></author><author><keyname>Zhan</keyname><forenames>Zheng</forenames></author><author><keyname>Ding</keyname><forenames>Caiwei</forenames></author><author><keyname>Wang</keyname><forenames>Yanzhi</forenames></author><author><keyname>Lu</keyname><forenames>Jin</forenames></author><author><keyname>Yuan</keyname><forenames>Geng</forenames></author></authors><title>Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient   Zeroth-order LLM Fine-tuning</title><categories>cs.LG cs.AI cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models (LLMs) excel across various tasks, but standard first-order (FO) fine-tuning demands considerable memory, significantly limiting real-world deployment. Recently, zeroth-order (ZO) optimization stood out as a promising memory-efficient training paradigm, avoiding backward passes and relying solely on forward passes for gradient estimation, making it attractive for resource-constrained scenarios. However, ZO method lags far behind FO method in both convergence speed and accuracy. To bridge the gap, we introduce a novel layer-wise divergence analysis that uncovers the distinct update pattern of FO and ZO optimization. Aiming to resemble the learning capacity of FO method from the findings, we propose \textbf{Di}vergence-driven \textbf{Z}eroth-\textbf{O}rder (\textbf{DiZO}) optimization. DiZO conducts divergence-driven layer adaptation by incorporating projections to ZO updates, generating diverse-magnitude updates precisely scaled to layer-wise individual optimization needs. Our results demonstrate that DiZO significantly reduces the needed iterations for convergence without sacrificing throughput, cutting training GPU hours by up to 48\% on various datasets. Moreover, DiZO consistently outperforms the representative ZO baselines in fine-tuning RoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some cases, even surpasses memory-intensive FO fine-tuning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03307</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03307</id><created>2025-02-05</created><authors><author><keyname>Wang</keyname><forenames>Yu</forenames></author><author><keyname>Sang</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>Yi</forenames></author><author><keyname>Zhang</keyname><forenames>Yiwen</forenames></author></authors><title>Intent Representation Learning with Large Language Model for   Recommendation</title><categories>cs.IR</categories><comments>11 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intent-based recommender systems have garnered significant attention for uncovering latent fine-grained preferences. Intents, as underlying factors of interactions, are crucial for improving recommendation interpretability. Most methods define intents as learnable parameters updated alongside interactions. However, existing frameworks often overlook textual information (e.g., user reviews, item descriptions), which is crucial for alleviating the sparsity of interaction intents. Exploring these multimodal intents, especially the inherent differences in representation spaces, poses two key challenges: i) How to align multimodal intents and effectively mitigate noise issues; ii) How to extract and match latent key intents across modalities. To tackle these challenges, we propose a model-agnostic framework, Intent Representation Learning with Large Language Model (IRLLRec), which leverages large language models (LLMs) to construct multimodal intents and enhance recommendations. Specifically, IRLLRec employs a dual-tower architecture to learn multimodal intent representations. Next, we propose pairwise and translation alignment to eliminate inter-modal differences and enhance robustness against noisy input features. Finally, to better match textual and interaction-based intents, we employ momentum distillation to perform teacher-student learning on fused intent representations. Empirical evaluations on three datasets show that our IRLLRec framework outperforms baselines. The implementation is available at https://github.com/wangyu0627/IRLLRec. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03309</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03309</id><created>2025-02-05</created><authors><author><keyname>Didimo</keyname><forenames>Walter</forenames></author><author><keyname>Liotta</keyname><forenames>Giuseppe</forenames></author><author><keyname>Ortali</keyname><forenames>Giacomo</forenames></author><author><keyname>Patrignani</keyname><forenames>Maurizio</forenames></author></authors><title>Optimal Orthogonal Drawings in Linear Time</title><categories>cs.CG cs.DS</categories><comments>arXiv admin note: text overlap with arXiv:1910.11782</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A planar orthogonal drawing {\Gamma} of a connected planar graph G is a geometric representation of G such that the vertices are drawn as distinct points of the plane, the edges are drawn as chains of horizontal and vertical segments, and no two edges intersect except at common end-points. A bend of {\Gamma} is a point of an edge where a horizontal and a vertical segment meet. Drawing {\Gamma} is bend-minimum if it has the minimum number of bends over all possible planar orthogonal drawings of G. Its curve complexity is the maximum number of bends per edge. In this paper we present a linear-time algorithm for the computation of planar orthogonal drawings of 3-graphs (i.e., graphs with vertex-degree at most three), that minimizes both the total number of bends and the curve complexity. The algorithm works in the so-called variable embedding setting, that is, it can choose among the exponentially many planar embeddings of the input graph. While the time complexity of minimizing the total number of bends of a planar orthogonal drawing of a 3-graph in the variable embedding settings is a long standing, widely studied, open question, the existence of an orthogonal drawing that is optimal both in the total number of bends and in the curve complexity was previously unknown. Our result combines several graph decomposition techniques, novel data-structures, and efficient approaches to re-rooting decomposition trees. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03312</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03312</id><created>2025-02-05</created><authors><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames></author></authors><title>An 'Experimental Mathematics' Approach to Stolarsky Interspersions via   Automata Theory</title><categories>cs.FL cs.DM math.NT</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We look at the Stolarsky interspersions (such as the Wythoff array) one more time, this time using tools from automata theory. These tools allow easy verification of many of the published results on these arrays, as well as proofs of new results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03313</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03313</id><created>2025-02-05</created><authors><author><keyname>Khanna</keyname><forenames>Sanjeev</forenames></author><author><keyname>Li</keyname><forenames>Huan</forenames></author><author><keyname>Putterman</keyname><forenames>Aaron</forenames></author></authors><title>Near-optimal Linear Sketches and Fully-Dynamic Algorithms for Hypergraph   Spectral Sparsification</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  A hypergraph spectral sparsifier of a hypergraph $G$ is a weighted subgraph $H$ that approximates the Laplacian of $G$ to a specified precision. Recent work has shown that similar to ordinary graphs, there exist $\widetilde{O}(n)$-size hypergraph spectral sparsifiers. However, the task of computing such sparsifiers turns out to be much more involved, and all known algorithms rely on the notion of balanced weight assignments, whose computation inherently relies on repeated, complete access to the underlying hypergraph. We introduce a significantly simpler framework for hypergraph spectral sparsification which bypasses the need to compute such weight assignments, essentially reducing hypergraph sparsification to repeated effective resistance sampling in \textit{ordinary graphs}, which are obtained by \textit{oblivious vertex-sampling} of the original hypergraph.   Our framework immediately yields a simple, new nearly-linear time algorithm for nearly-linear size spectral hypergraph sparsification. Furthermore, as a direct consequence of our framework, we obtain the first nearly-optimal algorithms in several other models of computation, namely the linear sketching, fully dynamic, and online settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03317</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03317</id><created>2025-02-05</created><authors><author><keyname>Wang</keyname><forenames>Haokun</forenames></author><author><keyname>Wang</keyname><forenames>Qianhao</forenames></author><author><keyname>Gao</keyname><forenames>Fei</forenames></author><author><keyname>Shen</keyname><forenames>Shaojie</forenames></author></authors><title>Contact-Aware Motion Planning Among Movable Objects</title><categories>cs.RO</categories><comments>11 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most existing methods for motion planning of mobile robots involve generating collision-free trajectories. However, these methods focusing solely on contact avoidance may limit the robots' locomotion and can not be applied to tasks where contact is inevitable or intentional. To address these issues, we propose a novel contact-aware motion planning (CAMP) paradigm for robotic systems. Our approach incorporates contact between robots and movable objects as complementarity constraints in optimization-based trajectory planning. By leveraging augmented Lagrangian methods (ALMs), we efficiently solve the optimization problem with complementarity constraints, producing spatial-temporal optimal trajectories of the robots. Simulations demonstrate that, compared to the state-of-the-art method, our proposed CAMP method expands the reachable space of mobile robots, resulting in a significant improvement in the success rate of two types of fundamental tasks: navigation among movable objects (NAMO) and rearrangement of movable objects (RAMO). Real-world experiments show that the trajectories generated by our proposed method are feasible and quickly deployed in different tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03320</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03320</id><created>2025-02-05</created><authors><author><keyname>Middelburg</keyname><forenames>C. A.</forenames></author></authors><title>Complementing an imperative process algebra with a rely/guarantee logic</title><categories>cs.LO</categories><comments>30 pages, Sections 2 and 3 of this paper are abridged versions of   Sections 2 and 3 of arXiv:1906.04491</comments><acm-class>D.1.3; D.2.4; F.1.2; F.3.1</acm-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper concerns the relation between imperative process algebra and rely/guarantee logic. An imperative process algebra is complemented by a rely/guarantee logic that can be used to reason about how data change in the course of a process. The imperative process algebra used is the extension of ACP (Algebra of Communicating Processes) that is used earlier in a paper about the relation between imperative process algebra and Hoare logic. A complementing rely/guarantee logic that concerns judgments of partial correctness is treated in detail. The adaptation of this logic to weak and strong total correctness is also addressed. A simple example is given that suggests that a rely/guarantee logic is more suitable as a complementing logic than a Hoare logic if interfering parallel processes are involved. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03321</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03321</id><created>2025-02-05</created><authors><author><keyname>Han</keyname><forenames>Sangjun</forenames></author><author><keyname>Hur</keyname><forenames>Taeil</forenames></author><author><keyname>Hur</keyname><forenames>Youngmi</forenames></author><author><keyname>Lee</keyname><forenames>Kathy Sangkyung</forenames></author><author><keyname>Lee</keyname><forenames>Myungyoon</forenames></author><author><keyname>Lim</keyname><forenames>Hyojae</forenames></author></authors><title>Simplifying Formal Proof-Generating Models with ChatGPT and Basic   Searching Techniques</title><categories>cs.LO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The challenge of formal proof generation has a rich history, but with modern techniques, we may finally be at the stage of making actual progress in real-life mathematical problems. This paper explores the integration of ChatGPT and basic searching techniques to simplify generating formal proofs, with a particular focus on the miniF2F dataset. We demonstrate how combining a large language model like ChatGPT with a formal language such as Lean, which has the added advantage of being verifiable, enhances the efficiency and accessibility of formal proof generation. Despite its simplicity, our best-performing Lean-based model surpasses all known benchmarks with a 31.15% pass rate. We extend our experiments to include other datasets and employ alternative language models, showcasing our models' comparable performance in diverse settings and allowing for a more nuanced analysis of our results. Our findings offer insights into AI-assisted formal proof generation, suggesting a promising direction for future research in formal mathematical proof. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03322</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03322</id><created>2025-02-05</created><authors><author><keyname>Zappon</keyname><forenames>Elena</forenames></author><author><keyname>Azzolin</keyname><forenames>Luca</forenames></author><author><keyname>Gsell</keyname><forenames>Matthias A. F.</forenames></author><author><keyname>Thaler</keyname><forenames>Franz</forenames></author><author><keyname>Prassl</keyname><forenames>Anton J.</forenames></author><author><keyname>Arnold</keyname><forenames>Robert</forenames></author><author><keyname>Gillette</keyname><forenames>Karli</forenames></author><author><keyname>Kariman</keyname><forenames>Mohammadreza</forenames></author><author><keyname>Manninger-Wünscher</keyname><forenames>Martin</forenames></author><author><keyname>Scherr</keyname><forenames>Daniel</forenames></author><author><keyname>Neic</keyname><forenames>Aurel</forenames></author><author><keyname>Urschler</keyname><forenames>Martin</forenames></author><author><keyname>Augustin</keyname><forenames>Christoph M.</forenames></author><author><keyname>Vigmond</keyname><forenames>Edward J.</forenames></author><author><keyname>Plank</keyname><forenames>Gernot</forenames></author></authors><title>An efficient end-to-end computational framework for the generation of   ECG calibrated volumetric models of human atrial electrophysiology</title><categories>math.NA cs.CE cs.NA q-bio.TO</categories><comments>39 pages, 13 figures, 11 tables</comments><msc-class>92C50, 92C55, 92C30, 35Q92</msc-class><acm-class>G.1.10; I.6.4; I.6.5; J.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational models of atrial electrophysiology (EP) are increasingly utilized for applications such as the development of advanced mapping systems, personalized clinical therapy planning, and the generation of virtual cohorts and digital twins. These models have the potential to establish robust causal links between simulated in silico behaviors and observed human atrial EP, enabling safer, cost-effective, and comprehensive exploration of atrial dynamics. However, current state-of-the-art approaches lack the fidelity and scalability required for regulatory-grade applications, particularly in creating high-quality virtual cohorts or patient-specific digital twins. Challenges include anatomically accurate model generation, calibration to sparse and uncertain clinical data, and computational efficiency within a streamlined workflow. This study addresses these limitations by introducing novel methodologies integrated into an automated end-to-end workflow for generating high-fidelity digital twin snapshots and virtual cohorts of atrial EP. These innovations include: (i) automated multi-scale generation of volumetric biatrial models with detailed anatomical structures and fiber architecture; (ii) a robust method for defining space-varying atrial parameter fields; (iii) a parametric approach for modeling inter-atrial conduction pathways; and (iv) an efficient forward EP model for high-fidelity electrocardiogram computation. We evaluated this workflow on a cohort of 50 atrial fibrillation patients, producing high-quality meshes suitable for reaction-eikonal and reaction-diffusion models and demonstrating the ability to simulate atrial ECGs under parametrically controlled conditions. These advancements represent a critical step toward scalable, precise, and clinically applicable digital twin models and virtual cohorts, enabling enhanced patient-specific predictions and therapeutic planning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03323</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03323</id><created>2025-02-05</created><authors><author><keyname>Abbas</keyname><forenames>Momin</forenames></author><author><keyname>Azmat</keyname><forenames>Muneeza</forenames></author><author><keyname>Horesh</keyname><forenames>Raya</forenames></author><author><keyname>Yurochkin</keyname><forenames>Mikhail</forenames></author></authors><title>Out-of-Distribution Detection using Synthetic Data Generation</title><categories>cs.CL cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Distinguishing in- and out-of-distribution (OOD) inputs is crucial for reliable deployment of classification systems. However, OOD data is typically unavailable or difficult to collect, posing a significant challenge for accurate OOD detection. In this work, we present a method that harnesses the generative capabilities of Large Language Models (LLMs) to create high-quality synthetic OOD proxies, eliminating the dependency on any external OOD data source. We study the efficacy of our method on classical text classification tasks such as toxicity detection and sentiment classification as well as classification tasks arising in LLM development and deployment, such as training a reward model for RLHF and detecting misaligned generations. Extensive experiments on nine InD-OOD dataset pairs and various model sizes show that our approach dramatically lowers false positive rates (achieving a perfect zero in some cases) while maintaining high accuracy on in-distribution tasks, outperforming baseline methods by a significant margin. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03325</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03325</id><created>2025-02-05</created><authors><author><keyname>Chen</keyname><forenames>Qiguang</forenames></author><author><keyname>Qin</keyname><forenames>Libo</forenames></author><author><keyname>Liu</keyname><forenames>Jinhao</forenames></author><author><keyname>Peng</keyname><forenames>Dengyun</forenames></author><author><keyname>Wang</keyname><forenames>Jiaqi</forenames></author><author><keyname>Hu</keyname><forenames>Mengkang</forenames></author><author><keyname>Chen</keyname><forenames>Zhi</forenames></author><author><keyname>Che</keyname><forenames>Wanxiang</forenames></author><author><keyname>Liu</keyname><forenames>Ting</forenames></author></authors><title>ECM: A Unified Electronic Circuit Model for Explaining the Emergence of   In-Context Learning and Chain-of-Thought in Large Language Model</title><categories>cs.CL cs.AI</categories><comments>Manuscript</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Recent advancements in large language models (LLMs) have led to significant successes across various applications, where the most noticeable is to a series of emerging capabilities, particularly in the areas of In-Context Learning (ICL) and Chain-of-Thought (CoT). To better understand and control model performance, many studies have begun investigating the underlying causes of these phenomena and their impact on task outcomes. However, existing explanatory frameworks predominantly focus on isolating and explaining ICL and CoT independently, leading to an incomplete understanding of their combined influence on model performance. To address this gap, we propose the Electronic Circuit Model (ECM), which provides a foundation for developing scalable, learnable policies and improving the management of AI-generated content. Specifically, ECM conceptualizes model behavior as an electronic circuit: ICL is represented as semantic magnetic field to providing an additional voltage following Faraday's Law, while CoT is modeled as series resistors to constrain the model output performance following Ohm's Law. Experimental results demonstrate that the ECM effectively predicts and explains LLM performance across a variety of prompting strategies. Furthermore, we apply ECM to advanced reasoning strategy optimization on a series of tasks, such as the International Olympiad in Informatics (IOI) and the International Mathematical Olympiad (IMO), achieving competitive performance that surpasses nearly 80% of top human competitors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03327</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03327</id><created>2025-02-05</created><authors><author><keyname>Kratsios</keyname><forenames>Anastasis</forenames></author><author><keyname>Furuya</keyname><forenames>Takashi</forenames></author></authors><title>Is In-Context Universality Enough? MLPs are Also Universal In-Context</title><categories>stat.ML cs.LG cs.NA cs.NE math.NA math.PR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The success of transformers is often linked to their ability to perform in-context learning. Recent work shows that transformers are universal in context, capable of approximating any real-valued continuous function of a context (a probability measure over $\mathcal{X}\subseteq \mathbb{R}^d$) and a query $x\in \mathcal{X}$. This raises the question: Does in-context universality explain their advantage over classical models? We answer this in the negative by proving that MLPs with trainable activation functions are also universal in-context. This suggests the transformer's success is likely due to other factors like inductive bias or training stability. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03330</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03330</id><created>2025-02-05</created><authors><author><keyname>Garg</keyname><forenames>Aryan</forenames></author><author><keyname>Jiang</keyname><forenames>Yue</forenames></author><author><keyname>Oulasvirta</keyname><forenames>Antti</forenames></author></authors><title>Controllable GUI Exploration</title><categories>cs.HC cs.AI cs.CV cs.GR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  During the early stages of interface design, designers need to produce multiple sketches to explore a design space. Design tools often fail to support this critical stage, because they insist on specifying more details than necessary. Although recent advances in generative AI have raised hopes of solving this issue, in practice they fail because expressing loose ideas in a prompt is impractical. In this paper, we propose a diffusion-based approach to the low-effort generation of interface sketches. It breaks new ground by allowing flexible control of the generation process via three types of inputs: A) prompts, B) wireframes, and C) visual flows. The designer can provide any combination of these as input at any level of detail, and will get a diverse gallery of low-fidelity solutions in response. The unique benefit is that large design spaces can be explored rapidly with very little effort in input-specification. We present qualitative results for various combinations of input specifications. Additionally, we demonstrate that our model aligns more accurately with these specifications than other models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03332</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03332</id><created>2025-02-05</created><authors><author><keyname>Janati</keyname><forenames>Yazid</forenames></author><author><keyname>Moufad</keyname><forenames>Badr</forenames></author><author><keyname>Qassime</keyname><forenames>Mehdi Abou El</forenames></author><author><keyname>Durmus</keyname><forenames>Alain</forenames></author><author><keyname>Moulines</keyname><forenames>Eric</forenames></author><author><keyname>Olsson</keyname><forenames>Jimmy</forenames></author></authors><title>A Mixture-Based Framework for Guiding Diffusion Models</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Denoising diffusion models have driven significant progress in the field of Bayesian inverse problems. Recent approaches use pre-trained diffusion models as priors to solve a wide range of such problems, only leveraging inference-time compute and thereby eliminating the need to retrain task-specific models on the same dataset. To approximate the posterior of a Bayesian inverse problem, a diffusion model samples from a sequence of intermediate posterior distributions, each with an intractable likelihood function. This work proposes a novel mixture approximation of these intermediate distributions. Since direct gradient-based sampling of these mixtures is infeasible due to intractable terms, we propose a practical method based on Gibbs sampling. We validate our approach through extensive experiments on image inverse problems, utilizing both pixel- and latent-space diffusion priors, as well as on source separation with an audio diffusion model. The code is available at https://www.github.com/badr-moufad/mgdm </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03333</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03333</id><created>2025-02-05</created><authors><author><keyname>Deperrois</keyname><forenames>Nicolas</forenames></author><author><keyname>Matsuo</keyname><forenames>Hidetoshi</forenames></author><author><keyname>Ruipérez-Campillo</keyname><forenames>Samuel</forenames></author><author><keyname>Vandenhirtz</keyname><forenames>Moritz</forenames></author><author><keyname>Laguna</keyname><forenames>Sonia</forenames></author><author><keyname>Ryser</keyname><forenames>Alain</forenames></author><author><keyname>Fujimoto</keyname><forenames>Koji</forenames></author><author><keyname>Nishio</keyname><forenames>Mizuho</forenames></author><author><keyname>Sutter</keyname><forenames>Thomas M.</forenames></author><author><keyname>Vogt</keyname><forenames>Julia E.</forenames></author><author><keyname>Kluckert</keyname><forenames>Jonas</forenames></author><author><keyname>Frauenfelder</keyname><forenames>Thomas</forenames></author><author><keyname>Blüthgen</keyname><forenames>Christian</forenames></author><author><keyname>Nooralahzadeh</keyname><forenames>Farhad</forenames></author><author><keyname>Krauthammer</keyname><forenames>Michael</forenames></author></authors><title>RadVLM: A Multitask Conversational Vision-Language Model for Radiology</title><categories>cs.CV cs.AI</categories><comments>21 pages, 15 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The widespread use of chest X-rays (CXRs), coupled with a shortage of radiologists, has driven growing interest in automated CXR analysis and AI-assisted reporting. While existing vision-language models (VLMs) show promise in specific tasks such as report generation or abnormality detection, they often lack support for interactive diagnostic capabilities. In this work we present RadVLM, a compact, multitask conversational foundation model designed for CXR interpretation. To this end, we curate a large-scale instruction dataset comprising over 1 million image-instruction pairs containing both single-turn tasks -- such as report generation, abnormality classification, and visual grounding -- and multi-turn, multi-task conversational interactions. After fine-tuning RadVLM on this instruction dataset, we evaluate it across different tasks along with re-implemented baseline VLMs. Our results show that RadVLM achieves state-of-the-art performance in conversational capabilities and visual grounding while remaining competitive in other radiology tasks. Ablation studies further highlight the benefit of joint training across multiple tasks, particularly for scenarios with limited annotated data. Together, these findings highlight the potential of RadVLM as a clinically relevant AI assistant, providing structured CXR interpretation and conversational capabilities to support more effective and accessible diagnostic workflows. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03335</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03335</id><created>2025-02-05</created><authors><author><keyname>Wu</keyname><forenames>Haotian</forenames></author><author><keyname>Chen</keyname><forenames>Gongpu</forenames></author><author><keyname>Gündüz</keyname><forenames>Deniz</forenames></author></authors><title>Actions Speak Louder Than Words: Rate-Reward Trade-off in Markov   Decision Processes</title><categories>cs.IT math.IT</categories><msc-class>94</msc-class><acm-class>E.4</acm-class><journal-ref>The International Conference on Learning Representations (ICLR)   2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The impact of communication on decision-making systems has been extensively studied under the assumption of dedicated communication channels. We instead consider communicating through actions, where the message is embedded into the actions of an agent which interacts with the environment in a Markov decision process (MDP) framework. We conceptualize the MDP environment as a finite-state channel (FSC), where the actions of the agent serve as the channel input, while the states of the MDP observed by another agent (i.e., receiver) serve as the channel output. Here, we treat the environment as a communication channel over which the agent communicates through its actions, while at the same time, trying to maximize its reward. We first characterize the optimal information theoretic trade-off between the average reward and the rate of reliable communication in the infinite-horizon regime. Then, we propose a novel framework to design a joint control/coding policy, termed \textit{Act2Comm}, which seamlessly embeds messages into actions. From a communication perspective, \textit{Act2Comm} functions as a learning-based channel coding scheme for non-differentiable FSCs under input-output constraints. From a control standpoint, \textit{Act2Comm} learns an MDP policy that incorporates communication capabilities, though at the cost of some control performance. Overall, \textit{Act2Comm} effectively balances the dual objectives of control and communication in this environment. Experimental results validate \textit{Act2Comm}'s capability to enable reliable communication while maintaining a certain level of control performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03338</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03338</id><created>2025-02-05</created><authors><author><keyname>Katanic</keyname><forenames>Milos</forenames></author><author><keyname>Guo</keyname><forenames>Yi</forenames></author><author><keyname>Lygeros</keyname><forenames>John</forenames></author><author><keyname>Hug</keyname><forenames>Gabriela</forenames></author></authors><title>Optimal PMU Placement for Kalman Filtering of DAE Power System Models</title><categories>eess.SY cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal sensor placement is essential for minimizing costs and ensuring accurate state estimation in power systems. This paper introduces a novel method for optimal sensor placement for dynamic state estimation of power systems modeled by differential-algebraic equations. The method identifies optimal sensor locations by minimizing the steady-state covariance matrix of the Kalman filter, thus minimizing the error of joint differential and algebraic state estimation. The problem is reformulated as a mixed-integer semidefinite program and effectively solved using off-the-shelf numerical solvers. Numerical results demonstrate the merits of the proposed approach by benchmarking its performance in phasor measurement unit placement in comparison to greedy algorithms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03340</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03340</id><created>2025-02-05</created><authors><author><keyname>Licciardi</keyname><forenames>Alessandro</forenames></author><author><keyname>Leo</keyname><forenames>Davide</forenames></author><author><keyname>Faní</keyname><forenames>Eros</forenames></author><author><keyname>Caputo</keyname><forenames>Barbara</forenames></author><author><keyname>Ciccone</keyname><forenames>Marco</forenames></author></authors><title>Interaction-Aware Gaussian Weighting for Clustered Federated Learning</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Federated Learning (FL) emerged as a decentralized paradigm to train models while preserving privacy. However, conventional FL struggles with data heterogeneity and class imbalance, which degrade model performance. Clustered FL balances personalization and decentralized training by grouping clients with analogous data distributions, enabling improved accuracy while adhering to privacy constraints. This approach effectively mitigates the adverse impact of heterogeneity in FL. In this work, we propose a novel clustered FL method, FedGWC (Federated Gaussian Weighting Clustering), which groups clients based on their data distribution, allowing training of a more robust and personalized model on the identified clusters. FedGWC identifies homogeneous clusters by transforming individual empirical losses to model client interactions with a Gaussian reward mechanism. Additionally, we introduce the Wasserstein Adjusted Score, a new clustering metric for FL to evaluate cluster cohesion with respect to the individual class distribution. Our experiments on benchmark datasets show that FedGWC outperforms existing FL algorithms in cluster quality and classification accuracy, validating the efficacy of our approach. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03341</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03341</id><created>2025-02-05</created><authors><author><keyname>Leisenberger</keyname><forenames>Harald</forenames></author><author><keyname>Pernkopf</keyname><forenames>Franz</forenames></author></authors><title>Adaptive Variational Inference in Probabilistic Graphical Models: Beyond   Bethe, Tree-Reweighted, and Convex Free Energies</title><categories>stat.ML cs.AI cs.LG</categories><comments>This work has been submitted to the Conference on Uncertainty in   Artificial Intelligence (UAI) 2025 for possible publication</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Variational inference in probabilistic graphical models aims to approximate fundamental quantities such as marginal distributions and the partition function. Popular approaches are the Bethe approximation, tree-reweighted, and other types of convex free energies. These approximations are efficient but can fail if the model is complex and highly interactive. In this work, we analyze two classes of approximations that include the above methods as special cases: first, if the model parameters are changed; and second, if the entropy approximation is changed. We discuss benefits and drawbacks of either approach, and deduce from this analysis how a free energy approximation should ideally be constructed. Based on our observations, we propose approximations that automatically adapt to a given model and demonstrate their effectiveness for a range of difficult problems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03346</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03346</id><created>2025-02-05</created><authors><author><keyname>Yang</keyname><forenames>Elvin</forenames></author><author><keyname>Mavrogiannis</keyname><forenames>Christoforos</forenames></author></authors><title>Implicit Communication in Human-Robot Collaborative Transport</title><categories>cs.RO</categories><comments>Preprint. Accepted to HRI 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We focus on human-robot collaborative transport, in which a robot and a user collaboratively move an object to a goal pose. In the absence of explicit communication, this problem is challenging because it demands tight implicit coordination between two heterogeneous agents, who have very different sensing, actuation, and reasoning capabilities. Our key insight is that the two agents can coordinate fluently by encoding subtle, communicative signals into actions that affect the state of the transported object. To this end, we design an inference mechanism that probabilistically maps observations of joint actions executed by the two agents to a set of joint strategies of workspace traversal. Based on this mechanism, we define a cost representing the human's uncertainty over the unfolding traversal strategy and introduce it into a model predictive controller that balances between uncertainty minimization and efficiency maximization. We deploy our framework on a mobile manipulator (Hello Robot Stretch) and evaluate it in a within-subjects lab study (N=24). We show that our framework enables greater team performance and empowers the robot to be perceived as a significantly more fluent and competent partner compared to baselines lacking a communicative mechanism. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03347</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03347</id><created>2025-02-05</created><authors><author><keyname>Busso</keyname><forenames>Matteo</forenames></author><author><keyname>Bontempelli</keyname><forenames>Andrea</forenames></author><author><keyname>Malcotti</keyname><forenames>Leonardo Javier</forenames></author><author><keyname>Meegahapola</keyname><forenames>Lakmal</forenames></author><author><keyname>Kun</keyname><forenames>Peter</forenames></author><author><keyname>Diwakar</keyname><forenames>Shyam</forenames></author><author><keyname>Nutakki</keyname><forenames>Chaitanya</forenames></author><author><keyname>Britez</keyname><forenames>Marcelo Dario Rodas</forenames></author><author><keyname>Xu</keyname><forenames>Hao</forenames></author><author><keyname>Song</keyname><forenames>Donglei</forenames></author><author><keyname>Correa</keyname><forenames>Salvador Ruiz</forenames></author><author><keyname>Mendoza-Lara</keyname><forenames>Andrea-Rebeca</forenames></author><author><keyname>Gaskell</keyname><forenames>George</forenames></author><author><keyname>Stares</keyname><forenames>Sally</forenames></author><author><keyname>Bidoglia</keyname><forenames>Miriam</forenames></author><author><keyname>Ganbold</keyname><forenames>Amarsanaa</forenames></author><author><keyname>Chagnaa</keyname><forenames>Altangerel</forenames></author><author><keyname>Cernuzzi</keyname><forenames>Luca</forenames></author><author><keyname>Hume</keyname><forenames>Alethia</forenames></author><author><keyname>Chenu-Abente</keyname><forenames>Ronald</forenames></author><author><keyname>Asiku</keyname><forenames>Roy Alia</forenames></author><author><keyname>Kayongo</keyname><forenames>Ivan</forenames></author><author><keyname>Gatica-Perez</keyname><forenames>Daniel</forenames></author><author><keyname>de Götzen</keyname><forenames>Amalia</forenames></author><author><keyname>Bison</keyname><forenames>Ivano</forenames></author><author><keyname>Giunchiglia</keyname><forenames>Fausto</forenames></author></authors><title>DiversityOne: A Multi-Country Smartphone Sensor Dataset for Everyday   Life Behavior Modeling</title><categories>cs.CY cs.SI</categories><doi>10.1145/3712289</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Understanding everyday life behavior of young adults through personal devices, e.g., smartphones and smartwatches, is key for various applications, from enhancing the user experience in mobile apps to enabling appropriate interventions in digital health apps. Towards this goal, previous studies have relied on datasets combining passive sensor data with human-provided annotations or self-reports. However, many existing datasets are limited in scope, often focusing on specific countries primarily in the Global North, involving a small number of participants, or using a limited range of pre-processed sensors. These limitations restrict the ability to capture cross-country variations of human behavior, including the possibility of studying model generalization, and robustness. To address this gap, we introduce DiversityOne, a dataset which spans eight countries (China, Denmark, India, Italy, Mexico, Mongolia, Paraguay, and the United Kingdom) and includes data from 782 college students over four weeks. DiversityOne contains data from 26 smartphone sensor modalities and 350K+ self-reports. As of today, it is one of the largest and most diverse publicly available datasets, while featuring extensive demographic and psychosocial survey data. DiversityOne opens the possibility of studying important research problems in ubiquitous computing, particularly in domain adaptation and generalization across countries, all research areas so far largely underexplored because of the lack of adequate datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03349</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03349</id><created>2025-02-05</created><authors><author><keyname>Cusumano-Towner</keyname><forenames>Marco</forenames></author><author><keyname>Hafner</keyname><forenames>David</forenames></author><author><keyname>Hertzberg</keyname><forenames>Alex</forenames></author><author><keyname>Huval</keyname><forenames>Brody</forenames></author><author><keyname>Petrenko</keyname><forenames>Aleksei</forenames></author><author><keyname>Vinitsky</keyname><forenames>Eugene</forenames></author><author><keyname>Wijmans</keyname><forenames>Erik</forenames></author><author><keyname>Killian</keyname><forenames>Taylor</forenames></author><author><keyname>Bowers</keyname><forenames>Stuart</forenames></author><author><keyname>Sener</keyname><forenames>Ozan</forenames></author><author><keyname>Krähenbühl</keyname><forenames>Philipp</forenames></author><author><keyname>Koltun</keyname><forenames>Vladlen</forenames></author></authors><title>Robust Autonomy Emerges from Self-Play</title><categories>cs.LG cs.AI cs.RO</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Self-play has powered breakthroughs in two-player and multi-player games. Here we show that self-play is a surprisingly effective strategy in another domain. We show that robust and naturalistic driving emerges entirely from self-play in simulation at unprecedented scale -- 1.6~billion~km of driving. This is enabled by Gigaflow, a batched simulator that can synthesize and train on 42 years of subjective driving experience per hour on a single 8-GPU node. The resulting policy achieves state-of-the-art performance on three independent autonomous driving benchmarks. The policy outperforms the prior state of the art when tested on recorded real-world scenarios, amidst human drivers, without ever seeing human data during training. The policy is realistic when assessed against human references and achieves unprecedented robustness, averaging 17.5 years of continuous driving between incidents in simulation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03350</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03350</id><created>2025-02-05</created><authors><author><keyname>Li</keyname><forenames>Ziyan</forenames></author><author><keyname>Hiratani</keyname><forenames>Naoki</forenames></author></authors><title>Optimal Task Order for Continual Learning of Multiple Tasks</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Continual learning of multiple tasks remains a major challenge for neural networks. Here, we investigate how task order influences continual learning and propose a strategy for optimizing it. Leveraging a linear teacher-student model with latent factors, we derive an analytical expression relating task similarity and ordering to learning performance. Our analysis reveals two principles that hold under a wide parameter range: (1) tasks should be arranged from the least representative to the most typical, and (2) adjacent tasks should be dissimilar. We validate these rules on both synthetic data and real-world image classification datasets (Fashion-MNIST, CIFAR-10, CIFAR-100), demonstrating consistent performance improvements in both multilayer perceptrons and convolutional neural networks. Our work thus presents a generalizable framework for task-order optimization in task-incremental continual learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03356</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03356</id><created>2025-02-05</created><authors><author><keyname>Sun</keyname><forenames>Max Muchen</forenames></author><author><keyname>Trautman</keyname><forenames>Pete</forenames></author><author><keyname>Murphey</keyname><forenames>Todd</forenames></author></authors><title>Inverse Mixed Strategy Games with Generative Trajectory Models</title><categories>cs.RO</categories><comments>Accepted to ICRA 2025. 8 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Game-theoretic models are effective tools for modeling multi-agent interactions, especially when robots need to coordinate with humans. However, applying these models requires inferring their specifications from observed behaviors -- a challenging task known as the inverse game problem. Existing inverse game approaches often struggle to account for behavioral uncertainty and measurement noise, and leverage both offline and online data. To address these limitations, we propose an inverse game method that integrates a generative trajectory model into a differentiable mixed-strategy game framework. By representing the mixed strategy with a conditional variational autoencoder (CVAE), our method can infer high-dimensional, multi-modal behavior distributions from noisy measurements while adapting in real-time to new observations. We extensively evaluate our method in a simulated navigation benchmark, where the observations are generated by an unknown game model. Despite the model mismatch, our method can infer Nash-optimal actions comparable to those of the ground-truth model and the oracle inverse game baseline, even in the presence of uncertain agent objectives and noisy measurements. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03358</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03358</id><created>2025-02-05</created><authors><author><keyname>Xia</keyname><forenames>Menglin</forenames></author><author><keyname>Ruehle</keyname><forenames>Victor</forenames></author><author><keyname>Rajmohan</keyname><forenames>Saravan</forenames></author><author><keyname>Shokri</keyname><forenames>Reza</forenames></author></authors><title>Minerva: A Programmable Memory Test Benchmark for Language Models</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  How effectively can LLM-based AI assistants utilize their memory (context) to perform various tasks? Traditional data benchmarks, which are often manually crafted, suffer from several limitations: they are static, susceptible to overfitting, difficult to interpret, and lack actionable insights--failing to pinpoint the specific capabilities a model lacks when it does not pass a test. In this paper, we present a framework for automatically generating a comprehensive set of tests to evaluate models' abilities to use their memory effectively. Our framework extends the range of capability tests beyond the commonly explored (passkey, key-value, needle in the haystack) search, a dominant focus in the literature. Specifically, we evaluate models on atomic tasks such as searching, recalling, editing, matching, comparing information in context memory, and performing basic operations when inputs are structured into distinct blocks, simulating real-world data. Additionally, we design composite tests to investigate the models' ability to maintain state while operating on memory. Our benchmark enables an interpretable, detailed assessment of memory capabilities of LLMs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03359</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03359</id><created>2025-02-05</created><authors><author><keyname>Rabinowitz</keyname><forenames>Ryan</forenames></author><author><keyname>Cruz</keyname><forenames>Steve</forenames></author><author><keyname>Günther</keyname><forenames>Manuel</forenames></author><author><keyname>Boult</keyname><forenames>Terrance E.</forenames></author></authors><title>GHOST: Gaussian Hypothesis Open-Set Technique</title><categories>cs.CV cs.AI cs.LG</categories><comments>Accepted at AAAI Conference on Artificial Intelligence 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Evaluations of large-scale recognition methods typically focus on overall performance. While this approach is common, it often fails to provide insights into performance across individual classes, which can lead to fairness issues and misrepresentation. Addressing these gaps is crucial for accurately assessing how well methods handle novel or unseen classes and ensuring a fair evaluation. To address fairness in Open-Set Recognition (OSR), we demonstrate that per-class performance can vary dramatically. We introduce Gaussian Hypothesis Open Set Technique (GHOST), a novel hyperparameter-free algorithm that models deep features using class-wise multivariate Gaussian distributions with diagonal covariance matrices. We apply Z-score normalization to logits to mitigate the impact of feature magnitudes that deviate from the model's expectations, thereby reducing the likelihood of the network assigning a high score to an unknown sample. We evaluate GHOST across multiple ImageNet-1K pre-trained deep networks and test it with four different unknown datasets. Using standard metrics such as AUOSCR, AUROC and FPR95, we achieve statistically significant improvements, advancing the state-of-the-art in large-scale OSR. Source code is provided online. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03360</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03360</id><created>2025-02-05</created><authors><author><keyname>Arberet</keyname><forenames>Simon</forenames></author><author><keyname>Ghesu</keyname><forenames>Florin C.</forenames></author><author><keyname>Gao</keyname><forenames>Riqiang</forenames></author><author><keyname>Kraus</keyname><forenames>Martin</forenames></author><author><keyname>Sackett</keyname><forenames>Jonathan</forenames></author><author><keyname>Kuusela</keyname><forenames>Esa</forenames></author><author><keyname>Kamen</keyname><forenames>Ali</forenames></author></authors><title>A Beam's Eye View to Fluence Maps 3D Network for Ultra Fast VMAT   Radiotherapy Planning</title><categories>eess.IV cs.AI physics.med-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Volumetric Modulated Arc Therapy (VMAT) revolutionizes cancer treatment by precisely delivering radiation while sparing healthy tissues. Fluence maps generation, crucial in VMAT planning, traditionally involves complex and iterative, and thus time consuming processes. These fluence maps are subsequently leveraged for leaf-sequence. The deep-learning approach presented in this article aims to expedite this by directly predicting fluence maps from patient data. We developed a 3D network which we trained in a supervised way using a combination of L1 and L2 losses, and RT plans generated by Eclipse and from the REQUITE dataset, taking the RT dose map as input and the fluence maps computed from the corresponding RT plans as target. Our network predicts jointly the 180 fluence maps corresponding to the 180 control points (CP) of single arc VMAT plans. In order to help the network, we pre-process the input dose by computing the projections of the 3D dose map to the beam's eye view (BEV) of the 180 CPs, in the same coordinate system as the fluence maps. We generated over 2000 VMAT plans using Eclipse to scale up the dataset size. Additionally, we evaluated various network architectures and analyzed the impact of increasing the dataset size. We are measuring the performance in the 2D fluence maps domain using image metrics (PSNR, SSIM), as well as in the 3D dose domain using the dose-volume histogram (DVH) on a validation dataset. The network inference, which does not include the data loading and processing, is less than 20ms. Using our proposed 3D network architecture as well as increasing the dataset size using Eclipse improved the fluence map reconstruction performance by approximately 8 dB in PSNR compared to a U-Net architecture trained on the original REQUITE dataset. The resulting DVHs are very close to the one of the input target dose. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03364</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03364</id><created>2025-02-05</created><authors><author><keyname>Hoddes</keyname><forenames>Tom</forenames></author><author><keyname>Bijamov</keyname><forenames>Alex</forenames></author><author><keyname>Joshi</keyname><forenames>Saket</forenames></author><author><keyname>Roggen</keyname><forenames>Daniel</forenames></author><author><keyname>Etemad</keyname><forenames>Ali</forenames></author><author><keyname>Harle</keyname><forenames>Robert</forenames></author><author><keyname>Racz</keyname><forenames>David</forenames></author></authors><title>Scaling laws in wearable human activity recognition</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Many deep architectures and self-supervised pre-training techniques have been proposed for human activity recognition (HAR) from wearable multimodal sensors. Scaling laws have the potential to help move towards more principled design by linking model capacity with pre-training data volume. Yet, scaling laws have not been established for HAR to the same extent as in language and vision. By conducting an exhaustive grid search on both amount of pre-training data and Transformer architectures, we establish the first known scaling laws for HAR. We show that pre-training loss scales with a power law relationship to amount of data and parameter count and that increasing the number of users in a dataset results in a steeper improvement in performance than increasing data per user, indicating that diversity of pre-training data is important, which contrasts to some previously reported findings in self-supervised HAR. We show that these scaling laws translate to downstream performance improvements on three HAR benchmark datasets of postures, modes of locomotion and activities of daily living: UCI HAR and WISDM Phone and WISDM Watch. Finally, we suggest some previously published works should be revisited in light of these scaling laws with more adequate model capacities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03365</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03365</id><created>2025-02-05</created><authors><author><keyname>Iannone</keyname><forenames>Emanuele</forenames></author><author><keyname>Bui</keyname><forenames>Quang-Cuong</forenames></author><author><keyname>Scandariato</keyname><forenames>Riccardo</forenames></author></authors><title>A Match Made in Heaven? Matching Test Cases and Vulnerabilities With the   VUTECO Approach</title><categories>cs.SE cs.CR cs.LG</categories><comments>This work was partially supported by EU-funded project Sec4AI4Sec   (grant no. 101120393)</comments><acm-class>D.2.5; D.2.7</acm-class><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Software vulnerabilities are commonly detected via static analysis, penetration testing, and fuzzing. They can also be found by running unit tests - so-called vulnerability-witnessing tests - that stimulate the security-sensitive behavior with crafted inputs. Developing such tests is difficult and time-consuming; thus, automated data-driven approaches could help developers intercept vulnerabilities earlier. However, training and validating such approaches require a lot of data, which is currently scarce. This paper introduces VUTECO, a deep learning-based approach for collecting instances of vulnerability-witnessing tests from Java repositories. VUTECO carries out two tasks: (1) the "Finding" task to determine whether a test case is security-related, and (2) the "Matching" task to relate a test case to the exact vulnerability it is witnessing. VUTECO successfully addresses the Finding task, achieving perfect precision and 0.83 F0.5 score on validated test cases in VUL4J and returning 102 out of 145 (70%) correct security-related test cases from 244 open-source Java projects. Despite showing sufficiently good performance for the Matching task - i.e., 0.86 precision and 0.68 F0.5 score - VUTECO failed to retrieve any valid match in the wild. Nevertheless, we observed that in almost all of the matches, the test case was still security-related despite being matched to the wrong vulnerability. In the end, VUTECO can help find vulnerability-witnessing tests, though the matching with the right vulnerability is yet to be solved; the findings obtained lay the stepping stone for future research on the matter. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03366</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03366</id><created>2025-02-05</created><authors><author><keyname>Mucsányi</keyname><forenames>Bálint</forenames></author><author><keyname>Da Costa</keyname><forenames>Nathaël</forenames></author><author><keyname>Hennig</keyname><forenames>Philipp</forenames></author></authors><title>Rethinking Approximate Gaussian Inference in Classification</title><categories>cs.LG stat.ML</categories><comments>29 pages, 15 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In classification tasks, softmax functions are ubiquitously used as output activations to produce predictive probabilities. Such outputs only capture aleatoric uncertainty. To capture epistemic uncertainty, approximate Gaussian inference methods have been proposed, which output Gaussian distributions over the logit space. Predictives are then obtained as the expectations of the Gaussian distributions pushed forward through the softmax. However, such softmax Gaussian integrals cannot be solved analytically, and Monte Carlo (MC) approximations can be costly and noisy. We propose a simple change in the learning objective which allows the exact computation of predictives and enjoys improved training dynamics, with no runtime or memory overhead. This framework is compatible with a family of output activation functions that includes the softmax, as well as element-wise normCDF and sigmoid. Moreover, it allows for approximating the Gaussian pushforwards with Dirichlet distributions by analytic moment matching. We evaluate our approach combined with several approximate Gaussian inference methods (Laplace, HET, SNGP) on large- and small-scale datasets (ImageNet, CIFAR-10), demonstrating improved uncertainty quantification capabilities compared to softmax MC sampling. Code is available at https://github.com/bmucsanyi/probit. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03367</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03367</id><created>2025-02-05</created><authors><author><keyname>Muthyala</keyname><forenames>Madhav R.</forenames></author><author><keyname>Sorourifar</keyname><forenames>Farshud</forenames></author><author><keyname>Peng</keyname><forenames>You</forenames></author><author><keyname>Paulson</keyname><forenames>Joel A.</forenames></author></authors><title>SyMANTIC: An Efficient Symbolic Regression Method for Interpretable and   Parsimonious Model Discovery in Science and Beyond</title><categories>cs.LG</categories><comments>Main and SI compiled into the pdf Main:48 pages, 7 figures SI: 29   pages, 2 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Symbolic regression (SR) is an emerging branch of machine learning focused on discovering simple and interpretable mathematical expressions from data. Although a wide-variety of SR methods have been developed, they often face challenges such as high computational cost, poor scalability with respect to the number of input dimensions, fragility to noise, and an inability to balance accuracy and complexity. This work introduces SyMANTIC, a novel SR algorithm that addresses these challenges. SyMANTIC efficiently identifies (potentially several) low-dimensional descriptors from a large set of candidates (from $\sim 10^5$ to $\sim 10^{10}$ or more) through a unique combination of mutual information-based feature selection, adaptive feature expansion, and recursively applied $\ell_0$-based sparse regression. In addition, it employs an information-theoretic measure to produce an approximate set of Pareto-optimal equations, each offering the best-found accuracy for a given complexity. Furthermore, our open-source implementation of SyMANTIC, built on the PyTorch ecosystem, facilitates easy installation and GPU acceleration. We demonstrate the effectiveness of SyMANTIC across a range of problems, including synthetic examples, scientific benchmarks, real-world material property predictions, and chaotic dynamical system identification from small datasets. Extensive comparisons show that SyMANTIC uncovers similar or more accurate models at a fraction of the cost of existing SR methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03368</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03368</id><created>2025-02-05</created><authors><author><keyname>Liu</keyname><forenames>Chunwei</forenames></author><author><keyname>Vitagliano</keyname><forenames>Gerardo</forenames></author><author><keyname>Rose</keyname><forenames>Brandon</forenames></author><author><keyname>Prinz</keyname><forenames>Matt</forenames></author><author><keyname>Samson</keyname><forenames>David Andrew</forenames></author><author><keyname>Cafarella</keyname><forenames>Michael</forenames></author></authors><title>PalimpChat: Declarative and Interactive AI analytics</title><categories>cs.AI cs.DB cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thanks to the advances in generative architectures and large language models, data scientists can now code pipelines of machine-learning operations to process large collections of unstructured data. Recent progress has seen the rise of declarative AI frameworks (e.g., Palimpzest, Lotus, and DocETL) to build optimized and increasingly complex pipelines, but these systems often remain accessible only to expert programmers. In this demonstration, we present PalimpChat, a chat-based interface to Palimpzest that bridges this gap by letting users create and run sophisticated AI pipelines through natural language alone. By integrating Archytas, a ReAct-based reasoning agent, and Palimpzest's suite of relational and LLM-based operators, PalimpChat provides a practical illustration of how a chat interface can make declarative AI frameworks truly accessible to non-experts.   Our demo system is publicly available online. At SIGMOD'25, participants can explore three real-world scenarios--scientific discovery, legal discovery, and real estate search--or apply PalimpChat to their own datasets. In this paper, we focus on how PalimpChat, supported by the Palimpzest optimizer, simplifies complex AI workflows such as extracting and analyzing biomedical data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03369</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03369</id><created>2025-02-05</created><authors><author><keyname>Peng</keyname><forenames>Zhenghao</forenames></author><author><keyname>Mo</keyname><forenames>Wenjie</forenames></author><author><keyname>Duan</keyname><forenames>Chenda</forenames></author><author><keyname>Li</keyname><forenames>Quanyi</forenames></author><author><keyname>Zhou</keyname><forenames>Bolei</forenames></author></authors><title>Learning from Active Human Involvement through Proxy Value Propagation</title><categories>cs.AI cs.RO</categories><comments>NeurIPS 2023 Spotlight. Project page:   https://metadriverse.github.io/pvp</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Learning from active human involvement enables the human subject to actively intervene and demonstrate to the AI agent during training. The interaction and corrective feedback from human brings safety and AI alignment to the learning process. In this work, we propose a new reward-free active human involvement method called Proxy Value Propagation for policy optimization. Our key insight is that a proxy value function can be designed to express human intents, wherein state-action pairs in the human demonstration are labeled with high values, while those agents' actions that are intervened receive low values. Through the TD-learning framework, labeled values of demonstrated state-action pairs are further propagated to other unlabeled data generated from agents' exploration. The proxy value function thus induces a policy that faithfully emulates human behaviors. Human-in-the-loop experiments show the generality and efficiency of our method. With minimal modification to existing reinforcement learning algorithms, our method can learn to solve continuous and discrete control tasks with various human control devices, including the challenging task of driving in Grand Theft Auto V. Demo video and code are available at: https://metadriverse.github.io/pvp </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03370</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03370</id><created>2025-02-05</created><authors><author><keyname>Naeem</keyname><forenames>Muhammad Ahtsam</forenames></author><author><keyname>Saleem</keyname><forenames>Muhammad Asim</forenames></author><author><keyname>Sharif</keyname><forenames>Muhammad Imran</forenames></author><author><keyname>Akber</keyname><forenames>Shahzad</forenames></author><author><keyname>Saleem</keyname><forenames>Sajjad</forenames></author><author><keyname>Akhtar</keyname><forenames>Zahid</forenames></author><author><keyname>Siddique</keyname><forenames>Kamran</forenames></author></authors><title>Deep Learning-Based Approach for Identification of Potato Leaf Diseases   Using Wrapper Feature Selection and Feature Concatenation</title><categories>cs.CV cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The potato is a widely grown crop in many regions of the world. In recent decades, potato farming has gained incredible traction in the world. Potatoes are susceptible to several illnesses that stunt their development. This plant seems to have significant leaf disease. Early Blight and Late Blight are two prevalent leaf diseases that affect potato plants. The early detection of these diseases would be beneficial for enhancing the yield of this crop. The ideal solution is to use image processing to identify and analyze these disorders. Here, we present an autonomous method based on image processing and machine learning to detect late blight disease affecting potato leaves. The proposed method comprises four different phases: (1) Histogram Equalization is used to improve the quality of the input image; (2) feature extraction is performed using a Deep CNN model, then these extracted features are concatenated; (3) feature selection is performed using wrapper-based feature selection; (4) classification is performed using an SVM classifier and its variants. This proposed method achieves the highest accuracy of 99% using SVM by selecting 550 features. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03373</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03373</id><created>2025-02-05</created><authors><author><keyname>Yeo</keyname><forenames>Edward</forenames></author><author><keyname>Tong</keyname><forenames>Yuxuan</forenames></author><author><keyname>Niu</keyname><forenames>Morry</forenames></author><author><keyname>Neubig</keyname><forenames>Graham</forenames></author><author><keyname>Yue</keyname><forenames>Xiang</forenames></author></authors><title>Demystifying Long Chain-of-Thought Reasoning in LLMs</title><categories>cs.CL cs.LG</categories><comments>Preprint, under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scaling inference compute enhances reasoning in large language models (LLMs), with long chains-of-thought (CoTs) enabling strategies like backtracking and error correction. Reinforcement learning (RL) has emerged as a crucial method for developing these capabilities, yet the conditions under which long CoTs emerge remain unclear, and RL training requires careful design choices. In this study, we systematically investigate the mechanics of long CoT reasoning, identifying the key factors that enable models to generate long CoT trajectories. Through extensive supervised fine-tuning (SFT) and RL experiments, we present four main findings: (1) While SFT is not strictly necessary, it simplifies training and improves efficiency; (2) Reasoning capabilities tend to emerge with increased training compute, but their development is not guaranteed, making reward shaping crucial for stabilizing CoT length growth; (3) Scaling verifiable reward signals is critical for RL. We find that leveraging noisy, web-extracted solutions with filtering mechanisms shows strong potential, particularly for out-of-distribution (OOD) tasks such as STEM reasoning; and (4) Core abilities like error correction are inherently present in base models, but incentivizing these skills effectively for complex tasks via RL demands significant compute, and measuring their emergence requires a nuanced approach. These insights provide practical guidance for optimizing training strategies to enhance long CoT reasoning in LLMs. Our code is available at: https://github.com/eddycmu/demystify-long-cot. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03375</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03375</id><created>2025-02-05</created><authors><author><keyname>Hu</keyname><forenames>Songwen</forenames></author><author><keyname>Rossi</keyname><forenames>Ryan A.</forenames></author><author><keyname>Yu</keyname><forenames>Tong</forenames></author><author><keyname>Wu</keyname><forenames>Junda</forenames></author><author><keyname>Zhao</keyname><forenames>Handong</forenames></author><author><keyname>Kim</keyname><forenames>Sungchul</forenames></author><author><keyname>Li</keyname><forenames>Shuai</forenames></author></authors><title>Interactive Visualization Recommendation with Hier-SUCB</title><categories>cs.IR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Visualization recommendation aims to enable rapid visual analysis of massive datasets. In real-world scenarios, it is essential to quickly gather and comprehend user preferences to cover users from diverse backgrounds, including varying skill levels and analytical tasks. Previous approaches to personalized visualization recommendations are non-interactive and rely on initial user data for new users. As a result, these models cannot effectively explore options or adapt to real-time feedback. To address this limitation, we propose an interactive personalized visualization recommendation (PVisRec) system that learns on user feedback from previous interactions. For more interactive and accurate recommendations, we propose Hier-SUCB, a contextual combinatorial semi-bandit in the PVisRec setting. Theoretically, we show an improved overall regret bound with the same rank of time but an improved rank of action space. We further demonstrate the effectiveness of Hier-SUCB through extensive experiments where it is comparable to offline methods and outperforms other bandit algorithms in the setting of visualization recommendation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03376</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03376</id><created>2025-02-05</created><authors><author><keyname>Anneken</keyname><forenames>Mathias</forenames></author><author><keyname>Burkart</keyname><forenames>Nadia</forenames></author><author><keyname>Jeschke</keyname><forenames>Fabian</forenames></author><author><keyname>Kuwertz-Wolf</keyname><forenames>Achim</forenames></author><author><keyname>Mueller</keyname><forenames>Almuth</forenames></author><author><keyname>Schumann</keyname><forenames>Arne</forenames></author><author><keyname>Teutsch</keyname><forenames>Michael</forenames></author></authors><title>Ethical Considerations for the Military Use of Artificial Intelligence   in Visual Reconnaissance</title><categories>cs.CY cs.CV</categories><comments>White Paper, 30 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This white paper underscores the critical importance of responsibly deploying Artificial Intelligence (AI) in military contexts, emphasizing a commitment to ethical and legal standards. The evolving role of AI in the military goes beyond mere technical applications, necessitating a framework grounded in ethical principles. The discussion within the paper delves into ethical AI principles, particularly focusing on the Fairness, Accountability, Transparency, and Ethics (FATE) guidelines. Noteworthy considerations encompass transparency, justice, non-maleficence, and responsibility. Importantly, the paper extends its examination to military-specific ethical considerations, drawing insights from the Just War theory and principles established by prominent entities. In addition to the identified principles, the paper introduces further ethical considerations specifically tailored for military AI applications. These include traceability, proportionality, governability, responsibility, and reliability. The application of these ethical principles is discussed on the basis of three use cases in the domains of sea, air, and land. Methods of automated sensor data analysis, eXplainable AI (XAI), and intuitive user experience are utilized to specify the use cases close to real-world scenarios. This comprehensive approach to ethical considerations in military AI reflects a commitment to aligning technological advancements with established ethical frameworks. It recognizes the need for a balance between leveraging AI's potential benefits in military operations while upholding moral and legal standards. The inclusion of these ethical principles serves as a foundation for responsible and accountable use of AI in the complex and dynamic landscape of military scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03377</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03377</id><created>2025-02-05</created><authors><author><keyname>Ahmed</keyname><forenames>Abdullahi Isa</forenames></author><author><keyname>Amhoud</keyname><forenames>El Mehdi</forenames></author></authors><title>Energy-Efficient Flying LoRa Gateways: A Multi-Agent Reinforcement   Learning Approach</title><categories>cs.NI cs.LG</categories><comments>6 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  With the rapid development of next-generation Internet of Things (NG-IoT) networks, the increasing number of connected devices has led to a surge in power consumption. This rise in energy demand poses significant challenges to resource availability and raises sustainability concerns for large-scale IoT deployments. Efficient energy utilization in communication networks, particularly for power-constrained IoT devices, has thus become a critical area of research. In this paper, we deployed flying LoRa gateways (GWs) mounted on unmanned aerial vehicles (UAVs) to collect data from LoRa end devices (EDs) and transmit it to a central server. Our primary objective is to maximize the global system energy efficiency (EE) of wireless LoRa networks by joint optimization of transmission power (TP), spreading factor (SF), bandwidth (W), and ED association. To solve this challenging problem, we model the problem as a partially observable Markov decision process (POMDP), where each flying LoRa GW acts as a learning agent using a cooperative Multi-Agent Reinforcement Learning (MARL) approach under centralized training and decentralized execution (CTDE). Simulation results demonstrate that our proposed method, based on the multi-agent proximal policy optimization (MAPPO) algorithm, significantly improves the global system EE and surpasses the conventional MARL schemes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03378</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03378</id><created>2025-02-05</created><authors><author><keyname>Schulmann</keyname><forenames>Haya</forenames></author><author><keyname>Zhao</keyname><forenames>Shujie</forenames></author></authors><title>Learning to Identify Conflicts in RPKI</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The long history of misconfigurations and errors in RPKI indicates that they cannot be easily avoided and will most probably persist also in the future. These errors create conflicts between BGP announcements and their covering ROAs, causing the RPKI validation to result in status invalid. Networks that enforce RPKI filtering with Route Origin Validation (ROV) would block such conflicting BGP announcements and as a result lose traffic from the corresponding origins. Since the business incentives of networks are tightly coupled with the traffic they relay, filtering legitimate traffic leads to a loss of revenue, reducing the motivation to filter invalid announcements with ROV.   In this work, we introduce a new mechanism, LOV, designed for whitelisting benign conflicts on an Internet scale. The resulting whitelist is made available to RPKI supporting ASes to avoid filtering RPKI-invalid but benign routes. Saving legitimate traffic resolves one main obstacle towards RPKI deployment. We measure live BGP updates using LOV during a period of half a year and whitelist 52,846 routes with benign origin errors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03381</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03381</id><created>2025-02-05</created><authors><author><keyname>Tan</keyname><forenames>Shiyi</forenames></author><author><keyname>Orăsan</keyname><forenames>Constantin</forenames></author><author><keyname>Braun</keyname><forenames>Sabine</forenames></author></authors><title>Integrating automatic speech recognition into remote healthcare   interpreting: A pilot study of its impact on interpreting quality</title><categories>cs.CL</categories><comments>to appear in the Proceedings of Translation and the Computer (TC46)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports on the results from a pilot study investigating the impact of automatic speech recognition (ASR) technology on interpreting quality in remote healthcare interpreting settings. Employing a within-subjects experiment design with four randomised conditions, this study utilises scripted medical consultations to simulate dialogue interpreting tasks. It involves four trainee interpreters with a language combination of Chinese and English. It also gathers participants' experience and perceptions of ASR support through cued retrospective reports and semi-structured interviews. Preliminary data suggest that the availability of ASR, specifically the access to full ASR transcripts and to ChatGPT-generated summaries based on ASR, effectively improved interpreting quality. Varying types of ASR output had different impacts on the distribution of interpreting error types. Participants reported similar interactive experiences with the technology, expressing their preference for full ASR transcripts. This pilot study shows encouraging results of applying ASR to dialogue-based healthcare interpreting and offers insights into the optimal ways to present ASR output to enhance interpreter experience and performance. However, it should be emphasised that the main purpose of this study was to validate the methodology and that further research with a larger sample size is necessary to confirm these findings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03382</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03382</id><created>2025-02-05</created><authors><author><keyname>Labiausse</keyname><forenames>Tom</forenames></author><author><keyname>Mazaré</keyname><forenames>Laurent</forenames></author><author><keyname>Grave</keyname><forenames>Edouard</forenames></author><author><keyname>Pérez</keyname><forenames>Patrick</forenames></author><author><keyname>Défossez</keyname><forenames>Alexandre</forenames></author><author><keyname>Zeghidour</keyname><forenames>Neil</forenames></author></authors><title>High-Fidelity Simultaneous Speech-To-Speech Translation</title><categories>cs.CL cs.SD eess.AS</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We introduce Hibiki, a decoder-only model for simultaneous speech translation. Hibiki leverages a multistream language model to synchronously process source and target speech, and jointly produces text and audio tokens to perform speech-to-text and speech-to-speech translation. We furthermore address the fundamental challenge of simultaneous interpretation, which unlike its consecutive counterpart, where one waits for the end of the source utterance to start translating, adapts its flow to accumulate just enough context to produce a correct translation in real-time, chunk by chunk. To do so, we introduce a weakly-supervised method that leverages the perplexity of an off-the-shelf text translation system to identify optimal delays on a per-word basis and create aligned synthetic data. After supervised training, Hibiki performs adaptive, simultaneous speech translation with vanilla temperature sampling. On a French-English simultaneous speech translation task, Hibiki demonstrates state-of-the-art performance in translation quality, speaker fidelity and naturalness. Moreover, the simplicity of its inference process makes it compatible with batched translation and even real-time on-device deployment. We provide examples as well as models and inference code. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03383</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03383</id><created>2025-02-05</created><authors><author><keyname>Wu</keyname><forenames>Dennis</forenames></author><author><keyname>He</keyname><forenames>Yihan</forenames></author><author><keyname>Cao</keyname><forenames>Yuan</forenames></author><author><keyname>Fan</keyname><forenames>Jianqing</forenames></author><author><keyname>Liu</keyname><forenames>Han</forenames></author></authors><title>Transformers and Their Roles as Time Series Foundation Models</title><categories>cs.LG cs.AI</categories><comments>34 Pages, 2 Figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We give a comprehensive analysis of transformers as time series foundation models, focusing on their approximation and generalization capabilities. First, we demonstrate that there exist transformers that fit an autoregressive model on input univariate time series via gradient descent. We then analyze MOIRAI, a multivariate time series foundation model capable of handling an arbitrary number of covariates. We prove that it is capable of automatically fitting autoregressive models with an arbitrary number of covariates, offering insights into its design and empirical success. For generalization, we establish bounds for pretraining when the data satisfies Dobrushin's condition. Experiments support our theoretical findings, highlighting the efficacy of transformers as time series foundation models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03386</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03386</id><created>2025-02-05</created><authors><author><keyname>Du</keyname><forenames>Junliang</forenames></author><author><keyname>Dou</keyname><forenames>Shiyu</forenames></author><author><keyname>Yang</keyname><forenames>Bohuan</forenames></author><author><keyname>Hu</keyname><forenames>Jiacheng</forenames></author><author><keyname>An</keyname><forenames>Tai</forenames></author></authors><title>A Structured Reasoning Framework for Unbalanced Data Classification   Using Probabilistic Models</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a Markov network model for unbalanced data, aiming to solve the problems of classification bias and insufficient minority class recognition ability of traditional machine learning models in environments with uneven class distribution. By constructing joint probability distribution and conditional dependency, the model can achieve global modeling and reasoning optimization of sample categories. The study introduced marginal probability estimation and weighted loss optimization strategies, combined with regularization constraints and structured reasoning methods, effectively improving the generalization ability and robustness of the model. In the experimental stage, a real credit card fraud detection dataset was selected and compared with models such as logistic regression, support vector machine, random forest and XGBoost. The experimental results show that the Markov network performs well in indicators such as weighted accuracy, F1 score, and AUC-ROC, significantly outperforming traditional classification models, demonstrating its strong decision-making ability and applicability in unbalanced data scenarios. Future research can focus on efficient model training, structural optimization, and deep learning integration in large-scale unbalanced data environments and promote its wide application in practical applications such as financial risk control, medical diagnosis, and intelligent monitoring. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03387</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03387</id><created>2025-02-05</created><authors><author><keyname>Ye</keyname><forenames>Yixin</forenames></author><author><keyname>Huang</keyname><forenames>Zhen</forenames></author><author><keyname>Xiao</keyname><forenames>Yang</forenames></author><author><keyname>Chern</keyname><forenames>Ethan</forenames></author><author><keyname>Xia</keyname><forenames>Shijie</forenames></author><author><keyname>Liu</keyname><forenames>Pengfei</forenames></author></authors><title>LIMO: Less is More for Reasoning</title><categories>cs.CL cs.AI</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a fundamental discovery that challenges our understanding of how complex reasoning emerges in large language models. While conventional wisdom suggests that sophisticated reasoning tasks demand extensive training data (&gt;100,000 examples), we demonstrate that complex mathematical reasoning abilities can be effectively elicited with surprisingly few examples. Through comprehensive experiments, our proposed model LIMO demonstrates unprecedented performance in mathematical reasoning. With merely 817 curated training samples, LIMO achieves 57.1% accuracy on AIME and 94.8% on MATH, improving from previous SFT-based models' 6.5% and 59.2% respectively, while only using 1% of the training data required by previous approaches. LIMO demonstrates exceptional out-of-distribution generalization, achieving 40.5% absolute improvement across 10 diverse benchmarks, outperforming models trained on 100x more data, challenging the notion that SFT leads to memorization rather than generalization. Based on these results, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning capabilities can emerge through minimal but precisely orchestrated demonstrations of cognitive processes. This hypothesis posits that the elicitation threshold for complex reasoning is determined by two key factors: (1) the completeness of the model's encoded knowledge foundation during pre-training, and (2) the effectiveness of post-training examples as "cognitive templates" that show the model how to utilize its knowledge base to solve complex reasoning tasks. To facilitate reproducibility and future research in data-efficient reasoning, we release LIMO as a comprehensive open-source suite at https://github.com/GAIR-NLP/LIMO. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03391</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03391</id><created>2025-02-05</created><authors><author><keyname>Bassan</keyname><forenames>Shahaf</forenames></author><author><keyname>Gur</keyname><forenames>Shlomit</forenames></author><author><keyname>Eliav</keyname><forenames>Ron</forenames></author></authors><title>Explain Yourself, Briefly! Self-Explaining Neural Networks with Concise   Sufficient Reasons</title><categories>cs.LG cs.LO</categories><comments>To appear in ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Minimal sufficient reasons represent a prevalent form of explanation - the smallest subset of input features which, when held constant at their corresponding values, ensure that the prediction remains unchanged. Previous post-hoc methods attempt to obtain such explanations but face two main limitations: (1) Obtaining these subsets poses a computational challenge, leading most scalable methods to converge towards suboptimal, less meaningful subsets; (2) These methods heavily rely on sampling out-of-distribution input assignments, potentially resulting in counterintuitive behaviors. To tackle these limitations, we propose in this work a self-supervised training approach, which we term *sufficient subset training* (SST). Using SST, we train models to generate concise sufficient reasons for their predictions as an integral part of their output. Our results indicate that our framework produces succinct and faithful subsets substantially more efficiently than competing post-hoc methods, while maintaining comparable predictive performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03393</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03393</id><created>2025-02-05</created><authors><author><keyname>Liu</keyname><forenames>Zewen</forenames></author><author><keyname>Ni</keyname><forenames>Juntong</forenames></author><author><keyname>Lau</keyname><forenames>Max S. Y.</forenames></author><author><keyname>Jin</keyname><forenames>Wei</forenames></author></authors><title>CAPE: Covariate-Adjusted Pre-Training for Epidemic Time Series   Forecasting</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Accurate forecasting of epidemic infection trajectories is crucial for safeguarding public health. However, limited data availability during emerging outbreaks and the complex interaction between environmental factors and disease dynamics present significant challenges for effective forecasting. In response, we introduce CAPE, a novel epidemic pre-training framework designed to harness extensive disease datasets from diverse regions and integrate environmental factors directly into the modeling process for more informed decision-making on downstream diseases. Based on a covariate adjustment framework, CAPE utilizes pre-training combined with hierarchical environment contrasting to identify universal patterns across diseases while estimating latent environmental influences. We have compiled a diverse collection of epidemic time series datasets and validated the effectiveness of CAPE under various evaluation scenarios, including full-shot, few-shot, zero-shot, cross-location, and cross-disease settings, where it outperforms the leading baseline by an average of 9.9% in full-shot and 14.3% in zero-shot settings. The code will be released upon acceptance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03395</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03395</id><created>2025-02-05</created><authors><author><keyname>Arab</keyname><forenames>Issar</forenames></author><author><keyname>Benitez</keyname><forenames>Rodrigo</forenames></author></authors><title>Benchmarking Time Series Forecasting Models: From Statistical Techniques   to Foundation Models in Real-World Applications</title><categories>cs.LG cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Time series forecasting is essential for operational intelligence in the hospitality industry, and particularly challenging in large-scale, distributed systems. This study evaluates the performance of statistical, machine learning (ML), deep learning, and foundation models in forecasting hourly sales over a 14-day horizon using real-world data from a network of thousands of restaurants across Germany. The forecasting solution includes features such as weather conditions, calendar events, and time-of-day patterns. Results demonstrate the strong performance of ML-based meta-models and highlight the emerging potential of foundation models like Chronos and TimesFM, which deliver competitive performance with minimal feature engineering, leveraging only the pre-trained model (zero-shot inference). Additionally, a hybrid PySpark-Pandas approach proves to be a robust solution for achieving horizontal scalability in large-scale deployments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03396</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03396</id><created>2025-02-05</created><authors><author><keyname>Al-Shareeda</keyname><forenames>Sarah</forenames></author><author><keyname>Celik</keyname><forenames>Yasar</forenames></author><author><keyname>Bilgili</keyname><forenames>Bilge</forenames></author><author><keyname>Al-Dubai</keyname><forenames>Ahmed</forenames></author><author><keyname>Canberk</keyname><forenames>Berk</forenames></author></authors><title>Accurate AI-Driven Emergency Vehicle Location Tracking in Healthcare ITS   Digital Twin</title><categories>cs.LG cs.AI cs.ET</categories><comments>8 pages, 8 figures, 5th IEEE Middle East &amp; North Africa   COMMunications Conference (MENACOMM'25), Lebanon Feb 20-23, 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Creating a Digital Twin (DT) for Healthcare Intelligent Transportation Systems (HITS) is a hot research trend focusing on enhancing HITS management, particularly in emergencies where ambulance vehicles must arrive at the crash scene on time and track their real-time location is crucial to the medical authorities. Despite the claim of real-time representation, a temporal misalignment persists between the physical and virtual domains, leading to discrepancies in the ambulance's location representation. This study proposes integrating AI predictive models, specifically Support Vector Regression (SVR) and Deep Neural Networks (DNN), within a constructed mock DT data pipeline framework to anticipate the medical vehicle's next location in the virtual world. These models align virtual representations with their physical counterparts, i.e., metaphorically offsetting the synchronization delay between the two worlds. Trained meticulously on a historical geospatial dataset, SVR and DNN exhibit exceptional prediction accuracy in MATLAB and Python environments. Through various testing scenarios, we visually demonstrate the efficacy of our methodology, showcasing SVR and DNN's key role in significantly reducing the witnessed gap within the HITS's DT. This transformative approach enhances real-time synchronization in emergency HITS by approximately 88% to 93%. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03397</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03397</id><created>2025-02-05</created><authors><author><keyname>Zhan</keyname><forenames>Hongli</forenames></author><author><keyname>Azmat</keyname><forenames>Muneeza</forenames></author><author><keyname>Horesh</keyname><forenames>Raya</forenames></author><author><keyname>Li</keyname><forenames>Junyi Jessy</forenames></author><author><keyname>Yurochkin</keyname><forenames>Mikhail</forenames></author></authors><title>SPRI: Aligning Large Language Models with Context-Situated Principles</title><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Aligning Large Language Models to integrate and reflect human values, especially for tasks that demand intricate human oversight, is arduous since it is resource-intensive and time-consuming to depend on human expertise for context-specific guidance. Prior work has utilized predefined sets of rules or principles to steer the behavior of models (Bai et al., 2022; Sun et al., 2023). However, these principles tend to be generic, making it challenging to adapt them to each individual input query or context. In this work, we present Situated-PRInciples (SPRI), a framework requiring minimal or no human effort that is designed to automatically generate guiding principles in real-time for each input query and utilize them to align each response. We evaluate SPRI on three tasks, and show that 1) SPRI can derive principles in a complex domain-specific task that leads to on-par performance as expert-crafted ones; 2) SPRI-generated principles lead to instance-specific rubrics that outperform prior LLM-as-a-judge frameworks; 3) using SPRI to generate synthetic SFT data leads to substantial improvement on truthfulness. We release our code and model generations at https://github.com/honglizhan/SPRI-public. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03398</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03398</id><created>2025-02-05</created><authors><author><keyname>Jbaar</keyname><forenames>Mamoon A. Al</forenames></author><author><keyname>Yousif</keyname><forenames>Adel Jalal</forenames></author><author><keyname>Ali</keyname><forenames>Qutaiba I.</forenames></author></authors><title>The Adoption of Artificial Intelligence in Different Network Security   Concepts</title><categories>cs.CR cs.NI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The obstacles of each security system combined with the increase of cyber-attacks, negatively affect the effectiveness of network security management and rise the activities to be taken by the security staff and network administrators. So, there is a growing need for the automated auditing and intelligent reporting strategies for reliable network security with as less model complexity as possible. Newly, artificial intelligence has been effectively applied to various network security issues, and numerous studies have been conducted that utilize various artificial intelligence techniques for the purposes of encryption and secure communication, in addition to using artificial intelligence to perform a large number of data encryption operations in record time. The aim of the study is to present and discuss the most prominent methods of artificial intelligence recently used in the field of network security including user authentication, Key exchanging, encryption/decryption, data integrity and intrusion detection system. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03400</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03400</id><created>2025-02-05</created><authors><author><keyname>Mao</keyname><forenames>Xinyu</forenames></author><author><keyname>Leelanupab</keyname><forenames>Teerapong</forenames></author><author><keyname>Scells</keyname><forenames>Harrisen</forenames></author><author><keyname>Zuccon</keyname><forenames>Guido</forenames></author></authors><title>DenseReviewer: A Screening Prioritisation Tool for Systematic Review   based on Dense Retrieval</title><categories>cs.IR</categories><comments>Accepted at ECIR 2025</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Screening is a time-consuming and labour-intensive yet required task for medical systematic reviews, as tens of thousands of studies often need to be screened. Prioritising relevant studies to be screened allows downstream systematic review creation tasks to start earlier and save time. In previous work, we developed a dense retrieval method to prioritise relevant studies with reviewer feedback during the title and abstract screening stage. Our method outperforms previous active learning methods in both effectiveness and efficiency. In this demo, we extend this prior work by creating (1) a web-based screening tool that enables end-users to screen studies exploiting state-of-the-art methods and (2) a Python library that integrates models and feedback mechanisms and allows researchers to develop and demonstrate new active learning methods. We describe the tool's design and showcase how it can aid screening. The tool is available at https://densereviewer.ielab.io. The source code is also open sourced at https://github.com/ielab/densereviewer. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03402</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03402</id><created>2025-02-05</created><authors><author><keyname>Absar</keyname><forenames>Javed</forenames></author><author><keyname>Narang</keyname><forenames>Samarth</forenames></author><author><keyname>Baskaran</keyname><forenames>Muthu</forenames></author></authors><title>Tensor Evolution: A framework for Fast Evaluation of Tensor Computations   using Recurrences</title><categories>cs.PL cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new mathematical framework for analysis and optimization of tensor expressions within an enclosing loop. Tensors are multi-dimensional arrays of values. They are common in high performance computing (HPC) and machine learning domains. Our framework extends Scalar Evolution -- an important optimization pass implemented in both LLVM and GCC -- to tensors. Scalar Evolution (SCEV) relies on the theory of `Chain of Recurrences' for its mathematical underpinnings. We use the same theory for Tensor Evolution (TeV). While some concepts from SCEV map easily to TeV -- e.g. element-wise operations; tensors introduce new operations such as concatenation, slicing, broadcast, reduction, and reshape which have no equivalent in scalars and SCEV. Not all computations are amenable to TeV analysis but it can play a part in the optimization and analysis parts of ML and HPC compilers. Also, for many mathematical/compiler ideas, applications may go beyond what was initially envisioned, once others build on it and take it further. We hope for a similar trajectory for the tensor-evolution concept. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03403</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03403</id><created>2025-02-05</created><authors><author><keyname>Al-Shareeda</keyname><forenames>Sarah</forenames></author><author><keyname>Ozguner</keyname><forenames>Fusun</forenames></author><author><keyname>Redmill</keyname><forenames>Keith</forenames></author><author><keyname>Duong</keyname><forenames>Trung Q.</forenames></author><author><keyname>Canberk</keyname><forenames>Berk</forenames></author></authors><title>Lightweight Authenticated Task Offloading in 6G-Cloud Vehicular Twin   Networks</title><categories>cs.CR cs.AI</categories><comments>6 pages, 3 figures, IEEE Wireless Communications and Networking   Conference (WCNC2025), Milan, Italy, 24-27 March 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Task offloading management in 6G vehicular networks is crucial for maintaining network efficiency, particularly as vehicles generate substantial data. Integrating secure communication through authentication introduces additional computational and communication overhead, significantly impacting offloading efficiency and latency. This paper presents a unified framework incorporating lightweight Identity-Based Cryptographic (IBC) authentication into task offloading within cloud-based 6G Vehicular Twin Networks (VTNs). Utilizing Proximal Policy Optimization (PPO) in Deep Reinforcement Learning (DRL), our approach optimizes authenticated offloading decisions to minimize latency and enhance resource allocation. Performance evaluation under varying network sizes, task sizes, and data rates reveals that IBC authentication can reduce offloading efficiency by up to 50% due to the added overhead. Besides, increasing network size and task size can further reduce offloading efficiency by up to 91.7%. As a countermeasure, increasing the transmission data rate can improve the offloading performance by as much as 63%, even in the presence of authentication overhead. The code for the simulations and experiments detailed in this paper is available on GitHub for further reference and reproducibility [1]. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03405</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03405</id><created>2025-02-05</created><authors><author><keyname>Ghriss</keyname><forenames>Ayoub</forenames></author><author><keyname>Monteleoni</keyname><forenames>Claire</forenames></author></authors><title>Deep Clustering via Probabilistic Ratio-Cut Optimization</title><categories>cs.LG cs.CV</categories><comments>Proceedings of the 28th International Conference on Artificial   Intelligence and Statistics (AISTATS) 2025, Mai Khao, Thailand. PMLR: Volume   258</comments><journal-ref>Proceedings of Machine Learning Research (2008), Volume 258</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a novel approach for optimizing the graph ratio-cut by modeling the binary assignments as random variables. We provide an upper bound on the expected ratio-cut, as well as an unbiased estimate of its gradient, to learn the parameters of the assignment variables in an online setting. The clustering resulting from our probabilistic approach (PRCut) outperforms the Rayleigh quotient relaxation of the combinatorial problem, its online learning extensions, and several widely used methods. We demonstrate that the PRCut clustering closely aligns with the similarity measure and can perform as well as a supervised classifier when label-based similarities are provided. This novel approach can leverage out-of-the-box self-supervised representations to achieve competitive performance and serve as an evaluation method for the quality of these representations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03407</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03407</id><created>2025-02-05</created><authors><author><keyname>Goldowsky-Dill</keyname><forenames>Nicholas</forenames></author><author><keyname>Chughtai</keyname><forenames>Bilal</forenames></author><author><keyname>Heimersheim</keyname><forenames>Stefan</forenames></author><author><keyname>Hobbhahn</keyname><forenames>Marius</forenames></author></authors><title>Detecting Strategic Deception Using Linear Probes</title><categories>cs.LG</categories><comments>Website: http://data.apolloresearch.ai/dd/ Code:   http://www.github.com/ApolloResearch/deception-detection/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  AI models might use deceptive strategies as part of scheming or misaligned behaviour. Monitoring outputs alone is insufficient, since the AI might produce seemingly benign outputs while their internal reasoning is misaligned. We thus evaluate if linear probes can robustly detect deception by monitoring model activations. We test two probe-training datasets, one with contrasting instructions to be honest or deceptive (following Zou et al., 2023) and one of responses to simple roleplaying scenarios. We test whether these probes generalize to realistic settings where Llama-3.3-70B-Instruct behaves deceptively, such as concealing insider trading (Scheurer et al., 2023) and purposely underperforming on safety evaluations (Benton et al., 2024). We find that our probe distinguishes honest and deceptive responses with AUROCs between 0.96 and 0.999 on our evaluation datasets. If we set the decision threshold to have a 1% false positive rate on chat data not related to deception, our probe catches 95-99% of the deceptive responses. Overall we think white-box probes are promising for future monitoring systems, but current performance is insufficient as a robust defence against deception. Our probes' outputs can be viewed at data.apolloresearch.ai/dd and our code at github.com/ApolloResearch/deception-detection. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03409</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03409</id><created>2025-02-05</created><authors><author><keyname>Pond</keyname><forenames>Ellie</forenames></author><author><keyname>Hale</keyname><forenames>Matthew</forenames></author></authors><title>Verification and Synthesis Methods for High-Order Control Barrier   Functions</title><categories>eess.SY cs.SY</categories><comments>16 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  High-order control barrier functions (HOCBFs) can be used to provide autonomous systems with safety, though computational methods to verify and synthesize these functions remain lacking. In this work, we address this need by formulating SOS programs that verify and synthesize HOCBFs, such that continued safety is always guaranteed forward in time. We first propose a verification SOS program for systems with (i) one or multiple HOCBFs, (ii) a control Lyapunov function (CLF), and (iii) input constraints, and we show that a solution to this problem guarantees that the online implementation of the system is always safe. Next, we propose a sequence of SOS programs that synthesize the class K functions used in an HOCBF, and we show that this sequence of problems ensures that a system is guaranteed to remain safe while running. After that, a synthesis framework is given that ensures real-time safety for systems with (i) multiple HOCBFs, (ii) a CLF, and (iii) input constraints. Our developments are illustrated in numerical simulations for a system with seven HOCBFs of maximum relative degree two, with 14 total unknown class K functions, all of which are successfully synthesized in a way that produces safe autonomy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03411</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03411</id><created>2025-02-05</created><authors><author><keyname>Tovanich</keyname><forenames>Natkamon</forenames></author><author><keyname>Coquidé</keyname><forenames>Célestin</forenames></author><author><keyname>Cazabet</keyname><forenames>Rémy</forenames></author></authors><title>Cryptocurrency Network Analysis</title><categories>cs.SI cs.CY cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryptocurrency network analysis consists of applying the tools and methods of social network analysis to transactional data issued from cryptocurrencies. The main difference with most online social networks is that users do not exchange textual content but instead value -- in systems designed mainly as cryptocurrency, such as Bitcoin -- or digital items and services in more permissive systems based on smart contracts such as Ethereum. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03412</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03412</id><created>2025-02-05</created><authors><author><keyname>Haghighi</keyname><forenames>Rouzbeh</forenames></author><author><keyname>Hassan</keyname><forenames>Ali</forenames></author><author><keyname>Bui</keyname><forenames>Van-Hai</forenames></author><author><keyname>Hussain</keyname><forenames>Akhtar</forenames></author><author><keyname>Su</keyname><forenames>Wencong</forenames></author></authors><title>Deep Reinforcement Learning-Based Optimization of Second-Life Battery   Utilization in Electric Vehicles Charging Stations</title><categories>eess.SY cs.LG cs.SY</categories><comments>5 pages, 6 figures, Accepted, 2025 IEEE Power and Energy Society   General Meeting (PESGM 2025), Austin, TX, USA</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The rapid rise in electric vehicle (EV) adoption presents significant challenges in managing the vast number of retired EV batteries. Research indicates that second-life batteries (SLBs) from EVs typically retain considerable residual capacity, offering extended utility. These batteries can be effectively repurposed for use in EV charging stations (EVCS), providing a cost-effective alternative to new batteries and reducing overall planning costs. Integrating battery energy storage systems (BESS) with SLBs into EVCS is a promising strategy to alleviate system overload. However, efficient operation of EVCS with integrated BESS is hindered by uncertainties such as fluctuating EV arrival and departure times and variable power prices from the grid. This paper presents a deep reinforcement learning-based (DRL) planning framework for EV charging stations with BESS, leveraging SLBs. We employ the advanced soft actor-critic (SAC) approach, training the model on a year's worth of data to account for seasonal variations, including weekdays and holidays. A tailored reward function enables effective offline training, allowing real-time optimization of EVCS operations under uncertainty. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03416</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03416</id><created>2025-02-05</created><authors><author><keyname>Arunruangsirilert</keyname><forenames>Kasidis</forenames></author><author><keyname>Wongprasert</keyname><forenames>Pasapong</forenames></author><author><keyname>Katto</keyname><forenames>Jiro</forenames></author></authors><title>Performance Analysis of 5G FR2 (mmWave) Downlink 256QAM on Commercial 5G   Networks</title><categories>cs.NI eess.SP</categories><comments>2025 IEEE International Conference on Communications (ICC), 8-12 June   2025, Montreal, Canada</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 5G New Radio (NR) standard introduces new frequency bands allocated in Frequency Range 2 (FR2) to support enhanced Mobile Broadband (eMBB) in congested environments and enables new use cases such as Ultra-Reliable Low Latency Communication (URLLC). The 3GPP introduced 256QAM support for FR2 frequency bands to further enhance downlink capacity. However, sustaining 256QAM on FR2 in practical environments is challenging due to strong path loss and susceptibility to distortion. While 256QAM can improve theoretical throughput by 33%, compared to 64QAM, and is widely adopted in FR1, its real-world impact when utilized in FR2 is questionable, given the significant path loss and distortions experienced in the FR2 range. Additionally, using higher modulation correlates to higher BLER, increased instability, and retransmission. Moreover, 256QAM also utilizes a different MCS table defining the modulation and code rate at different Channel Quality Indexes (CQI), affecting the UE's link adaptation behavior. This paper investigates the real-world performance of 256QAM utilization on FR2 bands in two countries, across three RAN manufacturers, and in both NSA (EN-DC) and SA (NR-DC) configurations, under various scenarios, including open-air plazas, city centers, footbridges, train station platforms, and stationary environments. The results show that 256QAM provides a reasonable throughput gain when stationary but marginal improvements when there is UE mobility while increasing the probability of NACK responses, increasing BLER, and the number of retransmissions. Finally, MATLAB simulations are run to validate the findings as well as explore the effect of the recently introduced 1024QAM on FR2. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03417</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03417</id><created>2025-02-05</created><authors><author><keyname>Borisyuk</keyname><forenames>Fedor</forenames></author><author><keyname>Hertel</keyname><forenames>Lars</forenames></author><author><keyname>Parameswaran</keyname><forenames>Ganesh</forenames></author><author><keyname>Srivastava</keyname><forenames>Gaurav</forenames></author><author><keyname>Ramanujam</keyname><forenames>Sudarshan Srinivasa</forenames></author><author><keyname>Ocejo</keyname><forenames>Borja</forenames></author><author><keyname>Du</keyname><forenames>Peng</forenames></author><author><keyname>Akterskii</keyname><forenames>Andrei</forenames></author><author><keyname>Daftary</keyname><forenames>Neil</forenames></author><author><keyname>Tang</keyname><forenames>Shao</forenames></author><author><keyname>Sun</keyname><forenames>Daqi</forenames></author><author><keyname>Xiao</keyname><forenames>Qiang Charles</forenames></author><author><keyname>Nathani</keyname><forenames>Deepesh</forenames></author><author><keyname>Kothari</keyname><forenames>Mohit</forenames></author><author><keyname>Dai</keyname><forenames>Yun</forenames></author><author><keyname>Gupta</keyname><forenames>Aman</forenames></author></authors><title>From Features to Transformers: Redefining Ranking for Scalable Impact</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present LiGR, a large-scale ranking framework developed at LinkedIn that brings state-of-the-art transformer-based modeling architectures into production. We introduce a modified transformer architecture that incorporates learned normalization and simultaneous set-wise attention to user history and ranked items. This architecture enables several breakthrough achievements, including: (1) the deprecation of most manually designed feature engineering, outperforming the prior state-of-the-art system using only few features (compared to hundreds in the baseline), (2) validation of the scaling law for ranking systems, showing improved performance with larger models, more training data, and longer context sequences, and (3) simultaneous joint scoring of items in a set-wise manner, leading to automated improvements in diversity. To enable efficient serving of large ranking models, we describe techniques to scale inference effectively using single-pass processing of user history and set-wise attention. We also summarize key insights from various ablation studies and A/B tests, highlighting the most impactful technical approaches. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03418</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03418</id><created>2025-02-05</created><authors><author><keyname>Sadr</keyname><forenames>Nikta Gohari</forenames></author><author><keyname>Madhusudan</keyname><forenames>Sangmitra</forenames></author><author><keyname>Emami</keyname><forenames>Ali</forenames></author></authors><title>Think or Step-by-Step? UnZIPping the Black Box in Zero-Shot Prompts</title><categories>cs.CL</categories><comments>8 pages (excluding references)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Zero-shot prompting techniques have significantly improved the performance of Large Language Models (LLMs). However, we lack a clear understanding of why zero-shot prompts are so effective. For example, in the prompt "Let's think step-by-step," is "think" or "step-by-step" more crucial to its success? Existing interpretability methods, such as gradient-based and attention-based approaches, are computationally intensive and restricted to open-source models. We introduce the ZIP score (Zero-shot Importance of Perturbation score), a versatile metric applicable to both open and closed-source models, based on systematic input word perturbations. Our experiments across four recent LLMs, seven widely-used prompts, and several tasks, reveal interesting patterns in word importance. For instance, while both 'step-by-step' and 'think' show high ZIP scores, which one is more influential depends on the model and task. We validate our method using controlled experiments and compare our results with human judgments, finding that proprietary models align more closely with human intuition regarding word significance. These findings enhance our understanding of LLM behavior and contribute to developing more effective zero-shot prompts and improved model analysis. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03419</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03419</id><created>2025-02-05</created><authors><author><keyname>Ramaseri-Chandra</keyname><forenames>Ananth N.</forenames></author><author><keyname>Reza</keyname><forenames>Hassan</forenames></author></authors><title>Dynamic Cybersickness Mitigation via Adaptive FFR and FoV adjustments</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel adaptive Virtual Reality (VR) system that aims to mitigate cybersickness in immersive environments through dynamic, real-time adjustments. The system predicts cybersickness levels in real-time using a machine learning (ML) model trained on head tracking and kinematic data. The adaptive system adjusts foveated rendering (FFR) strength and field of view (FOV) to enhance user comfort. With a goal to balance usability with system performance, we believe our approach will optimize both user experience and performance. Adapting responsively to user needs, our work explores the potential of a machine learning-based feedback loop for user experience management, contributing to a user-centric VR system design. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03420</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03420</id><created>2025-02-05</created><authors><author><keyname>Novikov</keyname><forenames>Alexey A.</forenames></author><author><keyname>Vranka</keyname><forenames>Miroslav</forenames></author><author><keyname>David</keyname><forenames>François</forenames></author><author><keyname>Voronin</keyname><forenames>Artem</forenames></author></authors><title>Can Text-to-Image Generative Models Accurately Depict Age? A Comparative   Study on Synthetic Portrait Generation and Age Estimation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text-to-image generative models have shown remarkable progress in producing diverse and photorealistic outputs. In this paper, we present a comprehensive analysis of their effectiveness in creating synthetic portraits that accurately represent various demographic attributes, with a special focus on age, nationality, and gender. Our evaluation employs prompts specifying detailed profiles (e.g., Photorealistic selfie photo of a 32-year-old Canadian male), covering a broad spectrum of 212 nationalities, 30 distinct ages from 10 to 78, and balanced gender representation. We compare the generated images against ground truth age estimates from two established age estimation models to assess how faithfully age is depicted. Our findings reveal that although text-to-image models can consistently generate faces reflecting different identities, the accuracy with which they capture specific ages and do so across diverse demographic backgrounds remains highly variable. These results suggest that current synthetic data may be insufficiently reliable for high-stakes age-related tasks requiring robust precision, unless practitioners are prepared to invest in significant filtering and curation. Nevertheless, they may still be useful in less sensitive or exploratory applications, where absolute age precision is not critical. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03421</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03421</id><created>2025-02-05</created><authors><author><keyname>Basu</keyname><forenames>Meheli</forenames></author><author><keyname>Dutta</keyname><forenames>Aniruddha</forenames></author><author><keyname>Shah</keyname><forenames>Purvi</forenames></author></authors><title>Investigating Corporate Social Responsibility Initiatives: Examining the   case of corporate Covid-19 response</title><categories>cs.IR</categories><comments>7 Tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In todays age of freely available information, policy makers have to take into account a huge amount of information while making decisions affecting relevant stakeholders. While increase in the amount of information sources and documents increases credibility of decisions based on the corpus of available text, it is challenging for policymakers to make sense of this information. This paper demonstrates how policy makers can implement some of the most popular topic recognition methods, Latent Dirichlet Allocation, Deep Distributed Representation method, text summarization approaches, Word Based Sentence Ranking method and TextRank for sentence extraction method, to sum up the content of large volume of documents to understand the gist of the overload of information. We have applied popular NLP methods to corporate press releases during the early period and advanced period of Covid-19 pandemic which has resulted in a global unprecedented health and socio-economic crisis, when policymaking and regulations have become especially important to standardize corporate practices for employee and social welfare in the face of similar future unseen crises. The steps undertaken in this study can be replicated to yield insights from relevant documents in any other social decision-making context. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03422</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03422</id><created>2025-02-05</created><authors><author><keyname>Herdt</keyname><forenames>Rudolf</forenames></author><author><keyname>Baguer</keyname><forenames>Daniel Otero</forenames></author></authors><title>Concept Based Explanations and Class Contrasting</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Explaining deep neural networks is challenging, due to their large size and non-linearity. In this paper, we introduce a concept-based explanation method, in order to explain the prediction for an individual class, as well as contrasting any two classes, i.e. explain why the model predicts one class over the other. We test it on several openly available classification models trained on ImageNet1K, as well as on a segmentation model trained to detect tumor in stained tissue samples. We perform both qualitative and quantitative tests. For example, for a ResNet50 model from pytorch model zoo, we can use the explanation for why the model predicts a class 'A' to automatically select six dataset crops where the model does not predict class 'A'. The model then predicts class 'A' again for the newly combined image in 71\% of the cases (works for 710 out of the 1000 classes). The code including an .ipynb example is available on git: https://github.com/rherdt185/concept-based-explanations-and-class-contrasting. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03424</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03424</id><created>2025-02-05</created><authors><author><keyname>Xinjie</keyname><forenames>Yuan</forenames></author><author><keyname>Mosalam</keyname><forenames>Khalid M.</forenames></author></authors><title>Prediction of the Most Fire-Sensitive Point in Building Structures with   Differentiable Agents for Thermal Simulators</title><categories>cs.LG</categories><comments>This paper is currently under review at Computer-Aided Civil and   Infrastructure Engineering</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fire safety is a critical area of research in civil and mechanical engineering, particularly in ensuring the structural stability of buildings during fire events. The Most Fire-Sensitive Point (MFSP) in a structure is the location where a fire would cause the greatest impact on structural stability. Accurate prediction of the MFSP is vital for streamlining structural assessments and optimizing the design process. This paper presents a novel framework for MFSP prediction using a neural network-based approach that integrates fire dynamics and finite element analysis through a differentiable agent model. The framework focuses on predicting the Maximum Interstory Drift Ratio (MIDR), a key indicator of structural performance under fire conditions. By leveraging the differentiable agent model, we efficiently generate labeled data for MFSP and directly train a predictor for this critical metric. To achieve this, we generated extensive simulation data encompassing structural and fire scenarios and employed graph neural networks to represent the building structures. Transfer learning was applied to optimize the training process, and an edge update mechanism was introduced to dynamically adjust edge attributes, reflecting property changes under fire conditions. The proposed model was rigorously evaluated on simulation data, demonstrating strong performance in accurately predicting both MIDR and MFSP, thus advancing fire safety analysis for building structures. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03425</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03425</id><created>2025-02-05</created><authors><author><keyname>Sghaier</keyname><forenames>Oussama Ben</forenames></author><author><keyname>Weyssow</keyname><forenames>Martin</forenames></author><author><keyname>Sahraoui</keyname><forenames>Houari</forenames></author></authors><title>Harnessing Large Language Models for Curated Code Reviews</title><categories>cs.SE</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In code review, generating structured and relevant comments is crucial for identifying code issues and facilitating accurate code changes that ensure an efficient code review process. Well-crafted comments not only streamline the code review itself but are also essential for subsequent tasks like code refinement, where the code is modified to satisfy the input review comment. Although various AI-based approaches aimed to automate comment generation, their effectiveness remains limited by the quality of the training data. Existing code review datasets are often noisy and unrefined, posing limitations to the learning potential of AI models and hindering the automation process.   To address these challenges, we propose a curation pipeline designed to enhance the quality of the largest publicly available code review dataset. We begin by establishing an evaluation framework, incorporating specific criteria and categories to empirically study the initial quality of the dataset. Using a large language model (LLM)-driven approach, we then apply our curation pipeline to refine the dataset. A comparative analysis of the newly curated dataset, based on the same evaluation framework, demonstrates substantial improvements in the clarity and conciseness of the comments. Additionally, we assess the impact of the curated dataset on automating downstream tasks, specifically comment generation and code refinement. Our findings show that the curated dataset leads to enhanced model performance in generating more accurate comments. Curated comments are also more useful as they lead to more accurate code refinement. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03426</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03426</id><created>2025-02-05</created><authors><author><keyname>Xu</keyname><forenames>Zhihong</forenames></author><author><keyname>Wang</keyname><forenames>Dongxia</forenames></author><author><keyname>Du</keyname><forenames>Peng</forenames></author><author><keyname>Cao</keyname><forenames>Yang</forenames></author><author><keyname>Guo</keyname><forenames>Qing</forenames></author></authors><title>TruePose: Human-Parsing-guided Attention Diffusion for Full-ID   Preserving Pose Transfer</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pose-Guided Person Image Synthesis (PGPIS) generates images that maintain a subject's identity from a source image while adopting a specified target pose (e.g., skeleton). While diffusion-based PGPIS methods effectively preserve facial features during pose transformation, they often struggle to accurately maintain clothing details from the source image throughout the diffusion process. This limitation becomes particularly problematic when there is a substantial difference between the source and target poses, significantly impacting PGPIS applications in the fashion industry where clothing style preservation is crucial for copyright protection. Our analysis reveals that this limitation primarily stems from the conditional diffusion model's attention modules failing to adequately capture and preserve clothing patterns. To address this limitation, we propose human-parsing-guided attention diffusion, a novel approach that effectively preserves both facial and clothing appearance while generating high-quality results. We propose a human-parsing-aware Siamese network that consists of three key components: dual identical UNets (TargetNet for diffusion denoising and SourceNet for source image embedding extraction), a human-parsing-guided fusion attention (HPFA), and a CLIP-guided attention alignment (CAA). The HPFA and CAA modules can embed the face and clothes patterns into the target image generation adaptively and effectively. Extensive experiments on both the in-shop clothes retrieval benchmark and the latest in-the-wild human editing dataset demonstrate our method's significant advantages over 13 baseline approaches for preserving both facial and clothes appearance in the source image. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03427</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03427</id><created>2025-02-05</created><authors><author><keyname>Nododile</keyname><forenames>Thandile</forenames></author><author><keyname>Nyirenda</keyname><forenames>Clement</forenames></author></authors><title>A Hybrid Blockchain-IPFS Solution for Secure and Scalable Data   Collection and Storage for Smart Water Meters</title><categories>cs.CR cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scalable and secure data management is important in Internet of Things (IoT) applications such as smart water meters, where traditional blockchain storage can be restrictive due to high data volumes. This paper investigates a hybrid blockchain and InterPlanetary File System (IPFS) approach designed to optimise storage efficiency, enhance throughput, and reduce block time by offloading large data off-chain to IPFS while preserving on-chain integrity. A substrate-based private blockchain was developed to store smart water meter (SWM) data, and controlled experiments were conducted to evaluate blockchain performance with and without IPFS. Key metrics, including block size, block time, and transaction throughput, were analysed across varying data volumes and node counts. Results show that integrating IPFS significantly reduces on-chain storage demands, leading to smaller block sizes, increased throughput, and improved block times compared to blockchain-only storage. These findings highlight the potential of hybrid blockchain-IPFS models for efficiently and securely managing high-volume IoT data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03429</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03429</id><created>2025-02-05</created><authors><author><keyname>Liu</keyname><forenames>Ming</forenames></author><author><keyname>Chen</keyname><forenames>Hao</forenames></author><author><keyname>Wang</keyname><forenames>Jindong</forenames></author><author><keyname>Wang</keyname><forenames>Liwen</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Bhiksha Raj</forenames></author><author><keyname>Zhang</keyname><forenames>Wensheng</forenames></author></authors><title>On Fairness of Unified Multimodal Large Language Model for Image   Generation</title><categories>cs.CL cs.AI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Unified multimodal large language models (U-MLLMs) have demonstrated impressive performance in visual understanding and generation in an end-to-end pipeline. Compared with generation-only models (e.g., Stable Diffusion), U-MLLMs may raise new questions about bias in their outputs, which can be affected by their unified capabilities. This gap is particularly concerning given the under-explored risk of propagating harmful stereotypes. In this paper, we benchmark the latest U-MLLMs and find that most exhibit significant demographic biases, such as gender and race bias. To better understand and mitigate this issue, we propose a locate-then-fix strategy, where we audit and show how the individual model component is affected by bias. Our analysis shows that bias originates primarily from the language model. More interestingly, we observe a "partial alignment" phenomenon in U-MLLMs, where understanding bias appears minimal, but generation bias remains substantial. Thus, we propose a novel balanced preference model to balance the demographic distribution with synthetic data. Experiments demonstrate that our approach reduces demographic bias while preserving semantic fidelity. We hope our findings underscore the need for more holistic interpretation and debiasing strategies of U-MLLMs in the future. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03430</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03430</id><created>2025-02-05</created><authors><author><keyname>Biffi</keyname><forenames>Carlo</forenames></author><author><keyname>Roffo</keyname><forenames>Giorgio</forenames></author><author><keyname>Salvagnini</keyname><forenames>Pietro</forenames></author><author><keyname>Cherubini</keyname><forenames>Andrea</forenames></author></authors><title>A Temporal Convolutional Network-Based Approach and a Benchmark Dataset   for Colonoscopy Video Temporal Segmentation</title><categories>cs.CV eess.IV</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Following recent advancements in computer-aided detection and diagnosis systems for colonoscopy, the automated reporting of colonoscopy procedures is set to further revolutionize clinical practice. A crucial yet underexplored aspect in the development of these systems is the creation of computer vision models capable of autonomously segmenting full-procedure colonoscopy videos into anatomical sections and procedural phases. In this work, we aim to create the first open-access dataset for this task and propose a state-of-the-art approach, benchmarked against competitive models. We annotated the publicly available REAL-Colon dataset, consisting of 2.7 million frames from 60 complete colonoscopy videos, with frame-level labels for anatomical locations and colonoscopy phases across nine categories. We then present ColonTCN, a learning-based architecture that employs custom temporal convolutional blocks designed to efficiently capture long temporal dependencies for the temporal segmentation of colonoscopy videos. We also propose a dual k-fold cross-validation evaluation protocol for this benchmark, which includes model assessment on unseen, multi-center data.ColonTCN achieves state-of-the-art performance in classification accuracy while maintaining a low parameter count when evaluated using the two proposed k-fold cross-validation settings, outperforming competitive models. We report ablation studies to provide insights into the challenges of this task and highlight the benefits of the custom temporal convolutional blocks, which enhance learning and improve model efficiency. We believe that the proposed open-access benchmark and the ColonTCN approach represent a significant advancement in the temporal segmentation of colonoscopy procedures, fostering further open-access research to address this clinical need. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03433</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03433</id><created>2025-02-05</created><authors><author><keyname>Buzelin</keyname><forenames>Arthur</forenames></author><author><keyname>Dutenhefner</keyname><forenames>Pedro Robles</forenames></author><author><keyname>Locatelli</keyname><forenames>Marcelo Sartori</forenames></author><author><keyname>Malaquias</keyname><forenames>Samira</forenames></author><author><keyname>Bento</keyname><forenames>Pedro</forenames></author><author><keyname>Aquino</keyname><forenames>Yan</forenames></author><author><keyname>Dayrell</keyname><forenames>Lucas</forenames></author><author><keyname>Estanislau</keyname><forenames>Victoria</forenames></author><author><keyname>Santana</keyname><forenames>Caio</forenames></author><author><keyname>Alzamora</keyname><forenames>Pedro</forenames></author><author><keyname>Vasconcelos</keyname><forenames>Marisa</forenames></author><author><keyname>Meira</keyname><forenames>Wagner</forenames><suffix>Jr.</suffix></author><author><keyname>Almeida</keyname><forenames>Virgilio</forenames></author></authors><title>Analyzing Political Discourse on Discord during the 2024 U.S.   Presidential Election</title><categories>cs.SI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Social media networks have amplified the reach of social and political movements, but most research focuses on mainstream platforms such as X, Reddit, and Facebook, overlooking Discord. As a rapidly growing, community-driven platform with optional decentralized moderation, Discord offers unique opportunities to study political discourse. This study analyzes over 30 million messages from political servers on Discord discussing the 2024 U.S. elections. Servers were classified as Republican-aligned, Democratic-aligned, or unaligned based on their descriptions. We tracked changes in political conversation during key campaign events and identified distinct political valence and implicit biases in semantic association through embedding analysis. We observed that Republican servers emphasized economic policies, while Democratic servers focused on equality-related and progressive causes. Furthermore, we detected an increase in toxic language, such as sexism, in Republican-aligned servers after Kamala Harris's nomination. These findings provide a first look at political behavior on Discord, highlighting its growing role in shaping and understanding online political engagement. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03435</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03435</id><created>2025-02-05</created><authors><author><keyname>Wu</keyname><forenames>Yu-Han</forenames></author><author><keyname>Marion</keyname><forenames>Pierre</forenames></author><author><keyname>Biau</keyname><forenames>Gérard</forenames></author><author><keyname>Boyer</keyname><forenames>Claire</forenames></author></authors><title>Taking a Big Step: Large Learning Rates in Denoising Score Matching   Prevent Memorization</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Denoising score matching plays a pivotal role in the performance of diffusion-based generative models. However, the empirical optimal score--the exact solution to the denoising score matching--leads to memorization, where generated samples replicate the training data. Yet, in practice, only a moderate degree of memorization is observed, even without explicit regularization. In this paper, we investigate this phenomenon by uncovering an implicit regularization mechanism driven by large learning rates. Specifically, we show that in the small-noise regime, the empirical optimal score exhibits high irregularity. We then prove that, when trained by stochastic gradient descent with a large enough learning rate, neural networks cannot stably converge to a local minimum with arbitrarily small excess risk. Consequently, the learned score cannot be arbitrarily close to the empirical optimal score, thereby mitigating memorization. To make the analysis tractable, we consider one-dimensional data and two-layer neural networks. Experiments validate the crucial role of the learning rate in preventing memorization, even beyond the one-dimensional setting. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03438</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03438</id><created>2025-02-05</created><authors><author><keyname>Xin</keyname><forenames>Ran</forenames></author><author><keyname>Xi</keyname><forenames>Chenguang</forenames></author><author><keyname>Yang</keyname><forenames>Jie</forenames></author><author><keyname>Chen</keyname><forenames>Feng</forenames></author><author><keyname>Wu</keyname><forenames>Hang</forenames></author><author><keyname>Xiao</keyname><forenames>Xia</forenames></author><author><keyname>Sun</keyname><forenames>Yifan</forenames></author><author><keyname>Zheng</keyname><forenames>Shen</forenames></author><author><keyname>Shen</keyname><forenames>Kai</forenames></author></authors><title>BFS-Prover: Scalable Best-First Tree Search for LLM-based Automatic   Theorem Proving</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advancements in large language models (LLMs) have spurred growing interest in automatic theorem proving using Lean4, where effective tree search methods are crucial for navigating proof search spaces. While the existing approaches primarily rely on value functions and Monte Carlo Tree Search (MCTS), the potential of simpler methods like Best-First Search (BFS) remains underexplored. This paper investigates whether BFS can achieve competitive performance in large-scale theorem proving tasks. We present \texttt{BFS-Prover}, a scalable expert iteration framework, featuring three key innovations. First, we implement strategic data filtering at each expert iteration round, excluding problems solvable via beam search node expansion to focus on harder cases. Second, we improve the sample efficiency of BFS through Direct Preference Optimization (DPO) applied to state-tactic pairs automatically annotated with compiler error feedback, refining the LLM's policy to prioritize productive expansions. Third, we employ length normalization in BFS to encourage exploration of deeper proof paths. \texttt{BFS-Prover} achieves a score of $71.31$ on the MiniF2F test set and therefore challenges the perceived necessity of complex tree search methods, demonstrating that BFS can achieve competitive performance when properly scaled. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03439</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03439</id><created>2025-02-05</created><authors><author><keyname>Linwu</keyname><forenames>Jun</forenames></author><author><keyname>Khurana</keyname><forenames>Varun</forenames></author><author><keyname>Karris</keyname><forenames>Nicholas</forenames></author><author><keyname>Cloninger</keyname><forenames>Alexander</forenames></author></authors><title>Linearized Optimal Transport pyLOT Library: A Toolkit for Machine   Learning on Point Clouds</title><categories>stat.ML cs.LG cs.MS stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The pyLOT library offers a Python implementation of linearized optimal transport (LOT) techniques and methods to use in downstream tasks. The pipeline embeds probability distributions into a Hilbert space via the Optimal Transport maps from a fixed reference distribution, and this linearization allows downstream tasks to be completed using off the shelf (linear) machine learning algorithms. We provide a case study of performing ML on 3D scans of lemur teeth, where the original questions of classification, clustering, dimension reduction, and data generation reduce to simple linear operations performed on the LOT embedded representations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03441</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03441</id><created>2025-02-05</created><authors><author><keyname>Ali</keyname><forenames>Qutaiba I.</forenames></author></authors><title>Building a Smart, Secured and Sustainable Campus: A Self-Powered   Wireless Network for Environmental Monitoring</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The objective of this study is to propose a self-powered wireless network solution that utilizes strategically deployed wireless sensor nodes within buildings for environmental data collection, while integrating advanced security measures and sustainable power management strategies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03444</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03444</id><created>2025-02-05</created><authors><author><keyname>Chen</keyname><forenames>Hao</forenames></author><author><keyname>Han</keyname><forenames>Yujin</forenames></author><author><keyname>Chen</keyname><forenames>Fangyi</forenames></author><author><keyname>Li</keyname><forenames>Xiang</forenames></author><author><keyname>Wang</keyname><forenames>Yidong</forenames></author><author><keyname>Wang</keyname><forenames>Jindong</forenames></author><author><keyname>Wang</keyname><forenames>Ze</forenames></author><author><keyname>Liu</keyname><forenames>Zicheng</forenames></author><author><keyname>Zou</keyname><forenames>Difan</forenames></author><author><keyname>Raj</keyname><forenames>Bhiksha</forenames></author></authors><title>Masked Autoencoders Are Effective Tokenizers for Diffusion Models</title><categories>cs.CV cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent advances in latent diffusion models have demonstrated their effectiveness for high-resolution image synthesis. However, the properties of the latent space from tokenizer for better learning and generation of diffusion models remain under-explored. Theoretically and empirically, we find that improved generation quality is closely tied to the latent distributions with better structure, such as the ones with fewer Gaussian Mixture modes and more discriminative features. Motivated by these insights, we propose MAETok, an autoencoder (AE) leveraging mask modeling to learn semantically rich latent space while maintaining reconstruction fidelity. Extensive experiments validate our analysis, demonstrating that the variational form of autoencoders is not necessary, and a discriminative latent space from AE alone enables state-of-the-art performance on ImageNet generation using only 128 tokens. MAETok achieves significant practical improvements, enabling a gFID of 1.69 with 76x faster training and 31x higher inference throughput for 512x512 generation. Our findings show that the structure of the latent space, rather than variational constraints, is crucial for effective diffusion models. Code and trained models are released. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03445</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03445</id><created>2025-02-05</created><authors><author><keyname>Tang</keyname><forenames>Wei</forenames></author><author><keyname>Martonosi</keyname><forenames>Margaret</forenames></author></authors><title>TensorQC: Towards Scalable Distributed Quantum Computing via Tensor   Networks</title><categories>cs.ET quant-ph</categories><comments>14 pages, 14 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  A quantum processing unit (QPU) must contain a large number of high quality qubits to produce accurate results for problems at useful scales. In contrast, most scientific and industry classical computation workloads happen in parallel on distributed systems, which rely on copying data across multiple cores. Unfortunately, copying quantum data is theoretically prohibited due to the quantum non-cloning theory. Instead, quantum circuit cutting techniques cut a large quantum circuit into multiple smaller subcircuits, distribute the subcircuits on parallel QPUs and reconstruct the results with classical computing. Such techniques make distributed hybrid quantum computing (DHQC) a possibility but also introduce an exponential classical co-processing cost in the number of cuts and easily become intractable. This paper presents TensorQC, which leverages classical tensor networks to bring an exponential runtime advantage over state-of-the-art parallelization post-processing techniques. As a result, this paper demonstrates running benchmarks that are otherwise intractable for a standalone QPU and prior circuit cutting techniques. Specifically, this paper runs six realistic benchmarks using QPUs available nowadays and a single GPU, and reduces the QPU size and quality requirements by more than $10\times$ over purely quantum platforms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03446</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03446</id><created>2025-02-05</created><authors><author><keyname>Sommariva</keyname><forenames>Alvise</forenames></author><author><keyname>Vianello</keyname><forenames>Marco</forenames></author></authors><title>Cheap and stable quadrature on polyhedral elements</title><categories>math.NA cs.NA</categories><msc-class>65D32</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We discuss a cheap tetrahedra-free approach to the numerical integration of polynomials on polyhedral elements, based on hyperinterpolation in a bounding box and Chebyshev moment computation via the divergence theorem. No conditioning issues arise, since no matrix factorization or inversion is needed. The resulting quadrature formula is theoretically stable even in the presence of some negative weights. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03447</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03447</id><created>2025-02-05</created><authors><author><keyname>Cao</keyname><forenames>Yancheng</forenames></author><author><keyname>HE</keyname><forenames>Yangyang</forenames></author><author><keyname>Chen</keyname><forenames>Yonglin</forenames></author><author><keyname>Chen</keyname><forenames>Menghan</forenames></author><author><keyname>You</keyname><forenames>Shanhe</forenames></author><author><keyname>Qiu</keyname><forenames>Yulin</forenames></author><author><keyname>Liu</keyname><forenames>Min</forenames></author><author><keyname>Luo</keyname><forenames>Chuan</forenames></author><author><keyname>Zheng</keyname><forenames>Chen</forenames></author><author><keyname>Tong</keyname><forenames>Xin</forenames></author><author><keyname>Liang</keyname><forenames>Jing</forenames></author><author><keyname>Gong</keyname><forenames>Jiangtao</forenames></author></authors><title>Designing LLM-simulated Immersive Spaces to Enhance Autistic Children's   Social Affordances Understanding</title><categories>cs.HC</categories><comments>iui2025</comments><doi>10.1145/3708359.3712142.</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  One of the key challenges faced by autistic children is understanding social affordances in complex environments, which further impacts their ability to respond appropriately to social signals. In traffic scenarios, this impairment can even lead to safety concerns. In this paper, we introduce an LLM-simulated immersive projection environment designed to improve this ability in autistic children while ensuring their safety. We first propose 17 design considerations across four major categories, derived from a comprehensive review of previous research. Next, we developed a system called AIroad, which leverages LLMs to simulate drivers with varying social intents, expressed through explicit multimodal social signals. AIroad helps autistic children bridge the gap in recognizing the intentions behind behaviors and learning appropriate responses through various stimuli. A user study involving 14 participants demonstrated that this technology effectively engages autistic children and leads to significant improvements in their comprehension of social affordances in traffic scenarios. Additionally, parents reported high perceived usability of the system. These findings highlight the potential of combining LLM technology with immersive environments for the functional rehabilitation of autistic children in the future. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03449</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03449</id><created>2025-02-05</created><authors><author><keyname>Li</keyname><forenames>Xuan</forenames></author><author><keyname>Yu</keyname><forenames>Chang</forenames></author><author><keyname>Du</keyname><forenames>Wenxin</forenames></author><author><keyname>Jiang</keyname><forenames>Ying</forenames></author><author><keyname>Xie</keyname><forenames>Tianyi</forenames></author><author><keyname>Chen</keyname><forenames>Yunuo</forenames></author><author><keyname>Yang</keyname><forenames>Yin</forenames></author><author><keyname>Jiang</keyname><forenames>Chenfanfu</forenames></author></authors><title>Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion   Prior and Differentiable Physics</title><categories>cs.CV</categories><comments>Project page: https://dress-1-to-3.github.io/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in large models have significantly advanced image-to-3D reconstruction. However, the generated models are often fused into a single piece, limiting their applicability in downstream tasks. This paper focuses on 3D garment generation, a key area for applications like virtual try-on with dynamic garment animations, which require garments to be separable and simulation-ready. We introduce Dress-1-to-3, a novel pipeline that reconstructs physics-plausible, simulation-ready separated garments with sewing patterns and humans from an in-the-wild image. Starting with the image, our approach combines a pre-trained image-to-sewing pattern generation model for creating coarse sewing patterns with a pre-trained multi-view diffusion model to produce multi-view images. The sewing pattern is further refined using a differentiable garment simulator based on the generated multi-view images. Versatile experiments demonstrate that our optimization approach substantially enhances the geometric alignment of the reconstructed 3D garments and humans with the input image. Furthermore, by integrating a texture generation module and a human motion generation module, we produce customized physics-plausible and realistic dynamic garment demonstrations. Project page: https://dress-1-to-3.github.io/ </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03450</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03450</id><created>2025-02-05</created><authors><author><keyname>Chen</keyname><forenames>Yiye</forenames></author><author><keyname>Sawhney</keyname><forenames>Harpreet</forenames></author><author><keyname>Gydé</keyname><forenames>Nicholas</forenames></author><author><keyname>Jian</keyname><forenames>Yanan</forenames></author><author><keyname>Saunders</keyname><forenames>Jack</forenames></author><author><keyname>Vela</keyname><forenames>Patricio</forenames></author><author><keyname>Lundell</keyname><forenames>Ben</forenames></author></authors><title>A Schema-Guided Reason-while-Retrieve framework for Reasoning on Scene   Graphs with Large-Language-Models (LLMs)</title><categories>cs.LG cs.AI cs.MA cs.RO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Scene graphs have emerged as a structured and serializable environment representation for grounded spatial reasoning with Large Language Models (LLMs). In this work, we propose SG-RwR, a Schema-Guided Retrieve-while-Reason framework for reasoning and planning with scene graphs. Our approach employs two cooperative, code-writing LLM agents: a (1) Reasoner for task planning and information queries generation, and a (2) Retriever for extracting corresponding graph information following the queries. Two agents collaborate iteratively, enabling sequential reasoning and adaptive attention to graph information. Unlike prior works, both agents are prompted only with the scene graph schema rather than the full graph data, which reduces the hallucination by limiting input tokens, and drives the Reasoner to generate reasoning trace abstractly.Following the trace, the Retriever programmatically query the scene graph data based on the schema understanding, allowing dynamic and global attention on the graph that enhances alignment between reasoning and retrieval. Through experiments in multiple simulation environments, we show that our framework surpasses existing LLM-based approaches in numerical Q\&amp;A and planning tasks, and can benefit from task-level few-shot examples, even in the absence of agent-level demonstrations. Project code will be released. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03454</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03454</id><created>2025-02-05</created><authors><author><keyname>Piccinini</keyname><forenames>Mattia</forenames></author><author><keyname>Taddei</keyname><forenames>Sebastiano</forenames></author><author><keyname>Betz</keyname><forenames>Johannes</forenames></author><author><keyname>Biral</keyname><forenames>Francesco</forenames></author></authors><title>Kineto-Dynamical Planning and Accurate Execution of Minimum-Time   Maneuvers on Three-Dimensional Circuits</title><categories>cs.RO</categories><comments>This paper will be presented at the 2025 IEEE International   Conference on Robotics &amp; Automation (ICRA)</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Online planning and execution of minimum-time maneuvers on three-dimensional (3D) circuits is an open challenge in autonomous vehicle racing. In this paper, we present an artificial race driver (ARD) to learn the vehicle dynamics, plan and execute minimum-time maneuvers on a 3D track. ARD integrates a novel kineto-dynamical (KD) vehicle model for trajectory planning with economic nonlinear model predictive control (E-NMPC). We use a high-fidelity vehicle simulator (VS) to compare the closed-loop ARD results with a minimum-lap-time optimal control problem (MLT-VS), solved offline with the same VS. Our ARD sets lap times close to the MLT-VS, and the new KD model outperforms a literature benchmark. Finally, we study the vehicle trajectories, to assess the re-planning capabilities of ARD under execution errors. A video with the main results is available as supplementary material. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03459</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03459</id><created>2025-02-05</created><authors><author><keyname>Sinha</keyname><forenames>Arkaprava</forenames></author><author><keyname>Reilly</keyname><forenames>Dominick</forenames></author><author><keyname>Bremond</keyname><forenames>Francois</forenames></author><author><keyname>Wang</keyname><forenames>Pu</forenames></author><author><keyname>Das</keyname><forenames>Srijan</forenames></author></authors><title>SKI Models: Skeleton Induced Vision-Language Embeddings for   Understanding Activities of Daily Living</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The introduction of vision-language models like CLIP has enabled the development of foundational video models capable of generalizing to unseen videos and human actions. However, these models are typically trained on web videos, which often fail to capture the challenges present in Activities of Daily Living (ADL) videos. Existing works address ADL-specific challenges, such as similar appearances, subtle motion patterns, and multiple viewpoints, by combining 3D skeletons and RGB videos. However, these approaches are not integrated with language, limiting their ability to generalize to unseen action classes. In this paper, we introduce SKI models, which integrate 3D skeletons into the vision-language embedding space. SKI models leverage a skeleton-language model, SkeletonCLIP, to infuse skeleton information into Vision Language Models (VLMs) and Large Vision Language Models (LVLMs) through collaborative training. Notably, SKI models do not require skeleton data during inference, enhancing their robustness for real-world applications. The effectiveness of SKI models is validated on three popular ADL datasets for zero-shot action recognition and video caption generation tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03460</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03460</id><created>2025-02-05</created><authors><author><keyname>Wang</keyname><forenames>Boyao</forenames></author><author><keyname>Pan</keyname><forenames>Rui</forenames></author><author><keyname>Diao</keyname><forenames>Shizhe</forenames></author><author><keyname>Pan</keyname><forenames>Xingyuan</forenames></author><author><keyname>Zhang</keyname><forenames>Jipeng</forenames></author><author><keyname>Pi</keyname><forenames>Renjie</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language   Model Training</title><categories>cs.LG cs.AI cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Small language models (SLMs) have attracted considerable attention from both academia and industry due to their broad range of applications in edge devices. To obtain SLMs with strong performance, conventional approaches either pre-train the models from scratch, which incurs substantial computational costs, or compress/prune existing large language models (LLMs), which results in performance drops and falls short in comparison to pre-training. In this paper, we investigate the family of acceleration methods that involve both structured pruning and model training. We found 1) layer-wise adaptive pruning (Adapt-Pruner) is extremely effective in LLMs and yields significant improvements over existing pruning techniques, 2) adaptive pruning equipped with further training leads to models comparable to those pre-training from scratch, 3) incremental pruning brings non-trivial performance gain by interleaving pruning with training and only removing a small portion of neurons ($\sim$5%) at a time. Experimental results on LLaMA-3.1-8B demonstrate that Adapt-Pruner outperforms conventional pruning methods, such as LLM-Pruner, FLAP, and SliceGPT, by an average of 1%-7% in accuracy on commonsense benchmarks. Additionally, Adapt-Pruner restores the performance of MobileLLM-125M to 600M on the MMLU benchmark with 200$\times$ fewer tokens via pruning from its larger counterparts, and discovers a new 1B model that surpasses LLaMA-3.2-1B in multiple benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03461</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03461</id><created>2025-02-05</created><authors><author><keyname>Vendrow</keyname><forenames>Joshua</forenames></author><author><keyname>Vendrow</keyname><forenames>Edward</forenames></author><author><keyname>Beery</keyname><forenames>Sara</forenames></author><author><keyname>Madry</keyname><forenames>Aleksander</forenames></author></authors><title>Do Large Language Model Benchmarks Test Reliability?</title><categories>cs.LG cs.CL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  When deploying large language models (LLMs), it is important to ensure that these models are not only capable, but also reliable. Many benchmarks have been created to track LLMs' growing capabilities, however there has been no similar focus on measuring their reliability. To understand the potential ramifications of this gap, we investigate how well current benchmarks quantify model reliability. We find that pervasive label errors can compromise these evaluations, obscuring lingering model failures and hiding unreliable behavior.   Motivated by this gap in the evaluation of reliability, we then propose the concept of so-called platinum benchmarks, i.e., benchmarks carefully curated to minimize label errors and ambiguity. As a first attempt at constructing such benchmarks, we revise examples from fifteen existing popular benchmarks. We evaluate a wide range of models on these platinum benchmarks and find that, indeed, frontier LLMs still exhibit failures on simple tasks such as elementary-level math word problems. Analyzing these failures further reveals previously unidentified patterns of problems on which frontier models consistently struggle. We provide code at https://github.com/MadryLab/platinum-benchmarks </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03465</identifier><datestamp>2025-02-06</datestamp><setSpec>cs</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03465</id><created>2025-02-05</created><authors><author><keyname>Shen</keyname><forenames>Qiuhong</forenames></author><author><keyname>Yi</keyname><forenames>Xuanyu</forenames></author><author><keyname>Lin</keyname><forenames>Mingbao</forenames></author><author><keyname>Zhang</keyname><forenames>Hanwang</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author><author><keyname>Wang</keyname><forenames>Xinchao</forenames></author></authors><title>Seeing World Dynamics in a Nutshell</title><categories>cs.CV cs.AI cs.GR cs.MM</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We consider the problem of efficiently representing casually captured monocular videos in a spatially- and temporally-coherent manner. While existing approaches predominantly rely on 2D/2.5D techniques treating videos as collections of spatiotemporal pixels, they struggle with complex motions, occlusions, and geometric consistency due to absence of temporal coherence and explicit 3D structure. Drawing inspiration from monocular video as a projection of the dynamic 3D world, we explore representing videos in their intrinsic 3D form through continuous flows of Gaussian primitives in space-time. In this paper, we propose NutWorld, a novel framework that efficiently transforms monocular videos into dynamic 3D Gaussian representations in a single forward pass. At its core, NutWorld introduces a structured spatial-temporal aligned Gaussian (STAG) representation, enabling optimization-free scene modeling with effective depth and flow regularization. Through comprehensive experiments, we demonstrate that NutWorld achieves high-fidelity video reconstruction quality while enabling various downstream applications in real-time. Demos and code will be available at https://github.com/Nut-World/NutWorld. </abstract></arXiv></metadata></record>
