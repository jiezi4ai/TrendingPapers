<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:1811.12852</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>1811.12852</id><created>2018-11-30</created><updated>2025-02-06</updated><authors><author><keyname>Burnetas</keyname><forenames>Apostolos N.</forenames></author><author><keyname>Kanavetas</keyname><forenames>Odysseas</forenames></author><author><keyname>Katehakis</keyname><forenames>Michael N.</forenames></author></authors><title>Optimal Data Driven Resource Allocation under Multi-Armed Bandit   Observations</title><categories>stat.ML cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1509.02857</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the first asymptotically optimal strategy for a multi armed bandit (MAB) model under side constraints. The side constraints model situations in which bandit activations are limited by the availability of certain resources that are replenished at a constant rate. The main result involves the derivation of an asymptotic lower bound for the regret of feasible uniformly fast policies and the construction of policies that achieve this lower bound, under pertinent conditions. Further, we provide the explicit form of such policies for the case in which the unknown distributions are Normal with unknown means and known variances, for the case of Normal distributions with unknown means and unknown variances and for the case of arbitrary discrete distributions with finite support. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2202.04912</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2202.04912</id><created>2022-02-10</created><updated>2025-02-06</updated><authors><author><keyname>Qiu</keyname><forenames>Rui</forenames></author><author><keyname>Yu</keyname><forenames>Zhou</forenames></author><author><keyname>Zhu</keyname><forenames>Ruoqing</forenames></author></authors><title>Random Forest Weighted Local Fr\'echet Regression with Random Objects</title><categories>stat.ML cs.LG</categories><comments>This paper has been published in the Journal of Machine Learning   Research</comments><journal-ref>Journal of Machine Learning Research 25 (2024) 1-69</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical analysis is increasingly confronted with complex data from metric spaces. Petersen and M\"uller (2019) established a general paradigm of Fr\'echet regression with complex metric space valued responses and Euclidean predictors. However, the local approach therein involves nonparametric kernel smoothing and suffers from the curse of dimensionality. To address this issue, we in this paper propose a novel random forest weighted local Fr\'echet regression paradigm. The main mechanism of our approach relies on a locally adaptive kernel generated by random forests. Our first method uses these weights as the local average to solve the conditional Fr\'echet mean, while the second method performs local linear Fr\'echet regression, both significantly improving existing Fr\'echet regression methods. Based on the theory of infinite order U-processes and infinite order $M_{m_n}$-estimator, we establish the consistency, rate of convergence, and asymptotic normality for our local constant estimator, which covers the current large sample theory of random forests with Euclidean responses as a special case. Numerical studies show the superiority of our methods with several commonly encountered types of responses such as distribution functions, symmetric positive-definite matrices, and sphere data. The practical merits of our proposals are also demonstrated through the application to New York taxi data and human mortality data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2206.04902</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2206.04902</id><created>2022-06-10</created><updated>2024-11-13</updated><authors><author><keyname>Gruber</keyname><forenames>Luis</forenames></author><author><keyname>Kastner</keyname><forenames>Gregor</forenames></author></authors><title>Forecasting macroeconomic data with Bayesian VARs: Sparse or dense? It   depends!</title><categories>econ.EM stat.AP stat.ME</categories><journal-ref>International Journal of Forecasting (2025)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vector autogressions (VARs) are widely applied when it comes to modeling and forecasting macroeconomic variables. In high dimensions, however, they are prone to overfitting. Bayesian methods, more concretely shrinkage priors, have shown to be successful in improving prediction performance. In the present paper, we introduce the semi-global framework, in which we replace the traditional global shrinkage parameter with group-specific shrinkage parameters. We show how this framework can be applied to various shrinkage priors, such as global-local priors and stochastic search variable selection priors. We demonstrate the virtues of the proposed framework in an extensive simulation study and in an empirical application forecasting data of the US economy. Further, we shed more light on the ongoing ``Illusion of Sparsity'' debate, finding that forecasting performances under sparse/dense priors vary across evaluated economic variables and across time frames. Dynamic model averaging, however, can combine the merits of both worlds. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2211.15353</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2211.15353</id><created>2022-11-25</created><updated>2025-02-07</updated><authors><author><keyname>Letizia</keyname><forenames>Nunzio A.</forenames></author><author><keyname>Tonello</keyname><forenames>Andrea M.</forenames></author></authors><title>Copula Density Neural Estimation</title><categories>cs.LG eess.SP stat.ML</categories><comments>6 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Probability density estimation from observed data constitutes a central task in statistics. Recent advancements in machine learning offer new tools but also pose new challenges. The big data era demands analysis of long-range spatial and long-term temporal dependencies in large collections of raw data, rendering neural networks an attractive solution for density estimation. In this paper, we exploit the concept of copula to explicitly build an estimate of the probability density function associated to any observed data. In particular, we separate univariate marginal distributions from the joint dependence structure in the data, the copula itself, and we model the latter with a neural network-based method referred to as copula density neural estimation (CODINE). Results show that the novel learning approach is capable of modeling complex distributions and it can be applied for mutual information estimation and data generation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2212.09544</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2212.09544</id><created>2022-12-19</created><authors><author><keyname>Gardini</keyname><forenames>Aldo</forenames></author><author><keyname>Greco</keyname><forenames>Fedele</forenames></author><author><keyname>Trivisano</keyname><forenames>Carlo</forenames></author></authors><title>Design and Structure Dependent Priors for Scale Parameters in Latent   Gaussian Models</title><categories>stat.ME math.ST stat.TH</categories><doi>10.1214/24-BA1454</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Many common correlation structures assumed for data can be described through latent Gaussian models. When Bayesian inference is carried out, it is required to set the prior distribution for scale parameters that rules the model components, possibly allowing to incorporate prior information. This task is particularly delicate and many contributions in the literature are devoted to investigating such aspects. We focus on the fact that the scale parameter controls the prior variability of the model component in a complex way since its dispersion is also affected by the correlation structure and the design. To overcome this issue that might confound the prior elicitation step, we propose to let the user specify the marginal prior of a measure of dispersion of the model component, integrating out the scale parameter, the structure and the design. Then, we analytically derive the implied prior for the scale parameter. Results from a simulation study, aimed at showing the behavior of the estimators sampling properties under the proposed prior elicitation strategy, are discussed. Lastly, some real data applications are explored to investigate prior sensitivity and allocation of explained variance among model components. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2304.13077</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2304.13077</id><created>2023-04-25</created><updated>2025-02-07</updated><authors><author><keyname>De Vito</keyname><forenames>Roberta</forenames></author><author><keyname>Avalos-Pacheco</keyname><forenames>Alejandra</forenames></author></authors><title>Multi-study factor regression model: an application in nutritional   epidemiology</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diet is a risk factor for many diseases. In nutritional epidemiology, studying reproducible dietary patterns is critical to reveal important associations with health. However, it is challenging: diverse cultural and ethnic backgrounds may critically impact eating patterns, showing heterogeneity, leading to incorrect dietary patterns and obscuring the components shared across different groups or populations. Moreover, covariate effects generated from observed variables, such as demographics and other confounders, can further bias these dietary patterns. Identifying the shared and group-specific dietary components and covariate effects is essential to drive accurate conclusions. To address these issues, we introduce a new modeling factor regression, the Multi-Study Factor Regression (MSFR) model. The MSFR model analyzes different populations simultaneously, achieving three goals: capturing shared component(s) across populations, identifying group-specific structures, and correcting for covariate effects. We use this novel method to derive common and ethnic-specific dietary patterns in a multi-center epidemiological study in Hispanic/Latinos community. Our model improves the accuracy of common and group dietary signals and yields better prediction than other techniques, revealing significant associations with health. In summary, we provide a tool to integrate different groups, giving accurate dietary signals crucial to inform public health policy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2305.13998</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2305.13998</id><created>2023-05-23</created><updated>2024-01-23</updated><authors><author><keyname>Saves</keyname><forenames>Paul</forenames></author><author><keyname>Lafage</keyname><forenames>Remi</forenames></author><author><keyname>Bartoli</keyname><forenames>Nathalie</forenames></author><author><keyname>Diouane</keyname><forenames>Youssef</forenames></author><author><keyname>Bussemaker</keyname><forenames>Jasper</forenames></author><author><keyname>Lefebvre</keyname><forenames>Thierry</forenames></author><author><keyname>Hwang</keyname><forenames>John T.</forenames></author><author><keyname>Morlier</keyname><forenames>Joseph</forenames></author><author><keyname>Martins</keyname><forenames>Joaquim R. R. A.</forenames></author></authors><title>SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and   Mixed Variables Gaussian Processes</title><categories>cs.LG cs.MS math.OC stat.CO</categories><comments>10.1016/j.advengsoft.2023.103571</comments><journal-ref>Advances in Engineering Software Volume 188, February 2024, 103571</journal-ref><doi>10.1016/j.advengsoft.2023.103571</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multifidelity data. To the best of our knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical and mixed inputs. This open-source software is distributed under the New BSD license. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2306.16033</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2306.16033</id><created>2023-06-28</created><authors><author><keyname>Gardini</keyname><forenames>Aldo</forenames></author></authors><title>Functional and variables selection in extreme value models for regional   flood frequency analysis</title><categories>stat.ME stat.AP</categories><doi>10.1007/s10651-023-00581-8</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The problem of estimating return levels of river discharge, relevant in flood frequency analysis, is tackled by relying on the extreme value theory. The Generalized Extreme Value (GEV) distribution is assumed to model annual maxima values of river discharge registered at multiple gauging stations belonging to the same river basin. The specific features of the data from the Upper Danube basin drive the definition of the proposed statistical model. Firstly, Bayesian P-splines are considered to account for the non-linear effects of station-specific covariates on the GEV parameters. Secondly, the problem of functional and variable selection is addressed by imposing a grouped horseshoe prior on the coefficients, to encourage the shrinkage of non-relevant components to zero. A cross-validation study is organized to compare the proposed modeling solution to other models, showing its potential in reducing the uncertainty of the ungauged predictions without affecting their calibration. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2307.10272</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2307.10272</id><created>2023-07-18</created><updated>2025-02-06</updated><authors><author><keyname>Takeishi</keyname><forenames>Shota</forenames></author></authors><title>A Shrinkage Likelihood Ratio Test for High-Dimensional Subgroup Analysis   with a Logistic-Normal Mixture Model</title><categories>math.ST stat.ME stat.TH</categories><comments>38 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In subgroup analysis, testing the existence of a subgroup with a differential treatment effect serves as protection against spurious subgroup discovery. Despite its importance, this hypothesis testing possesses a complicated nature: parameter characterizing subgroup classification is not identified under the null hypothesis of no subgroup. Due to this irregularity, the existing methods have the following two limitations. First, the asymptotic null distribution of test statistics often takes an intractable form, which necessitates computationally demanding resampling methods to calculate the critical value. Second, the dimension of personal attributes characterizing subgroup membership is not allowed to be of high dimension. To solve these two problems simultaneously, this study develops a shrinkage likelihood ratio test for the existence of a subgroup using a logistic-normal mixture model. The proposed test statistics are built on a modified likelihood function that shrinks possibly high-dimensional unidentified parameters toward zero under the null hypothesis while retaining power under the alternative. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2307.12982</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2307.12982</id><created>2023-07-24</created><updated>2025-02-07</updated><authors><author><keyname>Mukherjee</keyname><forenames>Soumendu Sundar</forenames></author></authors><title>Consistent model selection in the spiked Wigner model via AIC-type   criteria</title><categories>math.ST cs.IT math.IT stat.ME stat.ML stat.TH</categories><comments>25 pages, 2 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the spiked Wigner model \[   X = \sum_{i = 1}^k \lambda_i u_i u_i^\top + \sigma G, \] where $G$ is an $N \times N$ GOE random matrix, and the eigenvalues $\lambda_i$ are all spiked, i.e. above the Baik-Ben Arous-P\'ech\'e (BBP) threshold $\sigma$. We consider AIC-type model selection criteria of the form \[   -2 \, (\text{maximised log-likelihood}) + \gamma \, (\text{number of parameters}) \] for estimating the number $k$ of spikes. For $\gamma &gt; 2$, the above criterion is strongly consistent provided $\lambda_k &gt; \lambda_{\gamma}$, where $\lambda_{\gamma}$ is a threshold strictly above the BBP threshold, whereas for $\gamma &lt; 2$, it almost surely overestimates $k$. Although AIC (which corresponds to $\gamma = 2$) is not strongly consistent, we show that taking $\gamma = 2 + \delta_N$, where $\delta_N \to 0$ and $\delta_N \gg N^{-2/3}$, results in a weakly consistent estimator of $k$. We further show that a soft minimiser of AIC, where one chooses the least complex model whose AIC score is close to the minimum AIC score, is strongly consistent. Based on a spiked (generalised) Wigner representation, we also develop similar model selection criteria for consistently estimating the number of communities in a balanced stochastic block model under some sparsity restrictions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.07399</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.07399</id><created>2023-10-11</created><updated>2024-10-03</updated><authors><author><keyname>Bou-Rabee</keyname><forenames>Nawaf</forenames></author><author><keyname>Kleppe</keyname><forenames>Tore Selland</forenames></author></authors><title>Randomized Runge-Kutta-Nystr\"om Methods for Unadjusted Hamiltonian and   Kinetic Langevin Monte Carlo</title><categories>math.NA cs.NA math.PR stat.CO stat.ME stat.ML</categories><msc-class>60J05 (Primary) 65C05, 65L05, 65P10 (Secondary)</msc-class><journal-ref>Mathematics of Computation 2025</journal-ref><doi>10.1090/mcom/4061</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce $5/2$- and $7/2$-order $L^2$-accurate randomized Runge-Kutta-Nystr\"{o}m methods, tailored for approximating Hamiltonian flows within non-reversible Markov chain Monte Carlo samplers, such as unadjusted Hamiltonian Monte Carlo and unadjusted kinetic Langevin Monte Carlo. We establish quantitative $5/2$-order $L^2$-accuracy upper bounds under gradient and Hessian Lipschitz assumptions on the potential energy function. The numerical experiments demonstrate the superior efficiency of the proposed unadjusted samplers on a variety of well-behaved, high-dimensional target distributions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2311.02655</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2311.02655</id><created>2023-11-05</created><updated>2025-02-06</updated><authors><author><keyname>Horst</keyname><forenames>Ulrich</forenames></author><author><keyname>Xu</keyname><forenames>Wei</forenames></author></authors><title>Second-Order Regular Variation and Second-Order Approximation of Hawkes   Processes</title><categories>math.PR math.FA math.ST stat.TH</categories><comments>40 pages</comments><msc-class>Primary 26A12, 40E05, secondary 60G55, 60K05</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper provides and extends second-order versions of several fundamental theorems on first-order regularly varying functions such as Karamata's theorem/representation and Tauberian's theorem. Our results are used to establish second-order approximations for the mean and variance of Hawkes processes with general kernels. Our approximations provide novel insights into the asymptotic behavior of Hawkes processes. They are also of key importance when establishing functional limit theorems for Hawkes processes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.04711</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.04711</id><created>2024-02-07</created><updated>2024-05-26</updated><authors><author><keyname>Saves</keyname><forenames>Paul</forenames></author></authors><title>High-dimensional multidisciplinary design optimization for aircraft   eco-design / Optimisation multi-disciplinaire en grande dimension pour   l'\'eco-conception avion en avant-projet</title><categories>math.OC cs.MS stat.ML</categories><comments>PhD Thesis, Universit\'e de Toulouse, Toulouse, 2024 on Gaussian   Process kernels for Bayesian optimization in high dimension with mixed and   hierarchical variables at ISAE-SUPAERO. Keywords: Gaussian process, Black-box   optimization, Bayesian inference, Multidisciplinary design optimization,   Mixed hierarchical and categorical inputs, Eco-friendly aircraft design</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The objective of this Philosophiae Doctor (Ph.D) thesis is to propose an efficient approach for optimizing a multidisciplinary black-box model when the optimization problem is constrained and involves a large number of mixed integer design variables (typically 100 variables). The targeted optimization approach, called EGO, is based on a sequential enrichment of an adaptive surrogate model and, in this context, GP surrogate models are one of the most widely used in engineering problems to approximate time-consuming high fidelity models. EGO is a heuristic BO method that performs well in terms of solution quality. However, like any other global optimization method, EGO suffers from the curse of dimensionality, meaning that its performance is satisfactory on lower dimensional problems, but deteriorates as the dimensionality of the optimization search space increases. For realistic aircraft design problems, the typical size of the design variables can even exceed 100 and, thus, trying to solve directly the problems using EGO is ruled out. The latter is especially true when the problems involve both continuous and categorical variables increasing even more the size of the search space. In this Ph.D thesis, effective parameterization tools are investigated, including techniques like partial least squares regression, to significantly reduce the number of design variables. Additionally, Bayesian optimization is adapted to handle discrete variables and high-dimensional spaces in order to reduce the number of evaluations when optimizing innovative aircraft concepts such as the "DRAGON" hybrid airplane to reduce their climate impact. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.13196</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.13196</id><created>2024-03-19</created><updated>2025-02-07</updated><authors><author><keyname>Eskandar</keyname><forenames>Masih</forenames></author><author><keyname>Imtiaz</keyname><forenames>Tooba</forenames></author><author><keyname>Wang</keyname><forenames>Zifeng</forenames></author><author><keyname>Dy</keyname><forenames>Jennifer</forenames></author></authors><title>ADAPT to Robustify Prompt Tuning Vision Transformers</title><categories>cs.LG cs.AI cs.CV stat.ML</categories><comments>Published in Transactions on Machine Learning Research (2025)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The performance of deep models, including Vision Transformers, is known to be vulnerable to adversarial attacks. Many existing defenses against these attacks, such as adversarial training, rely on full-model fine-tuning to induce robustness in the models. These defenses require storing a copy of the entire model, that can have billions of parameters, for each task. At the same time, parameter-efficient prompt tuning is used to adapt large transformer-based models to downstream tasks without the need to save large copies. In this paper, we examine parameter-efficient prompt tuning of Vision Transformers for downstream tasks under the lens of robustness. We show that previous adversarial defense methods, when applied to the prompt tuning paradigm, suffer from gradient obfuscation and are vulnerable to adaptive attacks. We introduce ADAPT, a novel framework for performing adaptive adversarial training in the prompt tuning paradigm. Our method achieves competitive robust accuracy of ~40% w.r.t. SOTA robustness methods using full-model fine-tuning, by tuning only ~1% of the number of parameters. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.13340</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.13340</id><created>2024-03-20</created><updated>2025-02-07</updated><authors><author><keyname>Jiménez-Varón</keyname><forenames>Cristian F.</forenames></author><author><keyname>Sun</keyname><forenames>Ying</forenames></author><author><keyname>Shang</keyname><forenames>Han Lin</forenames></author></authors><title>Forecasting density-valued functional panel data</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce a statistical method for modeling and forecasting functional panel data represented by multiple densities. Density functions are nonnegative and have a constrained integral and thus do not constitute a linear vector space. We implement a center log-ratio transformation to transform densities into unconstrained functions. These functions exhibit cross-sectional correlation and temporal dependence. Via a functional analysis of variance decomposition, we decompose the unconstrained functional panel data into a deterministic trend component and a time-varying residual component. To produce forecasts for the time-varying component, a functional time series forecasting method, based on the estimation of the long-run covariance, is implemented. By combining the forecasts of the time-varying residual component with the deterministic trend component, we obtain $h$-step-ahead forecast curves for multiple populations. Illustrated by age- and sex-specific life-table death counts in the United States, we apply our proposed method to generate forecasts of the life-table death counts for 51 states. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.12120</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.12120</id><created>2024-06-17</created><updated>2025-02-06</updated><authors><author><keyname>Zhao</keyname><forenames>Yulai</forenames></author><author><keyname>Uehara</keyname><forenames>Masatoshi</forenames></author><author><keyname>Scalia</keyname><forenames>Gabriele</forenames></author><author><keyname>Kung</keyname><forenames>Sunyuan</forenames></author><author><keyname>Biancalani</keyname><forenames>Tommaso</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Hajiramezanali</keyname><forenames>Ehsan</forenames></author></authors><title>Adding Conditional Control to Diffusion Models with Reinforcement   Learning</title><categories>cs.LG cs.AI stat.ML</categories><comments>ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diffusion models are powerful generative models that allow for precise control over the characteristics of the generated samples. While these diffusion models trained on large datasets have achieved success, there is often a need to introduce additional controls in downstream fine-tuning processes, treating these powerful models as pre-trained diffusion models. This work presents a novel method based on reinforcement learning (RL) to add such controls using an offline dataset comprising inputs and labels. We formulate this task as an RL problem, with the classifier learned from the offline dataset and the KL divergence against pre-trained models serving as the reward functions. Our method, $\textbf{CTRL}$ ($\textbf{C}$onditioning pre-$\textbf{T}$rained diffusion models with $\textbf{R}$einforcement $\textbf{L}$earning), produces soft-optimal policies that maximize the abovementioned reward functions. We formally demonstrate that our method enables sampling from the conditional distribution with additional controls during inference. Our RL-based approach offers several advantages over existing methods. Compared to classifier-free guidance, it improves sample efficiency and can greatly simplify dataset construction by leveraging conditional independence between the inputs and additional controls. Additionally, unlike classifier guidance, it eliminates the need to train classifiers from intermediate states to additional controls. The code is available at https://github.com/zhaoyl18/CTRL. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.14535</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.14535</id><created>2024-06-20</created><updated>2025-02-06</updated><authors><author><keyname>Deng</keyname><forenames>Shiyuan</forenames></author><author><keyname>Tang</keyname><forenames>He</forenames></author><author><keyname>Bai</keyname><forenames>Shuyang</forenames></author></authors><title>On estimation and order selection for multivariate extremes via   clustering</title><categories>stat.ME math.ST stat.TH</categories><msc-class>62G32 (Primary), 60G70 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the estimation of multivariate extreme models with a discrete spectral measure using spherical clustering techniques. The primary contribution involves devising a method for selecting the order, that is, the number of clusters. The method consistently identifies the true order, i.e., the number of spectral atoms, and enjoys intuitive implementation in practice. Specifically, we introduce an extra penalty term to the well-known simplified average silhouette width, which penalizes small cluster sizes and small dissimilarities between cluster centers. Consequently, we provide a consistent method for determining the order of a max-linear factor model, where a typical information-based approach is not viable. Our second contribution is a large-deviation-type analysis for estimating the discrete spectral measure through clustering methods, which serves as an assessment of the convergence quality of clustering-based estimation for multivariate extremes. Additionally, as a third contribution, we discuss how estimating the discrete measure can lead to parameter estimations of heavy-tailed factor models. We also present simulations and real-data studies that demonstrate order selection and factor model estimation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.06120</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.06120</id><created>2024-07-08</created><updated>2024-11-16</updated><authors><author><keyname>Dong</keyname><forenames>Yijun</forenames></author><author><keyname>Phan</keyname><forenames>Hoang</forenames></author><author><keyname>Pan</keyname><forenames>Xiang</forenames></author><author><keyname>Lei</keyname><forenames>Qi</forenames></author></authors><title>Sketchy Moment Matching: Toward Fast and Provable Data Selection for   Finetuning</title><categories>cs.LG cs.NA math.NA stat.ML</categories><comments>NeurIPS 2024</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit data selection in a modern context of finetuning from a fundamental perspective. Extending the classical wisdom of variance minimization in low dimensions to high-dimensional finetuning, our generalization analysis unveils the importance of additionally reducing bias induced by low-rank approximation. Inspired by the variance-bias tradeoff in high dimensions from the theory, we introduce Sketchy Moment Matching (SkMM), a scalable data selection scheme with two stages. (i) First, the bias is controlled using gradient sketching that explores the finetuning parameter space for an informative low-dimensional subspace $\mathcal{S}$; (ii) then the variance is reduced over $\mathcal{S}$ via moment matching between the original and selected datasets. Theoretically, we show that gradient sketching is fast and provably accurate: selecting $n$ samples by reducing variance over $\mathcal{S}$ preserves the fast-rate generalization $O(\dim(\mathcal{S})/n)$, independent of the parameter dimension. Empirically, we concretize the variance-bias balance via synthetic experiments and demonstrate the effectiveness of SkMM for finetuning in real vision tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.21314</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.21314</id><created>2024-07-30</created><updated>2025-02-07</updated><authors><author><keyname>Li</keyname><forenames>Zhuoyuan</forenames></author><author><keyname>Dong</keyname><forenames>Bin</forenames></author><author><keyname>Zhang</keyname><forenames>Pingwen</forenames></author></authors><title>State-observation augmented diffusion model for nonlinear assimilation   with unknown dynamics</title><categories>cs.LG stat.ML</categories><msc-class>49N45, 60J60, 62F15, 68T20</msc-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Data assimilation has become a key technique for combining physical models with observational data to estimate state variables. However, classical assimilation algorithms often struggle with the high nonlinearity present in both physical and observational models. To address this challenge, a novel generative model, termed the State-Observation Augmented Diffusion (SOAD) model is proposed for data-driven assimilation. The marginal posterior associated with SOAD has been derived and then proved to match the true posterior distribution under mild assumptions, suggesting its theoretical advantages over previous score-based approaches. Experimental results also indicate that SOAD may offer improved performance compared to existing data-driven methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.03733</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.03733</id><created>2024-08-07</created><authors><author><keyname>Maillard</keyname><forenames>Antoine</forenames></author><author><keyname>Troiani</keyname><forenames>Emanuele</forenames></author><author><keyname>Martin</keyname><forenames>Simon</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Zdeborová</keyname><forenames>Lenka</forenames></author></authors><title>Bayes-optimal learning of an extensive-width neural network from   quadratically many samples</title><categories>stat.ML cond-mat.dis-nn cs.IT cs.LG math.IT math.PR</categories><comments>47 pages</comments><journal-ref>Advances in Neural Information Processing Systems 37 (NeurIPS   2024)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning a target function corresponding to a single hidden layer neural network, with a quadratic activation function after the first layer, and random weights. We consider the asymptotic limit where the input dimension and the network width are proportionally large. Recent work [Cui &amp; al '23] established that linear regression provides Bayes-optimal test error to learn such a function when the number of available samples is only linear in the dimension. That work stressed the open challenge of theoretically analyzing the optimal test error in the more interesting regime where the number of samples is quadratic in the dimension. In this paper, we solve this challenge for quadratic activations and derive a closed-form expression for the Bayes-optimal test error. We also provide an algorithm, that we call GAMP-RIE, which combines approximate message passing with rotationally invariant matrix denoising, and that asymptotically achieves the optimal performance. Technically, our result is enabled by establishing a link with recent works on optimal denoising of extensive-rank matrices and on the ellipsoid fitting problem. We further show empirically that, in the absence of noise, randomly-initialized gradient descent seems to sample the space of weights, leading to zero training loss, and averaging over initialization leads to a test error equal to the Bayes-optimal one. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.09004</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.09004</id><created>2024-08-16</created><updated>2025-02-06</updated><authors><author><keyname>Subedi</keyname><forenames>Unique</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Controlling Statistical, Discretization, and Truncation Errors in   Learning Fourier Linear Operators</title><categories>stat.ML cs.LG cs.NA math.NA</categories><comments>Added Experiments</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study learning-theoretic foundations of operator learning, using the linear layer of the Fourier Neural Operator architecture as a model problem. First, we identify three main errors that occur during the learning process: statistical error due to finite sample size, truncation error from finite rank approximation of the operator, and discretization error from handling functional data on a finite grid of domain points. Finally, we analyze a Discrete Fourier Transform (DFT) based least squares estimator, establishing both upper and lower bounds on the aforementioned errors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.09576</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.09576</id><created>2024-08-18</created><updated>2025-02-07</updated><authors><author><keyname>Oubari</keyname><forenames>Fouad</forenames></author><author><keyname>Baha</keyname><forenames>Mohamed El</forenames></author><author><keyname>Meunier</keyname><forenames>Raphael</forenames></author><author><keyname>Décatoire</keyname><forenames>Rodrigue</forenames></author><author><keyname>Mougeot</keyname><forenames>Mathilde</forenames></author></authors><title>A Markov Random Field Multi-Modal Variational AutoEncoder</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Recent advancements in multimodal Variational AutoEncoders (VAEs) have highlighted their potential for modeling complex data from multiple modalities. However, many existing approaches use relatively straightforward aggregating schemes that may not fully capture the complex dynamics present between different modalities. This work introduces a novel multimodal VAE that incorporates a Markov Random Field (MRF) into both the prior and posterior distributions. This integration aims to capture complex intermodal interactions more effectively. Unlike previous models, our approach is specifically designed to model and leverage the intricacies of these relationships, enabling a more faithful representation of multimodal data. Our experiments demonstrate that our model performs competitively on the standard PolyMNIST dataset and shows superior performance in managing complex intermodal dependencies in a specially designed synthetic dataset, intended to test intricate relationships. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.02363</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.02363</id><created>2024-09-03</created><updated>2025-02-06</updated><authors><author><keyname>Maiti</keyname><forenames>Ayan</forenames></author><author><keyname>Michelle</keyname><forenames>Michelle</forenames></author><author><keyname>Yang</keyname><forenames>Haizhao</forenames></author></authors><title>Optimal Neural Network Approximation for High-Dimensional Continuous   Functions</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the authors of Shen Yang Zhang (JMLR, 2022) developed a neural network with width $36d(2d + 1)$ and depth $11$, which utilizes a special activation function called the elementary universal activation function, to achieve the super approximation property for functions in $C([a,b]^d)$. That is, the constructed network only requires a fixed number of neurons (and thus parameters) to approximate a $d$-variate continuous function on a $d$-dimensional hypercube with arbitrary accuracy. More specifically, only $\mathcal{O}(d^2)$ neurons or parameters are used. One natural question is whether we can reduce the number of these neurons or parameters in such a network. By leveraging a variant of the Kolmogorov Superposition Theorem, our analysis shows that there is a neural network generated by the elementary universal activation function with at most $10889d+10887$ unique nonzero parameters such that this super approximation property is attained. Furthermore, we present a family of continuous functions that requires at least width $d$, and thus at least $d$ neurons or parameters, to achieve arbitrary accuracy in its approximation. This suggests that the number of unique nonzero parameters is optimal in the sense that it grows linearly with the input dimension $d$, unlike some approximation methods where parameters may grow exponentially with $d$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.09800</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.09800</id><created>2024-09-15</created><updated>2025-02-06</updated><authors><author><keyname>Calvello</keyname><forenames>Edoardo</forenames></author><author><keyname>Monmarché</keyname><forenames>Pierre</forenames></author><author><keyname>Stuart</keyname><forenames>Andrew M.</forenames></author><author><keyname>Vaes</keyname><forenames>Urbain</forenames></author></authors><title>Accuracy of the Ensemble Kalman Filter in the Near-Linear Setting</title><categories>math.ST cs.NA math.DS math.NA math.OC stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The filtering distribution captures the statistics of the state of a dynamical system from partial and noisy observations. Classical particle filters provably approximate this distribution in quite general settings; however they behave poorly for high dimensional problems, suffering weight collapse. This issue is circumvented by the ensemble Kalman filter which is an equal-weight interacting particle system. However, this finite particle system is only proven to approximate the true filter in the linear Gaussian case. In practice, however, it is applied in much broader settings; as a result, establishing its approximation properties more generally is important. There has been recent progress in the theoretical analysis of the algorithm, establishing stability and error estimates in non-Gaussian settings, but the assumptions on the dynamics and observation models rule out the unbounded vector fields that arise in practice and the analysis applies only to the mean field limit of the ensemble Kalman filter. The present work establishes error bounds between the filtering distribution and the finite particle ensemble Kalman filter when the dynamics and observation vector fields may be unbounded, allowing linear growth. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.11381</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.11381</id><created>2024-09-17</created><updated>2025-02-07</updated><authors><author><keyname>Banerjee</keyname><forenames>Debapratim</forenames></author><author><keyname>Mukherjee</keyname><forenames>Soumendu Sundar</forenames></author><author><keyname>Pal</keyname><forenames>Dipranjan</forenames></author></authors><title>Edge spectra of Gaussian random symmetric matrices with correlated   entries</title><categories>math.PR math-ph math.CO math.MP math.ST stat.TH</categories><comments>27 pages, 2 figures; abstract shortened to meet arXiv requirements</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the largest eigenvalue of a Gaussian random symmetric matrix $X_n$, with zero-mean, unit variance entries satisfying the condition $\sup_{(i, j) \ne (i', j')}|\mathbb{E}[X_{ij} X_{i'j'}]| = O(n^{-(1 + \varepsilon)})$, where $\varepsilon &gt; 0$. It follows from Catalano et al. (2024) that the empirical spectral distribution of $n^{-1/2} X_n$ converges weakly almost surely to the standard semi-circle law. Using a F\"{u}redi-Koml\'{o}s-type high moment analysis, we show that the largest eigenvalue $\lambda_1(n^{-1/2} X_n)$ of $n^{-1/2} X_n$ converges almost surely to $2$. This result is essentially optimal in the sense that one cannot take $\varepsilon = 0$ and still obtain an almost sure limit of $2$. We also derive Gaussian fluctuation results for the largest eigenvalue in the case where the entries have a common non-zero mean. Let $Y_n = X_n + \frac{\lambda}{\sqrt{n}}\mathbf{1} \mathbf{1}^\top$. When $\varepsilon \ge 1$ and $\lambda \gg n^{1/4}$, we show that \[   n^{1/2}\bigg(\lambda_1(n^{-1/2} Y_n) - \lambda - \frac{1}{\lambda}\bigg) \xrightarrow{d} \sqrt{2} Z, \] where $Z$ is a standard Gaussian. On the other hand, when $0 &lt; \varepsilon &lt; 1$, we have $\mathrm{Var}(\frac{1}{n}\sum_{i, j}X_{ij}) = O(n^{1 - \varepsilon})$. Assuming that $\mathrm{Var}(\frac{1}{n}\sum_{i, j} X_{ij}) = \sigma^2 n^{1 - \varepsilon} (1 + o(1))$, if $\lambda \gg n^{\varepsilon/4}$, then we have \[   n^{\varepsilon/2}\bigg(\lambda_1(n^{-1/2} Y_n) - \lambda - \frac{1}{\lambda}\bigg) \xrightarrow{d} \sigma Z. \] While the ranges of $\lambda$ in these fluctuation results are certainly not optimal, a striking aspect is that different scalings are required in the two regimes $0 &lt; \varepsilon &lt; 1$ and $\varepsilon \ge 1$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.19200</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.19200</id><created>2024-09-27</created><updated>2025-02-06</updated><authors><author><keyname>Bai</keyname><forenames>Site</forenames></author><author><keyname>Bullins</keyname><forenames>Brian</forenames></author></authors><title>Faster Acceleration for Steepest Descent</title><categories>math.OC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances (Sherman, 2017; Sidford and Tian, 2018; Cohen et al., 2021) have overcome the fundamental barrier of dimension dependence in the iteration complexity of solving $\ell_\infty$ regression with first-order methods. Yet it remains unclear to what extent such acceleration can be achieved for general $\ell_p$ smooth functions. In this paper, we propose a new accelerated first-order method for convex optimization under non-Euclidean smoothness assumptions. In contrast to standard acceleration techniques, our approach uses primal-dual iterate sequences taken with respect to $\textit{differing}$ norms, which are then coupled using an $\textit{implicitly}$ determined interpolation parameter. For $\ell_p$ norm smooth problems in $d$ dimensions, our method provides an iteration complexity improvement of up to $O(d^{1-\frac{2}{p}})$ in terms of calls to a first-order oracle, thereby allowing us to circumvent long-standing barriers in accelerated non-Euclidean steepest descent. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.03159</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.03159</id><created>2024-10-04</created><updated>2025-02-07</updated><authors><author><keyname>Lu</keyname><forenames>Jiecheng</forenames></author><author><keyname>Han</keyname><forenames>Xu</forenames></author><author><keyname>Sun</keyname><forenames>Yan</forenames></author><author><keyname>Yang</keyname><forenames>Shihao</forenames></author></authors><title>WAVE: Weighted Autoregressive Varing Gate for Time Series Forecasting</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Weighted Autoregressive Varing gatE (WAVE) attention mechanism equipped with both Autoregressive (AR) and Moving-average (MA) components. It can adapt to various attention mechanisms, enhancing and decoupling their ability to capture long-range and local temporal patterns in time series data. In this paper, we first demonstrate that, for the time series forecasting (TSF) task, the previously overlooked decoder-only autoregressive Transformer model can achieve results comparable to the best baselines when appropriate tokenization and training methods are applied. Moreover, inspired by the ARMA model from statistics and recent advances in linear attention, we introduce the full ARMA structure into existing autoregressive attention mechanisms. By using an indirect MA weight generation method, we incorporate the MA term while maintaining the time complexity and parameter size of the underlying efficient attention models. We further explore how indirect parameter generation can produce implicit MA weights that align with the modeling requirements for local temporal impacts. Experimental results show that WAVE attention that incorporates the ARMA structure consistently improves the performance of various AR attentions on TSF tasks, achieving state-of-the-art results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.15361</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.15361</id><created>2024-10-20</created><updated>2025-02-06</updated><authors><author><keyname>Zhou</keyname><forenames>Han</forenames></author><author><keyname>Van Landeghem</keyname><forenames>Jordy</forenames></author><author><keyname>Popordanoska</keyname><forenames>Teodora</forenames></author><author><keyname>Blaschko</keyname><forenames>Matthew B.</forenames></author></authors><title>A Novel Characterization of the Population Area Under the Risk Coverage   Curve (AURC) and Rates of Finite Sample Estimators</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The selective classifier (SC) has been proposed for rank based uncertainty thresholding, which could have applications in safety critical areas such as medical diagnostics, autonomous driving, and the justice system. The Area Under the Risk-Coverage Curve (AURC) has emerged as the foremost evaluation metric for assessing the performance of SC systems. In this work, we present a formal statistical formulation of population AURC, presenting an equivalent expression that can be interpreted as a reweighted risk function. Through Monte Carlo methods, we derive empirical AURC plug-in estimators for finite sample scenarios. The weight estimators associated with these plug-in estimators are shown to be consistent, with low bias and tightly bounded mean squared error (MSE). The plug-in estimators are proven to converge at a rate of $\mathcal{O}(\sqrt{\ln(n)/n})$ demonstrating statistical consistency. We empirically validate the effectiveness of our estimators through experiments across multiple datasets, model architectures, and confidence score functions (CSFs), demonstrating consistency and effectiveness in fine-tuning AURC performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.18959</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.18959</id><created>2024-10-24</created><updated>2025-02-06</updated><authors><author><keyname>Williams</keyname><forenames>Andrew Robert</forenames></author><author><keyname>Ashok</keyname><forenames>Arjun</forenames></author><author><keyname>Marcotte</keyname><forenames>Étienne</forenames></author><author><keyname>Zantedeschi</keyname><forenames>Valentina</forenames></author><author><keyname>Subramanian</keyname><forenames>Jithendaraa</forenames></author><author><keyname>Riachi</keyname><forenames>Roland</forenames></author><author><keyname>Requeima</keyname><forenames>James</forenames></author><author><keyname>Lacoste</keyname><forenames>Alexandre</forenames></author><author><keyname>Rish</keyname><forenames>Irina</forenames></author><author><keyname>Chapados</keyname><forenames>Nicolas</forenames></author><author><keyname>Drouin</keyname><forenames>Alexandre</forenames></author></authors><title>Context is Key: A Benchmark for Forecasting with Essential Textual   Information</title><categories>cs.LG cs.AI stat.ML</categories><comments>Preprint; under review. First two authors contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forecasting is a critical task in decision-making across numerous domains. While historical numerical data provide a start, they fail to convey the complete context for reliable and accurate predictions. Human forecasters frequently rely on additional information, such as background knowledge and constraints, which can efficiently be communicated through natural language. However, in spite of recent progress with LLM-based forecasters, their ability to effectively integrate this textual information remains an open question. To address this, we introduce "Context is Key" (CiK), a time-series forecasting benchmark that pairs numerical data with diverse types of carefully crafted textual context, requiring models to integrate both modalities; crucially, every task in CiK requires understanding textual context to be solved successfully. We evaluate a range of approaches, including statistical models, time series foundation models, and LLM-based forecasters, and propose a simple yet effective LLM prompting method that outperforms all other tested methods on our benchmark. Our experiments highlight the importance of incorporating contextual information, demonstrate surprising performance when using LLM-based forecasting models, and also reveal some of their critical shortcomings. This benchmark aims to advance multimodal forecasting by promoting models that are both accurate and accessible to decision-makers with varied technical expertise. The benchmark can be visualized at https://servicenow.github.io/context-is-key-forecasting/v0/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.19217</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.19217</id><created>2024-10-24</created><authors><author><keyname>Wu</keyname><forenames>Changlong</forenames></author><author><keyname>Grama</keyname><forenames>Ananth</forenames></author><author><keyname>Szpankowski</keyname><forenames>Wojciech</forenames></author></authors><title>No Free Lunch: Fundamental Limits of Learning Non-Hallucinating   Generative Models</title><categories>cs.LG cs.AI stat.ML</categories><journal-ref>International Conference on Learning Representations (ICLR 2025).   URL: https://openreview.net/pdf?id=OwNoTs2r8e</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative models have shown impressive capabilities in synthesizing high-quality outputs across various domains. However, a persistent challenge is the occurrence of "hallucinations", where the model produces outputs that are plausible but invalid. While empirical strategies have been explored to mitigate this issue, a rigorous theoretical understanding remains elusive. In this paper, we develop a theoretical framework to analyze the learnability of non-hallucinating generative models from a learning-theoretic perspective. Our results reveal that non-hallucinating learning is statistically impossible when relying solely on the training dataset, even for a hypothesis class of size two and when the entire training set is truthful. To overcome these limitations, we show that incorporating inductive biases aligned with the actual facts into the learning process is essential. We provide a systematic approach to achieve this by restricting the facts set to a concept class of finite VC-dimension and demonstrate its effectiveness under various learning paradigms. Although our findings are primarily conceptual, they represent a first step towards a principled approach to addressing hallucinations in learning generative models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.19725</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.19725</id><created>2024-10-25</created><updated>2025-02-06</updated><authors><author><keyname>Subedi</keyname><forenames>Unique</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>On the Benefits of Active Data Collection in Operator Learning</title><categories>stat.ML cs.LG</categories><comments>Moved Proofs to the Appendix</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study active data collection strategies for operator learning when the target operator is linear and the input functions are drawn from a mean-zero stochastic process with continuous covariance kernels. With an active data collection strategy, we establish an error convergence rate in terms of the decay rate of the eigenvalues of the covariance kernel. We can achieve arbitrarily fast error convergence rates with sufficiently rapid eigenvalue decay of the covariance kernels. This contrasts with the passive (i.i.d.) data collection strategies, where the convergence rate is never faster than linear decay ($\sim n^{-1}$). In fact, for our setting, we show a \emph{non-vanishing} lower bound for any passive data collection strategy, regardless of the eigenvalues decay rate of the covariance kernel. Overall, our results show the benefit of active data collection strategies in operator learning over their passive counterparts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.20640</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.20640</id><created>2024-10-27</created><updated>2025-02-07</updated><authors><author><keyname>Rivera</keyname><forenames>Eduardo Ochoa</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Near Optimal Pure Exploration in Logistic Bandits</title><categories>stat.ML cs.LG</categories><comments>25 pages, 2 figures. arXiv admin note: text overlap with   arXiv:2006.16073 by other authors</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bandit algorithms have garnered significant attention due to their practical applications in real-world scenarios. However, beyond simple settings such as multi-arm or linear bandits, optimal algorithms remain scarce. Notably, no optimal solution exists for pure exploration problems in the context of generalized linear model (GLM) bandits. In this paper, we narrow this gap and develop the first track-and-stop algorithm for general pure exploration problems under the logistic bandit called logistic track-and-stop (Log-TS). Log-TS is an efficient algorithm that asymptotically matches an approximation for the instance-specific lower bound of the expected sample complexity up to a logarithmic factor. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.03727</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.03727</id><created>2024-12-04</created><updated>2025-02-07</updated><authors><author><keyname>Zhang</keyname><forenames>Zhiheng</forenames></author><author><keyname>Wang</keyname><forenames>Zichen</forenames></author></authors><title>Online Experimental Design With Estimation-Regret Trade-off Under   Network Interference</title><categories>cs.LG math.OC math.ST stat.TH</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network interference has attracted significant attention in the field of causal inference, encapsulating various sociological behaviors where the treatment assigned to one individual within a network may affect the outcomes of others, such as their neighbors. A key challenge in this setting is that standard causal inference methods often assume independent treatment effects among individuals, which may not hold in networked environments. To estimate interference-aware causal effects, a traditional approach is to inherit the independent settings, where practitioners randomly assign experimental participants into different groups and compare their outcomes. While effective in offline settings, this strategy becomes problematic in sequential experiments, where suboptimal decision persists, leading to substantial regret. To address this issue, we introduce a unified interference-aware framework for online experimental design. Compared to existing studies, we extend the definition of arm space by utilizing the statistical concept of exposure mapping, which allows for a more flexible and context-aware representation of treatment effects in networked settings. Crucially, we establish a Pareto-optimal trade-off between estimation accuracy and regret under the network concerning both time period and arm space, which remains superior to baseline models even without network interference. Furthermore, we propose an algorithmic implementation and discuss its generalization across different learning settings and network topology </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.04767</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.04767</id><created>2024-12-05</created><updated>2025-02-06</updated><authors><author><keyname>Tian</keyname><forenames>Bowei</forenames></author><author><keyname>Wang</keyname><forenames>Ziyao</forenames></author><author><keyname>He</keyname><forenames>Shwai</forenames></author><author><keyname>Ye</keyname><forenames>Wanghao</forenames></author><author><keyname>Sun</keyname><forenames>Guoheng</forenames></author><author><keyname>Dai</keyname><forenames>Yucong</forenames></author><author><keyname>Wu</keyname><forenames>Yongkai</forenames></author><author><keyname>Li</keyname><forenames>Ang</forenames></author></authors><title>Towards counterfactual fairness through auxiliary variables</title><categories>cs.LG cs.DS stat.ML</categories><comments>arXiv admin note: text overlap with arXiv:2307.08232 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The challenge of balancing fairness and predictive accuracy in machine learning models, especially when sensitive attributes such as race, gender, or age are considered, has motivated substantial research in recent years. Counterfactual fairness ensures that predictions remain consistent across counterfactual variations of sensitive attributes, which is a crucial concept in addressing societal biases. However, existing counterfactual fairness approaches usually overlook intrinsic information about sensitive features, limiting their ability to achieve fairness while simultaneously maintaining performance. To tackle this challenge, we introduce EXOgenous Causal reasoning (EXOC), a novel causal reasoning framework motivated by exogenous variables. It leverages auxiliary variables to uncover intrinsic properties that give rise to sensitive attributes. Our framework explicitly defines an auxiliary node and a control node that contribute to counterfactual fairness and control the information flow within the model. Our evaluation, conducted on synthetic and real-world datasets, validates EXOC's superiority, showing that it outperforms state-of-the-art approaches in achieving counterfactual fairness. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.08435</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.08435</id><created>2024-12-11</created><updated>2025-02-07</updated><authors><author><keyname>Zhao</keyname><forenames>Lifan</forenames></author><author><keyname>Shen</keyname><forenames>Yanyan</forenames></author></authors><title>Proactive Model Adaptation Against Concept Drift for Online Time Series   Forecasting</title><categories>cs.LG cs.AI cs.CE stat.ML</categories><comments>Accepted by KDD 2025</comments><doi>10.1145/3690624.3709210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time series forecasting always faces the challenge of concept drift, where data distributions evolve over time, leading to a decline in forecast model performance. Existing solutions are based on online learning, which continually organize recent time series observations as new training samples and update model parameters according to the forecasting feedback on recent data. However, they overlook a critical issue: obtaining ground-truth future values of each sample should be delayed until after the forecast horizon. This delay creates a temporal gap between the training samples and the test sample. Our empirical analysis reveals that the gap can introduce concept drift, causing forecast models to adapt to outdated concepts. In this paper, we present Proceed, a novel proactive model adaptation framework for online time series forecasting. Proceed first estimates the concept drift between the recently used training samples and the current test sample. It then employs an adaptation generator to efficiently translate the estimated drift into parameter adjustments, proactively adapting the model to the test sample. To enhance the generalization capability of the framework, Proceed is trained on synthetic diverse concept drifts. Extensive experiments on five real-world datasets across various forecast models demonstrate that Proceed brings more performance improvements than the state-of-the-art online learning methods, significantly facilitating forecast models' resilience against concept drifts. Code is available at https://github.com/SJTU-DMTai/OnlineTSF. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.20471</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.20471</id><created>2024-12-29</created><updated>2025-02-07</updated><authors><author><keyname>Cai</keyname><forenames>Yang</forenames></author><author><keyname>Mitra</keyname><forenames>Siddharth</forenames></author><author><keyname>Wang</keyname><forenames>Xiuyuan</forenames></author><author><keyname>Wibisono</keyname><forenames>Andre</forenames></author></authors><title>On the Convergence of Min-Max Langevin Dynamics and Algorithm</title><categories>cs.GT cs.LG math.OC stat.ML</categories><comments>v2: Revised introduction and presentation of results</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study zero-sum games in the space of probability distributions over the Euclidean space $\mathbb{R}^d$ with entropy regularization, in the setting when the interaction function between the players is smooth and strongly convex-strongly concave. We prove an exponential convergence guarantee for the mean-field min-max Langevin dynamics to compute the equilibrium distribution of the zero-sum game. We also study the finite-particle approximation of the mean-field min-max Langevin dynamics, both in continuous and discrete times. We prove biased convergence guarantees for the continuous-time finite-particle min-max Langevin dynamics to the stationary mean-field equilibrium distribution with an explicit bias term which does not scale with the number of particles. We also prove biased convergence guarantees for the discrete-time finite-particle min-max Langevin algorithm to the stationary mean-field equilibrium distribution with an additional bias term which scales with the step size and the number of particles. This provides an explicit iteration complexity for the average particle along the finite-particle algorithm to approximately compute the equilibrium distribution of the zero-sum game. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.01437</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.01437</id><created>2024-12-23</created><updated>2025-02-06</updated><authors><author><keyname>Murphy</keyname><forenames>Charles</forenames></author><author><keyname>Lizotte</keyname><forenames>Simon</forenames></author><author><keyname>Thibault</keyname><forenames>François</forenames></author><author><keyname>Thibeault</keyname><forenames>Vincent</forenames></author><author><keyname>Desrosiers</keyname><forenames>Patrick</forenames></author><author><keyname>Allard</keyname><forenames>Antoine</forenames></author></authors><title>On the reconstruction limits of complex networks</title><categories>stat.AP cs.IT math.IT physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network reconstruction consists in retrieving the hidden interaction structure of a system from observations. Many reconstruction algorithms have been proposed, although less research has been devoted to describe their theoretical limitations. In this work, we adopt an information-theoretic perspective and define the reconstructability: The fraction of structural information recoverable from data. The reconstructability depends on the true data generating (TDG) model which is shown to set the reconstruction limit: any algorithm can perform, on average, at best like the TDG model. We show that the reconstructability is related to various performance measures, such as the probability of error and the Jaccard similarity. In an empirical context where the TDG model is unknown, we introduce the reconstruction index as an approximation of the reconstructability. We find that performing model selection is crucial for the validity of the reconstruction index as a proxy of the reconstructability of empirical time series and networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.07025</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.07025</id><created>2025-01-12</created><updated>2025-02-07</updated><authors><author><keyname>Zhang</keyname><forenames>Yong</forenames></author><author><keyname>Gyamfi</keyname><forenames>Eric Herrison</forenames></author></authors><title>A Weighted Similarity Metric for Community Detection in Sparse Data</title><categories>stat.ME cs.SI</categories><comments>This paper has been accepted for Workshop of AI for Social Impact at   AAAI 2025</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Many Natural Language Processing (NLP) related applications involves topics and sentiments derived from short documents such as consumer reviews and social media posts. Topics and sentiments of short documents are highly sparse because a short document generally covers a few topics among hundreds of candidates. Imputation of missing data is sometimes hard to justify and also often unpractical in highly sparse data. We developed a method for calculating a weighted similarity for highly sparse data without imputation. This weighted similarity is consist of three components to capture similarities based on both existence and lack of common properties and pattern of missing values. As a case study, we used a community detection algorithm and this weighted similarity to group different shampoo brands based on sparse topic sentiments derived from short consumer reviews. Compared with traditional imputation and similarity measures, the weighted similarity shows better performance in both general community structures and average community qualities. The performance is consistent and robust across metrics and community complexities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10117</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10117</id><created>2025-01-17</created><updated>2025-02-07</updated><authors><author><keyname>Liu</keyname><forenames>Weiguang</forenames></author><author><keyname>de Paula</keyname><forenames>Áureo</forenames></author><author><keyname>Tamer</keyname><forenames>Elie</forenames></author></authors><title>Prediction Sets and Conformal Inference with Censored Outcomes</title><categories>econ.EM stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given data on a scalar random variable $Y$, a prediction set for $Y$ with miscoverage level $\alpha$ is a set of values for $Y$ that contains a randomly drawn $Y$ with probability $1 - \alpha$, where $\alpha \in (0,1)$. Among all prediction sets that satisfy this coverage property, the oracle prediction set is the one with the smallest volume. This paper provides estimation methods of such prediction sets given observed conditioning covariates when $Y$ is \textit{censored} or \textit{measured in intervals}. We first characterise the oracle prediction set under interval censoring and develop a consistent estimator for the shortest prediction {\it interval} that satisfies this coverage property.These consistency results are extended to accommodate cases where the prediction set consists of multiple disjoint intervals. We use conformal inference to construct a prediction set that achieves finite-sample validity under censoring and maintains consistency as sample size increases, using a conformity score function designed for interval data. The procedure accommodates the prediction uncertainty that is irreducible (due to the stochastic nature of outcomes), the modelling uncertainty due to partial identification and also sampling uncertainty that gets reduced as samples get larger. We conduct a set of Monte Carlo simulations and an application to data from the Current Population Survey. The results highlight the robustness and efficiency of the proposed methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10974</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10974</id><created>2025-01-19</created><updated>2025-02-06</updated><authors><author><keyname>Huang</keyname><forenames>Yu-Han</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Sequential Change Detection for Learning in Piecewise Stationary Bandit   Environments</title><categories>cs.IT cs.SY eess.SY math.IT stat.OT</categories><comments>15 pages, 2 figures. arXiv admin note: text overlap with   arXiv:2501.01291</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A finite-horizon variant of the quickest change detection problem is investigated, which is motivated by a change detection problem that arises in piecewise stationary bandits. The goal is to minimize the \emph{latency}, which is smallest threshold such that the probability that the detection delay exceeds the threshold is below a desired low level, while controlling the false alarm probability to a desired low level. When the pre- and post-change distributions are unknown, two tests are proposed as candidate solutions. These tests are shown to attain order optimality in terms of the horizon. Furthermore, the growth in their latencies with respect to the false alarm probability and late detection probability satisfies a property that is desirable in regret analysis for piecewise stationary bandits. Numerical results are provided to validate the theoretical performance results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14974</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14974</id><created>2025-01-24</created><updated>2025-02-06</updated><authors><author><keyname>Deng</keyname><forenames>Fengnan</forenames></author><author><keyname>Vidyashankar</keyname><forenames>Anand N.</forenames></author></authors><title>Private Minimum Hellinger Distance Estimation via Hellinger Distance   Differential Privacy</title><categories>math.ST cs.CR math.PR stat.ME stat.ML stat.TH</categories><msc-class>62F35, 68P27, 62E20, 60E05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective functions based on Hellinger distance yield robust and efficient estimators of model parameters. Motivated by privacy and regulatory requirements encountered in contemporary applications, we derive in this paper \emph{private minimum Hellinger distance estimators}. The estimators satisfy a new privacy constraint, namely, Hellinger differential privacy, while retaining the robustness and efficiency properties. We demonstrate that Hellinger differential privacy shares several features of standard differential privacy while allowing for sharper inference. Additionally, for computational purposes, we also develop Hellinger differentially private gradient descent and Newton-Raphson algorithms. We illustrate the behavior of our estimators in finite samples using numerical experiments and verify that they retain robustness properties under gross-error contamination. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18502</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18502</id><created>2025-01-30</created><updated>2025-02-07</updated><authors><author><keyname>Kumar</keyname><forenames>Ritesh</forenames></author><author><keyname>Vatedka</keyname><forenames>Shashank</forenames></author></authors><title>One-Bit Distributed Mean Estimation with Unknown Variance</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>21 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this work, we study the problem of distributed mean estimation with $1$-bit communication constraints when the variance is unknown. We focus on the specific case where each user has access to one i.i.d. sample drawn from a distribution that belongs to a scale-location family, and is limited to sending just a single bit of information to a central server whose goal is to estimate the mean. We propose non-adaptive and adaptive estimators that are shown to be asymptotically normal. We derive bounds on the asymptotic (in the number of users) Mean Squared Error (MSE) achieved by these estimators. For a class of symmetric log-concave distributions, we derive matching lower bounds for the MSE achieved by adaptive estimators, proving the optimality of our scheme. We show that non-adaptive estimators can be strictly suboptimal by deriving a lower bound on the MSE achieved by any non-adaptive estimator for Gaussian distributions and demonstrating a positive gap between this and the MSE achieved by our adaptive scheme. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00168</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00168</id><created>2025-01-31</created><updated>2025-02-07</updated><authors><author><keyname>Herrera-Esposito</keyname><forenames>Daniel</forenames></author><author><keyname>Burge</keyname><forenames>Johannes</forenames></author></authors><title>Supervised Quadratic Feature Analysis: An Information Geometry Approach   to Dimensionality Reduction</title><categories>stat.ML cs.LG math.DG math.ST stat.TH</categories><comments>18 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Supervised dimensionality reduction aims to map labeled data to a low-dimensional feature space while maximizing class discriminability. Despite the availability of methods for learning complex non-linear features (e.g. Deep Learning), there is an enduring demand for dimensionality reduction methods that learn linear features due to their interpretability, low computational cost, and broad applicability. However, there is a gap between methods that optimize linear separability (e.g. LDA), and more flexible but computationally expensive methods that optimize over arbitrary class boundaries (e.g. metric-learning methods). Here, we present Supervised Quadratic Feature Analysis (SQFA), a dimensionality reduction method for learning linear features that maximize the differences between class-conditional first- and second-order statistics, which allow for quadratic discrimination. SQFA exploits the information geometry of second-order statistics in the symmetric positive definite manifold. We show that SQFA features support quadratic discriminability in real-world problems. We also provide a theoretical link, based on information geometry, between SQFA and the Quadratic Discriminant Analysis (QDA) classifier. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00182</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00182</id><created>2025-01-31</created><updated>2025-02-07</updated><authors><author><keyname>Seo</keyname><forenames>Jungwon</forenames></author><author><keyname>Catak</keyname><forenames>Ferhat Ozgur</forenames></author><author><keyname>Rong</keyname><forenames>Chunming</forenames></author></authors><title>Understanding Federated Learning from IID to Non-IID dataset: An   Experimental Study</title><categories>cs.LG cs.AI stat.ML</categories><journal-ref>36th Norwegian ICT Conference for Research and Education, NIKT   2024</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As privacy concerns and data regulations grow, federated learning (FL) has emerged as a promising approach for training machine learning models across decentralized data sources without sharing raw data. However, a significant challenge in FL is that client data are often non-IID (non-independent and identically distributed), leading to reduced performance compared to centralized learning. While many methods have been proposed to address this issue, their underlying mechanisms are often viewed from different perspectives. Through a comprehensive investigation from gradient descent to FL, and from IID to non-IID data settings, we find that inconsistencies in client loss landscapes primarily cause performance degradation in non-IID scenarios. From this understanding, we observe that existing methods can be grouped into two main strategies: (i) adjusting parameter update paths and (ii) modifying client loss landscapes. These findings offer a clear perspective on addressing non-IID challenges in FL and help guide future research in the field. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00838</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00838</id><created>2025-02-02</created><authors><author><keyname>Bussemaker</keyname><forenames>Jasper H.</forenames></author><author><keyname>Saves</keyname><forenames>Paul</forenames></author><author><keyname>Bartoli</keyname><forenames>Nathalie</forenames></author><author><keyname>Lefebvre</keyname><forenames>Thierry</forenames></author><author><keyname>Lafage</keyname><forenames>Rémi</forenames></author></authors><title>System Architecture Optimization Strategies: Dealing with Expensive   Hierarchical Problems</title><categories>math.OC cs.DM stat.AP</categories><comments>J Glob Optim (2024)</comments><doi>10.1007/s10898-024-01443-8</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Choosing the right system architecture for the problem at hand is challenging due to the large design space and high uncertainty in the early stage of the design process. Formulating the architecting process as an optimization problem may mitigate some of these challenges. This work investigates strategies for solving System Architecture Optimization (SAO) problems: expensive, black-box, hierarchical, mixed-discrete, constrained, multi-objective problems that may be subject to hidden constraints. Imputation ratio, correction ratio, correction fraction, and max rate diversity metrics are defined for characterizing hierar chical design spaces. This work considers two classes of optimization algorithms for SAO: Multi-Objective Evolutionary Algorithms (MOEA) such as NSGA-II, and Bayesian Optimization (BO) algorithms. A new Gaussian process kernel is presented that enables modeling hierarchical categorical variables, extending previous work on modeling continuous and integer hierarchical variables. Next, a hierarchical sampling algorithm that uses design space hierarchy to group design vectors by active design variables is developed. Then, it is demonstrated that integrating more hierarchy information in the optimization algorithms yields better optimization results for BO algorithms. Several realistic single-objective and multi-objective test problems are used for investigations. Finally, the BO algorithm is applied to a jet engine architecture optimization problem. This work shows that the developed BO algorithm can effectively solve the problem with one order of magnitude less function evaluations than NSGA-II. The algorithms and problems used in this work are implemented in the open-source Python library SBArchOpt. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00854</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00854</id><created>2025-02-02</created><authors><author><keyname>Priem</keyname><forenames>Rémy</forenames></author><author><keyname>Diouane</keyname><forenames>Youssef</forenames></author><author><keyname>Bartoli</keyname><forenames>Nathalie</forenames></author><author><keyname>Dubreuil</keyname><forenames>Sylvain</forenames></author><author><keyname>Saves</keyname><forenames>Paul</forenames></author></authors><title>High-Dimensional Bayesian Optimization Using Both Random and Supervised   Embeddings</title><categories>math.OC cs.LG stat.ML</categories><journal-ref>AIAA Journal 2025 63:1, 162-173</journal-ref><doi>10.2514/1.J063488</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Bayesian optimization (BO) is one of the most powerful strategies to solve computationally expensive-to-evaluate blackbox optimization problems. However, BO methods are conventionally used for optimization problems of small dimension because of the curse of dimensionality. In this paper, a high-dimensionnal optimization method incorporating linear embedding subspaces of small dimension is proposed to efficiently perform the optimization. An adaptive learning strategy for these linear embeddings is carried out in conjunction with the optimization. The resulting BO method, named efficient global optimization coupled with random and supervised embedding (EGORSE), combines in an adaptive way both random and supervised linear embeddings. EGORSE has been compared to state-of-the-art algorithms and tested on academic examples with a number of design variables ranging from 10 to 600. The obtained results show the high potential of EGORSE to solve high-dimensional blackbox optimization problems, in terms of both CPU time and the limited number of calls to the expensive blackbox simulation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02496</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02496</id><created>2025-02-04</created><updated>2025-02-07</updated><authors><author><keyname>Kolb</keyname><forenames>Chris</forenames></author><author><keyname>Weber</keyname><forenames>Tobias</forenames></author><author><keyname>Bischl</keyname><forenames>Bernd</forenames></author><author><keyname>Rügamer</keyname><forenames>David</forenames></author></authors><title>Deep Weight Factorization: Sparse Learning Through the Lens of   Artificial Symmetries</title><categories>cs.LG stat.ML</categories><comments>accepted at ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sparse regularization techniques are well-established in machine learning, yet their application in neural networks remains challenging due to the non-differentiability of penalties like the $L_1$ norm, which is incompatible with stochastic gradient descent. A promising alternative is shallow weight factorization, where weights are decomposed into two factors, allowing for smooth optimization of $L_1$-penalized neural networks by adding differentiable $L_2$ regularization to the factors. In this work, we introduce deep weight factorization, extending previous shallow approaches to more than two factors. We theoretically establish equivalence of our deep factorization with non-convex sparse regularization and analyze its impact on training dynamics and optimization. Due to the limitations posed by standard training practices, we propose a tailored initialization scheme and identify important learning rate requirements necessary for training factorized networks. We demonstrate the effectiveness of our deep weight factorization through experiments on various architectures and datasets, consistently outperforming its shallow counterpart and widely used pruning methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02861</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02861</id><created>2025-02-04</created><updated>2025-02-06</updated><authors><author><keyname>Shen</keyname><forenames>Judy Hanwen</forenames></author><author><keyname>Vitercik</keyname><forenames>Ellen</forenames></author><author><keyname>Wikum</keyname><forenames>Anders</forenames></author></authors><title>Algorithms with Calibrated Machine Learning Predictions</title><categories>stat.ML cs.DS cs.LG</categories><comments>Replacement: updated acknowledgments</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The field of algorithms with predictions incorporates machine learning advice in the design of online algorithms to improve real-world performance. While this theoretical framework often assumes uniform reliability across all predictions, modern machine learning models can now provide instance-level uncertainty estimates. In this paper, we propose calibration as a principled and practical tool to bridge this gap, demonstrating the benefits of calibrated advice through two case studies: the ski rental and online job scheduling problems. For ski rental, we design an algorithm that achieves optimal prediction-dependent performance and prove that, in high-variance settings, calibrated advice offers more effective guidance than alternative methods for uncertainty quantification. For job scheduling, we demonstrate that using a calibrated predictor leads to significant performance improvements over existing methods. Evaluations on real-world data validate our theoretical findings, highlighting the practical impact of calibration for algorithms with predictions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03479</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03479</id><created>2025-01-27</created><updated>2025-02-06</updated><authors><author><keyname>Cuicizion</keyname><forenames>Eliuvish</forenames></author><author><keyname>Ri</keyname><forenames>Itsugo</forenames></author><author><keyname>Holmes</keyname><forenames>Elaine</forenames></author><author><keyname>Lyeutsaon</keyname><forenames>Sho</forenames></author><author><keyname>Chern</keyname><forenames>Jawad Hassan</forenames></author></authors><title>A Tutorial on Markov Renewal and Semi-Markov Proportional Hazards Model</title><categories>stat.AP stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Transition probability estimation plays a critical role in multi-state modeling, especially in clinical research. This paper investigates the application of semi-Markov and Markov renewal frameworks to the EBMT dataset, focusing on six clinical states encountered during hematopoietic stem cell transplantation. By comparing Aalen-Johansen (AJ) and Dabrowska-Sun-Horowitz (DSH) estimators, we demonstrate that semi-Markov models, which incorporate sojourn times, provide a more nuanced and temporally sensitive depiction of patient trajectories compared to memoryless Markov models. The DSH estimator consistently yields smoother probability curves, particularly for transitions involving prolonged states. We use empirical process theory and Burkholder-Davis-Gundy inequality to show weak convergence of the estimator. Future work includes extending the framework to accommodate advanced covariate structures and non-Markovian dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03587</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03587</id><created>2025-02-05</created><updated>2025-02-06</updated><authors><author><keyname>von Seeger</keyname><forenames>Anneke</forenames></author><author><keyname>Zou</keyname><forenames>Dongmian</forenames></author><author><keyname>Lerman</keyname><forenames>Gilad</forenames></author></authors><title>Stein Discrepancy for Unsupervised Domain Adaptation</title><categories>cs.LG stat.ML</categories><comments>24 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Unsupervised domain adaptation (UDA) leverages information from a labeled source dataset to improve accuracy on a related but unlabeled target dataset. A common approach to UDA is aligning representations from the source and target domains by minimizing the distance between their data distributions. Previous methods have employed distances such as Wasserstein distance and maximum mean discrepancy. However, these approaches are less effective when the target data is significantly scarcer than the source data. Stein discrepancy is an asymmetric distance between distributions that relies on one distribution only through its score function. In this paper, we propose a novel UDA method that uses Stein discrepancy to measure the distance between source and target domains. We develop a learning framework using both non-kernelized and kernelized Stein discrepancy. Theoretically, we derive an upper bound for the generalization error. Numerical experiments show that our method outperforms existing methods using other domain discrepancy measures when only small amounts of target data are available. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03942</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03942</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Holst</keyname><forenames>Klaus Kähler</forenames></author><author><keyname>Nordland</keyname><forenames>Andreas</forenames></author><author><keyname>Furkjær</keyname><forenames>Julie</forenames></author><author><keyname>Damgaard</keyname><forenames>Lars Holm</forenames></author><author><keyname>Pipper</keyname><forenames>Christian Bressen</forenames></author></authors><title>A retake on the analysis of scores truncated by terminal events</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of data from randomized controlled trials in vulnerable populations requires special attention when assessing treatment effect by a score measuring, e.g., disease stage or activity together with onset of prevalent terminal events. In reality, it is impossible to disentangle a disease score from the terminal event, since the score is not clinically meaningful after this event. In this work, we propose to assess treatment interventions simultaneously on disease score and the terminal event. Our proposal is based on a natural data-generating mechanism respecting that a disease score does not exist beyond the terminal event. We use modern semi-parametric statistical methods to provide robust and efficient estimation of the risk of terminal event and expected disease score conditional on no terminal event at a pre-specified landmark time. We also use the simultaneous asymptotic behavior of our estimators to develop a powerful closed testing procedure for confirmatory assessment of treatment effect on both onset of terminal event and level of disease score. A simulation study mimicking a large-scale outcome trial in chronic kidney patients as well as an analysis of that trial is provided to assess performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04168</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04168</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Ferradini</keyname><forenames>Carla</forenames></author><author><keyname>Gitton</keyname><forenames>Victor</forenames></author><author><keyname>Vilasini</keyname><forenames>V.</forenames></author></authors><title>Cyclic quantum causal modelling with a graph separation theorem</title><categories>quant-ph math.ST stat.ML stat.TH</categories><comments>41+41 pages. A companion paper by the same authors, focussing on   cyclic classical (functional) causal models has been submitted to the arXiv   concurrently with primary class [math.ST], v2 only differs from v1 in   including the arXiv number of the companion paper. Comments are welcome</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Causal modelling frameworks link observable correlations to causal explanations, which is a crucial aspect of science. These models represent causal relationships through directed graphs, with vertices and edges denoting systems and transformations within a theory. Most studies focus on acyclic causal graphs, where well-defined probability rules and powerful graph-theoretic properties like the d-separation theorem apply. However, understanding complex feedback processes and exotic fundamental scenarios with causal loops requires cyclic causal models, where such results do not generally hold. While progress has been made in classical cyclic causal models, challenges remain in uniquely fixing probability distributions and identifying graph-separation properties applicable in general cyclic models. In cyclic quantum scenarios, existing frameworks have focussed on a subset of possible cyclic causal scenarios, with graph-separation properties yet unexplored. This work proposes a framework applicable to all consistent quantum and classical cyclic causal models on finite-dimensional systems. We address these challenges by introducing a robust probability rule and a novel graph-separation property, p-separation, which we prove to be sound and complete for all such models. Our approach maps cyclic causal models to acyclic ones with post-selection, leveraging the post-selected quantum teleportation protocol. We characterize these protocols and their success probabilities along the way. We also establish connections between this formalism and other classical and quantum frameworks to inform a more unified perspective on causality. This provides a foundation for more general cyclic causal discovery algorithms and to systematically extend open problems and techniques from acyclic informational networks (e.g., certification of non-classicality) to cyclic causal structures and networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04171</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04171</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Ferradini</keyname><forenames>Carla</forenames></author><author><keyname>Gitton</keyname><forenames>Victor</forenames></author><author><keyname>Vilasini</keyname><forenames>V.</forenames></author></authors><title>Cyclic functional causal models beyond unique solvability with a graph   separation theorem</title><categories>math.ST quant-ph stat.ML stat.TH</categories><comments>33+16 pages. A companion paper by the same authors, focussing on   cyclic quantum causal models has been submitted to the arXiv concurrently   with primary class [quant-ph], v2 only differs from v1 in including the arXiv   number of the companion paper. Comments are welcome</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Functional causal models (fCMs) specify functional dependencies between random variables associated to the vertices of a graph. In directed acyclic graphs (DAGs), fCMs are well-understood: a unique probability distribution on the random variables can be easily specified, and a crucial graph-separation result called the d-separation theorem allows one to characterize conditional independences between the variables. However, fCMs on cyclic graphs pose challenges due to the absence of a systematic way to assign a unique probability distribution to the fCM's variables, the failure of the d-separation theorem, and lack of a generalization of this theorem that is applicable to all consistent cyclic fCMs. In this work, we develop a causal modeling framework applicable to all cyclic fCMs involving finite-cardinality variables, except inconsistent ones admitting no solutions. Our probability rule assigns a unique distribution even to non-uniquely solvable cyclic fCMs and reduces to the known rule for uniquely solvable fCMs. We identify a class of fCMs, called averagely uniquely solvable, that we show to be the largest class where the probabilities admit a Markov factorization. Furthermore, we introduce a new graph-separation property, p-separation, and prove this to be sound and complete for all consistent finite-cardinality cyclic fCMs while recovering the d-separation theorem for DAGs. These results are obtained by considering classical post-selected teleportation protocols inspired by analogous protocols in quantum information theory. We discuss further avenues for exploration, linking in particular problems in cyclic fCMs and in quantum causality. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04208</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04208</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Grünwald</keyname><forenames>Peter D.</forenames></author><author><keyname>Koolen</keyname><forenames>Wouter M.</forenames></author></authors><title>Supermartingales for One-Sided Tests: Sufficient Monotone Likelihood   Ratios are Sufficient</title><categories>math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The t-statistic is a widely-used scale-invariant statistic for testing the null hypothesis that the mean is zero. Martingale methods enable sequential testing with the t-statistic at every sample size, while controlling the probability of falsely rejecting the null. For one-sided sequential tests, which reject when the t-statistic is too positive, a natural question is whether they also control false rejection when the true mean is negative. We prove that this is the case using monotone likelihood ratios and sufficient statistics. We develop applications to the scale-invariant t-test, the location-invariant $\chi^2$-test and sequential linear regression with nuisance covariates. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04339</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04339</id><created>2025-02-01</created><authors><author><keyname>George</keyname><forenames>Anand Jerry</forenames></author><author><keyname>Veiga</keyname><forenames>Rodrigo</forenames></author><author><keyname>Macris</keyname><forenames>Nicolas</forenames></author></authors><title>Analysis of Diffusion Models for Manifold Data</title><categories>math.ST cond-mat.dis-nn cs.IT cs.LG math.IT math.PR stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We analyze the time reversed dynamics of generative diffusion models. If the exact empirical score function is used in a regime of large dimension and exponentially large number of samples, these models are known to undergo transitions between distinct dynamical regimes. We extend this analysis and compute the transitions for an analytically tractable manifold model where the statistical model for the data is a mixture of lower dimensional Gaussians embedded in higher dimensional space. We compute the so-called speciation and collapse transition times, as a function of the ratio of manifold-to-ambient space dimensions, and other characteristics of the data model. An important tool used in our analysis is the exact formula for the mutual information (or free energy) of Generalized Linear Models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04372</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04372</id><created>2025-02-05</created><authors><author><keyname>Genari</keyname><forenames>Juliano</forenames></author><author><keyname>Goedert</keyname><forenames>Guilherme Tegoni</forenames></author></authors><title>Mining Unstructured Medical Texts With Conformal Active Learning</title><categories>cs.CL cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The extraction of relevant data from Electronic Health Records (EHRs) is crucial to identifying symptoms and automating epidemiological surveillance processes. By harnessing the vast amount of unstructured text in EHRs, we can detect patterns that indicate the onset of disease outbreaks, enabling faster, more targeted public health responses. Our proposed framework provides a flexible and efficient solution for mining data from unstructured texts, significantly reducing the need for extensive manual labeling by specialists. Experiments show that our framework achieving strong performance with as few as 200 manually labeled texts, even for complex classification problems. Additionally, our approach can function with simple lightweight models, achieving competitive and occasionally even better results compared to more resource-intensive deep learning models. This capability not only accelerates processing times but also preserves patient privacy, as the data can be processed on weaker on-site hardware rather than being transferred to external systems. Our methodology, therefore, offers a practical, scalable, and privacy-conscious approach to real-time epidemiological monitoring, equipping health institutions to respond rapidly and effectively to emerging health threats. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04422</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04422</id><created>2025-02-06</created><authors><author><keyname>Yadav</keyname><forenames>Pooja</forenames></author><author><keyname>Srivastava</keyname><forenames>Tanuja</forenames></author></authors><title>The Maximum Likelihood Degree of Farlie Gumbel Morgenstern Bivariate   Exponential Distribution</title><categories>math.ST math.AC stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum likelihood degree of a statistical model refers to the number of solutions, where the derivative of the log-likelihood function is zero, over the complex field. This paper examines the maximum likelihood degree of the parameter in Farlie-Gumbel-Morgenstern bivariate exponential distribution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04464</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04464</id><created>2025-02-06</created><authors><author><keyname>Jadoul</keyname><forenames>Yannick</forenames></author><author><keyname>Tufarelli</keyname><forenames>Tommaso</forenames></author><author><keyname>Coissac</keyname><forenames>Chloé</forenames></author><author><keyname>Gamba</keyname><forenames>Marco</forenames></author><author><keyname>Ravignani</keyname><forenames>Andrea</forenames></author></authors><title>Hidden assumptions of integer ratio analyses in bioacoustics and music</title><categories>stat.AP</categories><comments>19 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Rhythm is ubiquitous in human culture and in nature, but hard to capture in all its complexity. A key dimension of rhythm, integer ratio categories occur when the relationship between temporal intervals can be expressed as small-integer ratios. Recent work has found integer ratio categories in most human musical cultures and some animal species' vocalizations or behavioral displays. But biological systems are noisy, and empirically measured intervals rarely form an exact small-integer ratio. Here, we mathematically assess whether the leading integer ratio analysis method makes valid statistical and biological assumptions. In particular, we (1) make the temporal properties of empirical ratios explicit, both in general and for the typical use in the literature; (2) show how the choice of ratio formula affects the probability distribution of rhythm ratios and ensuing statistical results; (3) guide the reader to carefully consider the assumptions and null hypotheses of the statistical analysis; (4) present a comprehensive methodology to statistically test integer ratios for any null hypothesis of choice. Our observations have implications for both past and future research in music cognition and animal behavior: They suggest how to interpret past findings and provide tools to choose the correct null hypotheses in future empirical work. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04491</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04491</id><created>2025-02-06</created><authors><author><keyname>Cheng</keyname><forenames>Ziheng</forenames></author><author><keyname>Xie</keyname><forenames>Tianyu</forenames></author><author><keyname>Zhang</keyname><forenames>Shiyue</forenames></author><author><keyname>Zhang</keyname><forenames>Cheng</forenames></author></authors><title>Provable Sample-Efficient Transfer Learning Conditional Diffusion Models   via Representation Learning</title><categories>cs.LG math.ST stat.ML stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While conditional diffusion models have achieved remarkable success in various applications, they require abundant data to train from scratch, which is often infeasible in practice. To address this issue, transfer learning has emerged as an essential paradigm in small data regimes. Despite its empirical success, the theoretical underpinnings of transfer learning conditional diffusion models remain unexplored. In this paper, we take the first step towards understanding the sample efficiency of transfer learning conditional diffusion models through the lens of representation learning. Inspired by practical training procedures, we assume that there exists a low-dimensional representation of conditions shared across all tasks. Our analysis shows that with a well-learned representation from source tasks, the samplecomplexity of target tasks can be reduced substantially. In addition, we investigate the practical implications of our theoretical results in several real-world applications of conditional diffusion models. Numerical experiments are also conducted to verify our results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04543</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04543</id><created>2025-02-06</created><authors><author><keyname>Lu</keyname><forenames>Zhou</forenames></author><author><keyname>Sun</keyname><forenames>Y. Jennifer</forenames></author><author><keyname>Zhang</keyname><forenames>Zhiyu</forenames></author></authors><title>Sparsity-Based Interpolation of External, Internal and Swap Regret</title><categories>stat.ML cs.LG</categories><comments>Equal contribution, alphabetical order</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Focusing on the expert problem in online learning, this paper studies the interpolation of several performance metrics via $\phi$-regret minimization, which measures the performance of an algorithm by its regret with respect to an arbitrary action modification rule $\phi$. With $d$ experts and $T\gg d$ rounds in total, we present a single algorithm achieving the instance-adaptive $\phi$-regret bound \begin{equation*} \tilde O\left(\min\left\{\sqrt{d-d^{\mathrm{unif}}_\phi+1},\sqrt{d-d^{\mathrm{self}}_\phi}\right\}\cdot\sqrt{T}\right), \end{equation*} where $d^{\mathrm{unif}}_\phi$ is the maximum amount of experts modified identically by $\phi$, and $d^{\mathrm{self}}_\phi$ is the amount of experts that $\phi$ trivially modifies to themselves. By recovering the optimal $O(\sqrt{T\log d})$ external regret bound when $d^{\mathrm{unif}}_\phi=d$, the standard $\tilde O(\sqrt{T})$ internal regret bound when $d^{\mathrm{self}}_\phi=d-1$ and the optimal $\tilde O(\sqrt{dT})$ swap regret bound in the worst case, we improve existing results in the intermediate regimes. In addition, the same algorithm achieves the optimal quantile regret bound, which corresponds to even easier settings of $\phi$ than the external regret.   Building on the classical reduction from $\phi$-regret minimization to external regret minimization on stochastic matrices, our main idea is to further convert the latter to online linear regression using Haar-wavelet-inspired matrix features. Then, we apply a particular $L_1$-version of comparator-adaptive online learning algorithms to exploit the sparsity in this regression subroutine. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04550</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04550</id><created>2025-02-06</created><authors><author><keyname>Faes</keyname><forenames>Luca</forenames></author><author><keyname>Sparacino</keyname><forenames>Laura</forenames></author><author><keyname>Mijatovic</keyname><forenames>Gorana</forenames></author><author><keyname>Antonacci</keyname><forenames>Yuri</forenames></author><author><keyname>Ricci</keyname><forenames>Leonardo</forenames></author><author><keyname>Marinazzo</keyname><forenames>Daniele</forenames></author><author><keyname>Stramaglia</keyname><forenames>Sebastiano</forenames></author></authors><title>Partial Information Rate Decomposition</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Partial Information Decomposition (PID) is a principled and flexible method to unveil complex high-order interactions in multi-unit network systems. Though being defined exclusively for random variables, PID is ubiquitously applied to multivariate time series taken as realizations of random processes with temporal statistical structure. Here, to overcome the incorrect depiction of high-order effects by PID schemes applied to dynamic networks, we introduce the framework of Partial Information Rate Decomposition (PIRD). PIRD is formalized applying lattice theory to decompose the information shared dynamically between a target random process and a set of source processes, implemented for Gaussian processes through a spectral expansion of information rates, and demonstrated in practice analyzing time series from large-scale climate oscillations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04553</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04553</id><created>2025-02-06</created><authors><author><keyname>Swanson</keyname><forenames>David</forenames></author><author><keyname>Sherry</keyname><forenames>Alexander</forenames></author><author><keyname>Tang</keyname><forenames>Chad</forenames></author></authors><title>Variance component mixture modelling for longitudinal T-cell receptor   clonal dynamics</title><categories>stat.ME</categories><msc-class>62F15 (Primary), 62H30 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Studies of T cells and their clonally unique receptors have shown promise in elucidating the association between immune response and human disease. Methods to identify T-cell receptor clones which expand or contract in response to certain therapeutic strategies have so far been limited to longitudinal pairwise comparisons of clone frequency with multiplicity adjustment. Here we develop a more general mixture model approach for arbitrary follow-up and missingness which partitions dynamic longitudinal clone frequency behavior from static. While it is common to mix on the location or scale parameter of a family of distributions, the model instead mixes on the parameterization itself, the dynamic component allowing for a variable, Gamma-distributed Poisson mean parameter over longitudinal follow-up, while the static component mean is time invariant. Leveraging conjugacy, one can integrate out the mean parameter for the dynamic and static components to yield distinct posterior predictive distributions whose expressions are a product of negative binomials and a single negative multinomial, respectively, each modified according to an offset for receptor read count normalization. An EM-algorithm is developed to estimate hyperparameters and component membership, and validity of the approach is demonstrated in simulation. The model identifies a statistically significant and clinically relevant increase in TCR clonal dynamism among metastasis-directed radiation therapy in a cohort of prostate cancer patients. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04555</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04555</id><created>2025-02-06</created><authors><author><keyname>Sparacino</keyname><forenames>Laura</forenames></author><author><keyname>Mijatovic</keyname><forenames>Gorana</forenames></author><author><keyname>Antonacci</keyname><forenames>Yuri</forenames></author><author><keyname>Ricci</keyname><forenames>Leonardo</forenames></author><author><keyname>Marinazzo</keyname><forenames>Daniele</forenames></author><author><keyname>Stramaglia</keyname><forenames>Sebastiano</forenames></author><author><keyname>Faes</keyname><forenames>Luca</forenames></author></authors><title>Decomposing Multivariate Information Rates in Networks of Random   Processes</title><categories>stat.ME cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The Partial Information Decomposition (PID) framework has emerged as a powerful tool for analyzing high-order interdependencies in complex network systems. However, its application to dynamic processes remains challenging due to the implicit assumption of memorylessness, which often falls in real-world scenarios. In this work, we introduce the framework of Partial Information Rate Decomposition (PIRD) that extends PID to random processes with temporal correlations. By leveraging mutual information rate (MIR) instead of mutual information (MI), our approach decomposes the dynamic information shared by multivariate random processes into unique, redundant, and synergistic contributions obtained aggregating information rate atoms in a principled manner. To solve PIRD, we define a pointwise redundancy rate function based on the minimum MI principle applied locally in the frequency-domain representation of the processes. The framework is validated in benchmark simulations of Gaussian systems, demonstrating its advantages over traditional PID in capturing temporal correlations and showing how the spectral representation may reveal scale-specific higher-order interaction that are obscured in the time domain. Furthermore, we apply PIRD to a physiological network comprising cerebrovascular and cardiovascular variables, revealing frequency-dependent redundant information exchange during a protocol of postural stress. Our results highlight the necessity of accounting for the full temporal statistical structure and spectral content of vector random processes to meaningfully perform information decomposition in network systems with dynamic behavior such as those typically encountered in neuroscience and physiology. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04574</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04574</id><created>2025-02-06</created><authors><author><keyname>Gong</keyname><forenames>ZhuQing</forenames></author><author><keyname>Zuo</keyname><forenames>XiNian</forenames></author></authors><title>Dark Brain Energy: Toward an Integrative Model of Spontaneous Slow   Oscillations</title><categories>q-bio.NC cs.IT math.IT stat.AP</categories><comments>38 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Neural oscillations facilitate the functioning of the human brain in spatial and temporal dimensions at various frequencies. These oscillations feature a universal frequency architecture that is governed by brain anatomy, ensuring frequency specificity remains invariant across different measurement techniques. Initial magnetic resonance imaging (MRI) methodology constrained functional MRI (fMRI) investigations to a singular frequency range, thereby neglecting the frequency characteristics inherent in blood oxygen level-dependent oscillations. With advancements in MRI technology, it has become feasible to decode intricate brain activities via multi-band frequency analysis (MBFA). During the past decade, the utilization of MBFA in fMRI studies has surged, unveiling frequency-dependent characteristics of spontaneous slow oscillations (SSOs) believed to base dark energy in the brain. There remains a dearth of conclusive insights and hypotheses pertaining to the properties and functionalities of SSOs in distinct bands. We surveyed the SSO MBFA studies during the past 15 years to delineate the attributes of SSOs and enlighten their correlated functions. We further proposed a model to elucidate the hierarchical organization of multi-band SSOs by integrating their function, aimed at bridging theoretical gaps and guiding future MBFA research endeavors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04575</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04575</id><created>2025-02-06</created><authors><author><keyname>Guo</keyname><forenames>Wei</forenames></author><author><keyname>Tao</keyname><forenames>Molei</forenames></author><author><keyname>Chen</keyname><forenames>Yongxin</forenames></author></authors><title>Complexity Analysis of Normalizing Constant Estimation: from Jarzynski   Equality to Annealed Importance Sampling and beyond</title><categories>stat.ML cs.LG cs.NA math.NA physics.comp-ph stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given an unnormalized probability density $\pi\propto\mathrm{e}^{-V}$, estimating its normalizing constant $Z=\int_{\mathbb{R}^d}\mathrm{e}^{-V(x)}\mathrm{d}x$ or free energy $F=-\log Z$ is a crucial problem in Bayesian statistics, statistical mechanics, and machine learning. It is challenging especially in high dimensions or when $\pi$ is multimodal. To mitigate the high variance of conventional importance sampling estimators, annealing-based methods such as Jarzynski equality and annealed importance sampling are commonly adopted, yet their quantitative complexity guarantees remain largely unexplored. We take a first step toward a non-asymptotic analysis of annealed importance sampling. In particular, we derive an oracle complexity of $\widetilde{O}\left(\frac{d\beta^2{\mathcal{A}}^2}{\varepsilon^4}\right)$ for estimating $Z$ within $\varepsilon$ relative error with high probability, where $\beta$ is the smoothness of $V$ and $\mathcal{A}$ denotes the action of a curve of probability measures interpolating $\pi$ and a tractable reference distribution. Our analysis, leveraging Girsanov theorem and optimal transport, does not explicitly require isoperimetric assumptions on the target distribution. Finally, to tackle the large action of the widely used geometric interpolation of probability distributions, we propose a new normalizing constant estimation algorithm based on reverse diffusion samplers and establish a framework for analyzing its complexity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04591</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04591</id><created>2025-02-06</created><authors><author><keyname>Deidda</keyname><forenames>Piero</forenames></author><author><keyname>Zhang</keyname><forenames>Kaicheng</forenames></author><author><keyname>Higham</keyname><forenames>Desmond</forenames></author><author><keyname>Tudisco</keyname><forenames>Francesco</forenames></author></authors><title>Rethinking Oversmoothing in Graph Neural Networks: A Rank-Based   Perspective</title><categories>cs.LG cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Oversmoothing is a fundamental challenge in graph neural networks (GNNs): as the number of layers increases, node embeddings become increasingly similar, and model performance drops sharply. Traditionally, oversmoothing has been quantified using metrics that measure the similarity of neighbouring node features, such as the Dirichlet energy. While these metrics are related to oversmoothing, we argue they have critical limitations and fail to reliably capture oversmoothing in realistic scenarios. For instance, they provide meaningful insights only for very deep networks and under somewhat strict conditions on the norm of network weights and feature representations. As an alternative, we propose measuring oversmoothing by examining the numerical or effective rank of the feature representations. We provide theoretical support for this approach, demonstrating that the numerical rank of feature representations converges to one for a broad family of nonlinear activation functions under the assumption of nonnegative trained weights. To the best of our knowledge, this is the first result that proves the occurrence of oversmoothing without assumptions on the boundedness of the weight matrices. Along with the theoretical findings, we provide extensive numerical evaluation across diverse graph architectures. Our results show that rank-based metrics consistently capture oversmoothing, whereas energy-based metrics often fail. Notably, we reveal that a significant drop in the rank aligns closely with performance degradation, even in scenarios where energy metrics remain unchanged. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04593</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04593</id><created>2025-02-06</created><authors><author><keyname>Rezaei</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Dieng</keyname><forenames>Adji Bousso</forenames></author></authors><title>The $\alpha$-Alternator: Dynamic Adaptation To Varying Noise Levels In   Sequences Using The Vendi Score For Improved Robustness and Performance</title><categories>cs.LG cs.AI cs.NE stat.ML</categories><comments>The codebase will be made available upon publication. This paper is   dedicated to Patrice Lumumba</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Current state-of-the-art dynamical models, such as Mamba, assume the same level of noisiness for all elements of a given sequence, which limits their performance on noisy temporal data. In this paper, we introduce the $\alpha$-Alternator, a novel generative model for time-dependent data that dynamically adapts to the complexity introduced by varying noise levels in sequences. The $\alpha$-Alternator leverages the Vendi Score (VS), a flexible similarity-based diversity metric, to adjust, at each time step $t$, the influence of the sequence element at time $t$ and the latent representation of the dynamics up to that time step on the predicted future dynamics. This influence is captured by a parameter that is learned and shared across all sequences in a given dataset. The sign of this parameter determines the direction of influence. A negative value indicates a noisy dataset, where a sequence element that increases the VS is considered noisy, and the model relies more on the latent history when processing that element. Conversely, when the parameter is positive, a sequence element that increases the VS is considered informative, and the $\alpha$-Alternator relies more on this new input than on the latent history when updating its predicted latent dynamics. The $\alpha$-Alternator is trained using a combination of observation masking and Alternator loss minimization. Masking simulates varying noise levels in sequences, enabling the model to be more robust to these fluctuations and improving its performance in trajectory prediction, imputation, and forecasting. Our experimental results demonstrate that the $\alpha$-Alternator outperforms both Alternators and state-of-the-art state-space models across neural decoding and time-series forecasting benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04654</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04654</id><created>2025-02-06</created><authors><author><keyname>Lim</keyname><forenames>Keunwoo</forenames></author><author><keyname>Ye</keyname><forenames>Ting</forenames></author><author><keyname>Han</keyname><forenames>Fang</forenames></author></authors><title>A sliced Wasserstein and diffusion approach to random coefficient models</title><categories>math.ST econ.EM stat.TH</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new minimum-distance estimator for linear random coefficient models. This estimator integrates the recently advanced sliced Wasserstein distance with the nearest neighbor methods, both of which enhance computational efficiency. We demonstrate that the proposed method is consistent in approximating the true distribution. Additionally, our formulation encourages a diffusion process-based algorithm, which holds independent interest and potential for broader applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04673</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04673</id><created>2025-02-07</created><authors><author><keyname>Neopane</keyname><forenames>Ojash</forenames></author><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author></authors><title>Optimistic Algorithms for Adaptive Estimation of the Average Treatment   Effect</title><categories>stat.ML cs.LG stat.ME</categories><comments>15 pages, 2 Figures</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Estimation and inference for the Average Treatment Effect (ATE) is a cornerstone of causal inference and often serves as the foundation for developing procedures for more complicated settings. Although traditionally analyzed in a batch setting, recent advances in martingale theory have paved the way for adaptive methods that can enhance the power of downstream inference. Despite these advances, progress in understanding and developing adaptive algorithms remains in its early stages. Existing work either focus on asymptotic analyses that overlook exploration-exploitation tradeoffs relevant in finite-sample regimes or rely on simpler but suboptimal estimators. In this work, we address these limitations by studying adaptive sampling procedures that take advantage of the asymptotically optimal Augmented Inverse Probability Weighting (AIPW) estimator. Our analysis uncovers challenges obscured by asymptotic approaches and introduces a novel algorithmic design principle reminiscent of optimism in multiarmed bandits. This principled approach enables our algorithm to achieve significant theoretical and empirical gains compared to prior methods. Our findings mark a step forward in advancing adaptive causal inference methods in theory and practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04681</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04681</id><created>2025-02-07</created><authors><author><keyname>Louit</keyname><forenames>Sydney</forenames></author><author><keyname>Clark</keyname><forenames>Evan</forenames></author><author><keyname>Gelbard</keyname><forenames>Alexander</forenames></author><author><keyname>Vivek</keyname><forenames>Niketna</forenames></author><author><keyname>Yan</keyname><forenames>Jun</forenames></author><author><keyname>Zhang</keyname><forenames>Panpan</forenames></author></authors><title>CALF-SBM: A Covariate-Assisted Latent Factor Stochastic Block Model</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel network generative model extended from the standard stochastic block model by concurrently utilizing observed node-level information and accounting for network-enabled nodal heterogeneity. The proposed model is so so-called covariate-assisted latent factor stochastic block model (CALF-SBM). The inference for the proposed model is done in a fully Bayesian framework. The primary application of CALF-SBM in the present research is focused on community detection, where a model-selection-based approach is employed to estimate the number of communities which is practically assumed unknown. To assess the performance of CALF-SBM, an extensive simulation study is carried out, including comparisons with multiple classical and modern network clustering algorithms. Lastly, the paper presents two real data applications, respectively based on an extremely new network data demonstrating collaborative relationships of otolaryngologists in the United States and a traditional aviation network data containing information about direct flights between airports in the United States and Canada. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04685</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04685</id><created>2025-02-07</created><authors><author><keyname>Zhang</keyname><forenames>Likun</forenames></author><author><keyname>Bhaganagar</keyname><forenames>Kiran</forenames></author><author><keyname>Wikle</keyname><forenames>Christopher K.</forenames></author></authors><title>Capturing Extreme Events in Turbulence using an Extreme Variational   Autoencoder (xVAE)</title><categories>physics.flu-dyn stat.AP stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Turbulent flow fields are characterized by extreme events that are statistically intermittent and carry a significant amount of energy and physical importance. To emulate these flows, we introduce the extreme variational Autoencoder (xVAE), which embeds a max-infinitely divisible process with heavy-tailed distributions into a standard VAE framework, enabling accurate modeling of extreme events. xVAEs are neural network models that reduce system dimensionality by learning non-linear latent representations of data. We demonstrate the effectiveness of xVAE in large-eddy simulation data of wildland fire plumes, where intense heat release and complex plume-atmosphere interactions generate extreme turbulence. Comparisons with the commonly used Proper Orthogonal Decomposition (POD) modes show that xVAE is more robust in capturing extreme values and provides a powerful uncertainty quantification framework using variational Bayes. Additionally, xVAE enables analysis of the so-called copulas of fields to assess risks associated with rare events while rigorously accounting for uncertainty, such as simultaneous exceedances of high thresholds across multiple locations. The proposed approach provides a new direction for studying realistic turbulent flows, such as high-speed aerodynamics, space propulsion, and atmospheric and oceanic systems that are characterized by extreme events. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04699</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04699</id><created>2025-02-07</created><authors><author><keyname>Lan</keyname><forenames>Hui</forenames></author><author><keyname>Chang</keyname><forenames>Haoge</forenames></author><author><keyname>Dillon</keyname><forenames>Eleanor</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author></authors><title>A Meta-learner for Heterogeneous Effects in Difference-in-Differences</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We address the problem of estimating heterogeneous treatment effects in panel data, adopting the popular Difference-in-Differences (DiD) framework under the conditional parallel trends assumption. We propose a novel doubly robust meta-learner for the Conditional Average Treatment Effect on the Treated (CATT), reducing the estimation to a convex risk minimization problem involving a set of auxiliary models. Our framework allows for the flexible estimation of the CATT, when conditioning on any subset of variables of interest using generic machine learning. Leveraging Neyman orthogonality, our proposed approach is robust to estimation errors in the auxiliary models. As a generalization to our main result, we develop a meta-learning approach for the estimation of general conditional functionals under covariate shift. We also provide an extension to the instrumented DiD setting with non-compliance. Empirical results demonstrate the superiority of our approach over existing baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04709</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04709</id><created>2025-02-07</created><authors><author><keyname>Miftachov</keyname><forenames>Ratmir</forenames></author><author><keyname>Reiß</keyname><forenames>Markus</forenames></author></authors><title>Early Stopping for Regression Trees</title><categories>math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We develop early stopping rules for growing regression tree estimators. The fully data-driven stopping rule is based on monitoring the global residual norm. The best-first search and the breadth-first search algorithms together with linear interpolation give rise to generalized projection or regularization flows. A general theory of early stopping is established. Oracle inequalities for the early-stopped regression tree are derived without any smoothness assumption on the regression function, assuming the original CART splitting rule, yet with a much broader scope. The remainder terms are of smaller order than the best achievable rates for Lipschitz functions in dimension $d\ge 2$. In real and synthetic data the early stopping regression tree estimators attain the statistical performance of cost-complexity pruning while significantly reducing computational costs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04723</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04723</id><created>2025-02-07</created><authors><author><keyname>Lyu</keyname><forenames>Ziyang</forenames></author><author><keyname>Sisson</keyname><forenames>S. A.</forenames></author><author><keyname>Welsh</keyname><forenames>A. H.</forenames></author></authors><title>Asymptotics for EBLUPs within crossed mixed effect models</title><categories>stat.ME</categories><comments>33 pages</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In this article, we derive the joint asymptotic distribution of empirical best linear unbiased predictors (EBLUPs) for individual and cell-level random effects in a crossed mixed effect model. Under mild conditions (which include moment conditions instead of normality for the random effects and model errors), we demonstrate that as the sizes of rows, columns, and, when we include interactions, cells simultaneously increase to infinity, the distribution of the differences between the EBLUPs and the random effects satisfy central limit theorems. These central limit theorems mean the EBLUPs asymptotically follow the convolution of the true random effect distribution and a normal distribution. Moreover, our results enable simple asymptotic approximations and estimators for the mean squared error (MSE) of the EBLUPs, which in turn facilitates the construction of asymptotic prediction intervals for the unobserved random effects. We show in simulations that our simple estimator of the MSE of the EBLUPs works very well in finite samples. Finally, we illustrate the use of the asymptotic prediction intervals with an analysis of movie rating data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04730</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04730</id><created>2025-02-07</created><authors><author><keyname>Xie</keyname><forenames>Tianyu</forenames></author><author><keyname>Richman</keyname><forenames>Harry</forenames></author><author><keyname>Gao</keyname><forenames>Jiansi</forenames></author><author><keyname>Matsen</keyname><forenames>Frederick A.</forenames><suffix>IV</suffix></author><author><keyname>Zhang</keyname><forenames>Cheng</forenames></author></authors><title>PhyloVAE: Unsupervised Learning of Phylogenetic Trees via Variational   Autoencoders</title><categories>stat.ML cs.LG q-bio.PE</categories><comments>ICLR 2025. 22 pages, 14 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Learning informative representations of phylogenetic tree structures is essential for analyzing evolutionary relationships. Classical distance-based methods have been widely used to project phylogenetic trees into Euclidean space, but they are often sensitive to the choice of distance metric and may lack sufficient resolution. In this paper, we introduce phylogenetic variational autoencoders (PhyloVAEs), an unsupervised learning framework designed for representation learning and generative modeling of tree topologies. Leveraging an efficient encoding mechanism inspired by autoregressive tree topology generation, we develop a deep latent-variable generative model that facilitates fast, parallelized topology generation. PhyloVAE combines this generative model with a collaborative inference model based on learnable topological features, allowing for high-resolution representations of phylogenetic tree samples. Extensive experiments demonstrate PhyloVAE's robust representation learning capabilities and fast generation of phylogenetic tree topologies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04750</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04750</id><created>2025-02-07</created><authors><author><keyname>Bui</keyname><forenames>Thang D.</forenames></author><author><keyname>Ashman</keyname><forenames>Matthew</forenames></author><author><keyname>Turner</keyname><forenames>Richard E.</forenames></author></authors><title>Tighter sparse variational Gaussian processes</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sparse variational Gaussian process (GP) approximations based on inducing points have become the de facto standard for scaling GPs to large datasets, owing to their theoretical elegance, computational efficiency, and ease of implementation. This paper introduces a provably tighter variational approximation by relaxing the standard assumption that the conditional approximate posterior given the inducing points must match that in the prior. The key innovation is to modify the conditional posterior to have smaller variances than that of the prior at the training points. We derive the collapsed bound for the regression case, describe how to use the proposed approximation in large data settings, and discuss its application to handle orthogonally structured inducing points and GP latent variable models. Extensive experiments on regression benchmarks, classification, and latent variable models demonstrate that the proposed approximation consistently matches or outperforms standard sparse variational GPs while maintaining the same computational cost. An implementation will be made available in all popular GP packages. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04793</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04793</id><created>2025-02-07</created><authors><author><keyname>Jeunen</keyname><forenames>Olivier</forenames></author></authors><title>$t$-Testing the Waters: Empirically Validating Assumptions for Reliable   A/B-Testing</title><categories>stat.ME cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A/B-tests are a cornerstone of experimental design on the web, with wide-ranging applications and use-cases. The statistical $t$-test comparing differences in means is the most commonly used method for assessing treatment effects, often justified through the Central Limit Theorem (CLT). The CLT ascertains that, as the sample size grows, the sampling distribution of the Average Treatment Effect converges to normality, making the $t$-test valid for sufficiently large sample sizes. When outcome measures are skewed or non-normal, quantifying what "sufficiently large" entails is not straightforward.   To ensure that confidence intervals maintain proper coverage and that $p$-values accurately reflect the false positive rate, it is critical to validate this normality assumption. We propose a practical method to test this, by analysing repeatedly resampled A/A-tests. When the normality assumption holds, the resulting $p$-value distribution should be uniform, and this property can be tested using the Kolmogorov-Smirnov test. This provides an efficient and effective way to empirically assess whether the $t$-test's assumptions are met, and the A/B-test is valid. We demonstrate our methodology and highlight how it helps to identify scenarios prone to inflated Type-I errors. Our approach provides a practical framework to ensure and improve the reliability and robustness of A/B-testing practices. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04807</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04807</id><created>2025-02-07</created><authors><author><keyname>Bashari</keyname><forenames>Meshi</forenames></author><author><keyname>Sesia</keyname><forenames>Matteo</forenames></author><author><keyname>Romano</keyname><forenames>Yaniv</forenames></author></authors><title>Robust Conformal Outlier Detection under Contaminated Reference Data</title><categories>stat.ML cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conformal prediction is a flexible framework for calibrating machine learning predictions, providing distribution-free statistical guarantees. In outlier detection, this calibration relies on a reference set of labeled inlier data to control the type-I error rate. However, obtaining a perfectly labeled inlier reference set is often unrealistic, and a more practical scenario involves access to a contaminated reference set containing a small fraction of outliers. This paper analyzes the impact of such contamination on the validity of conformal methods. We prove that under realistic, non-adversarial settings, calibration on contaminated data yields conservative type-I error control, shedding light on the inherent robustness of conformal methods. This conservativeness, however, typically results in a loss of power. To alleviate this limitation, we propose a novel, active data-cleaning framework that leverages a limited labeling budget and an outlier detection model to selectively annotate data points in the contaminated reference set that are suspected as outliers. By removing only the annotated outliers in this ``suspicious'' subset, we can effectively enhance power while mitigating the risk of inflating the type-I error rate, as supported by our theoretical analysis. Experiments on real datasets validate the conservative behavior of conformal methods under contamination and show that the proposed data-cleaning strategy improves power without sacrificing validity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04824</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04824</id><created>2025-02-07</created><authors><author><keyname>Blake</keyname><forenames>Joshua</forenames></author><author><keyname>Birrell</keyname><forenames>Paul</forenames></author><author><keyname>Walker</keyname><forenames>A. Sarah</forenames></author><author><keyname>Pouwels</keyname><forenames>Koen B.</forenames></author><author><keyname>House</keyname><forenames>Thomas</forenames></author><author><keyname>Tom</keyname><forenames>Brian D. M.</forenames></author><author><keyname>Kypraios</keyname><forenames>Theodore</forenames></author><author><keyname>De Angelis</keyname><forenames>Daniela</forenames></author></authors><title>Estimating the duration of RT-PCR positivity for SARS-CoV-2 from doubly   interval censored data with undetected infections</title><categories>stat.ME stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Monitoring the incidence of new infections during a pandemic is critical for an effective public health response. General population prevalence surveys for SARS-CoV-2 can provide high-quality data to estimate incidence. However, estimation relies on understanding the distribution of the duration that infections remain detectable. This study addresses this need using data from the Coronavirus Infection Survey (CIS), a long-term, longitudinal, general population survey conducted in the UK. Analyzing these data presents unique challenges, such as doubly interval censoring, undetected infections, and false negatives. We propose a Bayesian nonparametric survival analysis approach, estimating a discrete-time distribution of durations and integrating prior information derived from a complementary study. Our methodology is validated through a simulation study, including its resilience to model misspecification, and then applied to the CIS dataset. This results in the first estimate of the full duration distribution in a general population, as well as methodology that could be transferred to new contexts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04832</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04832</id><created>2025-02-07</created><authors><author><keyname>Ballarin</keyname><forenames>Giovanni</forenames></author><author><keyname>Grigoryeva</keyname><forenames>Lyudmila</forenames></author><author><keyname>Ortega</keyname><forenames>Juan-Pablo</forenames></author></authors><title>Memory Capacity of Nonlinear Recurrent Networks: Is it Informative?</title><categories>cs.LG stat.ML</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The total memory capacity (MC) of linear recurrent neural networks (RNNs) has been proven to be equal to the rank of the corresponding Kalman controllability matrix, and it is almost surely maximal for connectivity and input weight matrices drawn from regular distributions. This fact questions the usefulness of this metric in distinguishing the performance of linear RNNs in the processing of stochastic signals. This note shows that the MC of random nonlinear RNNs yields arbitrary values within established upper and lower bounds depending just on the input process scale. This confirms that the existing definition of MC in linear and nonlinear cases has no practical value. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04849</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04849</id><created>2025-02-07</created><authors><author><keyname>Yu</keyname><forenames>Yifeng</forenames></author><author><keyname>Yu</keyname><forenames>Lu</forenames></author></authors><title>Advancing Wasserstein Convergence Analysis of Score-Based Models:   Insights from Discretization and Second-Order Acceleration</title><categories>stat.ML cs.LG math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Score-based diffusion models have emerged as powerful tools in generative modeling, yet their theoretical foundations remain underexplored. In this work, we focus on the Wasserstein convergence analysis of score-based diffusion models. Specifically, we investigate the impact of various discretization schemes, including Euler discretization, exponential integrators, and midpoint randomization methods. Our analysis provides a quantitative comparison of these discrete approximations, emphasizing their influence on convergence behavior. Furthermore, we explore scenarios where Hessian information is available and propose an accelerated sampler based on the local linearization method. We demonstrate that this Hessian-based approach achieves faster convergence rates of order $\widetilde{\mathcal{O}}\left(\frac{1}{\varepsilon}\right)$ significantly improving upon the standard rate $\widetilde{\mathcal{O}}\left(\frac{1}{\varepsilon^2}\right)$ of vanilla diffusion models, where $\varepsilon$ denotes the target accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04867</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04867</id><created>2025-02-07</created><authors><author><keyname>Maclaren</keyname><forenames>Oliver J.</forenames></author><author><keyname>Nicholson</keyname><forenames>Ruanui</forenames></author><author><keyname>Trent</keyname><forenames>Joel A.</forenames></author><author><keyname>Rottenberry</keyname><forenames>Joshua</forenames></author><author><keyname>Simpson</keyname><forenames>Matthew</forenames></author></authors><title>Invariant Image Reparameterisation: A Unified Approach to Structural and   Practical Identifiability and Model Reduction</title><categories>stat.AP</categories><comments>25 pages + supplementary material</comments><msc-class>62F99 (Primary) 65L09, 93B30, 62F12, 34A55 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Both structural and practical parameter non-identifiability present fundamental challenges when using mathematical models to interpret data. This issue is particularly acute in complex, applied areas such as the life sciences or engineering, where determining appropriate model complexity is challenging. While several approaches exist for diagnosing and resolving parameter non-identifiability, including symbolic methods, profile likelihood analysis, and sloppiness analysis, these approaches have distinct limitations and are rarely combined. We present an integrated approach called Invariant Image Reparameterisation (IIR) that incorporates key elements of these methods in a new way. Our approach replaces symbolic computations with numerical calculations at a single reference estimate and an invariance condition that determines when this local calculation holds globally. Parameter combinations determined by this method are naturally ordered by degree of identifiability, and this supports model reduction by replacing a practically non-identified model with a structurally non-identified approximate model. This approximate model can be further parameterised in terms of identified parameters only. By treating parameter combinations determined by our approach as interest parameters within our established likelihood-based Profile-Wise Analysis (PWA) framework, we incorporate uncertainty quantification in terms of likelihood profiles and confidence sets. We provide a Julia library on GitHub (https://github.com/omaclaren/reparam) demonstrating our methodology across a range of mathematical models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04879</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04879</id><created>2025-02-07</created><authors><author><keyname>Gauthier</keyname><forenames>Etienne</forenames></author><author><keyname>Bach</keyname><forenames>Francis</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Statistical Collusion by Collectives on Learning Platforms</title><categories>stat.ML cs.LG</categories><comments>Code available at: https://github.com/GauthierE/statistical-collusion</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As platforms increasingly rely on learning algorithms, collectives may form and seek ways to influence these platforms to align with their own interests. This can be achieved by coordinated submission of altered data. To evaluate the potential impact of such behavior, it is essential to understand the computations that collectives must perform to impact platforms in this way. In particular, collectives need to make a priori assessments of the effect of the collective before taking action, as they may face potential risks when modifying their data. Moreover they need to develop implementable coordination algorithms based on quantities that can be inferred from observed data. We develop a framework that provides a theoretical and algorithmic treatment of these issues and present experimental results in a product evaluation domain. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04889</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04889</id><created>2025-02-07</created><authors><author><keyname>Bao</keyname><forenames>Han</forenames></author><author><keyname>Sakaue</keyname><forenames>Shinsaku</forenames></author><author><keyname>Takezawa</keyname><forenames>Yuki</forenames></author></authors><title>Any-stepsize Gradient Descent for Separable Data under Fenchel--Young   Losses</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The gradient descent (GD) has been one of the most common optimizer in machine learning. In particular, the loss landscape of a neural network is typically sharpened during the initial phase of training, making the training dynamics hover on the edge of stability. This is beyond our standard understanding of GD convergence in the stable regime where arbitrarily chosen stepsize is sufficiently smaller than the edge of stability. Recently, Wu et al. (COLT2024) have showed that GD converges with arbitrary stepsize under linearly separable logistic regression. Although their analysis hinges on the self-bounding property of the logistic loss, which seems to be a cornerstone to establish a modified descent lemma, our pilot study shows that other loss functions without the self-bounding property can make GD converge with arbitrary stepsize. To further understand what property of a loss function matters in GD, we aim to show arbitrary-stepsize GD convergence for a general loss function based on the framework of \emph{Fenchel--Young losses}. We essentially leverage the classical perceptron argument to derive the convergence rate for achieving $\epsilon$-optimal loss, which is possible for a majority of Fenchel--Young losses. Among typical loss functions, the Tsallis entropy achieves the GD convergence rate $T=\Omega(\epsilon^{-1/2})$, and the R{\'e}nyi entropy achieves the far better rate $T=\Omega(\epsilon^{-1/3})$. We argue that these better rate is possible because of \emph{separation margin} of loss functions, instead of the self-bounding property. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04891</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04891</id><created>2025-02-07</created><authors><author><keyname>Rubio-Madrigal</keyname><forenames>Celia</forenames></author><author><keyname>Jamadandi</keyname><forenames>Adarsh</forenames></author><author><keyname>Burkholz</keyname><forenames>Rebekka</forenames></author></authors><title>GNNs Getting ComFy: Community and Feature Similarity Guided Rewiring</title><categories>cs.LG cs.SI stat.ML</categories><comments>Accepted at ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Maximizing the spectral gap through graph rewiring has been proposed to enhance the performance of message-passing graph neural networks (GNNs) by addressing over-squashing. However, as we show, minimizing the spectral gap can also improve generalization. To explain this, we analyze how rewiring can benefit GNNs within the context of stochastic block models. Since spectral gap optimization primarily influences community strength, it improves performance when the community structure aligns with node labels. Building on this insight, we propose three distinct rewiring strategies that explicitly target community structure, node labels, and their alignment: (a) community structure-based rewiring (ComMa), a more computationally efficient alternative to spectral gap optimization that achieves similar goals; (b) feature similarity-based rewiring (FeaSt), which focuses on maximizing global homophily; and (c) a hybrid approach (ComFy), which enhances local feature similarity while preserving community structure to optimize label-community alignment. Extensive experiments confirm the effectiveness of these strategies and support our theoretical insights. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04892</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04892</id><created>2025-02-07</created><authors><author><keyname>Park</keyname><forenames>Joonhyeong</forenames></author><author><keyname>Park</keyname><forenames>Byoungwoo</forenames></author><author><keyname>Bang</keyname><forenames>Chang-Bae</forenames></author><author><keyname>Choi</keyname><forenames>Jungwon</forenames></author><author><keyname>Chung</keyname><forenames>Hyungjin</forenames></author><author><keyname>Kim</keyname><forenames>Byung-Hoon</forenames></author><author><keyname>Lee</keyname><forenames>Juho</forenames></author></authors><title>A Foundational Brain Dynamics Model via Stochastic Optimal Control</title><categories>cs.LG q-bio.NC stat.ML</categories><comments>The first two authors contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a foundational model for brain dynamics that utilizes stochastic optimal control (SOC) and amortized inference. Our method features a continuous-discrete state space model (SSM) that can robustly handle the intricate and noisy nature of fMRI signals. To address computational limitations, we implement an approximation strategy grounded in the SOC framework. Additionally, we present a simulation-free latent dynamics approach that employs locally linear approximations, facilitating efficient and scalable inference. For effective representation learning, we derive an Evidence Lower Bound (ELBO) from the SOC formulation, which integrates smoothly with recent advancements in self-supervised learning (SSL), thereby promoting robust and transferable representations. Pre-trained on extensive datasets such as the UKB, our model attains state-of-the-art results across a variety of downstream tasks, including demographic prediction, trait analysis, disease diagnosis, and prognosis. Moreover, evaluating on external datasets such as HCP-A, ABIDE, and ADHD200 further validates its superior abilities and resilience across different demographic and clinical distributions. Our foundational model provides a scalable and efficient approach for deciphering brain dynamics, opening up numerous applications in neuroscience. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04907</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04907</id><created>2025-02-07</created><authors><author><keyname>Gachon</keyname><forenames>Erell</forenames></author><author><keyname>Bigot</keyname><forenames>Jérémie</forenames></author><author><keyname>Cazelles</keyname><forenames>Elsa</forenames></author></authors><title>Scalable and consistent embedding of probability measures into Hilbert   spaces via measure quantization</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper is focused on statistical learning from data that come as probability measures. In this setting, popular approaches consist in embedding such data into a Hilbert space with either Linearized Optimal Transport or Kernel Mean Embedding. However, the cost of computing such embeddings prohibits their direct use in large-scale settings. We study two methods based on measure quantization for approximating input probability measures with discrete measures of small-support size. The first one is based on optimal quantization of each input measure, while the second one relies on mean-measure quantization. We study the consistency of such approximations, and its implication for scalable embeddings of probability measures into a Hilbert space at a low computational cost. We finally illustrate our findings with various numerical experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04938</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04938</id><created>2025-02-07</created><authors><author><keyname>Gardini</keyname><forenames>Aldo</forenames></author><author><keyname>Greco</keyname><forenames>Fedele</forenames></author><author><keyname>Trivisano</keyname><forenames>Carlo</forenames></author></authors><title>A note on auxiliary mixture sampling for Bayesian Poisson models</title><categories>stat.ME stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bayesian hierarchical Poisson models are an essential tool for analyzing count data. However, designing efficient algorithms to sample from the posterior distribution of the target parameters remains a challenging task for this class of models. Auxiliary mixture sampling algorithms have been proposed to address this issue. They involve two steps of data augmentations: the first leverages the theory of Poisson processes, and the second approximates the residual distribution of the resulting model through a mixture of Gaussian distributions. In this way, an approximated Gibbs sampler is obtained. In this paper, we focus on the accuracy of the approximation step, highlighting scenarios where the mixture fails to accurately represent the true underlying distribution, leading to a lack of convergence in the algorithm. We outline key features to monitor, in order to assess if the approximation performs as intended. Building on this, we propose a robust version of the auxiliary mixture sampling algorithm, which can detect approximation failures and incorporate a Metropolis-Hastings step when necessary. Finally, we evaluate the proposed algorithm together with the original mixture sampling algorithms on both simulated and real datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04945</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04945</id><created>2025-02-07</created><authors><author><keyname>Yanhao</keyname><affiliation>Max</affiliation></author><author><keyname>Wei</keyname></author><author><keyname>Jiang</keyname><forenames>Zhenling</forenames></author></authors><title>Estimating Parameters of Structural Models Using Neural Networks</title><categories>econ.EM stat.CO</categories><acm-class>G.3; J.4; I.2</acm-class><journal-ref>Marketing Science 44(1):102-128 (2024)</journal-ref><doi>10.1287/mksc.2022.0360</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an alternative use of machine learning. We train neural nets to provide the parameter estimate of a given (structural) econometric model, for example, discrete choice or consumer search. Training examples consist of datasets generated by the econometric model under a range of parameter values. The neural net takes the moments of a dataset as input and tries to recognize the parameter value underlying that dataset. Besides the point estimate, the neural net can also output statistical accuracy. This neural net estimator (NNE) tends to limited-information Bayesian posterior as the number of training datasets increases. We apply NNE to a consumer search model. It gives more accurate estimates at lighter computational costs than the prevailing approach. NNE is also robust to redundant moment inputs. In general, NNE offers the most benefits in applications where other estimation approaches require very heavy simulation costs. We provide code at: https://nnehome.github.io. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04949</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04949</id><created>2025-02-07</created><authors><author><keyname>Elsemüller</keyname><forenames>Lasse</forenames></author><author><keyname>Pratz</keyname><forenames>Valentin</forenames></author><author><keyname>von Krause</keyname><forenames>Mischa</forenames></author><author><keyname>Voss</keyname><forenames>Andreas</forenames></author><author><keyname>Bürkner</keyname><forenames>Paul-Christian</forenames></author><author><keyname>Radev</keyname><forenames>Stefan T.</forenames></author></authors><title>Does Unsupervised Domain Adaptation Improve the Robustness of Amortized   Bayesian Inference? A Systematic Evaluation</title><categories>stat.ML cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Neural networks are fragile when confronted with data that significantly deviates from their training distribution. This is true in particular for simulation-based inference methods, such as neural amortized Bayesian inference (ABI), where models trained on simulated data are deployed on noisy real-world observations. Recent robust approaches employ unsupervised domain adaptation (UDA) to match the embedding spaces of simulated and observed data. However, the lack of comprehensive evaluations across different domain mismatches raises concerns about the reliability in high-stakes applications. We address this gap by systematically testing UDA approaches across a wide range of misspecification scenarios in both a controlled and a high-dimensional benchmark. We demonstrate that aligning summary spaces between domains effectively mitigates the impact of unmodeled phenomena or noise. However, the same alignment mechanism can lead to failures under prior misspecifications - a critical finding with practical consequences. Our results underscore the need for careful consideration of misspecification types when using UDA techniques to increase the robustness of ABI in practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04970</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04970</id><created>2025-02-07</created><authors><author><keyname>Langbein</keyname><forenames>Sophie Hanna</forenames></author><author><keyname>Koenen</keyname><forenames>Niklas</forenames></author><author><keyname>Wright</keyname><forenames>Marvin N.</forenames></author></authors><title>Gradient-based Explanations for Deep Learning Survival Models</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deep learning survival models often outperform classical methods in time-to-event predictions, particularly in personalized medicine, but their "black box" nature hinders broader adoption. We propose a framework for gradient-based explanation methods tailored to survival neural networks, extending their use beyond regression and classification. We analyze the implications of their theoretical assumptions for time-dependent explanations in the survival setting and propose effective visualizations incorporating the temporal dimension. Experiments on synthetic data show that gradient-based methods capture the magnitude and direction of local and global feature effects, including time dependencies. We introduce GradSHAP(t), a gradient-based counterpart to SurvSHAP(t), which outperforms SurvSHAP(t) and SurvLIME in a computational speed vs. accuracy trade-off. Finally, we apply these methods to medical data with multi-modal inputs, revealing relevant tabular features and visual patterns, as well as their temporal dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04990</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04990</id><created>2025-02-07</created><authors><author><keyname>Pichler</keyname><forenames>Clemens</forenames></author><author><keyname>Jewson</keyname><forenames>Jack</forenames></author><author><keyname>Avalos-Pacheco</keyname><forenames>Alejandra</forenames></author></authors><title>Probabilistic Programming with Sufficient Statistics for faster Bayesian   Computation</title><categories>stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Probabilistic programming methods have revolutionised Bayesian inference, making it easier than ever for practitioners to perform Markov-chain-Monte-Carlo sampling from non-conjugate posterior distributions. Here we focus on Stan, arguably the most used probabilistic programming tool for Bayesian inference (Carpenter et al., 2017), and its interface with R via the brms (Burkner, 2017) and rstanarm (Goodrich et al., 2024) packages. Although easy to implement, these tools can become computationally prohibitive when applied to datasets with many observations or models with numerous parameters. While the use of sufficient statistics is well-established in theory, it has been surprisingly overlooked in state-of-the-art Stan software. We show that when the likelihood can be written in terms of sufficient statistics, considerable computational improvements can be made to current implementations. We demonstrate how this approach provides accurate inference at a fraction of the time than state-of-the-art implementations for Gaussian linear regression models with non-conjugate priors, hierarchical random effects models, and factor analysis models. Our results also show that moderate computational gains can be achieved even in models where the likelihood can only be partially written in terms of sufficient statistics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05021</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05021</id><created>2025-02-07</created><authors><author><keyname>van Heel</keyname><forenames>Simon Donker</forenames></author><author><keyname>Lange</keyname><forenames>Rutger-Jan</forenames></author><author><keyname>van Dijk</keyname><forenames>Dick</forenames></author><author><keyname>van Os</keyname><forenames>Bram</forenames></author></authors><title>Stability and performance guarantees for misspecified multivariate   score-driven filters</title><categories>stat.ME eess.SP stat.ML</categories><comments>71 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the problem of tracking latent time-varying parameter vectors under model misspecification. We analyze implicit and explicit score-driven (ISD and ESD) filters, which update a prediction of the parameters using the gradient of the logarithmic observation density (i.e., the score). In the ESD filter, the score is computed using the predicted parameter values, whereas in the ISD filter, the score is evaluated using the new, updated parameter values. For both filter types, we derive novel sufficient conditions for the exponential stability (i.e., invertibility) of the filtered parameter path and existence of a finite mean squared error (MSE) bound with respect to the pseudo-true parameter path. In addition, we present expressions for finite-sample and asymptotic MSE bounds. Our performance guarantees rely on mild moment conditions on the data-generating process, while our stability result is entirely agnostic about the true process. As a result, our primary conditions depend only on the characteristics of the filter; hence, they are verifiable in practice. Concavity of the postulated log density combined with simple parameter restrictions is sufficient (but not necessary) for ISD-filter stability, whereas ESD-filter stability additionally requires the score to be Lipschitz continuous. Extensive simulation studies validate our theoretical findings and demonstrate the enhanced stability and improved performance of ISD over ESD filters. An empirical application to U.S. Treasury-bill rates confirms the practical relevance of our contribution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05072</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05072</id><created>2025-02-07</created><authors><author><keyname>Barnett</keyname><forenames>Helen</forenames></author><author><keyname>Boix</keyname><forenames>Oliver</forenames></author><author><keyname>Kontos</keyname><forenames>Dimitris</forenames></author><author><keyname>Jaki</keyname><forenames>Thomas</forenames></author></authors><title>Joint TITE-CRM for Dual Agent Dose Finding Studies</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Dual agent dose-finding trials study the effect of a combination of more than one agent, where the objective is to find the Maximum Tolerated Dose Combination (MTC), the combination of doses of the two agents that is associated with a pre-specified risk of being unsafe. In a Phase I/II setting, the objective is to find a dose combination that is both safe and active, the Optimal Biological Dose (OBD), that optimizes a criterion based on both safety and activity. Since Oncology treatments are typically given over multiple cycles, both the safety and activity outcome can be considered as late-onset, potentially occurring in the later cycles of treatment. This work proposes two model-based designs for dual-agent dose finding studies with late-onset activity and late-onset toxicity outcomes, the Joint TITE-POCRM and the Joint TITE-BLRM. Their performance is compared alongside a model-assisted comparator in a comprehensive simulation study motivated by a real trial example, with an extension to consider alternative sized dosing grids. It is found that both model-based methods outperform the model-assisted design. Whilst on average the two model-based designs are comparable, this comparability is not consistent across scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05074</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05074</id><created>2025-02-07</created><authors><author><keyname>Atanasov</keyname><forenames>Alexander</forenames></author><author><keyname>Bordelon</keyname><forenames>Blake</forenames></author><author><keyname>Zavatone-Veth</keyname><forenames>Jacob A.</forenames></author><author><keyname>Paquette</keyname><forenames>Courtney</forenames></author><author><keyname>Pehlevan</keyname><forenames>Cengiz</forenames></author></authors><title>Two-Point Deterministic Equivalence for Stochastic Gradient Dynamics in   Linear Models</title><categories>cond-mat.dis-nn cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We derive a novel deterministic equivalence for the two-point function of a random matrix resolvent. Using this result, we give a unified derivation of the performance of a wide variety of high-dimensional linear models trained with stochastic gradient descent. This includes high-dimensional linear regression, kernel regression, and random feature models. Our results include previously known asymptotics as well as novel ones. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05075</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05075</id><created>2025-02-07</created><authors><author><keyname>Dong</keyname><forenames>Yijun</forenames></author><author><keyname>Li</keyname><forenames>Yicheng</forenames></author><author><keyname>Li</keyname><forenames>Yunai</forenames></author><author><keyname>Lee</keyname><forenames>Jason D.</forenames></author><author><keyname>Lei</keyname><forenames>Qi</forenames></author></authors><title>Discrepancies are Virtue: Weak-to-Strong Generalization through Lens of   Intrinsic Dimension</title><categories>cs.LG cs.NA math.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weak-to-strong (W2S) generalization is a type of finetuning (FT) where a strong (large) student model is trained on pseudo-labels generated by a weak teacher. Surprisingly, W2S FT often outperforms the weak teacher. We seek to understand this phenomenon through the observation that FT often occurs in intrinsically low-dimensional spaces. Leveraging the low intrinsic dimensionality of FT, we analyze W2S in the ridgeless regression setting from a variance reduction perspective. For a strong student - weak teacher pair with sufficiently expressive low-dimensional feature subspaces $\mathcal{V}_s, \mathcal{V}_w$, we provide an exact characterization of the variance that dominates the generalization error of W2S. This unveils a virtue of discrepancy between the strong and weak models in W2S: the variance of the weak teacher is inherited by the strong student in $\mathcal{V}_s \cap \mathcal{V}_w$, while reduced by a factor of $\dim(\mathcal{V}_s)/N$ in the subspace of discrepancy $\mathcal{V}_w \setminus \mathcal{V}_s$ with $N$ pseudo-labels for W2S. Further, our analysis casts light on the sample complexities and the scaling of performance gap recovery in W2S. The analysis is supported with experiments on both synthetic regression problems and real vision tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05094</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05094</id><created>2025-02-07</created><authors><author><keyname>Blanchet</keyname><forenames>Jose</forenames></author><author><keyname>Hamoudi</keyname><forenames>Yassine</forenames></author><author><keyname>Szegedy</keyname><forenames>Mario</forenames></author><author><keyname>Wang</keyname><forenames>Guanyang</forenames></author></authors><title>Non-linear Quantum Monte Carlo</title><categories>quant-ph cs.LG cs.NA math.NA stat.CO stat.ML</categories><comments>30 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The mean of a random variable can be understood as a $\textit{linear}$ functional on the space of probability distributions. Quantum computing is known to provide a quadratic speedup over classical Monte Carlo methods for mean estimation. In this paper, we investigate whether a similar quadratic speedup is achievable for estimating $\textit{non-linear}$ functionals of probability distributions. We propose a quantum-inside-quantum Monte Carlo algorithm that achieves such a speedup for a broad class of non-linear estimation problems, including nested conditional expectations and stochastic optimization. Our algorithm improves upon the direct application of the quantum multilevel Monte Carlo algorithm introduced by An et al.. The existing lower bound indicates that our algorithm is optimal up polylogarithmic factors. A key innovation of our approach is a new sequence of multilevel Monte Carlo approximations specifically designed for quantum computing, which is central to the algorithm's improved performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05102</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05102</id><created>2025-02-07</created><authors><author><keyname>Piancastelli</keyname><forenames>Luiza</forenames></author><author><keyname>Barreto-Souza</keyname><forenames>Wagner</forenames></author></authors><title>Time Series Analysis of Rankings: A GARCH-Type Approach</title><categories>stat.ME stat.CO stat.ML</categories><comments>Paper submitted for publication</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Ranking data are frequently obtained nowadays but there are still scarce methods for treating these data when temporally observed. The present paper contributes to this topic by proposing and developing novel models for handling time series of ranking data. We introduce a class of time-varying ranking models inspired by the Generalized AutoRegressive Conditional Heteroskedasticity (GARCH) models. More specifically, the temporal dynamics are defined by the conditional distribution of the current ranking given the past rankings, which are assumed to follow a Mallows distribution, which implicitly depends on a distance. Then, autoregressive and feedback components are incorporated into the model through the conditional expectation of the associated distances. Theoretical properties of our ranking GARCH models such as stationarity and ergodicity are established. The estimation of parameters is performed via maximum likelihood estimation when data is fully observed. We develop a Monte Carlo Expectation-Maximisation algorithm to deal with cases involving missing data. Monte Carlo simulation studies are presented to study the performance of the proposed estimators under both non-missing and missing data scenarios. A real data application about the weekly ranking of professional tennis players from 2015 to 2019 is presented under our proposed ranking GARCH models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05122</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05122</id><created>2025-02-07</created><authors><author><keyname>Xi</keyname><forenames>Johnny</forenames></author><author><keyname>Dance</keyname><forenames>Hugh</forenames></author><author><keyname>Orbanz</keyname><forenames>Peter</forenames></author><author><keyname>Bloem-Reddy</keyname><forenames>Benjamin</forenames></author></authors><title>Distinguishing Cause from Effect with Causal Velocity Models</title><categories>stat.ML cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bivariate structural causal models (SCM) are often used to infer causal direction by examining their goodness-of-fit under restricted model classes. In this paper, we describe a parametrization of bivariate SCMs in terms of a causal velocity by viewing the cause variable as time in a dynamical system. The velocity implicitly defines counterfactual curves via the solution of initial value problems where the observation specifies the initial condition. Using tools from measure transport, we obtain a unique correspondence between SCMs and the score function of the generated distribution via its causal velocity. Based on this, we derive an objective function that directly regresses the velocity against the score function, the latter of which can be estimated non-parametrically from observational data. We use this to develop a method for bivariate causal discovery that extends beyond known model classes such as additive or location scale noise, and that requires no assumptions on the noise distributions. When the score is estimated well, the objective is also useful for detecting model non-identifiability and misspecification. We present positive results in simulation and benchmark experiments where many existing methods fail, and perform ablation studies to examine the method's sensitivity to accurate score estimation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05127</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05127</id><created>2025-02-07</created><authors><author><keyname>Everink</keyname><forenames>Jasper M.</forenames></author><author><keyname>Amougou</keyname><forenames>Bernardin Tamo</forenames></author><author><keyname>Pereyra</keyname><forenames>Marcelo</forenames></author></authors><title>Self-supervised Conformal Prediction for Uncertainty Quantification in   Imaging Problems</title><categories>cs.CV stat.ME</categories><msc-class>62H35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most image restoration problems are ill-conditioned or ill-posed and hence involve significant uncertainty. Quantifying this uncertainty is crucial for reliably interpreting experimental results, particularly when reconstructed images inform critical decisions and science. However, most existing image restoration methods either fail to quantify uncertainty or provide estimates that are highly inaccurate. Conformal prediction has recently emerged as a flexible framework to equip any estimator with uncertainty quantification capabilities that, by construction, have nearly exact marginal coverage. To achieve this, conformal prediction relies on abundant ground truth data for calibration. However, in image restoration problems, reliable ground truth data is often expensive or not possible to acquire. Also, reliance on ground truth data can introduce large biases in situations of distribution shift between calibration and deployment. This paper seeks to develop a more robust approach to conformal prediction for image restoration problems by proposing a self-supervised conformal prediction method that leverages Stein's Unbiased Risk Estimator (SURE) to self-calibrate itself directly from the observed noisy measurements, bypassing the need for ground truth. The method is suitable for any linear imaging inverse problem that is ill-conditioned, and it is especially powerful when used with modern self-supervised image restoration techniques that can also be trained directly from measurement data. The proposed approach is demonstrated through numerical experiments on image denoising and deblurring, where it delivers results that are remarkably accurate and comparable to those obtained by supervised conformal prediction with ground truth data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05134</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05134</id><created>2025-02-07</created><authors><author><keyname>Kızıldağ</keyname><forenames>Eren C.</forenames></author></authors><title>Information-Theoretic Guarantees for Recovering Low-Rank Tensors from   Symmetric Rank-One Measurements</title><categories>math.ST cs.IT math.IT math.PR stat.ML stat.TH</categories><comments>Algorithmic Learning Theory (ALT), 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the sample complexity of recovering tensors with low symmetric rank from symmetric rank-one measurements. This setting is particularly motivated by the study of higher-order interactions and the analysis of two-layer neural networks with polynomial activations (polynomial networks). Using a covering numbers argument, we analyze the performance of the symmetric rank minimization program and establish near-optimal sample complexity bounds when the underlying distribution is log-concave. Our measurement model involves random symmetric rank-one tensors, which lead to involved probability calculations. To address these challenges, we employ the Carbery-Wright inequality, a powerful tool for studying anti-concentration properties of random polynomials, and leverage orthogonal polynomials. Additionally, we provide a sample complexity lower bound based on Fano's inequality, and discuss broader implications of our results for two-layer polynomial networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05155</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05155</id><created>2025-02-07</created><authors><author><keyname>Tang</keyname><forenames>Shiqin</forenames></author><author><keyname>Yu</keyname><forenames>Shujian</forenames></author><author><keyname>Dong</keyname><forenames>Yining</forenames></author><author><keyname>Qin</keyname><forenames>S. Joe</forenames></author></authors><title>Deep Dynamic Probabilistic Canonical Correlation Analysis</title><categories>cs.LG stat.ML</categories><comments>accepted by ICASSP-25, code is available at   \url{https://github.com/marcusstang/D2PCCA}</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents Deep Dynamic Probabilistic Canonical Correlation Analysis (D2PCCA), a model that integrates deep learning with probabilistic modeling to analyze nonlinear dynamical systems. Building on the probabilistic extensions of Canonical Correlation Analysis (CCA), D2PCCA captures nonlinear latent dynamics and supports enhancements such as KL annealing for improved convergence and normalizing flows for a more flexible posterior approximation. D2PCCA naturally extends to multiple observed variables, making it a versatile tool for encoding prior knowledge about sequential datasets and providing a probabilistic understanding of the system's dynamics. Experimental validation on real financial datasets demonstrates the effectiveness of D2PCCA and its extensions in capturing latent dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05161</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05161</id><created>2025-02-07</created><authors><author><keyname>Antonczak</keyname><forenames>Brittany</forenames></author><author><keyname>Fay</keyname><forenames>Meg</forenames></author><author><keyname>Chawla</keyname><forenames>Aviral</forenames></author><author><keyname>Rowangould</keyname><forenames>Gregory</forenames></author></authors><title>Estimated Roadway Segment Traffic Data by Vehicle Class for the United   States: A Machine Learning Approach</title><categories>stat.AP</categories><comments>14 pages including references, 4 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Highway Performance Monitoring System, managed by the Federal Highway Administration, provides essential data on average annual daily traffic across U.S. roadways, but it has limited representation of medium- and heavy-duty vehicles on non-interstate roads. This gap limits research and policy analysis on the impacts of truck traffic, especially concerning air quality and public health. To address this, we use random forest regression to estimate medium- and heavy-duty vehicle traffic volumes in areas with sparse data. This results in a more comprehensive dataset, which enables the estimation of traffic density at the census block level as a proxy for traffic-related air pollution exposure. Our high-resolution spatial data products, rigorously validated, provide a more accurate representation of truck traffic and its environmental and health impacts. These datasets are valuable for transportation planning, public health research, and policy decisions aimed at mitigating the effects of truck traffic on vulnerable communities exposed to air pollution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:1811.12852</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>1811.12852</id><created>2018-11-30</created><updated>2025-02-06</updated><authors><author><keyname>Burnetas</keyname><forenames>Apostolos N.</forenames></author><author><keyname>Kanavetas</keyname><forenames>Odysseas</forenames></author><author><keyname>Katehakis</keyname><forenames>Michael N.</forenames></author></authors><title>Optimal Data Driven Resource Allocation under Multi-Armed Bandit   Observations</title><categories>stat.ML cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1509.02857</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the first asymptotically optimal strategy for a multi armed bandit (MAB) model under side constraints. The side constraints model situations in which bandit activations are limited by the availability of certain resources that are replenished at a constant rate. The main result involves the derivation of an asymptotic lower bound for the regret of feasible uniformly fast policies and the construction of policies that achieve this lower bound, under pertinent conditions. Further, we provide the explicit form of such policies for the case in which the unknown distributions are Normal with unknown means and known variances, for the case of Normal distributions with unknown means and unknown variances and for the case of arbitrary discrete distributions with finite support. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2202.04912</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2202.04912</id><created>2022-02-10</created><updated>2025-02-06</updated><authors><author><keyname>Qiu</keyname><forenames>Rui</forenames></author><author><keyname>Yu</keyname><forenames>Zhou</forenames></author><author><keyname>Zhu</keyname><forenames>Ruoqing</forenames></author></authors><title>Random Forest Weighted Local Fr\'echet Regression with Random Objects</title><categories>stat.ML cs.LG</categories><comments>This paper has been published in the Journal of Machine Learning   Research</comments><journal-ref>Journal of Machine Learning Research 25 (2024) 1-69</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical analysis is increasingly confronted with complex data from metric spaces. Petersen and M\"uller (2019) established a general paradigm of Fr\'echet regression with complex metric space valued responses and Euclidean predictors. However, the local approach therein involves nonparametric kernel smoothing and suffers from the curse of dimensionality. To address this issue, we in this paper propose a novel random forest weighted local Fr\'echet regression paradigm. The main mechanism of our approach relies on a locally adaptive kernel generated by random forests. Our first method uses these weights as the local average to solve the conditional Fr\'echet mean, while the second method performs local linear Fr\'echet regression, both significantly improving existing Fr\'echet regression methods. Based on the theory of infinite order U-processes and infinite order $M_{m_n}$-estimator, we establish the consistency, rate of convergence, and asymptotic normality for our local constant estimator, which covers the current large sample theory of random forests with Euclidean responses as a special case. Numerical studies show the superiority of our methods with several commonly encountered types of responses such as distribution functions, symmetric positive-definite matrices, and sphere data. The practical merits of our proposals are also demonstrated through the application to New York taxi data and human mortality data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2206.04902</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2206.04902</id><created>2022-06-10</created><updated>2024-11-13</updated><authors><author><keyname>Gruber</keyname><forenames>Luis</forenames></author><author><keyname>Kastner</keyname><forenames>Gregor</forenames></author></authors><title>Forecasting macroeconomic data with Bayesian VARs: Sparse or dense? It   depends!</title><categories>econ.EM stat.AP stat.ME</categories><journal-ref>International Journal of Forecasting (2025)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vector autogressions (VARs) are widely applied when it comes to modeling and forecasting macroeconomic variables. In high dimensions, however, they are prone to overfitting. Bayesian methods, more concretely shrinkage priors, have shown to be successful in improving prediction performance. In the present paper, we introduce the semi-global framework, in which we replace the traditional global shrinkage parameter with group-specific shrinkage parameters. We show how this framework can be applied to various shrinkage priors, such as global-local priors and stochastic search variable selection priors. We demonstrate the virtues of the proposed framework in an extensive simulation study and in an empirical application forecasting data of the US economy. Further, we shed more light on the ongoing ``Illusion of Sparsity'' debate, finding that forecasting performances under sparse/dense priors vary across evaluated economic variables and across time frames. Dynamic model averaging, however, can combine the merits of both worlds. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2211.15353</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2211.15353</id><created>2022-11-25</created><updated>2025-02-07</updated><authors><author><keyname>Letizia</keyname><forenames>Nunzio A.</forenames></author><author><keyname>Tonello</keyname><forenames>Andrea M.</forenames></author></authors><title>Copula Density Neural Estimation</title><categories>cs.LG eess.SP stat.ML</categories><comments>6 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Probability density estimation from observed data constitutes a central task in statistics. Recent advancements in machine learning offer new tools but also pose new challenges. The big data era demands analysis of long-range spatial and long-term temporal dependencies in large collections of raw data, rendering neural networks an attractive solution for density estimation. In this paper, we exploit the concept of copula to explicitly build an estimate of the probability density function associated to any observed data. In particular, we separate univariate marginal distributions from the joint dependence structure in the data, the copula itself, and we model the latter with a neural network-based method referred to as copula density neural estimation (CODINE). Results show that the novel learning approach is capable of modeling complex distributions and it can be applied for mutual information estimation and data generation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2212.09544</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2212.09544</id><created>2022-12-19</created><authors><author><keyname>Gardini</keyname><forenames>Aldo</forenames></author><author><keyname>Greco</keyname><forenames>Fedele</forenames></author><author><keyname>Trivisano</keyname><forenames>Carlo</forenames></author></authors><title>Design and Structure Dependent Priors for Scale Parameters in Latent   Gaussian Models</title><categories>stat.ME math.ST stat.TH</categories><doi>10.1214/24-BA1454</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Many common correlation structures assumed for data can be described through latent Gaussian models. When Bayesian inference is carried out, it is required to set the prior distribution for scale parameters that rules the model components, possibly allowing to incorporate prior information. This task is particularly delicate and many contributions in the literature are devoted to investigating such aspects. We focus on the fact that the scale parameter controls the prior variability of the model component in a complex way since its dispersion is also affected by the correlation structure and the design. To overcome this issue that might confound the prior elicitation step, we propose to let the user specify the marginal prior of a measure of dispersion of the model component, integrating out the scale parameter, the structure and the design. Then, we analytically derive the implied prior for the scale parameter. Results from a simulation study, aimed at showing the behavior of the estimators sampling properties under the proposed prior elicitation strategy, are discussed. Lastly, some real data applications are explored to investigate prior sensitivity and allocation of explained variance among model components. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2304.13077</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2304.13077</id><created>2023-04-25</created><updated>2025-02-07</updated><authors><author><keyname>De Vito</keyname><forenames>Roberta</forenames></author><author><keyname>Avalos-Pacheco</keyname><forenames>Alejandra</forenames></author></authors><title>Multi-study factor regression model: an application in nutritional   epidemiology</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diet is a risk factor for many diseases. In nutritional epidemiology, studying reproducible dietary patterns is critical to reveal important associations with health. However, it is challenging: diverse cultural and ethnic backgrounds may critically impact eating patterns, showing heterogeneity, leading to incorrect dietary patterns and obscuring the components shared across different groups or populations. Moreover, covariate effects generated from observed variables, such as demographics and other confounders, can further bias these dietary patterns. Identifying the shared and group-specific dietary components and covariate effects is essential to drive accurate conclusions. To address these issues, we introduce a new modeling factor regression, the Multi-Study Factor Regression (MSFR) model. The MSFR model analyzes different populations simultaneously, achieving three goals: capturing shared component(s) across populations, identifying group-specific structures, and correcting for covariate effects. We use this novel method to derive common and ethnic-specific dietary patterns in a multi-center epidemiological study in Hispanic/Latinos community. Our model improves the accuracy of common and group dietary signals and yields better prediction than other techniques, revealing significant associations with health. In summary, we provide a tool to integrate different groups, giving accurate dietary signals crucial to inform public health policy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2305.13998</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2305.13998</id><created>2023-05-23</created><updated>2024-01-23</updated><authors><author><keyname>Saves</keyname><forenames>Paul</forenames></author><author><keyname>Lafage</keyname><forenames>Remi</forenames></author><author><keyname>Bartoli</keyname><forenames>Nathalie</forenames></author><author><keyname>Diouane</keyname><forenames>Youssef</forenames></author><author><keyname>Bussemaker</keyname><forenames>Jasper</forenames></author><author><keyname>Lefebvre</keyname><forenames>Thierry</forenames></author><author><keyname>Hwang</keyname><forenames>John T.</forenames></author><author><keyname>Morlier</keyname><forenames>Joseph</forenames></author><author><keyname>Martins</keyname><forenames>Joaquim R. R. A.</forenames></author></authors><title>SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and   Mixed Variables Gaussian Processes</title><categories>cs.LG cs.MS math.OC stat.CO</categories><comments>10.1016/j.advengsoft.2023.103571</comments><journal-ref>Advances in Engineering Software Volume 188, February 2024, 103571</journal-ref><doi>10.1016/j.advengsoft.2023.103571</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multifidelity data. To the best of our knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical and mixed inputs. This open-source software is distributed under the New BSD license. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2306.16033</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2306.16033</id><created>2023-06-28</created><authors><author><keyname>Gardini</keyname><forenames>Aldo</forenames></author></authors><title>Functional and variables selection in extreme value models for regional   flood frequency analysis</title><categories>stat.ME stat.AP</categories><doi>10.1007/s10651-023-00581-8</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The problem of estimating return levels of river discharge, relevant in flood frequency analysis, is tackled by relying on the extreme value theory. The Generalized Extreme Value (GEV) distribution is assumed to model annual maxima values of river discharge registered at multiple gauging stations belonging to the same river basin. The specific features of the data from the Upper Danube basin drive the definition of the proposed statistical model. Firstly, Bayesian P-splines are considered to account for the non-linear effects of station-specific covariates on the GEV parameters. Secondly, the problem of functional and variable selection is addressed by imposing a grouped horseshoe prior on the coefficients, to encourage the shrinkage of non-relevant components to zero. A cross-validation study is organized to compare the proposed modeling solution to other models, showing its potential in reducing the uncertainty of the ungauged predictions without affecting their calibration. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2307.10272</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2307.10272</id><created>2023-07-18</created><updated>2025-02-06</updated><authors><author><keyname>Takeishi</keyname><forenames>Shota</forenames></author></authors><title>A Shrinkage Likelihood Ratio Test for High-Dimensional Subgroup Analysis   with a Logistic-Normal Mixture Model</title><categories>math.ST stat.ME stat.TH</categories><comments>38 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In subgroup analysis, testing the existence of a subgroup with a differential treatment effect serves as protection against spurious subgroup discovery. Despite its importance, this hypothesis testing possesses a complicated nature: parameter characterizing subgroup classification is not identified under the null hypothesis of no subgroup. Due to this irregularity, the existing methods have the following two limitations. First, the asymptotic null distribution of test statistics often takes an intractable form, which necessitates computationally demanding resampling methods to calculate the critical value. Second, the dimension of personal attributes characterizing subgroup membership is not allowed to be of high dimension. To solve these two problems simultaneously, this study develops a shrinkage likelihood ratio test for the existence of a subgroup using a logistic-normal mixture model. The proposed test statistics are built on a modified likelihood function that shrinks possibly high-dimensional unidentified parameters toward zero under the null hypothesis while retaining power under the alternative. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2307.12982</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2307.12982</id><created>2023-07-24</created><updated>2025-02-07</updated><authors><author><keyname>Mukherjee</keyname><forenames>Soumendu Sundar</forenames></author></authors><title>Consistent model selection in the spiked Wigner model via AIC-type   criteria</title><categories>math.ST cs.IT math.IT stat.ME stat.ML stat.TH</categories><comments>25 pages, 2 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the spiked Wigner model \[   X = \sum_{i = 1}^k \lambda_i u_i u_i^\top + \sigma G, \] where $G$ is an $N \times N$ GOE random matrix, and the eigenvalues $\lambda_i$ are all spiked, i.e. above the Baik-Ben Arous-P\'ech\'e (BBP) threshold $\sigma$. We consider AIC-type model selection criteria of the form \[   -2 \, (\text{maximised log-likelihood}) + \gamma \, (\text{number of parameters}) \] for estimating the number $k$ of spikes. For $\gamma &gt; 2$, the above criterion is strongly consistent provided $\lambda_k &gt; \lambda_{\gamma}$, where $\lambda_{\gamma}$ is a threshold strictly above the BBP threshold, whereas for $\gamma &lt; 2$, it almost surely overestimates $k$. Although AIC (which corresponds to $\gamma = 2$) is not strongly consistent, we show that taking $\gamma = 2 + \delta_N$, where $\delta_N \to 0$ and $\delta_N \gg N^{-2/3}$, results in a weakly consistent estimator of $k$. We further show that a soft minimiser of AIC, where one chooses the least complex model whose AIC score is close to the minimum AIC score, is strongly consistent. Based on a spiked (generalised) Wigner representation, we also develop similar model selection criteria for consistently estimating the number of communities in a balanced stochastic block model under some sparsity restrictions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.07399</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.07399</id><created>2023-10-11</created><updated>2024-10-03</updated><authors><author><keyname>Bou-Rabee</keyname><forenames>Nawaf</forenames></author><author><keyname>Kleppe</keyname><forenames>Tore Selland</forenames></author></authors><title>Randomized Runge-Kutta-Nystr\"om Methods for Unadjusted Hamiltonian and   Kinetic Langevin Monte Carlo</title><categories>math.NA cs.NA math.PR stat.CO stat.ME stat.ML</categories><msc-class>60J05 (Primary) 65C05, 65L05, 65P10 (Secondary)</msc-class><journal-ref>Mathematics of Computation 2025</journal-ref><doi>10.1090/mcom/4061</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce $5/2$- and $7/2$-order $L^2$-accurate randomized Runge-Kutta-Nystr\"{o}m methods, tailored for approximating Hamiltonian flows within non-reversible Markov chain Monte Carlo samplers, such as unadjusted Hamiltonian Monte Carlo and unadjusted kinetic Langevin Monte Carlo. We establish quantitative $5/2$-order $L^2$-accuracy upper bounds under gradient and Hessian Lipschitz assumptions on the potential energy function. The numerical experiments demonstrate the superior efficiency of the proposed unadjusted samplers on a variety of well-behaved, high-dimensional target distributions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2311.02655</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2311.02655</id><created>2023-11-05</created><updated>2025-02-06</updated><authors><author><keyname>Horst</keyname><forenames>Ulrich</forenames></author><author><keyname>Xu</keyname><forenames>Wei</forenames></author></authors><title>Second-Order Regular Variation and Second-Order Approximation of Hawkes   Processes</title><categories>math.PR math.FA math.ST stat.TH</categories><comments>40 pages</comments><msc-class>Primary 26A12, 40E05, secondary 60G55, 60K05</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper provides and extends second-order versions of several fundamental theorems on first-order regularly varying functions such as Karamata's theorem/representation and Tauberian's theorem. Our results are used to establish second-order approximations for the mean and variance of Hawkes processes with general kernels. Our approximations provide novel insights into the asymptotic behavior of Hawkes processes. They are also of key importance when establishing functional limit theorems for Hawkes processes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.04711</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.04711</id><created>2024-02-07</created><updated>2024-05-26</updated><authors><author><keyname>Saves</keyname><forenames>Paul</forenames></author></authors><title>High-dimensional multidisciplinary design optimization for aircraft   eco-design / Optimisation multi-disciplinaire en grande dimension pour   l'\'eco-conception avion en avant-projet</title><categories>math.OC cs.MS stat.ML</categories><comments>PhD Thesis, Universit\'e de Toulouse, Toulouse, 2024 on Gaussian   Process kernels for Bayesian optimization in high dimension with mixed and   hierarchical variables at ISAE-SUPAERO. Keywords: Gaussian process, Black-box   optimization, Bayesian inference, Multidisciplinary design optimization,   Mixed hierarchical and categorical inputs, Eco-friendly aircraft design</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The objective of this Philosophiae Doctor (Ph.D) thesis is to propose an efficient approach for optimizing a multidisciplinary black-box model when the optimization problem is constrained and involves a large number of mixed integer design variables (typically 100 variables). The targeted optimization approach, called EGO, is based on a sequential enrichment of an adaptive surrogate model and, in this context, GP surrogate models are one of the most widely used in engineering problems to approximate time-consuming high fidelity models. EGO is a heuristic BO method that performs well in terms of solution quality. However, like any other global optimization method, EGO suffers from the curse of dimensionality, meaning that its performance is satisfactory on lower dimensional problems, but deteriorates as the dimensionality of the optimization search space increases. For realistic aircraft design problems, the typical size of the design variables can even exceed 100 and, thus, trying to solve directly the problems using EGO is ruled out. The latter is especially true when the problems involve both continuous and categorical variables increasing even more the size of the search space. In this Ph.D thesis, effective parameterization tools are investigated, including techniques like partial least squares regression, to significantly reduce the number of design variables. Additionally, Bayesian optimization is adapted to handle discrete variables and high-dimensional spaces in order to reduce the number of evaluations when optimizing innovative aircraft concepts such as the "DRAGON" hybrid airplane to reduce their climate impact. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.13196</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.13196</id><created>2024-03-19</created><updated>2025-02-07</updated><authors><author><keyname>Eskandar</keyname><forenames>Masih</forenames></author><author><keyname>Imtiaz</keyname><forenames>Tooba</forenames></author><author><keyname>Wang</keyname><forenames>Zifeng</forenames></author><author><keyname>Dy</keyname><forenames>Jennifer</forenames></author></authors><title>ADAPT to Robustify Prompt Tuning Vision Transformers</title><categories>cs.LG cs.AI cs.CV stat.ML</categories><comments>Published in Transactions on Machine Learning Research (2025)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The performance of deep models, including Vision Transformers, is known to be vulnerable to adversarial attacks. Many existing defenses against these attacks, such as adversarial training, rely on full-model fine-tuning to induce robustness in the models. These defenses require storing a copy of the entire model, that can have billions of parameters, for each task. At the same time, parameter-efficient prompt tuning is used to adapt large transformer-based models to downstream tasks without the need to save large copies. In this paper, we examine parameter-efficient prompt tuning of Vision Transformers for downstream tasks under the lens of robustness. We show that previous adversarial defense methods, when applied to the prompt tuning paradigm, suffer from gradient obfuscation and are vulnerable to adaptive attacks. We introduce ADAPT, a novel framework for performing adaptive adversarial training in the prompt tuning paradigm. Our method achieves competitive robust accuracy of ~40% w.r.t. SOTA robustness methods using full-model fine-tuning, by tuning only ~1% of the number of parameters. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.13340</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.13340</id><created>2024-03-20</created><updated>2025-02-07</updated><authors><author><keyname>Jiménez-Varón</keyname><forenames>Cristian F.</forenames></author><author><keyname>Sun</keyname><forenames>Ying</forenames></author><author><keyname>Shang</keyname><forenames>Han Lin</forenames></author></authors><title>Forecasting density-valued functional panel data</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce a statistical method for modeling and forecasting functional panel data represented by multiple densities. Density functions are nonnegative and have a constrained integral and thus do not constitute a linear vector space. We implement a center log-ratio transformation to transform densities into unconstrained functions. These functions exhibit cross-sectional correlation and temporal dependence. Via a functional analysis of variance decomposition, we decompose the unconstrained functional panel data into a deterministic trend component and a time-varying residual component. To produce forecasts for the time-varying component, a functional time series forecasting method, based on the estimation of the long-run covariance, is implemented. By combining the forecasts of the time-varying residual component with the deterministic trend component, we obtain $h$-step-ahead forecast curves for multiple populations. Illustrated by age- and sex-specific life-table death counts in the United States, we apply our proposed method to generate forecasts of the life-table death counts for 51 states. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.12120</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.12120</id><created>2024-06-17</created><updated>2025-02-06</updated><authors><author><keyname>Zhao</keyname><forenames>Yulai</forenames></author><author><keyname>Uehara</keyname><forenames>Masatoshi</forenames></author><author><keyname>Scalia</keyname><forenames>Gabriele</forenames></author><author><keyname>Kung</keyname><forenames>Sunyuan</forenames></author><author><keyname>Biancalani</keyname><forenames>Tommaso</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Hajiramezanali</keyname><forenames>Ehsan</forenames></author></authors><title>Adding Conditional Control to Diffusion Models with Reinforcement   Learning</title><categories>cs.LG cs.AI stat.ML</categories><comments>ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diffusion models are powerful generative models that allow for precise control over the characteristics of the generated samples. While these diffusion models trained on large datasets have achieved success, there is often a need to introduce additional controls in downstream fine-tuning processes, treating these powerful models as pre-trained diffusion models. This work presents a novel method based on reinforcement learning (RL) to add such controls using an offline dataset comprising inputs and labels. We formulate this task as an RL problem, with the classifier learned from the offline dataset and the KL divergence against pre-trained models serving as the reward functions. Our method, $\textbf{CTRL}$ ($\textbf{C}$onditioning pre-$\textbf{T}$rained diffusion models with $\textbf{R}$einforcement $\textbf{L}$earning), produces soft-optimal policies that maximize the abovementioned reward functions. We formally demonstrate that our method enables sampling from the conditional distribution with additional controls during inference. Our RL-based approach offers several advantages over existing methods. Compared to classifier-free guidance, it improves sample efficiency and can greatly simplify dataset construction by leveraging conditional independence between the inputs and additional controls. Additionally, unlike classifier guidance, it eliminates the need to train classifiers from intermediate states to additional controls. The code is available at https://github.com/zhaoyl18/CTRL. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.14535</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.14535</id><created>2024-06-20</created><updated>2025-02-06</updated><authors><author><keyname>Deng</keyname><forenames>Shiyuan</forenames></author><author><keyname>Tang</keyname><forenames>He</forenames></author><author><keyname>Bai</keyname><forenames>Shuyang</forenames></author></authors><title>On estimation and order selection for multivariate extremes via   clustering</title><categories>stat.ME math.ST stat.TH</categories><msc-class>62G32 (Primary), 60G70 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the estimation of multivariate extreme models with a discrete spectral measure using spherical clustering techniques. The primary contribution involves devising a method for selecting the order, that is, the number of clusters. The method consistently identifies the true order, i.e., the number of spectral atoms, and enjoys intuitive implementation in practice. Specifically, we introduce an extra penalty term to the well-known simplified average silhouette width, which penalizes small cluster sizes and small dissimilarities between cluster centers. Consequently, we provide a consistent method for determining the order of a max-linear factor model, where a typical information-based approach is not viable. Our second contribution is a large-deviation-type analysis for estimating the discrete spectral measure through clustering methods, which serves as an assessment of the convergence quality of clustering-based estimation for multivariate extremes. Additionally, as a third contribution, we discuss how estimating the discrete measure can lead to parameter estimations of heavy-tailed factor models. We also present simulations and real-data studies that demonstrate order selection and factor model estimation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.06120</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.06120</id><created>2024-07-08</created><updated>2024-11-16</updated><authors><author><keyname>Dong</keyname><forenames>Yijun</forenames></author><author><keyname>Phan</keyname><forenames>Hoang</forenames></author><author><keyname>Pan</keyname><forenames>Xiang</forenames></author><author><keyname>Lei</keyname><forenames>Qi</forenames></author></authors><title>Sketchy Moment Matching: Toward Fast and Provable Data Selection for   Finetuning</title><categories>cs.LG cs.NA math.NA stat.ML</categories><comments>NeurIPS 2024</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit data selection in a modern context of finetuning from a fundamental perspective. Extending the classical wisdom of variance minimization in low dimensions to high-dimensional finetuning, our generalization analysis unveils the importance of additionally reducing bias induced by low-rank approximation. Inspired by the variance-bias tradeoff in high dimensions from the theory, we introduce Sketchy Moment Matching (SkMM), a scalable data selection scheme with two stages. (i) First, the bias is controlled using gradient sketching that explores the finetuning parameter space for an informative low-dimensional subspace $\mathcal{S}$; (ii) then the variance is reduced over $\mathcal{S}$ via moment matching between the original and selected datasets. Theoretically, we show that gradient sketching is fast and provably accurate: selecting $n$ samples by reducing variance over $\mathcal{S}$ preserves the fast-rate generalization $O(\dim(\mathcal{S})/n)$, independent of the parameter dimension. Empirically, we concretize the variance-bias balance via synthetic experiments and demonstrate the effectiveness of SkMM for finetuning in real vision tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.21314</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.21314</id><created>2024-07-30</created><updated>2025-02-07</updated><authors><author><keyname>Li</keyname><forenames>Zhuoyuan</forenames></author><author><keyname>Dong</keyname><forenames>Bin</forenames></author><author><keyname>Zhang</keyname><forenames>Pingwen</forenames></author></authors><title>State-observation augmented diffusion model for nonlinear assimilation   with unknown dynamics</title><categories>cs.LG stat.ML</categories><msc-class>49N45, 60J60, 62F15, 68T20</msc-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Data assimilation has become a key technique for combining physical models with observational data to estimate state variables. However, classical assimilation algorithms often struggle with the high nonlinearity present in both physical and observational models. To address this challenge, a novel generative model, termed the State-Observation Augmented Diffusion (SOAD) model is proposed for data-driven assimilation. The marginal posterior associated with SOAD has been derived and then proved to match the true posterior distribution under mild assumptions, suggesting its theoretical advantages over previous score-based approaches. Experimental results also indicate that SOAD may offer improved performance compared to existing data-driven methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.03733</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.03733</id><created>2024-08-07</created><authors><author><keyname>Maillard</keyname><forenames>Antoine</forenames></author><author><keyname>Troiani</keyname><forenames>Emanuele</forenames></author><author><keyname>Martin</keyname><forenames>Simon</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Zdeborová</keyname><forenames>Lenka</forenames></author></authors><title>Bayes-optimal learning of an extensive-width neural network from   quadratically many samples</title><categories>stat.ML cond-mat.dis-nn cs.IT cs.LG math.IT math.PR</categories><comments>47 pages</comments><journal-ref>Advances in Neural Information Processing Systems 37 (NeurIPS   2024)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning a target function corresponding to a single hidden layer neural network, with a quadratic activation function after the first layer, and random weights. We consider the asymptotic limit where the input dimension and the network width are proportionally large. Recent work [Cui &amp; al '23] established that linear regression provides Bayes-optimal test error to learn such a function when the number of available samples is only linear in the dimension. That work stressed the open challenge of theoretically analyzing the optimal test error in the more interesting regime where the number of samples is quadratic in the dimension. In this paper, we solve this challenge for quadratic activations and derive a closed-form expression for the Bayes-optimal test error. We also provide an algorithm, that we call GAMP-RIE, which combines approximate message passing with rotationally invariant matrix denoising, and that asymptotically achieves the optimal performance. Technically, our result is enabled by establishing a link with recent works on optimal denoising of extensive-rank matrices and on the ellipsoid fitting problem. We further show empirically that, in the absence of noise, randomly-initialized gradient descent seems to sample the space of weights, leading to zero training loss, and averaging over initialization leads to a test error equal to the Bayes-optimal one. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.09004</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.09004</id><created>2024-08-16</created><updated>2025-02-06</updated><authors><author><keyname>Subedi</keyname><forenames>Unique</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Controlling Statistical, Discretization, and Truncation Errors in   Learning Fourier Linear Operators</title><categories>stat.ML cs.LG cs.NA math.NA</categories><comments>Added Experiments</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study learning-theoretic foundations of operator learning, using the linear layer of the Fourier Neural Operator architecture as a model problem. First, we identify three main errors that occur during the learning process: statistical error due to finite sample size, truncation error from finite rank approximation of the operator, and discretization error from handling functional data on a finite grid of domain points. Finally, we analyze a Discrete Fourier Transform (DFT) based least squares estimator, establishing both upper and lower bounds on the aforementioned errors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.09576</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.09576</id><created>2024-08-18</created><updated>2025-02-07</updated><authors><author><keyname>Oubari</keyname><forenames>Fouad</forenames></author><author><keyname>Baha</keyname><forenames>Mohamed El</forenames></author><author><keyname>Meunier</keyname><forenames>Raphael</forenames></author><author><keyname>Décatoire</keyname><forenames>Rodrigue</forenames></author><author><keyname>Mougeot</keyname><forenames>Mathilde</forenames></author></authors><title>A Markov Random Field Multi-Modal Variational AutoEncoder</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Recent advancements in multimodal Variational AutoEncoders (VAEs) have highlighted their potential for modeling complex data from multiple modalities. However, many existing approaches use relatively straightforward aggregating schemes that may not fully capture the complex dynamics present between different modalities. This work introduces a novel multimodal VAE that incorporates a Markov Random Field (MRF) into both the prior and posterior distributions. This integration aims to capture complex intermodal interactions more effectively. Unlike previous models, our approach is specifically designed to model and leverage the intricacies of these relationships, enabling a more faithful representation of multimodal data. Our experiments demonstrate that our model performs competitively on the standard PolyMNIST dataset and shows superior performance in managing complex intermodal dependencies in a specially designed synthetic dataset, intended to test intricate relationships. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.02363</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.02363</id><created>2024-09-03</created><updated>2025-02-06</updated><authors><author><keyname>Maiti</keyname><forenames>Ayan</forenames></author><author><keyname>Michelle</keyname><forenames>Michelle</forenames></author><author><keyname>Yang</keyname><forenames>Haizhao</forenames></author></authors><title>Optimal Neural Network Approximation for High-Dimensional Continuous   Functions</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the authors of Shen Yang Zhang (JMLR, 2022) developed a neural network with width $36d(2d + 1)$ and depth $11$, which utilizes a special activation function called the elementary universal activation function, to achieve the super approximation property for functions in $C([a,b]^d)$. That is, the constructed network only requires a fixed number of neurons (and thus parameters) to approximate a $d$-variate continuous function on a $d$-dimensional hypercube with arbitrary accuracy. More specifically, only $\mathcal{O}(d^2)$ neurons or parameters are used. One natural question is whether we can reduce the number of these neurons or parameters in such a network. By leveraging a variant of the Kolmogorov Superposition Theorem, our analysis shows that there is a neural network generated by the elementary universal activation function with at most $10889d+10887$ unique nonzero parameters such that this super approximation property is attained. Furthermore, we present a family of continuous functions that requires at least width $d$, and thus at least $d$ neurons or parameters, to achieve arbitrary accuracy in its approximation. This suggests that the number of unique nonzero parameters is optimal in the sense that it grows linearly with the input dimension $d$, unlike some approximation methods where parameters may grow exponentially with $d$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.09800</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.09800</id><created>2024-09-15</created><updated>2025-02-06</updated><authors><author><keyname>Calvello</keyname><forenames>Edoardo</forenames></author><author><keyname>Monmarché</keyname><forenames>Pierre</forenames></author><author><keyname>Stuart</keyname><forenames>Andrew M.</forenames></author><author><keyname>Vaes</keyname><forenames>Urbain</forenames></author></authors><title>Accuracy of the Ensemble Kalman Filter in the Near-Linear Setting</title><categories>math.ST cs.NA math.DS math.NA math.OC stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The filtering distribution captures the statistics of the state of a dynamical system from partial and noisy observations. Classical particle filters provably approximate this distribution in quite general settings; however they behave poorly for high dimensional problems, suffering weight collapse. This issue is circumvented by the ensemble Kalman filter which is an equal-weight interacting particle system. However, this finite particle system is only proven to approximate the true filter in the linear Gaussian case. In practice, however, it is applied in much broader settings; as a result, establishing its approximation properties more generally is important. There has been recent progress in the theoretical analysis of the algorithm, establishing stability and error estimates in non-Gaussian settings, but the assumptions on the dynamics and observation models rule out the unbounded vector fields that arise in practice and the analysis applies only to the mean field limit of the ensemble Kalman filter. The present work establishes error bounds between the filtering distribution and the finite particle ensemble Kalman filter when the dynamics and observation vector fields may be unbounded, allowing linear growth. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.11381</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.11381</id><created>2024-09-17</created><updated>2025-02-07</updated><authors><author><keyname>Banerjee</keyname><forenames>Debapratim</forenames></author><author><keyname>Mukherjee</keyname><forenames>Soumendu Sundar</forenames></author><author><keyname>Pal</keyname><forenames>Dipranjan</forenames></author></authors><title>Edge spectra of Gaussian random symmetric matrices with correlated   entries</title><categories>math.PR math-ph math.CO math.MP math.ST stat.TH</categories><comments>27 pages, 2 figures; abstract shortened to meet arXiv requirements</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the largest eigenvalue of a Gaussian random symmetric matrix $X_n$, with zero-mean, unit variance entries satisfying the condition $\sup_{(i, j) \ne (i', j')}|\mathbb{E}[X_{ij} X_{i'j'}]| = O(n^{-(1 + \varepsilon)})$, where $\varepsilon &gt; 0$. It follows from Catalano et al. (2024) that the empirical spectral distribution of $n^{-1/2} X_n$ converges weakly almost surely to the standard semi-circle law. Using a F\"{u}redi-Koml\'{o}s-type high moment analysis, we show that the largest eigenvalue $\lambda_1(n^{-1/2} X_n)$ of $n^{-1/2} X_n$ converges almost surely to $2$. This result is essentially optimal in the sense that one cannot take $\varepsilon = 0$ and still obtain an almost sure limit of $2$. We also derive Gaussian fluctuation results for the largest eigenvalue in the case where the entries have a common non-zero mean. Let $Y_n = X_n + \frac{\lambda}{\sqrt{n}}\mathbf{1} \mathbf{1}^\top$. When $\varepsilon \ge 1$ and $\lambda \gg n^{1/4}$, we show that \[   n^{1/2}\bigg(\lambda_1(n^{-1/2} Y_n) - \lambda - \frac{1}{\lambda}\bigg) \xrightarrow{d} \sqrt{2} Z, \] where $Z$ is a standard Gaussian. On the other hand, when $0 &lt; \varepsilon &lt; 1$, we have $\mathrm{Var}(\frac{1}{n}\sum_{i, j}X_{ij}) = O(n^{1 - \varepsilon})$. Assuming that $\mathrm{Var}(\frac{1}{n}\sum_{i, j} X_{ij}) = \sigma^2 n^{1 - \varepsilon} (1 + o(1))$, if $\lambda \gg n^{\varepsilon/4}$, then we have \[   n^{\varepsilon/2}\bigg(\lambda_1(n^{-1/2} Y_n) - \lambda - \frac{1}{\lambda}\bigg) \xrightarrow{d} \sigma Z. \] While the ranges of $\lambda$ in these fluctuation results are certainly not optimal, a striking aspect is that different scalings are required in the two regimes $0 &lt; \varepsilon &lt; 1$ and $\varepsilon \ge 1$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.19200</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.19200</id><created>2024-09-27</created><updated>2025-02-06</updated><authors><author><keyname>Bai</keyname><forenames>Site</forenames></author><author><keyname>Bullins</keyname><forenames>Brian</forenames></author></authors><title>Faster Acceleration for Steepest Descent</title><categories>math.OC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances (Sherman, 2017; Sidford and Tian, 2018; Cohen et al., 2021) have overcome the fundamental barrier of dimension dependence in the iteration complexity of solving $\ell_\infty$ regression with first-order methods. Yet it remains unclear to what extent such acceleration can be achieved for general $\ell_p$ smooth functions. In this paper, we propose a new accelerated first-order method for convex optimization under non-Euclidean smoothness assumptions. In contrast to standard acceleration techniques, our approach uses primal-dual iterate sequences taken with respect to $\textit{differing}$ norms, which are then coupled using an $\textit{implicitly}$ determined interpolation parameter. For $\ell_p$ norm smooth problems in $d$ dimensions, our method provides an iteration complexity improvement of up to $O(d^{1-\frac{2}{p}})$ in terms of calls to a first-order oracle, thereby allowing us to circumvent long-standing barriers in accelerated non-Euclidean steepest descent. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.03159</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.03159</id><created>2024-10-04</created><updated>2025-02-07</updated><authors><author><keyname>Lu</keyname><forenames>Jiecheng</forenames></author><author><keyname>Han</keyname><forenames>Xu</forenames></author><author><keyname>Sun</keyname><forenames>Yan</forenames></author><author><keyname>Yang</keyname><forenames>Shihao</forenames></author></authors><title>WAVE: Weighted Autoregressive Varing Gate for Time Series Forecasting</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Weighted Autoregressive Varing gatE (WAVE) attention mechanism equipped with both Autoregressive (AR) and Moving-average (MA) components. It can adapt to various attention mechanisms, enhancing and decoupling their ability to capture long-range and local temporal patterns in time series data. In this paper, we first demonstrate that, for the time series forecasting (TSF) task, the previously overlooked decoder-only autoregressive Transformer model can achieve results comparable to the best baselines when appropriate tokenization and training methods are applied. Moreover, inspired by the ARMA model from statistics and recent advances in linear attention, we introduce the full ARMA structure into existing autoregressive attention mechanisms. By using an indirect MA weight generation method, we incorporate the MA term while maintaining the time complexity and parameter size of the underlying efficient attention models. We further explore how indirect parameter generation can produce implicit MA weights that align with the modeling requirements for local temporal impacts. Experimental results show that WAVE attention that incorporates the ARMA structure consistently improves the performance of various AR attentions on TSF tasks, achieving state-of-the-art results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.15361</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.15361</id><created>2024-10-20</created><updated>2025-02-06</updated><authors><author><keyname>Zhou</keyname><forenames>Han</forenames></author><author><keyname>Van Landeghem</keyname><forenames>Jordy</forenames></author><author><keyname>Popordanoska</keyname><forenames>Teodora</forenames></author><author><keyname>Blaschko</keyname><forenames>Matthew B.</forenames></author></authors><title>A Novel Characterization of the Population Area Under the Risk Coverage   Curve (AURC) and Rates of Finite Sample Estimators</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The selective classifier (SC) has been proposed for rank based uncertainty thresholding, which could have applications in safety critical areas such as medical diagnostics, autonomous driving, and the justice system. The Area Under the Risk-Coverage Curve (AURC) has emerged as the foremost evaluation metric for assessing the performance of SC systems. In this work, we present a formal statistical formulation of population AURC, presenting an equivalent expression that can be interpreted as a reweighted risk function. Through Monte Carlo methods, we derive empirical AURC plug-in estimators for finite sample scenarios. The weight estimators associated with these plug-in estimators are shown to be consistent, with low bias and tightly bounded mean squared error (MSE). The plug-in estimators are proven to converge at a rate of $\mathcal{O}(\sqrt{\ln(n)/n})$ demonstrating statistical consistency. We empirically validate the effectiveness of our estimators through experiments across multiple datasets, model architectures, and confidence score functions (CSFs), demonstrating consistency and effectiveness in fine-tuning AURC performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.18959</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.18959</id><created>2024-10-24</created><updated>2025-02-06</updated><authors><author><keyname>Williams</keyname><forenames>Andrew Robert</forenames></author><author><keyname>Ashok</keyname><forenames>Arjun</forenames></author><author><keyname>Marcotte</keyname><forenames>Étienne</forenames></author><author><keyname>Zantedeschi</keyname><forenames>Valentina</forenames></author><author><keyname>Subramanian</keyname><forenames>Jithendaraa</forenames></author><author><keyname>Riachi</keyname><forenames>Roland</forenames></author><author><keyname>Requeima</keyname><forenames>James</forenames></author><author><keyname>Lacoste</keyname><forenames>Alexandre</forenames></author><author><keyname>Rish</keyname><forenames>Irina</forenames></author><author><keyname>Chapados</keyname><forenames>Nicolas</forenames></author><author><keyname>Drouin</keyname><forenames>Alexandre</forenames></author></authors><title>Context is Key: A Benchmark for Forecasting with Essential Textual   Information</title><categories>cs.LG cs.AI stat.ML</categories><comments>Preprint; under review. First two authors contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forecasting is a critical task in decision-making across numerous domains. While historical numerical data provide a start, they fail to convey the complete context for reliable and accurate predictions. Human forecasters frequently rely on additional information, such as background knowledge and constraints, which can efficiently be communicated through natural language. However, in spite of recent progress with LLM-based forecasters, their ability to effectively integrate this textual information remains an open question. To address this, we introduce "Context is Key" (CiK), a time-series forecasting benchmark that pairs numerical data with diverse types of carefully crafted textual context, requiring models to integrate both modalities; crucially, every task in CiK requires understanding textual context to be solved successfully. We evaluate a range of approaches, including statistical models, time series foundation models, and LLM-based forecasters, and propose a simple yet effective LLM prompting method that outperforms all other tested methods on our benchmark. Our experiments highlight the importance of incorporating contextual information, demonstrate surprising performance when using LLM-based forecasting models, and also reveal some of their critical shortcomings. This benchmark aims to advance multimodal forecasting by promoting models that are both accurate and accessible to decision-makers with varied technical expertise. The benchmark can be visualized at https://servicenow.github.io/context-is-key-forecasting/v0/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.19217</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.19217</id><created>2024-10-24</created><authors><author><keyname>Wu</keyname><forenames>Changlong</forenames></author><author><keyname>Grama</keyname><forenames>Ananth</forenames></author><author><keyname>Szpankowski</keyname><forenames>Wojciech</forenames></author></authors><title>No Free Lunch: Fundamental Limits of Learning Non-Hallucinating   Generative Models</title><categories>cs.LG cs.AI stat.ML</categories><journal-ref>International Conference on Learning Representations (ICLR 2025).   URL: https://openreview.net/pdf?id=OwNoTs2r8e</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative models have shown impressive capabilities in synthesizing high-quality outputs across various domains. However, a persistent challenge is the occurrence of "hallucinations", where the model produces outputs that are plausible but invalid. While empirical strategies have been explored to mitigate this issue, a rigorous theoretical understanding remains elusive. In this paper, we develop a theoretical framework to analyze the learnability of non-hallucinating generative models from a learning-theoretic perspective. Our results reveal that non-hallucinating learning is statistically impossible when relying solely on the training dataset, even for a hypothesis class of size two and when the entire training set is truthful. To overcome these limitations, we show that incorporating inductive biases aligned with the actual facts into the learning process is essential. We provide a systematic approach to achieve this by restricting the facts set to a concept class of finite VC-dimension and demonstrate its effectiveness under various learning paradigms. Although our findings are primarily conceptual, they represent a first step towards a principled approach to addressing hallucinations in learning generative models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.19725</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.19725</id><created>2024-10-25</created><updated>2025-02-06</updated><authors><author><keyname>Subedi</keyname><forenames>Unique</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>On the Benefits of Active Data Collection in Operator Learning</title><categories>stat.ML cs.LG</categories><comments>Moved Proofs to the Appendix</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study active data collection strategies for operator learning when the target operator is linear and the input functions are drawn from a mean-zero stochastic process with continuous covariance kernels. With an active data collection strategy, we establish an error convergence rate in terms of the decay rate of the eigenvalues of the covariance kernel. We can achieve arbitrarily fast error convergence rates with sufficiently rapid eigenvalue decay of the covariance kernels. This contrasts with the passive (i.i.d.) data collection strategies, where the convergence rate is never faster than linear decay ($\sim n^{-1}$). In fact, for our setting, we show a \emph{non-vanishing} lower bound for any passive data collection strategy, regardless of the eigenvalues decay rate of the covariance kernel. Overall, our results show the benefit of active data collection strategies in operator learning over their passive counterparts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.20640</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.20640</id><created>2024-10-27</created><updated>2025-02-07</updated><authors><author><keyname>Rivera</keyname><forenames>Eduardo Ochoa</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Near Optimal Pure Exploration in Logistic Bandits</title><categories>stat.ML cs.LG</categories><comments>25 pages, 2 figures. arXiv admin note: text overlap with   arXiv:2006.16073 by other authors</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bandit algorithms have garnered significant attention due to their practical applications in real-world scenarios. However, beyond simple settings such as multi-arm or linear bandits, optimal algorithms remain scarce. Notably, no optimal solution exists for pure exploration problems in the context of generalized linear model (GLM) bandits. In this paper, we narrow this gap and develop the first track-and-stop algorithm for general pure exploration problems under the logistic bandit called logistic track-and-stop (Log-TS). Log-TS is an efficient algorithm that asymptotically matches an approximation for the instance-specific lower bound of the expected sample complexity up to a logarithmic factor. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.03727</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.03727</id><created>2024-12-04</created><updated>2025-02-07</updated><authors><author><keyname>Zhang</keyname><forenames>Zhiheng</forenames></author><author><keyname>Wang</keyname><forenames>Zichen</forenames></author></authors><title>Online Experimental Design With Estimation-Regret Trade-off Under   Network Interference</title><categories>cs.LG math.OC math.ST stat.TH</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network interference has attracted significant attention in the field of causal inference, encapsulating various sociological behaviors where the treatment assigned to one individual within a network may affect the outcomes of others, such as their neighbors. A key challenge in this setting is that standard causal inference methods often assume independent treatment effects among individuals, which may not hold in networked environments. To estimate interference-aware causal effects, a traditional approach is to inherit the independent settings, where practitioners randomly assign experimental participants into different groups and compare their outcomes. While effective in offline settings, this strategy becomes problematic in sequential experiments, where suboptimal decision persists, leading to substantial regret. To address this issue, we introduce a unified interference-aware framework for online experimental design. Compared to existing studies, we extend the definition of arm space by utilizing the statistical concept of exposure mapping, which allows for a more flexible and context-aware representation of treatment effects in networked settings. Crucially, we establish a Pareto-optimal trade-off between estimation accuracy and regret under the network concerning both time period and arm space, which remains superior to baseline models even without network interference. Furthermore, we propose an algorithmic implementation and discuss its generalization across different learning settings and network topology </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.04767</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.04767</id><created>2024-12-05</created><updated>2025-02-06</updated><authors><author><keyname>Tian</keyname><forenames>Bowei</forenames></author><author><keyname>Wang</keyname><forenames>Ziyao</forenames></author><author><keyname>He</keyname><forenames>Shwai</forenames></author><author><keyname>Ye</keyname><forenames>Wanghao</forenames></author><author><keyname>Sun</keyname><forenames>Guoheng</forenames></author><author><keyname>Dai</keyname><forenames>Yucong</forenames></author><author><keyname>Wu</keyname><forenames>Yongkai</forenames></author><author><keyname>Li</keyname><forenames>Ang</forenames></author></authors><title>Towards counterfactual fairness through auxiliary variables</title><categories>cs.LG cs.DS stat.ML</categories><comments>arXiv admin note: text overlap with arXiv:2307.08232 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The challenge of balancing fairness and predictive accuracy in machine learning models, especially when sensitive attributes such as race, gender, or age are considered, has motivated substantial research in recent years. Counterfactual fairness ensures that predictions remain consistent across counterfactual variations of sensitive attributes, which is a crucial concept in addressing societal biases. However, existing counterfactual fairness approaches usually overlook intrinsic information about sensitive features, limiting their ability to achieve fairness while simultaneously maintaining performance. To tackle this challenge, we introduce EXOgenous Causal reasoning (EXOC), a novel causal reasoning framework motivated by exogenous variables. It leverages auxiliary variables to uncover intrinsic properties that give rise to sensitive attributes. Our framework explicitly defines an auxiliary node and a control node that contribute to counterfactual fairness and control the information flow within the model. Our evaluation, conducted on synthetic and real-world datasets, validates EXOC's superiority, showing that it outperforms state-of-the-art approaches in achieving counterfactual fairness. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.08435</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.08435</id><created>2024-12-11</created><updated>2025-02-07</updated><authors><author><keyname>Zhao</keyname><forenames>Lifan</forenames></author><author><keyname>Shen</keyname><forenames>Yanyan</forenames></author></authors><title>Proactive Model Adaptation Against Concept Drift for Online Time Series   Forecasting</title><categories>cs.LG cs.AI cs.CE stat.ML</categories><comments>Accepted by KDD 2025</comments><doi>10.1145/3690624.3709210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time series forecasting always faces the challenge of concept drift, where data distributions evolve over time, leading to a decline in forecast model performance. Existing solutions are based on online learning, which continually organize recent time series observations as new training samples and update model parameters according to the forecasting feedback on recent data. However, they overlook a critical issue: obtaining ground-truth future values of each sample should be delayed until after the forecast horizon. This delay creates a temporal gap between the training samples and the test sample. Our empirical analysis reveals that the gap can introduce concept drift, causing forecast models to adapt to outdated concepts. In this paper, we present Proceed, a novel proactive model adaptation framework for online time series forecasting. Proceed first estimates the concept drift between the recently used training samples and the current test sample. It then employs an adaptation generator to efficiently translate the estimated drift into parameter adjustments, proactively adapting the model to the test sample. To enhance the generalization capability of the framework, Proceed is trained on synthetic diverse concept drifts. Extensive experiments on five real-world datasets across various forecast models demonstrate that Proceed brings more performance improvements than the state-of-the-art online learning methods, significantly facilitating forecast models' resilience against concept drifts. Code is available at https://github.com/SJTU-DMTai/OnlineTSF. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.20471</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.20471</id><created>2024-12-29</created><updated>2025-02-07</updated><authors><author><keyname>Cai</keyname><forenames>Yang</forenames></author><author><keyname>Mitra</keyname><forenames>Siddharth</forenames></author><author><keyname>Wang</keyname><forenames>Xiuyuan</forenames></author><author><keyname>Wibisono</keyname><forenames>Andre</forenames></author></authors><title>On the Convergence of Min-Max Langevin Dynamics and Algorithm</title><categories>cs.GT cs.LG math.OC stat.ML</categories><comments>v2: Revised introduction and presentation of results</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study zero-sum games in the space of probability distributions over the Euclidean space $\mathbb{R}^d$ with entropy regularization, in the setting when the interaction function between the players is smooth and strongly convex-strongly concave. We prove an exponential convergence guarantee for the mean-field min-max Langevin dynamics to compute the equilibrium distribution of the zero-sum game. We also study the finite-particle approximation of the mean-field min-max Langevin dynamics, both in continuous and discrete times. We prove biased convergence guarantees for the continuous-time finite-particle min-max Langevin dynamics to the stationary mean-field equilibrium distribution with an explicit bias term which does not scale with the number of particles. We also prove biased convergence guarantees for the discrete-time finite-particle min-max Langevin algorithm to the stationary mean-field equilibrium distribution with an additional bias term which scales with the step size and the number of particles. This provides an explicit iteration complexity for the average particle along the finite-particle algorithm to approximately compute the equilibrium distribution of the zero-sum game. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.01437</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.01437</id><created>2024-12-23</created><updated>2025-02-06</updated><authors><author><keyname>Murphy</keyname><forenames>Charles</forenames></author><author><keyname>Lizotte</keyname><forenames>Simon</forenames></author><author><keyname>Thibault</keyname><forenames>François</forenames></author><author><keyname>Thibeault</keyname><forenames>Vincent</forenames></author><author><keyname>Desrosiers</keyname><forenames>Patrick</forenames></author><author><keyname>Allard</keyname><forenames>Antoine</forenames></author></authors><title>On the reconstruction limits of complex networks</title><categories>stat.AP cs.IT math.IT physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network reconstruction consists in retrieving the hidden interaction structure of a system from observations. Many reconstruction algorithms have been proposed, although less research has been devoted to describe their theoretical limitations. In this work, we adopt an information-theoretic perspective and define the reconstructability: The fraction of structural information recoverable from data. The reconstructability depends on the true data generating (TDG) model which is shown to set the reconstruction limit: any algorithm can perform, on average, at best like the TDG model. We show that the reconstructability is related to various performance measures, such as the probability of error and the Jaccard similarity. In an empirical context where the TDG model is unknown, we introduce the reconstruction index as an approximation of the reconstructability. We find that performing model selection is crucial for the validity of the reconstruction index as a proxy of the reconstructability of empirical time series and networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.07025</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.07025</id><created>2025-01-12</created><updated>2025-02-07</updated><authors><author><keyname>Zhang</keyname><forenames>Yong</forenames></author><author><keyname>Gyamfi</keyname><forenames>Eric Herrison</forenames></author></authors><title>A Weighted Similarity Metric for Community Detection in Sparse Data</title><categories>stat.ME cs.SI</categories><comments>This paper has been accepted for Workshop of AI for Social Impact at   AAAI 2025</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Many Natural Language Processing (NLP) related applications involves topics and sentiments derived from short documents such as consumer reviews and social media posts. Topics and sentiments of short documents are highly sparse because a short document generally covers a few topics among hundreds of candidates. Imputation of missing data is sometimes hard to justify and also often unpractical in highly sparse data. We developed a method for calculating a weighted similarity for highly sparse data without imputation. This weighted similarity is consist of three components to capture similarities based on both existence and lack of common properties and pattern of missing values. As a case study, we used a community detection algorithm and this weighted similarity to group different shampoo brands based on sparse topic sentiments derived from short consumer reviews. Compared with traditional imputation and similarity measures, the weighted similarity shows better performance in both general community structures and average community qualities. The performance is consistent and robust across metrics and community complexities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10117</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10117</id><created>2025-01-17</created><updated>2025-02-07</updated><authors><author><keyname>Liu</keyname><forenames>Weiguang</forenames></author><author><keyname>de Paula</keyname><forenames>Áureo</forenames></author><author><keyname>Tamer</keyname><forenames>Elie</forenames></author></authors><title>Prediction Sets and Conformal Inference with Censored Outcomes</title><categories>econ.EM stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given data on a scalar random variable $Y$, a prediction set for $Y$ with miscoverage level $\alpha$ is a set of values for $Y$ that contains a randomly drawn $Y$ with probability $1 - \alpha$, where $\alpha \in (0,1)$. Among all prediction sets that satisfy this coverage property, the oracle prediction set is the one with the smallest volume. This paper provides estimation methods of such prediction sets given observed conditioning covariates when $Y$ is \textit{censored} or \textit{measured in intervals}. We first characterise the oracle prediction set under interval censoring and develop a consistent estimator for the shortest prediction {\it interval} that satisfies this coverage property.These consistency results are extended to accommodate cases where the prediction set consists of multiple disjoint intervals. We use conformal inference to construct a prediction set that achieves finite-sample validity under censoring and maintains consistency as sample size increases, using a conformity score function designed for interval data. The procedure accommodates the prediction uncertainty that is irreducible (due to the stochastic nature of outcomes), the modelling uncertainty due to partial identification and also sampling uncertainty that gets reduced as samples get larger. We conduct a set of Monte Carlo simulations and an application to data from the Current Population Survey. The results highlight the robustness and efficiency of the proposed methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10974</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10974</id><created>2025-01-19</created><updated>2025-02-06</updated><authors><author><keyname>Huang</keyname><forenames>Yu-Han</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Sequential Change Detection for Learning in Piecewise Stationary Bandit   Environments</title><categories>cs.IT cs.SY eess.SY math.IT stat.OT</categories><comments>15 pages, 2 figures. arXiv admin note: text overlap with   arXiv:2501.01291</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A finite-horizon variant of the quickest change detection problem is investigated, which is motivated by a change detection problem that arises in piecewise stationary bandits. The goal is to minimize the \emph{latency}, which is smallest threshold such that the probability that the detection delay exceeds the threshold is below a desired low level, while controlling the false alarm probability to a desired low level. When the pre- and post-change distributions are unknown, two tests are proposed as candidate solutions. These tests are shown to attain order optimality in terms of the horizon. Furthermore, the growth in their latencies with respect to the false alarm probability and late detection probability satisfies a property that is desirable in regret analysis for piecewise stationary bandits. Numerical results are provided to validate the theoretical performance results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14974</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14974</id><created>2025-01-24</created><updated>2025-02-06</updated><authors><author><keyname>Deng</keyname><forenames>Fengnan</forenames></author><author><keyname>Vidyashankar</keyname><forenames>Anand N.</forenames></author></authors><title>Private Minimum Hellinger Distance Estimation via Hellinger Distance   Differential Privacy</title><categories>math.ST cs.CR math.PR stat.ME stat.ML stat.TH</categories><msc-class>62F35, 68P27, 62E20, 60E05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective functions based on Hellinger distance yield robust and efficient estimators of model parameters. Motivated by privacy and regulatory requirements encountered in contemporary applications, we derive in this paper \emph{private minimum Hellinger distance estimators}. The estimators satisfy a new privacy constraint, namely, Hellinger differential privacy, while retaining the robustness and efficiency properties. We demonstrate that Hellinger differential privacy shares several features of standard differential privacy while allowing for sharper inference. Additionally, for computational purposes, we also develop Hellinger differentially private gradient descent and Newton-Raphson algorithms. We illustrate the behavior of our estimators in finite samples using numerical experiments and verify that they retain robustness properties under gross-error contamination. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18502</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18502</id><created>2025-01-30</created><updated>2025-02-07</updated><authors><author><keyname>Kumar</keyname><forenames>Ritesh</forenames></author><author><keyname>Vatedka</keyname><forenames>Shashank</forenames></author></authors><title>One-Bit Distributed Mean Estimation with Unknown Variance</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>21 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this work, we study the problem of distributed mean estimation with $1$-bit communication constraints when the variance is unknown. We focus on the specific case where each user has access to one i.i.d. sample drawn from a distribution that belongs to a scale-location family, and is limited to sending just a single bit of information to a central server whose goal is to estimate the mean. We propose non-adaptive and adaptive estimators that are shown to be asymptotically normal. We derive bounds on the asymptotic (in the number of users) Mean Squared Error (MSE) achieved by these estimators. For a class of symmetric log-concave distributions, we derive matching lower bounds for the MSE achieved by adaptive estimators, proving the optimality of our scheme. We show that non-adaptive estimators can be strictly suboptimal by deriving a lower bound on the MSE achieved by any non-adaptive estimator for Gaussian distributions and demonstrating a positive gap between this and the MSE achieved by our adaptive scheme. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00168</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00168</id><created>2025-01-31</created><updated>2025-02-07</updated><authors><author><keyname>Herrera-Esposito</keyname><forenames>Daniel</forenames></author><author><keyname>Burge</keyname><forenames>Johannes</forenames></author></authors><title>Supervised Quadratic Feature Analysis: An Information Geometry Approach   to Dimensionality Reduction</title><categories>stat.ML cs.LG math.DG math.ST stat.TH</categories><comments>18 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Supervised dimensionality reduction aims to map labeled data to a low-dimensional feature space while maximizing class discriminability. Despite the availability of methods for learning complex non-linear features (e.g. Deep Learning), there is an enduring demand for dimensionality reduction methods that learn linear features due to their interpretability, low computational cost, and broad applicability. However, there is a gap between methods that optimize linear separability (e.g. LDA), and more flexible but computationally expensive methods that optimize over arbitrary class boundaries (e.g. metric-learning methods). Here, we present Supervised Quadratic Feature Analysis (SQFA), a dimensionality reduction method for learning linear features that maximize the differences between class-conditional first- and second-order statistics, which allow for quadratic discrimination. SQFA exploits the information geometry of second-order statistics in the symmetric positive definite manifold. We show that SQFA features support quadratic discriminability in real-world problems. We also provide a theoretical link, based on information geometry, between SQFA and the Quadratic Discriminant Analysis (QDA) classifier. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00182</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00182</id><created>2025-01-31</created><updated>2025-02-07</updated><authors><author><keyname>Seo</keyname><forenames>Jungwon</forenames></author><author><keyname>Catak</keyname><forenames>Ferhat Ozgur</forenames></author><author><keyname>Rong</keyname><forenames>Chunming</forenames></author></authors><title>Understanding Federated Learning from IID to Non-IID dataset: An   Experimental Study</title><categories>cs.LG cs.AI stat.ML</categories><journal-ref>36th Norwegian ICT Conference for Research and Education, NIKT   2024</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As privacy concerns and data regulations grow, federated learning (FL) has emerged as a promising approach for training machine learning models across decentralized data sources without sharing raw data. However, a significant challenge in FL is that client data are often non-IID (non-independent and identically distributed), leading to reduced performance compared to centralized learning. While many methods have been proposed to address this issue, their underlying mechanisms are often viewed from different perspectives. Through a comprehensive investigation from gradient descent to FL, and from IID to non-IID data settings, we find that inconsistencies in client loss landscapes primarily cause performance degradation in non-IID scenarios. From this understanding, we observe that existing methods can be grouped into two main strategies: (i) adjusting parameter update paths and (ii) modifying client loss landscapes. These findings offer a clear perspective on addressing non-IID challenges in FL and help guide future research in the field. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00838</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00838</id><created>2025-02-02</created><authors><author><keyname>Bussemaker</keyname><forenames>Jasper H.</forenames></author><author><keyname>Saves</keyname><forenames>Paul</forenames></author><author><keyname>Bartoli</keyname><forenames>Nathalie</forenames></author><author><keyname>Lefebvre</keyname><forenames>Thierry</forenames></author><author><keyname>Lafage</keyname><forenames>Rémi</forenames></author></authors><title>System Architecture Optimization Strategies: Dealing with Expensive   Hierarchical Problems</title><categories>math.OC cs.DM stat.AP</categories><comments>J Glob Optim (2024)</comments><doi>10.1007/s10898-024-01443-8</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Choosing the right system architecture for the problem at hand is challenging due to the large design space and high uncertainty in the early stage of the design process. Formulating the architecting process as an optimization problem may mitigate some of these challenges. This work investigates strategies for solving System Architecture Optimization (SAO) problems: expensive, black-box, hierarchical, mixed-discrete, constrained, multi-objective problems that may be subject to hidden constraints. Imputation ratio, correction ratio, correction fraction, and max rate diversity metrics are defined for characterizing hierar chical design spaces. This work considers two classes of optimization algorithms for SAO: Multi-Objective Evolutionary Algorithms (MOEA) such as NSGA-II, and Bayesian Optimization (BO) algorithms. A new Gaussian process kernel is presented that enables modeling hierarchical categorical variables, extending previous work on modeling continuous and integer hierarchical variables. Next, a hierarchical sampling algorithm that uses design space hierarchy to group design vectors by active design variables is developed. Then, it is demonstrated that integrating more hierarchy information in the optimization algorithms yields better optimization results for BO algorithms. Several realistic single-objective and multi-objective test problems are used for investigations. Finally, the BO algorithm is applied to a jet engine architecture optimization problem. This work shows that the developed BO algorithm can effectively solve the problem with one order of magnitude less function evaluations than NSGA-II. The algorithms and problems used in this work are implemented in the open-source Python library SBArchOpt. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00854</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00854</id><created>2025-02-02</created><authors><author><keyname>Priem</keyname><forenames>Rémy</forenames></author><author><keyname>Diouane</keyname><forenames>Youssef</forenames></author><author><keyname>Bartoli</keyname><forenames>Nathalie</forenames></author><author><keyname>Dubreuil</keyname><forenames>Sylvain</forenames></author><author><keyname>Saves</keyname><forenames>Paul</forenames></author></authors><title>High-Dimensional Bayesian Optimization Using Both Random and Supervised   Embeddings</title><categories>math.OC cs.LG stat.ML</categories><journal-ref>AIAA Journal 2025 63:1, 162-173</journal-ref><doi>10.2514/1.J063488</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Bayesian optimization (BO) is one of the most powerful strategies to solve computationally expensive-to-evaluate blackbox optimization problems. However, BO methods are conventionally used for optimization problems of small dimension because of the curse of dimensionality. In this paper, a high-dimensionnal optimization method incorporating linear embedding subspaces of small dimension is proposed to efficiently perform the optimization. An adaptive learning strategy for these linear embeddings is carried out in conjunction with the optimization. The resulting BO method, named efficient global optimization coupled with random and supervised embedding (EGORSE), combines in an adaptive way both random and supervised linear embeddings. EGORSE has been compared to state-of-the-art algorithms and tested on academic examples with a number of design variables ranging from 10 to 600. The obtained results show the high potential of EGORSE to solve high-dimensional blackbox optimization problems, in terms of both CPU time and the limited number of calls to the expensive blackbox simulation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02496</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02496</id><created>2025-02-04</created><updated>2025-02-07</updated><authors><author><keyname>Kolb</keyname><forenames>Chris</forenames></author><author><keyname>Weber</keyname><forenames>Tobias</forenames></author><author><keyname>Bischl</keyname><forenames>Bernd</forenames></author><author><keyname>Rügamer</keyname><forenames>David</forenames></author></authors><title>Deep Weight Factorization: Sparse Learning Through the Lens of   Artificial Symmetries</title><categories>cs.LG stat.ML</categories><comments>accepted at ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sparse regularization techniques are well-established in machine learning, yet their application in neural networks remains challenging due to the non-differentiability of penalties like the $L_1$ norm, which is incompatible with stochastic gradient descent. A promising alternative is shallow weight factorization, where weights are decomposed into two factors, allowing for smooth optimization of $L_1$-penalized neural networks by adding differentiable $L_2$ regularization to the factors. In this work, we introduce deep weight factorization, extending previous shallow approaches to more than two factors. We theoretically establish equivalence of our deep factorization with non-convex sparse regularization and analyze its impact on training dynamics and optimization. Due to the limitations posed by standard training practices, we propose a tailored initialization scheme and identify important learning rate requirements necessary for training factorized networks. We demonstrate the effectiveness of our deep weight factorization through experiments on various architectures and datasets, consistently outperforming its shallow counterpart and widely used pruning methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02861</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02861</id><created>2025-02-04</created><updated>2025-02-06</updated><authors><author><keyname>Shen</keyname><forenames>Judy Hanwen</forenames></author><author><keyname>Vitercik</keyname><forenames>Ellen</forenames></author><author><keyname>Wikum</keyname><forenames>Anders</forenames></author></authors><title>Algorithms with Calibrated Machine Learning Predictions</title><categories>stat.ML cs.DS cs.LG</categories><comments>Replacement: updated acknowledgments</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The field of algorithms with predictions incorporates machine learning advice in the design of online algorithms to improve real-world performance. While this theoretical framework often assumes uniform reliability across all predictions, modern machine learning models can now provide instance-level uncertainty estimates. In this paper, we propose calibration as a principled and practical tool to bridge this gap, demonstrating the benefits of calibrated advice through two case studies: the ski rental and online job scheduling problems. For ski rental, we design an algorithm that achieves optimal prediction-dependent performance and prove that, in high-variance settings, calibrated advice offers more effective guidance than alternative methods for uncertainty quantification. For job scheduling, we demonstrate that using a calibrated predictor leads to significant performance improvements over existing methods. Evaluations on real-world data validate our theoretical findings, highlighting the practical impact of calibration for algorithms with predictions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03479</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03479</id><created>2025-01-27</created><updated>2025-02-06</updated><authors><author><keyname>Cuicizion</keyname><forenames>Eliuvish</forenames></author><author><keyname>Ri</keyname><forenames>Itsugo</forenames></author><author><keyname>Holmes</keyname><forenames>Elaine</forenames></author><author><keyname>Lyeutsaon</keyname><forenames>Sho</forenames></author><author><keyname>Chern</keyname><forenames>Jawad Hassan</forenames></author></authors><title>A Tutorial on Markov Renewal and Semi-Markov Proportional Hazards Model</title><categories>stat.AP stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Transition probability estimation plays a critical role in multi-state modeling, especially in clinical research. This paper investigates the application of semi-Markov and Markov renewal frameworks to the EBMT dataset, focusing on six clinical states encountered during hematopoietic stem cell transplantation. By comparing Aalen-Johansen (AJ) and Dabrowska-Sun-Horowitz (DSH) estimators, we demonstrate that semi-Markov models, which incorporate sojourn times, provide a more nuanced and temporally sensitive depiction of patient trajectories compared to memoryless Markov models. The DSH estimator consistently yields smoother probability curves, particularly for transitions involving prolonged states. We use empirical process theory and Burkholder-Davis-Gundy inequality to show weak convergence of the estimator. Future work includes extending the framework to accommodate advanced covariate structures and non-Markovian dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03587</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03587</id><created>2025-02-05</created><updated>2025-02-06</updated><authors><author><keyname>von Seeger</keyname><forenames>Anneke</forenames></author><author><keyname>Zou</keyname><forenames>Dongmian</forenames></author><author><keyname>Lerman</keyname><forenames>Gilad</forenames></author></authors><title>Stein Discrepancy for Unsupervised Domain Adaptation</title><categories>cs.LG stat.ML</categories><comments>24 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Unsupervised domain adaptation (UDA) leverages information from a labeled source dataset to improve accuracy on a related but unlabeled target dataset. A common approach to UDA is aligning representations from the source and target domains by minimizing the distance between their data distributions. Previous methods have employed distances such as Wasserstein distance and maximum mean discrepancy. However, these approaches are less effective when the target data is significantly scarcer than the source data. Stein discrepancy is an asymmetric distance between distributions that relies on one distribution only through its score function. In this paper, we propose a novel UDA method that uses Stein discrepancy to measure the distance between source and target domains. We develop a learning framework using both non-kernelized and kernelized Stein discrepancy. Theoretically, we derive an upper bound for the generalization error. Numerical experiments show that our method outperforms existing methods using other domain discrepancy measures when only small amounts of target data are available. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03942</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03942</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Holst</keyname><forenames>Klaus Kähler</forenames></author><author><keyname>Nordland</keyname><forenames>Andreas</forenames></author><author><keyname>Furkjær</keyname><forenames>Julie</forenames></author><author><keyname>Damgaard</keyname><forenames>Lars Holm</forenames></author><author><keyname>Pipper</keyname><forenames>Christian Bressen</forenames></author></authors><title>A retake on the analysis of scores truncated by terminal events</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of data from randomized controlled trials in vulnerable populations requires special attention when assessing treatment effect by a score measuring, e.g., disease stage or activity together with onset of prevalent terminal events. In reality, it is impossible to disentangle a disease score from the terminal event, since the score is not clinically meaningful after this event. In this work, we propose to assess treatment interventions simultaneously on disease score and the terminal event. Our proposal is based on a natural data-generating mechanism respecting that a disease score does not exist beyond the terminal event. We use modern semi-parametric statistical methods to provide robust and efficient estimation of the risk of terminal event and expected disease score conditional on no terminal event at a pre-specified landmark time. We also use the simultaneous asymptotic behavior of our estimators to develop a powerful closed testing procedure for confirmatory assessment of treatment effect on both onset of terminal event and level of disease score. A simulation study mimicking a large-scale outcome trial in chronic kidney patients as well as an analysis of that trial is provided to assess performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04168</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04168</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Ferradini</keyname><forenames>Carla</forenames></author><author><keyname>Gitton</keyname><forenames>Victor</forenames></author><author><keyname>Vilasini</keyname><forenames>V.</forenames></author></authors><title>Cyclic quantum causal modelling with a graph separation theorem</title><categories>quant-ph math.ST stat.ML stat.TH</categories><comments>41+41 pages. A companion paper by the same authors, focussing on   cyclic classical (functional) causal models has been submitted to the arXiv   concurrently with primary class [math.ST], v2 only differs from v1 in   including the arXiv number of the companion paper. Comments are welcome</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Causal modelling frameworks link observable correlations to causal explanations, which is a crucial aspect of science. These models represent causal relationships through directed graphs, with vertices and edges denoting systems and transformations within a theory. Most studies focus on acyclic causal graphs, where well-defined probability rules and powerful graph-theoretic properties like the d-separation theorem apply. However, understanding complex feedback processes and exotic fundamental scenarios with causal loops requires cyclic causal models, where such results do not generally hold. While progress has been made in classical cyclic causal models, challenges remain in uniquely fixing probability distributions and identifying graph-separation properties applicable in general cyclic models. In cyclic quantum scenarios, existing frameworks have focussed on a subset of possible cyclic causal scenarios, with graph-separation properties yet unexplored. This work proposes a framework applicable to all consistent quantum and classical cyclic causal models on finite-dimensional systems. We address these challenges by introducing a robust probability rule and a novel graph-separation property, p-separation, which we prove to be sound and complete for all such models. Our approach maps cyclic causal models to acyclic ones with post-selection, leveraging the post-selected quantum teleportation protocol. We characterize these protocols and their success probabilities along the way. We also establish connections between this formalism and other classical and quantum frameworks to inform a more unified perspective on causality. This provides a foundation for more general cyclic causal discovery algorithms and to systematically extend open problems and techniques from acyclic informational networks (e.g., certification of non-classicality) to cyclic causal structures and networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04171</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04171</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Ferradini</keyname><forenames>Carla</forenames></author><author><keyname>Gitton</keyname><forenames>Victor</forenames></author><author><keyname>Vilasini</keyname><forenames>V.</forenames></author></authors><title>Cyclic functional causal models beyond unique solvability with a graph   separation theorem</title><categories>math.ST quant-ph stat.ML stat.TH</categories><comments>33+16 pages. A companion paper by the same authors, focussing on   cyclic quantum causal models has been submitted to the arXiv concurrently   with primary class [quant-ph], v2 only differs from v1 in including the arXiv   number of the companion paper. Comments are welcome</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Functional causal models (fCMs) specify functional dependencies between random variables associated to the vertices of a graph. In directed acyclic graphs (DAGs), fCMs are well-understood: a unique probability distribution on the random variables can be easily specified, and a crucial graph-separation result called the d-separation theorem allows one to characterize conditional independences between the variables. However, fCMs on cyclic graphs pose challenges due to the absence of a systematic way to assign a unique probability distribution to the fCM's variables, the failure of the d-separation theorem, and lack of a generalization of this theorem that is applicable to all consistent cyclic fCMs. In this work, we develop a causal modeling framework applicable to all cyclic fCMs involving finite-cardinality variables, except inconsistent ones admitting no solutions. Our probability rule assigns a unique distribution even to non-uniquely solvable cyclic fCMs and reduces to the known rule for uniquely solvable fCMs. We identify a class of fCMs, called averagely uniquely solvable, that we show to be the largest class where the probabilities admit a Markov factorization. Furthermore, we introduce a new graph-separation property, p-separation, and prove this to be sound and complete for all consistent finite-cardinality cyclic fCMs while recovering the d-separation theorem for DAGs. These results are obtained by considering classical post-selected teleportation protocols inspired by analogous protocols in quantum information theory. We discuss further avenues for exploration, linking in particular problems in cyclic fCMs and in quantum causality. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04208</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04208</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Grünwald</keyname><forenames>Peter D.</forenames></author><author><keyname>Koolen</keyname><forenames>Wouter M.</forenames></author></authors><title>Supermartingales for One-Sided Tests: Sufficient Monotone Likelihood   Ratios are Sufficient</title><categories>math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The t-statistic is a widely-used scale-invariant statistic for testing the null hypothesis that the mean is zero. Martingale methods enable sequential testing with the t-statistic at every sample size, while controlling the probability of falsely rejecting the null. For one-sided sequential tests, which reject when the t-statistic is too positive, a natural question is whether they also control false rejection when the true mean is negative. We prove that this is the case using monotone likelihood ratios and sufficient statistics. We develop applications to the scale-invariant t-test, the location-invariant $\chi^2$-test and sequential linear regression with nuisance covariates. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04339</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04339</id><created>2025-02-01</created><authors><author><keyname>George</keyname><forenames>Anand Jerry</forenames></author><author><keyname>Veiga</keyname><forenames>Rodrigo</forenames></author><author><keyname>Macris</keyname><forenames>Nicolas</forenames></author></authors><title>Analysis of Diffusion Models for Manifold Data</title><categories>math.ST cond-mat.dis-nn cs.IT cs.LG math.IT math.PR stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We analyze the time reversed dynamics of generative diffusion models. If the exact empirical score function is used in a regime of large dimension and exponentially large number of samples, these models are known to undergo transitions between distinct dynamical regimes. We extend this analysis and compute the transitions for an analytically tractable manifold model where the statistical model for the data is a mixture of lower dimensional Gaussians embedded in higher dimensional space. We compute the so-called speciation and collapse transition times, as a function of the ratio of manifold-to-ambient space dimensions, and other characteristics of the data model. An important tool used in our analysis is the exact formula for the mutual information (or free energy) of Generalized Linear Models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04372</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04372</id><created>2025-02-05</created><authors><author><keyname>Genari</keyname><forenames>Juliano</forenames></author><author><keyname>Goedert</keyname><forenames>Guilherme Tegoni</forenames></author></authors><title>Mining Unstructured Medical Texts With Conformal Active Learning</title><categories>cs.CL cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The extraction of relevant data from Electronic Health Records (EHRs) is crucial to identifying symptoms and automating epidemiological surveillance processes. By harnessing the vast amount of unstructured text in EHRs, we can detect patterns that indicate the onset of disease outbreaks, enabling faster, more targeted public health responses. Our proposed framework provides a flexible and efficient solution for mining data from unstructured texts, significantly reducing the need for extensive manual labeling by specialists. Experiments show that our framework achieving strong performance with as few as 200 manually labeled texts, even for complex classification problems. Additionally, our approach can function with simple lightweight models, achieving competitive and occasionally even better results compared to more resource-intensive deep learning models. This capability not only accelerates processing times but also preserves patient privacy, as the data can be processed on weaker on-site hardware rather than being transferred to external systems. Our methodology, therefore, offers a practical, scalable, and privacy-conscious approach to real-time epidemiological monitoring, equipping health institutions to respond rapidly and effectively to emerging health threats. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04422</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04422</id><created>2025-02-06</created><authors><author><keyname>Yadav</keyname><forenames>Pooja</forenames></author><author><keyname>Srivastava</keyname><forenames>Tanuja</forenames></author></authors><title>The Maximum Likelihood Degree of Farlie Gumbel Morgenstern Bivariate   Exponential Distribution</title><categories>math.ST math.AC stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum likelihood degree of a statistical model refers to the number of solutions, where the derivative of the log-likelihood function is zero, over the complex field. This paper examines the maximum likelihood degree of the parameter in Farlie-Gumbel-Morgenstern bivariate exponential distribution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04464</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04464</id><created>2025-02-06</created><authors><author><keyname>Jadoul</keyname><forenames>Yannick</forenames></author><author><keyname>Tufarelli</keyname><forenames>Tommaso</forenames></author><author><keyname>Coissac</keyname><forenames>Chloé</forenames></author><author><keyname>Gamba</keyname><forenames>Marco</forenames></author><author><keyname>Ravignani</keyname><forenames>Andrea</forenames></author></authors><title>Hidden assumptions of integer ratio analyses in bioacoustics and music</title><categories>stat.AP</categories><comments>19 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Rhythm is ubiquitous in human culture and in nature, but hard to capture in all its complexity. A key dimension of rhythm, integer ratio categories occur when the relationship between temporal intervals can be expressed as small-integer ratios. Recent work has found integer ratio categories in most human musical cultures and some animal species' vocalizations or behavioral displays. But biological systems are noisy, and empirically measured intervals rarely form an exact small-integer ratio. Here, we mathematically assess whether the leading integer ratio analysis method makes valid statistical and biological assumptions. In particular, we (1) make the temporal properties of empirical ratios explicit, both in general and for the typical use in the literature; (2) show how the choice of ratio formula affects the probability distribution of rhythm ratios and ensuing statistical results; (3) guide the reader to carefully consider the assumptions and null hypotheses of the statistical analysis; (4) present a comprehensive methodology to statistically test integer ratios for any null hypothesis of choice. Our observations have implications for both past and future research in music cognition and animal behavior: They suggest how to interpret past findings and provide tools to choose the correct null hypotheses in future empirical work. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04491</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04491</id><created>2025-02-06</created><authors><author><keyname>Cheng</keyname><forenames>Ziheng</forenames></author><author><keyname>Xie</keyname><forenames>Tianyu</forenames></author><author><keyname>Zhang</keyname><forenames>Shiyue</forenames></author><author><keyname>Zhang</keyname><forenames>Cheng</forenames></author></authors><title>Provable Sample-Efficient Transfer Learning Conditional Diffusion Models   via Representation Learning</title><categories>cs.LG math.ST stat.ML stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While conditional diffusion models have achieved remarkable success in various applications, they require abundant data to train from scratch, which is often infeasible in practice. To address this issue, transfer learning has emerged as an essential paradigm in small data regimes. Despite its empirical success, the theoretical underpinnings of transfer learning conditional diffusion models remain unexplored. In this paper, we take the first step towards understanding the sample efficiency of transfer learning conditional diffusion models through the lens of representation learning. Inspired by practical training procedures, we assume that there exists a low-dimensional representation of conditions shared across all tasks. Our analysis shows that with a well-learned representation from source tasks, the samplecomplexity of target tasks can be reduced substantially. In addition, we investigate the practical implications of our theoretical results in several real-world applications of conditional diffusion models. Numerical experiments are also conducted to verify our results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04543</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04543</id><created>2025-02-06</created><authors><author><keyname>Lu</keyname><forenames>Zhou</forenames></author><author><keyname>Sun</keyname><forenames>Y. Jennifer</forenames></author><author><keyname>Zhang</keyname><forenames>Zhiyu</forenames></author></authors><title>Sparsity-Based Interpolation of External, Internal and Swap Regret</title><categories>stat.ML cs.LG</categories><comments>Equal contribution, alphabetical order</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Focusing on the expert problem in online learning, this paper studies the interpolation of several performance metrics via $\phi$-regret minimization, which measures the performance of an algorithm by its regret with respect to an arbitrary action modification rule $\phi$. With $d$ experts and $T\gg d$ rounds in total, we present a single algorithm achieving the instance-adaptive $\phi$-regret bound \begin{equation*} \tilde O\left(\min\left\{\sqrt{d-d^{\mathrm{unif}}_\phi+1},\sqrt{d-d^{\mathrm{self}}_\phi}\right\}\cdot\sqrt{T}\right), \end{equation*} where $d^{\mathrm{unif}}_\phi$ is the maximum amount of experts modified identically by $\phi$, and $d^{\mathrm{self}}_\phi$ is the amount of experts that $\phi$ trivially modifies to themselves. By recovering the optimal $O(\sqrt{T\log d})$ external regret bound when $d^{\mathrm{unif}}_\phi=d$, the standard $\tilde O(\sqrt{T})$ internal regret bound when $d^{\mathrm{self}}_\phi=d-1$ and the optimal $\tilde O(\sqrt{dT})$ swap regret bound in the worst case, we improve existing results in the intermediate regimes. In addition, the same algorithm achieves the optimal quantile regret bound, which corresponds to even easier settings of $\phi$ than the external regret.   Building on the classical reduction from $\phi$-regret minimization to external regret minimization on stochastic matrices, our main idea is to further convert the latter to online linear regression using Haar-wavelet-inspired matrix features. Then, we apply a particular $L_1$-version of comparator-adaptive online learning algorithms to exploit the sparsity in this regression subroutine. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04550</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04550</id><created>2025-02-06</created><authors><author><keyname>Faes</keyname><forenames>Luca</forenames></author><author><keyname>Sparacino</keyname><forenames>Laura</forenames></author><author><keyname>Mijatovic</keyname><forenames>Gorana</forenames></author><author><keyname>Antonacci</keyname><forenames>Yuri</forenames></author><author><keyname>Ricci</keyname><forenames>Leonardo</forenames></author><author><keyname>Marinazzo</keyname><forenames>Daniele</forenames></author><author><keyname>Stramaglia</keyname><forenames>Sebastiano</forenames></author></authors><title>Partial Information Rate Decomposition</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Partial Information Decomposition (PID) is a principled and flexible method to unveil complex high-order interactions in multi-unit network systems. Though being defined exclusively for random variables, PID is ubiquitously applied to multivariate time series taken as realizations of random processes with temporal statistical structure. Here, to overcome the incorrect depiction of high-order effects by PID schemes applied to dynamic networks, we introduce the framework of Partial Information Rate Decomposition (PIRD). PIRD is formalized applying lattice theory to decompose the information shared dynamically between a target random process and a set of source processes, implemented for Gaussian processes through a spectral expansion of information rates, and demonstrated in practice analyzing time series from large-scale climate oscillations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04553</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04553</id><created>2025-02-06</created><authors><author><keyname>Swanson</keyname><forenames>David</forenames></author><author><keyname>Sherry</keyname><forenames>Alexander</forenames></author><author><keyname>Tang</keyname><forenames>Chad</forenames></author></authors><title>Variance component mixture modelling for longitudinal T-cell receptor   clonal dynamics</title><categories>stat.ME</categories><msc-class>62F15 (Primary), 62H30 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Studies of T cells and their clonally unique receptors have shown promise in elucidating the association between immune response and human disease. Methods to identify T-cell receptor clones which expand or contract in response to certain therapeutic strategies have so far been limited to longitudinal pairwise comparisons of clone frequency with multiplicity adjustment. Here we develop a more general mixture model approach for arbitrary follow-up and missingness which partitions dynamic longitudinal clone frequency behavior from static. While it is common to mix on the location or scale parameter of a family of distributions, the model instead mixes on the parameterization itself, the dynamic component allowing for a variable, Gamma-distributed Poisson mean parameter over longitudinal follow-up, while the static component mean is time invariant. Leveraging conjugacy, one can integrate out the mean parameter for the dynamic and static components to yield distinct posterior predictive distributions whose expressions are a product of negative binomials and a single negative multinomial, respectively, each modified according to an offset for receptor read count normalization. An EM-algorithm is developed to estimate hyperparameters and component membership, and validity of the approach is demonstrated in simulation. The model identifies a statistically significant and clinically relevant increase in TCR clonal dynamism among metastasis-directed radiation therapy in a cohort of prostate cancer patients. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04555</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04555</id><created>2025-02-06</created><authors><author><keyname>Sparacino</keyname><forenames>Laura</forenames></author><author><keyname>Mijatovic</keyname><forenames>Gorana</forenames></author><author><keyname>Antonacci</keyname><forenames>Yuri</forenames></author><author><keyname>Ricci</keyname><forenames>Leonardo</forenames></author><author><keyname>Marinazzo</keyname><forenames>Daniele</forenames></author><author><keyname>Stramaglia</keyname><forenames>Sebastiano</forenames></author><author><keyname>Faes</keyname><forenames>Luca</forenames></author></authors><title>Decomposing Multivariate Information Rates in Networks of Random   Processes</title><categories>stat.ME cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The Partial Information Decomposition (PID) framework has emerged as a powerful tool for analyzing high-order interdependencies in complex network systems. However, its application to dynamic processes remains challenging due to the implicit assumption of memorylessness, which often falls in real-world scenarios. In this work, we introduce the framework of Partial Information Rate Decomposition (PIRD) that extends PID to random processes with temporal correlations. By leveraging mutual information rate (MIR) instead of mutual information (MI), our approach decomposes the dynamic information shared by multivariate random processes into unique, redundant, and synergistic contributions obtained aggregating information rate atoms in a principled manner. To solve PIRD, we define a pointwise redundancy rate function based on the minimum MI principle applied locally in the frequency-domain representation of the processes. The framework is validated in benchmark simulations of Gaussian systems, demonstrating its advantages over traditional PID in capturing temporal correlations and showing how the spectral representation may reveal scale-specific higher-order interaction that are obscured in the time domain. Furthermore, we apply PIRD to a physiological network comprising cerebrovascular and cardiovascular variables, revealing frequency-dependent redundant information exchange during a protocol of postural stress. Our results highlight the necessity of accounting for the full temporal statistical structure and spectral content of vector random processes to meaningfully perform information decomposition in network systems with dynamic behavior such as those typically encountered in neuroscience and physiology. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04574</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04574</id><created>2025-02-06</created><authors><author><keyname>Gong</keyname><forenames>ZhuQing</forenames></author><author><keyname>Zuo</keyname><forenames>XiNian</forenames></author></authors><title>Dark Brain Energy: Toward an Integrative Model of Spontaneous Slow   Oscillations</title><categories>q-bio.NC cs.IT math.IT stat.AP</categories><comments>38 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Neural oscillations facilitate the functioning of the human brain in spatial and temporal dimensions at various frequencies. These oscillations feature a universal frequency architecture that is governed by brain anatomy, ensuring frequency specificity remains invariant across different measurement techniques. Initial magnetic resonance imaging (MRI) methodology constrained functional MRI (fMRI) investigations to a singular frequency range, thereby neglecting the frequency characteristics inherent in blood oxygen level-dependent oscillations. With advancements in MRI technology, it has become feasible to decode intricate brain activities via multi-band frequency analysis (MBFA). During the past decade, the utilization of MBFA in fMRI studies has surged, unveiling frequency-dependent characteristics of spontaneous slow oscillations (SSOs) believed to base dark energy in the brain. There remains a dearth of conclusive insights and hypotheses pertaining to the properties and functionalities of SSOs in distinct bands. We surveyed the SSO MBFA studies during the past 15 years to delineate the attributes of SSOs and enlighten their correlated functions. We further proposed a model to elucidate the hierarchical organization of multi-band SSOs by integrating their function, aimed at bridging theoretical gaps and guiding future MBFA research endeavors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04575</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04575</id><created>2025-02-06</created><authors><author><keyname>Guo</keyname><forenames>Wei</forenames></author><author><keyname>Tao</keyname><forenames>Molei</forenames></author><author><keyname>Chen</keyname><forenames>Yongxin</forenames></author></authors><title>Complexity Analysis of Normalizing Constant Estimation: from Jarzynski   Equality to Annealed Importance Sampling and beyond</title><categories>stat.ML cs.LG cs.NA math.NA physics.comp-ph stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given an unnormalized probability density $\pi\propto\mathrm{e}^{-V}$, estimating its normalizing constant $Z=\int_{\mathbb{R}^d}\mathrm{e}^{-V(x)}\mathrm{d}x$ or free energy $F=-\log Z$ is a crucial problem in Bayesian statistics, statistical mechanics, and machine learning. It is challenging especially in high dimensions or when $\pi$ is multimodal. To mitigate the high variance of conventional importance sampling estimators, annealing-based methods such as Jarzynski equality and annealed importance sampling are commonly adopted, yet their quantitative complexity guarantees remain largely unexplored. We take a first step toward a non-asymptotic analysis of annealed importance sampling. In particular, we derive an oracle complexity of $\widetilde{O}\left(\frac{d\beta^2{\mathcal{A}}^2}{\varepsilon^4}\right)$ for estimating $Z$ within $\varepsilon$ relative error with high probability, where $\beta$ is the smoothness of $V$ and $\mathcal{A}$ denotes the action of a curve of probability measures interpolating $\pi$ and a tractable reference distribution. Our analysis, leveraging Girsanov theorem and optimal transport, does not explicitly require isoperimetric assumptions on the target distribution. Finally, to tackle the large action of the widely used geometric interpolation of probability distributions, we propose a new normalizing constant estimation algorithm based on reverse diffusion samplers and establish a framework for analyzing its complexity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04591</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04591</id><created>2025-02-06</created><authors><author><keyname>Deidda</keyname><forenames>Piero</forenames></author><author><keyname>Zhang</keyname><forenames>Kaicheng</forenames></author><author><keyname>Higham</keyname><forenames>Desmond</forenames></author><author><keyname>Tudisco</keyname><forenames>Francesco</forenames></author></authors><title>Rethinking Oversmoothing in Graph Neural Networks: A Rank-Based   Perspective</title><categories>cs.LG cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Oversmoothing is a fundamental challenge in graph neural networks (GNNs): as the number of layers increases, node embeddings become increasingly similar, and model performance drops sharply. Traditionally, oversmoothing has been quantified using metrics that measure the similarity of neighbouring node features, such as the Dirichlet energy. While these metrics are related to oversmoothing, we argue they have critical limitations and fail to reliably capture oversmoothing in realistic scenarios. For instance, they provide meaningful insights only for very deep networks and under somewhat strict conditions on the norm of network weights and feature representations. As an alternative, we propose measuring oversmoothing by examining the numerical or effective rank of the feature representations. We provide theoretical support for this approach, demonstrating that the numerical rank of feature representations converges to one for a broad family of nonlinear activation functions under the assumption of nonnegative trained weights. To the best of our knowledge, this is the first result that proves the occurrence of oversmoothing without assumptions on the boundedness of the weight matrices. Along with the theoretical findings, we provide extensive numerical evaluation across diverse graph architectures. Our results show that rank-based metrics consistently capture oversmoothing, whereas energy-based metrics often fail. Notably, we reveal that a significant drop in the rank aligns closely with performance degradation, even in scenarios where energy metrics remain unchanged. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04593</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04593</id><created>2025-02-06</created><authors><author><keyname>Rezaei</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Dieng</keyname><forenames>Adji Bousso</forenames></author></authors><title>The $\alpha$-Alternator: Dynamic Adaptation To Varying Noise Levels In   Sequences Using The Vendi Score For Improved Robustness and Performance</title><categories>cs.LG cs.AI cs.NE stat.ML</categories><comments>The codebase will be made available upon publication. This paper is   dedicated to Patrice Lumumba</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Current state-of-the-art dynamical models, such as Mamba, assume the same level of noisiness for all elements of a given sequence, which limits their performance on noisy temporal data. In this paper, we introduce the $\alpha$-Alternator, a novel generative model for time-dependent data that dynamically adapts to the complexity introduced by varying noise levels in sequences. The $\alpha$-Alternator leverages the Vendi Score (VS), a flexible similarity-based diversity metric, to adjust, at each time step $t$, the influence of the sequence element at time $t$ and the latent representation of the dynamics up to that time step on the predicted future dynamics. This influence is captured by a parameter that is learned and shared across all sequences in a given dataset. The sign of this parameter determines the direction of influence. A negative value indicates a noisy dataset, where a sequence element that increases the VS is considered noisy, and the model relies more on the latent history when processing that element. Conversely, when the parameter is positive, a sequence element that increases the VS is considered informative, and the $\alpha$-Alternator relies more on this new input than on the latent history when updating its predicted latent dynamics. The $\alpha$-Alternator is trained using a combination of observation masking and Alternator loss minimization. Masking simulates varying noise levels in sequences, enabling the model to be more robust to these fluctuations and improving its performance in trajectory prediction, imputation, and forecasting. Our experimental results demonstrate that the $\alpha$-Alternator outperforms both Alternators and state-of-the-art state-space models across neural decoding and time-series forecasting benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04654</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04654</id><created>2025-02-06</created><authors><author><keyname>Lim</keyname><forenames>Keunwoo</forenames></author><author><keyname>Ye</keyname><forenames>Ting</forenames></author><author><keyname>Han</keyname><forenames>Fang</forenames></author></authors><title>A sliced Wasserstein and diffusion approach to random coefficient models</title><categories>math.ST econ.EM stat.TH</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new minimum-distance estimator for linear random coefficient models. This estimator integrates the recently advanced sliced Wasserstein distance with the nearest neighbor methods, both of which enhance computational efficiency. We demonstrate that the proposed method is consistent in approximating the true distribution. Additionally, our formulation encourages a diffusion process-based algorithm, which holds independent interest and potential for broader applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04673</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04673</id><created>2025-02-07</created><authors><author><keyname>Neopane</keyname><forenames>Ojash</forenames></author><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author></authors><title>Optimistic Algorithms for Adaptive Estimation of the Average Treatment   Effect</title><categories>stat.ML cs.LG stat.ME</categories><comments>15 pages, 2 Figures</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Estimation and inference for the Average Treatment Effect (ATE) is a cornerstone of causal inference and often serves as the foundation for developing procedures for more complicated settings. Although traditionally analyzed in a batch setting, recent advances in martingale theory have paved the way for adaptive methods that can enhance the power of downstream inference. Despite these advances, progress in understanding and developing adaptive algorithms remains in its early stages. Existing work either focus on asymptotic analyses that overlook exploration-exploitation tradeoffs relevant in finite-sample regimes or rely on simpler but suboptimal estimators. In this work, we address these limitations by studying adaptive sampling procedures that take advantage of the asymptotically optimal Augmented Inverse Probability Weighting (AIPW) estimator. Our analysis uncovers challenges obscured by asymptotic approaches and introduces a novel algorithmic design principle reminiscent of optimism in multiarmed bandits. This principled approach enables our algorithm to achieve significant theoretical and empirical gains compared to prior methods. Our findings mark a step forward in advancing adaptive causal inference methods in theory and practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04681</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04681</id><created>2025-02-07</created><authors><author><keyname>Louit</keyname><forenames>Sydney</forenames></author><author><keyname>Clark</keyname><forenames>Evan</forenames></author><author><keyname>Gelbard</keyname><forenames>Alexander</forenames></author><author><keyname>Vivek</keyname><forenames>Niketna</forenames></author><author><keyname>Yan</keyname><forenames>Jun</forenames></author><author><keyname>Zhang</keyname><forenames>Panpan</forenames></author></authors><title>CALF-SBM: A Covariate-Assisted Latent Factor Stochastic Block Model</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel network generative model extended from the standard stochastic block model by concurrently utilizing observed node-level information and accounting for network-enabled nodal heterogeneity. The proposed model is so so-called covariate-assisted latent factor stochastic block model (CALF-SBM). The inference for the proposed model is done in a fully Bayesian framework. The primary application of CALF-SBM in the present research is focused on community detection, where a model-selection-based approach is employed to estimate the number of communities which is practically assumed unknown. To assess the performance of CALF-SBM, an extensive simulation study is carried out, including comparisons with multiple classical and modern network clustering algorithms. Lastly, the paper presents two real data applications, respectively based on an extremely new network data demonstrating collaborative relationships of otolaryngologists in the United States and a traditional aviation network data containing information about direct flights between airports in the United States and Canada. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04685</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04685</id><created>2025-02-07</created><authors><author><keyname>Zhang</keyname><forenames>Likun</forenames></author><author><keyname>Bhaganagar</keyname><forenames>Kiran</forenames></author><author><keyname>Wikle</keyname><forenames>Christopher K.</forenames></author></authors><title>Capturing Extreme Events in Turbulence using an Extreme Variational   Autoencoder (xVAE)</title><categories>physics.flu-dyn stat.AP stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Turbulent flow fields are characterized by extreme events that are statistically intermittent and carry a significant amount of energy and physical importance. To emulate these flows, we introduce the extreme variational Autoencoder (xVAE), which embeds a max-infinitely divisible process with heavy-tailed distributions into a standard VAE framework, enabling accurate modeling of extreme events. xVAEs are neural network models that reduce system dimensionality by learning non-linear latent representations of data. We demonstrate the effectiveness of xVAE in large-eddy simulation data of wildland fire plumes, where intense heat release and complex plume-atmosphere interactions generate extreme turbulence. Comparisons with the commonly used Proper Orthogonal Decomposition (POD) modes show that xVAE is more robust in capturing extreme values and provides a powerful uncertainty quantification framework using variational Bayes. Additionally, xVAE enables analysis of the so-called copulas of fields to assess risks associated with rare events while rigorously accounting for uncertainty, such as simultaneous exceedances of high thresholds across multiple locations. The proposed approach provides a new direction for studying realistic turbulent flows, such as high-speed aerodynamics, space propulsion, and atmospheric and oceanic systems that are characterized by extreme events. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04699</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04699</id><created>2025-02-07</created><authors><author><keyname>Lan</keyname><forenames>Hui</forenames></author><author><keyname>Chang</keyname><forenames>Haoge</forenames></author><author><keyname>Dillon</keyname><forenames>Eleanor</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author></authors><title>A Meta-learner for Heterogeneous Effects in Difference-in-Differences</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We address the problem of estimating heterogeneous treatment effects in panel data, adopting the popular Difference-in-Differences (DiD) framework under the conditional parallel trends assumption. We propose a novel doubly robust meta-learner for the Conditional Average Treatment Effect on the Treated (CATT), reducing the estimation to a convex risk minimization problem involving a set of auxiliary models. Our framework allows for the flexible estimation of the CATT, when conditioning on any subset of variables of interest using generic machine learning. Leveraging Neyman orthogonality, our proposed approach is robust to estimation errors in the auxiliary models. As a generalization to our main result, we develop a meta-learning approach for the estimation of general conditional functionals under covariate shift. We also provide an extension to the instrumented DiD setting with non-compliance. Empirical results demonstrate the superiority of our approach over existing baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04709</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04709</id><created>2025-02-07</created><authors><author><keyname>Miftachov</keyname><forenames>Ratmir</forenames></author><author><keyname>Reiß</keyname><forenames>Markus</forenames></author></authors><title>Early Stopping for Regression Trees</title><categories>math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We develop early stopping rules for growing regression tree estimators. The fully data-driven stopping rule is based on monitoring the global residual norm. The best-first search and the breadth-first search algorithms together with linear interpolation give rise to generalized projection or regularization flows. A general theory of early stopping is established. Oracle inequalities for the early-stopped regression tree are derived without any smoothness assumption on the regression function, assuming the original CART splitting rule, yet with a much broader scope. The remainder terms are of smaller order than the best achievable rates for Lipschitz functions in dimension $d\ge 2$. In real and synthetic data the early stopping regression tree estimators attain the statistical performance of cost-complexity pruning while significantly reducing computational costs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04723</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04723</id><created>2025-02-07</created><authors><author><keyname>Lyu</keyname><forenames>Ziyang</forenames></author><author><keyname>Sisson</keyname><forenames>S. A.</forenames></author><author><keyname>Welsh</keyname><forenames>A. H.</forenames></author></authors><title>Asymptotics for EBLUPs within crossed mixed effect models</title><categories>stat.ME</categories><comments>33 pages</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In this article, we derive the joint asymptotic distribution of empirical best linear unbiased predictors (EBLUPs) for individual and cell-level random effects in a crossed mixed effect model. Under mild conditions (which include moment conditions instead of normality for the random effects and model errors), we demonstrate that as the sizes of rows, columns, and, when we include interactions, cells simultaneously increase to infinity, the distribution of the differences between the EBLUPs and the random effects satisfy central limit theorems. These central limit theorems mean the EBLUPs asymptotically follow the convolution of the true random effect distribution and a normal distribution. Moreover, our results enable simple asymptotic approximations and estimators for the mean squared error (MSE) of the EBLUPs, which in turn facilitates the construction of asymptotic prediction intervals for the unobserved random effects. We show in simulations that our simple estimator of the MSE of the EBLUPs works very well in finite samples. Finally, we illustrate the use of the asymptotic prediction intervals with an analysis of movie rating data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04730</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04730</id><created>2025-02-07</created><authors><author><keyname>Xie</keyname><forenames>Tianyu</forenames></author><author><keyname>Richman</keyname><forenames>Harry</forenames></author><author><keyname>Gao</keyname><forenames>Jiansi</forenames></author><author><keyname>Matsen</keyname><forenames>Frederick A.</forenames><suffix>IV</suffix></author><author><keyname>Zhang</keyname><forenames>Cheng</forenames></author></authors><title>PhyloVAE: Unsupervised Learning of Phylogenetic Trees via Variational   Autoencoders</title><categories>stat.ML cs.LG q-bio.PE</categories><comments>ICLR 2025. 22 pages, 14 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Learning informative representations of phylogenetic tree structures is essential for analyzing evolutionary relationships. Classical distance-based methods have been widely used to project phylogenetic trees into Euclidean space, but they are often sensitive to the choice of distance metric and may lack sufficient resolution. In this paper, we introduce phylogenetic variational autoencoders (PhyloVAEs), an unsupervised learning framework designed for representation learning and generative modeling of tree topologies. Leveraging an efficient encoding mechanism inspired by autoregressive tree topology generation, we develop a deep latent-variable generative model that facilitates fast, parallelized topology generation. PhyloVAE combines this generative model with a collaborative inference model based on learnable topological features, allowing for high-resolution representations of phylogenetic tree samples. Extensive experiments demonstrate PhyloVAE's robust representation learning capabilities and fast generation of phylogenetic tree topologies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04750</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04750</id><created>2025-02-07</created><authors><author><keyname>Bui</keyname><forenames>Thang D.</forenames></author><author><keyname>Ashman</keyname><forenames>Matthew</forenames></author><author><keyname>Turner</keyname><forenames>Richard E.</forenames></author></authors><title>Tighter sparse variational Gaussian processes</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sparse variational Gaussian process (GP) approximations based on inducing points have become the de facto standard for scaling GPs to large datasets, owing to their theoretical elegance, computational efficiency, and ease of implementation. This paper introduces a provably tighter variational approximation by relaxing the standard assumption that the conditional approximate posterior given the inducing points must match that in the prior. The key innovation is to modify the conditional posterior to have smaller variances than that of the prior at the training points. We derive the collapsed bound for the regression case, describe how to use the proposed approximation in large data settings, and discuss its application to handle orthogonally structured inducing points and GP latent variable models. Extensive experiments on regression benchmarks, classification, and latent variable models demonstrate that the proposed approximation consistently matches or outperforms standard sparse variational GPs while maintaining the same computational cost. An implementation will be made available in all popular GP packages. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04793</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04793</id><created>2025-02-07</created><authors><author><keyname>Jeunen</keyname><forenames>Olivier</forenames></author></authors><title>$t$-Testing the Waters: Empirically Validating Assumptions for Reliable   A/B-Testing</title><categories>stat.ME cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A/B-tests are a cornerstone of experimental design on the web, with wide-ranging applications and use-cases. The statistical $t$-test comparing differences in means is the most commonly used method for assessing treatment effects, often justified through the Central Limit Theorem (CLT). The CLT ascertains that, as the sample size grows, the sampling distribution of the Average Treatment Effect converges to normality, making the $t$-test valid for sufficiently large sample sizes. When outcome measures are skewed or non-normal, quantifying what "sufficiently large" entails is not straightforward.   To ensure that confidence intervals maintain proper coverage and that $p$-values accurately reflect the false positive rate, it is critical to validate this normality assumption. We propose a practical method to test this, by analysing repeatedly resampled A/A-tests. When the normality assumption holds, the resulting $p$-value distribution should be uniform, and this property can be tested using the Kolmogorov-Smirnov test. This provides an efficient and effective way to empirically assess whether the $t$-test's assumptions are met, and the A/B-test is valid. We demonstrate our methodology and highlight how it helps to identify scenarios prone to inflated Type-I errors. Our approach provides a practical framework to ensure and improve the reliability and robustness of A/B-testing practices. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04807</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04807</id><created>2025-02-07</created><authors><author><keyname>Bashari</keyname><forenames>Meshi</forenames></author><author><keyname>Sesia</keyname><forenames>Matteo</forenames></author><author><keyname>Romano</keyname><forenames>Yaniv</forenames></author></authors><title>Robust Conformal Outlier Detection under Contaminated Reference Data</title><categories>stat.ML cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conformal prediction is a flexible framework for calibrating machine learning predictions, providing distribution-free statistical guarantees. In outlier detection, this calibration relies on a reference set of labeled inlier data to control the type-I error rate. However, obtaining a perfectly labeled inlier reference set is often unrealistic, and a more practical scenario involves access to a contaminated reference set containing a small fraction of outliers. This paper analyzes the impact of such contamination on the validity of conformal methods. We prove that under realistic, non-adversarial settings, calibration on contaminated data yields conservative type-I error control, shedding light on the inherent robustness of conformal methods. This conservativeness, however, typically results in a loss of power. To alleviate this limitation, we propose a novel, active data-cleaning framework that leverages a limited labeling budget and an outlier detection model to selectively annotate data points in the contaminated reference set that are suspected as outliers. By removing only the annotated outliers in this ``suspicious'' subset, we can effectively enhance power while mitigating the risk of inflating the type-I error rate, as supported by our theoretical analysis. Experiments on real datasets validate the conservative behavior of conformal methods under contamination and show that the proposed data-cleaning strategy improves power without sacrificing validity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04824</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04824</id><created>2025-02-07</created><authors><author><keyname>Blake</keyname><forenames>Joshua</forenames></author><author><keyname>Birrell</keyname><forenames>Paul</forenames></author><author><keyname>Walker</keyname><forenames>A. Sarah</forenames></author><author><keyname>Pouwels</keyname><forenames>Koen B.</forenames></author><author><keyname>House</keyname><forenames>Thomas</forenames></author><author><keyname>Tom</keyname><forenames>Brian D. M.</forenames></author><author><keyname>Kypraios</keyname><forenames>Theodore</forenames></author><author><keyname>De Angelis</keyname><forenames>Daniela</forenames></author></authors><title>Estimating the duration of RT-PCR positivity for SARS-CoV-2 from doubly   interval censored data with undetected infections</title><categories>stat.ME stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Monitoring the incidence of new infections during a pandemic is critical for an effective public health response. General population prevalence surveys for SARS-CoV-2 can provide high-quality data to estimate incidence. However, estimation relies on understanding the distribution of the duration that infections remain detectable. This study addresses this need using data from the Coronavirus Infection Survey (CIS), a long-term, longitudinal, general population survey conducted in the UK. Analyzing these data presents unique challenges, such as doubly interval censoring, undetected infections, and false negatives. We propose a Bayesian nonparametric survival analysis approach, estimating a discrete-time distribution of durations and integrating prior information derived from a complementary study. Our methodology is validated through a simulation study, including its resilience to model misspecification, and then applied to the CIS dataset. This results in the first estimate of the full duration distribution in a general population, as well as methodology that could be transferred to new contexts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04832</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04832</id><created>2025-02-07</created><authors><author><keyname>Ballarin</keyname><forenames>Giovanni</forenames></author><author><keyname>Grigoryeva</keyname><forenames>Lyudmila</forenames></author><author><keyname>Ortega</keyname><forenames>Juan-Pablo</forenames></author></authors><title>Memory Capacity of Nonlinear Recurrent Networks: Is it Informative?</title><categories>cs.LG stat.ML</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The total memory capacity (MC) of linear recurrent neural networks (RNNs) has been proven to be equal to the rank of the corresponding Kalman controllability matrix, and it is almost surely maximal for connectivity and input weight matrices drawn from regular distributions. This fact questions the usefulness of this metric in distinguishing the performance of linear RNNs in the processing of stochastic signals. This note shows that the MC of random nonlinear RNNs yields arbitrary values within established upper and lower bounds depending just on the input process scale. This confirms that the existing definition of MC in linear and nonlinear cases has no practical value. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04849</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04849</id><created>2025-02-07</created><authors><author><keyname>Yu</keyname><forenames>Yifeng</forenames></author><author><keyname>Yu</keyname><forenames>Lu</forenames></author></authors><title>Advancing Wasserstein Convergence Analysis of Score-Based Models:   Insights from Discretization and Second-Order Acceleration</title><categories>stat.ML cs.LG math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Score-based diffusion models have emerged as powerful tools in generative modeling, yet their theoretical foundations remain underexplored. In this work, we focus on the Wasserstein convergence analysis of score-based diffusion models. Specifically, we investigate the impact of various discretization schemes, including Euler discretization, exponential integrators, and midpoint randomization methods. Our analysis provides a quantitative comparison of these discrete approximations, emphasizing their influence on convergence behavior. Furthermore, we explore scenarios where Hessian information is available and propose an accelerated sampler based on the local linearization method. We demonstrate that this Hessian-based approach achieves faster convergence rates of order $\widetilde{\mathcal{O}}\left(\frac{1}{\varepsilon}\right)$ significantly improving upon the standard rate $\widetilde{\mathcal{O}}\left(\frac{1}{\varepsilon^2}\right)$ of vanilla diffusion models, where $\varepsilon$ denotes the target accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04867</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04867</id><created>2025-02-07</created><authors><author><keyname>Maclaren</keyname><forenames>Oliver J.</forenames></author><author><keyname>Nicholson</keyname><forenames>Ruanui</forenames></author><author><keyname>Trent</keyname><forenames>Joel A.</forenames></author><author><keyname>Rottenberry</keyname><forenames>Joshua</forenames></author><author><keyname>Simpson</keyname><forenames>Matthew</forenames></author></authors><title>Invariant Image Reparameterisation: A Unified Approach to Structural and   Practical Identifiability and Model Reduction</title><categories>stat.AP</categories><comments>25 pages + supplementary material</comments><msc-class>62F99 (Primary) 65L09, 93B30, 62F12, 34A55 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Both structural and practical parameter non-identifiability present fundamental challenges when using mathematical models to interpret data. This issue is particularly acute in complex, applied areas such as the life sciences or engineering, where determining appropriate model complexity is challenging. While several approaches exist for diagnosing and resolving parameter non-identifiability, including symbolic methods, profile likelihood analysis, and sloppiness analysis, these approaches have distinct limitations and are rarely combined. We present an integrated approach called Invariant Image Reparameterisation (IIR) that incorporates key elements of these methods in a new way. Our approach replaces symbolic computations with numerical calculations at a single reference estimate and an invariance condition that determines when this local calculation holds globally. Parameter combinations determined by this method are naturally ordered by degree of identifiability, and this supports model reduction by replacing a practically non-identified model with a structurally non-identified approximate model. This approximate model can be further parameterised in terms of identified parameters only. By treating parameter combinations determined by our approach as interest parameters within our established likelihood-based Profile-Wise Analysis (PWA) framework, we incorporate uncertainty quantification in terms of likelihood profiles and confidence sets. We provide a Julia library on GitHub (https://github.com/omaclaren/reparam) demonstrating our methodology across a range of mathematical models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04879</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04879</id><created>2025-02-07</created><authors><author><keyname>Gauthier</keyname><forenames>Etienne</forenames></author><author><keyname>Bach</keyname><forenames>Francis</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Statistical Collusion by Collectives on Learning Platforms</title><categories>stat.ML cs.LG</categories><comments>Code available at: https://github.com/GauthierE/statistical-collusion</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As platforms increasingly rely on learning algorithms, collectives may form and seek ways to influence these platforms to align with their own interests. This can be achieved by coordinated submission of altered data. To evaluate the potential impact of such behavior, it is essential to understand the computations that collectives must perform to impact platforms in this way. In particular, collectives need to make a priori assessments of the effect of the collective before taking action, as they may face potential risks when modifying their data. Moreover they need to develop implementable coordination algorithms based on quantities that can be inferred from observed data. We develop a framework that provides a theoretical and algorithmic treatment of these issues and present experimental results in a product evaluation domain. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04889</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04889</id><created>2025-02-07</created><authors><author><keyname>Bao</keyname><forenames>Han</forenames></author><author><keyname>Sakaue</keyname><forenames>Shinsaku</forenames></author><author><keyname>Takezawa</keyname><forenames>Yuki</forenames></author></authors><title>Any-stepsize Gradient Descent for Separable Data under Fenchel--Young   Losses</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The gradient descent (GD) has been one of the most common optimizer in machine learning. In particular, the loss landscape of a neural network is typically sharpened during the initial phase of training, making the training dynamics hover on the edge of stability. This is beyond our standard understanding of GD convergence in the stable regime where arbitrarily chosen stepsize is sufficiently smaller than the edge of stability. Recently, Wu et al. (COLT2024) have showed that GD converges with arbitrary stepsize under linearly separable logistic regression. Although their analysis hinges on the self-bounding property of the logistic loss, which seems to be a cornerstone to establish a modified descent lemma, our pilot study shows that other loss functions without the self-bounding property can make GD converge with arbitrary stepsize. To further understand what property of a loss function matters in GD, we aim to show arbitrary-stepsize GD convergence for a general loss function based on the framework of \emph{Fenchel--Young losses}. We essentially leverage the classical perceptron argument to derive the convergence rate for achieving $\epsilon$-optimal loss, which is possible for a majority of Fenchel--Young losses. Among typical loss functions, the Tsallis entropy achieves the GD convergence rate $T=\Omega(\epsilon^{-1/2})$, and the R{\'e}nyi entropy achieves the far better rate $T=\Omega(\epsilon^{-1/3})$. We argue that these better rate is possible because of \emph{separation margin} of loss functions, instead of the self-bounding property. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04891</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04891</id><created>2025-02-07</created><authors><author><keyname>Rubio-Madrigal</keyname><forenames>Celia</forenames></author><author><keyname>Jamadandi</keyname><forenames>Adarsh</forenames></author><author><keyname>Burkholz</keyname><forenames>Rebekka</forenames></author></authors><title>GNNs Getting ComFy: Community and Feature Similarity Guided Rewiring</title><categories>cs.LG cs.SI stat.ML</categories><comments>Accepted at ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Maximizing the spectral gap through graph rewiring has been proposed to enhance the performance of message-passing graph neural networks (GNNs) by addressing over-squashing. However, as we show, minimizing the spectral gap can also improve generalization. To explain this, we analyze how rewiring can benefit GNNs within the context of stochastic block models. Since spectral gap optimization primarily influences community strength, it improves performance when the community structure aligns with node labels. Building on this insight, we propose three distinct rewiring strategies that explicitly target community structure, node labels, and their alignment: (a) community structure-based rewiring (ComMa), a more computationally efficient alternative to spectral gap optimization that achieves similar goals; (b) feature similarity-based rewiring (FeaSt), which focuses on maximizing global homophily; and (c) a hybrid approach (ComFy), which enhances local feature similarity while preserving community structure to optimize label-community alignment. Extensive experiments confirm the effectiveness of these strategies and support our theoretical insights. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04892</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04892</id><created>2025-02-07</created><authors><author><keyname>Park</keyname><forenames>Joonhyeong</forenames></author><author><keyname>Park</keyname><forenames>Byoungwoo</forenames></author><author><keyname>Bang</keyname><forenames>Chang-Bae</forenames></author><author><keyname>Choi</keyname><forenames>Jungwon</forenames></author><author><keyname>Chung</keyname><forenames>Hyungjin</forenames></author><author><keyname>Kim</keyname><forenames>Byung-Hoon</forenames></author><author><keyname>Lee</keyname><forenames>Juho</forenames></author></authors><title>A Foundational Brain Dynamics Model via Stochastic Optimal Control</title><categories>cs.LG q-bio.NC stat.ML</categories><comments>The first two authors contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a foundational model for brain dynamics that utilizes stochastic optimal control (SOC) and amortized inference. Our method features a continuous-discrete state space model (SSM) that can robustly handle the intricate and noisy nature of fMRI signals. To address computational limitations, we implement an approximation strategy grounded in the SOC framework. Additionally, we present a simulation-free latent dynamics approach that employs locally linear approximations, facilitating efficient and scalable inference. For effective representation learning, we derive an Evidence Lower Bound (ELBO) from the SOC formulation, which integrates smoothly with recent advancements in self-supervised learning (SSL), thereby promoting robust and transferable representations. Pre-trained on extensive datasets such as the UKB, our model attains state-of-the-art results across a variety of downstream tasks, including demographic prediction, trait analysis, disease diagnosis, and prognosis. Moreover, evaluating on external datasets such as HCP-A, ABIDE, and ADHD200 further validates its superior abilities and resilience across different demographic and clinical distributions. Our foundational model provides a scalable and efficient approach for deciphering brain dynamics, opening up numerous applications in neuroscience. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04907</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04907</id><created>2025-02-07</created><authors><author><keyname>Gachon</keyname><forenames>Erell</forenames></author><author><keyname>Bigot</keyname><forenames>Jérémie</forenames></author><author><keyname>Cazelles</keyname><forenames>Elsa</forenames></author></authors><title>Scalable and consistent embedding of probability measures into Hilbert   spaces via measure quantization</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper is focused on statistical learning from data that come as probability measures. In this setting, popular approaches consist in embedding such data into a Hilbert space with either Linearized Optimal Transport or Kernel Mean Embedding. However, the cost of computing such embeddings prohibits their direct use in large-scale settings. We study two methods based on measure quantization for approximating input probability measures with discrete measures of small-support size. The first one is based on optimal quantization of each input measure, while the second one relies on mean-measure quantization. We study the consistency of such approximations, and its implication for scalable embeddings of probability measures into a Hilbert space at a low computational cost. We finally illustrate our findings with various numerical experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04938</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04938</id><created>2025-02-07</created><authors><author><keyname>Gardini</keyname><forenames>Aldo</forenames></author><author><keyname>Greco</keyname><forenames>Fedele</forenames></author><author><keyname>Trivisano</keyname><forenames>Carlo</forenames></author></authors><title>A note on auxiliary mixture sampling for Bayesian Poisson models</title><categories>stat.ME stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bayesian hierarchical Poisson models are an essential tool for analyzing count data. However, designing efficient algorithms to sample from the posterior distribution of the target parameters remains a challenging task for this class of models. Auxiliary mixture sampling algorithms have been proposed to address this issue. They involve two steps of data augmentations: the first leverages the theory of Poisson processes, and the second approximates the residual distribution of the resulting model through a mixture of Gaussian distributions. In this way, an approximated Gibbs sampler is obtained. In this paper, we focus on the accuracy of the approximation step, highlighting scenarios where the mixture fails to accurately represent the true underlying distribution, leading to a lack of convergence in the algorithm. We outline key features to monitor, in order to assess if the approximation performs as intended. Building on this, we propose a robust version of the auxiliary mixture sampling algorithm, which can detect approximation failures and incorporate a Metropolis-Hastings step when necessary. Finally, we evaluate the proposed algorithm together with the original mixture sampling algorithms on both simulated and real datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04945</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04945</id><created>2025-02-07</created><authors><author><keyname>Yanhao</keyname><affiliation>Max</affiliation></author><author><keyname>Wei</keyname></author><author><keyname>Jiang</keyname><forenames>Zhenling</forenames></author></authors><title>Estimating Parameters of Structural Models Using Neural Networks</title><categories>econ.EM stat.CO</categories><acm-class>G.3; J.4; I.2</acm-class><journal-ref>Marketing Science 44(1):102-128 (2024)</journal-ref><doi>10.1287/mksc.2022.0360</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an alternative use of machine learning. We train neural nets to provide the parameter estimate of a given (structural) econometric model, for example, discrete choice or consumer search. Training examples consist of datasets generated by the econometric model under a range of parameter values. The neural net takes the moments of a dataset as input and tries to recognize the parameter value underlying that dataset. Besides the point estimate, the neural net can also output statistical accuracy. This neural net estimator (NNE) tends to limited-information Bayesian posterior as the number of training datasets increases. We apply NNE to a consumer search model. It gives more accurate estimates at lighter computational costs than the prevailing approach. NNE is also robust to redundant moment inputs. In general, NNE offers the most benefits in applications where other estimation approaches require very heavy simulation costs. We provide code at: https://nnehome.github.io. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04949</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04949</id><created>2025-02-07</created><authors><author><keyname>Elsemüller</keyname><forenames>Lasse</forenames></author><author><keyname>Pratz</keyname><forenames>Valentin</forenames></author><author><keyname>von Krause</keyname><forenames>Mischa</forenames></author><author><keyname>Voss</keyname><forenames>Andreas</forenames></author><author><keyname>Bürkner</keyname><forenames>Paul-Christian</forenames></author><author><keyname>Radev</keyname><forenames>Stefan T.</forenames></author></authors><title>Does Unsupervised Domain Adaptation Improve the Robustness of Amortized   Bayesian Inference? A Systematic Evaluation</title><categories>stat.ML cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Neural networks are fragile when confronted with data that significantly deviates from their training distribution. This is true in particular for simulation-based inference methods, such as neural amortized Bayesian inference (ABI), where models trained on simulated data are deployed on noisy real-world observations. Recent robust approaches employ unsupervised domain adaptation (UDA) to match the embedding spaces of simulated and observed data. However, the lack of comprehensive evaluations across different domain mismatches raises concerns about the reliability in high-stakes applications. We address this gap by systematically testing UDA approaches across a wide range of misspecification scenarios in both a controlled and a high-dimensional benchmark. We demonstrate that aligning summary spaces between domains effectively mitigates the impact of unmodeled phenomena or noise. However, the same alignment mechanism can lead to failures under prior misspecifications - a critical finding with practical consequences. Our results underscore the need for careful consideration of misspecification types when using UDA techniques to increase the robustness of ABI in practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04970</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04970</id><created>2025-02-07</created><authors><author><keyname>Langbein</keyname><forenames>Sophie Hanna</forenames></author><author><keyname>Koenen</keyname><forenames>Niklas</forenames></author><author><keyname>Wright</keyname><forenames>Marvin N.</forenames></author></authors><title>Gradient-based Explanations for Deep Learning Survival Models</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deep learning survival models often outperform classical methods in time-to-event predictions, particularly in personalized medicine, but their "black box" nature hinders broader adoption. We propose a framework for gradient-based explanation methods tailored to survival neural networks, extending their use beyond regression and classification. We analyze the implications of their theoretical assumptions for time-dependent explanations in the survival setting and propose effective visualizations incorporating the temporal dimension. Experiments on synthetic data show that gradient-based methods capture the magnitude and direction of local and global feature effects, including time dependencies. We introduce GradSHAP(t), a gradient-based counterpart to SurvSHAP(t), which outperforms SurvSHAP(t) and SurvLIME in a computational speed vs. accuracy trade-off. Finally, we apply these methods to medical data with multi-modal inputs, revealing relevant tabular features and visual patterns, as well as their temporal dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04990</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04990</id><created>2025-02-07</created><authors><author><keyname>Pichler</keyname><forenames>Clemens</forenames></author><author><keyname>Jewson</keyname><forenames>Jack</forenames></author><author><keyname>Avalos-Pacheco</keyname><forenames>Alejandra</forenames></author></authors><title>Probabilistic Programming with Sufficient Statistics for faster Bayesian   Computation</title><categories>stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Probabilistic programming methods have revolutionised Bayesian inference, making it easier than ever for practitioners to perform Markov-chain-Monte-Carlo sampling from non-conjugate posterior distributions. Here we focus on Stan, arguably the most used probabilistic programming tool for Bayesian inference (Carpenter et al., 2017), and its interface with R via the brms (Burkner, 2017) and rstanarm (Goodrich et al., 2024) packages. Although easy to implement, these tools can become computationally prohibitive when applied to datasets with many observations or models with numerous parameters. While the use of sufficient statistics is well-established in theory, it has been surprisingly overlooked in state-of-the-art Stan software. We show that when the likelihood can be written in terms of sufficient statistics, considerable computational improvements can be made to current implementations. We demonstrate how this approach provides accurate inference at a fraction of the time than state-of-the-art implementations for Gaussian linear regression models with non-conjugate priors, hierarchical random effects models, and factor analysis models. Our results also show that moderate computational gains can be achieved even in models where the likelihood can only be partially written in terms of sufficient statistics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05021</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05021</id><created>2025-02-07</created><authors><author><keyname>van Heel</keyname><forenames>Simon Donker</forenames></author><author><keyname>Lange</keyname><forenames>Rutger-Jan</forenames></author><author><keyname>van Dijk</keyname><forenames>Dick</forenames></author><author><keyname>van Os</keyname><forenames>Bram</forenames></author></authors><title>Stability and performance guarantees for misspecified multivariate   score-driven filters</title><categories>stat.ME eess.SP stat.ML</categories><comments>71 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the problem of tracking latent time-varying parameter vectors under model misspecification. We analyze implicit and explicit score-driven (ISD and ESD) filters, which update a prediction of the parameters using the gradient of the logarithmic observation density (i.e., the score). In the ESD filter, the score is computed using the predicted parameter values, whereas in the ISD filter, the score is evaluated using the new, updated parameter values. For both filter types, we derive novel sufficient conditions for the exponential stability (i.e., invertibility) of the filtered parameter path and existence of a finite mean squared error (MSE) bound with respect to the pseudo-true parameter path. In addition, we present expressions for finite-sample and asymptotic MSE bounds. Our performance guarantees rely on mild moment conditions on the data-generating process, while our stability result is entirely agnostic about the true process. As a result, our primary conditions depend only on the characteristics of the filter; hence, they are verifiable in practice. Concavity of the postulated log density combined with simple parameter restrictions is sufficient (but not necessary) for ISD-filter stability, whereas ESD-filter stability additionally requires the score to be Lipschitz continuous. Extensive simulation studies validate our theoretical findings and demonstrate the enhanced stability and improved performance of ISD over ESD filters. An empirical application to U.S. Treasury-bill rates confirms the practical relevance of our contribution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05072</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05072</id><created>2025-02-07</created><authors><author><keyname>Barnett</keyname><forenames>Helen</forenames></author><author><keyname>Boix</keyname><forenames>Oliver</forenames></author><author><keyname>Kontos</keyname><forenames>Dimitris</forenames></author><author><keyname>Jaki</keyname><forenames>Thomas</forenames></author></authors><title>Joint TITE-CRM for Dual Agent Dose Finding Studies</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Dual agent dose-finding trials study the effect of a combination of more than one agent, where the objective is to find the Maximum Tolerated Dose Combination (MTC), the combination of doses of the two agents that is associated with a pre-specified risk of being unsafe. In a Phase I/II setting, the objective is to find a dose combination that is both safe and active, the Optimal Biological Dose (OBD), that optimizes a criterion based on both safety and activity. Since Oncology treatments are typically given over multiple cycles, both the safety and activity outcome can be considered as late-onset, potentially occurring in the later cycles of treatment. This work proposes two model-based designs for dual-agent dose finding studies with late-onset activity and late-onset toxicity outcomes, the Joint TITE-POCRM and the Joint TITE-BLRM. Their performance is compared alongside a model-assisted comparator in a comprehensive simulation study motivated by a real trial example, with an extension to consider alternative sized dosing grids. It is found that both model-based methods outperform the model-assisted design. Whilst on average the two model-based designs are comparable, this comparability is not consistent across scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05074</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05074</id><created>2025-02-07</created><authors><author><keyname>Atanasov</keyname><forenames>Alexander</forenames></author><author><keyname>Bordelon</keyname><forenames>Blake</forenames></author><author><keyname>Zavatone-Veth</keyname><forenames>Jacob A.</forenames></author><author><keyname>Paquette</keyname><forenames>Courtney</forenames></author><author><keyname>Pehlevan</keyname><forenames>Cengiz</forenames></author></authors><title>Two-Point Deterministic Equivalence for Stochastic Gradient Dynamics in   Linear Models</title><categories>cond-mat.dis-nn cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We derive a novel deterministic equivalence for the two-point function of a random matrix resolvent. Using this result, we give a unified derivation of the performance of a wide variety of high-dimensional linear models trained with stochastic gradient descent. This includes high-dimensional linear regression, kernel regression, and random feature models. Our results include previously known asymptotics as well as novel ones. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05075</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05075</id><created>2025-02-07</created><authors><author><keyname>Dong</keyname><forenames>Yijun</forenames></author><author><keyname>Li</keyname><forenames>Yicheng</forenames></author><author><keyname>Li</keyname><forenames>Yunai</forenames></author><author><keyname>Lee</keyname><forenames>Jason D.</forenames></author><author><keyname>Lei</keyname><forenames>Qi</forenames></author></authors><title>Discrepancies are Virtue: Weak-to-Strong Generalization through Lens of   Intrinsic Dimension</title><categories>cs.LG cs.NA math.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weak-to-strong (W2S) generalization is a type of finetuning (FT) where a strong (large) student model is trained on pseudo-labels generated by a weak teacher. Surprisingly, W2S FT often outperforms the weak teacher. We seek to understand this phenomenon through the observation that FT often occurs in intrinsically low-dimensional spaces. Leveraging the low intrinsic dimensionality of FT, we analyze W2S in the ridgeless regression setting from a variance reduction perspective. For a strong student - weak teacher pair with sufficiently expressive low-dimensional feature subspaces $\mathcal{V}_s, \mathcal{V}_w$, we provide an exact characterization of the variance that dominates the generalization error of W2S. This unveils a virtue of discrepancy between the strong and weak models in W2S: the variance of the weak teacher is inherited by the strong student in $\mathcal{V}_s \cap \mathcal{V}_w$, while reduced by a factor of $\dim(\mathcal{V}_s)/N$ in the subspace of discrepancy $\mathcal{V}_w \setminus \mathcal{V}_s$ with $N$ pseudo-labels for W2S. Further, our analysis casts light on the sample complexities and the scaling of performance gap recovery in W2S. The analysis is supported with experiments on both synthetic regression problems and real vision tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05094</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05094</id><created>2025-02-07</created><authors><author><keyname>Blanchet</keyname><forenames>Jose</forenames></author><author><keyname>Hamoudi</keyname><forenames>Yassine</forenames></author><author><keyname>Szegedy</keyname><forenames>Mario</forenames></author><author><keyname>Wang</keyname><forenames>Guanyang</forenames></author></authors><title>Non-linear Quantum Monte Carlo</title><categories>quant-ph cs.LG cs.NA math.NA stat.CO stat.ML</categories><comments>30 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The mean of a random variable can be understood as a $\textit{linear}$ functional on the space of probability distributions. Quantum computing is known to provide a quadratic speedup over classical Monte Carlo methods for mean estimation. In this paper, we investigate whether a similar quadratic speedup is achievable for estimating $\textit{non-linear}$ functionals of probability distributions. We propose a quantum-inside-quantum Monte Carlo algorithm that achieves such a speedup for a broad class of non-linear estimation problems, including nested conditional expectations and stochastic optimization. Our algorithm improves upon the direct application of the quantum multilevel Monte Carlo algorithm introduced by An et al.. The existing lower bound indicates that our algorithm is optimal up polylogarithmic factors. A key innovation of our approach is a new sequence of multilevel Monte Carlo approximations specifically designed for quantum computing, which is central to the algorithm's improved performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05102</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05102</id><created>2025-02-07</created><authors><author><keyname>Piancastelli</keyname><forenames>Luiza</forenames></author><author><keyname>Barreto-Souza</keyname><forenames>Wagner</forenames></author></authors><title>Time Series Analysis of Rankings: A GARCH-Type Approach</title><categories>stat.ME stat.CO stat.ML</categories><comments>Paper submitted for publication</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Ranking data are frequently obtained nowadays but there are still scarce methods for treating these data when temporally observed. The present paper contributes to this topic by proposing and developing novel models for handling time series of ranking data. We introduce a class of time-varying ranking models inspired by the Generalized AutoRegressive Conditional Heteroskedasticity (GARCH) models. More specifically, the temporal dynamics are defined by the conditional distribution of the current ranking given the past rankings, which are assumed to follow a Mallows distribution, which implicitly depends on a distance. Then, autoregressive and feedback components are incorporated into the model through the conditional expectation of the associated distances. Theoretical properties of our ranking GARCH models such as stationarity and ergodicity are established. The estimation of parameters is performed via maximum likelihood estimation when data is fully observed. We develop a Monte Carlo Expectation-Maximisation algorithm to deal with cases involving missing data. Monte Carlo simulation studies are presented to study the performance of the proposed estimators under both non-missing and missing data scenarios. A real data application about the weekly ranking of professional tennis players from 2015 to 2019 is presented under our proposed ranking GARCH models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05122</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05122</id><created>2025-02-07</created><authors><author><keyname>Xi</keyname><forenames>Johnny</forenames></author><author><keyname>Dance</keyname><forenames>Hugh</forenames></author><author><keyname>Orbanz</keyname><forenames>Peter</forenames></author><author><keyname>Bloem-Reddy</keyname><forenames>Benjamin</forenames></author></authors><title>Distinguishing Cause from Effect with Causal Velocity Models</title><categories>stat.ML cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bivariate structural causal models (SCM) are often used to infer causal direction by examining their goodness-of-fit under restricted model classes. In this paper, we describe a parametrization of bivariate SCMs in terms of a causal velocity by viewing the cause variable as time in a dynamical system. The velocity implicitly defines counterfactual curves via the solution of initial value problems where the observation specifies the initial condition. Using tools from measure transport, we obtain a unique correspondence between SCMs and the score function of the generated distribution via its causal velocity. Based on this, we derive an objective function that directly regresses the velocity against the score function, the latter of which can be estimated non-parametrically from observational data. We use this to develop a method for bivariate causal discovery that extends beyond known model classes such as additive or location scale noise, and that requires no assumptions on the noise distributions. When the score is estimated well, the objective is also useful for detecting model non-identifiability and misspecification. We present positive results in simulation and benchmark experiments where many existing methods fail, and perform ablation studies to examine the method's sensitivity to accurate score estimation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05127</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05127</id><created>2025-02-07</created><authors><author><keyname>Everink</keyname><forenames>Jasper M.</forenames></author><author><keyname>Amougou</keyname><forenames>Bernardin Tamo</forenames></author><author><keyname>Pereyra</keyname><forenames>Marcelo</forenames></author></authors><title>Self-supervised Conformal Prediction for Uncertainty Quantification in   Imaging Problems</title><categories>cs.CV stat.ME</categories><msc-class>62H35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most image restoration problems are ill-conditioned or ill-posed and hence involve significant uncertainty. Quantifying this uncertainty is crucial for reliably interpreting experimental results, particularly when reconstructed images inform critical decisions and science. However, most existing image restoration methods either fail to quantify uncertainty or provide estimates that are highly inaccurate. Conformal prediction has recently emerged as a flexible framework to equip any estimator with uncertainty quantification capabilities that, by construction, have nearly exact marginal coverage. To achieve this, conformal prediction relies on abundant ground truth data for calibration. However, in image restoration problems, reliable ground truth data is often expensive or not possible to acquire. Also, reliance on ground truth data can introduce large biases in situations of distribution shift between calibration and deployment. This paper seeks to develop a more robust approach to conformal prediction for image restoration problems by proposing a self-supervised conformal prediction method that leverages Stein's Unbiased Risk Estimator (SURE) to self-calibrate itself directly from the observed noisy measurements, bypassing the need for ground truth. The method is suitable for any linear imaging inverse problem that is ill-conditioned, and it is especially powerful when used with modern self-supervised image restoration techniques that can also be trained directly from measurement data. The proposed approach is demonstrated through numerical experiments on image denoising and deblurring, where it delivers results that are remarkably accurate and comparable to those obtained by supervised conformal prediction with ground truth data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05134</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05134</id><created>2025-02-07</created><authors><author><keyname>Kızıldağ</keyname><forenames>Eren C.</forenames></author></authors><title>Information-Theoretic Guarantees for Recovering Low-Rank Tensors from   Symmetric Rank-One Measurements</title><categories>math.ST cs.IT math.IT math.PR stat.ML stat.TH</categories><comments>Algorithmic Learning Theory (ALT), 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the sample complexity of recovering tensors with low symmetric rank from symmetric rank-one measurements. This setting is particularly motivated by the study of higher-order interactions and the analysis of two-layer neural networks with polynomial activations (polynomial networks). Using a covering numbers argument, we analyze the performance of the symmetric rank minimization program and establish near-optimal sample complexity bounds when the underlying distribution is log-concave. Our measurement model involves random symmetric rank-one tensors, which lead to involved probability calculations. To address these challenges, we employ the Carbery-Wright inequality, a powerful tool for studying anti-concentration properties of random polynomials, and leverage orthogonal polynomials. Additionally, we provide a sample complexity lower bound based on Fano's inequality, and discuss broader implications of our results for two-layer polynomial networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05155</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05155</id><created>2025-02-07</created><authors><author><keyname>Tang</keyname><forenames>Shiqin</forenames></author><author><keyname>Yu</keyname><forenames>Shujian</forenames></author><author><keyname>Dong</keyname><forenames>Yining</forenames></author><author><keyname>Qin</keyname><forenames>S. Joe</forenames></author></authors><title>Deep Dynamic Probabilistic Canonical Correlation Analysis</title><categories>cs.LG stat.ML</categories><comments>accepted by ICASSP-25, code is available at   \url{https://github.com/marcusstang/D2PCCA}</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents Deep Dynamic Probabilistic Canonical Correlation Analysis (D2PCCA), a model that integrates deep learning with probabilistic modeling to analyze nonlinear dynamical systems. Building on the probabilistic extensions of Canonical Correlation Analysis (CCA), D2PCCA captures nonlinear latent dynamics and supports enhancements such as KL annealing for improved convergence and normalizing flows for a more flexible posterior approximation. D2PCCA naturally extends to multiple observed variables, making it a versatile tool for encoding prior knowledge about sequential datasets and providing a probabilistic understanding of the system's dynamics. Experimental validation on real financial datasets demonstrates the effectiveness of D2PCCA and its extensions in capturing latent dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05161</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05161</id><created>2025-02-07</created><authors><author><keyname>Antonczak</keyname><forenames>Brittany</forenames></author><author><keyname>Fay</keyname><forenames>Meg</forenames></author><author><keyname>Chawla</keyname><forenames>Aviral</forenames></author><author><keyname>Rowangould</keyname><forenames>Gregory</forenames></author></authors><title>Estimated Roadway Segment Traffic Data by Vehicle Class for the United   States: A Machine Learning Approach</title><categories>stat.AP</categories><comments>14 pages including references, 4 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Highway Performance Monitoring System, managed by the Federal Highway Administration, provides essential data on average annual daily traffic across U.S. roadways, but it has limited representation of medium- and heavy-duty vehicles on non-interstate roads. This gap limits research and policy analysis on the impacts of truck traffic, especially concerning air quality and public health. To address this, we use random forest regression to estimate medium- and heavy-duty vehicle traffic volumes in areas with sparse data. This results in a more comprehensive dataset, which enables the estimation of traffic density at the census block level as a proxy for traffic-related air pollution exposure. Our high-resolution spatial data products, rigorously validated, provide a more accurate representation of truck traffic and its environmental and health impacts. These datasets are valuable for transportation planning, public health research, and policy decisions aimed at mitigating the effects of truck traffic on vulnerable communities exposed to air pollution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:1811.12852</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>1811.12852</id><created>2018-11-30</created><updated>2025-02-06</updated><authors><author><keyname>Burnetas</keyname><forenames>Apostolos N.</forenames></author><author><keyname>Kanavetas</keyname><forenames>Odysseas</forenames></author><author><keyname>Katehakis</keyname><forenames>Michael N.</forenames></author></authors><title>Optimal Data Driven Resource Allocation under Multi-Armed Bandit   Observations</title><categories>stat.ML cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1509.02857</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the first asymptotically optimal strategy for a multi armed bandit (MAB) model under side constraints. The side constraints model situations in which bandit activations are limited by the availability of certain resources that are replenished at a constant rate. The main result involves the derivation of an asymptotic lower bound for the regret of feasible uniformly fast policies and the construction of policies that achieve this lower bound, under pertinent conditions. Further, we provide the explicit form of such policies for the case in which the unknown distributions are Normal with unknown means and known variances, for the case of Normal distributions with unknown means and unknown variances and for the case of arbitrary discrete distributions with finite support. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2202.04912</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2202.04912</id><created>2022-02-10</created><updated>2025-02-06</updated><authors><author><keyname>Qiu</keyname><forenames>Rui</forenames></author><author><keyname>Yu</keyname><forenames>Zhou</forenames></author><author><keyname>Zhu</keyname><forenames>Ruoqing</forenames></author></authors><title>Random Forest Weighted Local Fr\'echet Regression with Random Objects</title><categories>stat.ML cs.LG</categories><comments>This paper has been published in the Journal of Machine Learning   Research</comments><journal-ref>Journal of Machine Learning Research 25 (2024) 1-69</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical analysis is increasingly confronted with complex data from metric spaces. Petersen and M\"uller (2019) established a general paradigm of Fr\'echet regression with complex metric space valued responses and Euclidean predictors. However, the local approach therein involves nonparametric kernel smoothing and suffers from the curse of dimensionality. To address this issue, we in this paper propose a novel random forest weighted local Fr\'echet regression paradigm. The main mechanism of our approach relies on a locally adaptive kernel generated by random forests. Our first method uses these weights as the local average to solve the conditional Fr\'echet mean, while the second method performs local linear Fr\'echet regression, both significantly improving existing Fr\'echet regression methods. Based on the theory of infinite order U-processes and infinite order $M_{m_n}$-estimator, we establish the consistency, rate of convergence, and asymptotic normality for our local constant estimator, which covers the current large sample theory of random forests with Euclidean responses as a special case. Numerical studies show the superiority of our methods with several commonly encountered types of responses such as distribution functions, symmetric positive-definite matrices, and sphere data. The practical merits of our proposals are also demonstrated through the application to New York taxi data and human mortality data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2206.04902</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2206.04902</id><created>2022-06-10</created><updated>2024-11-13</updated><authors><author><keyname>Gruber</keyname><forenames>Luis</forenames></author><author><keyname>Kastner</keyname><forenames>Gregor</forenames></author></authors><title>Forecasting macroeconomic data with Bayesian VARs: Sparse or dense? It   depends!</title><categories>econ.EM stat.AP stat.ME</categories><journal-ref>International Journal of Forecasting (2025)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vector autogressions (VARs) are widely applied when it comes to modeling and forecasting macroeconomic variables. In high dimensions, however, they are prone to overfitting. Bayesian methods, more concretely shrinkage priors, have shown to be successful in improving prediction performance. In the present paper, we introduce the semi-global framework, in which we replace the traditional global shrinkage parameter with group-specific shrinkage parameters. We show how this framework can be applied to various shrinkage priors, such as global-local priors and stochastic search variable selection priors. We demonstrate the virtues of the proposed framework in an extensive simulation study and in an empirical application forecasting data of the US economy. Further, we shed more light on the ongoing ``Illusion of Sparsity'' debate, finding that forecasting performances under sparse/dense priors vary across evaluated economic variables and across time frames. Dynamic model averaging, however, can combine the merits of both worlds. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2211.15353</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2211.15353</id><created>2022-11-25</created><updated>2025-02-07</updated><authors><author><keyname>Letizia</keyname><forenames>Nunzio A.</forenames></author><author><keyname>Tonello</keyname><forenames>Andrea M.</forenames></author></authors><title>Copula Density Neural Estimation</title><categories>cs.LG eess.SP stat.ML</categories><comments>6 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Probability density estimation from observed data constitutes a central task in statistics. Recent advancements in machine learning offer new tools but also pose new challenges. The big data era demands analysis of long-range spatial and long-term temporal dependencies in large collections of raw data, rendering neural networks an attractive solution for density estimation. In this paper, we exploit the concept of copula to explicitly build an estimate of the probability density function associated to any observed data. In particular, we separate univariate marginal distributions from the joint dependence structure in the data, the copula itself, and we model the latter with a neural network-based method referred to as copula density neural estimation (CODINE). Results show that the novel learning approach is capable of modeling complex distributions and it can be applied for mutual information estimation and data generation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2212.09544</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2212.09544</id><created>2022-12-19</created><authors><author><keyname>Gardini</keyname><forenames>Aldo</forenames></author><author><keyname>Greco</keyname><forenames>Fedele</forenames></author><author><keyname>Trivisano</keyname><forenames>Carlo</forenames></author></authors><title>Design and Structure Dependent Priors for Scale Parameters in Latent   Gaussian Models</title><categories>stat.ME math.ST stat.TH</categories><doi>10.1214/24-BA1454</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Many common correlation structures assumed for data can be described through latent Gaussian models. When Bayesian inference is carried out, it is required to set the prior distribution for scale parameters that rules the model components, possibly allowing to incorporate prior information. This task is particularly delicate and many contributions in the literature are devoted to investigating such aspects. We focus on the fact that the scale parameter controls the prior variability of the model component in a complex way since its dispersion is also affected by the correlation structure and the design. To overcome this issue that might confound the prior elicitation step, we propose to let the user specify the marginal prior of a measure of dispersion of the model component, integrating out the scale parameter, the structure and the design. Then, we analytically derive the implied prior for the scale parameter. Results from a simulation study, aimed at showing the behavior of the estimators sampling properties under the proposed prior elicitation strategy, are discussed. Lastly, some real data applications are explored to investigate prior sensitivity and allocation of explained variance among model components. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2304.13077</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2304.13077</id><created>2023-04-25</created><updated>2025-02-07</updated><authors><author><keyname>De Vito</keyname><forenames>Roberta</forenames></author><author><keyname>Avalos-Pacheco</keyname><forenames>Alejandra</forenames></author></authors><title>Multi-study factor regression model: an application in nutritional   epidemiology</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diet is a risk factor for many diseases. In nutritional epidemiology, studying reproducible dietary patterns is critical to reveal important associations with health. However, it is challenging: diverse cultural and ethnic backgrounds may critically impact eating patterns, showing heterogeneity, leading to incorrect dietary patterns and obscuring the components shared across different groups or populations. Moreover, covariate effects generated from observed variables, such as demographics and other confounders, can further bias these dietary patterns. Identifying the shared and group-specific dietary components and covariate effects is essential to drive accurate conclusions. To address these issues, we introduce a new modeling factor regression, the Multi-Study Factor Regression (MSFR) model. The MSFR model analyzes different populations simultaneously, achieving three goals: capturing shared component(s) across populations, identifying group-specific structures, and correcting for covariate effects. We use this novel method to derive common and ethnic-specific dietary patterns in a multi-center epidemiological study in Hispanic/Latinos community. Our model improves the accuracy of common and group dietary signals and yields better prediction than other techniques, revealing significant associations with health. In summary, we provide a tool to integrate different groups, giving accurate dietary signals crucial to inform public health policy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2305.13998</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2305.13998</id><created>2023-05-23</created><updated>2024-01-23</updated><authors><author><keyname>Saves</keyname><forenames>Paul</forenames></author><author><keyname>Lafage</keyname><forenames>Remi</forenames></author><author><keyname>Bartoli</keyname><forenames>Nathalie</forenames></author><author><keyname>Diouane</keyname><forenames>Youssef</forenames></author><author><keyname>Bussemaker</keyname><forenames>Jasper</forenames></author><author><keyname>Lefebvre</keyname><forenames>Thierry</forenames></author><author><keyname>Hwang</keyname><forenames>John T.</forenames></author><author><keyname>Morlier</keyname><forenames>Joseph</forenames></author><author><keyname>Martins</keyname><forenames>Joaquim R. R. A.</forenames></author></authors><title>SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and   Mixed Variables Gaussian Processes</title><categories>cs.LG cs.MS math.OC stat.CO</categories><comments>10.1016/j.advengsoft.2023.103571</comments><journal-ref>Advances in Engineering Software Volume 188, February 2024, 103571</journal-ref><doi>10.1016/j.advengsoft.2023.103571</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multifidelity data. To the best of our knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical and mixed inputs. This open-source software is distributed under the New BSD license. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2306.16033</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2306.16033</id><created>2023-06-28</created><authors><author><keyname>Gardini</keyname><forenames>Aldo</forenames></author></authors><title>Functional and variables selection in extreme value models for regional   flood frequency analysis</title><categories>stat.ME stat.AP</categories><doi>10.1007/s10651-023-00581-8</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The problem of estimating return levels of river discharge, relevant in flood frequency analysis, is tackled by relying on the extreme value theory. The Generalized Extreme Value (GEV) distribution is assumed to model annual maxima values of river discharge registered at multiple gauging stations belonging to the same river basin. The specific features of the data from the Upper Danube basin drive the definition of the proposed statistical model. Firstly, Bayesian P-splines are considered to account for the non-linear effects of station-specific covariates on the GEV parameters. Secondly, the problem of functional and variable selection is addressed by imposing a grouped horseshoe prior on the coefficients, to encourage the shrinkage of non-relevant components to zero. A cross-validation study is organized to compare the proposed modeling solution to other models, showing its potential in reducing the uncertainty of the ungauged predictions without affecting their calibration. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2307.10272</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2307.10272</id><created>2023-07-18</created><updated>2025-02-06</updated><authors><author><keyname>Takeishi</keyname><forenames>Shota</forenames></author></authors><title>A Shrinkage Likelihood Ratio Test for High-Dimensional Subgroup Analysis   with a Logistic-Normal Mixture Model</title><categories>math.ST stat.ME stat.TH</categories><comments>38 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In subgroup analysis, testing the existence of a subgroup with a differential treatment effect serves as protection against spurious subgroup discovery. Despite its importance, this hypothesis testing possesses a complicated nature: parameter characterizing subgroup classification is not identified under the null hypothesis of no subgroup. Due to this irregularity, the existing methods have the following two limitations. First, the asymptotic null distribution of test statistics often takes an intractable form, which necessitates computationally demanding resampling methods to calculate the critical value. Second, the dimension of personal attributes characterizing subgroup membership is not allowed to be of high dimension. To solve these two problems simultaneously, this study develops a shrinkage likelihood ratio test for the existence of a subgroup using a logistic-normal mixture model. The proposed test statistics are built on a modified likelihood function that shrinks possibly high-dimensional unidentified parameters toward zero under the null hypothesis while retaining power under the alternative. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2307.12982</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2307.12982</id><created>2023-07-24</created><updated>2025-02-07</updated><authors><author><keyname>Mukherjee</keyname><forenames>Soumendu Sundar</forenames></author></authors><title>Consistent model selection in the spiked Wigner model via AIC-type   criteria</title><categories>math.ST cs.IT math.IT stat.ME stat.ML stat.TH</categories><comments>25 pages, 2 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the spiked Wigner model \[   X = \sum_{i = 1}^k \lambda_i u_i u_i^\top + \sigma G, \] where $G$ is an $N \times N$ GOE random matrix, and the eigenvalues $\lambda_i$ are all spiked, i.e. above the Baik-Ben Arous-P\'ech\'e (BBP) threshold $\sigma$. We consider AIC-type model selection criteria of the form \[   -2 \, (\text{maximised log-likelihood}) + \gamma \, (\text{number of parameters}) \] for estimating the number $k$ of spikes. For $\gamma &gt; 2$, the above criterion is strongly consistent provided $\lambda_k &gt; \lambda_{\gamma}$, where $\lambda_{\gamma}$ is a threshold strictly above the BBP threshold, whereas for $\gamma &lt; 2$, it almost surely overestimates $k$. Although AIC (which corresponds to $\gamma = 2$) is not strongly consistent, we show that taking $\gamma = 2 + \delta_N$, where $\delta_N \to 0$ and $\delta_N \gg N^{-2/3}$, results in a weakly consistent estimator of $k$. We further show that a soft minimiser of AIC, where one chooses the least complex model whose AIC score is close to the minimum AIC score, is strongly consistent. Based on a spiked (generalised) Wigner representation, we also develop similar model selection criteria for consistently estimating the number of communities in a balanced stochastic block model under some sparsity restrictions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.07399</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.07399</id><created>2023-10-11</created><updated>2024-10-03</updated><authors><author><keyname>Bou-Rabee</keyname><forenames>Nawaf</forenames></author><author><keyname>Kleppe</keyname><forenames>Tore Selland</forenames></author></authors><title>Randomized Runge-Kutta-Nystr\"om Methods for Unadjusted Hamiltonian and   Kinetic Langevin Monte Carlo</title><categories>math.NA cs.NA math.PR stat.CO stat.ME stat.ML</categories><msc-class>60J05 (Primary) 65C05, 65L05, 65P10 (Secondary)</msc-class><journal-ref>Mathematics of Computation 2025</journal-ref><doi>10.1090/mcom/4061</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce $5/2$- and $7/2$-order $L^2$-accurate randomized Runge-Kutta-Nystr\"{o}m methods, tailored for approximating Hamiltonian flows within non-reversible Markov chain Monte Carlo samplers, such as unadjusted Hamiltonian Monte Carlo and unadjusted kinetic Langevin Monte Carlo. We establish quantitative $5/2$-order $L^2$-accuracy upper bounds under gradient and Hessian Lipschitz assumptions on the potential energy function. The numerical experiments demonstrate the superior efficiency of the proposed unadjusted samplers on a variety of well-behaved, high-dimensional target distributions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2311.02655</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2311.02655</id><created>2023-11-05</created><updated>2025-02-06</updated><authors><author><keyname>Horst</keyname><forenames>Ulrich</forenames></author><author><keyname>Xu</keyname><forenames>Wei</forenames></author></authors><title>Second-Order Regular Variation and Second-Order Approximation of Hawkes   Processes</title><categories>math.PR math.FA math.ST stat.TH</categories><comments>40 pages</comments><msc-class>Primary 26A12, 40E05, secondary 60G55, 60K05</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper provides and extends second-order versions of several fundamental theorems on first-order regularly varying functions such as Karamata's theorem/representation and Tauberian's theorem. Our results are used to establish second-order approximations for the mean and variance of Hawkes processes with general kernels. Our approximations provide novel insights into the asymptotic behavior of Hawkes processes. They are also of key importance when establishing functional limit theorems for Hawkes processes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.04711</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.04711</id><created>2024-02-07</created><updated>2024-05-26</updated><authors><author><keyname>Saves</keyname><forenames>Paul</forenames></author></authors><title>High-dimensional multidisciplinary design optimization for aircraft   eco-design / Optimisation multi-disciplinaire en grande dimension pour   l'\'eco-conception avion en avant-projet</title><categories>math.OC cs.MS stat.ML</categories><comments>PhD Thesis, Universit\'e de Toulouse, Toulouse, 2024 on Gaussian   Process kernels for Bayesian optimization in high dimension with mixed and   hierarchical variables at ISAE-SUPAERO. Keywords: Gaussian process, Black-box   optimization, Bayesian inference, Multidisciplinary design optimization,   Mixed hierarchical and categorical inputs, Eco-friendly aircraft design</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The objective of this Philosophiae Doctor (Ph.D) thesis is to propose an efficient approach for optimizing a multidisciplinary black-box model when the optimization problem is constrained and involves a large number of mixed integer design variables (typically 100 variables). The targeted optimization approach, called EGO, is based on a sequential enrichment of an adaptive surrogate model and, in this context, GP surrogate models are one of the most widely used in engineering problems to approximate time-consuming high fidelity models. EGO is a heuristic BO method that performs well in terms of solution quality. However, like any other global optimization method, EGO suffers from the curse of dimensionality, meaning that its performance is satisfactory on lower dimensional problems, but deteriorates as the dimensionality of the optimization search space increases. For realistic aircraft design problems, the typical size of the design variables can even exceed 100 and, thus, trying to solve directly the problems using EGO is ruled out. The latter is especially true when the problems involve both continuous and categorical variables increasing even more the size of the search space. In this Ph.D thesis, effective parameterization tools are investigated, including techniques like partial least squares regression, to significantly reduce the number of design variables. Additionally, Bayesian optimization is adapted to handle discrete variables and high-dimensional spaces in order to reduce the number of evaluations when optimizing innovative aircraft concepts such as the "DRAGON" hybrid airplane to reduce their climate impact. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.13196</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.13196</id><created>2024-03-19</created><updated>2025-02-07</updated><authors><author><keyname>Eskandar</keyname><forenames>Masih</forenames></author><author><keyname>Imtiaz</keyname><forenames>Tooba</forenames></author><author><keyname>Wang</keyname><forenames>Zifeng</forenames></author><author><keyname>Dy</keyname><forenames>Jennifer</forenames></author></authors><title>ADAPT to Robustify Prompt Tuning Vision Transformers</title><categories>cs.LG cs.AI cs.CV stat.ML</categories><comments>Published in Transactions on Machine Learning Research (2025)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The performance of deep models, including Vision Transformers, is known to be vulnerable to adversarial attacks. Many existing defenses against these attacks, such as adversarial training, rely on full-model fine-tuning to induce robustness in the models. These defenses require storing a copy of the entire model, that can have billions of parameters, for each task. At the same time, parameter-efficient prompt tuning is used to adapt large transformer-based models to downstream tasks without the need to save large copies. In this paper, we examine parameter-efficient prompt tuning of Vision Transformers for downstream tasks under the lens of robustness. We show that previous adversarial defense methods, when applied to the prompt tuning paradigm, suffer from gradient obfuscation and are vulnerable to adaptive attacks. We introduce ADAPT, a novel framework for performing adaptive adversarial training in the prompt tuning paradigm. Our method achieves competitive robust accuracy of ~40% w.r.t. SOTA robustness methods using full-model fine-tuning, by tuning only ~1% of the number of parameters. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.13340</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.13340</id><created>2024-03-20</created><updated>2025-02-07</updated><authors><author><keyname>Jiménez-Varón</keyname><forenames>Cristian F.</forenames></author><author><keyname>Sun</keyname><forenames>Ying</forenames></author><author><keyname>Shang</keyname><forenames>Han Lin</forenames></author></authors><title>Forecasting density-valued functional panel data</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce a statistical method for modeling and forecasting functional panel data represented by multiple densities. Density functions are nonnegative and have a constrained integral and thus do not constitute a linear vector space. We implement a center log-ratio transformation to transform densities into unconstrained functions. These functions exhibit cross-sectional correlation and temporal dependence. Via a functional analysis of variance decomposition, we decompose the unconstrained functional panel data into a deterministic trend component and a time-varying residual component. To produce forecasts for the time-varying component, a functional time series forecasting method, based on the estimation of the long-run covariance, is implemented. By combining the forecasts of the time-varying residual component with the deterministic trend component, we obtain $h$-step-ahead forecast curves for multiple populations. Illustrated by age- and sex-specific life-table death counts in the United States, we apply our proposed method to generate forecasts of the life-table death counts for 51 states. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.12120</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.12120</id><created>2024-06-17</created><updated>2025-02-06</updated><authors><author><keyname>Zhao</keyname><forenames>Yulai</forenames></author><author><keyname>Uehara</keyname><forenames>Masatoshi</forenames></author><author><keyname>Scalia</keyname><forenames>Gabriele</forenames></author><author><keyname>Kung</keyname><forenames>Sunyuan</forenames></author><author><keyname>Biancalani</keyname><forenames>Tommaso</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Hajiramezanali</keyname><forenames>Ehsan</forenames></author></authors><title>Adding Conditional Control to Diffusion Models with Reinforcement   Learning</title><categories>cs.LG cs.AI stat.ML</categories><comments>ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diffusion models are powerful generative models that allow for precise control over the characteristics of the generated samples. While these diffusion models trained on large datasets have achieved success, there is often a need to introduce additional controls in downstream fine-tuning processes, treating these powerful models as pre-trained diffusion models. This work presents a novel method based on reinforcement learning (RL) to add such controls using an offline dataset comprising inputs and labels. We formulate this task as an RL problem, with the classifier learned from the offline dataset and the KL divergence against pre-trained models serving as the reward functions. Our method, $\textbf{CTRL}$ ($\textbf{C}$onditioning pre-$\textbf{T}$rained diffusion models with $\textbf{R}$einforcement $\textbf{L}$earning), produces soft-optimal policies that maximize the abovementioned reward functions. We formally demonstrate that our method enables sampling from the conditional distribution with additional controls during inference. Our RL-based approach offers several advantages over existing methods. Compared to classifier-free guidance, it improves sample efficiency and can greatly simplify dataset construction by leveraging conditional independence between the inputs and additional controls. Additionally, unlike classifier guidance, it eliminates the need to train classifiers from intermediate states to additional controls. The code is available at https://github.com/zhaoyl18/CTRL. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.14535</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.14535</id><created>2024-06-20</created><updated>2025-02-06</updated><authors><author><keyname>Deng</keyname><forenames>Shiyuan</forenames></author><author><keyname>Tang</keyname><forenames>He</forenames></author><author><keyname>Bai</keyname><forenames>Shuyang</forenames></author></authors><title>On estimation and order selection for multivariate extremes via   clustering</title><categories>stat.ME math.ST stat.TH</categories><msc-class>62G32 (Primary), 60G70 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the estimation of multivariate extreme models with a discrete spectral measure using spherical clustering techniques. The primary contribution involves devising a method for selecting the order, that is, the number of clusters. The method consistently identifies the true order, i.e., the number of spectral atoms, and enjoys intuitive implementation in practice. Specifically, we introduce an extra penalty term to the well-known simplified average silhouette width, which penalizes small cluster sizes and small dissimilarities between cluster centers. Consequently, we provide a consistent method for determining the order of a max-linear factor model, where a typical information-based approach is not viable. Our second contribution is a large-deviation-type analysis for estimating the discrete spectral measure through clustering methods, which serves as an assessment of the convergence quality of clustering-based estimation for multivariate extremes. Additionally, as a third contribution, we discuss how estimating the discrete measure can lead to parameter estimations of heavy-tailed factor models. We also present simulations and real-data studies that demonstrate order selection and factor model estimation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.06120</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.06120</id><created>2024-07-08</created><updated>2024-11-16</updated><authors><author><keyname>Dong</keyname><forenames>Yijun</forenames></author><author><keyname>Phan</keyname><forenames>Hoang</forenames></author><author><keyname>Pan</keyname><forenames>Xiang</forenames></author><author><keyname>Lei</keyname><forenames>Qi</forenames></author></authors><title>Sketchy Moment Matching: Toward Fast and Provable Data Selection for   Finetuning</title><categories>cs.LG cs.NA math.NA stat.ML</categories><comments>NeurIPS 2024</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit data selection in a modern context of finetuning from a fundamental perspective. Extending the classical wisdom of variance minimization in low dimensions to high-dimensional finetuning, our generalization analysis unveils the importance of additionally reducing bias induced by low-rank approximation. Inspired by the variance-bias tradeoff in high dimensions from the theory, we introduce Sketchy Moment Matching (SkMM), a scalable data selection scheme with two stages. (i) First, the bias is controlled using gradient sketching that explores the finetuning parameter space for an informative low-dimensional subspace $\mathcal{S}$; (ii) then the variance is reduced over $\mathcal{S}$ via moment matching between the original and selected datasets. Theoretically, we show that gradient sketching is fast and provably accurate: selecting $n$ samples by reducing variance over $\mathcal{S}$ preserves the fast-rate generalization $O(\dim(\mathcal{S})/n)$, independent of the parameter dimension. Empirically, we concretize the variance-bias balance via synthetic experiments and demonstrate the effectiveness of SkMM for finetuning in real vision tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.21314</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.21314</id><created>2024-07-30</created><updated>2025-02-07</updated><authors><author><keyname>Li</keyname><forenames>Zhuoyuan</forenames></author><author><keyname>Dong</keyname><forenames>Bin</forenames></author><author><keyname>Zhang</keyname><forenames>Pingwen</forenames></author></authors><title>State-observation augmented diffusion model for nonlinear assimilation   with unknown dynamics</title><categories>cs.LG stat.ML</categories><msc-class>49N45, 60J60, 62F15, 68T20</msc-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Data assimilation has become a key technique for combining physical models with observational data to estimate state variables. However, classical assimilation algorithms often struggle with the high nonlinearity present in both physical and observational models. To address this challenge, a novel generative model, termed the State-Observation Augmented Diffusion (SOAD) model is proposed for data-driven assimilation. The marginal posterior associated with SOAD has been derived and then proved to match the true posterior distribution under mild assumptions, suggesting its theoretical advantages over previous score-based approaches. Experimental results also indicate that SOAD may offer improved performance compared to existing data-driven methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.03733</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.03733</id><created>2024-08-07</created><authors><author><keyname>Maillard</keyname><forenames>Antoine</forenames></author><author><keyname>Troiani</keyname><forenames>Emanuele</forenames></author><author><keyname>Martin</keyname><forenames>Simon</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Zdeborová</keyname><forenames>Lenka</forenames></author></authors><title>Bayes-optimal learning of an extensive-width neural network from   quadratically many samples</title><categories>stat.ML cond-mat.dis-nn cs.IT cs.LG math.IT math.PR</categories><comments>47 pages</comments><journal-ref>Advances in Neural Information Processing Systems 37 (NeurIPS   2024)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning a target function corresponding to a single hidden layer neural network, with a quadratic activation function after the first layer, and random weights. We consider the asymptotic limit where the input dimension and the network width are proportionally large. Recent work [Cui &amp; al '23] established that linear regression provides Bayes-optimal test error to learn such a function when the number of available samples is only linear in the dimension. That work stressed the open challenge of theoretically analyzing the optimal test error in the more interesting regime where the number of samples is quadratic in the dimension. In this paper, we solve this challenge for quadratic activations and derive a closed-form expression for the Bayes-optimal test error. We also provide an algorithm, that we call GAMP-RIE, which combines approximate message passing with rotationally invariant matrix denoising, and that asymptotically achieves the optimal performance. Technically, our result is enabled by establishing a link with recent works on optimal denoising of extensive-rank matrices and on the ellipsoid fitting problem. We further show empirically that, in the absence of noise, randomly-initialized gradient descent seems to sample the space of weights, leading to zero training loss, and averaging over initialization leads to a test error equal to the Bayes-optimal one. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.09004</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.09004</id><created>2024-08-16</created><updated>2025-02-06</updated><authors><author><keyname>Subedi</keyname><forenames>Unique</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Controlling Statistical, Discretization, and Truncation Errors in   Learning Fourier Linear Operators</title><categories>stat.ML cs.LG cs.NA math.NA</categories><comments>Added Experiments</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study learning-theoretic foundations of operator learning, using the linear layer of the Fourier Neural Operator architecture as a model problem. First, we identify three main errors that occur during the learning process: statistical error due to finite sample size, truncation error from finite rank approximation of the operator, and discretization error from handling functional data on a finite grid of domain points. Finally, we analyze a Discrete Fourier Transform (DFT) based least squares estimator, establishing both upper and lower bounds on the aforementioned errors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.09576</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.09576</id><created>2024-08-18</created><updated>2025-02-07</updated><authors><author><keyname>Oubari</keyname><forenames>Fouad</forenames></author><author><keyname>Baha</keyname><forenames>Mohamed El</forenames></author><author><keyname>Meunier</keyname><forenames>Raphael</forenames></author><author><keyname>Décatoire</keyname><forenames>Rodrigue</forenames></author><author><keyname>Mougeot</keyname><forenames>Mathilde</forenames></author></authors><title>A Markov Random Field Multi-Modal Variational AutoEncoder</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Recent advancements in multimodal Variational AutoEncoders (VAEs) have highlighted their potential for modeling complex data from multiple modalities. However, many existing approaches use relatively straightforward aggregating schemes that may not fully capture the complex dynamics present between different modalities. This work introduces a novel multimodal VAE that incorporates a Markov Random Field (MRF) into both the prior and posterior distributions. This integration aims to capture complex intermodal interactions more effectively. Unlike previous models, our approach is specifically designed to model and leverage the intricacies of these relationships, enabling a more faithful representation of multimodal data. Our experiments demonstrate that our model performs competitively on the standard PolyMNIST dataset and shows superior performance in managing complex intermodal dependencies in a specially designed synthetic dataset, intended to test intricate relationships. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.02363</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.02363</id><created>2024-09-03</created><updated>2025-02-06</updated><authors><author><keyname>Maiti</keyname><forenames>Ayan</forenames></author><author><keyname>Michelle</keyname><forenames>Michelle</forenames></author><author><keyname>Yang</keyname><forenames>Haizhao</forenames></author></authors><title>Optimal Neural Network Approximation for High-Dimensional Continuous   Functions</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the authors of Shen Yang Zhang (JMLR, 2022) developed a neural network with width $36d(2d + 1)$ and depth $11$, which utilizes a special activation function called the elementary universal activation function, to achieve the super approximation property for functions in $C([a,b]^d)$. That is, the constructed network only requires a fixed number of neurons (and thus parameters) to approximate a $d$-variate continuous function on a $d$-dimensional hypercube with arbitrary accuracy. More specifically, only $\mathcal{O}(d^2)$ neurons or parameters are used. One natural question is whether we can reduce the number of these neurons or parameters in such a network. By leveraging a variant of the Kolmogorov Superposition Theorem, our analysis shows that there is a neural network generated by the elementary universal activation function with at most $10889d+10887$ unique nonzero parameters such that this super approximation property is attained. Furthermore, we present a family of continuous functions that requires at least width $d$, and thus at least $d$ neurons or parameters, to achieve arbitrary accuracy in its approximation. This suggests that the number of unique nonzero parameters is optimal in the sense that it grows linearly with the input dimension $d$, unlike some approximation methods where parameters may grow exponentially with $d$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.09800</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.09800</id><created>2024-09-15</created><updated>2025-02-06</updated><authors><author><keyname>Calvello</keyname><forenames>Edoardo</forenames></author><author><keyname>Monmarché</keyname><forenames>Pierre</forenames></author><author><keyname>Stuart</keyname><forenames>Andrew M.</forenames></author><author><keyname>Vaes</keyname><forenames>Urbain</forenames></author></authors><title>Accuracy of the Ensemble Kalman Filter in the Near-Linear Setting</title><categories>math.ST cs.NA math.DS math.NA math.OC stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The filtering distribution captures the statistics of the state of a dynamical system from partial and noisy observations. Classical particle filters provably approximate this distribution in quite general settings; however they behave poorly for high dimensional problems, suffering weight collapse. This issue is circumvented by the ensemble Kalman filter which is an equal-weight interacting particle system. However, this finite particle system is only proven to approximate the true filter in the linear Gaussian case. In practice, however, it is applied in much broader settings; as a result, establishing its approximation properties more generally is important. There has been recent progress in the theoretical analysis of the algorithm, establishing stability and error estimates in non-Gaussian settings, but the assumptions on the dynamics and observation models rule out the unbounded vector fields that arise in practice and the analysis applies only to the mean field limit of the ensemble Kalman filter. The present work establishes error bounds between the filtering distribution and the finite particle ensemble Kalman filter when the dynamics and observation vector fields may be unbounded, allowing linear growth. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.11381</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.11381</id><created>2024-09-17</created><updated>2025-02-07</updated><authors><author><keyname>Banerjee</keyname><forenames>Debapratim</forenames></author><author><keyname>Mukherjee</keyname><forenames>Soumendu Sundar</forenames></author><author><keyname>Pal</keyname><forenames>Dipranjan</forenames></author></authors><title>Edge spectra of Gaussian random symmetric matrices with correlated   entries</title><categories>math.PR math-ph math.CO math.MP math.ST stat.TH</categories><comments>27 pages, 2 figures; abstract shortened to meet arXiv requirements</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the largest eigenvalue of a Gaussian random symmetric matrix $X_n$, with zero-mean, unit variance entries satisfying the condition $\sup_{(i, j) \ne (i', j')}|\mathbb{E}[X_{ij} X_{i'j'}]| = O(n^{-(1 + \varepsilon)})$, where $\varepsilon &gt; 0$. It follows from Catalano et al. (2024) that the empirical spectral distribution of $n^{-1/2} X_n$ converges weakly almost surely to the standard semi-circle law. Using a F\"{u}redi-Koml\'{o}s-type high moment analysis, we show that the largest eigenvalue $\lambda_1(n^{-1/2} X_n)$ of $n^{-1/2} X_n$ converges almost surely to $2$. This result is essentially optimal in the sense that one cannot take $\varepsilon = 0$ and still obtain an almost sure limit of $2$. We also derive Gaussian fluctuation results for the largest eigenvalue in the case where the entries have a common non-zero mean. Let $Y_n = X_n + \frac{\lambda}{\sqrt{n}}\mathbf{1} \mathbf{1}^\top$. When $\varepsilon \ge 1$ and $\lambda \gg n^{1/4}$, we show that \[   n^{1/2}\bigg(\lambda_1(n^{-1/2} Y_n) - \lambda - \frac{1}{\lambda}\bigg) \xrightarrow{d} \sqrt{2} Z, \] where $Z$ is a standard Gaussian. On the other hand, when $0 &lt; \varepsilon &lt; 1$, we have $\mathrm{Var}(\frac{1}{n}\sum_{i, j}X_{ij}) = O(n^{1 - \varepsilon})$. Assuming that $\mathrm{Var}(\frac{1}{n}\sum_{i, j} X_{ij}) = \sigma^2 n^{1 - \varepsilon} (1 + o(1))$, if $\lambda \gg n^{\varepsilon/4}$, then we have \[   n^{\varepsilon/2}\bigg(\lambda_1(n^{-1/2} Y_n) - \lambda - \frac{1}{\lambda}\bigg) \xrightarrow{d} \sigma Z. \] While the ranges of $\lambda$ in these fluctuation results are certainly not optimal, a striking aspect is that different scalings are required in the two regimes $0 &lt; \varepsilon &lt; 1$ and $\varepsilon \ge 1$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.19200</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.19200</id><created>2024-09-27</created><updated>2025-02-06</updated><authors><author><keyname>Bai</keyname><forenames>Site</forenames></author><author><keyname>Bullins</keyname><forenames>Brian</forenames></author></authors><title>Faster Acceleration for Steepest Descent</title><categories>math.OC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances (Sherman, 2017; Sidford and Tian, 2018; Cohen et al., 2021) have overcome the fundamental barrier of dimension dependence in the iteration complexity of solving $\ell_\infty$ regression with first-order methods. Yet it remains unclear to what extent such acceleration can be achieved for general $\ell_p$ smooth functions. In this paper, we propose a new accelerated first-order method for convex optimization under non-Euclidean smoothness assumptions. In contrast to standard acceleration techniques, our approach uses primal-dual iterate sequences taken with respect to $\textit{differing}$ norms, which are then coupled using an $\textit{implicitly}$ determined interpolation parameter. For $\ell_p$ norm smooth problems in $d$ dimensions, our method provides an iteration complexity improvement of up to $O(d^{1-\frac{2}{p}})$ in terms of calls to a first-order oracle, thereby allowing us to circumvent long-standing barriers in accelerated non-Euclidean steepest descent. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.03159</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.03159</id><created>2024-10-04</created><updated>2025-02-07</updated><authors><author><keyname>Lu</keyname><forenames>Jiecheng</forenames></author><author><keyname>Han</keyname><forenames>Xu</forenames></author><author><keyname>Sun</keyname><forenames>Yan</forenames></author><author><keyname>Yang</keyname><forenames>Shihao</forenames></author></authors><title>WAVE: Weighted Autoregressive Varing Gate for Time Series Forecasting</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Weighted Autoregressive Varing gatE (WAVE) attention mechanism equipped with both Autoregressive (AR) and Moving-average (MA) components. It can adapt to various attention mechanisms, enhancing and decoupling their ability to capture long-range and local temporal patterns in time series data. In this paper, we first demonstrate that, for the time series forecasting (TSF) task, the previously overlooked decoder-only autoregressive Transformer model can achieve results comparable to the best baselines when appropriate tokenization and training methods are applied. Moreover, inspired by the ARMA model from statistics and recent advances in linear attention, we introduce the full ARMA structure into existing autoregressive attention mechanisms. By using an indirect MA weight generation method, we incorporate the MA term while maintaining the time complexity and parameter size of the underlying efficient attention models. We further explore how indirect parameter generation can produce implicit MA weights that align with the modeling requirements for local temporal impacts. Experimental results show that WAVE attention that incorporates the ARMA structure consistently improves the performance of various AR attentions on TSF tasks, achieving state-of-the-art results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.15361</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.15361</id><created>2024-10-20</created><updated>2025-02-06</updated><authors><author><keyname>Zhou</keyname><forenames>Han</forenames></author><author><keyname>Van Landeghem</keyname><forenames>Jordy</forenames></author><author><keyname>Popordanoska</keyname><forenames>Teodora</forenames></author><author><keyname>Blaschko</keyname><forenames>Matthew B.</forenames></author></authors><title>A Novel Characterization of the Population Area Under the Risk Coverage   Curve (AURC) and Rates of Finite Sample Estimators</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The selective classifier (SC) has been proposed for rank based uncertainty thresholding, which could have applications in safety critical areas such as medical diagnostics, autonomous driving, and the justice system. The Area Under the Risk-Coverage Curve (AURC) has emerged as the foremost evaluation metric for assessing the performance of SC systems. In this work, we present a formal statistical formulation of population AURC, presenting an equivalent expression that can be interpreted as a reweighted risk function. Through Monte Carlo methods, we derive empirical AURC plug-in estimators for finite sample scenarios. The weight estimators associated with these plug-in estimators are shown to be consistent, with low bias and tightly bounded mean squared error (MSE). The plug-in estimators are proven to converge at a rate of $\mathcal{O}(\sqrt{\ln(n)/n})$ demonstrating statistical consistency. We empirically validate the effectiveness of our estimators through experiments across multiple datasets, model architectures, and confidence score functions (CSFs), demonstrating consistency and effectiveness in fine-tuning AURC performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.18959</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.18959</id><created>2024-10-24</created><updated>2025-02-06</updated><authors><author><keyname>Williams</keyname><forenames>Andrew Robert</forenames></author><author><keyname>Ashok</keyname><forenames>Arjun</forenames></author><author><keyname>Marcotte</keyname><forenames>Étienne</forenames></author><author><keyname>Zantedeschi</keyname><forenames>Valentina</forenames></author><author><keyname>Subramanian</keyname><forenames>Jithendaraa</forenames></author><author><keyname>Riachi</keyname><forenames>Roland</forenames></author><author><keyname>Requeima</keyname><forenames>James</forenames></author><author><keyname>Lacoste</keyname><forenames>Alexandre</forenames></author><author><keyname>Rish</keyname><forenames>Irina</forenames></author><author><keyname>Chapados</keyname><forenames>Nicolas</forenames></author><author><keyname>Drouin</keyname><forenames>Alexandre</forenames></author></authors><title>Context is Key: A Benchmark for Forecasting with Essential Textual   Information</title><categories>cs.LG cs.AI stat.ML</categories><comments>Preprint; under review. First two authors contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forecasting is a critical task in decision-making across numerous domains. While historical numerical data provide a start, they fail to convey the complete context for reliable and accurate predictions. Human forecasters frequently rely on additional information, such as background knowledge and constraints, which can efficiently be communicated through natural language. However, in spite of recent progress with LLM-based forecasters, their ability to effectively integrate this textual information remains an open question. To address this, we introduce "Context is Key" (CiK), a time-series forecasting benchmark that pairs numerical data with diverse types of carefully crafted textual context, requiring models to integrate both modalities; crucially, every task in CiK requires understanding textual context to be solved successfully. We evaluate a range of approaches, including statistical models, time series foundation models, and LLM-based forecasters, and propose a simple yet effective LLM prompting method that outperforms all other tested methods on our benchmark. Our experiments highlight the importance of incorporating contextual information, demonstrate surprising performance when using LLM-based forecasting models, and also reveal some of their critical shortcomings. This benchmark aims to advance multimodal forecasting by promoting models that are both accurate and accessible to decision-makers with varied technical expertise. The benchmark can be visualized at https://servicenow.github.io/context-is-key-forecasting/v0/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.19217</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.19217</id><created>2024-10-24</created><authors><author><keyname>Wu</keyname><forenames>Changlong</forenames></author><author><keyname>Grama</keyname><forenames>Ananth</forenames></author><author><keyname>Szpankowski</keyname><forenames>Wojciech</forenames></author></authors><title>No Free Lunch: Fundamental Limits of Learning Non-Hallucinating   Generative Models</title><categories>cs.LG cs.AI stat.ML</categories><journal-ref>International Conference on Learning Representations (ICLR 2025).   URL: https://openreview.net/pdf?id=OwNoTs2r8e</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative models have shown impressive capabilities in synthesizing high-quality outputs across various domains. However, a persistent challenge is the occurrence of "hallucinations", where the model produces outputs that are plausible but invalid. While empirical strategies have been explored to mitigate this issue, a rigorous theoretical understanding remains elusive. In this paper, we develop a theoretical framework to analyze the learnability of non-hallucinating generative models from a learning-theoretic perspective. Our results reveal that non-hallucinating learning is statistically impossible when relying solely on the training dataset, even for a hypothesis class of size two and when the entire training set is truthful. To overcome these limitations, we show that incorporating inductive biases aligned with the actual facts into the learning process is essential. We provide a systematic approach to achieve this by restricting the facts set to a concept class of finite VC-dimension and demonstrate its effectiveness under various learning paradigms. Although our findings are primarily conceptual, they represent a first step towards a principled approach to addressing hallucinations in learning generative models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.19725</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.19725</id><created>2024-10-25</created><updated>2025-02-06</updated><authors><author><keyname>Subedi</keyname><forenames>Unique</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>On the Benefits of Active Data Collection in Operator Learning</title><categories>stat.ML cs.LG</categories><comments>Moved Proofs to the Appendix</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study active data collection strategies for operator learning when the target operator is linear and the input functions are drawn from a mean-zero stochastic process with continuous covariance kernels. With an active data collection strategy, we establish an error convergence rate in terms of the decay rate of the eigenvalues of the covariance kernel. We can achieve arbitrarily fast error convergence rates with sufficiently rapid eigenvalue decay of the covariance kernels. This contrasts with the passive (i.i.d.) data collection strategies, where the convergence rate is never faster than linear decay ($\sim n^{-1}$). In fact, for our setting, we show a \emph{non-vanishing} lower bound for any passive data collection strategy, regardless of the eigenvalues decay rate of the covariance kernel. Overall, our results show the benefit of active data collection strategies in operator learning over their passive counterparts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.20640</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.20640</id><created>2024-10-27</created><updated>2025-02-07</updated><authors><author><keyname>Rivera</keyname><forenames>Eduardo Ochoa</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Near Optimal Pure Exploration in Logistic Bandits</title><categories>stat.ML cs.LG</categories><comments>25 pages, 2 figures. arXiv admin note: text overlap with   arXiv:2006.16073 by other authors</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bandit algorithms have garnered significant attention due to their practical applications in real-world scenarios. However, beyond simple settings such as multi-arm or linear bandits, optimal algorithms remain scarce. Notably, no optimal solution exists for pure exploration problems in the context of generalized linear model (GLM) bandits. In this paper, we narrow this gap and develop the first track-and-stop algorithm for general pure exploration problems under the logistic bandit called logistic track-and-stop (Log-TS). Log-TS is an efficient algorithm that asymptotically matches an approximation for the instance-specific lower bound of the expected sample complexity up to a logarithmic factor. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.03727</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.03727</id><created>2024-12-04</created><updated>2025-02-07</updated><authors><author><keyname>Zhang</keyname><forenames>Zhiheng</forenames></author><author><keyname>Wang</keyname><forenames>Zichen</forenames></author></authors><title>Online Experimental Design With Estimation-Regret Trade-off Under   Network Interference</title><categories>cs.LG math.OC math.ST stat.TH</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network interference has attracted significant attention in the field of causal inference, encapsulating various sociological behaviors where the treatment assigned to one individual within a network may affect the outcomes of others, such as their neighbors. A key challenge in this setting is that standard causal inference methods often assume independent treatment effects among individuals, which may not hold in networked environments. To estimate interference-aware causal effects, a traditional approach is to inherit the independent settings, where practitioners randomly assign experimental participants into different groups and compare their outcomes. While effective in offline settings, this strategy becomes problematic in sequential experiments, where suboptimal decision persists, leading to substantial regret. To address this issue, we introduce a unified interference-aware framework for online experimental design. Compared to existing studies, we extend the definition of arm space by utilizing the statistical concept of exposure mapping, which allows for a more flexible and context-aware representation of treatment effects in networked settings. Crucially, we establish a Pareto-optimal trade-off between estimation accuracy and regret under the network concerning both time period and arm space, which remains superior to baseline models even without network interference. Furthermore, we propose an algorithmic implementation and discuss its generalization across different learning settings and network topology </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.04767</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.04767</id><created>2024-12-05</created><updated>2025-02-06</updated><authors><author><keyname>Tian</keyname><forenames>Bowei</forenames></author><author><keyname>Wang</keyname><forenames>Ziyao</forenames></author><author><keyname>He</keyname><forenames>Shwai</forenames></author><author><keyname>Ye</keyname><forenames>Wanghao</forenames></author><author><keyname>Sun</keyname><forenames>Guoheng</forenames></author><author><keyname>Dai</keyname><forenames>Yucong</forenames></author><author><keyname>Wu</keyname><forenames>Yongkai</forenames></author><author><keyname>Li</keyname><forenames>Ang</forenames></author></authors><title>Towards counterfactual fairness through auxiliary variables</title><categories>cs.LG cs.DS stat.ML</categories><comments>arXiv admin note: text overlap with arXiv:2307.08232 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The challenge of balancing fairness and predictive accuracy in machine learning models, especially when sensitive attributes such as race, gender, or age are considered, has motivated substantial research in recent years. Counterfactual fairness ensures that predictions remain consistent across counterfactual variations of sensitive attributes, which is a crucial concept in addressing societal biases. However, existing counterfactual fairness approaches usually overlook intrinsic information about sensitive features, limiting their ability to achieve fairness while simultaneously maintaining performance. To tackle this challenge, we introduce EXOgenous Causal reasoning (EXOC), a novel causal reasoning framework motivated by exogenous variables. It leverages auxiliary variables to uncover intrinsic properties that give rise to sensitive attributes. Our framework explicitly defines an auxiliary node and a control node that contribute to counterfactual fairness and control the information flow within the model. Our evaluation, conducted on synthetic and real-world datasets, validates EXOC's superiority, showing that it outperforms state-of-the-art approaches in achieving counterfactual fairness. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.08435</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.08435</id><created>2024-12-11</created><updated>2025-02-07</updated><authors><author><keyname>Zhao</keyname><forenames>Lifan</forenames></author><author><keyname>Shen</keyname><forenames>Yanyan</forenames></author></authors><title>Proactive Model Adaptation Against Concept Drift for Online Time Series   Forecasting</title><categories>cs.LG cs.AI cs.CE stat.ML</categories><comments>Accepted by KDD 2025</comments><doi>10.1145/3690624.3709210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time series forecasting always faces the challenge of concept drift, where data distributions evolve over time, leading to a decline in forecast model performance. Existing solutions are based on online learning, which continually organize recent time series observations as new training samples and update model parameters according to the forecasting feedback on recent data. However, they overlook a critical issue: obtaining ground-truth future values of each sample should be delayed until after the forecast horizon. This delay creates a temporal gap between the training samples and the test sample. Our empirical analysis reveals that the gap can introduce concept drift, causing forecast models to adapt to outdated concepts. In this paper, we present Proceed, a novel proactive model adaptation framework for online time series forecasting. Proceed first estimates the concept drift between the recently used training samples and the current test sample. It then employs an adaptation generator to efficiently translate the estimated drift into parameter adjustments, proactively adapting the model to the test sample. To enhance the generalization capability of the framework, Proceed is trained on synthetic diverse concept drifts. Extensive experiments on five real-world datasets across various forecast models demonstrate that Proceed brings more performance improvements than the state-of-the-art online learning methods, significantly facilitating forecast models' resilience against concept drifts. Code is available at https://github.com/SJTU-DMTai/OnlineTSF. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.20471</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.20471</id><created>2024-12-29</created><updated>2025-02-07</updated><authors><author><keyname>Cai</keyname><forenames>Yang</forenames></author><author><keyname>Mitra</keyname><forenames>Siddharth</forenames></author><author><keyname>Wang</keyname><forenames>Xiuyuan</forenames></author><author><keyname>Wibisono</keyname><forenames>Andre</forenames></author></authors><title>On the Convergence of Min-Max Langevin Dynamics and Algorithm</title><categories>cs.GT cs.LG math.OC stat.ML</categories><comments>v2: Revised introduction and presentation of results</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study zero-sum games in the space of probability distributions over the Euclidean space $\mathbb{R}^d$ with entropy regularization, in the setting when the interaction function between the players is smooth and strongly convex-strongly concave. We prove an exponential convergence guarantee for the mean-field min-max Langevin dynamics to compute the equilibrium distribution of the zero-sum game. We also study the finite-particle approximation of the mean-field min-max Langevin dynamics, both in continuous and discrete times. We prove biased convergence guarantees for the continuous-time finite-particle min-max Langevin dynamics to the stationary mean-field equilibrium distribution with an explicit bias term which does not scale with the number of particles. We also prove biased convergence guarantees for the discrete-time finite-particle min-max Langevin algorithm to the stationary mean-field equilibrium distribution with an additional bias term which scales with the step size and the number of particles. This provides an explicit iteration complexity for the average particle along the finite-particle algorithm to approximately compute the equilibrium distribution of the zero-sum game. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.01437</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.01437</id><created>2024-12-23</created><updated>2025-02-06</updated><authors><author><keyname>Murphy</keyname><forenames>Charles</forenames></author><author><keyname>Lizotte</keyname><forenames>Simon</forenames></author><author><keyname>Thibault</keyname><forenames>François</forenames></author><author><keyname>Thibeault</keyname><forenames>Vincent</forenames></author><author><keyname>Desrosiers</keyname><forenames>Patrick</forenames></author><author><keyname>Allard</keyname><forenames>Antoine</forenames></author></authors><title>On the reconstruction limits of complex networks</title><categories>stat.AP cs.IT math.IT physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network reconstruction consists in retrieving the hidden interaction structure of a system from observations. Many reconstruction algorithms have been proposed, although less research has been devoted to describe their theoretical limitations. In this work, we adopt an information-theoretic perspective and define the reconstructability: The fraction of structural information recoverable from data. The reconstructability depends on the true data generating (TDG) model which is shown to set the reconstruction limit: any algorithm can perform, on average, at best like the TDG model. We show that the reconstructability is related to various performance measures, such as the probability of error and the Jaccard similarity. In an empirical context where the TDG model is unknown, we introduce the reconstruction index as an approximation of the reconstructability. We find that performing model selection is crucial for the validity of the reconstruction index as a proxy of the reconstructability of empirical time series and networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.07025</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.07025</id><created>2025-01-12</created><updated>2025-02-07</updated><authors><author><keyname>Zhang</keyname><forenames>Yong</forenames></author><author><keyname>Gyamfi</keyname><forenames>Eric Herrison</forenames></author></authors><title>A Weighted Similarity Metric for Community Detection in Sparse Data</title><categories>stat.ME cs.SI</categories><comments>This paper has been accepted for Workshop of AI for Social Impact at   AAAI 2025</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Many Natural Language Processing (NLP) related applications involves topics and sentiments derived from short documents such as consumer reviews and social media posts. Topics and sentiments of short documents are highly sparse because a short document generally covers a few topics among hundreds of candidates. Imputation of missing data is sometimes hard to justify and also often unpractical in highly sparse data. We developed a method for calculating a weighted similarity for highly sparse data without imputation. This weighted similarity is consist of three components to capture similarities based on both existence and lack of common properties and pattern of missing values. As a case study, we used a community detection algorithm and this weighted similarity to group different shampoo brands based on sparse topic sentiments derived from short consumer reviews. Compared with traditional imputation and similarity measures, the weighted similarity shows better performance in both general community structures and average community qualities. The performance is consistent and robust across metrics and community complexities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10117</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10117</id><created>2025-01-17</created><updated>2025-02-07</updated><authors><author><keyname>Liu</keyname><forenames>Weiguang</forenames></author><author><keyname>de Paula</keyname><forenames>Áureo</forenames></author><author><keyname>Tamer</keyname><forenames>Elie</forenames></author></authors><title>Prediction Sets and Conformal Inference with Censored Outcomes</title><categories>econ.EM stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given data on a scalar random variable $Y$, a prediction set for $Y$ with miscoverage level $\alpha$ is a set of values for $Y$ that contains a randomly drawn $Y$ with probability $1 - \alpha$, where $\alpha \in (0,1)$. Among all prediction sets that satisfy this coverage property, the oracle prediction set is the one with the smallest volume. This paper provides estimation methods of such prediction sets given observed conditioning covariates when $Y$ is \textit{censored} or \textit{measured in intervals}. We first characterise the oracle prediction set under interval censoring and develop a consistent estimator for the shortest prediction {\it interval} that satisfies this coverage property.These consistency results are extended to accommodate cases where the prediction set consists of multiple disjoint intervals. We use conformal inference to construct a prediction set that achieves finite-sample validity under censoring and maintains consistency as sample size increases, using a conformity score function designed for interval data. The procedure accommodates the prediction uncertainty that is irreducible (due to the stochastic nature of outcomes), the modelling uncertainty due to partial identification and also sampling uncertainty that gets reduced as samples get larger. We conduct a set of Monte Carlo simulations and an application to data from the Current Population Survey. The results highlight the robustness and efficiency of the proposed methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10974</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10974</id><created>2025-01-19</created><updated>2025-02-06</updated><authors><author><keyname>Huang</keyname><forenames>Yu-Han</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Sequential Change Detection for Learning in Piecewise Stationary Bandit   Environments</title><categories>cs.IT cs.SY eess.SY math.IT stat.OT</categories><comments>15 pages, 2 figures. arXiv admin note: text overlap with   arXiv:2501.01291</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A finite-horizon variant of the quickest change detection problem is investigated, which is motivated by a change detection problem that arises in piecewise stationary bandits. The goal is to minimize the \emph{latency}, which is smallest threshold such that the probability that the detection delay exceeds the threshold is below a desired low level, while controlling the false alarm probability to a desired low level. When the pre- and post-change distributions are unknown, two tests are proposed as candidate solutions. These tests are shown to attain order optimality in terms of the horizon. Furthermore, the growth in their latencies with respect to the false alarm probability and late detection probability satisfies a property that is desirable in regret analysis for piecewise stationary bandits. Numerical results are provided to validate the theoretical performance results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14974</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14974</id><created>2025-01-24</created><updated>2025-02-06</updated><authors><author><keyname>Deng</keyname><forenames>Fengnan</forenames></author><author><keyname>Vidyashankar</keyname><forenames>Anand N.</forenames></author></authors><title>Private Minimum Hellinger Distance Estimation via Hellinger Distance   Differential Privacy</title><categories>math.ST cs.CR math.PR stat.ME stat.ML stat.TH</categories><msc-class>62F35, 68P27, 62E20, 60E05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective functions based on Hellinger distance yield robust and efficient estimators of model parameters. Motivated by privacy and regulatory requirements encountered in contemporary applications, we derive in this paper \emph{private minimum Hellinger distance estimators}. The estimators satisfy a new privacy constraint, namely, Hellinger differential privacy, while retaining the robustness and efficiency properties. We demonstrate that Hellinger differential privacy shares several features of standard differential privacy while allowing for sharper inference. Additionally, for computational purposes, we also develop Hellinger differentially private gradient descent and Newton-Raphson algorithms. We illustrate the behavior of our estimators in finite samples using numerical experiments and verify that they retain robustness properties under gross-error contamination. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18502</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18502</id><created>2025-01-30</created><updated>2025-02-07</updated><authors><author><keyname>Kumar</keyname><forenames>Ritesh</forenames></author><author><keyname>Vatedka</keyname><forenames>Shashank</forenames></author></authors><title>One-Bit Distributed Mean Estimation with Unknown Variance</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>21 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this work, we study the problem of distributed mean estimation with $1$-bit communication constraints when the variance is unknown. We focus on the specific case where each user has access to one i.i.d. sample drawn from a distribution that belongs to a scale-location family, and is limited to sending just a single bit of information to a central server whose goal is to estimate the mean. We propose non-adaptive and adaptive estimators that are shown to be asymptotically normal. We derive bounds on the asymptotic (in the number of users) Mean Squared Error (MSE) achieved by these estimators. For a class of symmetric log-concave distributions, we derive matching lower bounds for the MSE achieved by adaptive estimators, proving the optimality of our scheme. We show that non-adaptive estimators can be strictly suboptimal by deriving a lower bound on the MSE achieved by any non-adaptive estimator for Gaussian distributions and demonstrating a positive gap between this and the MSE achieved by our adaptive scheme. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00168</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00168</id><created>2025-01-31</created><updated>2025-02-07</updated><authors><author><keyname>Herrera-Esposito</keyname><forenames>Daniel</forenames></author><author><keyname>Burge</keyname><forenames>Johannes</forenames></author></authors><title>Supervised Quadratic Feature Analysis: An Information Geometry Approach   to Dimensionality Reduction</title><categories>stat.ML cs.LG math.DG math.ST stat.TH</categories><comments>18 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Supervised dimensionality reduction aims to map labeled data to a low-dimensional feature space while maximizing class discriminability. Despite the availability of methods for learning complex non-linear features (e.g. Deep Learning), there is an enduring demand for dimensionality reduction methods that learn linear features due to their interpretability, low computational cost, and broad applicability. However, there is a gap between methods that optimize linear separability (e.g. LDA), and more flexible but computationally expensive methods that optimize over arbitrary class boundaries (e.g. metric-learning methods). Here, we present Supervised Quadratic Feature Analysis (SQFA), a dimensionality reduction method for learning linear features that maximize the differences between class-conditional first- and second-order statistics, which allow for quadratic discrimination. SQFA exploits the information geometry of second-order statistics in the symmetric positive definite manifold. We show that SQFA features support quadratic discriminability in real-world problems. We also provide a theoretical link, based on information geometry, between SQFA and the Quadratic Discriminant Analysis (QDA) classifier. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00182</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00182</id><created>2025-01-31</created><updated>2025-02-07</updated><authors><author><keyname>Seo</keyname><forenames>Jungwon</forenames></author><author><keyname>Catak</keyname><forenames>Ferhat Ozgur</forenames></author><author><keyname>Rong</keyname><forenames>Chunming</forenames></author></authors><title>Understanding Federated Learning from IID to Non-IID dataset: An   Experimental Study</title><categories>cs.LG cs.AI stat.ML</categories><journal-ref>36th Norwegian ICT Conference for Research and Education, NIKT   2024</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As privacy concerns and data regulations grow, federated learning (FL) has emerged as a promising approach for training machine learning models across decentralized data sources without sharing raw data. However, a significant challenge in FL is that client data are often non-IID (non-independent and identically distributed), leading to reduced performance compared to centralized learning. While many methods have been proposed to address this issue, their underlying mechanisms are often viewed from different perspectives. Through a comprehensive investigation from gradient descent to FL, and from IID to non-IID data settings, we find that inconsistencies in client loss landscapes primarily cause performance degradation in non-IID scenarios. From this understanding, we observe that existing methods can be grouped into two main strategies: (i) adjusting parameter update paths and (ii) modifying client loss landscapes. These findings offer a clear perspective on addressing non-IID challenges in FL and help guide future research in the field. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00838</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00838</id><created>2025-02-02</created><authors><author><keyname>Bussemaker</keyname><forenames>Jasper H.</forenames></author><author><keyname>Saves</keyname><forenames>Paul</forenames></author><author><keyname>Bartoli</keyname><forenames>Nathalie</forenames></author><author><keyname>Lefebvre</keyname><forenames>Thierry</forenames></author><author><keyname>Lafage</keyname><forenames>Rémi</forenames></author></authors><title>System Architecture Optimization Strategies: Dealing with Expensive   Hierarchical Problems</title><categories>math.OC cs.DM stat.AP</categories><comments>J Glob Optim (2024)</comments><doi>10.1007/s10898-024-01443-8</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Choosing the right system architecture for the problem at hand is challenging due to the large design space and high uncertainty in the early stage of the design process. Formulating the architecting process as an optimization problem may mitigate some of these challenges. This work investigates strategies for solving System Architecture Optimization (SAO) problems: expensive, black-box, hierarchical, mixed-discrete, constrained, multi-objective problems that may be subject to hidden constraints. Imputation ratio, correction ratio, correction fraction, and max rate diversity metrics are defined for characterizing hierar chical design spaces. This work considers two classes of optimization algorithms for SAO: Multi-Objective Evolutionary Algorithms (MOEA) such as NSGA-II, and Bayesian Optimization (BO) algorithms. A new Gaussian process kernel is presented that enables modeling hierarchical categorical variables, extending previous work on modeling continuous and integer hierarchical variables. Next, a hierarchical sampling algorithm that uses design space hierarchy to group design vectors by active design variables is developed. Then, it is demonstrated that integrating more hierarchy information in the optimization algorithms yields better optimization results for BO algorithms. Several realistic single-objective and multi-objective test problems are used for investigations. Finally, the BO algorithm is applied to a jet engine architecture optimization problem. This work shows that the developed BO algorithm can effectively solve the problem with one order of magnitude less function evaluations than NSGA-II. The algorithms and problems used in this work are implemented in the open-source Python library SBArchOpt. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00854</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00854</id><created>2025-02-02</created><authors><author><keyname>Priem</keyname><forenames>Rémy</forenames></author><author><keyname>Diouane</keyname><forenames>Youssef</forenames></author><author><keyname>Bartoli</keyname><forenames>Nathalie</forenames></author><author><keyname>Dubreuil</keyname><forenames>Sylvain</forenames></author><author><keyname>Saves</keyname><forenames>Paul</forenames></author></authors><title>High-Dimensional Bayesian Optimization Using Both Random and Supervised   Embeddings</title><categories>math.OC cs.LG stat.ML</categories><journal-ref>AIAA Journal 2025 63:1, 162-173</journal-ref><doi>10.2514/1.J063488</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Bayesian optimization (BO) is one of the most powerful strategies to solve computationally expensive-to-evaluate blackbox optimization problems. However, BO methods are conventionally used for optimization problems of small dimension because of the curse of dimensionality. In this paper, a high-dimensionnal optimization method incorporating linear embedding subspaces of small dimension is proposed to efficiently perform the optimization. An adaptive learning strategy for these linear embeddings is carried out in conjunction with the optimization. The resulting BO method, named efficient global optimization coupled with random and supervised embedding (EGORSE), combines in an adaptive way both random and supervised linear embeddings. EGORSE has been compared to state-of-the-art algorithms and tested on academic examples with a number of design variables ranging from 10 to 600. The obtained results show the high potential of EGORSE to solve high-dimensional blackbox optimization problems, in terms of both CPU time and the limited number of calls to the expensive blackbox simulation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02496</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02496</id><created>2025-02-04</created><updated>2025-02-07</updated><authors><author><keyname>Kolb</keyname><forenames>Chris</forenames></author><author><keyname>Weber</keyname><forenames>Tobias</forenames></author><author><keyname>Bischl</keyname><forenames>Bernd</forenames></author><author><keyname>Rügamer</keyname><forenames>David</forenames></author></authors><title>Deep Weight Factorization: Sparse Learning Through the Lens of   Artificial Symmetries</title><categories>cs.LG stat.ML</categories><comments>accepted at ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sparse regularization techniques are well-established in machine learning, yet their application in neural networks remains challenging due to the non-differentiability of penalties like the $L_1$ norm, which is incompatible with stochastic gradient descent. A promising alternative is shallow weight factorization, where weights are decomposed into two factors, allowing for smooth optimization of $L_1$-penalized neural networks by adding differentiable $L_2$ regularization to the factors. In this work, we introduce deep weight factorization, extending previous shallow approaches to more than two factors. We theoretically establish equivalence of our deep factorization with non-convex sparse regularization and analyze its impact on training dynamics and optimization. Due to the limitations posed by standard training practices, we propose a tailored initialization scheme and identify important learning rate requirements necessary for training factorized networks. We demonstrate the effectiveness of our deep weight factorization through experiments on various architectures and datasets, consistently outperforming its shallow counterpart and widely used pruning methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02861</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02861</id><created>2025-02-04</created><updated>2025-02-06</updated><authors><author><keyname>Shen</keyname><forenames>Judy Hanwen</forenames></author><author><keyname>Vitercik</keyname><forenames>Ellen</forenames></author><author><keyname>Wikum</keyname><forenames>Anders</forenames></author></authors><title>Algorithms with Calibrated Machine Learning Predictions</title><categories>stat.ML cs.DS cs.LG</categories><comments>Replacement: updated acknowledgments</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The field of algorithms with predictions incorporates machine learning advice in the design of online algorithms to improve real-world performance. While this theoretical framework often assumes uniform reliability across all predictions, modern machine learning models can now provide instance-level uncertainty estimates. In this paper, we propose calibration as a principled and practical tool to bridge this gap, demonstrating the benefits of calibrated advice through two case studies: the ski rental and online job scheduling problems. For ski rental, we design an algorithm that achieves optimal prediction-dependent performance and prove that, in high-variance settings, calibrated advice offers more effective guidance than alternative methods for uncertainty quantification. For job scheduling, we demonstrate that using a calibrated predictor leads to significant performance improvements over existing methods. Evaluations on real-world data validate our theoretical findings, highlighting the practical impact of calibration for algorithms with predictions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03479</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03479</id><created>2025-01-27</created><updated>2025-02-06</updated><authors><author><keyname>Cuicizion</keyname><forenames>Eliuvish</forenames></author><author><keyname>Ri</keyname><forenames>Itsugo</forenames></author><author><keyname>Holmes</keyname><forenames>Elaine</forenames></author><author><keyname>Lyeutsaon</keyname><forenames>Sho</forenames></author><author><keyname>Chern</keyname><forenames>Jawad Hassan</forenames></author></authors><title>A Tutorial on Markov Renewal and Semi-Markov Proportional Hazards Model</title><categories>stat.AP stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Transition probability estimation plays a critical role in multi-state modeling, especially in clinical research. This paper investigates the application of semi-Markov and Markov renewal frameworks to the EBMT dataset, focusing on six clinical states encountered during hematopoietic stem cell transplantation. By comparing Aalen-Johansen (AJ) and Dabrowska-Sun-Horowitz (DSH) estimators, we demonstrate that semi-Markov models, which incorporate sojourn times, provide a more nuanced and temporally sensitive depiction of patient trajectories compared to memoryless Markov models. The DSH estimator consistently yields smoother probability curves, particularly for transitions involving prolonged states. We use empirical process theory and Burkholder-Davis-Gundy inequality to show weak convergence of the estimator. Future work includes extending the framework to accommodate advanced covariate structures and non-Markovian dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03587</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03587</id><created>2025-02-05</created><updated>2025-02-06</updated><authors><author><keyname>von Seeger</keyname><forenames>Anneke</forenames></author><author><keyname>Zou</keyname><forenames>Dongmian</forenames></author><author><keyname>Lerman</keyname><forenames>Gilad</forenames></author></authors><title>Stein Discrepancy for Unsupervised Domain Adaptation</title><categories>cs.LG stat.ML</categories><comments>24 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Unsupervised domain adaptation (UDA) leverages information from a labeled source dataset to improve accuracy on a related but unlabeled target dataset. A common approach to UDA is aligning representations from the source and target domains by minimizing the distance between their data distributions. Previous methods have employed distances such as Wasserstein distance and maximum mean discrepancy. However, these approaches are less effective when the target data is significantly scarcer than the source data. Stein discrepancy is an asymmetric distance between distributions that relies on one distribution only through its score function. In this paper, we propose a novel UDA method that uses Stein discrepancy to measure the distance between source and target domains. We develop a learning framework using both non-kernelized and kernelized Stein discrepancy. Theoretically, we derive an upper bound for the generalization error. Numerical experiments show that our method outperforms existing methods using other domain discrepancy measures when only small amounts of target data are available. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03942</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03942</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Holst</keyname><forenames>Klaus Kähler</forenames></author><author><keyname>Nordland</keyname><forenames>Andreas</forenames></author><author><keyname>Furkjær</keyname><forenames>Julie</forenames></author><author><keyname>Damgaard</keyname><forenames>Lars Holm</forenames></author><author><keyname>Pipper</keyname><forenames>Christian Bressen</forenames></author></authors><title>A retake on the analysis of scores truncated by terminal events</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of data from randomized controlled trials in vulnerable populations requires special attention when assessing treatment effect by a score measuring, e.g., disease stage or activity together with onset of prevalent terminal events. In reality, it is impossible to disentangle a disease score from the terminal event, since the score is not clinically meaningful after this event. In this work, we propose to assess treatment interventions simultaneously on disease score and the terminal event. Our proposal is based on a natural data-generating mechanism respecting that a disease score does not exist beyond the terminal event. We use modern semi-parametric statistical methods to provide robust and efficient estimation of the risk of terminal event and expected disease score conditional on no terminal event at a pre-specified landmark time. We also use the simultaneous asymptotic behavior of our estimators to develop a powerful closed testing procedure for confirmatory assessment of treatment effect on both onset of terminal event and level of disease score. A simulation study mimicking a large-scale outcome trial in chronic kidney patients as well as an analysis of that trial is provided to assess performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04168</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04168</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Ferradini</keyname><forenames>Carla</forenames></author><author><keyname>Gitton</keyname><forenames>Victor</forenames></author><author><keyname>Vilasini</keyname><forenames>V.</forenames></author></authors><title>Cyclic quantum causal modelling with a graph separation theorem</title><categories>quant-ph math.ST stat.ML stat.TH</categories><comments>41+41 pages. A companion paper by the same authors, focussing on   cyclic classical (functional) causal models has been submitted to the arXiv   concurrently with primary class [math.ST], v2 only differs from v1 in   including the arXiv number of the companion paper. Comments are welcome</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Causal modelling frameworks link observable correlations to causal explanations, which is a crucial aspect of science. These models represent causal relationships through directed graphs, with vertices and edges denoting systems and transformations within a theory. Most studies focus on acyclic causal graphs, where well-defined probability rules and powerful graph-theoretic properties like the d-separation theorem apply. However, understanding complex feedback processes and exotic fundamental scenarios with causal loops requires cyclic causal models, where such results do not generally hold. While progress has been made in classical cyclic causal models, challenges remain in uniquely fixing probability distributions and identifying graph-separation properties applicable in general cyclic models. In cyclic quantum scenarios, existing frameworks have focussed on a subset of possible cyclic causal scenarios, with graph-separation properties yet unexplored. This work proposes a framework applicable to all consistent quantum and classical cyclic causal models on finite-dimensional systems. We address these challenges by introducing a robust probability rule and a novel graph-separation property, p-separation, which we prove to be sound and complete for all such models. Our approach maps cyclic causal models to acyclic ones with post-selection, leveraging the post-selected quantum teleportation protocol. We characterize these protocols and their success probabilities along the way. We also establish connections between this formalism and other classical and quantum frameworks to inform a more unified perspective on causality. This provides a foundation for more general cyclic causal discovery algorithms and to systematically extend open problems and techniques from acyclic informational networks (e.g., certification of non-classicality) to cyclic causal structures and networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04171</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04171</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Ferradini</keyname><forenames>Carla</forenames></author><author><keyname>Gitton</keyname><forenames>Victor</forenames></author><author><keyname>Vilasini</keyname><forenames>V.</forenames></author></authors><title>Cyclic functional causal models beyond unique solvability with a graph   separation theorem</title><categories>math.ST quant-ph stat.ML stat.TH</categories><comments>33+16 pages. A companion paper by the same authors, focussing on   cyclic quantum causal models has been submitted to the arXiv concurrently   with primary class [quant-ph], v2 only differs from v1 in including the arXiv   number of the companion paper. Comments are welcome</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Functional causal models (fCMs) specify functional dependencies between random variables associated to the vertices of a graph. In directed acyclic graphs (DAGs), fCMs are well-understood: a unique probability distribution on the random variables can be easily specified, and a crucial graph-separation result called the d-separation theorem allows one to characterize conditional independences between the variables. However, fCMs on cyclic graphs pose challenges due to the absence of a systematic way to assign a unique probability distribution to the fCM's variables, the failure of the d-separation theorem, and lack of a generalization of this theorem that is applicable to all consistent cyclic fCMs. In this work, we develop a causal modeling framework applicable to all cyclic fCMs involving finite-cardinality variables, except inconsistent ones admitting no solutions. Our probability rule assigns a unique distribution even to non-uniquely solvable cyclic fCMs and reduces to the known rule for uniquely solvable fCMs. We identify a class of fCMs, called averagely uniquely solvable, that we show to be the largest class where the probabilities admit a Markov factorization. Furthermore, we introduce a new graph-separation property, p-separation, and prove this to be sound and complete for all consistent finite-cardinality cyclic fCMs while recovering the d-separation theorem for DAGs. These results are obtained by considering classical post-selected teleportation protocols inspired by analogous protocols in quantum information theory. We discuss further avenues for exploration, linking in particular problems in cyclic fCMs and in quantum causality. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04208</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04208</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Grünwald</keyname><forenames>Peter D.</forenames></author><author><keyname>Koolen</keyname><forenames>Wouter M.</forenames></author></authors><title>Supermartingales for One-Sided Tests: Sufficient Monotone Likelihood   Ratios are Sufficient</title><categories>math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The t-statistic is a widely-used scale-invariant statistic for testing the null hypothesis that the mean is zero. Martingale methods enable sequential testing with the t-statistic at every sample size, while controlling the probability of falsely rejecting the null. For one-sided sequential tests, which reject when the t-statistic is too positive, a natural question is whether they also control false rejection when the true mean is negative. We prove that this is the case using monotone likelihood ratios and sufficient statistics. We develop applications to the scale-invariant t-test, the location-invariant $\chi^2$-test and sequential linear regression with nuisance covariates. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04339</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04339</id><created>2025-02-01</created><authors><author><keyname>George</keyname><forenames>Anand Jerry</forenames></author><author><keyname>Veiga</keyname><forenames>Rodrigo</forenames></author><author><keyname>Macris</keyname><forenames>Nicolas</forenames></author></authors><title>Analysis of Diffusion Models for Manifold Data</title><categories>math.ST cond-mat.dis-nn cs.IT cs.LG math.IT math.PR stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We analyze the time reversed dynamics of generative diffusion models. If the exact empirical score function is used in a regime of large dimension and exponentially large number of samples, these models are known to undergo transitions between distinct dynamical regimes. We extend this analysis and compute the transitions for an analytically tractable manifold model where the statistical model for the data is a mixture of lower dimensional Gaussians embedded in higher dimensional space. We compute the so-called speciation and collapse transition times, as a function of the ratio of manifold-to-ambient space dimensions, and other characteristics of the data model. An important tool used in our analysis is the exact formula for the mutual information (or free energy) of Generalized Linear Models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04372</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04372</id><created>2025-02-05</created><authors><author><keyname>Genari</keyname><forenames>Juliano</forenames></author><author><keyname>Goedert</keyname><forenames>Guilherme Tegoni</forenames></author></authors><title>Mining Unstructured Medical Texts With Conformal Active Learning</title><categories>cs.CL cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The extraction of relevant data from Electronic Health Records (EHRs) is crucial to identifying symptoms and automating epidemiological surveillance processes. By harnessing the vast amount of unstructured text in EHRs, we can detect patterns that indicate the onset of disease outbreaks, enabling faster, more targeted public health responses. Our proposed framework provides a flexible and efficient solution for mining data from unstructured texts, significantly reducing the need for extensive manual labeling by specialists. Experiments show that our framework achieving strong performance with as few as 200 manually labeled texts, even for complex classification problems. Additionally, our approach can function with simple lightweight models, achieving competitive and occasionally even better results compared to more resource-intensive deep learning models. This capability not only accelerates processing times but also preserves patient privacy, as the data can be processed on weaker on-site hardware rather than being transferred to external systems. Our methodology, therefore, offers a practical, scalable, and privacy-conscious approach to real-time epidemiological monitoring, equipping health institutions to respond rapidly and effectively to emerging health threats. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04422</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04422</id><created>2025-02-06</created><authors><author><keyname>Yadav</keyname><forenames>Pooja</forenames></author><author><keyname>Srivastava</keyname><forenames>Tanuja</forenames></author></authors><title>The Maximum Likelihood Degree of Farlie Gumbel Morgenstern Bivariate   Exponential Distribution</title><categories>math.ST math.AC stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum likelihood degree of a statistical model refers to the number of solutions, where the derivative of the log-likelihood function is zero, over the complex field. This paper examines the maximum likelihood degree of the parameter in Farlie-Gumbel-Morgenstern bivariate exponential distribution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04464</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04464</id><created>2025-02-06</created><authors><author><keyname>Jadoul</keyname><forenames>Yannick</forenames></author><author><keyname>Tufarelli</keyname><forenames>Tommaso</forenames></author><author><keyname>Coissac</keyname><forenames>Chloé</forenames></author><author><keyname>Gamba</keyname><forenames>Marco</forenames></author><author><keyname>Ravignani</keyname><forenames>Andrea</forenames></author></authors><title>Hidden assumptions of integer ratio analyses in bioacoustics and music</title><categories>stat.AP</categories><comments>19 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Rhythm is ubiquitous in human culture and in nature, but hard to capture in all its complexity. A key dimension of rhythm, integer ratio categories occur when the relationship between temporal intervals can be expressed as small-integer ratios. Recent work has found integer ratio categories in most human musical cultures and some animal species' vocalizations or behavioral displays. But biological systems are noisy, and empirically measured intervals rarely form an exact small-integer ratio. Here, we mathematically assess whether the leading integer ratio analysis method makes valid statistical and biological assumptions. In particular, we (1) make the temporal properties of empirical ratios explicit, both in general and for the typical use in the literature; (2) show how the choice of ratio formula affects the probability distribution of rhythm ratios and ensuing statistical results; (3) guide the reader to carefully consider the assumptions and null hypotheses of the statistical analysis; (4) present a comprehensive methodology to statistically test integer ratios for any null hypothesis of choice. Our observations have implications for both past and future research in music cognition and animal behavior: They suggest how to interpret past findings and provide tools to choose the correct null hypotheses in future empirical work. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04491</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04491</id><created>2025-02-06</created><authors><author><keyname>Cheng</keyname><forenames>Ziheng</forenames></author><author><keyname>Xie</keyname><forenames>Tianyu</forenames></author><author><keyname>Zhang</keyname><forenames>Shiyue</forenames></author><author><keyname>Zhang</keyname><forenames>Cheng</forenames></author></authors><title>Provable Sample-Efficient Transfer Learning Conditional Diffusion Models   via Representation Learning</title><categories>cs.LG math.ST stat.ML stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While conditional diffusion models have achieved remarkable success in various applications, they require abundant data to train from scratch, which is often infeasible in practice. To address this issue, transfer learning has emerged as an essential paradigm in small data regimes. Despite its empirical success, the theoretical underpinnings of transfer learning conditional diffusion models remain unexplored. In this paper, we take the first step towards understanding the sample efficiency of transfer learning conditional diffusion models through the lens of representation learning. Inspired by practical training procedures, we assume that there exists a low-dimensional representation of conditions shared across all tasks. Our analysis shows that with a well-learned representation from source tasks, the samplecomplexity of target tasks can be reduced substantially. In addition, we investigate the practical implications of our theoretical results in several real-world applications of conditional diffusion models. Numerical experiments are also conducted to verify our results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04543</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04543</id><created>2025-02-06</created><authors><author><keyname>Lu</keyname><forenames>Zhou</forenames></author><author><keyname>Sun</keyname><forenames>Y. Jennifer</forenames></author><author><keyname>Zhang</keyname><forenames>Zhiyu</forenames></author></authors><title>Sparsity-Based Interpolation of External, Internal and Swap Regret</title><categories>stat.ML cs.LG</categories><comments>Equal contribution, alphabetical order</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Focusing on the expert problem in online learning, this paper studies the interpolation of several performance metrics via $\phi$-regret minimization, which measures the performance of an algorithm by its regret with respect to an arbitrary action modification rule $\phi$. With $d$ experts and $T\gg d$ rounds in total, we present a single algorithm achieving the instance-adaptive $\phi$-regret bound \begin{equation*} \tilde O\left(\min\left\{\sqrt{d-d^{\mathrm{unif}}_\phi+1},\sqrt{d-d^{\mathrm{self}}_\phi}\right\}\cdot\sqrt{T}\right), \end{equation*} where $d^{\mathrm{unif}}_\phi$ is the maximum amount of experts modified identically by $\phi$, and $d^{\mathrm{self}}_\phi$ is the amount of experts that $\phi$ trivially modifies to themselves. By recovering the optimal $O(\sqrt{T\log d})$ external regret bound when $d^{\mathrm{unif}}_\phi=d$, the standard $\tilde O(\sqrt{T})$ internal regret bound when $d^{\mathrm{self}}_\phi=d-1$ and the optimal $\tilde O(\sqrt{dT})$ swap regret bound in the worst case, we improve existing results in the intermediate regimes. In addition, the same algorithm achieves the optimal quantile regret bound, which corresponds to even easier settings of $\phi$ than the external regret.   Building on the classical reduction from $\phi$-regret minimization to external regret minimization on stochastic matrices, our main idea is to further convert the latter to online linear regression using Haar-wavelet-inspired matrix features. Then, we apply a particular $L_1$-version of comparator-adaptive online learning algorithms to exploit the sparsity in this regression subroutine. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04550</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04550</id><created>2025-02-06</created><authors><author><keyname>Faes</keyname><forenames>Luca</forenames></author><author><keyname>Sparacino</keyname><forenames>Laura</forenames></author><author><keyname>Mijatovic</keyname><forenames>Gorana</forenames></author><author><keyname>Antonacci</keyname><forenames>Yuri</forenames></author><author><keyname>Ricci</keyname><forenames>Leonardo</forenames></author><author><keyname>Marinazzo</keyname><forenames>Daniele</forenames></author><author><keyname>Stramaglia</keyname><forenames>Sebastiano</forenames></author></authors><title>Partial Information Rate Decomposition</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Partial Information Decomposition (PID) is a principled and flexible method to unveil complex high-order interactions in multi-unit network systems. Though being defined exclusively for random variables, PID is ubiquitously applied to multivariate time series taken as realizations of random processes with temporal statistical structure. Here, to overcome the incorrect depiction of high-order effects by PID schemes applied to dynamic networks, we introduce the framework of Partial Information Rate Decomposition (PIRD). PIRD is formalized applying lattice theory to decompose the information shared dynamically between a target random process and a set of source processes, implemented for Gaussian processes through a spectral expansion of information rates, and demonstrated in practice analyzing time series from large-scale climate oscillations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04553</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04553</id><created>2025-02-06</created><authors><author><keyname>Swanson</keyname><forenames>David</forenames></author><author><keyname>Sherry</keyname><forenames>Alexander</forenames></author><author><keyname>Tang</keyname><forenames>Chad</forenames></author></authors><title>Variance component mixture modelling for longitudinal T-cell receptor   clonal dynamics</title><categories>stat.ME</categories><msc-class>62F15 (Primary), 62H30 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Studies of T cells and their clonally unique receptors have shown promise in elucidating the association between immune response and human disease. Methods to identify T-cell receptor clones which expand or contract in response to certain therapeutic strategies have so far been limited to longitudinal pairwise comparisons of clone frequency with multiplicity adjustment. Here we develop a more general mixture model approach for arbitrary follow-up and missingness which partitions dynamic longitudinal clone frequency behavior from static. While it is common to mix on the location or scale parameter of a family of distributions, the model instead mixes on the parameterization itself, the dynamic component allowing for a variable, Gamma-distributed Poisson mean parameter over longitudinal follow-up, while the static component mean is time invariant. Leveraging conjugacy, one can integrate out the mean parameter for the dynamic and static components to yield distinct posterior predictive distributions whose expressions are a product of negative binomials and a single negative multinomial, respectively, each modified according to an offset for receptor read count normalization. An EM-algorithm is developed to estimate hyperparameters and component membership, and validity of the approach is demonstrated in simulation. The model identifies a statistically significant and clinically relevant increase in TCR clonal dynamism among metastasis-directed radiation therapy in a cohort of prostate cancer patients. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04555</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04555</id><created>2025-02-06</created><authors><author><keyname>Sparacino</keyname><forenames>Laura</forenames></author><author><keyname>Mijatovic</keyname><forenames>Gorana</forenames></author><author><keyname>Antonacci</keyname><forenames>Yuri</forenames></author><author><keyname>Ricci</keyname><forenames>Leonardo</forenames></author><author><keyname>Marinazzo</keyname><forenames>Daniele</forenames></author><author><keyname>Stramaglia</keyname><forenames>Sebastiano</forenames></author><author><keyname>Faes</keyname><forenames>Luca</forenames></author></authors><title>Decomposing Multivariate Information Rates in Networks of Random   Processes</title><categories>stat.ME cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The Partial Information Decomposition (PID) framework has emerged as a powerful tool for analyzing high-order interdependencies in complex network systems. However, its application to dynamic processes remains challenging due to the implicit assumption of memorylessness, which often falls in real-world scenarios. In this work, we introduce the framework of Partial Information Rate Decomposition (PIRD) that extends PID to random processes with temporal correlations. By leveraging mutual information rate (MIR) instead of mutual information (MI), our approach decomposes the dynamic information shared by multivariate random processes into unique, redundant, and synergistic contributions obtained aggregating information rate atoms in a principled manner. To solve PIRD, we define a pointwise redundancy rate function based on the minimum MI principle applied locally in the frequency-domain representation of the processes. The framework is validated in benchmark simulations of Gaussian systems, demonstrating its advantages over traditional PID in capturing temporal correlations and showing how the spectral representation may reveal scale-specific higher-order interaction that are obscured in the time domain. Furthermore, we apply PIRD to a physiological network comprising cerebrovascular and cardiovascular variables, revealing frequency-dependent redundant information exchange during a protocol of postural stress. Our results highlight the necessity of accounting for the full temporal statistical structure and spectral content of vector random processes to meaningfully perform information decomposition in network systems with dynamic behavior such as those typically encountered in neuroscience and physiology. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04574</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04574</id><created>2025-02-06</created><authors><author><keyname>Gong</keyname><forenames>ZhuQing</forenames></author><author><keyname>Zuo</keyname><forenames>XiNian</forenames></author></authors><title>Dark Brain Energy: Toward an Integrative Model of Spontaneous Slow   Oscillations</title><categories>q-bio.NC cs.IT math.IT stat.AP</categories><comments>38 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Neural oscillations facilitate the functioning of the human brain in spatial and temporal dimensions at various frequencies. These oscillations feature a universal frequency architecture that is governed by brain anatomy, ensuring frequency specificity remains invariant across different measurement techniques. Initial magnetic resonance imaging (MRI) methodology constrained functional MRI (fMRI) investigations to a singular frequency range, thereby neglecting the frequency characteristics inherent in blood oxygen level-dependent oscillations. With advancements in MRI technology, it has become feasible to decode intricate brain activities via multi-band frequency analysis (MBFA). During the past decade, the utilization of MBFA in fMRI studies has surged, unveiling frequency-dependent characteristics of spontaneous slow oscillations (SSOs) believed to base dark energy in the brain. There remains a dearth of conclusive insights and hypotheses pertaining to the properties and functionalities of SSOs in distinct bands. We surveyed the SSO MBFA studies during the past 15 years to delineate the attributes of SSOs and enlighten their correlated functions. We further proposed a model to elucidate the hierarchical organization of multi-band SSOs by integrating their function, aimed at bridging theoretical gaps and guiding future MBFA research endeavors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04575</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04575</id><created>2025-02-06</created><authors><author><keyname>Guo</keyname><forenames>Wei</forenames></author><author><keyname>Tao</keyname><forenames>Molei</forenames></author><author><keyname>Chen</keyname><forenames>Yongxin</forenames></author></authors><title>Complexity Analysis of Normalizing Constant Estimation: from Jarzynski   Equality to Annealed Importance Sampling and beyond</title><categories>stat.ML cs.LG cs.NA math.NA physics.comp-ph stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given an unnormalized probability density $\pi\propto\mathrm{e}^{-V}$, estimating its normalizing constant $Z=\int_{\mathbb{R}^d}\mathrm{e}^{-V(x)}\mathrm{d}x$ or free energy $F=-\log Z$ is a crucial problem in Bayesian statistics, statistical mechanics, and machine learning. It is challenging especially in high dimensions or when $\pi$ is multimodal. To mitigate the high variance of conventional importance sampling estimators, annealing-based methods such as Jarzynski equality and annealed importance sampling are commonly adopted, yet their quantitative complexity guarantees remain largely unexplored. We take a first step toward a non-asymptotic analysis of annealed importance sampling. In particular, we derive an oracle complexity of $\widetilde{O}\left(\frac{d\beta^2{\mathcal{A}}^2}{\varepsilon^4}\right)$ for estimating $Z$ within $\varepsilon$ relative error with high probability, where $\beta$ is the smoothness of $V$ and $\mathcal{A}$ denotes the action of a curve of probability measures interpolating $\pi$ and a tractable reference distribution. Our analysis, leveraging Girsanov theorem and optimal transport, does not explicitly require isoperimetric assumptions on the target distribution. Finally, to tackle the large action of the widely used geometric interpolation of probability distributions, we propose a new normalizing constant estimation algorithm based on reverse diffusion samplers and establish a framework for analyzing its complexity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04591</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04591</id><created>2025-02-06</created><authors><author><keyname>Deidda</keyname><forenames>Piero</forenames></author><author><keyname>Zhang</keyname><forenames>Kaicheng</forenames></author><author><keyname>Higham</keyname><forenames>Desmond</forenames></author><author><keyname>Tudisco</keyname><forenames>Francesco</forenames></author></authors><title>Rethinking Oversmoothing in Graph Neural Networks: A Rank-Based   Perspective</title><categories>cs.LG cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Oversmoothing is a fundamental challenge in graph neural networks (GNNs): as the number of layers increases, node embeddings become increasingly similar, and model performance drops sharply. Traditionally, oversmoothing has been quantified using metrics that measure the similarity of neighbouring node features, such as the Dirichlet energy. While these metrics are related to oversmoothing, we argue they have critical limitations and fail to reliably capture oversmoothing in realistic scenarios. For instance, they provide meaningful insights only for very deep networks and under somewhat strict conditions on the norm of network weights and feature representations. As an alternative, we propose measuring oversmoothing by examining the numerical or effective rank of the feature representations. We provide theoretical support for this approach, demonstrating that the numerical rank of feature representations converges to one for a broad family of nonlinear activation functions under the assumption of nonnegative trained weights. To the best of our knowledge, this is the first result that proves the occurrence of oversmoothing without assumptions on the boundedness of the weight matrices. Along with the theoretical findings, we provide extensive numerical evaluation across diverse graph architectures. Our results show that rank-based metrics consistently capture oversmoothing, whereas energy-based metrics often fail. Notably, we reveal that a significant drop in the rank aligns closely with performance degradation, even in scenarios where energy metrics remain unchanged. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04593</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04593</id><created>2025-02-06</created><authors><author><keyname>Rezaei</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Dieng</keyname><forenames>Adji Bousso</forenames></author></authors><title>The $\alpha$-Alternator: Dynamic Adaptation To Varying Noise Levels In   Sequences Using The Vendi Score For Improved Robustness and Performance</title><categories>cs.LG cs.AI cs.NE stat.ML</categories><comments>The codebase will be made available upon publication. This paper is   dedicated to Patrice Lumumba</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Current state-of-the-art dynamical models, such as Mamba, assume the same level of noisiness for all elements of a given sequence, which limits their performance on noisy temporal data. In this paper, we introduce the $\alpha$-Alternator, a novel generative model for time-dependent data that dynamically adapts to the complexity introduced by varying noise levels in sequences. The $\alpha$-Alternator leverages the Vendi Score (VS), a flexible similarity-based diversity metric, to adjust, at each time step $t$, the influence of the sequence element at time $t$ and the latent representation of the dynamics up to that time step on the predicted future dynamics. This influence is captured by a parameter that is learned and shared across all sequences in a given dataset. The sign of this parameter determines the direction of influence. A negative value indicates a noisy dataset, where a sequence element that increases the VS is considered noisy, and the model relies more on the latent history when processing that element. Conversely, when the parameter is positive, a sequence element that increases the VS is considered informative, and the $\alpha$-Alternator relies more on this new input than on the latent history when updating its predicted latent dynamics. The $\alpha$-Alternator is trained using a combination of observation masking and Alternator loss minimization. Masking simulates varying noise levels in sequences, enabling the model to be more robust to these fluctuations and improving its performance in trajectory prediction, imputation, and forecasting. Our experimental results demonstrate that the $\alpha$-Alternator outperforms both Alternators and state-of-the-art state-space models across neural decoding and time-series forecasting benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04654</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04654</id><created>2025-02-06</created><authors><author><keyname>Lim</keyname><forenames>Keunwoo</forenames></author><author><keyname>Ye</keyname><forenames>Ting</forenames></author><author><keyname>Han</keyname><forenames>Fang</forenames></author></authors><title>A sliced Wasserstein and diffusion approach to random coefficient models</title><categories>math.ST econ.EM stat.TH</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new minimum-distance estimator for linear random coefficient models. This estimator integrates the recently advanced sliced Wasserstein distance with the nearest neighbor methods, both of which enhance computational efficiency. We demonstrate that the proposed method is consistent in approximating the true distribution. Additionally, our formulation encourages a diffusion process-based algorithm, which holds independent interest and potential for broader applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04673</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04673</id><created>2025-02-07</created><authors><author><keyname>Neopane</keyname><forenames>Ojash</forenames></author><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author></authors><title>Optimistic Algorithms for Adaptive Estimation of the Average Treatment   Effect</title><categories>stat.ML cs.LG stat.ME</categories><comments>15 pages, 2 Figures</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Estimation and inference for the Average Treatment Effect (ATE) is a cornerstone of causal inference and often serves as the foundation for developing procedures for more complicated settings. Although traditionally analyzed in a batch setting, recent advances in martingale theory have paved the way for adaptive methods that can enhance the power of downstream inference. Despite these advances, progress in understanding and developing adaptive algorithms remains in its early stages. Existing work either focus on asymptotic analyses that overlook exploration-exploitation tradeoffs relevant in finite-sample regimes or rely on simpler but suboptimal estimators. In this work, we address these limitations by studying adaptive sampling procedures that take advantage of the asymptotically optimal Augmented Inverse Probability Weighting (AIPW) estimator. Our analysis uncovers challenges obscured by asymptotic approaches and introduces a novel algorithmic design principle reminiscent of optimism in multiarmed bandits. This principled approach enables our algorithm to achieve significant theoretical and empirical gains compared to prior methods. Our findings mark a step forward in advancing adaptive causal inference methods in theory and practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04681</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04681</id><created>2025-02-07</created><authors><author><keyname>Louit</keyname><forenames>Sydney</forenames></author><author><keyname>Clark</keyname><forenames>Evan</forenames></author><author><keyname>Gelbard</keyname><forenames>Alexander</forenames></author><author><keyname>Vivek</keyname><forenames>Niketna</forenames></author><author><keyname>Yan</keyname><forenames>Jun</forenames></author><author><keyname>Zhang</keyname><forenames>Panpan</forenames></author></authors><title>CALF-SBM: A Covariate-Assisted Latent Factor Stochastic Block Model</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel network generative model extended from the standard stochastic block model by concurrently utilizing observed node-level information and accounting for network-enabled nodal heterogeneity. The proposed model is so so-called covariate-assisted latent factor stochastic block model (CALF-SBM). The inference for the proposed model is done in a fully Bayesian framework. The primary application of CALF-SBM in the present research is focused on community detection, where a model-selection-based approach is employed to estimate the number of communities which is practically assumed unknown. To assess the performance of CALF-SBM, an extensive simulation study is carried out, including comparisons with multiple classical and modern network clustering algorithms. Lastly, the paper presents two real data applications, respectively based on an extremely new network data demonstrating collaborative relationships of otolaryngologists in the United States and a traditional aviation network data containing information about direct flights between airports in the United States and Canada. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04685</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04685</id><created>2025-02-07</created><authors><author><keyname>Zhang</keyname><forenames>Likun</forenames></author><author><keyname>Bhaganagar</keyname><forenames>Kiran</forenames></author><author><keyname>Wikle</keyname><forenames>Christopher K.</forenames></author></authors><title>Capturing Extreme Events in Turbulence using an Extreme Variational   Autoencoder (xVAE)</title><categories>physics.flu-dyn stat.AP stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Turbulent flow fields are characterized by extreme events that are statistically intermittent and carry a significant amount of energy and physical importance. To emulate these flows, we introduce the extreme variational Autoencoder (xVAE), which embeds a max-infinitely divisible process with heavy-tailed distributions into a standard VAE framework, enabling accurate modeling of extreme events. xVAEs are neural network models that reduce system dimensionality by learning non-linear latent representations of data. We demonstrate the effectiveness of xVAE in large-eddy simulation data of wildland fire plumes, where intense heat release and complex plume-atmosphere interactions generate extreme turbulence. Comparisons with the commonly used Proper Orthogonal Decomposition (POD) modes show that xVAE is more robust in capturing extreme values and provides a powerful uncertainty quantification framework using variational Bayes. Additionally, xVAE enables analysis of the so-called copulas of fields to assess risks associated with rare events while rigorously accounting for uncertainty, such as simultaneous exceedances of high thresholds across multiple locations. The proposed approach provides a new direction for studying realistic turbulent flows, such as high-speed aerodynamics, space propulsion, and atmospheric and oceanic systems that are characterized by extreme events. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04699</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04699</id><created>2025-02-07</created><authors><author><keyname>Lan</keyname><forenames>Hui</forenames></author><author><keyname>Chang</keyname><forenames>Haoge</forenames></author><author><keyname>Dillon</keyname><forenames>Eleanor</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author></authors><title>A Meta-learner for Heterogeneous Effects in Difference-in-Differences</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We address the problem of estimating heterogeneous treatment effects in panel data, adopting the popular Difference-in-Differences (DiD) framework under the conditional parallel trends assumption. We propose a novel doubly robust meta-learner for the Conditional Average Treatment Effect on the Treated (CATT), reducing the estimation to a convex risk minimization problem involving a set of auxiliary models. Our framework allows for the flexible estimation of the CATT, when conditioning on any subset of variables of interest using generic machine learning. Leveraging Neyman orthogonality, our proposed approach is robust to estimation errors in the auxiliary models. As a generalization to our main result, we develop a meta-learning approach for the estimation of general conditional functionals under covariate shift. We also provide an extension to the instrumented DiD setting with non-compliance. Empirical results demonstrate the superiority of our approach over existing baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04709</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04709</id><created>2025-02-07</created><authors><author><keyname>Miftachov</keyname><forenames>Ratmir</forenames></author><author><keyname>Reiß</keyname><forenames>Markus</forenames></author></authors><title>Early Stopping for Regression Trees</title><categories>math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We develop early stopping rules for growing regression tree estimators. The fully data-driven stopping rule is based on monitoring the global residual norm. The best-first search and the breadth-first search algorithms together with linear interpolation give rise to generalized projection or regularization flows. A general theory of early stopping is established. Oracle inequalities for the early-stopped regression tree are derived without any smoothness assumption on the regression function, assuming the original CART splitting rule, yet with a much broader scope. The remainder terms are of smaller order than the best achievable rates for Lipschitz functions in dimension $d\ge 2$. In real and synthetic data the early stopping regression tree estimators attain the statistical performance of cost-complexity pruning while significantly reducing computational costs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04723</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04723</id><created>2025-02-07</created><authors><author><keyname>Lyu</keyname><forenames>Ziyang</forenames></author><author><keyname>Sisson</keyname><forenames>S. A.</forenames></author><author><keyname>Welsh</keyname><forenames>A. H.</forenames></author></authors><title>Asymptotics for EBLUPs within crossed mixed effect models</title><categories>stat.ME</categories><comments>33 pages</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In this article, we derive the joint asymptotic distribution of empirical best linear unbiased predictors (EBLUPs) for individual and cell-level random effects in a crossed mixed effect model. Under mild conditions (which include moment conditions instead of normality for the random effects and model errors), we demonstrate that as the sizes of rows, columns, and, when we include interactions, cells simultaneously increase to infinity, the distribution of the differences between the EBLUPs and the random effects satisfy central limit theorems. These central limit theorems mean the EBLUPs asymptotically follow the convolution of the true random effect distribution and a normal distribution. Moreover, our results enable simple asymptotic approximations and estimators for the mean squared error (MSE) of the EBLUPs, which in turn facilitates the construction of asymptotic prediction intervals for the unobserved random effects. We show in simulations that our simple estimator of the MSE of the EBLUPs works very well in finite samples. Finally, we illustrate the use of the asymptotic prediction intervals with an analysis of movie rating data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04730</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04730</id><created>2025-02-07</created><authors><author><keyname>Xie</keyname><forenames>Tianyu</forenames></author><author><keyname>Richman</keyname><forenames>Harry</forenames></author><author><keyname>Gao</keyname><forenames>Jiansi</forenames></author><author><keyname>Matsen</keyname><forenames>Frederick A.</forenames><suffix>IV</suffix></author><author><keyname>Zhang</keyname><forenames>Cheng</forenames></author></authors><title>PhyloVAE: Unsupervised Learning of Phylogenetic Trees via Variational   Autoencoders</title><categories>stat.ML cs.LG q-bio.PE</categories><comments>ICLR 2025. 22 pages, 14 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Learning informative representations of phylogenetic tree structures is essential for analyzing evolutionary relationships. Classical distance-based methods have been widely used to project phylogenetic trees into Euclidean space, but they are often sensitive to the choice of distance metric and may lack sufficient resolution. In this paper, we introduce phylogenetic variational autoencoders (PhyloVAEs), an unsupervised learning framework designed for representation learning and generative modeling of tree topologies. Leveraging an efficient encoding mechanism inspired by autoregressive tree topology generation, we develop a deep latent-variable generative model that facilitates fast, parallelized topology generation. PhyloVAE combines this generative model with a collaborative inference model based on learnable topological features, allowing for high-resolution representations of phylogenetic tree samples. Extensive experiments demonstrate PhyloVAE's robust representation learning capabilities and fast generation of phylogenetic tree topologies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04750</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04750</id><created>2025-02-07</created><authors><author><keyname>Bui</keyname><forenames>Thang D.</forenames></author><author><keyname>Ashman</keyname><forenames>Matthew</forenames></author><author><keyname>Turner</keyname><forenames>Richard E.</forenames></author></authors><title>Tighter sparse variational Gaussian processes</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sparse variational Gaussian process (GP) approximations based on inducing points have become the de facto standard for scaling GPs to large datasets, owing to their theoretical elegance, computational efficiency, and ease of implementation. This paper introduces a provably tighter variational approximation by relaxing the standard assumption that the conditional approximate posterior given the inducing points must match that in the prior. The key innovation is to modify the conditional posterior to have smaller variances than that of the prior at the training points. We derive the collapsed bound for the regression case, describe how to use the proposed approximation in large data settings, and discuss its application to handle orthogonally structured inducing points and GP latent variable models. Extensive experiments on regression benchmarks, classification, and latent variable models demonstrate that the proposed approximation consistently matches or outperforms standard sparse variational GPs while maintaining the same computational cost. An implementation will be made available in all popular GP packages. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04793</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04793</id><created>2025-02-07</created><authors><author><keyname>Jeunen</keyname><forenames>Olivier</forenames></author></authors><title>$t$-Testing the Waters: Empirically Validating Assumptions for Reliable   A/B-Testing</title><categories>stat.ME cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A/B-tests are a cornerstone of experimental design on the web, with wide-ranging applications and use-cases. The statistical $t$-test comparing differences in means is the most commonly used method for assessing treatment effects, often justified through the Central Limit Theorem (CLT). The CLT ascertains that, as the sample size grows, the sampling distribution of the Average Treatment Effect converges to normality, making the $t$-test valid for sufficiently large sample sizes. When outcome measures are skewed or non-normal, quantifying what "sufficiently large" entails is not straightforward.   To ensure that confidence intervals maintain proper coverage and that $p$-values accurately reflect the false positive rate, it is critical to validate this normality assumption. We propose a practical method to test this, by analysing repeatedly resampled A/A-tests. When the normality assumption holds, the resulting $p$-value distribution should be uniform, and this property can be tested using the Kolmogorov-Smirnov test. This provides an efficient and effective way to empirically assess whether the $t$-test's assumptions are met, and the A/B-test is valid. We demonstrate our methodology and highlight how it helps to identify scenarios prone to inflated Type-I errors. Our approach provides a practical framework to ensure and improve the reliability and robustness of A/B-testing practices. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04807</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04807</id><created>2025-02-07</created><authors><author><keyname>Bashari</keyname><forenames>Meshi</forenames></author><author><keyname>Sesia</keyname><forenames>Matteo</forenames></author><author><keyname>Romano</keyname><forenames>Yaniv</forenames></author></authors><title>Robust Conformal Outlier Detection under Contaminated Reference Data</title><categories>stat.ML cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conformal prediction is a flexible framework for calibrating machine learning predictions, providing distribution-free statistical guarantees. In outlier detection, this calibration relies on a reference set of labeled inlier data to control the type-I error rate. However, obtaining a perfectly labeled inlier reference set is often unrealistic, and a more practical scenario involves access to a contaminated reference set containing a small fraction of outliers. This paper analyzes the impact of such contamination on the validity of conformal methods. We prove that under realistic, non-adversarial settings, calibration on contaminated data yields conservative type-I error control, shedding light on the inherent robustness of conformal methods. This conservativeness, however, typically results in a loss of power. To alleviate this limitation, we propose a novel, active data-cleaning framework that leverages a limited labeling budget and an outlier detection model to selectively annotate data points in the contaminated reference set that are suspected as outliers. By removing only the annotated outliers in this ``suspicious'' subset, we can effectively enhance power while mitigating the risk of inflating the type-I error rate, as supported by our theoretical analysis. Experiments on real datasets validate the conservative behavior of conformal methods under contamination and show that the proposed data-cleaning strategy improves power without sacrificing validity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04824</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04824</id><created>2025-02-07</created><authors><author><keyname>Blake</keyname><forenames>Joshua</forenames></author><author><keyname>Birrell</keyname><forenames>Paul</forenames></author><author><keyname>Walker</keyname><forenames>A. Sarah</forenames></author><author><keyname>Pouwels</keyname><forenames>Koen B.</forenames></author><author><keyname>House</keyname><forenames>Thomas</forenames></author><author><keyname>Tom</keyname><forenames>Brian D. M.</forenames></author><author><keyname>Kypraios</keyname><forenames>Theodore</forenames></author><author><keyname>De Angelis</keyname><forenames>Daniela</forenames></author></authors><title>Estimating the duration of RT-PCR positivity for SARS-CoV-2 from doubly   interval censored data with undetected infections</title><categories>stat.ME stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Monitoring the incidence of new infections during a pandemic is critical for an effective public health response. General population prevalence surveys for SARS-CoV-2 can provide high-quality data to estimate incidence. However, estimation relies on understanding the distribution of the duration that infections remain detectable. This study addresses this need using data from the Coronavirus Infection Survey (CIS), a long-term, longitudinal, general population survey conducted in the UK. Analyzing these data presents unique challenges, such as doubly interval censoring, undetected infections, and false negatives. We propose a Bayesian nonparametric survival analysis approach, estimating a discrete-time distribution of durations and integrating prior information derived from a complementary study. Our methodology is validated through a simulation study, including its resilience to model misspecification, and then applied to the CIS dataset. This results in the first estimate of the full duration distribution in a general population, as well as methodology that could be transferred to new contexts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04832</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04832</id><created>2025-02-07</created><authors><author><keyname>Ballarin</keyname><forenames>Giovanni</forenames></author><author><keyname>Grigoryeva</keyname><forenames>Lyudmila</forenames></author><author><keyname>Ortega</keyname><forenames>Juan-Pablo</forenames></author></authors><title>Memory Capacity of Nonlinear Recurrent Networks: Is it Informative?</title><categories>cs.LG stat.ML</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The total memory capacity (MC) of linear recurrent neural networks (RNNs) has been proven to be equal to the rank of the corresponding Kalman controllability matrix, and it is almost surely maximal for connectivity and input weight matrices drawn from regular distributions. This fact questions the usefulness of this metric in distinguishing the performance of linear RNNs in the processing of stochastic signals. This note shows that the MC of random nonlinear RNNs yields arbitrary values within established upper and lower bounds depending just on the input process scale. This confirms that the existing definition of MC in linear and nonlinear cases has no practical value. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04849</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04849</id><created>2025-02-07</created><authors><author><keyname>Yu</keyname><forenames>Yifeng</forenames></author><author><keyname>Yu</keyname><forenames>Lu</forenames></author></authors><title>Advancing Wasserstein Convergence Analysis of Score-Based Models:   Insights from Discretization and Second-Order Acceleration</title><categories>stat.ML cs.LG math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Score-based diffusion models have emerged as powerful tools in generative modeling, yet their theoretical foundations remain underexplored. In this work, we focus on the Wasserstein convergence analysis of score-based diffusion models. Specifically, we investigate the impact of various discretization schemes, including Euler discretization, exponential integrators, and midpoint randomization methods. Our analysis provides a quantitative comparison of these discrete approximations, emphasizing their influence on convergence behavior. Furthermore, we explore scenarios where Hessian information is available and propose an accelerated sampler based on the local linearization method. We demonstrate that this Hessian-based approach achieves faster convergence rates of order $\widetilde{\mathcal{O}}\left(\frac{1}{\varepsilon}\right)$ significantly improving upon the standard rate $\widetilde{\mathcal{O}}\left(\frac{1}{\varepsilon^2}\right)$ of vanilla diffusion models, where $\varepsilon$ denotes the target accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04867</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04867</id><created>2025-02-07</created><authors><author><keyname>Maclaren</keyname><forenames>Oliver J.</forenames></author><author><keyname>Nicholson</keyname><forenames>Ruanui</forenames></author><author><keyname>Trent</keyname><forenames>Joel A.</forenames></author><author><keyname>Rottenberry</keyname><forenames>Joshua</forenames></author><author><keyname>Simpson</keyname><forenames>Matthew</forenames></author></authors><title>Invariant Image Reparameterisation: A Unified Approach to Structural and   Practical Identifiability and Model Reduction</title><categories>stat.AP</categories><comments>25 pages + supplementary material</comments><msc-class>62F99 (Primary) 65L09, 93B30, 62F12, 34A55 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Both structural and practical parameter non-identifiability present fundamental challenges when using mathematical models to interpret data. This issue is particularly acute in complex, applied areas such as the life sciences or engineering, where determining appropriate model complexity is challenging. While several approaches exist for diagnosing and resolving parameter non-identifiability, including symbolic methods, profile likelihood analysis, and sloppiness analysis, these approaches have distinct limitations and are rarely combined. We present an integrated approach called Invariant Image Reparameterisation (IIR) that incorporates key elements of these methods in a new way. Our approach replaces symbolic computations with numerical calculations at a single reference estimate and an invariance condition that determines when this local calculation holds globally. Parameter combinations determined by this method are naturally ordered by degree of identifiability, and this supports model reduction by replacing a practically non-identified model with a structurally non-identified approximate model. This approximate model can be further parameterised in terms of identified parameters only. By treating parameter combinations determined by our approach as interest parameters within our established likelihood-based Profile-Wise Analysis (PWA) framework, we incorporate uncertainty quantification in terms of likelihood profiles and confidence sets. We provide a Julia library on GitHub (https://github.com/omaclaren/reparam) demonstrating our methodology across a range of mathematical models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04879</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04879</id><created>2025-02-07</created><authors><author><keyname>Gauthier</keyname><forenames>Etienne</forenames></author><author><keyname>Bach</keyname><forenames>Francis</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Statistical Collusion by Collectives on Learning Platforms</title><categories>stat.ML cs.LG</categories><comments>Code available at: https://github.com/GauthierE/statistical-collusion</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As platforms increasingly rely on learning algorithms, collectives may form and seek ways to influence these platforms to align with their own interests. This can be achieved by coordinated submission of altered data. To evaluate the potential impact of such behavior, it is essential to understand the computations that collectives must perform to impact platforms in this way. In particular, collectives need to make a priori assessments of the effect of the collective before taking action, as they may face potential risks when modifying their data. Moreover they need to develop implementable coordination algorithms based on quantities that can be inferred from observed data. We develop a framework that provides a theoretical and algorithmic treatment of these issues and present experimental results in a product evaluation domain. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04889</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04889</id><created>2025-02-07</created><authors><author><keyname>Bao</keyname><forenames>Han</forenames></author><author><keyname>Sakaue</keyname><forenames>Shinsaku</forenames></author><author><keyname>Takezawa</keyname><forenames>Yuki</forenames></author></authors><title>Any-stepsize Gradient Descent for Separable Data under Fenchel--Young   Losses</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The gradient descent (GD) has been one of the most common optimizer in machine learning. In particular, the loss landscape of a neural network is typically sharpened during the initial phase of training, making the training dynamics hover on the edge of stability. This is beyond our standard understanding of GD convergence in the stable regime where arbitrarily chosen stepsize is sufficiently smaller than the edge of stability. Recently, Wu et al. (COLT2024) have showed that GD converges with arbitrary stepsize under linearly separable logistic regression. Although their analysis hinges on the self-bounding property of the logistic loss, which seems to be a cornerstone to establish a modified descent lemma, our pilot study shows that other loss functions without the self-bounding property can make GD converge with arbitrary stepsize. To further understand what property of a loss function matters in GD, we aim to show arbitrary-stepsize GD convergence for a general loss function based on the framework of \emph{Fenchel--Young losses}. We essentially leverage the classical perceptron argument to derive the convergence rate for achieving $\epsilon$-optimal loss, which is possible for a majority of Fenchel--Young losses. Among typical loss functions, the Tsallis entropy achieves the GD convergence rate $T=\Omega(\epsilon^{-1/2})$, and the R{\'e}nyi entropy achieves the far better rate $T=\Omega(\epsilon^{-1/3})$. We argue that these better rate is possible because of \emph{separation margin} of loss functions, instead of the self-bounding property. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04891</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04891</id><created>2025-02-07</created><authors><author><keyname>Rubio-Madrigal</keyname><forenames>Celia</forenames></author><author><keyname>Jamadandi</keyname><forenames>Adarsh</forenames></author><author><keyname>Burkholz</keyname><forenames>Rebekka</forenames></author></authors><title>GNNs Getting ComFy: Community and Feature Similarity Guided Rewiring</title><categories>cs.LG cs.SI stat.ML</categories><comments>Accepted at ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Maximizing the spectral gap through graph rewiring has been proposed to enhance the performance of message-passing graph neural networks (GNNs) by addressing over-squashing. However, as we show, minimizing the spectral gap can also improve generalization. To explain this, we analyze how rewiring can benefit GNNs within the context of stochastic block models. Since spectral gap optimization primarily influences community strength, it improves performance when the community structure aligns with node labels. Building on this insight, we propose three distinct rewiring strategies that explicitly target community structure, node labels, and their alignment: (a) community structure-based rewiring (ComMa), a more computationally efficient alternative to spectral gap optimization that achieves similar goals; (b) feature similarity-based rewiring (FeaSt), which focuses on maximizing global homophily; and (c) a hybrid approach (ComFy), which enhances local feature similarity while preserving community structure to optimize label-community alignment. Extensive experiments confirm the effectiveness of these strategies and support our theoretical insights. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04892</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04892</id><created>2025-02-07</created><authors><author><keyname>Park</keyname><forenames>Joonhyeong</forenames></author><author><keyname>Park</keyname><forenames>Byoungwoo</forenames></author><author><keyname>Bang</keyname><forenames>Chang-Bae</forenames></author><author><keyname>Choi</keyname><forenames>Jungwon</forenames></author><author><keyname>Chung</keyname><forenames>Hyungjin</forenames></author><author><keyname>Kim</keyname><forenames>Byung-Hoon</forenames></author><author><keyname>Lee</keyname><forenames>Juho</forenames></author></authors><title>A Foundational Brain Dynamics Model via Stochastic Optimal Control</title><categories>cs.LG q-bio.NC stat.ML</categories><comments>The first two authors contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a foundational model for brain dynamics that utilizes stochastic optimal control (SOC) and amortized inference. Our method features a continuous-discrete state space model (SSM) that can robustly handle the intricate and noisy nature of fMRI signals. To address computational limitations, we implement an approximation strategy grounded in the SOC framework. Additionally, we present a simulation-free latent dynamics approach that employs locally linear approximations, facilitating efficient and scalable inference. For effective representation learning, we derive an Evidence Lower Bound (ELBO) from the SOC formulation, which integrates smoothly with recent advancements in self-supervised learning (SSL), thereby promoting robust and transferable representations. Pre-trained on extensive datasets such as the UKB, our model attains state-of-the-art results across a variety of downstream tasks, including demographic prediction, trait analysis, disease diagnosis, and prognosis. Moreover, evaluating on external datasets such as HCP-A, ABIDE, and ADHD200 further validates its superior abilities and resilience across different demographic and clinical distributions. Our foundational model provides a scalable and efficient approach for deciphering brain dynamics, opening up numerous applications in neuroscience. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04907</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04907</id><created>2025-02-07</created><authors><author><keyname>Gachon</keyname><forenames>Erell</forenames></author><author><keyname>Bigot</keyname><forenames>Jérémie</forenames></author><author><keyname>Cazelles</keyname><forenames>Elsa</forenames></author></authors><title>Scalable and consistent embedding of probability measures into Hilbert   spaces via measure quantization</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper is focused on statistical learning from data that come as probability measures. In this setting, popular approaches consist in embedding such data into a Hilbert space with either Linearized Optimal Transport or Kernel Mean Embedding. However, the cost of computing such embeddings prohibits their direct use in large-scale settings. We study two methods based on measure quantization for approximating input probability measures with discrete measures of small-support size. The first one is based on optimal quantization of each input measure, while the second one relies on mean-measure quantization. We study the consistency of such approximations, and its implication for scalable embeddings of probability measures into a Hilbert space at a low computational cost. We finally illustrate our findings with various numerical experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04938</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04938</id><created>2025-02-07</created><authors><author><keyname>Gardini</keyname><forenames>Aldo</forenames></author><author><keyname>Greco</keyname><forenames>Fedele</forenames></author><author><keyname>Trivisano</keyname><forenames>Carlo</forenames></author></authors><title>A note on auxiliary mixture sampling for Bayesian Poisson models</title><categories>stat.ME stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bayesian hierarchical Poisson models are an essential tool for analyzing count data. However, designing efficient algorithms to sample from the posterior distribution of the target parameters remains a challenging task for this class of models. Auxiliary mixture sampling algorithms have been proposed to address this issue. They involve two steps of data augmentations: the first leverages the theory of Poisson processes, and the second approximates the residual distribution of the resulting model through a mixture of Gaussian distributions. In this way, an approximated Gibbs sampler is obtained. In this paper, we focus on the accuracy of the approximation step, highlighting scenarios where the mixture fails to accurately represent the true underlying distribution, leading to a lack of convergence in the algorithm. We outline key features to monitor, in order to assess if the approximation performs as intended. Building on this, we propose a robust version of the auxiliary mixture sampling algorithm, which can detect approximation failures and incorporate a Metropolis-Hastings step when necessary. Finally, we evaluate the proposed algorithm together with the original mixture sampling algorithms on both simulated and real datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04945</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04945</id><created>2025-02-07</created><authors><author><keyname>Yanhao</keyname><affiliation>Max</affiliation></author><author><keyname>Wei</keyname></author><author><keyname>Jiang</keyname><forenames>Zhenling</forenames></author></authors><title>Estimating Parameters of Structural Models Using Neural Networks</title><categories>econ.EM stat.CO</categories><acm-class>G.3; J.4; I.2</acm-class><journal-ref>Marketing Science 44(1):102-128 (2024)</journal-ref><doi>10.1287/mksc.2022.0360</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an alternative use of machine learning. We train neural nets to provide the parameter estimate of a given (structural) econometric model, for example, discrete choice or consumer search. Training examples consist of datasets generated by the econometric model under a range of parameter values. The neural net takes the moments of a dataset as input and tries to recognize the parameter value underlying that dataset. Besides the point estimate, the neural net can also output statistical accuracy. This neural net estimator (NNE) tends to limited-information Bayesian posterior as the number of training datasets increases. We apply NNE to a consumer search model. It gives more accurate estimates at lighter computational costs than the prevailing approach. NNE is also robust to redundant moment inputs. In general, NNE offers the most benefits in applications where other estimation approaches require very heavy simulation costs. We provide code at: https://nnehome.github.io. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04949</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04949</id><created>2025-02-07</created><authors><author><keyname>Elsemüller</keyname><forenames>Lasse</forenames></author><author><keyname>Pratz</keyname><forenames>Valentin</forenames></author><author><keyname>von Krause</keyname><forenames>Mischa</forenames></author><author><keyname>Voss</keyname><forenames>Andreas</forenames></author><author><keyname>Bürkner</keyname><forenames>Paul-Christian</forenames></author><author><keyname>Radev</keyname><forenames>Stefan T.</forenames></author></authors><title>Does Unsupervised Domain Adaptation Improve the Robustness of Amortized   Bayesian Inference? A Systematic Evaluation</title><categories>stat.ML cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Neural networks are fragile when confronted with data that significantly deviates from their training distribution. This is true in particular for simulation-based inference methods, such as neural amortized Bayesian inference (ABI), where models trained on simulated data are deployed on noisy real-world observations. Recent robust approaches employ unsupervised domain adaptation (UDA) to match the embedding spaces of simulated and observed data. However, the lack of comprehensive evaluations across different domain mismatches raises concerns about the reliability in high-stakes applications. We address this gap by systematically testing UDA approaches across a wide range of misspecification scenarios in both a controlled and a high-dimensional benchmark. We demonstrate that aligning summary spaces between domains effectively mitigates the impact of unmodeled phenomena or noise. However, the same alignment mechanism can lead to failures under prior misspecifications - a critical finding with practical consequences. Our results underscore the need for careful consideration of misspecification types when using UDA techniques to increase the robustness of ABI in practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04970</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04970</id><created>2025-02-07</created><authors><author><keyname>Langbein</keyname><forenames>Sophie Hanna</forenames></author><author><keyname>Koenen</keyname><forenames>Niklas</forenames></author><author><keyname>Wright</keyname><forenames>Marvin N.</forenames></author></authors><title>Gradient-based Explanations for Deep Learning Survival Models</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deep learning survival models often outperform classical methods in time-to-event predictions, particularly in personalized medicine, but their "black box" nature hinders broader adoption. We propose a framework for gradient-based explanation methods tailored to survival neural networks, extending their use beyond regression and classification. We analyze the implications of their theoretical assumptions for time-dependent explanations in the survival setting and propose effective visualizations incorporating the temporal dimension. Experiments on synthetic data show that gradient-based methods capture the magnitude and direction of local and global feature effects, including time dependencies. We introduce GradSHAP(t), a gradient-based counterpart to SurvSHAP(t), which outperforms SurvSHAP(t) and SurvLIME in a computational speed vs. accuracy trade-off. Finally, we apply these methods to medical data with multi-modal inputs, revealing relevant tabular features and visual patterns, as well as their temporal dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04990</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04990</id><created>2025-02-07</created><authors><author><keyname>Pichler</keyname><forenames>Clemens</forenames></author><author><keyname>Jewson</keyname><forenames>Jack</forenames></author><author><keyname>Avalos-Pacheco</keyname><forenames>Alejandra</forenames></author></authors><title>Probabilistic Programming with Sufficient Statistics for faster Bayesian   Computation</title><categories>stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Probabilistic programming methods have revolutionised Bayesian inference, making it easier than ever for practitioners to perform Markov-chain-Monte-Carlo sampling from non-conjugate posterior distributions. Here we focus on Stan, arguably the most used probabilistic programming tool for Bayesian inference (Carpenter et al., 2017), and its interface with R via the brms (Burkner, 2017) and rstanarm (Goodrich et al., 2024) packages. Although easy to implement, these tools can become computationally prohibitive when applied to datasets with many observations or models with numerous parameters. While the use of sufficient statistics is well-established in theory, it has been surprisingly overlooked in state-of-the-art Stan software. We show that when the likelihood can be written in terms of sufficient statistics, considerable computational improvements can be made to current implementations. We demonstrate how this approach provides accurate inference at a fraction of the time than state-of-the-art implementations for Gaussian linear regression models with non-conjugate priors, hierarchical random effects models, and factor analysis models. Our results also show that moderate computational gains can be achieved even in models where the likelihood can only be partially written in terms of sufficient statistics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05021</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05021</id><created>2025-02-07</created><authors><author><keyname>van Heel</keyname><forenames>Simon Donker</forenames></author><author><keyname>Lange</keyname><forenames>Rutger-Jan</forenames></author><author><keyname>van Dijk</keyname><forenames>Dick</forenames></author><author><keyname>van Os</keyname><forenames>Bram</forenames></author></authors><title>Stability and performance guarantees for misspecified multivariate   score-driven filters</title><categories>stat.ME eess.SP stat.ML</categories><comments>71 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the problem of tracking latent time-varying parameter vectors under model misspecification. We analyze implicit and explicit score-driven (ISD and ESD) filters, which update a prediction of the parameters using the gradient of the logarithmic observation density (i.e., the score). In the ESD filter, the score is computed using the predicted parameter values, whereas in the ISD filter, the score is evaluated using the new, updated parameter values. For both filter types, we derive novel sufficient conditions for the exponential stability (i.e., invertibility) of the filtered parameter path and existence of a finite mean squared error (MSE) bound with respect to the pseudo-true parameter path. In addition, we present expressions for finite-sample and asymptotic MSE bounds. Our performance guarantees rely on mild moment conditions on the data-generating process, while our stability result is entirely agnostic about the true process. As a result, our primary conditions depend only on the characteristics of the filter; hence, they are verifiable in practice. Concavity of the postulated log density combined with simple parameter restrictions is sufficient (but not necessary) for ISD-filter stability, whereas ESD-filter stability additionally requires the score to be Lipschitz continuous. Extensive simulation studies validate our theoretical findings and demonstrate the enhanced stability and improved performance of ISD over ESD filters. An empirical application to U.S. Treasury-bill rates confirms the practical relevance of our contribution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05072</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05072</id><created>2025-02-07</created><authors><author><keyname>Barnett</keyname><forenames>Helen</forenames></author><author><keyname>Boix</keyname><forenames>Oliver</forenames></author><author><keyname>Kontos</keyname><forenames>Dimitris</forenames></author><author><keyname>Jaki</keyname><forenames>Thomas</forenames></author></authors><title>Joint TITE-CRM for Dual Agent Dose Finding Studies</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Dual agent dose-finding trials study the effect of a combination of more than one agent, where the objective is to find the Maximum Tolerated Dose Combination (MTC), the combination of doses of the two agents that is associated with a pre-specified risk of being unsafe. In a Phase I/II setting, the objective is to find a dose combination that is both safe and active, the Optimal Biological Dose (OBD), that optimizes a criterion based on both safety and activity. Since Oncology treatments are typically given over multiple cycles, both the safety and activity outcome can be considered as late-onset, potentially occurring in the later cycles of treatment. This work proposes two model-based designs for dual-agent dose finding studies with late-onset activity and late-onset toxicity outcomes, the Joint TITE-POCRM and the Joint TITE-BLRM. Their performance is compared alongside a model-assisted comparator in a comprehensive simulation study motivated by a real trial example, with an extension to consider alternative sized dosing grids. It is found that both model-based methods outperform the model-assisted design. Whilst on average the two model-based designs are comparable, this comparability is not consistent across scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05074</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05074</id><created>2025-02-07</created><authors><author><keyname>Atanasov</keyname><forenames>Alexander</forenames></author><author><keyname>Bordelon</keyname><forenames>Blake</forenames></author><author><keyname>Zavatone-Veth</keyname><forenames>Jacob A.</forenames></author><author><keyname>Paquette</keyname><forenames>Courtney</forenames></author><author><keyname>Pehlevan</keyname><forenames>Cengiz</forenames></author></authors><title>Two-Point Deterministic Equivalence for Stochastic Gradient Dynamics in   Linear Models</title><categories>cond-mat.dis-nn cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We derive a novel deterministic equivalence for the two-point function of a random matrix resolvent. Using this result, we give a unified derivation of the performance of a wide variety of high-dimensional linear models trained with stochastic gradient descent. This includes high-dimensional linear regression, kernel regression, and random feature models. Our results include previously known asymptotics as well as novel ones. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05075</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05075</id><created>2025-02-07</created><authors><author><keyname>Dong</keyname><forenames>Yijun</forenames></author><author><keyname>Li</keyname><forenames>Yicheng</forenames></author><author><keyname>Li</keyname><forenames>Yunai</forenames></author><author><keyname>Lee</keyname><forenames>Jason D.</forenames></author><author><keyname>Lei</keyname><forenames>Qi</forenames></author></authors><title>Discrepancies are Virtue: Weak-to-Strong Generalization through Lens of   Intrinsic Dimension</title><categories>cs.LG cs.NA math.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weak-to-strong (W2S) generalization is a type of finetuning (FT) where a strong (large) student model is trained on pseudo-labels generated by a weak teacher. Surprisingly, W2S FT often outperforms the weak teacher. We seek to understand this phenomenon through the observation that FT often occurs in intrinsically low-dimensional spaces. Leveraging the low intrinsic dimensionality of FT, we analyze W2S in the ridgeless regression setting from a variance reduction perspective. For a strong student - weak teacher pair with sufficiently expressive low-dimensional feature subspaces $\mathcal{V}_s, \mathcal{V}_w$, we provide an exact characterization of the variance that dominates the generalization error of W2S. This unveils a virtue of discrepancy between the strong and weak models in W2S: the variance of the weak teacher is inherited by the strong student in $\mathcal{V}_s \cap \mathcal{V}_w$, while reduced by a factor of $\dim(\mathcal{V}_s)/N$ in the subspace of discrepancy $\mathcal{V}_w \setminus \mathcal{V}_s$ with $N$ pseudo-labels for W2S. Further, our analysis casts light on the sample complexities and the scaling of performance gap recovery in W2S. The analysis is supported with experiments on both synthetic regression problems and real vision tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05094</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05094</id><created>2025-02-07</created><authors><author><keyname>Blanchet</keyname><forenames>Jose</forenames></author><author><keyname>Hamoudi</keyname><forenames>Yassine</forenames></author><author><keyname>Szegedy</keyname><forenames>Mario</forenames></author><author><keyname>Wang</keyname><forenames>Guanyang</forenames></author></authors><title>Non-linear Quantum Monte Carlo</title><categories>quant-ph cs.LG cs.NA math.NA stat.CO stat.ML</categories><comments>30 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The mean of a random variable can be understood as a $\textit{linear}$ functional on the space of probability distributions. Quantum computing is known to provide a quadratic speedup over classical Monte Carlo methods for mean estimation. In this paper, we investigate whether a similar quadratic speedup is achievable for estimating $\textit{non-linear}$ functionals of probability distributions. We propose a quantum-inside-quantum Monte Carlo algorithm that achieves such a speedup for a broad class of non-linear estimation problems, including nested conditional expectations and stochastic optimization. Our algorithm improves upon the direct application of the quantum multilevel Monte Carlo algorithm introduced by An et al.. The existing lower bound indicates that our algorithm is optimal up polylogarithmic factors. A key innovation of our approach is a new sequence of multilevel Monte Carlo approximations specifically designed for quantum computing, which is central to the algorithm's improved performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05102</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05102</id><created>2025-02-07</created><authors><author><keyname>Piancastelli</keyname><forenames>Luiza</forenames></author><author><keyname>Barreto-Souza</keyname><forenames>Wagner</forenames></author></authors><title>Time Series Analysis of Rankings: A GARCH-Type Approach</title><categories>stat.ME stat.CO stat.ML</categories><comments>Paper submitted for publication</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Ranking data are frequently obtained nowadays but there are still scarce methods for treating these data when temporally observed. The present paper contributes to this topic by proposing and developing novel models for handling time series of ranking data. We introduce a class of time-varying ranking models inspired by the Generalized AutoRegressive Conditional Heteroskedasticity (GARCH) models. More specifically, the temporal dynamics are defined by the conditional distribution of the current ranking given the past rankings, which are assumed to follow a Mallows distribution, which implicitly depends on a distance. Then, autoregressive and feedback components are incorporated into the model through the conditional expectation of the associated distances. Theoretical properties of our ranking GARCH models such as stationarity and ergodicity are established. The estimation of parameters is performed via maximum likelihood estimation when data is fully observed. We develop a Monte Carlo Expectation-Maximisation algorithm to deal with cases involving missing data. Monte Carlo simulation studies are presented to study the performance of the proposed estimators under both non-missing and missing data scenarios. A real data application about the weekly ranking of professional tennis players from 2015 to 2019 is presented under our proposed ranking GARCH models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05122</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05122</id><created>2025-02-07</created><authors><author><keyname>Xi</keyname><forenames>Johnny</forenames></author><author><keyname>Dance</keyname><forenames>Hugh</forenames></author><author><keyname>Orbanz</keyname><forenames>Peter</forenames></author><author><keyname>Bloem-Reddy</keyname><forenames>Benjamin</forenames></author></authors><title>Distinguishing Cause from Effect with Causal Velocity Models</title><categories>stat.ML cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bivariate structural causal models (SCM) are often used to infer causal direction by examining their goodness-of-fit under restricted model classes. In this paper, we describe a parametrization of bivariate SCMs in terms of a causal velocity by viewing the cause variable as time in a dynamical system. The velocity implicitly defines counterfactual curves via the solution of initial value problems where the observation specifies the initial condition. Using tools from measure transport, we obtain a unique correspondence between SCMs and the score function of the generated distribution via its causal velocity. Based on this, we derive an objective function that directly regresses the velocity against the score function, the latter of which can be estimated non-parametrically from observational data. We use this to develop a method for bivariate causal discovery that extends beyond known model classes such as additive or location scale noise, and that requires no assumptions on the noise distributions. When the score is estimated well, the objective is also useful for detecting model non-identifiability and misspecification. We present positive results in simulation and benchmark experiments where many existing methods fail, and perform ablation studies to examine the method's sensitivity to accurate score estimation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05127</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05127</id><created>2025-02-07</created><authors><author><keyname>Everink</keyname><forenames>Jasper M.</forenames></author><author><keyname>Amougou</keyname><forenames>Bernardin Tamo</forenames></author><author><keyname>Pereyra</keyname><forenames>Marcelo</forenames></author></authors><title>Self-supervised Conformal Prediction for Uncertainty Quantification in   Imaging Problems</title><categories>cs.CV stat.ME</categories><msc-class>62H35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most image restoration problems are ill-conditioned or ill-posed and hence involve significant uncertainty. Quantifying this uncertainty is crucial for reliably interpreting experimental results, particularly when reconstructed images inform critical decisions and science. However, most existing image restoration methods either fail to quantify uncertainty or provide estimates that are highly inaccurate. Conformal prediction has recently emerged as a flexible framework to equip any estimator with uncertainty quantification capabilities that, by construction, have nearly exact marginal coverage. To achieve this, conformal prediction relies on abundant ground truth data for calibration. However, in image restoration problems, reliable ground truth data is often expensive or not possible to acquire. Also, reliance on ground truth data can introduce large biases in situations of distribution shift between calibration and deployment. This paper seeks to develop a more robust approach to conformal prediction for image restoration problems by proposing a self-supervised conformal prediction method that leverages Stein's Unbiased Risk Estimator (SURE) to self-calibrate itself directly from the observed noisy measurements, bypassing the need for ground truth. The method is suitable for any linear imaging inverse problem that is ill-conditioned, and it is especially powerful when used with modern self-supervised image restoration techniques that can also be trained directly from measurement data. The proposed approach is demonstrated through numerical experiments on image denoising and deblurring, where it delivers results that are remarkably accurate and comparable to those obtained by supervised conformal prediction with ground truth data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05134</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05134</id><created>2025-02-07</created><authors><author><keyname>Kızıldağ</keyname><forenames>Eren C.</forenames></author></authors><title>Information-Theoretic Guarantees for Recovering Low-Rank Tensors from   Symmetric Rank-One Measurements</title><categories>math.ST cs.IT math.IT math.PR stat.ML stat.TH</categories><comments>Algorithmic Learning Theory (ALT), 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the sample complexity of recovering tensors with low symmetric rank from symmetric rank-one measurements. This setting is particularly motivated by the study of higher-order interactions and the analysis of two-layer neural networks with polynomial activations (polynomial networks). Using a covering numbers argument, we analyze the performance of the symmetric rank minimization program and establish near-optimal sample complexity bounds when the underlying distribution is log-concave. Our measurement model involves random symmetric rank-one tensors, which lead to involved probability calculations. To address these challenges, we employ the Carbery-Wright inequality, a powerful tool for studying anti-concentration properties of random polynomials, and leverage orthogonal polynomials. Additionally, we provide a sample complexity lower bound based on Fano's inequality, and discuss broader implications of our results for two-layer polynomial networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05155</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05155</id><created>2025-02-07</created><authors><author><keyname>Tang</keyname><forenames>Shiqin</forenames></author><author><keyname>Yu</keyname><forenames>Shujian</forenames></author><author><keyname>Dong</keyname><forenames>Yining</forenames></author><author><keyname>Qin</keyname><forenames>S. Joe</forenames></author></authors><title>Deep Dynamic Probabilistic Canonical Correlation Analysis</title><categories>cs.LG stat.ML</categories><comments>accepted by ICASSP-25, code is available at   \url{https://github.com/marcusstang/D2PCCA}</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents Deep Dynamic Probabilistic Canonical Correlation Analysis (D2PCCA), a model that integrates deep learning with probabilistic modeling to analyze nonlinear dynamical systems. Building on the probabilistic extensions of Canonical Correlation Analysis (CCA), D2PCCA captures nonlinear latent dynamics and supports enhancements such as KL annealing for improved convergence and normalizing flows for a more flexible posterior approximation. D2PCCA naturally extends to multiple observed variables, making it a versatile tool for encoding prior knowledge about sequential datasets and providing a probabilistic understanding of the system's dynamics. Experimental validation on real financial datasets demonstrates the effectiveness of D2PCCA and its extensions in capturing latent dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05161</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05161</id><created>2025-02-07</created><authors><author><keyname>Antonczak</keyname><forenames>Brittany</forenames></author><author><keyname>Fay</keyname><forenames>Meg</forenames></author><author><keyname>Chawla</keyname><forenames>Aviral</forenames></author><author><keyname>Rowangould</keyname><forenames>Gregory</forenames></author></authors><title>Estimated Roadway Segment Traffic Data by Vehicle Class for the United   States: A Machine Learning Approach</title><categories>stat.AP</categories><comments>14 pages including references, 4 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Highway Performance Monitoring System, managed by the Federal Highway Administration, provides essential data on average annual daily traffic across U.S. roadways, but it has limited representation of medium- and heavy-duty vehicles on non-interstate roads. This gap limits research and policy analysis on the impacts of truck traffic, especially concerning air quality and public health. To address this, we use random forest regression to estimate medium- and heavy-duty vehicle traffic volumes in areas with sparse data. This results in a more comprehensive dataset, which enables the estimation of traffic density at the census block level as a proxy for traffic-related air pollution exposure. Our high-resolution spatial data products, rigorously validated, provide a more accurate representation of truck traffic and its environmental and health impacts. These datasets are valuable for transportation planning, public health research, and policy decisions aimed at mitigating the effects of truck traffic on vulnerable communities exposed to air pollution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:1811.12852</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>1811.12852</id><created>2018-11-30</created><updated>2025-02-06</updated><authors><author><keyname>Burnetas</keyname><forenames>Apostolos N.</forenames></author><author><keyname>Kanavetas</keyname><forenames>Odysseas</forenames></author><author><keyname>Katehakis</keyname><forenames>Michael N.</forenames></author></authors><title>Optimal Data Driven Resource Allocation under Multi-Armed Bandit   Observations</title><categories>stat.ML cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1509.02857</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the first asymptotically optimal strategy for a multi armed bandit (MAB) model under side constraints. The side constraints model situations in which bandit activations are limited by the availability of certain resources that are replenished at a constant rate. The main result involves the derivation of an asymptotic lower bound for the regret of feasible uniformly fast policies and the construction of policies that achieve this lower bound, under pertinent conditions. Further, we provide the explicit form of such policies for the case in which the unknown distributions are Normal with unknown means and known variances, for the case of Normal distributions with unknown means and unknown variances and for the case of arbitrary discrete distributions with finite support. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2202.04912</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2202.04912</id><created>2022-02-10</created><updated>2025-02-06</updated><authors><author><keyname>Qiu</keyname><forenames>Rui</forenames></author><author><keyname>Yu</keyname><forenames>Zhou</forenames></author><author><keyname>Zhu</keyname><forenames>Ruoqing</forenames></author></authors><title>Random Forest Weighted Local Fr\'echet Regression with Random Objects</title><categories>stat.ML cs.LG</categories><comments>This paper has been published in the Journal of Machine Learning   Research</comments><journal-ref>Journal of Machine Learning Research 25 (2024) 1-69</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical analysis is increasingly confronted with complex data from metric spaces. Petersen and M\"uller (2019) established a general paradigm of Fr\'echet regression with complex metric space valued responses and Euclidean predictors. However, the local approach therein involves nonparametric kernel smoothing and suffers from the curse of dimensionality. To address this issue, we in this paper propose a novel random forest weighted local Fr\'echet regression paradigm. The main mechanism of our approach relies on a locally adaptive kernel generated by random forests. Our first method uses these weights as the local average to solve the conditional Fr\'echet mean, while the second method performs local linear Fr\'echet regression, both significantly improving existing Fr\'echet regression methods. Based on the theory of infinite order U-processes and infinite order $M_{m_n}$-estimator, we establish the consistency, rate of convergence, and asymptotic normality for our local constant estimator, which covers the current large sample theory of random forests with Euclidean responses as a special case. Numerical studies show the superiority of our methods with several commonly encountered types of responses such as distribution functions, symmetric positive-definite matrices, and sphere data. The practical merits of our proposals are also demonstrated through the application to New York taxi data and human mortality data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2206.04902</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2206.04902</id><created>2022-06-10</created><updated>2024-11-13</updated><authors><author><keyname>Gruber</keyname><forenames>Luis</forenames></author><author><keyname>Kastner</keyname><forenames>Gregor</forenames></author></authors><title>Forecasting macroeconomic data with Bayesian VARs: Sparse or dense? It   depends!</title><categories>econ.EM stat.AP stat.ME</categories><journal-ref>International Journal of Forecasting (2025)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vector autogressions (VARs) are widely applied when it comes to modeling and forecasting macroeconomic variables. In high dimensions, however, they are prone to overfitting. Bayesian methods, more concretely shrinkage priors, have shown to be successful in improving prediction performance. In the present paper, we introduce the semi-global framework, in which we replace the traditional global shrinkage parameter with group-specific shrinkage parameters. We show how this framework can be applied to various shrinkage priors, such as global-local priors and stochastic search variable selection priors. We demonstrate the virtues of the proposed framework in an extensive simulation study and in an empirical application forecasting data of the US economy. Further, we shed more light on the ongoing ``Illusion of Sparsity'' debate, finding that forecasting performances under sparse/dense priors vary across evaluated economic variables and across time frames. Dynamic model averaging, however, can combine the merits of both worlds. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2211.15353</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2211.15353</id><created>2022-11-25</created><updated>2025-02-07</updated><authors><author><keyname>Letizia</keyname><forenames>Nunzio A.</forenames></author><author><keyname>Tonello</keyname><forenames>Andrea M.</forenames></author></authors><title>Copula Density Neural Estimation</title><categories>cs.LG eess.SP stat.ML</categories><comments>6 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Probability density estimation from observed data constitutes a central task in statistics. Recent advancements in machine learning offer new tools but also pose new challenges. The big data era demands analysis of long-range spatial and long-term temporal dependencies in large collections of raw data, rendering neural networks an attractive solution for density estimation. In this paper, we exploit the concept of copula to explicitly build an estimate of the probability density function associated to any observed data. In particular, we separate univariate marginal distributions from the joint dependence structure in the data, the copula itself, and we model the latter with a neural network-based method referred to as copula density neural estimation (CODINE). Results show that the novel learning approach is capable of modeling complex distributions and it can be applied for mutual information estimation and data generation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2212.09544</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2212.09544</id><created>2022-12-19</created><authors><author><keyname>Gardini</keyname><forenames>Aldo</forenames></author><author><keyname>Greco</keyname><forenames>Fedele</forenames></author><author><keyname>Trivisano</keyname><forenames>Carlo</forenames></author></authors><title>Design and Structure Dependent Priors for Scale Parameters in Latent   Gaussian Models</title><categories>stat.ME math.ST stat.TH</categories><doi>10.1214/24-BA1454</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Many common correlation structures assumed for data can be described through latent Gaussian models. When Bayesian inference is carried out, it is required to set the prior distribution for scale parameters that rules the model components, possibly allowing to incorporate prior information. This task is particularly delicate and many contributions in the literature are devoted to investigating such aspects. We focus on the fact that the scale parameter controls the prior variability of the model component in a complex way since its dispersion is also affected by the correlation structure and the design. To overcome this issue that might confound the prior elicitation step, we propose to let the user specify the marginal prior of a measure of dispersion of the model component, integrating out the scale parameter, the structure and the design. Then, we analytically derive the implied prior for the scale parameter. Results from a simulation study, aimed at showing the behavior of the estimators sampling properties under the proposed prior elicitation strategy, are discussed. Lastly, some real data applications are explored to investigate prior sensitivity and allocation of explained variance among model components. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2304.13077</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2304.13077</id><created>2023-04-25</created><updated>2025-02-07</updated><authors><author><keyname>De Vito</keyname><forenames>Roberta</forenames></author><author><keyname>Avalos-Pacheco</keyname><forenames>Alejandra</forenames></author></authors><title>Multi-study factor regression model: an application in nutritional   epidemiology</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diet is a risk factor for many diseases. In nutritional epidemiology, studying reproducible dietary patterns is critical to reveal important associations with health. However, it is challenging: diverse cultural and ethnic backgrounds may critically impact eating patterns, showing heterogeneity, leading to incorrect dietary patterns and obscuring the components shared across different groups or populations. Moreover, covariate effects generated from observed variables, such as demographics and other confounders, can further bias these dietary patterns. Identifying the shared and group-specific dietary components and covariate effects is essential to drive accurate conclusions. To address these issues, we introduce a new modeling factor regression, the Multi-Study Factor Regression (MSFR) model. The MSFR model analyzes different populations simultaneously, achieving three goals: capturing shared component(s) across populations, identifying group-specific structures, and correcting for covariate effects. We use this novel method to derive common and ethnic-specific dietary patterns in a multi-center epidemiological study in Hispanic/Latinos community. Our model improves the accuracy of common and group dietary signals and yields better prediction than other techniques, revealing significant associations with health. In summary, we provide a tool to integrate different groups, giving accurate dietary signals crucial to inform public health policy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2305.13998</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2305.13998</id><created>2023-05-23</created><updated>2024-01-23</updated><authors><author><keyname>Saves</keyname><forenames>Paul</forenames></author><author><keyname>Lafage</keyname><forenames>Remi</forenames></author><author><keyname>Bartoli</keyname><forenames>Nathalie</forenames></author><author><keyname>Diouane</keyname><forenames>Youssef</forenames></author><author><keyname>Bussemaker</keyname><forenames>Jasper</forenames></author><author><keyname>Lefebvre</keyname><forenames>Thierry</forenames></author><author><keyname>Hwang</keyname><forenames>John T.</forenames></author><author><keyname>Morlier</keyname><forenames>Joseph</forenames></author><author><keyname>Martins</keyname><forenames>Joaquim R. R. A.</forenames></author></authors><title>SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and   Mixed Variables Gaussian Processes</title><categories>cs.LG cs.MS math.OC stat.CO</categories><comments>10.1016/j.advengsoft.2023.103571</comments><journal-ref>Advances in Engineering Software Volume 188, February 2024, 103571</journal-ref><doi>10.1016/j.advengsoft.2023.103571</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multifidelity data. To the best of our knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical and mixed inputs. This open-source software is distributed under the New BSD license. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2306.16033</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2306.16033</id><created>2023-06-28</created><authors><author><keyname>Gardini</keyname><forenames>Aldo</forenames></author></authors><title>Functional and variables selection in extreme value models for regional   flood frequency analysis</title><categories>stat.ME stat.AP</categories><doi>10.1007/s10651-023-00581-8</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The problem of estimating return levels of river discharge, relevant in flood frequency analysis, is tackled by relying on the extreme value theory. The Generalized Extreme Value (GEV) distribution is assumed to model annual maxima values of river discharge registered at multiple gauging stations belonging to the same river basin. The specific features of the data from the Upper Danube basin drive the definition of the proposed statistical model. Firstly, Bayesian P-splines are considered to account for the non-linear effects of station-specific covariates on the GEV parameters. Secondly, the problem of functional and variable selection is addressed by imposing a grouped horseshoe prior on the coefficients, to encourage the shrinkage of non-relevant components to zero. A cross-validation study is organized to compare the proposed modeling solution to other models, showing its potential in reducing the uncertainty of the ungauged predictions without affecting their calibration. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2307.10272</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2307.10272</id><created>2023-07-18</created><updated>2025-02-06</updated><authors><author><keyname>Takeishi</keyname><forenames>Shota</forenames></author></authors><title>A Shrinkage Likelihood Ratio Test for High-Dimensional Subgroup Analysis   with a Logistic-Normal Mixture Model</title><categories>math.ST stat.ME stat.TH</categories><comments>38 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In subgroup analysis, testing the existence of a subgroup with a differential treatment effect serves as protection against spurious subgroup discovery. Despite its importance, this hypothesis testing possesses a complicated nature: parameter characterizing subgroup classification is not identified under the null hypothesis of no subgroup. Due to this irregularity, the existing methods have the following two limitations. First, the asymptotic null distribution of test statistics often takes an intractable form, which necessitates computationally demanding resampling methods to calculate the critical value. Second, the dimension of personal attributes characterizing subgroup membership is not allowed to be of high dimension. To solve these two problems simultaneously, this study develops a shrinkage likelihood ratio test for the existence of a subgroup using a logistic-normal mixture model. The proposed test statistics are built on a modified likelihood function that shrinks possibly high-dimensional unidentified parameters toward zero under the null hypothesis while retaining power under the alternative. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2307.12982</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2307.12982</id><created>2023-07-24</created><updated>2025-02-07</updated><authors><author><keyname>Mukherjee</keyname><forenames>Soumendu Sundar</forenames></author></authors><title>Consistent model selection in the spiked Wigner model via AIC-type   criteria</title><categories>math.ST cs.IT math.IT stat.ME stat.ML stat.TH</categories><comments>25 pages, 2 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the spiked Wigner model \[   X = \sum_{i = 1}^k \lambda_i u_i u_i^\top + \sigma G, \] where $G$ is an $N \times N$ GOE random matrix, and the eigenvalues $\lambda_i$ are all spiked, i.e. above the Baik-Ben Arous-P\'ech\'e (BBP) threshold $\sigma$. We consider AIC-type model selection criteria of the form \[   -2 \, (\text{maximised log-likelihood}) + \gamma \, (\text{number of parameters}) \] for estimating the number $k$ of spikes. For $\gamma &gt; 2$, the above criterion is strongly consistent provided $\lambda_k &gt; \lambda_{\gamma}$, where $\lambda_{\gamma}$ is a threshold strictly above the BBP threshold, whereas for $\gamma &lt; 2$, it almost surely overestimates $k$. Although AIC (which corresponds to $\gamma = 2$) is not strongly consistent, we show that taking $\gamma = 2 + \delta_N$, where $\delta_N \to 0$ and $\delta_N \gg N^{-2/3}$, results in a weakly consistent estimator of $k$. We further show that a soft minimiser of AIC, where one chooses the least complex model whose AIC score is close to the minimum AIC score, is strongly consistent. Based on a spiked (generalised) Wigner representation, we also develop similar model selection criteria for consistently estimating the number of communities in a balanced stochastic block model under some sparsity restrictions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.07399</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.07399</id><created>2023-10-11</created><updated>2024-10-03</updated><authors><author><keyname>Bou-Rabee</keyname><forenames>Nawaf</forenames></author><author><keyname>Kleppe</keyname><forenames>Tore Selland</forenames></author></authors><title>Randomized Runge-Kutta-Nystr\"om Methods for Unadjusted Hamiltonian and   Kinetic Langevin Monte Carlo</title><categories>math.NA cs.NA math.PR stat.CO stat.ME stat.ML</categories><msc-class>60J05 (Primary) 65C05, 65L05, 65P10 (Secondary)</msc-class><journal-ref>Mathematics of Computation 2025</journal-ref><doi>10.1090/mcom/4061</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce $5/2$- and $7/2$-order $L^2$-accurate randomized Runge-Kutta-Nystr\"{o}m methods, tailored for approximating Hamiltonian flows within non-reversible Markov chain Monte Carlo samplers, such as unadjusted Hamiltonian Monte Carlo and unadjusted kinetic Langevin Monte Carlo. We establish quantitative $5/2$-order $L^2$-accuracy upper bounds under gradient and Hessian Lipschitz assumptions on the potential energy function. The numerical experiments demonstrate the superior efficiency of the proposed unadjusted samplers on a variety of well-behaved, high-dimensional target distributions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2311.02655</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2311.02655</id><created>2023-11-05</created><updated>2025-02-06</updated><authors><author><keyname>Horst</keyname><forenames>Ulrich</forenames></author><author><keyname>Xu</keyname><forenames>Wei</forenames></author></authors><title>Second-Order Regular Variation and Second-Order Approximation of Hawkes   Processes</title><categories>math.PR math.FA math.ST stat.TH</categories><comments>40 pages</comments><msc-class>Primary 26A12, 40E05, secondary 60G55, 60K05</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper provides and extends second-order versions of several fundamental theorems on first-order regularly varying functions such as Karamata's theorem/representation and Tauberian's theorem. Our results are used to establish second-order approximations for the mean and variance of Hawkes processes with general kernels. Our approximations provide novel insights into the asymptotic behavior of Hawkes processes. They are also of key importance when establishing functional limit theorems for Hawkes processes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.04711</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.04711</id><created>2024-02-07</created><updated>2024-05-26</updated><authors><author><keyname>Saves</keyname><forenames>Paul</forenames></author></authors><title>High-dimensional multidisciplinary design optimization for aircraft   eco-design / Optimisation multi-disciplinaire en grande dimension pour   l'\'eco-conception avion en avant-projet</title><categories>math.OC cs.MS stat.ML</categories><comments>PhD Thesis, Universit\'e de Toulouse, Toulouse, 2024 on Gaussian   Process kernels for Bayesian optimization in high dimension with mixed and   hierarchical variables at ISAE-SUPAERO. Keywords: Gaussian process, Black-box   optimization, Bayesian inference, Multidisciplinary design optimization,   Mixed hierarchical and categorical inputs, Eco-friendly aircraft design</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The objective of this Philosophiae Doctor (Ph.D) thesis is to propose an efficient approach for optimizing a multidisciplinary black-box model when the optimization problem is constrained and involves a large number of mixed integer design variables (typically 100 variables). The targeted optimization approach, called EGO, is based on a sequential enrichment of an adaptive surrogate model and, in this context, GP surrogate models are one of the most widely used in engineering problems to approximate time-consuming high fidelity models. EGO is a heuristic BO method that performs well in terms of solution quality. However, like any other global optimization method, EGO suffers from the curse of dimensionality, meaning that its performance is satisfactory on lower dimensional problems, but deteriorates as the dimensionality of the optimization search space increases. For realistic aircraft design problems, the typical size of the design variables can even exceed 100 and, thus, trying to solve directly the problems using EGO is ruled out. The latter is especially true when the problems involve both continuous and categorical variables increasing even more the size of the search space. In this Ph.D thesis, effective parameterization tools are investigated, including techniques like partial least squares regression, to significantly reduce the number of design variables. Additionally, Bayesian optimization is adapted to handle discrete variables and high-dimensional spaces in order to reduce the number of evaluations when optimizing innovative aircraft concepts such as the "DRAGON" hybrid airplane to reduce their climate impact. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.13196</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.13196</id><created>2024-03-19</created><updated>2025-02-07</updated><authors><author><keyname>Eskandar</keyname><forenames>Masih</forenames></author><author><keyname>Imtiaz</keyname><forenames>Tooba</forenames></author><author><keyname>Wang</keyname><forenames>Zifeng</forenames></author><author><keyname>Dy</keyname><forenames>Jennifer</forenames></author></authors><title>ADAPT to Robustify Prompt Tuning Vision Transformers</title><categories>cs.LG cs.AI cs.CV stat.ML</categories><comments>Published in Transactions on Machine Learning Research (2025)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The performance of deep models, including Vision Transformers, is known to be vulnerable to adversarial attacks. Many existing defenses against these attacks, such as adversarial training, rely on full-model fine-tuning to induce robustness in the models. These defenses require storing a copy of the entire model, that can have billions of parameters, for each task. At the same time, parameter-efficient prompt tuning is used to adapt large transformer-based models to downstream tasks without the need to save large copies. In this paper, we examine parameter-efficient prompt tuning of Vision Transformers for downstream tasks under the lens of robustness. We show that previous adversarial defense methods, when applied to the prompt tuning paradigm, suffer from gradient obfuscation and are vulnerable to adaptive attacks. We introduce ADAPT, a novel framework for performing adaptive adversarial training in the prompt tuning paradigm. Our method achieves competitive robust accuracy of ~40% w.r.t. SOTA robustness methods using full-model fine-tuning, by tuning only ~1% of the number of parameters. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.13340</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.13340</id><created>2024-03-20</created><updated>2025-02-07</updated><authors><author><keyname>Jiménez-Varón</keyname><forenames>Cristian F.</forenames></author><author><keyname>Sun</keyname><forenames>Ying</forenames></author><author><keyname>Shang</keyname><forenames>Han Lin</forenames></author></authors><title>Forecasting density-valued functional panel data</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce a statistical method for modeling and forecasting functional panel data represented by multiple densities. Density functions are nonnegative and have a constrained integral and thus do not constitute a linear vector space. We implement a center log-ratio transformation to transform densities into unconstrained functions. These functions exhibit cross-sectional correlation and temporal dependence. Via a functional analysis of variance decomposition, we decompose the unconstrained functional panel data into a deterministic trend component and a time-varying residual component. To produce forecasts for the time-varying component, a functional time series forecasting method, based on the estimation of the long-run covariance, is implemented. By combining the forecasts of the time-varying residual component with the deterministic trend component, we obtain $h$-step-ahead forecast curves for multiple populations. Illustrated by age- and sex-specific life-table death counts in the United States, we apply our proposed method to generate forecasts of the life-table death counts for 51 states. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.12120</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.12120</id><created>2024-06-17</created><updated>2025-02-06</updated><authors><author><keyname>Zhao</keyname><forenames>Yulai</forenames></author><author><keyname>Uehara</keyname><forenames>Masatoshi</forenames></author><author><keyname>Scalia</keyname><forenames>Gabriele</forenames></author><author><keyname>Kung</keyname><forenames>Sunyuan</forenames></author><author><keyname>Biancalani</keyname><forenames>Tommaso</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Hajiramezanali</keyname><forenames>Ehsan</forenames></author></authors><title>Adding Conditional Control to Diffusion Models with Reinforcement   Learning</title><categories>cs.LG cs.AI stat.ML</categories><comments>ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diffusion models are powerful generative models that allow for precise control over the characteristics of the generated samples. While these diffusion models trained on large datasets have achieved success, there is often a need to introduce additional controls in downstream fine-tuning processes, treating these powerful models as pre-trained diffusion models. This work presents a novel method based on reinforcement learning (RL) to add such controls using an offline dataset comprising inputs and labels. We formulate this task as an RL problem, with the classifier learned from the offline dataset and the KL divergence against pre-trained models serving as the reward functions. Our method, $\textbf{CTRL}$ ($\textbf{C}$onditioning pre-$\textbf{T}$rained diffusion models with $\textbf{R}$einforcement $\textbf{L}$earning), produces soft-optimal policies that maximize the abovementioned reward functions. We formally demonstrate that our method enables sampling from the conditional distribution with additional controls during inference. Our RL-based approach offers several advantages over existing methods. Compared to classifier-free guidance, it improves sample efficiency and can greatly simplify dataset construction by leveraging conditional independence between the inputs and additional controls. Additionally, unlike classifier guidance, it eliminates the need to train classifiers from intermediate states to additional controls. The code is available at https://github.com/zhaoyl18/CTRL. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.14535</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.14535</id><created>2024-06-20</created><updated>2025-02-06</updated><authors><author><keyname>Deng</keyname><forenames>Shiyuan</forenames></author><author><keyname>Tang</keyname><forenames>He</forenames></author><author><keyname>Bai</keyname><forenames>Shuyang</forenames></author></authors><title>On estimation and order selection for multivariate extremes via   clustering</title><categories>stat.ME math.ST stat.TH</categories><msc-class>62G32 (Primary), 60G70 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the estimation of multivariate extreme models with a discrete spectral measure using spherical clustering techniques. The primary contribution involves devising a method for selecting the order, that is, the number of clusters. The method consistently identifies the true order, i.e., the number of spectral atoms, and enjoys intuitive implementation in practice. Specifically, we introduce an extra penalty term to the well-known simplified average silhouette width, which penalizes small cluster sizes and small dissimilarities between cluster centers. Consequently, we provide a consistent method for determining the order of a max-linear factor model, where a typical information-based approach is not viable. Our second contribution is a large-deviation-type analysis for estimating the discrete spectral measure through clustering methods, which serves as an assessment of the convergence quality of clustering-based estimation for multivariate extremes. Additionally, as a third contribution, we discuss how estimating the discrete measure can lead to parameter estimations of heavy-tailed factor models. We also present simulations and real-data studies that demonstrate order selection and factor model estimation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.06120</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.06120</id><created>2024-07-08</created><updated>2024-11-16</updated><authors><author><keyname>Dong</keyname><forenames>Yijun</forenames></author><author><keyname>Phan</keyname><forenames>Hoang</forenames></author><author><keyname>Pan</keyname><forenames>Xiang</forenames></author><author><keyname>Lei</keyname><forenames>Qi</forenames></author></authors><title>Sketchy Moment Matching: Toward Fast and Provable Data Selection for   Finetuning</title><categories>cs.LG cs.NA math.NA stat.ML</categories><comments>NeurIPS 2024</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit data selection in a modern context of finetuning from a fundamental perspective. Extending the classical wisdom of variance minimization in low dimensions to high-dimensional finetuning, our generalization analysis unveils the importance of additionally reducing bias induced by low-rank approximation. Inspired by the variance-bias tradeoff in high dimensions from the theory, we introduce Sketchy Moment Matching (SkMM), a scalable data selection scheme with two stages. (i) First, the bias is controlled using gradient sketching that explores the finetuning parameter space for an informative low-dimensional subspace $\mathcal{S}$; (ii) then the variance is reduced over $\mathcal{S}$ via moment matching between the original and selected datasets. Theoretically, we show that gradient sketching is fast and provably accurate: selecting $n$ samples by reducing variance over $\mathcal{S}$ preserves the fast-rate generalization $O(\dim(\mathcal{S})/n)$, independent of the parameter dimension. Empirically, we concretize the variance-bias balance via synthetic experiments and demonstrate the effectiveness of SkMM for finetuning in real vision tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.21314</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.21314</id><created>2024-07-30</created><updated>2025-02-07</updated><authors><author><keyname>Li</keyname><forenames>Zhuoyuan</forenames></author><author><keyname>Dong</keyname><forenames>Bin</forenames></author><author><keyname>Zhang</keyname><forenames>Pingwen</forenames></author></authors><title>State-observation augmented diffusion model for nonlinear assimilation   with unknown dynamics</title><categories>cs.LG stat.ML</categories><msc-class>49N45, 60J60, 62F15, 68T20</msc-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Data assimilation has become a key technique for combining physical models with observational data to estimate state variables. However, classical assimilation algorithms often struggle with the high nonlinearity present in both physical and observational models. To address this challenge, a novel generative model, termed the State-Observation Augmented Diffusion (SOAD) model is proposed for data-driven assimilation. The marginal posterior associated with SOAD has been derived and then proved to match the true posterior distribution under mild assumptions, suggesting its theoretical advantages over previous score-based approaches. Experimental results also indicate that SOAD may offer improved performance compared to existing data-driven methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.03733</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.03733</id><created>2024-08-07</created><authors><author><keyname>Maillard</keyname><forenames>Antoine</forenames></author><author><keyname>Troiani</keyname><forenames>Emanuele</forenames></author><author><keyname>Martin</keyname><forenames>Simon</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Zdeborová</keyname><forenames>Lenka</forenames></author></authors><title>Bayes-optimal learning of an extensive-width neural network from   quadratically many samples</title><categories>stat.ML cond-mat.dis-nn cs.IT cs.LG math.IT math.PR</categories><comments>47 pages</comments><journal-ref>Advances in Neural Information Processing Systems 37 (NeurIPS   2024)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning a target function corresponding to a single hidden layer neural network, with a quadratic activation function after the first layer, and random weights. We consider the asymptotic limit where the input dimension and the network width are proportionally large. Recent work [Cui &amp; al '23] established that linear regression provides Bayes-optimal test error to learn such a function when the number of available samples is only linear in the dimension. That work stressed the open challenge of theoretically analyzing the optimal test error in the more interesting regime where the number of samples is quadratic in the dimension. In this paper, we solve this challenge for quadratic activations and derive a closed-form expression for the Bayes-optimal test error. We also provide an algorithm, that we call GAMP-RIE, which combines approximate message passing with rotationally invariant matrix denoising, and that asymptotically achieves the optimal performance. Technically, our result is enabled by establishing a link with recent works on optimal denoising of extensive-rank matrices and on the ellipsoid fitting problem. We further show empirically that, in the absence of noise, randomly-initialized gradient descent seems to sample the space of weights, leading to zero training loss, and averaging over initialization leads to a test error equal to the Bayes-optimal one. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.09004</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.09004</id><created>2024-08-16</created><updated>2025-02-06</updated><authors><author><keyname>Subedi</keyname><forenames>Unique</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Controlling Statistical, Discretization, and Truncation Errors in   Learning Fourier Linear Operators</title><categories>stat.ML cs.LG cs.NA math.NA</categories><comments>Added Experiments</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study learning-theoretic foundations of operator learning, using the linear layer of the Fourier Neural Operator architecture as a model problem. First, we identify three main errors that occur during the learning process: statistical error due to finite sample size, truncation error from finite rank approximation of the operator, and discretization error from handling functional data on a finite grid of domain points. Finally, we analyze a Discrete Fourier Transform (DFT) based least squares estimator, establishing both upper and lower bounds on the aforementioned errors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.09576</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.09576</id><created>2024-08-18</created><updated>2025-02-07</updated><authors><author><keyname>Oubari</keyname><forenames>Fouad</forenames></author><author><keyname>Baha</keyname><forenames>Mohamed El</forenames></author><author><keyname>Meunier</keyname><forenames>Raphael</forenames></author><author><keyname>Décatoire</keyname><forenames>Rodrigue</forenames></author><author><keyname>Mougeot</keyname><forenames>Mathilde</forenames></author></authors><title>A Markov Random Field Multi-Modal Variational AutoEncoder</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Recent advancements in multimodal Variational AutoEncoders (VAEs) have highlighted their potential for modeling complex data from multiple modalities. However, many existing approaches use relatively straightforward aggregating schemes that may not fully capture the complex dynamics present between different modalities. This work introduces a novel multimodal VAE that incorporates a Markov Random Field (MRF) into both the prior and posterior distributions. This integration aims to capture complex intermodal interactions more effectively. Unlike previous models, our approach is specifically designed to model and leverage the intricacies of these relationships, enabling a more faithful representation of multimodal data. Our experiments demonstrate that our model performs competitively on the standard PolyMNIST dataset and shows superior performance in managing complex intermodal dependencies in a specially designed synthetic dataset, intended to test intricate relationships. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.02363</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.02363</id><created>2024-09-03</created><updated>2025-02-06</updated><authors><author><keyname>Maiti</keyname><forenames>Ayan</forenames></author><author><keyname>Michelle</keyname><forenames>Michelle</forenames></author><author><keyname>Yang</keyname><forenames>Haizhao</forenames></author></authors><title>Optimal Neural Network Approximation for High-Dimensional Continuous   Functions</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the authors of Shen Yang Zhang (JMLR, 2022) developed a neural network with width $36d(2d + 1)$ and depth $11$, which utilizes a special activation function called the elementary universal activation function, to achieve the super approximation property for functions in $C([a,b]^d)$. That is, the constructed network only requires a fixed number of neurons (and thus parameters) to approximate a $d$-variate continuous function on a $d$-dimensional hypercube with arbitrary accuracy. More specifically, only $\mathcal{O}(d^2)$ neurons or parameters are used. One natural question is whether we can reduce the number of these neurons or parameters in such a network. By leveraging a variant of the Kolmogorov Superposition Theorem, our analysis shows that there is a neural network generated by the elementary universal activation function with at most $10889d+10887$ unique nonzero parameters such that this super approximation property is attained. Furthermore, we present a family of continuous functions that requires at least width $d$, and thus at least $d$ neurons or parameters, to achieve arbitrary accuracy in its approximation. This suggests that the number of unique nonzero parameters is optimal in the sense that it grows linearly with the input dimension $d$, unlike some approximation methods where parameters may grow exponentially with $d$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.09800</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.09800</id><created>2024-09-15</created><updated>2025-02-06</updated><authors><author><keyname>Calvello</keyname><forenames>Edoardo</forenames></author><author><keyname>Monmarché</keyname><forenames>Pierre</forenames></author><author><keyname>Stuart</keyname><forenames>Andrew M.</forenames></author><author><keyname>Vaes</keyname><forenames>Urbain</forenames></author></authors><title>Accuracy of the Ensemble Kalman Filter in the Near-Linear Setting</title><categories>math.ST cs.NA math.DS math.NA math.OC stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The filtering distribution captures the statistics of the state of a dynamical system from partial and noisy observations. Classical particle filters provably approximate this distribution in quite general settings; however they behave poorly for high dimensional problems, suffering weight collapse. This issue is circumvented by the ensemble Kalman filter which is an equal-weight interacting particle system. However, this finite particle system is only proven to approximate the true filter in the linear Gaussian case. In practice, however, it is applied in much broader settings; as a result, establishing its approximation properties more generally is important. There has been recent progress in the theoretical analysis of the algorithm, establishing stability and error estimates in non-Gaussian settings, but the assumptions on the dynamics and observation models rule out the unbounded vector fields that arise in practice and the analysis applies only to the mean field limit of the ensemble Kalman filter. The present work establishes error bounds between the filtering distribution and the finite particle ensemble Kalman filter when the dynamics and observation vector fields may be unbounded, allowing linear growth. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.11381</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.11381</id><created>2024-09-17</created><updated>2025-02-07</updated><authors><author><keyname>Banerjee</keyname><forenames>Debapratim</forenames></author><author><keyname>Mukherjee</keyname><forenames>Soumendu Sundar</forenames></author><author><keyname>Pal</keyname><forenames>Dipranjan</forenames></author></authors><title>Edge spectra of Gaussian random symmetric matrices with correlated   entries</title><categories>math.PR math-ph math.CO math.MP math.ST stat.TH</categories><comments>27 pages, 2 figures; abstract shortened to meet arXiv requirements</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the largest eigenvalue of a Gaussian random symmetric matrix $X_n$, with zero-mean, unit variance entries satisfying the condition $\sup_{(i, j) \ne (i', j')}|\mathbb{E}[X_{ij} X_{i'j'}]| = O(n^{-(1 + \varepsilon)})$, where $\varepsilon &gt; 0$. It follows from Catalano et al. (2024) that the empirical spectral distribution of $n^{-1/2} X_n$ converges weakly almost surely to the standard semi-circle law. Using a F\"{u}redi-Koml\'{o}s-type high moment analysis, we show that the largest eigenvalue $\lambda_1(n^{-1/2} X_n)$ of $n^{-1/2} X_n$ converges almost surely to $2$. This result is essentially optimal in the sense that one cannot take $\varepsilon = 0$ and still obtain an almost sure limit of $2$. We also derive Gaussian fluctuation results for the largest eigenvalue in the case where the entries have a common non-zero mean. Let $Y_n = X_n + \frac{\lambda}{\sqrt{n}}\mathbf{1} \mathbf{1}^\top$. When $\varepsilon \ge 1$ and $\lambda \gg n^{1/4}$, we show that \[   n^{1/2}\bigg(\lambda_1(n^{-1/2} Y_n) - \lambda - \frac{1}{\lambda}\bigg) \xrightarrow{d} \sqrt{2} Z, \] where $Z$ is a standard Gaussian. On the other hand, when $0 &lt; \varepsilon &lt; 1$, we have $\mathrm{Var}(\frac{1}{n}\sum_{i, j}X_{ij}) = O(n^{1 - \varepsilon})$. Assuming that $\mathrm{Var}(\frac{1}{n}\sum_{i, j} X_{ij}) = \sigma^2 n^{1 - \varepsilon} (1 + o(1))$, if $\lambda \gg n^{\varepsilon/4}$, then we have \[   n^{\varepsilon/2}\bigg(\lambda_1(n^{-1/2} Y_n) - \lambda - \frac{1}{\lambda}\bigg) \xrightarrow{d} \sigma Z. \] While the ranges of $\lambda$ in these fluctuation results are certainly not optimal, a striking aspect is that different scalings are required in the two regimes $0 &lt; \varepsilon &lt; 1$ and $\varepsilon \ge 1$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.19200</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.19200</id><created>2024-09-27</created><updated>2025-02-06</updated><authors><author><keyname>Bai</keyname><forenames>Site</forenames></author><author><keyname>Bullins</keyname><forenames>Brian</forenames></author></authors><title>Faster Acceleration for Steepest Descent</title><categories>math.OC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances (Sherman, 2017; Sidford and Tian, 2018; Cohen et al., 2021) have overcome the fundamental barrier of dimension dependence in the iteration complexity of solving $\ell_\infty$ regression with first-order methods. Yet it remains unclear to what extent such acceleration can be achieved for general $\ell_p$ smooth functions. In this paper, we propose a new accelerated first-order method for convex optimization under non-Euclidean smoothness assumptions. In contrast to standard acceleration techniques, our approach uses primal-dual iterate sequences taken with respect to $\textit{differing}$ norms, which are then coupled using an $\textit{implicitly}$ determined interpolation parameter. For $\ell_p$ norm smooth problems in $d$ dimensions, our method provides an iteration complexity improvement of up to $O(d^{1-\frac{2}{p}})$ in terms of calls to a first-order oracle, thereby allowing us to circumvent long-standing barriers in accelerated non-Euclidean steepest descent. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.03159</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.03159</id><created>2024-10-04</created><updated>2025-02-07</updated><authors><author><keyname>Lu</keyname><forenames>Jiecheng</forenames></author><author><keyname>Han</keyname><forenames>Xu</forenames></author><author><keyname>Sun</keyname><forenames>Yan</forenames></author><author><keyname>Yang</keyname><forenames>Shihao</forenames></author></authors><title>WAVE: Weighted Autoregressive Varing Gate for Time Series Forecasting</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Weighted Autoregressive Varing gatE (WAVE) attention mechanism equipped with both Autoregressive (AR) and Moving-average (MA) components. It can adapt to various attention mechanisms, enhancing and decoupling their ability to capture long-range and local temporal patterns in time series data. In this paper, we first demonstrate that, for the time series forecasting (TSF) task, the previously overlooked decoder-only autoregressive Transformer model can achieve results comparable to the best baselines when appropriate tokenization and training methods are applied. Moreover, inspired by the ARMA model from statistics and recent advances in linear attention, we introduce the full ARMA structure into existing autoregressive attention mechanisms. By using an indirect MA weight generation method, we incorporate the MA term while maintaining the time complexity and parameter size of the underlying efficient attention models. We further explore how indirect parameter generation can produce implicit MA weights that align with the modeling requirements for local temporal impacts. Experimental results show that WAVE attention that incorporates the ARMA structure consistently improves the performance of various AR attentions on TSF tasks, achieving state-of-the-art results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.15361</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.15361</id><created>2024-10-20</created><updated>2025-02-06</updated><authors><author><keyname>Zhou</keyname><forenames>Han</forenames></author><author><keyname>Van Landeghem</keyname><forenames>Jordy</forenames></author><author><keyname>Popordanoska</keyname><forenames>Teodora</forenames></author><author><keyname>Blaschko</keyname><forenames>Matthew B.</forenames></author></authors><title>A Novel Characterization of the Population Area Under the Risk Coverage   Curve (AURC) and Rates of Finite Sample Estimators</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The selective classifier (SC) has been proposed for rank based uncertainty thresholding, which could have applications in safety critical areas such as medical diagnostics, autonomous driving, and the justice system. The Area Under the Risk-Coverage Curve (AURC) has emerged as the foremost evaluation metric for assessing the performance of SC systems. In this work, we present a formal statistical formulation of population AURC, presenting an equivalent expression that can be interpreted as a reweighted risk function. Through Monte Carlo methods, we derive empirical AURC plug-in estimators for finite sample scenarios. The weight estimators associated with these plug-in estimators are shown to be consistent, with low bias and tightly bounded mean squared error (MSE). The plug-in estimators are proven to converge at a rate of $\mathcal{O}(\sqrt{\ln(n)/n})$ demonstrating statistical consistency. We empirically validate the effectiveness of our estimators through experiments across multiple datasets, model architectures, and confidence score functions (CSFs), demonstrating consistency and effectiveness in fine-tuning AURC performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.18959</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.18959</id><created>2024-10-24</created><updated>2025-02-06</updated><authors><author><keyname>Williams</keyname><forenames>Andrew Robert</forenames></author><author><keyname>Ashok</keyname><forenames>Arjun</forenames></author><author><keyname>Marcotte</keyname><forenames>Étienne</forenames></author><author><keyname>Zantedeschi</keyname><forenames>Valentina</forenames></author><author><keyname>Subramanian</keyname><forenames>Jithendaraa</forenames></author><author><keyname>Riachi</keyname><forenames>Roland</forenames></author><author><keyname>Requeima</keyname><forenames>James</forenames></author><author><keyname>Lacoste</keyname><forenames>Alexandre</forenames></author><author><keyname>Rish</keyname><forenames>Irina</forenames></author><author><keyname>Chapados</keyname><forenames>Nicolas</forenames></author><author><keyname>Drouin</keyname><forenames>Alexandre</forenames></author></authors><title>Context is Key: A Benchmark for Forecasting with Essential Textual   Information</title><categories>cs.LG cs.AI stat.ML</categories><comments>Preprint; under review. First two authors contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forecasting is a critical task in decision-making across numerous domains. While historical numerical data provide a start, they fail to convey the complete context for reliable and accurate predictions. Human forecasters frequently rely on additional information, such as background knowledge and constraints, which can efficiently be communicated through natural language. However, in spite of recent progress with LLM-based forecasters, their ability to effectively integrate this textual information remains an open question. To address this, we introduce "Context is Key" (CiK), a time-series forecasting benchmark that pairs numerical data with diverse types of carefully crafted textual context, requiring models to integrate both modalities; crucially, every task in CiK requires understanding textual context to be solved successfully. We evaluate a range of approaches, including statistical models, time series foundation models, and LLM-based forecasters, and propose a simple yet effective LLM prompting method that outperforms all other tested methods on our benchmark. Our experiments highlight the importance of incorporating contextual information, demonstrate surprising performance when using LLM-based forecasting models, and also reveal some of their critical shortcomings. This benchmark aims to advance multimodal forecasting by promoting models that are both accurate and accessible to decision-makers with varied technical expertise. The benchmark can be visualized at https://servicenow.github.io/context-is-key-forecasting/v0/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.19217</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.19217</id><created>2024-10-24</created><authors><author><keyname>Wu</keyname><forenames>Changlong</forenames></author><author><keyname>Grama</keyname><forenames>Ananth</forenames></author><author><keyname>Szpankowski</keyname><forenames>Wojciech</forenames></author></authors><title>No Free Lunch: Fundamental Limits of Learning Non-Hallucinating   Generative Models</title><categories>cs.LG cs.AI stat.ML</categories><journal-ref>International Conference on Learning Representations (ICLR 2025).   URL: https://openreview.net/pdf?id=OwNoTs2r8e</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative models have shown impressive capabilities in synthesizing high-quality outputs across various domains. However, a persistent challenge is the occurrence of "hallucinations", where the model produces outputs that are plausible but invalid. While empirical strategies have been explored to mitigate this issue, a rigorous theoretical understanding remains elusive. In this paper, we develop a theoretical framework to analyze the learnability of non-hallucinating generative models from a learning-theoretic perspective. Our results reveal that non-hallucinating learning is statistically impossible when relying solely on the training dataset, even for a hypothesis class of size two and when the entire training set is truthful. To overcome these limitations, we show that incorporating inductive biases aligned with the actual facts into the learning process is essential. We provide a systematic approach to achieve this by restricting the facts set to a concept class of finite VC-dimension and demonstrate its effectiveness under various learning paradigms. Although our findings are primarily conceptual, they represent a first step towards a principled approach to addressing hallucinations in learning generative models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.19725</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.19725</id><created>2024-10-25</created><updated>2025-02-06</updated><authors><author><keyname>Subedi</keyname><forenames>Unique</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>On the Benefits of Active Data Collection in Operator Learning</title><categories>stat.ML cs.LG</categories><comments>Moved Proofs to the Appendix</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study active data collection strategies for operator learning when the target operator is linear and the input functions are drawn from a mean-zero stochastic process with continuous covariance kernels. With an active data collection strategy, we establish an error convergence rate in terms of the decay rate of the eigenvalues of the covariance kernel. We can achieve arbitrarily fast error convergence rates with sufficiently rapid eigenvalue decay of the covariance kernels. This contrasts with the passive (i.i.d.) data collection strategies, where the convergence rate is never faster than linear decay ($\sim n^{-1}$). In fact, for our setting, we show a \emph{non-vanishing} lower bound for any passive data collection strategy, regardless of the eigenvalues decay rate of the covariance kernel. Overall, our results show the benefit of active data collection strategies in operator learning over their passive counterparts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.20640</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.20640</id><created>2024-10-27</created><updated>2025-02-07</updated><authors><author><keyname>Rivera</keyname><forenames>Eduardo Ochoa</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Near Optimal Pure Exploration in Logistic Bandits</title><categories>stat.ML cs.LG</categories><comments>25 pages, 2 figures. arXiv admin note: text overlap with   arXiv:2006.16073 by other authors</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bandit algorithms have garnered significant attention due to their practical applications in real-world scenarios. However, beyond simple settings such as multi-arm or linear bandits, optimal algorithms remain scarce. Notably, no optimal solution exists for pure exploration problems in the context of generalized linear model (GLM) bandits. In this paper, we narrow this gap and develop the first track-and-stop algorithm for general pure exploration problems under the logistic bandit called logistic track-and-stop (Log-TS). Log-TS is an efficient algorithm that asymptotically matches an approximation for the instance-specific lower bound of the expected sample complexity up to a logarithmic factor. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.03727</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.03727</id><created>2024-12-04</created><updated>2025-02-07</updated><authors><author><keyname>Zhang</keyname><forenames>Zhiheng</forenames></author><author><keyname>Wang</keyname><forenames>Zichen</forenames></author></authors><title>Online Experimental Design With Estimation-Regret Trade-off Under   Network Interference</title><categories>cs.LG math.OC math.ST stat.TH</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network interference has attracted significant attention in the field of causal inference, encapsulating various sociological behaviors where the treatment assigned to one individual within a network may affect the outcomes of others, such as their neighbors. A key challenge in this setting is that standard causal inference methods often assume independent treatment effects among individuals, which may not hold in networked environments. To estimate interference-aware causal effects, a traditional approach is to inherit the independent settings, where practitioners randomly assign experimental participants into different groups and compare their outcomes. While effective in offline settings, this strategy becomes problematic in sequential experiments, where suboptimal decision persists, leading to substantial regret. To address this issue, we introduce a unified interference-aware framework for online experimental design. Compared to existing studies, we extend the definition of arm space by utilizing the statistical concept of exposure mapping, which allows for a more flexible and context-aware representation of treatment effects in networked settings. Crucially, we establish a Pareto-optimal trade-off between estimation accuracy and regret under the network concerning both time period and arm space, which remains superior to baseline models even without network interference. Furthermore, we propose an algorithmic implementation and discuss its generalization across different learning settings and network topology </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.04767</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.04767</id><created>2024-12-05</created><updated>2025-02-06</updated><authors><author><keyname>Tian</keyname><forenames>Bowei</forenames></author><author><keyname>Wang</keyname><forenames>Ziyao</forenames></author><author><keyname>He</keyname><forenames>Shwai</forenames></author><author><keyname>Ye</keyname><forenames>Wanghao</forenames></author><author><keyname>Sun</keyname><forenames>Guoheng</forenames></author><author><keyname>Dai</keyname><forenames>Yucong</forenames></author><author><keyname>Wu</keyname><forenames>Yongkai</forenames></author><author><keyname>Li</keyname><forenames>Ang</forenames></author></authors><title>Towards counterfactual fairness through auxiliary variables</title><categories>cs.LG cs.DS stat.ML</categories><comments>arXiv admin note: text overlap with arXiv:2307.08232 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The challenge of balancing fairness and predictive accuracy in machine learning models, especially when sensitive attributes such as race, gender, or age are considered, has motivated substantial research in recent years. Counterfactual fairness ensures that predictions remain consistent across counterfactual variations of sensitive attributes, which is a crucial concept in addressing societal biases. However, existing counterfactual fairness approaches usually overlook intrinsic information about sensitive features, limiting their ability to achieve fairness while simultaneously maintaining performance. To tackle this challenge, we introduce EXOgenous Causal reasoning (EXOC), a novel causal reasoning framework motivated by exogenous variables. It leverages auxiliary variables to uncover intrinsic properties that give rise to sensitive attributes. Our framework explicitly defines an auxiliary node and a control node that contribute to counterfactual fairness and control the information flow within the model. Our evaluation, conducted on synthetic and real-world datasets, validates EXOC's superiority, showing that it outperforms state-of-the-art approaches in achieving counterfactual fairness. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.08435</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.08435</id><created>2024-12-11</created><updated>2025-02-07</updated><authors><author><keyname>Zhao</keyname><forenames>Lifan</forenames></author><author><keyname>Shen</keyname><forenames>Yanyan</forenames></author></authors><title>Proactive Model Adaptation Against Concept Drift for Online Time Series   Forecasting</title><categories>cs.LG cs.AI cs.CE stat.ML</categories><comments>Accepted by KDD 2025</comments><doi>10.1145/3690624.3709210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time series forecasting always faces the challenge of concept drift, where data distributions evolve over time, leading to a decline in forecast model performance. Existing solutions are based on online learning, which continually organize recent time series observations as new training samples and update model parameters according to the forecasting feedback on recent data. However, they overlook a critical issue: obtaining ground-truth future values of each sample should be delayed until after the forecast horizon. This delay creates a temporal gap between the training samples and the test sample. Our empirical analysis reveals that the gap can introduce concept drift, causing forecast models to adapt to outdated concepts. In this paper, we present Proceed, a novel proactive model adaptation framework for online time series forecasting. Proceed first estimates the concept drift between the recently used training samples and the current test sample. It then employs an adaptation generator to efficiently translate the estimated drift into parameter adjustments, proactively adapting the model to the test sample. To enhance the generalization capability of the framework, Proceed is trained on synthetic diverse concept drifts. Extensive experiments on five real-world datasets across various forecast models demonstrate that Proceed brings more performance improvements than the state-of-the-art online learning methods, significantly facilitating forecast models' resilience against concept drifts. Code is available at https://github.com/SJTU-DMTai/OnlineTSF. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.20471</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.20471</id><created>2024-12-29</created><updated>2025-02-07</updated><authors><author><keyname>Cai</keyname><forenames>Yang</forenames></author><author><keyname>Mitra</keyname><forenames>Siddharth</forenames></author><author><keyname>Wang</keyname><forenames>Xiuyuan</forenames></author><author><keyname>Wibisono</keyname><forenames>Andre</forenames></author></authors><title>On the Convergence of Min-Max Langevin Dynamics and Algorithm</title><categories>cs.GT cs.LG math.OC stat.ML</categories><comments>v2: Revised introduction and presentation of results</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study zero-sum games in the space of probability distributions over the Euclidean space $\mathbb{R}^d$ with entropy regularization, in the setting when the interaction function between the players is smooth and strongly convex-strongly concave. We prove an exponential convergence guarantee for the mean-field min-max Langevin dynamics to compute the equilibrium distribution of the zero-sum game. We also study the finite-particle approximation of the mean-field min-max Langevin dynamics, both in continuous and discrete times. We prove biased convergence guarantees for the continuous-time finite-particle min-max Langevin dynamics to the stationary mean-field equilibrium distribution with an explicit bias term which does not scale with the number of particles. We also prove biased convergence guarantees for the discrete-time finite-particle min-max Langevin algorithm to the stationary mean-field equilibrium distribution with an additional bias term which scales with the step size and the number of particles. This provides an explicit iteration complexity for the average particle along the finite-particle algorithm to approximately compute the equilibrium distribution of the zero-sum game. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.01437</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.01437</id><created>2024-12-23</created><updated>2025-02-06</updated><authors><author><keyname>Murphy</keyname><forenames>Charles</forenames></author><author><keyname>Lizotte</keyname><forenames>Simon</forenames></author><author><keyname>Thibault</keyname><forenames>François</forenames></author><author><keyname>Thibeault</keyname><forenames>Vincent</forenames></author><author><keyname>Desrosiers</keyname><forenames>Patrick</forenames></author><author><keyname>Allard</keyname><forenames>Antoine</forenames></author></authors><title>On the reconstruction limits of complex networks</title><categories>stat.AP cs.IT math.IT physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network reconstruction consists in retrieving the hidden interaction structure of a system from observations. Many reconstruction algorithms have been proposed, although less research has been devoted to describe their theoretical limitations. In this work, we adopt an information-theoretic perspective and define the reconstructability: The fraction of structural information recoverable from data. The reconstructability depends on the true data generating (TDG) model which is shown to set the reconstruction limit: any algorithm can perform, on average, at best like the TDG model. We show that the reconstructability is related to various performance measures, such as the probability of error and the Jaccard similarity. In an empirical context where the TDG model is unknown, we introduce the reconstruction index as an approximation of the reconstructability. We find that performing model selection is crucial for the validity of the reconstruction index as a proxy of the reconstructability of empirical time series and networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.07025</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.07025</id><created>2025-01-12</created><updated>2025-02-07</updated><authors><author><keyname>Zhang</keyname><forenames>Yong</forenames></author><author><keyname>Gyamfi</keyname><forenames>Eric Herrison</forenames></author></authors><title>A Weighted Similarity Metric for Community Detection in Sparse Data</title><categories>stat.ME cs.SI</categories><comments>This paper has been accepted for Workshop of AI for Social Impact at   AAAI 2025</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Many Natural Language Processing (NLP) related applications involves topics and sentiments derived from short documents such as consumer reviews and social media posts. Topics and sentiments of short documents are highly sparse because a short document generally covers a few topics among hundreds of candidates. Imputation of missing data is sometimes hard to justify and also often unpractical in highly sparse data. We developed a method for calculating a weighted similarity for highly sparse data without imputation. This weighted similarity is consist of three components to capture similarities based on both existence and lack of common properties and pattern of missing values. As a case study, we used a community detection algorithm and this weighted similarity to group different shampoo brands based on sparse topic sentiments derived from short consumer reviews. Compared with traditional imputation and similarity measures, the weighted similarity shows better performance in both general community structures and average community qualities. The performance is consistent and robust across metrics and community complexities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10117</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10117</id><created>2025-01-17</created><updated>2025-02-07</updated><authors><author><keyname>Liu</keyname><forenames>Weiguang</forenames></author><author><keyname>de Paula</keyname><forenames>Áureo</forenames></author><author><keyname>Tamer</keyname><forenames>Elie</forenames></author></authors><title>Prediction Sets and Conformal Inference with Censored Outcomes</title><categories>econ.EM stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given data on a scalar random variable $Y$, a prediction set for $Y$ with miscoverage level $\alpha$ is a set of values for $Y$ that contains a randomly drawn $Y$ with probability $1 - \alpha$, where $\alpha \in (0,1)$. Among all prediction sets that satisfy this coverage property, the oracle prediction set is the one with the smallest volume. This paper provides estimation methods of such prediction sets given observed conditioning covariates when $Y$ is \textit{censored} or \textit{measured in intervals}. We first characterise the oracle prediction set under interval censoring and develop a consistent estimator for the shortest prediction {\it interval} that satisfies this coverage property.These consistency results are extended to accommodate cases where the prediction set consists of multiple disjoint intervals. We use conformal inference to construct a prediction set that achieves finite-sample validity under censoring and maintains consistency as sample size increases, using a conformity score function designed for interval data. The procedure accommodates the prediction uncertainty that is irreducible (due to the stochastic nature of outcomes), the modelling uncertainty due to partial identification and also sampling uncertainty that gets reduced as samples get larger. We conduct a set of Monte Carlo simulations and an application to data from the Current Population Survey. The results highlight the robustness and efficiency of the proposed methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10974</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10974</id><created>2025-01-19</created><updated>2025-02-06</updated><authors><author><keyname>Huang</keyname><forenames>Yu-Han</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Sequential Change Detection for Learning in Piecewise Stationary Bandit   Environments</title><categories>cs.IT cs.SY eess.SY math.IT stat.OT</categories><comments>15 pages, 2 figures. arXiv admin note: text overlap with   arXiv:2501.01291</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A finite-horizon variant of the quickest change detection problem is investigated, which is motivated by a change detection problem that arises in piecewise stationary bandits. The goal is to minimize the \emph{latency}, which is smallest threshold such that the probability that the detection delay exceeds the threshold is below a desired low level, while controlling the false alarm probability to a desired low level. When the pre- and post-change distributions are unknown, two tests are proposed as candidate solutions. These tests are shown to attain order optimality in terms of the horizon. Furthermore, the growth in their latencies with respect to the false alarm probability and late detection probability satisfies a property that is desirable in regret analysis for piecewise stationary bandits. Numerical results are provided to validate the theoretical performance results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14974</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14974</id><created>2025-01-24</created><updated>2025-02-06</updated><authors><author><keyname>Deng</keyname><forenames>Fengnan</forenames></author><author><keyname>Vidyashankar</keyname><forenames>Anand N.</forenames></author></authors><title>Private Minimum Hellinger Distance Estimation via Hellinger Distance   Differential Privacy</title><categories>math.ST cs.CR math.PR stat.ME stat.ML stat.TH</categories><msc-class>62F35, 68P27, 62E20, 60E05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective functions based on Hellinger distance yield robust and efficient estimators of model parameters. Motivated by privacy and regulatory requirements encountered in contemporary applications, we derive in this paper \emph{private minimum Hellinger distance estimators}. The estimators satisfy a new privacy constraint, namely, Hellinger differential privacy, while retaining the robustness and efficiency properties. We demonstrate that Hellinger differential privacy shares several features of standard differential privacy while allowing for sharper inference. Additionally, for computational purposes, we also develop Hellinger differentially private gradient descent and Newton-Raphson algorithms. We illustrate the behavior of our estimators in finite samples using numerical experiments and verify that they retain robustness properties under gross-error contamination. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18502</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18502</id><created>2025-01-30</created><updated>2025-02-07</updated><authors><author><keyname>Kumar</keyname><forenames>Ritesh</forenames></author><author><keyname>Vatedka</keyname><forenames>Shashank</forenames></author></authors><title>One-Bit Distributed Mean Estimation with Unknown Variance</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>21 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this work, we study the problem of distributed mean estimation with $1$-bit communication constraints when the variance is unknown. We focus on the specific case where each user has access to one i.i.d. sample drawn from a distribution that belongs to a scale-location family, and is limited to sending just a single bit of information to a central server whose goal is to estimate the mean. We propose non-adaptive and adaptive estimators that are shown to be asymptotically normal. We derive bounds on the asymptotic (in the number of users) Mean Squared Error (MSE) achieved by these estimators. For a class of symmetric log-concave distributions, we derive matching lower bounds for the MSE achieved by adaptive estimators, proving the optimality of our scheme. We show that non-adaptive estimators can be strictly suboptimal by deriving a lower bound on the MSE achieved by any non-adaptive estimator for Gaussian distributions and demonstrating a positive gap between this and the MSE achieved by our adaptive scheme. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00168</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00168</id><created>2025-01-31</created><updated>2025-02-07</updated><authors><author><keyname>Herrera-Esposito</keyname><forenames>Daniel</forenames></author><author><keyname>Burge</keyname><forenames>Johannes</forenames></author></authors><title>Supervised Quadratic Feature Analysis: An Information Geometry Approach   to Dimensionality Reduction</title><categories>stat.ML cs.LG math.DG math.ST stat.TH</categories><comments>18 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Supervised dimensionality reduction aims to map labeled data to a low-dimensional feature space while maximizing class discriminability. Despite the availability of methods for learning complex non-linear features (e.g. Deep Learning), there is an enduring demand for dimensionality reduction methods that learn linear features due to their interpretability, low computational cost, and broad applicability. However, there is a gap between methods that optimize linear separability (e.g. LDA), and more flexible but computationally expensive methods that optimize over arbitrary class boundaries (e.g. metric-learning methods). Here, we present Supervised Quadratic Feature Analysis (SQFA), a dimensionality reduction method for learning linear features that maximize the differences between class-conditional first- and second-order statistics, which allow for quadratic discrimination. SQFA exploits the information geometry of second-order statistics in the symmetric positive definite manifold. We show that SQFA features support quadratic discriminability in real-world problems. We also provide a theoretical link, based on information geometry, between SQFA and the Quadratic Discriminant Analysis (QDA) classifier. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00182</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00182</id><created>2025-01-31</created><updated>2025-02-07</updated><authors><author><keyname>Seo</keyname><forenames>Jungwon</forenames></author><author><keyname>Catak</keyname><forenames>Ferhat Ozgur</forenames></author><author><keyname>Rong</keyname><forenames>Chunming</forenames></author></authors><title>Understanding Federated Learning from IID to Non-IID dataset: An   Experimental Study</title><categories>cs.LG cs.AI stat.ML</categories><journal-ref>36th Norwegian ICT Conference for Research and Education, NIKT   2024</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As privacy concerns and data regulations grow, federated learning (FL) has emerged as a promising approach for training machine learning models across decentralized data sources without sharing raw data. However, a significant challenge in FL is that client data are often non-IID (non-independent and identically distributed), leading to reduced performance compared to centralized learning. While many methods have been proposed to address this issue, their underlying mechanisms are often viewed from different perspectives. Through a comprehensive investigation from gradient descent to FL, and from IID to non-IID data settings, we find that inconsistencies in client loss landscapes primarily cause performance degradation in non-IID scenarios. From this understanding, we observe that existing methods can be grouped into two main strategies: (i) adjusting parameter update paths and (ii) modifying client loss landscapes. These findings offer a clear perspective on addressing non-IID challenges in FL and help guide future research in the field. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00838</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00838</id><created>2025-02-02</created><authors><author><keyname>Bussemaker</keyname><forenames>Jasper H.</forenames></author><author><keyname>Saves</keyname><forenames>Paul</forenames></author><author><keyname>Bartoli</keyname><forenames>Nathalie</forenames></author><author><keyname>Lefebvre</keyname><forenames>Thierry</forenames></author><author><keyname>Lafage</keyname><forenames>Rémi</forenames></author></authors><title>System Architecture Optimization Strategies: Dealing with Expensive   Hierarchical Problems</title><categories>math.OC cs.DM stat.AP</categories><comments>J Glob Optim (2024)</comments><doi>10.1007/s10898-024-01443-8</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Choosing the right system architecture for the problem at hand is challenging due to the large design space and high uncertainty in the early stage of the design process. Formulating the architecting process as an optimization problem may mitigate some of these challenges. This work investigates strategies for solving System Architecture Optimization (SAO) problems: expensive, black-box, hierarchical, mixed-discrete, constrained, multi-objective problems that may be subject to hidden constraints. Imputation ratio, correction ratio, correction fraction, and max rate diversity metrics are defined for characterizing hierar chical design spaces. This work considers two classes of optimization algorithms for SAO: Multi-Objective Evolutionary Algorithms (MOEA) such as NSGA-II, and Bayesian Optimization (BO) algorithms. A new Gaussian process kernel is presented that enables modeling hierarchical categorical variables, extending previous work on modeling continuous and integer hierarchical variables. Next, a hierarchical sampling algorithm that uses design space hierarchy to group design vectors by active design variables is developed. Then, it is demonstrated that integrating more hierarchy information in the optimization algorithms yields better optimization results for BO algorithms. Several realistic single-objective and multi-objective test problems are used for investigations. Finally, the BO algorithm is applied to a jet engine architecture optimization problem. This work shows that the developed BO algorithm can effectively solve the problem with one order of magnitude less function evaluations than NSGA-II. The algorithms and problems used in this work are implemented in the open-source Python library SBArchOpt. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00854</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00854</id><created>2025-02-02</created><authors><author><keyname>Priem</keyname><forenames>Rémy</forenames></author><author><keyname>Diouane</keyname><forenames>Youssef</forenames></author><author><keyname>Bartoli</keyname><forenames>Nathalie</forenames></author><author><keyname>Dubreuil</keyname><forenames>Sylvain</forenames></author><author><keyname>Saves</keyname><forenames>Paul</forenames></author></authors><title>High-Dimensional Bayesian Optimization Using Both Random and Supervised   Embeddings</title><categories>math.OC cs.LG stat.ML</categories><journal-ref>AIAA Journal 2025 63:1, 162-173</journal-ref><doi>10.2514/1.J063488</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Bayesian optimization (BO) is one of the most powerful strategies to solve computationally expensive-to-evaluate blackbox optimization problems. However, BO methods are conventionally used for optimization problems of small dimension because of the curse of dimensionality. In this paper, a high-dimensionnal optimization method incorporating linear embedding subspaces of small dimension is proposed to efficiently perform the optimization. An adaptive learning strategy for these linear embeddings is carried out in conjunction with the optimization. The resulting BO method, named efficient global optimization coupled with random and supervised embedding (EGORSE), combines in an adaptive way both random and supervised linear embeddings. EGORSE has been compared to state-of-the-art algorithms and tested on academic examples with a number of design variables ranging from 10 to 600. The obtained results show the high potential of EGORSE to solve high-dimensional blackbox optimization problems, in terms of both CPU time and the limited number of calls to the expensive blackbox simulation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02496</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02496</id><created>2025-02-04</created><updated>2025-02-07</updated><authors><author><keyname>Kolb</keyname><forenames>Chris</forenames></author><author><keyname>Weber</keyname><forenames>Tobias</forenames></author><author><keyname>Bischl</keyname><forenames>Bernd</forenames></author><author><keyname>Rügamer</keyname><forenames>David</forenames></author></authors><title>Deep Weight Factorization: Sparse Learning Through the Lens of   Artificial Symmetries</title><categories>cs.LG stat.ML</categories><comments>accepted at ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sparse regularization techniques are well-established in machine learning, yet their application in neural networks remains challenging due to the non-differentiability of penalties like the $L_1$ norm, which is incompatible with stochastic gradient descent. A promising alternative is shallow weight factorization, where weights are decomposed into two factors, allowing for smooth optimization of $L_1$-penalized neural networks by adding differentiable $L_2$ regularization to the factors. In this work, we introduce deep weight factorization, extending previous shallow approaches to more than two factors. We theoretically establish equivalence of our deep factorization with non-convex sparse regularization and analyze its impact on training dynamics and optimization. Due to the limitations posed by standard training practices, we propose a tailored initialization scheme and identify important learning rate requirements necessary for training factorized networks. We demonstrate the effectiveness of our deep weight factorization through experiments on various architectures and datasets, consistently outperforming its shallow counterpart and widely used pruning methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02861</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02861</id><created>2025-02-04</created><updated>2025-02-06</updated><authors><author><keyname>Shen</keyname><forenames>Judy Hanwen</forenames></author><author><keyname>Vitercik</keyname><forenames>Ellen</forenames></author><author><keyname>Wikum</keyname><forenames>Anders</forenames></author></authors><title>Algorithms with Calibrated Machine Learning Predictions</title><categories>stat.ML cs.DS cs.LG</categories><comments>Replacement: updated acknowledgments</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The field of algorithms with predictions incorporates machine learning advice in the design of online algorithms to improve real-world performance. While this theoretical framework often assumes uniform reliability across all predictions, modern machine learning models can now provide instance-level uncertainty estimates. In this paper, we propose calibration as a principled and practical tool to bridge this gap, demonstrating the benefits of calibrated advice through two case studies: the ski rental and online job scheduling problems. For ski rental, we design an algorithm that achieves optimal prediction-dependent performance and prove that, in high-variance settings, calibrated advice offers more effective guidance than alternative methods for uncertainty quantification. For job scheduling, we demonstrate that using a calibrated predictor leads to significant performance improvements over existing methods. Evaluations on real-world data validate our theoretical findings, highlighting the practical impact of calibration for algorithms with predictions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03479</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03479</id><created>2025-01-27</created><updated>2025-02-06</updated><authors><author><keyname>Cuicizion</keyname><forenames>Eliuvish</forenames></author><author><keyname>Ri</keyname><forenames>Itsugo</forenames></author><author><keyname>Holmes</keyname><forenames>Elaine</forenames></author><author><keyname>Lyeutsaon</keyname><forenames>Sho</forenames></author><author><keyname>Chern</keyname><forenames>Jawad Hassan</forenames></author></authors><title>A Tutorial on Markov Renewal and Semi-Markov Proportional Hazards Model</title><categories>stat.AP stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Transition probability estimation plays a critical role in multi-state modeling, especially in clinical research. This paper investigates the application of semi-Markov and Markov renewal frameworks to the EBMT dataset, focusing on six clinical states encountered during hematopoietic stem cell transplantation. By comparing Aalen-Johansen (AJ) and Dabrowska-Sun-Horowitz (DSH) estimators, we demonstrate that semi-Markov models, which incorporate sojourn times, provide a more nuanced and temporally sensitive depiction of patient trajectories compared to memoryless Markov models. The DSH estimator consistently yields smoother probability curves, particularly for transitions involving prolonged states. We use empirical process theory and Burkholder-Davis-Gundy inequality to show weak convergence of the estimator. Future work includes extending the framework to accommodate advanced covariate structures and non-Markovian dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03587</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03587</id><created>2025-02-05</created><updated>2025-02-06</updated><authors><author><keyname>von Seeger</keyname><forenames>Anneke</forenames></author><author><keyname>Zou</keyname><forenames>Dongmian</forenames></author><author><keyname>Lerman</keyname><forenames>Gilad</forenames></author></authors><title>Stein Discrepancy for Unsupervised Domain Adaptation</title><categories>cs.LG stat.ML</categories><comments>24 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Unsupervised domain adaptation (UDA) leverages information from a labeled source dataset to improve accuracy on a related but unlabeled target dataset. A common approach to UDA is aligning representations from the source and target domains by minimizing the distance between their data distributions. Previous methods have employed distances such as Wasserstein distance and maximum mean discrepancy. However, these approaches are less effective when the target data is significantly scarcer than the source data. Stein discrepancy is an asymmetric distance between distributions that relies on one distribution only through its score function. In this paper, we propose a novel UDA method that uses Stein discrepancy to measure the distance between source and target domains. We develop a learning framework using both non-kernelized and kernelized Stein discrepancy. Theoretically, we derive an upper bound for the generalization error. Numerical experiments show that our method outperforms existing methods using other domain discrepancy measures when only small amounts of target data are available. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03942</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03942</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Holst</keyname><forenames>Klaus Kähler</forenames></author><author><keyname>Nordland</keyname><forenames>Andreas</forenames></author><author><keyname>Furkjær</keyname><forenames>Julie</forenames></author><author><keyname>Damgaard</keyname><forenames>Lars Holm</forenames></author><author><keyname>Pipper</keyname><forenames>Christian Bressen</forenames></author></authors><title>A retake on the analysis of scores truncated by terminal events</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of data from randomized controlled trials in vulnerable populations requires special attention when assessing treatment effect by a score measuring, e.g., disease stage or activity together with onset of prevalent terminal events. In reality, it is impossible to disentangle a disease score from the terminal event, since the score is not clinically meaningful after this event. In this work, we propose to assess treatment interventions simultaneously on disease score and the terminal event. Our proposal is based on a natural data-generating mechanism respecting that a disease score does not exist beyond the terminal event. We use modern semi-parametric statistical methods to provide robust and efficient estimation of the risk of terminal event and expected disease score conditional on no terminal event at a pre-specified landmark time. We also use the simultaneous asymptotic behavior of our estimators to develop a powerful closed testing procedure for confirmatory assessment of treatment effect on both onset of terminal event and level of disease score. A simulation study mimicking a large-scale outcome trial in chronic kidney patients as well as an analysis of that trial is provided to assess performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04168</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04168</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Ferradini</keyname><forenames>Carla</forenames></author><author><keyname>Gitton</keyname><forenames>Victor</forenames></author><author><keyname>Vilasini</keyname><forenames>V.</forenames></author></authors><title>Cyclic quantum causal modelling with a graph separation theorem</title><categories>quant-ph math.ST stat.ML stat.TH</categories><comments>41+41 pages. A companion paper by the same authors, focussing on   cyclic classical (functional) causal models has been submitted to the arXiv   concurrently with primary class [math.ST], v2 only differs from v1 in   including the arXiv number of the companion paper. Comments are welcome</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Causal modelling frameworks link observable correlations to causal explanations, which is a crucial aspect of science. These models represent causal relationships through directed graphs, with vertices and edges denoting systems and transformations within a theory. Most studies focus on acyclic causal graphs, where well-defined probability rules and powerful graph-theoretic properties like the d-separation theorem apply. However, understanding complex feedback processes and exotic fundamental scenarios with causal loops requires cyclic causal models, where such results do not generally hold. While progress has been made in classical cyclic causal models, challenges remain in uniquely fixing probability distributions and identifying graph-separation properties applicable in general cyclic models. In cyclic quantum scenarios, existing frameworks have focussed on a subset of possible cyclic causal scenarios, with graph-separation properties yet unexplored. This work proposes a framework applicable to all consistent quantum and classical cyclic causal models on finite-dimensional systems. We address these challenges by introducing a robust probability rule and a novel graph-separation property, p-separation, which we prove to be sound and complete for all such models. Our approach maps cyclic causal models to acyclic ones with post-selection, leveraging the post-selected quantum teleportation protocol. We characterize these protocols and their success probabilities along the way. We also establish connections between this formalism and other classical and quantum frameworks to inform a more unified perspective on causality. This provides a foundation for more general cyclic causal discovery algorithms and to systematically extend open problems and techniques from acyclic informational networks (e.g., certification of non-classicality) to cyclic causal structures and networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04171</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04171</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Ferradini</keyname><forenames>Carla</forenames></author><author><keyname>Gitton</keyname><forenames>Victor</forenames></author><author><keyname>Vilasini</keyname><forenames>V.</forenames></author></authors><title>Cyclic functional causal models beyond unique solvability with a graph   separation theorem</title><categories>math.ST quant-ph stat.ML stat.TH</categories><comments>33+16 pages. A companion paper by the same authors, focussing on   cyclic quantum causal models has been submitted to the arXiv concurrently   with primary class [quant-ph], v2 only differs from v1 in including the arXiv   number of the companion paper. Comments are welcome</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Functional causal models (fCMs) specify functional dependencies between random variables associated to the vertices of a graph. In directed acyclic graphs (DAGs), fCMs are well-understood: a unique probability distribution on the random variables can be easily specified, and a crucial graph-separation result called the d-separation theorem allows one to characterize conditional independences between the variables. However, fCMs on cyclic graphs pose challenges due to the absence of a systematic way to assign a unique probability distribution to the fCM's variables, the failure of the d-separation theorem, and lack of a generalization of this theorem that is applicable to all consistent cyclic fCMs. In this work, we develop a causal modeling framework applicable to all cyclic fCMs involving finite-cardinality variables, except inconsistent ones admitting no solutions. Our probability rule assigns a unique distribution even to non-uniquely solvable cyclic fCMs and reduces to the known rule for uniquely solvable fCMs. We identify a class of fCMs, called averagely uniquely solvable, that we show to be the largest class where the probabilities admit a Markov factorization. Furthermore, we introduce a new graph-separation property, p-separation, and prove this to be sound and complete for all consistent finite-cardinality cyclic fCMs while recovering the d-separation theorem for DAGs. These results are obtained by considering classical post-selected teleportation protocols inspired by analogous protocols in quantum information theory. We discuss further avenues for exploration, linking in particular problems in cyclic fCMs and in quantum causality. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04208</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04208</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Grünwald</keyname><forenames>Peter D.</forenames></author><author><keyname>Koolen</keyname><forenames>Wouter M.</forenames></author></authors><title>Supermartingales for One-Sided Tests: Sufficient Monotone Likelihood   Ratios are Sufficient</title><categories>math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The t-statistic is a widely-used scale-invariant statistic for testing the null hypothesis that the mean is zero. Martingale methods enable sequential testing with the t-statistic at every sample size, while controlling the probability of falsely rejecting the null. For one-sided sequential tests, which reject when the t-statistic is too positive, a natural question is whether they also control false rejection when the true mean is negative. We prove that this is the case using monotone likelihood ratios and sufficient statistics. We develop applications to the scale-invariant t-test, the location-invariant $\chi^2$-test and sequential linear regression with nuisance covariates. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04339</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04339</id><created>2025-02-01</created><authors><author><keyname>George</keyname><forenames>Anand Jerry</forenames></author><author><keyname>Veiga</keyname><forenames>Rodrigo</forenames></author><author><keyname>Macris</keyname><forenames>Nicolas</forenames></author></authors><title>Analysis of Diffusion Models for Manifold Data</title><categories>math.ST cond-mat.dis-nn cs.IT cs.LG math.IT math.PR stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We analyze the time reversed dynamics of generative diffusion models. If the exact empirical score function is used in a regime of large dimension and exponentially large number of samples, these models are known to undergo transitions between distinct dynamical regimes. We extend this analysis and compute the transitions for an analytically tractable manifold model where the statistical model for the data is a mixture of lower dimensional Gaussians embedded in higher dimensional space. We compute the so-called speciation and collapse transition times, as a function of the ratio of manifold-to-ambient space dimensions, and other characteristics of the data model. An important tool used in our analysis is the exact formula for the mutual information (or free energy) of Generalized Linear Models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04372</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04372</id><created>2025-02-05</created><authors><author><keyname>Genari</keyname><forenames>Juliano</forenames></author><author><keyname>Goedert</keyname><forenames>Guilherme Tegoni</forenames></author></authors><title>Mining Unstructured Medical Texts With Conformal Active Learning</title><categories>cs.CL cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The extraction of relevant data from Electronic Health Records (EHRs) is crucial to identifying symptoms and automating epidemiological surveillance processes. By harnessing the vast amount of unstructured text in EHRs, we can detect patterns that indicate the onset of disease outbreaks, enabling faster, more targeted public health responses. Our proposed framework provides a flexible and efficient solution for mining data from unstructured texts, significantly reducing the need for extensive manual labeling by specialists. Experiments show that our framework achieving strong performance with as few as 200 manually labeled texts, even for complex classification problems. Additionally, our approach can function with simple lightweight models, achieving competitive and occasionally even better results compared to more resource-intensive deep learning models. This capability not only accelerates processing times but also preserves patient privacy, as the data can be processed on weaker on-site hardware rather than being transferred to external systems. Our methodology, therefore, offers a practical, scalable, and privacy-conscious approach to real-time epidemiological monitoring, equipping health institutions to respond rapidly and effectively to emerging health threats. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04422</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04422</id><created>2025-02-06</created><authors><author><keyname>Yadav</keyname><forenames>Pooja</forenames></author><author><keyname>Srivastava</keyname><forenames>Tanuja</forenames></author></authors><title>The Maximum Likelihood Degree of Farlie Gumbel Morgenstern Bivariate   Exponential Distribution</title><categories>math.ST math.AC stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum likelihood degree of a statistical model refers to the number of solutions, where the derivative of the log-likelihood function is zero, over the complex field. This paper examines the maximum likelihood degree of the parameter in Farlie-Gumbel-Morgenstern bivariate exponential distribution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04464</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04464</id><created>2025-02-06</created><authors><author><keyname>Jadoul</keyname><forenames>Yannick</forenames></author><author><keyname>Tufarelli</keyname><forenames>Tommaso</forenames></author><author><keyname>Coissac</keyname><forenames>Chloé</forenames></author><author><keyname>Gamba</keyname><forenames>Marco</forenames></author><author><keyname>Ravignani</keyname><forenames>Andrea</forenames></author></authors><title>Hidden assumptions of integer ratio analyses in bioacoustics and music</title><categories>stat.AP</categories><comments>19 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Rhythm is ubiquitous in human culture and in nature, but hard to capture in all its complexity. A key dimension of rhythm, integer ratio categories occur when the relationship between temporal intervals can be expressed as small-integer ratios. Recent work has found integer ratio categories in most human musical cultures and some animal species' vocalizations or behavioral displays. But biological systems are noisy, and empirically measured intervals rarely form an exact small-integer ratio. Here, we mathematically assess whether the leading integer ratio analysis method makes valid statistical and biological assumptions. In particular, we (1) make the temporal properties of empirical ratios explicit, both in general and for the typical use in the literature; (2) show how the choice of ratio formula affects the probability distribution of rhythm ratios and ensuing statistical results; (3) guide the reader to carefully consider the assumptions and null hypotheses of the statistical analysis; (4) present a comprehensive methodology to statistically test integer ratios for any null hypothesis of choice. Our observations have implications for both past and future research in music cognition and animal behavior: They suggest how to interpret past findings and provide tools to choose the correct null hypotheses in future empirical work. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04491</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04491</id><created>2025-02-06</created><authors><author><keyname>Cheng</keyname><forenames>Ziheng</forenames></author><author><keyname>Xie</keyname><forenames>Tianyu</forenames></author><author><keyname>Zhang</keyname><forenames>Shiyue</forenames></author><author><keyname>Zhang</keyname><forenames>Cheng</forenames></author></authors><title>Provable Sample-Efficient Transfer Learning Conditional Diffusion Models   via Representation Learning</title><categories>cs.LG math.ST stat.ML stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While conditional diffusion models have achieved remarkable success in various applications, they require abundant data to train from scratch, which is often infeasible in practice. To address this issue, transfer learning has emerged as an essential paradigm in small data regimes. Despite its empirical success, the theoretical underpinnings of transfer learning conditional diffusion models remain unexplored. In this paper, we take the first step towards understanding the sample efficiency of transfer learning conditional diffusion models through the lens of representation learning. Inspired by practical training procedures, we assume that there exists a low-dimensional representation of conditions shared across all tasks. Our analysis shows that with a well-learned representation from source tasks, the samplecomplexity of target tasks can be reduced substantially. In addition, we investigate the practical implications of our theoretical results in several real-world applications of conditional diffusion models. Numerical experiments are also conducted to verify our results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04543</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04543</id><created>2025-02-06</created><authors><author><keyname>Lu</keyname><forenames>Zhou</forenames></author><author><keyname>Sun</keyname><forenames>Y. Jennifer</forenames></author><author><keyname>Zhang</keyname><forenames>Zhiyu</forenames></author></authors><title>Sparsity-Based Interpolation of External, Internal and Swap Regret</title><categories>stat.ML cs.LG</categories><comments>Equal contribution, alphabetical order</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Focusing on the expert problem in online learning, this paper studies the interpolation of several performance metrics via $\phi$-regret minimization, which measures the performance of an algorithm by its regret with respect to an arbitrary action modification rule $\phi$. With $d$ experts and $T\gg d$ rounds in total, we present a single algorithm achieving the instance-adaptive $\phi$-regret bound \begin{equation*} \tilde O\left(\min\left\{\sqrt{d-d^{\mathrm{unif}}_\phi+1},\sqrt{d-d^{\mathrm{self}}_\phi}\right\}\cdot\sqrt{T}\right), \end{equation*} where $d^{\mathrm{unif}}_\phi$ is the maximum amount of experts modified identically by $\phi$, and $d^{\mathrm{self}}_\phi$ is the amount of experts that $\phi$ trivially modifies to themselves. By recovering the optimal $O(\sqrt{T\log d})$ external regret bound when $d^{\mathrm{unif}}_\phi=d$, the standard $\tilde O(\sqrt{T})$ internal regret bound when $d^{\mathrm{self}}_\phi=d-1$ and the optimal $\tilde O(\sqrt{dT})$ swap regret bound in the worst case, we improve existing results in the intermediate regimes. In addition, the same algorithm achieves the optimal quantile regret bound, which corresponds to even easier settings of $\phi$ than the external regret.   Building on the classical reduction from $\phi$-regret minimization to external regret minimization on stochastic matrices, our main idea is to further convert the latter to online linear regression using Haar-wavelet-inspired matrix features. Then, we apply a particular $L_1$-version of comparator-adaptive online learning algorithms to exploit the sparsity in this regression subroutine. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04550</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04550</id><created>2025-02-06</created><authors><author><keyname>Faes</keyname><forenames>Luca</forenames></author><author><keyname>Sparacino</keyname><forenames>Laura</forenames></author><author><keyname>Mijatovic</keyname><forenames>Gorana</forenames></author><author><keyname>Antonacci</keyname><forenames>Yuri</forenames></author><author><keyname>Ricci</keyname><forenames>Leonardo</forenames></author><author><keyname>Marinazzo</keyname><forenames>Daniele</forenames></author><author><keyname>Stramaglia</keyname><forenames>Sebastiano</forenames></author></authors><title>Partial Information Rate Decomposition</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Partial Information Decomposition (PID) is a principled and flexible method to unveil complex high-order interactions in multi-unit network systems. Though being defined exclusively for random variables, PID is ubiquitously applied to multivariate time series taken as realizations of random processes with temporal statistical structure. Here, to overcome the incorrect depiction of high-order effects by PID schemes applied to dynamic networks, we introduce the framework of Partial Information Rate Decomposition (PIRD). PIRD is formalized applying lattice theory to decompose the information shared dynamically between a target random process and a set of source processes, implemented for Gaussian processes through a spectral expansion of information rates, and demonstrated in practice analyzing time series from large-scale climate oscillations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04553</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04553</id><created>2025-02-06</created><authors><author><keyname>Swanson</keyname><forenames>David</forenames></author><author><keyname>Sherry</keyname><forenames>Alexander</forenames></author><author><keyname>Tang</keyname><forenames>Chad</forenames></author></authors><title>Variance component mixture modelling for longitudinal T-cell receptor   clonal dynamics</title><categories>stat.ME</categories><msc-class>62F15 (Primary), 62H30 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Studies of T cells and their clonally unique receptors have shown promise in elucidating the association between immune response and human disease. Methods to identify T-cell receptor clones which expand or contract in response to certain therapeutic strategies have so far been limited to longitudinal pairwise comparisons of clone frequency with multiplicity adjustment. Here we develop a more general mixture model approach for arbitrary follow-up and missingness which partitions dynamic longitudinal clone frequency behavior from static. While it is common to mix on the location or scale parameter of a family of distributions, the model instead mixes on the parameterization itself, the dynamic component allowing for a variable, Gamma-distributed Poisson mean parameter over longitudinal follow-up, while the static component mean is time invariant. Leveraging conjugacy, one can integrate out the mean parameter for the dynamic and static components to yield distinct posterior predictive distributions whose expressions are a product of negative binomials and a single negative multinomial, respectively, each modified according to an offset for receptor read count normalization. An EM-algorithm is developed to estimate hyperparameters and component membership, and validity of the approach is demonstrated in simulation. The model identifies a statistically significant and clinically relevant increase in TCR clonal dynamism among metastasis-directed radiation therapy in a cohort of prostate cancer patients. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04555</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04555</id><created>2025-02-06</created><authors><author><keyname>Sparacino</keyname><forenames>Laura</forenames></author><author><keyname>Mijatovic</keyname><forenames>Gorana</forenames></author><author><keyname>Antonacci</keyname><forenames>Yuri</forenames></author><author><keyname>Ricci</keyname><forenames>Leonardo</forenames></author><author><keyname>Marinazzo</keyname><forenames>Daniele</forenames></author><author><keyname>Stramaglia</keyname><forenames>Sebastiano</forenames></author><author><keyname>Faes</keyname><forenames>Luca</forenames></author></authors><title>Decomposing Multivariate Information Rates in Networks of Random   Processes</title><categories>stat.ME cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The Partial Information Decomposition (PID) framework has emerged as a powerful tool for analyzing high-order interdependencies in complex network systems. However, its application to dynamic processes remains challenging due to the implicit assumption of memorylessness, which often falls in real-world scenarios. In this work, we introduce the framework of Partial Information Rate Decomposition (PIRD) that extends PID to random processes with temporal correlations. By leveraging mutual information rate (MIR) instead of mutual information (MI), our approach decomposes the dynamic information shared by multivariate random processes into unique, redundant, and synergistic contributions obtained aggregating information rate atoms in a principled manner. To solve PIRD, we define a pointwise redundancy rate function based on the minimum MI principle applied locally in the frequency-domain representation of the processes. The framework is validated in benchmark simulations of Gaussian systems, demonstrating its advantages over traditional PID in capturing temporal correlations and showing how the spectral representation may reveal scale-specific higher-order interaction that are obscured in the time domain. Furthermore, we apply PIRD to a physiological network comprising cerebrovascular and cardiovascular variables, revealing frequency-dependent redundant information exchange during a protocol of postural stress. Our results highlight the necessity of accounting for the full temporal statistical structure and spectral content of vector random processes to meaningfully perform information decomposition in network systems with dynamic behavior such as those typically encountered in neuroscience and physiology. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04574</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04574</id><created>2025-02-06</created><authors><author><keyname>Gong</keyname><forenames>ZhuQing</forenames></author><author><keyname>Zuo</keyname><forenames>XiNian</forenames></author></authors><title>Dark Brain Energy: Toward an Integrative Model of Spontaneous Slow   Oscillations</title><categories>q-bio.NC cs.IT math.IT stat.AP</categories><comments>38 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Neural oscillations facilitate the functioning of the human brain in spatial and temporal dimensions at various frequencies. These oscillations feature a universal frequency architecture that is governed by brain anatomy, ensuring frequency specificity remains invariant across different measurement techniques. Initial magnetic resonance imaging (MRI) methodology constrained functional MRI (fMRI) investigations to a singular frequency range, thereby neglecting the frequency characteristics inherent in blood oxygen level-dependent oscillations. With advancements in MRI technology, it has become feasible to decode intricate brain activities via multi-band frequency analysis (MBFA). During the past decade, the utilization of MBFA in fMRI studies has surged, unveiling frequency-dependent characteristics of spontaneous slow oscillations (SSOs) believed to base dark energy in the brain. There remains a dearth of conclusive insights and hypotheses pertaining to the properties and functionalities of SSOs in distinct bands. We surveyed the SSO MBFA studies during the past 15 years to delineate the attributes of SSOs and enlighten their correlated functions. We further proposed a model to elucidate the hierarchical organization of multi-band SSOs by integrating their function, aimed at bridging theoretical gaps and guiding future MBFA research endeavors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04575</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04575</id><created>2025-02-06</created><authors><author><keyname>Guo</keyname><forenames>Wei</forenames></author><author><keyname>Tao</keyname><forenames>Molei</forenames></author><author><keyname>Chen</keyname><forenames>Yongxin</forenames></author></authors><title>Complexity Analysis of Normalizing Constant Estimation: from Jarzynski   Equality to Annealed Importance Sampling and beyond</title><categories>stat.ML cs.LG cs.NA math.NA physics.comp-ph stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given an unnormalized probability density $\pi\propto\mathrm{e}^{-V}$, estimating its normalizing constant $Z=\int_{\mathbb{R}^d}\mathrm{e}^{-V(x)}\mathrm{d}x$ or free energy $F=-\log Z$ is a crucial problem in Bayesian statistics, statistical mechanics, and machine learning. It is challenging especially in high dimensions or when $\pi$ is multimodal. To mitigate the high variance of conventional importance sampling estimators, annealing-based methods such as Jarzynski equality and annealed importance sampling are commonly adopted, yet their quantitative complexity guarantees remain largely unexplored. We take a first step toward a non-asymptotic analysis of annealed importance sampling. In particular, we derive an oracle complexity of $\widetilde{O}\left(\frac{d\beta^2{\mathcal{A}}^2}{\varepsilon^4}\right)$ for estimating $Z$ within $\varepsilon$ relative error with high probability, where $\beta$ is the smoothness of $V$ and $\mathcal{A}$ denotes the action of a curve of probability measures interpolating $\pi$ and a tractable reference distribution. Our analysis, leveraging Girsanov theorem and optimal transport, does not explicitly require isoperimetric assumptions on the target distribution. Finally, to tackle the large action of the widely used geometric interpolation of probability distributions, we propose a new normalizing constant estimation algorithm based on reverse diffusion samplers and establish a framework for analyzing its complexity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04591</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04591</id><created>2025-02-06</created><authors><author><keyname>Deidda</keyname><forenames>Piero</forenames></author><author><keyname>Zhang</keyname><forenames>Kaicheng</forenames></author><author><keyname>Higham</keyname><forenames>Desmond</forenames></author><author><keyname>Tudisco</keyname><forenames>Francesco</forenames></author></authors><title>Rethinking Oversmoothing in Graph Neural Networks: A Rank-Based   Perspective</title><categories>cs.LG cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Oversmoothing is a fundamental challenge in graph neural networks (GNNs): as the number of layers increases, node embeddings become increasingly similar, and model performance drops sharply. Traditionally, oversmoothing has been quantified using metrics that measure the similarity of neighbouring node features, such as the Dirichlet energy. While these metrics are related to oversmoothing, we argue they have critical limitations and fail to reliably capture oversmoothing in realistic scenarios. For instance, they provide meaningful insights only for very deep networks and under somewhat strict conditions on the norm of network weights and feature representations. As an alternative, we propose measuring oversmoothing by examining the numerical or effective rank of the feature representations. We provide theoretical support for this approach, demonstrating that the numerical rank of feature representations converges to one for a broad family of nonlinear activation functions under the assumption of nonnegative trained weights. To the best of our knowledge, this is the first result that proves the occurrence of oversmoothing without assumptions on the boundedness of the weight matrices. Along with the theoretical findings, we provide extensive numerical evaluation across diverse graph architectures. Our results show that rank-based metrics consistently capture oversmoothing, whereas energy-based metrics often fail. Notably, we reveal that a significant drop in the rank aligns closely with performance degradation, even in scenarios where energy metrics remain unchanged. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04593</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04593</id><created>2025-02-06</created><authors><author><keyname>Rezaei</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Dieng</keyname><forenames>Adji Bousso</forenames></author></authors><title>The $\alpha$-Alternator: Dynamic Adaptation To Varying Noise Levels In   Sequences Using The Vendi Score For Improved Robustness and Performance</title><categories>cs.LG cs.AI cs.NE stat.ML</categories><comments>The codebase will be made available upon publication. This paper is   dedicated to Patrice Lumumba</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Current state-of-the-art dynamical models, such as Mamba, assume the same level of noisiness for all elements of a given sequence, which limits their performance on noisy temporal data. In this paper, we introduce the $\alpha$-Alternator, a novel generative model for time-dependent data that dynamically adapts to the complexity introduced by varying noise levels in sequences. The $\alpha$-Alternator leverages the Vendi Score (VS), a flexible similarity-based diversity metric, to adjust, at each time step $t$, the influence of the sequence element at time $t$ and the latent representation of the dynamics up to that time step on the predicted future dynamics. This influence is captured by a parameter that is learned and shared across all sequences in a given dataset. The sign of this parameter determines the direction of influence. A negative value indicates a noisy dataset, where a sequence element that increases the VS is considered noisy, and the model relies more on the latent history when processing that element. Conversely, when the parameter is positive, a sequence element that increases the VS is considered informative, and the $\alpha$-Alternator relies more on this new input than on the latent history when updating its predicted latent dynamics. The $\alpha$-Alternator is trained using a combination of observation masking and Alternator loss minimization. Masking simulates varying noise levels in sequences, enabling the model to be more robust to these fluctuations and improving its performance in trajectory prediction, imputation, and forecasting. Our experimental results demonstrate that the $\alpha$-Alternator outperforms both Alternators and state-of-the-art state-space models across neural decoding and time-series forecasting benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04654</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04654</id><created>2025-02-06</created><authors><author><keyname>Lim</keyname><forenames>Keunwoo</forenames></author><author><keyname>Ye</keyname><forenames>Ting</forenames></author><author><keyname>Han</keyname><forenames>Fang</forenames></author></authors><title>A sliced Wasserstein and diffusion approach to random coefficient models</title><categories>math.ST econ.EM stat.TH</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new minimum-distance estimator for linear random coefficient models. This estimator integrates the recently advanced sliced Wasserstein distance with the nearest neighbor methods, both of which enhance computational efficiency. We demonstrate that the proposed method is consistent in approximating the true distribution. Additionally, our formulation encourages a diffusion process-based algorithm, which holds independent interest and potential for broader applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04673</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04673</id><created>2025-02-07</created><authors><author><keyname>Neopane</keyname><forenames>Ojash</forenames></author><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author></authors><title>Optimistic Algorithms for Adaptive Estimation of the Average Treatment   Effect</title><categories>stat.ML cs.LG stat.ME</categories><comments>15 pages, 2 Figures</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Estimation and inference for the Average Treatment Effect (ATE) is a cornerstone of causal inference and often serves as the foundation for developing procedures for more complicated settings. Although traditionally analyzed in a batch setting, recent advances in martingale theory have paved the way for adaptive methods that can enhance the power of downstream inference. Despite these advances, progress in understanding and developing adaptive algorithms remains in its early stages. Existing work either focus on asymptotic analyses that overlook exploration-exploitation tradeoffs relevant in finite-sample regimes or rely on simpler but suboptimal estimators. In this work, we address these limitations by studying adaptive sampling procedures that take advantage of the asymptotically optimal Augmented Inverse Probability Weighting (AIPW) estimator. Our analysis uncovers challenges obscured by asymptotic approaches and introduces a novel algorithmic design principle reminiscent of optimism in multiarmed bandits. This principled approach enables our algorithm to achieve significant theoretical and empirical gains compared to prior methods. Our findings mark a step forward in advancing adaptive causal inference methods in theory and practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04681</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04681</id><created>2025-02-07</created><authors><author><keyname>Louit</keyname><forenames>Sydney</forenames></author><author><keyname>Clark</keyname><forenames>Evan</forenames></author><author><keyname>Gelbard</keyname><forenames>Alexander</forenames></author><author><keyname>Vivek</keyname><forenames>Niketna</forenames></author><author><keyname>Yan</keyname><forenames>Jun</forenames></author><author><keyname>Zhang</keyname><forenames>Panpan</forenames></author></authors><title>CALF-SBM: A Covariate-Assisted Latent Factor Stochastic Block Model</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel network generative model extended from the standard stochastic block model by concurrently utilizing observed node-level information and accounting for network-enabled nodal heterogeneity. The proposed model is so so-called covariate-assisted latent factor stochastic block model (CALF-SBM). The inference for the proposed model is done in a fully Bayesian framework. The primary application of CALF-SBM in the present research is focused on community detection, where a model-selection-based approach is employed to estimate the number of communities which is practically assumed unknown. To assess the performance of CALF-SBM, an extensive simulation study is carried out, including comparisons with multiple classical and modern network clustering algorithms. Lastly, the paper presents two real data applications, respectively based on an extremely new network data demonstrating collaborative relationships of otolaryngologists in the United States and a traditional aviation network data containing information about direct flights between airports in the United States and Canada. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04685</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04685</id><created>2025-02-07</created><authors><author><keyname>Zhang</keyname><forenames>Likun</forenames></author><author><keyname>Bhaganagar</keyname><forenames>Kiran</forenames></author><author><keyname>Wikle</keyname><forenames>Christopher K.</forenames></author></authors><title>Capturing Extreme Events in Turbulence using an Extreme Variational   Autoencoder (xVAE)</title><categories>physics.flu-dyn stat.AP stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Turbulent flow fields are characterized by extreme events that are statistically intermittent and carry a significant amount of energy and physical importance. To emulate these flows, we introduce the extreme variational Autoencoder (xVAE), which embeds a max-infinitely divisible process with heavy-tailed distributions into a standard VAE framework, enabling accurate modeling of extreme events. xVAEs are neural network models that reduce system dimensionality by learning non-linear latent representations of data. We demonstrate the effectiveness of xVAE in large-eddy simulation data of wildland fire plumes, where intense heat release and complex plume-atmosphere interactions generate extreme turbulence. Comparisons with the commonly used Proper Orthogonal Decomposition (POD) modes show that xVAE is more robust in capturing extreme values and provides a powerful uncertainty quantification framework using variational Bayes. Additionally, xVAE enables analysis of the so-called copulas of fields to assess risks associated with rare events while rigorously accounting for uncertainty, such as simultaneous exceedances of high thresholds across multiple locations. The proposed approach provides a new direction for studying realistic turbulent flows, such as high-speed aerodynamics, space propulsion, and atmospheric and oceanic systems that are characterized by extreme events. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04699</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04699</id><created>2025-02-07</created><authors><author><keyname>Lan</keyname><forenames>Hui</forenames></author><author><keyname>Chang</keyname><forenames>Haoge</forenames></author><author><keyname>Dillon</keyname><forenames>Eleanor</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author></authors><title>A Meta-learner for Heterogeneous Effects in Difference-in-Differences</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We address the problem of estimating heterogeneous treatment effects in panel data, adopting the popular Difference-in-Differences (DiD) framework under the conditional parallel trends assumption. We propose a novel doubly robust meta-learner for the Conditional Average Treatment Effect on the Treated (CATT), reducing the estimation to a convex risk minimization problem involving a set of auxiliary models. Our framework allows for the flexible estimation of the CATT, when conditioning on any subset of variables of interest using generic machine learning. Leveraging Neyman orthogonality, our proposed approach is robust to estimation errors in the auxiliary models. As a generalization to our main result, we develop a meta-learning approach for the estimation of general conditional functionals under covariate shift. We also provide an extension to the instrumented DiD setting with non-compliance. Empirical results demonstrate the superiority of our approach over existing baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04709</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04709</id><created>2025-02-07</created><authors><author><keyname>Miftachov</keyname><forenames>Ratmir</forenames></author><author><keyname>Reiß</keyname><forenames>Markus</forenames></author></authors><title>Early Stopping for Regression Trees</title><categories>math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We develop early stopping rules for growing regression tree estimators. The fully data-driven stopping rule is based on monitoring the global residual norm. The best-first search and the breadth-first search algorithms together with linear interpolation give rise to generalized projection or regularization flows. A general theory of early stopping is established. Oracle inequalities for the early-stopped regression tree are derived without any smoothness assumption on the regression function, assuming the original CART splitting rule, yet with a much broader scope. The remainder terms are of smaller order than the best achievable rates for Lipschitz functions in dimension $d\ge 2$. In real and synthetic data the early stopping regression tree estimators attain the statistical performance of cost-complexity pruning while significantly reducing computational costs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04723</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04723</id><created>2025-02-07</created><authors><author><keyname>Lyu</keyname><forenames>Ziyang</forenames></author><author><keyname>Sisson</keyname><forenames>S. A.</forenames></author><author><keyname>Welsh</keyname><forenames>A. H.</forenames></author></authors><title>Asymptotics for EBLUPs within crossed mixed effect models</title><categories>stat.ME</categories><comments>33 pages</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In this article, we derive the joint asymptotic distribution of empirical best linear unbiased predictors (EBLUPs) for individual and cell-level random effects in a crossed mixed effect model. Under mild conditions (which include moment conditions instead of normality for the random effects and model errors), we demonstrate that as the sizes of rows, columns, and, when we include interactions, cells simultaneously increase to infinity, the distribution of the differences between the EBLUPs and the random effects satisfy central limit theorems. These central limit theorems mean the EBLUPs asymptotically follow the convolution of the true random effect distribution and a normal distribution. Moreover, our results enable simple asymptotic approximations and estimators for the mean squared error (MSE) of the EBLUPs, which in turn facilitates the construction of asymptotic prediction intervals for the unobserved random effects. We show in simulations that our simple estimator of the MSE of the EBLUPs works very well in finite samples. Finally, we illustrate the use of the asymptotic prediction intervals with an analysis of movie rating data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04730</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04730</id><created>2025-02-07</created><authors><author><keyname>Xie</keyname><forenames>Tianyu</forenames></author><author><keyname>Richman</keyname><forenames>Harry</forenames></author><author><keyname>Gao</keyname><forenames>Jiansi</forenames></author><author><keyname>Matsen</keyname><forenames>Frederick A.</forenames><suffix>IV</suffix></author><author><keyname>Zhang</keyname><forenames>Cheng</forenames></author></authors><title>PhyloVAE: Unsupervised Learning of Phylogenetic Trees via Variational   Autoencoders</title><categories>stat.ML cs.LG q-bio.PE</categories><comments>ICLR 2025. 22 pages, 14 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Learning informative representations of phylogenetic tree structures is essential for analyzing evolutionary relationships. Classical distance-based methods have been widely used to project phylogenetic trees into Euclidean space, but they are often sensitive to the choice of distance metric and may lack sufficient resolution. In this paper, we introduce phylogenetic variational autoencoders (PhyloVAEs), an unsupervised learning framework designed for representation learning and generative modeling of tree topologies. Leveraging an efficient encoding mechanism inspired by autoregressive tree topology generation, we develop a deep latent-variable generative model that facilitates fast, parallelized topology generation. PhyloVAE combines this generative model with a collaborative inference model based on learnable topological features, allowing for high-resolution representations of phylogenetic tree samples. Extensive experiments demonstrate PhyloVAE's robust representation learning capabilities and fast generation of phylogenetic tree topologies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04750</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04750</id><created>2025-02-07</created><authors><author><keyname>Bui</keyname><forenames>Thang D.</forenames></author><author><keyname>Ashman</keyname><forenames>Matthew</forenames></author><author><keyname>Turner</keyname><forenames>Richard E.</forenames></author></authors><title>Tighter sparse variational Gaussian processes</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sparse variational Gaussian process (GP) approximations based on inducing points have become the de facto standard for scaling GPs to large datasets, owing to their theoretical elegance, computational efficiency, and ease of implementation. This paper introduces a provably tighter variational approximation by relaxing the standard assumption that the conditional approximate posterior given the inducing points must match that in the prior. The key innovation is to modify the conditional posterior to have smaller variances than that of the prior at the training points. We derive the collapsed bound for the regression case, describe how to use the proposed approximation in large data settings, and discuss its application to handle orthogonally structured inducing points and GP latent variable models. Extensive experiments on regression benchmarks, classification, and latent variable models demonstrate that the proposed approximation consistently matches or outperforms standard sparse variational GPs while maintaining the same computational cost. An implementation will be made available in all popular GP packages. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04793</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04793</id><created>2025-02-07</created><authors><author><keyname>Jeunen</keyname><forenames>Olivier</forenames></author></authors><title>$t$-Testing the Waters: Empirically Validating Assumptions for Reliable   A/B-Testing</title><categories>stat.ME cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A/B-tests are a cornerstone of experimental design on the web, with wide-ranging applications and use-cases. The statistical $t$-test comparing differences in means is the most commonly used method for assessing treatment effects, often justified through the Central Limit Theorem (CLT). The CLT ascertains that, as the sample size grows, the sampling distribution of the Average Treatment Effect converges to normality, making the $t$-test valid for sufficiently large sample sizes. When outcome measures are skewed or non-normal, quantifying what "sufficiently large" entails is not straightforward.   To ensure that confidence intervals maintain proper coverage and that $p$-values accurately reflect the false positive rate, it is critical to validate this normality assumption. We propose a practical method to test this, by analysing repeatedly resampled A/A-tests. When the normality assumption holds, the resulting $p$-value distribution should be uniform, and this property can be tested using the Kolmogorov-Smirnov test. This provides an efficient and effective way to empirically assess whether the $t$-test's assumptions are met, and the A/B-test is valid. We demonstrate our methodology and highlight how it helps to identify scenarios prone to inflated Type-I errors. Our approach provides a practical framework to ensure and improve the reliability and robustness of A/B-testing practices. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04807</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04807</id><created>2025-02-07</created><authors><author><keyname>Bashari</keyname><forenames>Meshi</forenames></author><author><keyname>Sesia</keyname><forenames>Matteo</forenames></author><author><keyname>Romano</keyname><forenames>Yaniv</forenames></author></authors><title>Robust Conformal Outlier Detection under Contaminated Reference Data</title><categories>stat.ML cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conformal prediction is a flexible framework for calibrating machine learning predictions, providing distribution-free statistical guarantees. In outlier detection, this calibration relies on a reference set of labeled inlier data to control the type-I error rate. However, obtaining a perfectly labeled inlier reference set is often unrealistic, and a more practical scenario involves access to a contaminated reference set containing a small fraction of outliers. This paper analyzes the impact of such contamination on the validity of conformal methods. We prove that under realistic, non-adversarial settings, calibration on contaminated data yields conservative type-I error control, shedding light on the inherent robustness of conformal methods. This conservativeness, however, typically results in a loss of power. To alleviate this limitation, we propose a novel, active data-cleaning framework that leverages a limited labeling budget and an outlier detection model to selectively annotate data points in the contaminated reference set that are suspected as outliers. By removing only the annotated outliers in this ``suspicious'' subset, we can effectively enhance power while mitigating the risk of inflating the type-I error rate, as supported by our theoretical analysis. Experiments on real datasets validate the conservative behavior of conformal methods under contamination and show that the proposed data-cleaning strategy improves power without sacrificing validity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04824</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04824</id><created>2025-02-07</created><authors><author><keyname>Blake</keyname><forenames>Joshua</forenames></author><author><keyname>Birrell</keyname><forenames>Paul</forenames></author><author><keyname>Walker</keyname><forenames>A. Sarah</forenames></author><author><keyname>Pouwels</keyname><forenames>Koen B.</forenames></author><author><keyname>House</keyname><forenames>Thomas</forenames></author><author><keyname>Tom</keyname><forenames>Brian D. M.</forenames></author><author><keyname>Kypraios</keyname><forenames>Theodore</forenames></author><author><keyname>De Angelis</keyname><forenames>Daniela</forenames></author></authors><title>Estimating the duration of RT-PCR positivity for SARS-CoV-2 from doubly   interval censored data with undetected infections</title><categories>stat.ME stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Monitoring the incidence of new infections during a pandemic is critical for an effective public health response. General population prevalence surveys for SARS-CoV-2 can provide high-quality data to estimate incidence. However, estimation relies on understanding the distribution of the duration that infections remain detectable. This study addresses this need using data from the Coronavirus Infection Survey (CIS), a long-term, longitudinal, general population survey conducted in the UK. Analyzing these data presents unique challenges, such as doubly interval censoring, undetected infections, and false negatives. We propose a Bayesian nonparametric survival analysis approach, estimating a discrete-time distribution of durations and integrating prior information derived from a complementary study. Our methodology is validated through a simulation study, including its resilience to model misspecification, and then applied to the CIS dataset. This results in the first estimate of the full duration distribution in a general population, as well as methodology that could be transferred to new contexts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04832</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04832</id><created>2025-02-07</created><authors><author><keyname>Ballarin</keyname><forenames>Giovanni</forenames></author><author><keyname>Grigoryeva</keyname><forenames>Lyudmila</forenames></author><author><keyname>Ortega</keyname><forenames>Juan-Pablo</forenames></author></authors><title>Memory Capacity of Nonlinear Recurrent Networks: Is it Informative?</title><categories>cs.LG stat.ML</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The total memory capacity (MC) of linear recurrent neural networks (RNNs) has been proven to be equal to the rank of the corresponding Kalman controllability matrix, and it is almost surely maximal for connectivity and input weight matrices drawn from regular distributions. This fact questions the usefulness of this metric in distinguishing the performance of linear RNNs in the processing of stochastic signals. This note shows that the MC of random nonlinear RNNs yields arbitrary values within established upper and lower bounds depending just on the input process scale. This confirms that the existing definition of MC in linear and nonlinear cases has no practical value. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04849</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04849</id><created>2025-02-07</created><authors><author><keyname>Yu</keyname><forenames>Yifeng</forenames></author><author><keyname>Yu</keyname><forenames>Lu</forenames></author></authors><title>Advancing Wasserstein Convergence Analysis of Score-Based Models:   Insights from Discretization and Second-Order Acceleration</title><categories>stat.ML cs.LG math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Score-based diffusion models have emerged as powerful tools in generative modeling, yet their theoretical foundations remain underexplored. In this work, we focus on the Wasserstein convergence analysis of score-based diffusion models. Specifically, we investigate the impact of various discretization schemes, including Euler discretization, exponential integrators, and midpoint randomization methods. Our analysis provides a quantitative comparison of these discrete approximations, emphasizing their influence on convergence behavior. Furthermore, we explore scenarios where Hessian information is available and propose an accelerated sampler based on the local linearization method. We demonstrate that this Hessian-based approach achieves faster convergence rates of order $\widetilde{\mathcal{O}}\left(\frac{1}{\varepsilon}\right)$ significantly improving upon the standard rate $\widetilde{\mathcal{O}}\left(\frac{1}{\varepsilon^2}\right)$ of vanilla diffusion models, where $\varepsilon$ denotes the target accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04867</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04867</id><created>2025-02-07</created><authors><author><keyname>Maclaren</keyname><forenames>Oliver J.</forenames></author><author><keyname>Nicholson</keyname><forenames>Ruanui</forenames></author><author><keyname>Trent</keyname><forenames>Joel A.</forenames></author><author><keyname>Rottenberry</keyname><forenames>Joshua</forenames></author><author><keyname>Simpson</keyname><forenames>Matthew</forenames></author></authors><title>Invariant Image Reparameterisation: A Unified Approach to Structural and   Practical Identifiability and Model Reduction</title><categories>stat.AP</categories><comments>25 pages + supplementary material</comments><msc-class>62F99 (Primary) 65L09, 93B30, 62F12, 34A55 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Both structural and practical parameter non-identifiability present fundamental challenges when using mathematical models to interpret data. This issue is particularly acute in complex, applied areas such as the life sciences or engineering, where determining appropriate model complexity is challenging. While several approaches exist for diagnosing and resolving parameter non-identifiability, including symbolic methods, profile likelihood analysis, and sloppiness analysis, these approaches have distinct limitations and are rarely combined. We present an integrated approach called Invariant Image Reparameterisation (IIR) that incorporates key elements of these methods in a new way. Our approach replaces symbolic computations with numerical calculations at a single reference estimate and an invariance condition that determines when this local calculation holds globally. Parameter combinations determined by this method are naturally ordered by degree of identifiability, and this supports model reduction by replacing a practically non-identified model with a structurally non-identified approximate model. This approximate model can be further parameterised in terms of identified parameters only. By treating parameter combinations determined by our approach as interest parameters within our established likelihood-based Profile-Wise Analysis (PWA) framework, we incorporate uncertainty quantification in terms of likelihood profiles and confidence sets. We provide a Julia library on GitHub (https://github.com/omaclaren/reparam) demonstrating our methodology across a range of mathematical models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04879</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04879</id><created>2025-02-07</created><authors><author><keyname>Gauthier</keyname><forenames>Etienne</forenames></author><author><keyname>Bach</keyname><forenames>Francis</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Statistical Collusion by Collectives on Learning Platforms</title><categories>stat.ML cs.LG</categories><comments>Code available at: https://github.com/GauthierE/statistical-collusion</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As platforms increasingly rely on learning algorithms, collectives may form and seek ways to influence these platforms to align with their own interests. This can be achieved by coordinated submission of altered data. To evaluate the potential impact of such behavior, it is essential to understand the computations that collectives must perform to impact platforms in this way. In particular, collectives need to make a priori assessments of the effect of the collective before taking action, as they may face potential risks when modifying their data. Moreover they need to develop implementable coordination algorithms based on quantities that can be inferred from observed data. We develop a framework that provides a theoretical and algorithmic treatment of these issues and present experimental results in a product evaluation domain. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04889</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04889</id><created>2025-02-07</created><authors><author><keyname>Bao</keyname><forenames>Han</forenames></author><author><keyname>Sakaue</keyname><forenames>Shinsaku</forenames></author><author><keyname>Takezawa</keyname><forenames>Yuki</forenames></author></authors><title>Any-stepsize Gradient Descent for Separable Data under Fenchel--Young   Losses</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The gradient descent (GD) has been one of the most common optimizer in machine learning. In particular, the loss landscape of a neural network is typically sharpened during the initial phase of training, making the training dynamics hover on the edge of stability. This is beyond our standard understanding of GD convergence in the stable regime where arbitrarily chosen stepsize is sufficiently smaller than the edge of stability. Recently, Wu et al. (COLT2024) have showed that GD converges with arbitrary stepsize under linearly separable logistic regression. Although their analysis hinges on the self-bounding property of the logistic loss, which seems to be a cornerstone to establish a modified descent lemma, our pilot study shows that other loss functions without the self-bounding property can make GD converge with arbitrary stepsize. To further understand what property of a loss function matters in GD, we aim to show arbitrary-stepsize GD convergence for a general loss function based on the framework of \emph{Fenchel--Young losses}. We essentially leverage the classical perceptron argument to derive the convergence rate for achieving $\epsilon$-optimal loss, which is possible for a majority of Fenchel--Young losses. Among typical loss functions, the Tsallis entropy achieves the GD convergence rate $T=\Omega(\epsilon^{-1/2})$, and the R{\'e}nyi entropy achieves the far better rate $T=\Omega(\epsilon^{-1/3})$. We argue that these better rate is possible because of \emph{separation margin} of loss functions, instead of the self-bounding property. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04891</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04891</id><created>2025-02-07</created><authors><author><keyname>Rubio-Madrigal</keyname><forenames>Celia</forenames></author><author><keyname>Jamadandi</keyname><forenames>Adarsh</forenames></author><author><keyname>Burkholz</keyname><forenames>Rebekka</forenames></author></authors><title>GNNs Getting ComFy: Community and Feature Similarity Guided Rewiring</title><categories>cs.LG cs.SI stat.ML</categories><comments>Accepted at ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Maximizing the spectral gap through graph rewiring has been proposed to enhance the performance of message-passing graph neural networks (GNNs) by addressing over-squashing. However, as we show, minimizing the spectral gap can also improve generalization. To explain this, we analyze how rewiring can benefit GNNs within the context of stochastic block models. Since spectral gap optimization primarily influences community strength, it improves performance when the community structure aligns with node labels. Building on this insight, we propose three distinct rewiring strategies that explicitly target community structure, node labels, and their alignment: (a) community structure-based rewiring (ComMa), a more computationally efficient alternative to spectral gap optimization that achieves similar goals; (b) feature similarity-based rewiring (FeaSt), which focuses on maximizing global homophily; and (c) a hybrid approach (ComFy), which enhances local feature similarity while preserving community structure to optimize label-community alignment. Extensive experiments confirm the effectiveness of these strategies and support our theoretical insights. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04892</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04892</id><created>2025-02-07</created><authors><author><keyname>Park</keyname><forenames>Joonhyeong</forenames></author><author><keyname>Park</keyname><forenames>Byoungwoo</forenames></author><author><keyname>Bang</keyname><forenames>Chang-Bae</forenames></author><author><keyname>Choi</keyname><forenames>Jungwon</forenames></author><author><keyname>Chung</keyname><forenames>Hyungjin</forenames></author><author><keyname>Kim</keyname><forenames>Byung-Hoon</forenames></author><author><keyname>Lee</keyname><forenames>Juho</forenames></author></authors><title>A Foundational Brain Dynamics Model via Stochastic Optimal Control</title><categories>cs.LG q-bio.NC stat.ML</categories><comments>The first two authors contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a foundational model for brain dynamics that utilizes stochastic optimal control (SOC) and amortized inference. Our method features a continuous-discrete state space model (SSM) that can robustly handle the intricate and noisy nature of fMRI signals. To address computational limitations, we implement an approximation strategy grounded in the SOC framework. Additionally, we present a simulation-free latent dynamics approach that employs locally linear approximations, facilitating efficient and scalable inference. For effective representation learning, we derive an Evidence Lower Bound (ELBO) from the SOC formulation, which integrates smoothly with recent advancements in self-supervised learning (SSL), thereby promoting robust and transferable representations. Pre-trained on extensive datasets such as the UKB, our model attains state-of-the-art results across a variety of downstream tasks, including demographic prediction, trait analysis, disease diagnosis, and prognosis. Moreover, evaluating on external datasets such as HCP-A, ABIDE, and ADHD200 further validates its superior abilities and resilience across different demographic and clinical distributions. Our foundational model provides a scalable and efficient approach for deciphering brain dynamics, opening up numerous applications in neuroscience. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04907</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04907</id><created>2025-02-07</created><authors><author><keyname>Gachon</keyname><forenames>Erell</forenames></author><author><keyname>Bigot</keyname><forenames>Jérémie</forenames></author><author><keyname>Cazelles</keyname><forenames>Elsa</forenames></author></authors><title>Scalable and consistent embedding of probability measures into Hilbert   spaces via measure quantization</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper is focused on statistical learning from data that come as probability measures. In this setting, popular approaches consist in embedding such data into a Hilbert space with either Linearized Optimal Transport or Kernel Mean Embedding. However, the cost of computing such embeddings prohibits their direct use in large-scale settings. We study two methods based on measure quantization for approximating input probability measures with discrete measures of small-support size. The first one is based on optimal quantization of each input measure, while the second one relies on mean-measure quantization. We study the consistency of such approximations, and its implication for scalable embeddings of probability measures into a Hilbert space at a low computational cost. We finally illustrate our findings with various numerical experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04938</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04938</id><created>2025-02-07</created><authors><author><keyname>Gardini</keyname><forenames>Aldo</forenames></author><author><keyname>Greco</keyname><forenames>Fedele</forenames></author><author><keyname>Trivisano</keyname><forenames>Carlo</forenames></author></authors><title>A note on auxiliary mixture sampling for Bayesian Poisson models</title><categories>stat.ME stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bayesian hierarchical Poisson models are an essential tool for analyzing count data. However, designing efficient algorithms to sample from the posterior distribution of the target parameters remains a challenging task for this class of models. Auxiliary mixture sampling algorithms have been proposed to address this issue. They involve two steps of data augmentations: the first leverages the theory of Poisson processes, and the second approximates the residual distribution of the resulting model through a mixture of Gaussian distributions. In this way, an approximated Gibbs sampler is obtained. In this paper, we focus on the accuracy of the approximation step, highlighting scenarios where the mixture fails to accurately represent the true underlying distribution, leading to a lack of convergence in the algorithm. We outline key features to monitor, in order to assess if the approximation performs as intended. Building on this, we propose a robust version of the auxiliary mixture sampling algorithm, which can detect approximation failures and incorporate a Metropolis-Hastings step when necessary. Finally, we evaluate the proposed algorithm together with the original mixture sampling algorithms on both simulated and real datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04945</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04945</id><created>2025-02-07</created><authors><author><keyname>Yanhao</keyname><affiliation>Max</affiliation></author><author><keyname>Wei</keyname></author><author><keyname>Jiang</keyname><forenames>Zhenling</forenames></author></authors><title>Estimating Parameters of Structural Models Using Neural Networks</title><categories>econ.EM stat.CO</categories><acm-class>G.3; J.4; I.2</acm-class><journal-ref>Marketing Science 44(1):102-128 (2024)</journal-ref><doi>10.1287/mksc.2022.0360</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an alternative use of machine learning. We train neural nets to provide the parameter estimate of a given (structural) econometric model, for example, discrete choice or consumer search. Training examples consist of datasets generated by the econometric model under a range of parameter values. The neural net takes the moments of a dataset as input and tries to recognize the parameter value underlying that dataset. Besides the point estimate, the neural net can also output statistical accuracy. This neural net estimator (NNE) tends to limited-information Bayesian posterior as the number of training datasets increases. We apply NNE to a consumer search model. It gives more accurate estimates at lighter computational costs than the prevailing approach. NNE is also robust to redundant moment inputs. In general, NNE offers the most benefits in applications where other estimation approaches require very heavy simulation costs. We provide code at: https://nnehome.github.io. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04949</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04949</id><created>2025-02-07</created><authors><author><keyname>Elsemüller</keyname><forenames>Lasse</forenames></author><author><keyname>Pratz</keyname><forenames>Valentin</forenames></author><author><keyname>von Krause</keyname><forenames>Mischa</forenames></author><author><keyname>Voss</keyname><forenames>Andreas</forenames></author><author><keyname>Bürkner</keyname><forenames>Paul-Christian</forenames></author><author><keyname>Radev</keyname><forenames>Stefan T.</forenames></author></authors><title>Does Unsupervised Domain Adaptation Improve the Robustness of Amortized   Bayesian Inference? A Systematic Evaluation</title><categories>stat.ML cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Neural networks are fragile when confronted with data that significantly deviates from their training distribution. This is true in particular for simulation-based inference methods, such as neural amortized Bayesian inference (ABI), where models trained on simulated data are deployed on noisy real-world observations. Recent robust approaches employ unsupervised domain adaptation (UDA) to match the embedding spaces of simulated and observed data. However, the lack of comprehensive evaluations across different domain mismatches raises concerns about the reliability in high-stakes applications. We address this gap by systematically testing UDA approaches across a wide range of misspecification scenarios in both a controlled and a high-dimensional benchmark. We demonstrate that aligning summary spaces between domains effectively mitigates the impact of unmodeled phenomena or noise. However, the same alignment mechanism can lead to failures under prior misspecifications - a critical finding with practical consequences. Our results underscore the need for careful consideration of misspecification types when using UDA techniques to increase the robustness of ABI in practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04970</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04970</id><created>2025-02-07</created><authors><author><keyname>Langbein</keyname><forenames>Sophie Hanna</forenames></author><author><keyname>Koenen</keyname><forenames>Niklas</forenames></author><author><keyname>Wright</keyname><forenames>Marvin N.</forenames></author></authors><title>Gradient-based Explanations for Deep Learning Survival Models</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deep learning survival models often outperform classical methods in time-to-event predictions, particularly in personalized medicine, but their "black box" nature hinders broader adoption. We propose a framework for gradient-based explanation methods tailored to survival neural networks, extending their use beyond regression and classification. We analyze the implications of their theoretical assumptions for time-dependent explanations in the survival setting and propose effective visualizations incorporating the temporal dimension. Experiments on synthetic data show that gradient-based methods capture the magnitude and direction of local and global feature effects, including time dependencies. We introduce GradSHAP(t), a gradient-based counterpart to SurvSHAP(t), which outperforms SurvSHAP(t) and SurvLIME in a computational speed vs. accuracy trade-off. Finally, we apply these methods to medical data with multi-modal inputs, revealing relevant tabular features and visual patterns, as well as their temporal dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04990</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04990</id><created>2025-02-07</created><authors><author><keyname>Pichler</keyname><forenames>Clemens</forenames></author><author><keyname>Jewson</keyname><forenames>Jack</forenames></author><author><keyname>Avalos-Pacheco</keyname><forenames>Alejandra</forenames></author></authors><title>Probabilistic Programming with Sufficient Statistics for faster Bayesian   Computation</title><categories>stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Probabilistic programming methods have revolutionised Bayesian inference, making it easier than ever for practitioners to perform Markov-chain-Monte-Carlo sampling from non-conjugate posterior distributions. Here we focus on Stan, arguably the most used probabilistic programming tool for Bayesian inference (Carpenter et al., 2017), and its interface with R via the brms (Burkner, 2017) and rstanarm (Goodrich et al., 2024) packages. Although easy to implement, these tools can become computationally prohibitive when applied to datasets with many observations or models with numerous parameters. While the use of sufficient statistics is well-established in theory, it has been surprisingly overlooked in state-of-the-art Stan software. We show that when the likelihood can be written in terms of sufficient statistics, considerable computational improvements can be made to current implementations. We demonstrate how this approach provides accurate inference at a fraction of the time than state-of-the-art implementations for Gaussian linear regression models with non-conjugate priors, hierarchical random effects models, and factor analysis models. Our results also show that moderate computational gains can be achieved even in models where the likelihood can only be partially written in terms of sufficient statistics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05021</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05021</id><created>2025-02-07</created><authors><author><keyname>van Heel</keyname><forenames>Simon Donker</forenames></author><author><keyname>Lange</keyname><forenames>Rutger-Jan</forenames></author><author><keyname>van Dijk</keyname><forenames>Dick</forenames></author><author><keyname>van Os</keyname><forenames>Bram</forenames></author></authors><title>Stability and performance guarantees for misspecified multivariate   score-driven filters</title><categories>stat.ME eess.SP stat.ML</categories><comments>71 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the problem of tracking latent time-varying parameter vectors under model misspecification. We analyze implicit and explicit score-driven (ISD and ESD) filters, which update a prediction of the parameters using the gradient of the logarithmic observation density (i.e., the score). In the ESD filter, the score is computed using the predicted parameter values, whereas in the ISD filter, the score is evaluated using the new, updated parameter values. For both filter types, we derive novel sufficient conditions for the exponential stability (i.e., invertibility) of the filtered parameter path and existence of a finite mean squared error (MSE) bound with respect to the pseudo-true parameter path. In addition, we present expressions for finite-sample and asymptotic MSE bounds. Our performance guarantees rely on mild moment conditions on the data-generating process, while our stability result is entirely agnostic about the true process. As a result, our primary conditions depend only on the characteristics of the filter; hence, they are verifiable in practice. Concavity of the postulated log density combined with simple parameter restrictions is sufficient (but not necessary) for ISD-filter stability, whereas ESD-filter stability additionally requires the score to be Lipschitz continuous. Extensive simulation studies validate our theoretical findings and demonstrate the enhanced stability and improved performance of ISD over ESD filters. An empirical application to U.S. Treasury-bill rates confirms the practical relevance of our contribution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05072</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05072</id><created>2025-02-07</created><authors><author><keyname>Barnett</keyname><forenames>Helen</forenames></author><author><keyname>Boix</keyname><forenames>Oliver</forenames></author><author><keyname>Kontos</keyname><forenames>Dimitris</forenames></author><author><keyname>Jaki</keyname><forenames>Thomas</forenames></author></authors><title>Joint TITE-CRM for Dual Agent Dose Finding Studies</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Dual agent dose-finding trials study the effect of a combination of more than one agent, where the objective is to find the Maximum Tolerated Dose Combination (MTC), the combination of doses of the two agents that is associated with a pre-specified risk of being unsafe. In a Phase I/II setting, the objective is to find a dose combination that is both safe and active, the Optimal Biological Dose (OBD), that optimizes a criterion based on both safety and activity. Since Oncology treatments are typically given over multiple cycles, both the safety and activity outcome can be considered as late-onset, potentially occurring in the later cycles of treatment. This work proposes two model-based designs for dual-agent dose finding studies with late-onset activity and late-onset toxicity outcomes, the Joint TITE-POCRM and the Joint TITE-BLRM. Their performance is compared alongside a model-assisted comparator in a comprehensive simulation study motivated by a real trial example, with an extension to consider alternative sized dosing grids. It is found that both model-based methods outperform the model-assisted design. Whilst on average the two model-based designs are comparable, this comparability is not consistent across scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05074</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05074</id><created>2025-02-07</created><authors><author><keyname>Atanasov</keyname><forenames>Alexander</forenames></author><author><keyname>Bordelon</keyname><forenames>Blake</forenames></author><author><keyname>Zavatone-Veth</keyname><forenames>Jacob A.</forenames></author><author><keyname>Paquette</keyname><forenames>Courtney</forenames></author><author><keyname>Pehlevan</keyname><forenames>Cengiz</forenames></author></authors><title>Two-Point Deterministic Equivalence for Stochastic Gradient Dynamics in   Linear Models</title><categories>cond-mat.dis-nn cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We derive a novel deterministic equivalence for the two-point function of a random matrix resolvent. Using this result, we give a unified derivation of the performance of a wide variety of high-dimensional linear models trained with stochastic gradient descent. This includes high-dimensional linear regression, kernel regression, and random feature models. Our results include previously known asymptotics as well as novel ones. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05075</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05075</id><created>2025-02-07</created><authors><author><keyname>Dong</keyname><forenames>Yijun</forenames></author><author><keyname>Li</keyname><forenames>Yicheng</forenames></author><author><keyname>Li</keyname><forenames>Yunai</forenames></author><author><keyname>Lee</keyname><forenames>Jason D.</forenames></author><author><keyname>Lei</keyname><forenames>Qi</forenames></author></authors><title>Discrepancies are Virtue: Weak-to-Strong Generalization through Lens of   Intrinsic Dimension</title><categories>cs.LG cs.NA math.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weak-to-strong (W2S) generalization is a type of finetuning (FT) where a strong (large) student model is trained on pseudo-labels generated by a weak teacher. Surprisingly, W2S FT often outperforms the weak teacher. We seek to understand this phenomenon through the observation that FT often occurs in intrinsically low-dimensional spaces. Leveraging the low intrinsic dimensionality of FT, we analyze W2S in the ridgeless regression setting from a variance reduction perspective. For a strong student - weak teacher pair with sufficiently expressive low-dimensional feature subspaces $\mathcal{V}_s, \mathcal{V}_w$, we provide an exact characterization of the variance that dominates the generalization error of W2S. This unveils a virtue of discrepancy between the strong and weak models in W2S: the variance of the weak teacher is inherited by the strong student in $\mathcal{V}_s \cap \mathcal{V}_w$, while reduced by a factor of $\dim(\mathcal{V}_s)/N$ in the subspace of discrepancy $\mathcal{V}_w \setminus \mathcal{V}_s$ with $N$ pseudo-labels for W2S. Further, our analysis casts light on the sample complexities and the scaling of performance gap recovery in W2S. The analysis is supported with experiments on both synthetic regression problems and real vision tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05094</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05094</id><created>2025-02-07</created><authors><author><keyname>Blanchet</keyname><forenames>Jose</forenames></author><author><keyname>Hamoudi</keyname><forenames>Yassine</forenames></author><author><keyname>Szegedy</keyname><forenames>Mario</forenames></author><author><keyname>Wang</keyname><forenames>Guanyang</forenames></author></authors><title>Non-linear Quantum Monte Carlo</title><categories>quant-ph cs.LG cs.NA math.NA stat.CO stat.ML</categories><comments>30 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The mean of a random variable can be understood as a $\textit{linear}$ functional on the space of probability distributions. Quantum computing is known to provide a quadratic speedup over classical Monte Carlo methods for mean estimation. In this paper, we investigate whether a similar quadratic speedup is achievable for estimating $\textit{non-linear}$ functionals of probability distributions. We propose a quantum-inside-quantum Monte Carlo algorithm that achieves such a speedup for a broad class of non-linear estimation problems, including nested conditional expectations and stochastic optimization. Our algorithm improves upon the direct application of the quantum multilevel Monte Carlo algorithm introduced by An et al.. The existing lower bound indicates that our algorithm is optimal up polylogarithmic factors. A key innovation of our approach is a new sequence of multilevel Monte Carlo approximations specifically designed for quantum computing, which is central to the algorithm's improved performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05102</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05102</id><created>2025-02-07</created><authors><author><keyname>Piancastelli</keyname><forenames>Luiza</forenames></author><author><keyname>Barreto-Souza</keyname><forenames>Wagner</forenames></author></authors><title>Time Series Analysis of Rankings: A GARCH-Type Approach</title><categories>stat.ME stat.CO stat.ML</categories><comments>Paper submitted for publication</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Ranking data are frequently obtained nowadays but there are still scarce methods for treating these data when temporally observed. The present paper contributes to this topic by proposing and developing novel models for handling time series of ranking data. We introduce a class of time-varying ranking models inspired by the Generalized AutoRegressive Conditional Heteroskedasticity (GARCH) models. More specifically, the temporal dynamics are defined by the conditional distribution of the current ranking given the past rankings, which are assumed to follow a Mallows distribution, which implicitly depends on a distance. Then, autoregressive and feedback components are incorporated into the model through the conditional expectation of the associated distances. Theoretical properties of our ranking GARCH models such as stationarity and ergodicity are established. The estimation of parameters is performed via maximum likelihood estimation when data is fully observed. We develop a Monte Carlo Expectation-Maximisation algorithm to deal with cases involving missing data. Monte Carlo simulation studies are presented to study the performance of the proposed estimators under both non-missing and missing data scenarios. A real data application about the weekly ranking of professional tennis players from 2015 to 2019 is presented under our proposed ranking GARCH models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05122</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05122</id><created>2025-02-07</created><authors><author><keyname>Xi</keyname><forenames>Johnny</forenames></author><author><keyname>Dance</keyname><forenames>Hugh</forenames></author><author><keyname>Orbanz</keyname><forenames>Peter</forenames></author><author><keyname>Bloem-Reddy</keyname><forenames>Benjamin</forenames></author></authors><title>Distinguishing Cause from Effect with Causal Velocity Models</title><categories>stat.ML cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bivariate structural causal models (SCM) are often used to infer causal direction by examining their goodness-of-fit under restricted model classes. In this paper, we describe a parametrization of bivariate SCMs in terms of a causal velocity by viewing the cause variable as time in a dynamical system. The velocity implicitly defines counterfactual curves via the solution of initial value problems where the observation specifies the initial condition. Using tools from measure transport, we obtain a unique correspondence between SCMs and the score function of the generated distribution via its causal velocity. Based on this, we derive an objective function that directly regresses the velocity against the score function, the latter of which can be estimated non-parametrically from observational data. We use this to develop a method for bivariate causal discovery that extends beyond known model classes such as additive or location scale noise, and that requires no assumptions on the noise distributions. When the score is estimated well, the objective is also useful for detecting model non-identifiability and misspecification. We present positive results in simulation and benchmark experiments where many existing methods fail, and perform ablation studies to examine the method's sensitivity to accurate score estimation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05127</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05127</id><created>2025-02-07</created><authors><author><keyname>Everink</keyname><forenames>Jasper M.</forenames></author><author><keyname>Amougou</keyname><forenames>Bernardin Tamo</forenames></author><author><keyname>Pereyra</keyname><forenames>Marcelo</forenames></author></authors><title>Self-supervised Conformal Prediction for Uncertainty Quantification in   Imaging Problems</title><categories>cs.CV stat.ME</categories><msc-class>62H35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most image restoration problems are ill-conditioned or ill-posed and hence involve significant uncertainty. Quantifying this uncertainty is crucial for reliably interpreting experimental results, particularly when reconstructed images inform critical decisions and science. However, most existing image restoration methods either fail to quantify uncertainty or provide estimates that are highly inaccurate. Conformal prediction has recently emerged as a flexible framework to equip any estimator with uncertainty quantification capabilities that, by construction, have nearly exact marginal coverage. To achieve this, conformal prediction relies on abundant ground truth data for calibration. However, in image restoration problems, reliable ground truth data is often expensive or not possible to acquire. Also, reliance on ground truth data can introduce large biases in situations of distribution shift between calibration and deployment. This paper seeks to develop a more robust approach to conformal prediction for image restoration problems by proposing a self-supervised conformal prediction method that leverages Stein's Unbiased Risk Estimator (SURE) to self-calibrate itself directly from the observed noisy measurements, bypassing the need for ground truth. The method is suitable for any linear imaging inverse problem that is ill-conditioned, and it is especially powerful when used with modern self-supervised image restoration techniques that can also be trained directly from measurement data. The proposed approach is demonstrated through numerical experiments on image denoising and deblurring, where it delivers results that are remarkably accurate and comparable to those obtained by supervised conformal prediction with ground truth data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05134</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05134</id><created>2025-02-07</created><authors><author><keyname>Kızıldağ</keyname><forenames>Eren C.</forenames></author></authors><title>Information-Theoretic Guarantees for Recovering Low-Rank Tensors from   Symmetric Rank-One Measurements</title><categories>math.ST cs.IT math.IT math.PR stat.ML stat.TH</categories><comments>Algorithmic Learning Theory (ALT), 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the sample complexity of recovering tensors with low symmetric rank from symmetric rank-one measurements. This setting is particularly motivated by the study of higher-order interactions and the analysis of two-layer neural networks with polynomial activations (polynomial networks). Using a covering numbers argument, we analyze the performance of the symmetric rank minimization program and establish near-optimal sample complexity bounds when the underlying distribution is log-concave. Our measurement model involves random symmetric rank-one tensors, which lead to involved probability calculations. To address these challenges, we employ the Carbery-Wright inequality, a powerful tool for studying anti-concentration properties of random polynomials, and leverage orthogonal polynomials. Additionally, we provide a sample complexity lower bound based on Fano's inequality, and discuss broader implications of our results for two-layer polynomial networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05155</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05155</id><created>2025-02-07</created><authors><author><keyname>Tang</keyname><forenames>Shiqin</forenames></author><author><keyname>Yu</keyname><forenames>Shujian</forenames></author><author><keyname>Dong</keyname><forenames>Yining</forenames></author><author><keyname>Qin</keyname><forenames>S. Joe</forenames></author></authors><title>Deep Dynamic Probabilistic Canonical Correlation Analysis</title><categories>cs.LG stat.ML</categories><comments>accepted by ICASSP-25, code is available at   \url{https://github.com/marcusstang/D2PCCA}</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents Deep Dynamic Probabilistic Canonical Correlation Analysis (D2PCCA), a model that integrates deep learning with probabilistic modeling to analyze nonlinear dynamical systems. Building on the probabilistic extensions of Canonical Correlation Analysis (CCA), D2PCCA captures nonlinear latent dynamics and supports enhancements such as KL annealing for improved convergence and normalizing flows for a more flexible posterior approximation. D2PCCA naturally extends to multiple observed variables, making it a versatile tool for encoding prior knowledge about sequential datasets and providing a probabilistic understanding of the system's dynamics. Experimental validation on real financial datasets demonstrates the effectiveness of D2PCCA and its extensions in capturing latent dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05161</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05161</id><created>2025-02-07</created><authors><author><keyname>Antonczak</keyname><forenames>Brittany</forenames></author><author><keyname>Fay</keyname><forenames>Meg</forenames></author><author><keyname>Chawla</keyname><forenames>Aviral</forenames></author><author><keyname>Rowangould</keyname><forenames>Gregory</forenames></author></authors><title>Estimated Roadway Segment Traffic Data by Vehicle Class for the United   States: A Machine Learning Approach</title><categories>stat.AP</categories><comments>14 pages including references, 4 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Highway Performance Monitoring System, managed by the Federal Highway Administration, provides essential data on average annual daily traffic across U.S. roadways, but it has limited representation of medium- and heavy-duty vehicles on non-interstate roads. This gap limits research and policy analysis on the impacts of truck traffic, especially concerning air quality and public health. To address this, we use random forest regression to estimate medium- and heavy-duty vehicle traffic volumes in areas with sparse data. This results in a more comprehensive dataset, which enables the estimation of traffic density at the census block level as a proxy for traffic-related air pollution exposure. Our high-resolution spatial data products, rigorously validated, provide a more accurate representation of truck traffic and its environmental and health impacts. These datasets are valuable for transportation planning, public health research, and policy decisions aimed at mitigating the effects of truck traffic on vulnerable communities exposed to air pollution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:1811.12852</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>1811.12852</id><created>2018-11-30</created><updated>2025-02-06</updated><authors><author><keyname>Burnetas</keyname><forenames>Apostolos N.</forenames></author><author><keyname>Kanavetas</keyname><forenames>Odysseas</forenames></author><author><keyname>Katehakis</keyname><forenames>Michael N.</forenames></author></authors><title>Optimal Data Driven Resource Allocation under Multi-Armed Bandit   Observations</title><categories>stat.ML cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1509.02857</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the first asymptotically optimal strategy for a multi armed bandit (MAB) model under side constraints. The side constraints model situations in which bandit activations are limited by the availability of certain resources that are replenished at a constant rate. The main result involves the derivation of an asymptotic lower bound for the regret of feasible uniformly fast policies and the construction of policies that achieve this lower bound, under pertinent conditions. Further, we provide the explicit form of such policies for the case in which the unknown distributions are Normal with unknown means and known variances, for the case of Normal distributions with unknown means and unknown variances and for the case of arbitrary discrete distributions with finite support. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2202.04912</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2202.04912</id><created>2022-02-10</created><updated>2025-02-06</updated><authors><author><keyname>Qiu</keyname><forenames>Rui</forenames></author><author><keyname>Yu</keyname><forenames>Zhou</forenames></author><author><keyname>Zhu</keyname><forenames>Ruoqing</forenames></author></authors><title>Random Forest Weighted Local Fr\'echet Regression with Random Objects</title><categories>stat.ML cs.LG</categories><comments>This paper has been published in the Journal of Machine Learning   Research</comments><journal-ref>Journal of Machine Learning Research 25 (2024) 1-69</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical analysis is increasingly confronted with complex data from metric spaces. Petersen and M\"uller (2019) established a general paradigm of Fr\'echet regression with complex metric space valued responses and Euclidean predictors. However, the local approach therein involves nonparametric kernel smoothing and suffers from the curse of dimensionality. To address this issue, we in this paper propose a novel random forest weighted local Fr\'echet regression paradigm. The main mechanism of our approach relies on a locally adaptive kernel generated by random forests. Our first method uses these weights as the local average to solve the conditional Fr\'echet mean, while the second method performs local linear Fr\'echet regression, both significantly improving existing Fr\'echet regression methods. Based on the theory of infinite order U-processes and infinite order $M_{m_n}$-estimator, we establish the consistency, rate of convergence, and asymptotic normality for our local constant estimator, which covers the current large sample theory of random forests with Euclidean responses as a special case. Numerical studies show the superiority of our methods with several commonly encountered types of responses such as distribution functions, symmetric positive-definite matrices, and sphere data. The practical merits of our proposals are also demonstrated through the application to New York taxi data and human mortality data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2206.04902</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2206.04902</id><created>2022-06-10</created><updated>2024-11-13</updated><authors><author><keyname>Gruber</keyname><forenames>Luis</forenames></author><author><keyname>Kastner</keyname><forenames>Gregor</forenames></author></authors><title>Forecasting macroeconomic data with Bayesian VARs: Sparse or dense? It   depends!</title><categories>econ.EM stat.AP stat.ME</categories><journal-ref>International Journal of Forecasting (2025)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vector autogressions (VARs) are widely applied when it comes to modeling and forecasting macroeconomic variables. In high dimensions, however, they are prone to overfitting. Bayesian methods, more concretely shrinkage priors, have shown to be successful in improving prediction performance. In the present paper, we introduce the semi-global framework, in which we replace the traditional global shrinkage parameter with group-specific shrinkage parameters. We show how this framework can be applied to various shrinkage priors, such as global-local priors and stochastic search variable selection priors. We demonstrate the virtues of the proposed framework in an extensive simulation study and in an empirical application forecasting data of the US economy. Further, we shed more light on the ongoing ``Illusion of Sparsity'' debate, finding that forecasting performances under sparse/dense priors vary across evaluated economic variables and across time frames. Dynamic model averaging, however, can combine the merits of both worlds. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2211.15353</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2211.15353</id><created>2022-11-25</created><updated>2025-02-07</updated><authors><author><keyname>Letizia</keyname><forenames>Nunzio A.</forenames></author><author><keyname>Tonello</keyname><forenames>Andrea M.</forenames></author></authors><title>Copula Density Neural Estimation</title><categories>cs.LG eess.SP stat.ML</categories><comments>6 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Probability density estimation from observed data constitutes a central task in statistics. Recent advancements in machine learning offer new tools but also pose new challenges. The big data era demands analysis of long-range spatial and long-term temporal dependencies in large collections of raw data, rendering neural networks an attractive solution for density estimation. In this paper, we exploit the concept of copula to explicitly build an estimate of the probability density function associated to any observed data. In particular, we separate univariate marginal distributions from the joint dependence structure in the data, the copula itself, and we model the latter with a neural network-based method referred to as copula density neural estimation (CODINE). Results show that the novel learning approach is capable of modeling complex distributions and it can be applied for mutual information estimation and data generation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2212.09544</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2212.09544</id><created>2022-12-19</created><authors><author><keyname>Gardini</keyname><forenames>Aldo</forenames></author><author><keyname>Greco</keyname><forenames>Fedele</forenames></author><author><keyname>Trivisano</keyname><forenames>Carlo</forenames></author></authors><title>Design and Structure Dependent Priors for Scale Parameters in Latent   Gaussian Models</title><categories>stat.ME math.ST stat.TH</categories><doi>10.1214/24-BA1454</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Many common correlation structures assumed for data can be described through latent Gaussian models. When Bayesian inference is carried out, it is required to set the prior distribution for scale parameters that rules the model components, possibly allowing to incorporate prior information. This task is particularly delicate and many contributions in the literature are devoted to investigating such aspects. We focus on the fact that the scale parameter controls the prior variability of the model component in a complex way since its dispersion is also affected by the correlation structure and the design. To overcome this issue that might confound the prior elicitation step, we propose to let the user specify the marginal prior of a measure of dispersion of the model component, integrating out the scale parameter, the structure and the design. Then, we analytically derive the implied prior for the scale parameter. Results from a simulation study, aimed at showing the behavior of the estimators sampling properties under the proposed prior elicitation strategy, are discussed. Lastly, some real data applications are explored to investigate prior sensitivity and allocation of explained variance among model components. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2304.13077</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2304.13077</id><created>2023-04-25</created><updated>2025-02-07</updated><authors><author><keyname>De Vito</keyname><forenames>Roberta</forenames></author><author><keyname>Avalos-Pacheco</keyname><forenames>Alejandra</forenames></author></authors><title>Multi-study factor regression model: an application in nutritional   epidemiology</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diet is a risk factor for many diseases. In nutritional epidemiology, studying reproducible dietary patterns is critical to reveal important associations with health. However, it is challenging: diverse cultural and ethnic backgrounds may critically impact eating patterns, showing heterogeneity, leading to incorrect dietary patterns and obscuring the components shared across different groups or populations. Moreover, covariate effects generated from observed variables, such as demographics and other confounders, can further bias these dietary patterns. Identifying the shared and group-specific dietary components and covariate effects is essential to drive accurate conclusions. To address these issues, we introduce a new modeling factor regression, the Multi-Study Factor Regression (MSFR) model. The MSFR model analyzes different populations simultaneously, achieving three goals: capturing shared component(s) across populations, identifying group-specific structures, and correcting for covariate effects. We use this novel method to derive common and ethnic-specific dietary patterns in a multi-center epidemiological study in Hispanic/Latinos community. Our model improves the accuracy of common and group dietary signals and yields better prediction than other techniques, revealing significant associations with health. In summary, we provide a tool to integrate different groups, giving accurate dietary signals crucial to inform public health policy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2305.13998</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2305.13998</id><created>2023-05-23</created><updated>2024-01-23</updated><authors><author><keyname>Saves</keyname><forenames>Paul</forenames></author><author><keyname>Lafage</keyname><forenames>Remi</forenames></author><author><keyname>Bartoli</keyname><forenames>Nathalie</forenames></author><author><keyname>Diouane</keyname><forenames>Youssef</forenames></author><author><keyname>Bussemaker</keyname><forenames>Jasper</forenames></author><author><keyname>Lefebvre</keyname><forenames>Thierry</forenames></author><author><keyname>Hwang</keyname><forenames>John T.</forenames></author><author><keyname>Morlier</keyname><forenames>Joseph</forenames></author><author><keyname>Martins</keyname><forenames>Joaquim R. R. A.</forenames></author></authors><title>SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and   Mixed Variables Gaussian Processes</title><categories>cs.LG cs.MS math.OC stat.CO</categories><comments>10.1016/j.advengsoft.2023.103571</comments><journal-ref>Advances in Engineering Software Volume 188, February 2024, 103571</journal-ref><doi>10.1016/j.advengsoft.2023.103571</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multifidelity data. To the best of our knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical and mixed inputs. This open-source software is distributed under the New BSD license. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2306.16033</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2306.16033</id><created>2023-06-28</created><authors><author><keyname>Gardini</keyname><forenames>Aldo</forenames></author></authors><title>Functional and variables selection in extreme value models for regional   flood frequency analysis</title><categories>stat.ME stat.AP</categories><doi>10.1007/s10651-023-00581-8</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The problem of estimating return levels of river discharge, relevant in flood frequency analysis, is tackled by relying on the extreme value theory. The Generalized Extreme Value (GEV) distribution is assumed to model annual maxima values of river discharge registered at multiple gauging stations belonging to the same river basin. The specific features of the data from the Upper Danube basin drive the definition of the proposed statistical model. Firstly, Bayesian P-splines are considered to account for the non-linear effects of station-specific covariates on the GEV parameters. Secondly, the problem of functional and variable selection is addressed by imposing a grouped horseshoe prior on the coefficients, to encourage the shrinkage of non-relevant components to zero. A cross-validation study is organized to compare the proposed modeling solution to other models, showing its potential in reducing the uncertainty of the ungauged predictions without affecting their calibration. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2307.10272</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2307.10272</id><created>2023-07-18</created><updated>2025-02-06</updated><authors><author><keyname>Takeishi</keyname><forenames>Shota</forenames></author></authors><title>A Shrinkage Likelihood Ratio Test for High-Dimensional Subgroup Analysis   with a Logistic-Normal Mixture Model</title><categories>math.ST stat.ME stat.TH</categories><comments>38 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In subgroup analysis, testing the existence of a subgroup with a differential treatment effect serves as protection against spurious subgroup discovery. Despite its importance, this hypothesis testing possesses a complicated nature: parameter characterizing subgroup classification is not identified under the null hypothesis of no subgroup. Due to this irregularity, the existing methods have the following two limitations. First, the asymptotic null distribution of test statistics often takes an intractable form, which necessitates computationally demanding resampling methods to calculate the critical value. Second, the dimension of personal attributes characterizing subgroup membership is not allowed to be of high dimension. To solve these two problems simultaneously, this study develops a shrinkage likelihood ratio test for the existence of a subgroup using a logistic-normal mixture model. The proposed test statistics are built on a modified likelihood function that shrinks possibly high-dimensional unidentified parameters toward zero under the null hypothesis while retaining power under the alternative. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2307.12982</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2307.12982</id><created>2023-07-24</created><updated>2025-02-07</updated><authors><author><keyname>Mukherjee</keyname><forenames>Soumendu Sundar</forenames></author></authors><title>Consistent model selection in the spiked Wigner model via AIC-type   criteria</title><categories>math.ST cs.IT math.IT stat.ME stat.ML stat.TH</categories><comments>25 pages, 2 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the spiked Wigner model \[   X = \sum_{i = 1}^k \lambda_i u_i u_i^\top + \sigma G, \] where $G$ is an $N \times N$ GOE random matrix, and the eigenvalues $\lambda_i$ are all spiked, i.e. above the Baik-Ben Arous-P\'ech\'e (BBP) threshold $\sigma$. We consider AIC-type model selection criteria of the form \[   -2 \, (\text{maximised log-likelihood}) + \gamma \, (\text{number of parameters}) \] for estimating the number $k$ of spikes. For $\gamma &gt; 2$, the above criterion is strongly consistent provided $\lambda_k &gt; \lambda_{\gamma}$, where $\lambda_{\gamma}$ is a threshold strictly above the BBP threshold, whereas for $\gamma &lt; 2$, it almost surely overestimates $k$. Although AIC (which corresponds to $\gamma = 2$) is not strongly consistent, we show that taking $\gamma = 2 + \delta_N$, where $\delta_N \to 0$ and $\delta_N \gg N^{-2/3}$, results in a weakly consistent estimator of $k$. We further show that a soft minimiser of AIC, where one chooses the least complex model whose AIC score is close to the minimum AIC score, is strongly consistent. Based on a spiked (generalised) Wigner representation, we also develop similar model selection criteria for consistently estimating the number of communities in a balanced stochastic block model under some sparsity restrictions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.07399</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.07399</id><created>2023-10-11</created><updated>2024-10-03</updated><authors><author><keyname>Bou-Rabee</keyname><forenames>Nawaf</forenames></author><author><keyname>Kleppe</keyname><forenames>Tore Selland</forenames></author></authors><title>Randomized Runge-Kutta-Nystr\"om Methods for Unadjusted Hamiltonian and   Kinetic Langevin Monte Carlo</title><categories>math.NA cs.NA math.PR stat.CO stat.ME stat.ML</categories><msc-class>60J05 (Primary) 65C05, 65L05, 65P10 (Secondary)</msc-class><journal-ref>Mathematics of Computation 2025</journal-ref><doi>10.1090/mcom/4061</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce $5/2$- and $7/2$-order $L^2$-accurate randomized Runge-Kutta-Nystr\"{o}m methods, tailored for approximating Hamiltonian flows within non-reversible Markov chain Monte Carlo samplers, such as unadjusted Hamiltonian Monte Carlo and unadjusted kinetic Langevin Monte Carlo. We establish quantitative $5/2$-order $L^2$-accuracy upper bounds under gradient and Hessian Lipschitz assumptions on the potential energy function. The numerical experiments demonstrate the superior efficiency of the proposed unadjusted samplers on a variety of well-behaved, high-dimensional target distributions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2311.02655</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2311.02655</id><created>2023-11-05</created><updated>2025-02-06</updated><authors><author><keyname>Horst</keyname><forenames>Ulrich</forenames></author><author><keyname>Xu</keyname><forenames>Wei</forenames></author></authors><title>Second-Order Regular Variation and Second-Order Approximation of Hawkes   Processes</title><categories>math.PR math.FA math.ST stat.TH</categories><comments>40 pages</comments><msc-class>Primary 26A12, 40E05, secondary 60G55, 60K05</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper provides and extends second-order versions of several fundamental theorems on first-order regularly varying functions such as Karamata's theorem/representation and Tauberian's theorem. Our results are used to establish second-order approximations for the mean and variance of Hawkes processes with general kernels. Our approximations provide novel insights into the asymptotic behavior of Hawkes processes. They are also of key importance when establishing functional limit theorems for Hawkes processes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.04711</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.04711</id><created>2024-02-07</created><updated>2024-05-26</updated><authors><author><keyname>Saves</keyname><forenames>Paul</forenames></author></authors><title>High-dimensional multidisciplinary design optimization for aircraft   eco-design / Optimisation multi-disciplinaire en grande dimension pour   l'\'eco-conception avion en avant-projet</title><categories>math.OC cs.MS stat.ML</categories><comments>PhD Thesis, Universit\'e de Toulouse, Toulouse, 2024 on Gaussian   Process kernels for Bayesian optimization in high dimension with mixed and   hierarchical variables at ISAE-SUPAERO. Keywords: Gaussian process, Black-box   optimization, Bayesian inference, Multidisciplinary design optimization,   Mixed hierarchical and categorical inputs, Eco-friendly aircraft design</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The objective of this Philosophiae Doctor (Ph.D) thesis is to propose an efficient approach for optimizing a multidisciplinary black-box model when the optimization problem is constrained and involves a large number of mixed integer design variables (typically 100 variables). The targeted optimization approach, called EGO, is based on a sequential enrichment of an adaptive surrogate model and, in this context, GP surrogate models are one of the most widely used in engineering problems to approximate time-consuming high fidelity models. EGO is a heuristic BO method that performs well in terms of solution quality. However, like any other global optimization method, EGO suffers from the curse of dimensionality, meaning that its performance is satisfactory on lower dimensional problems, but deteriorates as the dimensionality of the optimization search space increases. For realistic aircraft design problems, the typical size of the design variables can even exceed 100 and, thus, trying to solve directly the problems using EGO is ruled out. The latter is especially true when the problems involve both continuous and categorical variables increasing even more the size of the search space. In this Ph.D thesis, effective parameterization tools are investigated, including techniques like partial least squares regression, to significantly reduce the number of design variables. Additionally, Bayesian optimization is adapted to handle discrete variables and high-dimensional spaces in order to reduce the number of evaluations when optimizing innovative aircraft concepts such as the "DRAGON" hybrid airplane to reduce their climate impact. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.13196</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.13196</id><created>2024-03-19</created><updated>2025-02-07</updated><authors><author><keyname>Eskandar</keyname><forenames>Masih</forenames></author><author><keyname>Imtiaz</keyname><forenames>Tooba</forenames></author><author><keyname>Wang</keyname><forenames>Zifeng</forenames></author><author><keyname>Dy</keyname><forenames>Jennifer</forenames></author></authors><title>ADAPT to Robustify Prompt Tuning Vision Transformers</title><categories>cs.LG cs.AI cs.CV stat.ML</categories><comments>Published in Transactions on Machine Learning Research (2025)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The performance of deep models, including Vision Transformers, is known to be vulnerable to adversarial attacks. Many existing defenses against these attacks, such as adversarial training, rely on full-model fine-tuning to induce robustness in the models. These defenses require storing a copy of the entire model, that can have billions of parameters, for each task. At the same time, parameter-efficient prompt tuning is used to adapt large transformer-based models to downstream tasks without the need to save large copies. In this paper, we examine parameter-efficient prompt tuning of Vision Transformers for downstream tasks under the lens of robustness. We show that previous adversarial defense methods, when applied to the prompt tuning paradigm, suffer from gradient obfuscation and are vulnerable to adaptive attacks. We introduce ADAPT, a novel framework for performing adaptive adversarial training in the prompt tuning paradigm. Our method achieves competitive robust accuracy of ~40% w.r.t. SOTA robustness methods using full-model fine-tuning, by tuning only ~1% of the number of parameters. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.13340</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.13340</id><created>2024-03-20</created><updated>2025-02-07</updated><authors><author><keyname>Jiménez-Varón</keyname><forenames>Cristian F.</forenames></author><author><keyname>Sun</keyname><forenames>Ying</forenames></author><author><keyname>Shang</keyname><forenames>Han Lin</forenames></author></authors><title>Forecasting density-valued functional panel data</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce a statistical method for modeling and forecasting functional panel data represented by multiple densities. Density functions are nonnegative and have a constrained integral and thus do not constitute a linear vector space. We implement a center log-ratio transformation to transform densities into unconstrained functions. These functions exhibit cross-sectional correlation and temporal dependence. Via a functional analysis of variance decomposition, we decompose the unconstrained functional panel data into a deterministic trend component and a time-varying residual component. To produce forecasts for the time-varying component, a functional time series forecasting method, based on the estimation of the long-run covariance, is implemented. By combining the forecasts of the time-varying residual component with the deterministic trend component, we obtain $h$-step-ahead forecast curves for multiple populations. Illustrated by age- and sex-specific life-table death counts in the United States, we apply our proposed method to generate forecasts of the life-table death counts for 51 states. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.12120</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.12120</id><created>2024-06-17</created><updated>2025-02-06</updated><authors><author><keyname>Zhao</keyname><forenames>Yulai</forenames></author><author><keyname>Uehara</keyname><forenames>Masatoshi</forenames></author><author><keyname>Scalia</keyname><forenames>Gabriele</forenames></author><author><keyname>Kung</keyname><forenames>Sunyuan</forenames></author><author><keyname>Biancalani</keyname><forenames>Tommaso</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Hajiramezanali</keyname><forenames>Ehsan</forenames></author></authors><title>Adding Conditional Control to Diffusion Models with Reinforcement   Learning</title><categories>cs.LG cs.AI stat.ML</categories><comments>ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diffusion models are powerful generative models that allow for precise control over the characteristics of the generated samples. While these diffusion models trained on large datasets have achieved success, there is often a need to introduce additional controls in downstream fine-tuning processes, treating these powerful models as pre-trained diffusion models. This work presents a novel method based on reinforcement learning (RL) to add such controls using an offline dataset comprising inputs and labels. We formulate this task as an RL problem, with the classifier learned from the offline dataset and the KL divergence against pre-trained models serving as the reward functions. Our method, $\textbf{CTRL}$ ($\textbf{C}$onditioning pre-$\textbf{T}$rained diffusion models with $\textbf{R}$einforcement $\textbf{L}$earning), produces soft-optimal policies that maximize the abovementioned reward functions. We formally demonstrate that our method enables sampling from the conditional distribution with additional controls during inference. Our RL-based approach offers several advantages over existing methods. Compared to classifier-free guidance, it improves sample efficiency and can greatly simplify dataset construction by leveraging conditional independence between the inputs and additional controls. Additionally, unlike classifier guidance, it eliminates the need to train classifiers from intermediate states to additional controls. The code is available at https://github.com/zhaoyl18/CTRL. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.14535</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.14535</id><created>2024-06-20</created><updated>2025-02-06</updated><authors><author><keyname>Deng</keyname><forenames>Shiyuan</forenames></author><author><keyname>Tang</keyname><forenames>He</forenames></author><author><keyname>Bai</keyname><forenames>Shuyang</forenames></author></authors><title>On estimation and order selection for multivariate extremes via   clustering</title><categories>stat.ME math.ST stat.TH</categories><msc-class>62G32 (Primary), 60G70 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the estimation of multivariate extreme models with a discrete spectral measure using spherical clustering techniques. The primary contribution involves devising a method for selecting the order, that is, the number of clusters. The method consistently identifies the true order, i.e., the number of spectral atoms, and enjoys intuitive implementation in practice. Specifically, we introduce an extra penalty term to the well-known simplified average silhouette width, which penalizes small cluster sizes and small dissimilarities between cluster centers. Consequently, we provide a consistent method for determining the order of a max-linear factor model, where a typical information-based approach is not viable. Our second contribution is a large-deviation-type analysis for estimating the discrete spectral measure through clustering methods, which serves as an assessment of the convergence quality of clustering-based estimation for multivariate extremes. Additionally, as a third contribution, we discuss how estimating the discrete measure can lead to parameter estimations of heavy-tailed factor models. We also present simulations and real-data studies that demonstrate order selection and factor model estimation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.06120</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.06120</id><created>2024-07-08</created><updated>2024-11-16</updated><authors><author><keyname>Dong</keyname><forenames>Yijun</forenames></author><author><keyname>Phan</keyname><forenames>Hoang</forenames></author><author><keyname>Pan</keyname><forenames>Xiang</forenames></author><author><keyname>Lei</keyname><forenames>Qi</forenames></author></authors><title>Sketchy Moment Matching: Toward Fast and Provable Data Selection for   Finetuning</title><categories>cs.LG cs.NA math.NA stat.ML</categories><comments>NeurIPS 2024</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit data selection in a modern context of finetuning from a fundamental perspective. Extending the classical wisdom of variance minimization in low dimensions to high-dimensional finetuning, our generalization analysis unveils the importance of additionally reducing bias induced by low-rank approximation. Inspired by the variance-bias tradeoff in high dimensions from the theory, we introduce Sketchy Moment Matching (SkMM), a scalable data selection scheme with two stages. (i) First, the bias is controlled using gradient sketching that explores the finetuning parameter space for an informative low-dimensional subspace $\mathcal{S}$; (ii) then the variance is reduced over $\mathcal{S}$ via moment matching between the original and selected datasets. Theoretically, we show that gradient sketching is fast and provably accurate: selecting $n$ samples by reducing variance over $\mathcal{S}$ preserves the fast-rate generalization $O(\dim(\mathcal{S})/n)$, independent of the parameter dimension. Empirically, we concretize the variance-bias balance via synthetic experiments and demonstrate the effectiveness of SkMM for finetuning in real vision tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.21314</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.21314</id><created>2024-07-30</created><updated>2025-02-07</updated><authors><author><keyname>Li</keyname><forenames>Zhuoyuan</forenames></author><author><keyname>Dong</keyname><forenames>Bin</forenames></author><author><keyname>Zhang</keyname><forenames>Pingwen</forenames></author></authors><title>State-observation augmented diffusion model for nonlinear assimilation   with unknown dynamics</title><categories>cs.LG stat.ML</categories><msc-class>49N45, 60J60, 62F15, 68T20</msc-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Data assimilation has become a key technique for combining physical models with observational data to estimate state variables. However, classical assimilation algorithms often struggle with the high nonlinearity present in both physical and observational models. To address this challenge, a novel generative model, termed the State-Observation Augmented Diffusion (SOAD) model is proposed for data-driven assimilation. The marginal posterior associated with SOAD has been derived and then proved to match the true posterior distribution under mild assumptions, suggesting its theoretical advantages over previous score-based approaches. Experimental results also indicate that SOAD may offer improved performance compared to existing data-driven methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.03733</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.03733</id><created>2024-08-07</created><authors><author><keyname>Maillard</keyname><forenames>Antoine</forenames></author><author><keyname>Troiani</keyname><forenames>Emanuele</forenames></author><author><keyname>Martin</keyname><forenames>Simon</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Zdeborová</keyname><forenames>Lenka</forenames></author></authors><title>Bayes-optimal learning of an extensive-width neural network from   quadratically many samples</title><categories>stat.ML cond-mat.dis-nn cs.IT cs.LG math.IT math.PR</categories><comments>47 pages</comments><journal-ref>Advances in Neural Information Processing Systems 37 (NeurIPS   2024)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning a target function corresponding to a single hidden layer neural network, with a quadratic activation function after the first layer, and random weights. We consider the asymptotic limit where the input dimension and the network width are proportionally large. Recent work [Cui &amp; al '23] established that linear regression provides Bayes-optimal test error to learn such a function when the number of available samples is only linear in the dimension. That work stressed the open challenge of theoretically analyzing the optimal test error in the more interesting regime where the number of samples is quadratic in the dimension. In this paper, we solve this challenge for quadratic activations and derive a closed-form expression for the Bayes-optimal test error. We also provide an algorithm, that we call GAMP-RIE, which combines approximate message passing with rotationally invariant matrix denoising, and that asymptotically achieves the optimal performance. Technically, our result is enabled by establishing a link with recent works on optimal denoising of extensive-rank matrices and on the ellipsoid fitting problem. We further show empirically that, in the absence of noise, randomly-initialized gradient descent seems to sample the space of weights, leading to zero training loss, and averaging over initialization leads to a test error equal to the Bayes-optimal one. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.09004</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.09004</id><created>2024-08-16</created><updated>2025-02-06</updated><authors><author><keyname>Subedi</keyname><forenames>Unique</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Controlling Statistical, Discretization, and Truncation Errors in   Learning Fourier Linear Operators</title><categories>stat.ML cs.LG cs.NA math.NA</categories><comments>Added Experiments</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study learning-theoretic foundations of operator learning, using the linear layer of the Fourier Neural Operator architecture as a model problem. First, we identify three main errors that occur during the learning process: statistical error due to finite sample size, truncation error from finite rank approximation of the operator, and discretization error from handling functional data on a finite grid of domain points. Finally, we analyze a Discrete Fourier Transform (DFT) based least squares estimator, establishing both upper and lower bounds on the aforementioned errors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.09576</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.09576</id><created>2024-08-18</created><updated>2025-02-07</updated><authors><author><keyname>Oubari</keyname><forenames>Fouad</forenames></author><author><keyname>Baha</keyname><forenames>Mohamed El</forenames></author><author><keyname>Meunier</keyname><forenames>Raphael</forenames></author><author><keyname>Décatoire</keyname><forenames>Rodrigue</forenames></author><author><keyname>Mougeot</keyname><forenames>Mathilde</forenames></author></authors><title>A Markov Random Field Multi-Modal Variational AutoEncoder</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Recent advancements in multimodal Variational AutoEncoders (VAEs) have highlighted their potential for modeling complex data from multiple modalities. However, many existing approaches use relatively straightforward aggregating schemes that may not fully capture the complex dynamics present between different modalities. This work introduces a novel multimodal VAE that incorporates a Markov Random Field (MRF) into both the prior and posterior distributions. This integration aims to capture complex intermodal interactions more effectively. Unlike previous models, our approach is specifically designed to model and leverage the intricacies of these relationships, enabling a more faithful representation of multimodal data. Our experiments demonstrate that our model performs competitively on the standard PolyMNIST dataset and shows superior performance in managing complex intermodal dependencies in a specially designed synthetic dataset, intended to test intricate relationships. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.02363</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.02363</id><created>2024-09-03</created><updated>2025-02-06</updated><authors><author><keyname>Maiti</keyname><forenames>Ayan</forenames></author><author><keyname>Michelle</keyname><forenames>Michelle</forenames></author><author><keyname>Yang</keyname><forenames>Haizhao</forenames></author></authors><title>Optimal Neural Network Approximation for High-Dimensional Continuous   Functions</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the authors of Shen Yang Zhang (JMLR, 2022) developed a neural network with width $36d(2d + 1)$ and depth $11$, which utilizes a special activation function called the elementary universal activation function, to achieve the super approximation property for functions in $C([a,b]^d)$. That is, the constructed network only requires a fixed number of neurons (and thus parameters) to approximate a $d$-variate continuous function on a $d$-dimensional hypercube with arbitrary accuracy. More specifically, only $\mathcal{O}(d^2)$ neurons or parameters are used. One natural question is whether we can reduce the number of these neurons or parameters in such a network. By leveraging a variant of the Kolmogorov Superposition Theorem, our analysis shows that there is a neural network generated by the elementary universal activation function with at most $10889d+10887$ unique nonzero parameters such that this super approximation property is attained. Furthermore, we present a family of continuous functions that requires at least width $d$, and thus at least $d$ neurons or parameters, to achieve arbitrary accuracy in its approximation. This suggests that the number of unique nonzero parameters is optimal in the sense that it grows linearly with the input dimension $d$, unlike some approximation methods where parameters may grow exponentially with $d$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.09800</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.09800</id><created>2024-09-15</created><updated>2025-02-06</updated><authors><author><keyname>Calvello</keyname><forenames>Edoardo</forenames></author><author><keyname>Monmarché</keyname><forenames>Pierre</forenames></author><author><keyname>Stuart</keyname><forenames>Andrew M.</forenames></author><author><keyname>Vaes</keyname><forenames>Urbain</forenames></author></authors><title>Accuracy of the Ensemble Kalman Filter in the Near-Linear Setting</title><categories>math.ST cs.NA math.DS math.NA math.OC stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The filtering distribution captures the statistics of the state of a dynamical system from partial and noisy observations. Classical particle filters provably approximate this distribution in quite general settings; however they behave poorly for high dimensional problems, suffering weight collapse. This issue is circumvented by the ensemble Kalman filter which is an equal-weight interacting particle system. However, this finite particle system is only proven to approximate the true filter in the linear Gaussian case. In practice, however, it is applied in much broader settings; as a result, establishing its approximation properties more generally is important. There has been recent progress in the theoretical analysis of the algorithm, establishing stability and error estimates in non-Gaussian settings, but the assumptions on the dynamics and observation models rule out the unbounded vector fields that arise in practice and the analysis applies only to the mean field limit of the ensemble Kalman filter. The present work establishes error bounds between the filtering distribution and the finite particle ensemble Kalman filter when the dynamics and observation vector fields may be unbounded, allowing linear growth. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.11381</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.11381</id><created>2024-09-17</created><updated>2025-02-07</updated><authors><author><keyname>Banerjee</keyname><forenames>Debapratim</forenames></author><author><keyname>Mukherjee</keyname><forenames>Soumendu Sundar</forenames></author><author><keyname>Pal</keyname><forenames>Dipranjan</forenames></author></authors><title>Edge spectra of Gaussian random symmetric matrices with correlated   entries</title><categories>math.PR math-ph math.CO math.MP math.ST stat.TH</categories><comments>27 pages, 2 figures; abstract shortened to meet arXiv requirements</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the largest eigenvalue of a Gaussian random symmetric matrix $X_n$, with zero-mean, unit variance entries satisfying the condition $\sup_{(i, j) \ne (i', j')}|\mathbb{E}[X_{ij} X_{i'j'}]| = O(n^{-(1 + \varepsilon)})$, where $\varepsilon &gt; 0$. It follows from Catalano et al. (2024) that the empirical spectral distribution of $n^{-1/2} X_n$ converges weakly almost surely to the standard semi-circle law. Using a F\"{u}redi-Koml\'{o}s-type high moment analysis, we show that the largest eigenvalue $\lambda_1(n^{-1/2} X_n)$ of $n^{-1/2} X_n$ converges almost surely to $2$. This result is essentially optimal in the sense that one cannot take $\varepsilon = 0$ and still obtain an almost sure limit of $2$. We also derive Gaussian fluctuation results for the largest eigenvalue in the case where the entries have a common non-zero mean. Let $Y_n = X_n + \frac{\lambda}{\sqrt{n}}\mathbf{1} \mathbf{1}^\top$. When $\varepsilon \ge 1$ and $\lambda \gg n^{1/4}$, we show that \[   n^{1/2}\bigg(\lambda_1(n^{-1/2} Y_n) - \lambda - \frac{1}{\lambda}\bigg) \xrightarrow{d} \sqrt{2} Z, \] where $Z$ is a standard Gaussian. On the other hand, when $0 &lt; \varepsilon &lt; 1$, we have $\mathrm{Var}(\frac{1}{n}\sum_{i, j}X_{ij}) = O(n^{1 - \varepsilon})$. Assuming that $\mathrm{Var}(\frac{1}{n}\sum_{i, j} X_{ij}) = \sigma^2 n^{1 - \varepsilon} (1 + o(1))$, if $\lambda \gg n^{\varepsilon/4}$, then we have \[   n^{\varepsilon/2}\bigg(\lambda_1(n^{-1/2} Y_n) - \lambda - \frac{1}{\lambda}\bigg) \xrightarrow{d} \sigma Z. \] While the ranges of $\lambda$ in these fluctuation results are certainly not optimal, a striking aspect is that different scalings are required in the two regimes $0 &lt; \varepsilon &lt; 1$ and $\varepsilon \ge 1$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.19200</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.19200</id><created>2024-09-27</created><updated>2025-02-06</updated><authors><author><keyname>Bai</keyname><forenames>Site</forenames></author><author><keyname>Bullins</keyname><forenames>Brian</forenames></author></authors><title>Faster Acceleration for Steepest Descent</title><categories>math.OC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances (Sherman, 2017; Sidford and Tian, 2018; Cohen et al., 2021) have overcome the fundamental barrier of dimension dependence in the iteration complexity of solving $\ell_\infty$ regression with first-order methods. Yet it remains unclear to what extent such acceleration can be achieved for general $\ell_p$ smooth functions. In this paper, we propose a new accelerated first-order method for convex optimization under non-Euclidean smoothness assumptions. In contrast to standard acceleration techniques, our approach uses primal-dual iterate sequences taken with respect to $\textit{differing}$ norms, which are then coupled using an $\textit{implicitly}$ determined interpolation parameter. For $\ell_p$ norm smooth problems in $d$ dimensions, our method provides an iteration complexity improvement of up to $O(d^{1-\frac{2}{p}})$ in terms of calls to a first-order oracle, thereby allowing us to circumvent long-standing barriers in accelerated non-Euclidean steepest descent. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.03159</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.03159</id><created>2024-10-04</created><updated>2025-02-07</updated><authors><author><keyname>Lu</keyname><forenames>Jiecheng</forenames></author><author><keyname>Han</keyname><forenames>Xu</forenames></author><author><keyname>Sun</keyname><forenames>Yan</forenames></author><author><keyname>Yang</keyname><forenames>Shihao</forenames></author></authors><title>WAVE: Weighted Autoregressive Varing Gate for Time Series Forecasting</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Weighted Autoregressive Varing gatE (WAVE) attention mechanism equipped with both Autoregressive (AR) and Moving-average (MA) components. It can adapt to various attention mechanisms, enhancing and decoupling their ability to capture long-range and local temporal patterns in time series data. In this paper, we first demonstrate that, for the time series forecasting (TSF) task, the previously overlooked decoder-only autoregressive Transformer model can achieve results comparable to the best baselines when appropriate tokenization and training methods are applied. Moreover, inspired by the ARMA model from statistics and recent advances in linear attention, we introduce the full ARMA structure into existing autoregressive attention mechanisms. By using an indirect MA weight generation method, we incorporate the MA term while maintaining the time complexity and parameter size of the underlying efficient attention models. We further explore how indirect parameter generation can produce implicit MA weights that align with the modeling requirements for local temporal impacts. Experimental results show that WAVE attention that incorporates the ARMA structure consistently improves the performance of various AR attentions on TSF tasks, achieving state-of-the-art results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.15361</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.15361</id><created>2024-10-20</created><updated>2025-02-06</updated><authors><author><keyname>Zhou</keyname><forenames>Han</forenames></author><author><keyname>Van Landeghem</keyname><forenames>Jordy</forenames></author><author><keyname>Popordanoska</keyname><forenames>Teodora</forenames></author><author><keyname>Blaschko</keyname><forenames>Matthew B.</forenames></author></authors><title>A Novel Characterization of the Population Area Under the Risk Coverage   Curve (AURC) and Rates of Finite Sample Estimators</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The selective classifier (SC) has been proposed for rank based uncertainty thresholding, which could have applications in safety critical areas such as medical diagnostics, autonomous driving, and the justice system. The Area Under the Risk-Coverage Curve (AURC) has emerged as the foremost evaluation metric for assessing the performance of SC systems. In this work, we present a formal statistical formulation of population AURC, presenting an equivalent expression that can be interpreted as a reweighted risk function. Through Monte Carlo methods, we derive empirical AURC plug-in estimators for finite sample scenarios. The weight estimators associated with these plug-in estimators are shown to be consistent, with low bias and tightly bounded mean squared error (MSE). The plug-in estimators are proven to converge at a rate of $\mathcal{O}(\sqrt{\ln(n)/n})$ demonstrating statistical consistency. We empirically validate the effectiveness of our estimators through experiments across multiple datasets, model architectures, and confidence score functions (CSFs), demonstrating consistency and effectiveness in fine-tuning AURC performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.18959</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.18959</id><created>2024-10-24</created><updated>2025-02-06</updated><authors><author><keyname>Williams</keyname><forenames>Andrew Robert</forenames></author><author><keyname>Ashok</keyname><forenames>Arjun</forenames></author><author><keyname>Marcotte</keyname><forenames>Étienne</forenames></author><author><keyname>Zantedeschi</keyname><forenames>Valentina</forenames></author><author><keyname>Subramanian</keyname><forenames>Jithendaraa</forenames></author><author><keyname>Riachi</keyname><forenames>Roland</forenames></author><author><keyname>Requeima</keyname><forenames>James</forenames></author><author><keyname>Lacoste</keyname><forenames>Alexandre</forenames></author><author><keyname>Rish</keyname><forenames>Irina</forenames></author><author><keyname>Chapados</keyname><forenames>Nicolas</forenames></author><author><keyname>Drouin</keyname><forenames>Alexandre</forenames></author></authors><title>Context is Key: A Benchmark for Forecasting with Essential Textual   Information</title><categories>cs.LG cs.AI stat.ML</categories><comments>Preprint; under review. First two authors contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forecasting is a critical task in decision-making across numerous domains. While historical numerical data provide a start, they fail to convey the complete context for reliable and accurate predictions. Human forecasters frequently rely on additional information, such as background knowledge and constraints, which can efficiently be communicated through natural language. However, in spite of recent progress with LLM-based forecasters, their ability to effectively integrate this textual information remains an open question. To address this, we introduce "Context is Key" (CiK), a time-series forecasting benchmark that pairs numerical data with diverse types of carefully crafted textual context, requiring models to integrate both modalities; crucially, every task in CiK requires understanding textual context to be solved successfully. We evaluate a range of approaches, including statistical models, time series foundation models, and LLM-based forecasters, and propose a simple yet effective LLM prompting method that outperforms all other tested methods on our benchmark. Our experiments highlight the importance of incorporating contextual information, demonstrate surprising performance when using LLM-based forecasting models, and also reveal some of their critical shortcomings. This benchmark aims to advance multimodal forecasting by promoting models that are both accurate and accessible to decision-makers with varied technical expertise. The benchmark can be visualized at https://servicenow.github.io/context-is-key-forecasting/v0/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.19217</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.19217</id><created>2024-10-24</created><authors><author><keyname>Wu</keyname><forenames>Changlong</forenames></author><author><keyname>Grama</keyname><forenames>Ananth</forenames></author><author><keyname>Szpankowski</keyname><forenames>Wojciech</forenames></author></authors><title>No Free Lunch: Fundamental Limits of Learning Non-Hallucinating   Generative Models</title><categories>cs.LG cs.AI stat.ML</categories><journal-ref>International Conference on Learning Representations (ICLR 2025).   URL: https://openreview.net/pdf?id=OwNoTs2r8e</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative models have shown impressive capabilities in synthesizing high-quality outputs across various domains. However, a persistent challenge is the occurrence of "hallucinations", where the model produces outputs that are plausible but invalid. While empirical strategies have been explored to mitigate this issue, a rigorous theoretical understanding remains elusive. In this paper, we develop a theoretical framework to analyze the learnability of non-hallucinating generative models from a learning-theoretic perspective. Our results reveal that non-hallucinating learning is statistically impossible when relying solely on the training dataset, even for a hypothesis class of size two and when the entire training set is truthful. To overcome these limitations, we show that incorporating inductive biases aligned with the actual facts into the learning process is essential. We provide a systematic approach to achieve this by restricting the facts set to a concept class of finite VC-dimension and demonstrate its effectiveness under various learning paradigms. Although our findings are primarily conceptual, they represent a first step towards a principled approach to addressing hallucinations in learning generative models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.19725</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.19725</id><created>2024-10-25</created><updated>2025-02-06</updated><authors><author><keyname>Subedi</keyname><forenames>Unique</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>On the Benefits of Active Data Collection in Operator Learning</title><categories>stat.ML cs.LG</categories><comments>Moved Proofs to the Appendix</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study active data collection strategies for operator learning when the target operator is linear and the input functions are drawn from a mean-zero stochastic process with continuous covariance kernels. With an active data collection strategy, we establish an error convergence rate in terms of the decay rate of the eigenvalues of the covariance kernel. We can achieve arbitrarily fast error convergence rates with sufficiently rapid eigenvalue decay of the covariance kernels. This contrasts with the passive (i.i.d.) data collection strategies, where the convergence rate is never faster than linear decay ($\sim n^{-1}$). In fact, for our setting, we show a \emph{non-vanishing} lower bound for any passive data collection strategy, regardless of the eigenvalues decay rate of the covariance kernel. Overall, our results show the benefit of active data collection strategies in operator learning over their passive counterparts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.20640</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.20640</id><created>2024-10-27</created><updated>2025-02-07</updated><authors><author><keyname>Rivera</keyname><forenames>Eduardo Ochoa</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Near Optimal Pure Exploration in Logistic Bandits</title><categories>stat.ML cs.LG</categories><comments>25 pages, 2 figures. arXiv admin note: text overlap with   arXiv:2006.16073 by other authors</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bandit algorithms have garnered significant attention due to their practical applications in real-world scenarios. However, beyond simple settings such as multi-arm or linear bandits, optimal algorithms remain scarce. Notably, no optimal solution exists for pure exploration problems in the context of generalized linear model (GLM) bandits. In this paper, we narrow this gap and develop the first track-and-stop algorithm for general pure exploration problems under the logistic bandit called logistic track-and-stop (Log-TS). Log-TS is an efficient algorithm that asymptotically matches an approximation for the instance-specific lower bound of the expected sample complexity up to a logarithmic factor. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.03727</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.03727</id><created>2024-12-04</created><updated>2025-02-07</updated><authors><author><keyname>Zhang</keyname><forenames>Zhiheng</forenames></author><author><keyname>Wang</keyname><forenames>Zichen</forenames></author></authors><title>Online Experimental Design With Estimation-Regret Trade-off Under   Network Interference</title><categories>cs.LG math.OC math.ST stat.TH</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network interference has attracted significant attention in the field of causal inference, encapsulating various sociological behaviors where the treatment assigned to one individual within a network may affect the outcomes of others, such as their neighbors. A key challenge in this setting is that standard causal inference methods often assume independent treatment effects among individuals, which may not hold in networked environments. To estimate interference-aware causal effects, a traditional approach is to inherit the independent settings, where practitioners randomly assign experimental participants into different groups and compare their outcomes. While effective in offline settings, this strategy becomes problematic in sequential experiments, where suboptimal decision persists, leading to substantial regret. To address this issue, we introduce a unified interference-aware framework for online experimental design. Compared to existing studies, we extend the definition of arm space by utilizing the statistical concept of exposure mapping, which allows for a more flexible and context-aware representation of treatment effects in networked settings. Crucially, we establish a Pareto-optimal trade-off between estimation accuracy and regret under the network concerning both time period and arm space, which remains superior to baseline models even without network interference. Furthermore, we propose an algorithmic implementation and discuss its generalization across different learning settings and network topology </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.04767</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.04767</id><created>2024-12-05</created><updated>2025-02-06</updated><authors><author><keyname>Tian</keyname><forenames>Bowei</forenames></author><author><keyname>Wang</keyname><forenames>Ziyao</forenames></author><author><keyname>He</keyname><forenames>Shwai</forenames></author><author><keyname>Ye</keyname><forenames>Wanghao</forenames></author><author><keyname>Sun</keyname><forenames>Guoheng</forenames></author><author><keyname>Dai</keyname><forenames>Yucong</forenames></author><author><keyname>Wu</keyname><forenames>Yongkai</forenames></author><author><keyname>Li</keyname><forenames>Ang</forenames></author></authors><title>Towards counterfactual fairness through auxiliary variables</title><categories>cs.LG cs.DS stat.ML</categories><comments>arXiv admin note: text overlap with arXiv:2307.08232 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The challenge of balancing fairness and predictive accuracy in machine learning models, especially when sensitive attributes such as race, gender, or age are considered, has motivated substantial research in recent years. Counterfactual fairness ensures that predictions remain consistent across counterfactual variations of sensitive attributes, which is a crucial concept in addressing societal biases. However, existing counterfactual fairness approaches usually overlook intrinsic information about sensitive features, limiting their ability to achieve fairness while simultaneously maintaining performance. To tackle this challenge, we introduce EXOgenous Causal reasoning (EXOC), a novel causal reasoning framework motivated by exogenous variables. It leverages auxiliary variables to uncover intrinsic properties that give rise to sensitive attributes. Our framework explicitly defines an auxiliary node and a control node that contribute to counterfactual fairness and control the information flow within the model. Our evaluation, conducted on synthetic and real-world datasets, validates EXOC's superiority, showing that it outperforms state-of-the-art approaches in achieving counterfactual fairness. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.08435</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.08435</id><created>2024-12-11</created><updated>2025-02-07</updated><authors><author><keyname>Zhao</keyname><forenames>Lifan</forenames></author><author><keyname>Shen</keyname><forenames>Yanyan</forenames></author></authors><title>Proactive Model Adaptation Against Concept Drift for Online Time Series   Forecasting</title><categories>cs.LG cs.AI cs.CE stat.ML</categories><comments>Accepted by KDD 2025</comments><doi>10.1145/3690624.3709210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time series forecasting always faces the challenge of concept drift, where data distributions evolve over time, leading to a decline in forecast model performance. Existing solutions are based on online learning, which continually organize recent time series observations as new training samples and update model parameters according to the forecasting feedback on recent data. However, they overlook a critical issue: obtaining ground-truth future values of each sample should be delayed until after the forecast horizon. This delay creates a temporal gap between the training samples and the test sample. Our empirical analysis reveals that the gap can introduce concept drift, causing forecast models to adapt to outdated concepts. In this paper, we present Proceed, a novel proactive model adaptation framework for online time series forecasting. Proceed first estimates the concept drift between the recently used training samples and the current test sample. It then employs an adaptation generator to efficiently translate the estimated drift into parameter adjustments, proactively adapting the model to the test sample. To enhance the generalization capability of the framework, Proceed is trained on synthetic diverse concept drifts. Extensive experiments on five real-world datasets across various forecast models demonstrate that Proceed brings more performance improvements than the state-of-the-art online learning methods, significantly facilitating forecast models' resilience against concept drifts. Code is available at https://github.com/SJTU-DMTai/OnlineTSF. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.20471</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.20471</id><created>2024-12-29</created><updated>2025-02-07</updated><authors><author><keyname>Cai</keyname><forenames>Yang</forenames></author><author><keyname>Mitra</keyname><forenames>Siddharth</forenames></author><author><keyname>Wang</keyname><forenames>Xiuyuan</forenames></author><author><keyname>Wibisono</keyname><forenames>Andre</forenames></author></authors><title>On the Convergence of Min-Max Langevin Dynamics and Algorithm</title><categories>cs.GT cs.LG math.OC stat.ML</categories><comments>v2: Revised introduction and presentation of results</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study zero-sum games in the space of probability distributions over the Euclidean space $\mathbb{R}^d$ with entropy regularization, in the setting when the interaction function between the players is smooth and strongly convex-strongly concave. We prove an exponential convergence guarantee for the mean-field min-max Langevin dynamics to compute the equilibrium distribution of the zero-sum game. We also study the finite-particle approximation of the mean-field min-max Langevin dynamics, both in continuous and discrete times. We prove biased convergence guarantees for the continuous-time finite-particle min-max Langevin dynamics to the stationary mean-field equilibrium distribution with an explicit bias term which does not scale with the number of particles. We also prove biased convergence guarantees for the discrete-time finite-particle min-max Langevin algorithm to the stationary mean-field equilibrium distribution with an additional bias term which scales with the step size and the number of particles. This provides an explicit iteration complexity for the average particle along the finite-particle algorithm to approximately compute the equilibrium distribution of the zero-sum game. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.01437</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.01437</id><created>2024-12-23</created><updated>2025-02-06</updated><authors><author><keyname>Murphy</keyname><forenames>Charles</forenames></author><author><keyname>Lizotte</keyname><forenames>Simon</forenames></author><author><keyname>Thibault</keyname><forenames>François</forenames></author><author><keyname>Thibeault</keyname><forenames>Vincent</forenames></author><author><keyname>Desrosiers</keyname><forenames>Patrick</forenames></author><author><keyname>Allard</keyname><forenames>Antoine</forenames></author></authors><title>On the reconstruction limits of complex networks</title><categories>stat.AP cs.IT math.IT physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network reconstruction consists in retrieving the hidden interaction structure of a system from observations. Many reconstruction algorithms have been proposed, although less research has been devoted to describe their theoretical limitations. In this work, we adopt an information-theoretic perspective and define the reconstructability: The fraction of structural information recoverable from data. The reconstructability depends on the true data generating (TDG) model which is shown to set the reconstruction limit: any algorithm can perform, on average, at best like the TDG model. We show that the reconstructability is related to various performance measures, such as the probability of error and the Jaccard similarity. In an empirical context where the TDG model is unknown, we introduce the reconstruction index as an approximation of the reconstructability. We find that performing model selection is crucial for the validity of the reconstruction index as a proxy of the reconstructability of empirical time series and networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.07025</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.07025</id><created>2025-01-12</created><updated>2025-02-07</updated><authors><author><keyname>Zhang</keyname><forenames>Yong</forenames></author><author><keyname>Gyamfi</keyname><forenames>Eric Herrison</forenames></author></authors><title>A Weighted Similarity Metric for Community Detection in Sparse Data</title><categories>stat.ME cs.SI</categories><comments>This paper has been accepted for Workshop of AI for Social Impact at   AAAI 2025</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Many Natural Language Processing (NLP) related applications involves topics and sentiments derived from short documents such as consumer reviews and social media posts. Topics and sentiments of short documents are highly sparse because a short document generally covers a few topics among hundreds of candidates. Imputation of missing data is sometimes hard to justify and also often unpractical in highly sparse data. We developed a method for calculating a weighted similarity for highly sparse data without imputation. This weighted similarity is consist of three components to capture similarities based on both existence and lack of common properties and pattern of missing values. As a case study, we used a community detection algorithm and this weighted similarity to group different shampoo brands based on sparse topic sentiments derived from short consumer reviews. Compared with traditional imputation and similarity measures, the weighted similarity shows better performance in both general community structures and average community qualities. The performance is consistent and robust across metrics and community complexities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10117</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10117</id><created>2025-01-17</created><updated>2025-02-07</updated><authors><author><keyname>Liu</keyname><forenames>Weiguang</forenames></author><author><keyname>de Paula</keyname><forenames>Áureo</forenames></author><author><keyname>Tamer</keyname><forenames>Elie</forenames></author></authors><title>Prediction Sets and Conformal Inference with Censored Outcomes</title><categories>econ.EM stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given data on a scalar random variable $Y$, a prediction set for $Y$ with miscoverage level $\alpha$ is a set of values for $Y$ that contains a randomly drawn $Y$ with probability $1 - \alpha$, where $\alpha \in (0,1)$. Among all prediction sets that satisfy this coverage property, the oracle prediction set is the one with the smallest volume. This paper provides estimation methods of such prediction sets given observed conditioning covariates when $Y$ is \textit{censored} or \textit{measured in intervals}. We first characterise the oracle prediction set under interval censoring and develop a consistent estimator for the shortest prediction {\it interval} that satisfies this coverage property.These consistency results are extended to accommodate cases where the prediction set consists of multiple disjoint intervals. We use conformal inference to construct a prediction set that achieves finite-sample validity under censoring and maintains consistency as sample size increases, using a conformity score function designed for interval data. The procedure accommodates the prediction uncertainty that is irreducible (due to the stochastic nature of outcomes), the modelling uncertainty due to partial identification and also sampling uncertainty that gets reduced as samples get larger. We conduct a set of Monte Carlo simulations and an application to data from the Current Population Survey. The results highlight the robustness and efficiency of the proposed methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10974</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10974</id><created>2025-01-19</created><updated>2025-02-06</updated><authors><author><keyname>Huang</keyname><forenames>Yu-Han</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Sequential Change Detection for Learning in Piecewise Stationary Bandit   Environments</title><categories>cs.IT cs.SY eess.SY math.IT stat.OT</categories><comments>15 pages, 2 figures. arXiv admin note: text overlap with   arXiv:2501.01291</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A finite-horizon variant of the quickest change detection problem is investigated, which is motivated by a change detection problem that arises in piecewise stationary bandits. The goal is to minimize the \emph{latency}, which is smallest threshold such that the probability that the detection delay exceeds the threshold is below a desired low level, while controlling the false alarm probability to a desired low level. When the pre- and post-change distributions are unknown, two tests are proposed as candidate solutions. These tests are shown to attain order optimality in terms of the horizon. Furthermore, the growth in their latencies with respect to the false alarm probability and late detection probability satisfies a property that is desirable in regret analysis for piecewise stationary bandits. Numerical results are provided to validate the theoretical performance results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14974</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14974</id><created>2025-01-24</created><updated>2025-02-06</updated><authors><author><keyname>Deng</keyname><forenames>Fengnan</forenames></author><author><keyname>Vidyashankar</keyname><forenames>Anand N.</forenames></author></authors><title>Private Minimum Hellinger Distance Estimation via Hellinger Distance   Differential Privacy</title><categories>math.ST cs.CR math.PR stat.ME stat.ML stat.TH</categories><msc-class>62F35, 68P27, 62E20, 60E05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective functions based on Hellinger distance yield robust and efficient estimators of model parameters. Motivated by privacy and regulatory requirements encountered in contemporary applications, we derive in this paper \emph{private minimum Hellinger distance estimators}. The estimators satisfy a new privacy constraint, namely, Hellinger differential privacy, while retaining the robustness and efficiency properties. We demonstrate that Hellinger differential privacy shares several features of standard differential privacy while allowing for sharper inference. Additionally, for computational purposes, we also develop Hellinger differentially private gradient descent and Newton-Raphson algorithms. We illustrate the behavior of our estimators in finite samples using numerical experiments and verify that they retain robustness properties under gross-error contamination. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18502</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18502</id><created>2025-01-30</created><updated>2025-02-07</updated><authors><author><keyname>Kumar</keyname><forenames>Ritesh</forenames></author><author><keyname>Vatedka</keyname><forenames>Shashank</forenames></author></authors><title>One-Bit Distributed Mean Estimation with Unknown Variance</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>21 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this work, we study the problem of distributed mean estimation with $1$-bit communication constraints when the variance is unknown. We focus on the specific case where each user has access to one i.i.d. sample drawn from a distribution that belongs to a scale-location family, and is limited to sending just a single bit of information to a central server whose goal is to estimate the mean. We propose non-adaptive and adaptive estimators that are shown to be asymptotically normal. We derive bounds on the asymptotic (in the number of users) Mean Squared Error (MSE) achieved by these estimators. For a class of symmetric log-concave distributions, we derive matching lower bounds for the MSE achieved by adaptive estimators, proving the optimality of our scheme. We show that non-adaptive estimators can be strictly suboptimal by deriving a lower bound on the MSE achieved by any non-adaptive estimator for Gaussian distributions and demonstrating a positive gap between this and the MSE achieved by our adaptive scheme. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00168</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00168</id><created>2025-01-31</created><updated>2025-02-07</updated><authors><author><keyname>Herrera-Esposito</keyname><forenames>Daniel</forenames></author><author><keyname>Burge</keyname><forenames>Johannes</forenames></author></authors><title>Supervised Quadratic Feature Analysis: An Information Geometry Approach   to Dimensionality Reduction</title><categories>stat.ML cs.LG math.DG math.ST stat.TH</categories><comments>18 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Supervised dimensionality reduction aims to map labeled data to a low-dimensional feature space while maximizing class discriminability. Despite the availability of methods for learning complex non-linear features (e.g. Deep Learning), there is an enduring demand for dimensionality reduction methods that learn linear features due to their interpretability, low computational cost, and broad applicability. However, there is a gap between methods that optimize linear separability (e.g. LDA), and more flexible but computationally expensive methods that optimize over arbitrary class boundaries (e.g. metric-learning methods). Here, we present Supervised Quadratic Feature Analysis (SQFA), a dimensionality reduction method for learning linear features that maximize the differences between class-conditional first- and second-order statistics, which allow for quadratic discrimination. SQFA exploits the information geometry of second-order statistics in the symmetric positive definite manifold. We show that SQFA features support quadratic discriminability in real-world problems. We also provide a theoretical link, based on information geometry, between SQFA and the Quadratic Discriminant Analysis (QDA) classifier. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00182</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00182</id><created>2025-01-31</created><updated>2025-02-07</updated><authors><author><keyname>Seo</keyname><forenames>Jungwon</forenames></author><author><keyname>Catak</keyname><forenames>Ferhat Ozgur</forenames></author><author><keyname>Rong</keyname><forenames>Chunming</forenames></author></authors><title>Understanding Federated Learning from IID to Non-IID dataset: An   Experimental Study</title><categories>cs.LG cs.AI stat.ML</categories><journal-ref>36th Norwegian ICT Conference for Research and Education, NIKT   2024</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As privacy concerns and data regulations grow, federated learning (FL) has emerged as a promising approach for training machine learning models across decentralized data sources without sharing raw data. However, a significant challenge in FL is that client data are often non-IID (non-independent and identically distributed), leading to reduced performance compared to centralized learning. While many methods have been proposed to address this issue, their underlying mechanisms are often viewed from different perspectives. Through a comprehensive investigation from gradient descent to FL, and from IID to non-IID data settings, we find that inconsistencies in client loss landscapes primarily cause performance degradation in non-IID scenarios. From this understanding, we observe that existing methods can be grouped into two main strategies: (i) adjusting parameter update paths and (ii) modifying client loss landscapes. These findings offer a clear perspective on addressing non-IID challenges in FL and help guide future research in the field. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00838</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00838</id><created>2025-02-02</created><authors><author><keyname>Bussemaker</keyname><forenames>Jasper H.</forenames></author><author><keyname>Saves</keyname><forenames>Paul</forenames></author><author><keyname>Bartoli</keyname><forenames>Nathalie</forenames></author><author><keyname>Lefebvre</keyname><forenames>Thierry</forenames></author><author><keyname>Lafage</keyname><forenames>Rémi</forenames></author></authors><title>System Architecture Optimization Strategies: Dealing with Expensive   Hierarchical Problems</title><categories>math.OC cs.DM stat.AP</categories><comments>J Glob Optim (2024)</comments><doi>10.1007/s10898-024-01443-8</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Choosing the right system architecture for the problem at hand is challenging due to the large design space and high uncertainty in the early stage of the design process. Formulating the architecting process as an optimization problem may mitigate some of these challenges. This work investigates strategies for solving System Architecture Optimization (SAO) problems: expensive, black-box, hierarchical, mixed-discrete, constrained, multi-objective problems that may be subject to hidden constraints. Imputation ratio, correction ratio, correction fraction, and max rate diversity metrics are defined for characterizing hierar chical design spaces. This work considers two classes of optimization algorithms for SAO: Multi-Objective Evolutionary Algorithms (MOEA) such as NSGA-II, and Bayesian Optimization (BO) algorithms. A new Gaussian process kernel is presented that enables modeling hierarchical categorical variables, extending previous work on modeling continuous and integer hierarchical variables. Next, a hierarchical sampling algorithm that uses design space hierarchy to group design vectors by active design variables is developed. Then, it is demonstrated that integrating more hierarchy information in the optimization algorithms yields better optimization results for BO algorithms. Several realistic single-objective and multi-objective test problems are used for investigations. Finally, the BO algorithm is applied to a jet engine architecture optimization problem. This work shows that the developed BO algorithm can effectively solve the problem with one order of magnitude less function evaluations than NSGA-II. The algorithms and problems used in this work are implemented in the open-source Python library SBArchOpt. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00854</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00854</id><created>2025-02-02</created><authors><author><keyname>Priem</keyname><forenames>Rémy</forenames></author><author><keyname>Diouane</keyname><forenames>Youssef</forenames></author><author><keyname>Bartoli</keyname><forenames>Nathalie</forenames></author><author><keyname>Dubreuil</keyname><forenames>Sylvain</forenames></author><author><keyname>Saves</keyname><forenames>Paul</forenames></author></authors><title>High-Dimensional Bayesian Optimization Using Both Random and Supervised   Embeddings</title><categories>math.OC cs.LG stat.ML</categories><journal-ref>AIAA Journal 2025 63:1, 162-173</journal-ref><doi>10.2514/1.J063488</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Bayesian optimization (BO) is one of the most powerful strategies to solve computationally expensive-to-evaluate blackbox optimization problems. However, BO methods are conventionally used for optimization problems of small dimension because of the curse of dimensionality. In this paper, a high-dimensionnal optimization method incorporating linear embedding subspaces of small dimension is proposed to efficiently perform the optimization. An adaptive learning strategy for these linear embeddings is carried out in conjunction with the optimization. The resulting BO method, named efficient global optimization coupled with random and supervised embedding (EGORSE), combines in an adaptive way both random and supervised linear embeddings. EGORSE has been compared to state-of-the-art algorithms and tested on academic examples with a number of design variables ranging from 10 to 600. The obtained results show the high potential of EGORSE to solve high-dimensional blackbox optimization problems, in terms of both CPU time and the limited number of calls to the expensive blackbox simulation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02496</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02496</id><created>2025-02-04</created><updated>2025-02-07</updated><authors><author><keyname>Kolb</keyname><forenames>Chris</forenames></author><author><keyname>Weber</keyname><forenames>Tobias</forenames></author><author><keyname>Bischl</keyname><forenames>Bernd</forenames></author><author><keyname>Rügamer</keyname><forenames>David</forenames></author></authors><title>Deep Weight Factorization: Sparse Learning Through the Lens of   Artificial Symmetries</title><categories>cs.LG stat.ML</categories><comments>accepted at ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sparse regularization techniques are well-established in machine learning, yet their application in neural networks remains challenging due to the non-differentiability of penalties like the $L_1$ norm, which is incompatible with stochastic gradient descent. A promising alternative is shallow weight factorization, where weights are decomposed into two factors, allowing for smooth optimization of $L_1$-penalized neural networks by adding differentiable $L_2$ regularization to the factors. In this work, we introduce deep weight factorization, extending previous shallow approaches to more than two factors. We theoretically establish equivalence of our deep factorization with non-convex sparse regularization and analyze its impact on training dynamics and optimization. Due to the limitations posed by standard training practices, we propose a tailored initialization scheme and identify important learning rate requirements necessary for training factorized networks. We demonstrate the effectiveness of our deep weight factorization through experiments on various architectures and datasets, consistently outperforming its shallow counterpart and widely used pruning methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02861</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02861</id><created>2025-02-04</created><updated>2025-02-06</updated><authors><author><keyname>Shen</keyname><forenames>Judy Hanwen</forenames></author><author><keyname>Vitercik</keyname><forenames>Ellen</forenames></author><author><keyname>Wikum</keyname><forenames>Anders</forenames></author></authors><title>Algorithms with Calibrated Machine Learning Predictions</title><categories>stat.ML cs.DS cs.LG</categories><comments>Replacement: updated acknowledgments</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The field of algorithms with predictions incorporates machine learning advice in the design of online algorithms to improve real-world performance. While this theoretical framework often assumes uniform reliability across all predictions, modern machine learning models can now provide instance-level uncertainty estimates. In this paper, we propose calibration as a principled and practical tool to bridge this gap, demonstrating the benefits of calibrated advice through two case studies: the ski rental and online job scheduling problems. For ski rental, we design an algorithm that achieves optimal prediction-dependent performance and prove that, in high-variance settings, calibrated advice offers more effective guidance than alternative methods for uncertainty quantification. For job scheduling, we demonstrate that using a calibrated predictor leads to significant performance improvements over existing methods. Evaluations on real-world data validate our theoretical findings, highlighting the practical impact of calibration for algorithms with predictions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03479</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03479</id><created>2025-01-27</created><updated>2025-02-06</updated><authors><author><keyname>Cuicizion</keyname><forenames>Eliuvish</forenames></author><author><keyname>Ri</keyname><forenames>Itsugo</forenames></author><author><keyname>Holmes</keyname><forenames>Elaine</forenames></author><author><keyname>Lyeutsaon</keyname><forenames>Sho</forenames></author><author><keyname>Chern</keyname><forenames>Jawad Hassan</forenames></author></authors><title>A Tutorial on Markov Renewal and Semi-Markov Proportional Hazards Model</title><categories>stat.AP stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Transition probability estimation plays a critical role in multi-state modeling, especially in clinical research. This paper investigates the application of semi-Markov and Markov renewal frameworks to the EBMT dataset, focusing on six clinical states encountered during hematopoietic stem cell transplantation. By comparing Aalen-Johansen (AJ) and Dabrowska-Sun-Horowitz (DSH) estimators, we demonstrate that semi-Markov models, which incorporate sojourn times, provide a more nuanced and temporally sensitive depiction of patient trajectories compared to memoryless Markov models. The DSH estimator consistently yields smoother probability curves, particularly for transitions involving prolonged states. We use empirical process theory and Burkholder-Davis-Gundy inequality to show weak convergence of the estimator. Future work includes extending the framework to accommodate advanced covariate structures and non-Markovian dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03587</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03587</id><created>2025-02-05</created><updated>2025-02-06</updated><authors><author><keyname>von Seeger</keyname><forenames>Anneke</forenames></author><author><keyname>Zou</keyname><forenames>Dongmian</forenames></author><author><keyname>Lerman</keyname><forenames>Gilad</forenames></author></authors><title>Stein Discrepancy for Unsupervised Domain Adaptation</title><categories>cs.LG stat.ML</categories><comments>24 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Unsupervised domain adaptation (UDA) leverages information from a labeled source dataset to improve accuracy on a related but unlabeled target dataset. A common approach to UDA is aligning representations from the source and target domains by minimizing the distance between their data distributions. Previous methods have employed distances such as Wasserstein distance and maximum mean discrepancy. However, these approaches are less effective when the target data is significantly scarcer than the source data. Stein discrepancy is an asymmetric distance between distributions that relies on one distribution only through its score function. In this paper, we propose a novel UDA method that uses Stein discrepancy to measure the distance between source and target domains. We develop a learning framework using both non-kernelized and kernelized Stein discrepancy. Theoretically, we derive an upper bound for the generalization error. Numerical experiments show that our method outperforms existing methods using other domain discrepancy measures when only small amounts of target data are available. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03942</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03942</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Holst</keyname><forenames>Klaus Kähler</forenames></author><author><keyname>Nordland</keyname><forenames>Andreas</forenames></author><author><keyname>Furkjær</keyname><forenames>Julie</forenames></author><author><keyname>Damgaard</keyname><forenames>Lars Holm</forenames></author><author><keyname>Pipper</keyname><forenames>Christian Bressen</forenames></author></authors><title>A retake on the analysis of scores truncated by terminal events</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of data from randomized controlled trials in vulnerable populations requires special attention when assessing treatment effect by a score measuring, e.g., disease stage or activity together with onset of prevalent terminal events. In reality, it is impossible to disentangle a disease score from the terminal event, since the score is not clinically meaningful after this event. In this work, we propose to assess treatment interventions simultaneously on disease score and the terminal event. Our proposal is based on a natural data-generating mechanism respecting that a disease score does not exist beyond the terminal event. We use modern semi-parametric statistical methods to provide robust and efficient estimation of the risk of terminal event and expected disease score conditional on no terminal event at a pre-specified landmark time. We also use the simultaneous asymptotic behavior of our estimators to develop a powerful closed testing procedure for confirmatory assessment of treatment effect on both onset of terminal event and level of disease score. A simulation study mimicking a large-scale outcome trial in chronic kidney patients as well as an analysis of that trial is provided to assess performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04168</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04168</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Ferradini</keyname><forenames>Carla</forenames></author><author><keyname>Gitton</keyname><forenames>Victor</forenames></author><author><keyname>Vilasini</keyname><forenames>V.</forenames></author></authors><title>Cyclic quantum causal modelling with a graph separation theorem</title><categories>quant-ph math.ST stat.ML stat.TH</categories><comments>41+41 pages. A companion paper by the same authors, focussing on   cyclic classical (functional) causal models has been submitted to the arXiv   concurrently with primary class [math.ST], v2 only differs from v1 in   including the arXiv number of the companion paper. Comments are welcome</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Causal modelling frameworks link observable correlations to causal explanations, which is a crucial aspect of science. These models represent causal relationships through directed graphs, with vertices and edges denoting systems and transformations within a theory. Most studies focus on acyclic causal graphs, where well-defined probability rules and powerful graph-theoretic properties like the d-separation theorem apply. However, understanding complex feedback processes and exotic fundamental scenarios with causal loops requires cyclic causal models, where such results do not generally hold. While progress has been made in classical cyclic causal models, challenges remain in uniquely fixing probability distributions and identifying graph-separation properties applicable in general cyclic models. In cyclic quantum scenarios, existing frameworks have focussed on a subset of possible cyclic causal scenarios, with graph-separation properties yet unexplored. This work proposes a framework applicable to all consistent quantum and classical cyclic causal models on finite-dimensional systems. We address these challenges by introducing a robust probability rule and a novel graph-separation property, p-separation, which we prove to be sound and complete for all such models. Our approach maps cyclic causal models to acyclic ones with post-selection, leveraging the post-selected quantum teleportation protocol. We characterize these protocols and their success probabilities along the way. We also establish connections between this formalism and other classical and quantum frameworks to inform a more unified perspective on causality. This provides a foundation for more general cyclic causal discovery algorithms and to systematically extend open problems and techniques from acyclic informational networks (e.g., certification of non-classicality) to cyclic causal structures and networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04171</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04171</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Ferradini</keyname><forenames>Carla</forenames></author><author><keyname>Gitton</keyname><forenames>Victor</forenames></author><author><keyname>Vilasini</keyname><forenames>V.</forenames></author></authors><title>Cyclic functional causal models beyond unique solvability with a graph   separation theorem</title><categories>math.ST quant-ph stat.ML stat.TH</categories><comments>33+16 pages. A companion paper by the same authors, focussing on   cyclic quantum causal models has been submitted to the arXiv concurrently   with primary class [quant-ph], v2 only differs from v1 in including the arXiv   number of the companion paper. Comments are welcome</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Functional causal models (fCMs) specify functional dependencies between random variables associated to the vertices of a graph. In directed acyclic graphs (DAGs), fCMs are well-understood: a unique probability distribution on the random variables can be easily specified, and a crucial graph-separation result called the d-separation theorem allows one to characterize conditional independences between the variables. However, fCMs on cyclic graphs pose challenges due to the absence of a systematic way to assign a unique probability distribution to the fCM's variables, the failure of the d-separation theorem, and lack of a generalization of this theorem that is applicable to all consistent cyclic fCMs. In this work, we develop a causal modeling framework applicable to all cyclic fCMs involving finite-cardinality variables, except inconsistent ones admitting no solutions. Our probability rule assigns a unique distribution even to non-uniquely solvable cyclic fCMs and reduces to the known rule for uniquely solvable fCMs. We identify a class of fCMs, called averagely uniquely solvable, that we show to be the largest class where the probabilities admit a Markov factorization. Furthermore, we introduce a new graph-separation property, p-separation, and prove this to be sound and complete for all consistent finite-cardinality cyclic fCMs while recovering the d-separation theorem for DAGs. These results are obtained by considering classical post-selected teleportation protocols inspired by analogous protocols in quantum information theory. We discuss further avenues for exploration, linking in particular problems in cyclic fCMs and in quantum causality. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04208</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04208</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Grünwald</keyname><forenames>Peter D.</forenames></author><author><keyname>Koolen</keyname><forenames>Wouter M.</forenames></author></authors><title>Supermartingales for One-Sided Tests: Sufficient Monotone Likelihood   Ratios are Sufficient</title><categories>math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The t-statistic is a widely-used scale-invariant statistic for testing the null hypothesis that the mean is zero. Martingale methods enable sequential testing with the t-statistic at every sample size, while controlling the probability of falsely rejecting the null. For one-sided sequential tests, which reject when the t-statistic is too positive, a natural question is whether they also control false rejection when the true mean is negative. We prove that this is the case using monotone likelihood ratios and sufficient statistics. We develop applications to the scale-invariant t-test, the location-invariant $\chi^2$-test and sequential linear regression with nuisance covariates. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04339</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04339</id><created>2025-02-01</created><authors><author><keyname>George</keyname><forenames>Anand Jerry</forenames></author><author><keyname>Veiga</keyname><forenames>Rodrigo</forenames></author><author><keyname>Macris</keyname><forenames>Nicolas</forenames></author></authors><title>Analysis of Diffusion Models for Manifold Data</title><categories>math.ST cond-mat.dis-nn cs.IT cs.LG math.IT math.PR stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We analyze the time reversed dynamics of generative diffusion models. If the exact empirical score function is used in a regime of large dimension and exponentially large number of samples, these models are known to undergo transitions between distinct dynamical regimes. We extend this analysis and compute the transitions for an analytically tractable manifold model where the statistical model for the data is a mixture of lower dimensional Gaussians embedded in higher dimensional space. We compute the so-called speciation and collapse transition times, as a function of the ratio of manifold-to-ambient space dimensions, and other characteristics of the data model. An important tool used in our analysis is the exact formula for the mutual information (or free energy) of Generalized Linear Models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04372</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04372</id><created>2025-02-05</created><authors><author><keyname>Genari</keyname><forenames>Juliano</forenames></author><author><keyname>Goedert</keyname><forenames>Guilherme Tegoni</forenames></author></authors><title>Mining Unstructured Medical Texts With Conformal Active Learning</title><categories>cs.CL cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The extraction of relevant data from Electronic Health Records (EHRs) is crucial to identifying symptoms and automating epidemiological surveillance processes. By harnessing the vast amount of unstructured text in EHRs, we can detect patterns that indicate the onset of disease outbreaks, enabling faster, more targeted public health responses. Our proposed framework provides a flexible and efficient solution for mining data from unstructured texts, significantly reducing the need for extensive manual labeling by specialists. Experiments show that our framework achieving strong performance with as few as 200 manually labeled texts, even for complex classification problems. Additionally, our approach can function with simple lightweight models, achieving competitive and occasionally even better results compared to more resource-intensive deep learning models. This capability not only accelerates processing times but also preserves patient privacy, as the data can be processed on weaker on-site hardware rather than being transferred to external systems. Our methodology, therefore, offers a practical, scalable, and privacy-conscious approach to real-time epidemiological monitoring, equipping health institutions to respond rapidly and effectively to emerging health threats. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04422</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04422</id><created>2025-02-06</created><authors><author><keyname>Yadav</keyname><forenames>Pooja</forenames></author><author><keyname>Srivastava</keyname><forenames>Tanuja</forenames></author></authors><title>The Maximum Likelihood Degree of Farlie Gumbel Morgenstern Bivariate   Exponential Distribution</title><categories>math.ST math.AC stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum likelihood degree of a statistical model refers to the number of solutions, where the derivative of the log-likelihood function is zero, over the complex field. This paper examines the maximum likelihood degree of the parameter in Farlie-Gumbel-Morgenstern bivariate exponential distribution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04464</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04464</id><created>2025-02-06</created><authors><author><keyname>Jadoul</keyname><forenames>Yannick</forenames></author><author><keyname>Tufarelli</keyname><forenames>Tommaso</forenames></author><author><keyname>Coissac</keyname><forenames>Chloé</forenames></author><author><keyname>Gamba</keyname><forenames>Marco</forenames></author><author><keyname>Ravignani</keyname><forenames>Andrea</forenames></author></authors><title>Hidden assumptions of integer ratio analyses in bioacoustics and music</title><categories>stat.AP</categories><comments>19 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Rhythm is ubiquitous in human culture and in nature, but hard to capture in all its complexity. A key dimension of rhythm, integer ratio categories occur when the relationship between temporal intervals can be expressed as small-integer ratios. Recent work has found integer ratio categories in most human musical cultures and some animal species' vocalizations or behavioral displays. But biological systems are noisy, and empirically measured intervals rarely form an exact small-integer ratio. Here, we mathematically assess whether the leading integer ratio analysis method makes valid statistical and biological assumptions. In particular, we (1) make the temporal properties of empirical ratios explicit, both in general and for the typical use in the literature; (2) show how the choice of ratio formula affects the probability distribution of rhythm ratios and ensuing statistical results; (3) guide the reader to carefully consider the assumptions and null hypotheses of the statistical analysis; (4) present a comprehensive methodology to statistically test integer ratios for any null hypothesis of choice. Our observations have implications for both past and future research in music cognition and animal behavior: They suggest how to interpret past findings and provide tools to choose the correct null hypotheses in future empirical work. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04491</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04491</id><created>2025-02-06</created><authors><author><keyname>Cheng</keyname><forenames>Ziheng</forenames></author><author><keyname>Xie</keyname><forenames>Tianyu</forenames></author><author><keyname>Zhang</keyname><forenames>Shiyue</forenames></author><author><keyname>Zhang</keyname><forenames>Cheng</forenames></author></authors><title>Provable Sample-Efficient Transfer Learning Conditional Diffusion Models   via Representation Learning</title><categories>cs.LG math.ST stat.ML stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While conditional diffusion models have achieved remarkable success in various applications, they require abundant data to train from scratch, which is often infeasible in practice. To address this issue, transfer learning has emerged as an essential paradigm in small data regimes. Despite its empirical success, the theoretical underpinnings of transfer learning conditional diffusion models remain unexplored. In this paper, we take the first step towards understanding the sample efficiency of transfer learning conditional diffusion models through the lens of representation learning. Inspired by practical training procedures, we assume that there exists a low-dimensional representation of conditions shared across all tasks. Our analysis shows that with a well-learned representation from source tasks, the samplecomplexity of target tasks can be reduced substantially. In addition, we investigate the practical implications of our theoretical results in several real-world applications of conditional diffusion models. Numerical experiments are also conducted to verify our results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04543</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04543</id><created>2025-02-06</created><authors><author><keyname>Lu</keyname><forenames>Zhou</forenames></author><author><keyname>Sun</keyname><forenames>Y. Jennifer</forenames></author><author><keyname>Zhang</keyname><forenames>Zhiyu</forenames></author></authors><title>Sparsity-Based Interpolation of External, Internal and Swap Regret</title><categories>stat.ML cs.LG</categories><comments>Equal contribution, alphabetical order</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Focusing on the expert problem in online learning, this paper studies the interpolation of several performance metrics via $\phi$-regret minimization, which measures the performance of an algorithm by its regret with respect to an arbitrary action modification rule $\phi$. With $d$ experts and $T\gg d$ rounds in total, we present a single algorithm achieving the instance-adaptive $\phi$-regret bound \begin{equation*} \tilde O\left(\min\left\{\sqrt{d-d^{\mathrm{unif}}_\phi+1},\sqrt{d-d^{\mathrm{self}}_\phi}\right\}\cdot\sqrt{T}\right), \end{equation*} where $d^{\mathrm{unif}}_\phi$ is the maximum amount of experts modified identically by $\phi$, and $d^{\mathrm{self}}_\phi$ is the amount of experts that $\phi$ trivially modifies to themselves. By recovering the optimal $O(\sqrt{T\log d})$ external regret bound when $d^{\mathrm{unif}}_\phi=d$, the standard $\tilde O(\sqrt{T})$ internal regret bound when $d^{\mathrm{self}}_\phi=d-1$ and the optimal $\tilde O(\sqrt{dT})$ swap regret bound in the worst case, we improve existing results in the intermediate regimes. In addition, the same algorithm achieves the optimal quantile regret bound, which corresponds to even easier settings of $\phi$ than the external regret.   Building on the classical reduction from $\phi$-regret minimization to external regret minimization on stochastic matrices, our main idea is to further convert the latter to online linear regression using Haar-wavelet-inspired matrix features. Then, we apply a particular $L_1$-version of comparator-adaptive online learning algorithms to exploit the sparsity in this regression subroutine. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04550</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04550</id><created>2025-02-06</created><authors><author><keyname>Faes</keyname><forenames>Luca</forenames></author><author><keyname>Sparacino</keyname><forenames>Laura</forenames></author><author><keyname>Mijatovic</keyname><forenames>Gorana</forenames></author><author><keyname>Antonacci</keyname><forenames>Yuri</forenames></author><author><keyname>Ricci</keyname><forenames>Leonardo</forenames></author><author><keyname>Marinazzo</keyname><forenames>Daniele</forenames></author><author><keyname>Stramaglia</keyname><forenames>Sebastiano</forenames></author></authors><title>Partial Information Rate Decomposition</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Partial Information Decomposition (PID) is a principled and flexible method to unveil complex high-order interactions in multi-unit network systems. Though being defined exclusively for random variables, PID is ubiquitously applied to multivariate time series taken as realizations of random processes with temporal statistical structure. Here, to overcome the incorrect depiction of high-order effects by PID schemes applied to dynamic networks, we introduce the framework of Partial Information Rate Decomposition (PIRD). PIRD is formalized applying lattice theory to decompose the information shared dynamically between a target random process and a set of source processes, implemented for Gaussian processes through a spectral expansion of information rates, and demonstrated in practice analyzing time series from large-scale climate oscillations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04553</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04553</id><created>2025-02-06</created><authors><author><keyname>Swanson</keyname><forenames>David</forenames></author><author><keyname>Sherry</keyname><forenames>Alexander</forenames></author><author><keyname>Tang</keyname><forenames>Chad</forenames></author></authors><title>Variance component mixture modelling for longitudinal T-cell receptor   clonal dynamics</title><categories>stat.ME</categories><msc-class>62F15 (Primary), 62H30 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Studies of T cells and their clonally unique receptors have shown promise in elucidating the association between immune response and human disease. Methods to identify T-cell receptor clones which expand or contract in response to certain therapeutic strategies have so far been limited to longitudinal pairwise comparisons of clone frequency with multiplicity adjustment. Here we develop a more general mixture model approach for arbitrary follow-up and missingness which partitions dynamic longitudinal clone frequency behavior from static. While it is common to mix on the location or scale parameter of a family of distributions, the model instead mixes on the parameterization itself, the dynamic component allowing for a variable, Gamma-distributed Poisson mean parameter over longitudinal follow-up, while the static component mean is time invariant. Leveraging conjugacy, one can integrate out the mean parameter for the dynamic and static components to yield distinct posterior predictive distributions whose expressions are a product of negative binomials and a single negative multinomial, respectively, each modified according to an offset for receptor read count normalization. An EM-algorithm is developed to estimate hyperparameters and component membership, and validity of the approach is demonstrated in simulation. The model identifies a statistically significant and clinically relevant increase in TCR clonal dynamism among metastasis-directed radiation therapy in a cohort of prostate cancer patients. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04555</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04555</id><created>2025-02-06</created><authors><author><keyname>Sparacino</keyname><forenames>Laura</forenames></author><author><keyname>Mijatovic</keyname><forenames>Gorana</forenames></author><author><keyname>Antonacci</keyname><forenames>Yuri</forenames></author><author><keyname>Ricci</keyname><forenames>Leonardo</forenames></author><author><keyname>Marinazzo</keyname><forenames>Daniele</forenames></author><author><keyname>Stramaglia</keyname><forenames>Sebastiano</forenames></author><author><keyname>Faes</keyname><forenames>Luca</forenames></author></authors><title>Decomposing Multivariate Information Rates in Networks of Random   Processes</title><categories>stat.ME cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The Partial Information Decomposition (PID) framework has emerged as a powerful tool for analyzing high-order interdependencies in complex network systems. However, its application to dynamic processes remains challenging due to the implicit assumption of memorylessness, which often falls in real-world scenarios. In this work, we introduce the framework of Partial Information Rate Decomposition (PIRD) that extends PID to random processes with temporal correlations. By leveraging mutual information rate (MIR) instead of mutual information (MI), our approach decomposes the dynamic information shared by multivariate random processes into unique, redundant, and synergistic contributions obtained aggregating information rate atoms in a principled manner. To solve PIRD, we define a pointwise redundancy rate function based on the minimum MI principle applied locally in the frequency-domain representation of the processes. The framework is validated in benchmark simulations of Gaussian systems, demonstrating its advantages over traditional PID in capturing temporal correlations and showing how the spectral representation may reveal scale-specific higher-order interaction that are obscured in the time domain. Furthermore, we apply PIRD to a physiological network comprising cerebrovascular and cardiovascular variables, revealing frequency-dependent redundant information exchange during a protocol of postural stress. Our results highlight the necessity of accounting for the full temporal statistical structure and spectral content of vector random processes to meaningfully perform information decomposition in network systems with dynamic behavior such as those typically encountered in neuroscience and physiology. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04574</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04574</id><created>2025-02-06</created><authors><author><keyname>Gong</keyname><forenames>ZhuQing</forenames></author><author><keyname>Zuo</keyname><forenames>XiNian</forenames></author></authors><title>Dark Brain Energy: Toward an Integrative Model of Spontaneous Slow   Oscillations</title><categories>q-bio.NC cs.IT math.IT stat.AP</categories><comments>38 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Neural oscillations facilitate the functioning of the human brain in spatial and temporal dimensions at various frequencies. These oscillations feature a universal frequency architecture that is governed by brain anatomy, ensuring frequency specificity remains invariant across different measurement techniques. Initial magnetic resonance imaging (MRI) methodology constrained functional MRI (fMRI) investigations to a singular frequency range, thereby neglecting the frequency characteristics inherent in blood oxygen level-dependent oscillations. With advancements in MRI technology, it has become feasible to decode intricate brain activities via multi-band frequency analysis (MBFA). During the past decade, the utilization of MBFA in fMRI studies has surged, unveiling frequency-dependent characteristics of spontaneous slow oscillations (SSOs) believed to base dark energy in the brain. There remains a dearth of conclusive insights and hypotheses pertaining to the properties and functionalities of SSOs in distinct bands. We surveyed the SSO MBFA studies during the past 15 years to delineate the attributes of SSOs and enlighten their correlated functions. We further proposed a model to elucidate the hierarchical organization of multi-band SSOs by integrating their function, aimed at bridging theoretical gaps and guiding future MBFA research endeavors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04575</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04575</id><created>2025-02-06</created><authors><author><keyname>Guo</keyname><forenames>Wei</forenames></author><author><keyname>Tao</keyname><forenames>Molei</forenames></author><author><keyname>Chen</keyname><forenames>Yongxin</forenames></author></authors><title>Complexity Analysis of Normalizing Constant Estimation: from Jarzynski   Equality to Annealed Importance Sampling and beyond</title><categories>stat.ML cs.LG cs.NA math.NA physics.comp-ph stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given an unnormalized probability density $\pi\propto\mathrm{e}^{-V}$, estimating its normalizing constant $Z=\int_{\mathbb{R}^d}\mathrm{e}^{-V(x)}\mathrm{d}x$ or free energy $F=-\log Z$ is a crucial problem in Bayesian statistics, statistical mechanics, and machine learning. It is challenging especially in high dimensions or when $\pi$ is multimodal. To mitigate the high variance of conventional importance sampling estimators, annealing-based methods such as Jarzynski equality and annealed importance sampling are commonly adopted, yet their quantitative complexity guarantees remain largely unexplored. We take a first step toward a non-asymptotic analysis of annealed importance sampling. In particular, we derive an oracle complexity of $\widetilde{O}\left(\frac{d\beta^2{\mathcal{A}}^2}{\varepsilon^4}\right)$ for estimating $Z$ within $\varepsilon$ relative error with high probability, where $\beta$ is the smoothness of $V$ and $\mathcal{A}$ denotes the action of a curve of probability measures interpolating $\pi$ and a tractable reference distribution. Our analysis, leveraging Girsanov theorem and optimal transport, does not explicitly require isoperimetric assumptions on the target distribution. Finally, to tackle the large action of the widely used geometric interpolation of probability distributions, we propose a new normalizing constant estimation algorithm based on reverse diffusion samplers and establish a framework for analyzing its complexity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04591</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04591</id><created>2025-02-06</created><authors><author><keyname>Deidda</keyname><forenames>Piero</forenames></author><author><keyname>Zhang</keyname><forenames>Kaicheng</forenames></author><author><keyname>Higham</keyname><forenames>Desmond</forenames></author><author><keyname>Tudisco</keyname><forenames>Francesco</forenames></author></authors><title>Rethinking Oversmoothing in Graph Neural Networks: A Rank-Based   Perspective</title><categories>cs.LG cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Oversmoothing is a fundamental challenge in graph neural networks (GNNs): as the number of layers increases, node embeddings become increasingly similar, and model performance drops sharply. Traditionally, oversmoothing has been quantified using metrics that measure the similarity of neighbouring node features, such as the Dirichlet energy. While these metrics are related to oversmoothing, we argue they have critical limitations and fail to reliably capture oversmoothing in realistic scenarios. For instance, they provide meaningful insights only for very deep networks and under somewhat strict conditions on the norm of network weights and feature representations. As an alternative, we propose measuring oversmoothing by examining the numerical or effective rank of the feature representations. We provide theoretical support for this approach, demonstrating that the numerical rank of feature representations converges to one for a broad family of nonlinear activation functions under the assumption of nonnegative trained weights. To the best of our knowledge, this is the first result that proves the occurrence of oversmoothing without assumptions on the boundedness of the weight matrices. Along with the theoretical findings, we provide extensive numerical evaluation across diverse graph architectures. Our results show that rank-based metrics consistently capture oversmoothing, whereas energy-based metrics often fail. Notably, we reveal that a significant drop in the rank aligns closely with performance degradation, even in scenarios where energy metrics remain unchanged. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04593</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04593</id><created>2025-02-06</created><authors><author><keyname>Rezaei</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Dieng</keyname><forenames>Adji Bousso</forenames></author></authors><title>The $\alpha$-Alternator: Dynamic Adaptation To Varying Noise Levels In   Sequences Using The Vendi Score For Improved Robustness and Performance</title><categories>cs.LG cs.AI cs.NE stat.ML</categories><comments>The codebase will be made available upon publication. This paper is   dedicated to Patrice Lumumba</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Current state-of-the-art dynamical models, such as Mamba, assume the same level of noisiness for all elements of a given sequence, which limits their performance on noisy temporal data. In this paper, we introduce the $\alpha$-Alternator, a novel generative model for time-dependent data that dynamically adapts to the complexity introduced by varying noise levels in sequences. The $\alpha$-Alternator leverages the Vendi Score (VS), a flexible similarity-based diversity metric, to adjust, at each time step $t$, the influence of the sequence element at time $t$ and the latent representation of the dynamics up to that time step on the predicted future dynamics. This influence is captured by a parameter that is learned and shared across all sequences in a given dataset. The sign of this parameter determines the direction of influence. A negative value indicates a noisy dataset, where a sequence element that increases the VS is considered noisy, and the model relies more on the latent history when processing that element. Conversely, when the parameter is positive, a sequence element that increases the VS is considered informative, and the $\alpha$-Alternator relies more on this new input than on the latent history when updating its predicted latent dynamics. The $\alpha$-Alternator is trained using a combination of observation masking and Alternator loss minimization. Masking simulates varying noise levels in sequences, enabling the model to be more robust to these fluctuations and improving its performance in trajectory prediction, imputation, and forecasting. Our experimental results demonstrate that the $\alpha$-Alternator outperforms both Alternators and state-of-the-art state-space models across neural decoding and time-series forecasting benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04654</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04654</id><created>2025-02-06</created><authors><author><keyname>Lim</keyname><forenames>Keunwoo</forenames></author><author><keyname>Ye</keyname><forenames>Ting</forenames></author><author><keyname>Han</keyname><forenames>Fang</forenames></author></authors><title>A sliced Wasserstein and diffusion approach to random coefficient models</title><categories>math.ST econ.EM stat.TH</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new minimum-distance estimator for linear random coefficient models. This estimator integrates the recently advanced sliced Wasserstein distance with the nearest neighbor methods, both of which enhance computational efficiency. We demonstrate that the proposed method is consistent in approximating the true distribution. Additionally, our formulation encourages a diffusion process-based algorithm, which holds independent interest and potential for broader applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04673</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04673</id><created>2025-02-07</created><authors><author><keyname>Neopane</keyname><forenames>Ojash</forenames></author><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author></authors><title>Optimistic Algorithms for Adaptive Estimation of the Average Treatment   Effect</title><categories>stat.ML cs.LG stat.ME</categories><comments>15 pages, 2 Figures</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Estimation and inference for the Average Treatment Effect (ATE) is a cornerstone of causal inference and often serves as the foundation for developing procedures for more complicated settings. Although traditionally analyzed in a batch setting, recent advances in martingale theory have paved the way for adaptive methods that can enhance the power of downstream inference. Despite these advances, progress in understanding and developing adaptive algorithms remains in its early stages. Existing work either focus on asymptotic analyses that overlook exploration-exploitation tradeoffs relevant in finite-sample regimes or rely on simpler but suboptimal estimators. In this work, we address these limitations by studying adaptive sampling procedures that take advantage of the asymptotically optimal Augmented Inverse Probability Weighting (AIPW) estimator. Our analysis uncovers challenges obscured by asymptotic approaches and introduces a novel algorithmic design principle reminiscent of optimism in multiarmed bandits. This principled approach enables our algorithm to achieve significant theoretical and empirical gains compared to prior methods. Our findings mark a step forward in advancing adaptive causal inference methods in theory and practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04681</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04681</id><created>2025-02-07</created><authors><author><keyname>Louit</keyname><forenames>Sydney</forenames></author><author><keyname>Clark</keyname><forenames>Evan</forenames></author><author><keyname>Gelbard</keyname><forenames>Alexander</forenames></author><author><keyname>Vivek</keyname><forenames>Niketna</forenames></author><author><keyname>Yan</keyname><forenames>Jun</forenames></author><author><keyname>Zhang</keyname><forenames>Panpan</forenames></author></authors><title>CALF-SBM: A Covariate-Assisted Latent Factor Stochastic Block Model</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel network generative model extended from the standard stochastic block model by concurrently utilizing observed node-level information and accounting for network-enabled nodal heterogeneity. The proposed model is so so-called covariate-assisted latent factor stochastic block model (CALF-SBM). The inference for the proposed model is done in a fully Bayesian framework. The primary application of CALF-SBM in the present research is focused on community detection, where a model-selection-based approach is employed to estimate the number of communities which is practically assumed unknown. To assess the performance of CALF-SBM, an extensive simulation study is carried out, including comparisons with multiple classical and modern network clustering algorithms. Lastly, the paper presents two real data applications, respectively based on an extremely new network data demonstrating collaborative relationships of otolaryngologists in the United States and a traditional aviation network data containing information about direct flights between airports in the United States and Canada. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04685</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04685</id><created>2025-02-07</created><authors><author><keyname>Zhang</keyname><forenames>Likun</forenames></author><author><keyname>Bhaganagar</keyname><forenames>Kiran</forenames></author><author><keyname>Wikle</keyname><forenames>Christopher K.</forenames></author></authors><title>Capturing Extreme Events in Turbulence using an Extreme Variational   Autoencoder (xVAE)</title><categories>physics.flu-dyn stat.AP stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Turbulent flow fields are characterized by extreme events that are statistically intermittent and carry a significant amount of energy and physical importance. To emulate these flows, we introduce the extreme variational Autoencoder (xVAE), which embeds a max-infinitely divisible process with heavy-tailed distributions into a standard VAE framework, enabling accurate modeling of extreme events. xVAEs are neural network models that reduce system dimensionality by learning non-linear latent representations of data. We demonstrate the effectiveness of xVAE in large-eddy simulation data of wildland fire plumes, where intense heat release and complex plume-atmosphere interactions generate extreme turbulence. Comparisons with the commonly used Proper Orthogonal Decomposition (POD) modes show that xVAE is more robust in capturing extreme values and provides a powerful uncertainty quantification framework using variational Bayes. Additionally, xVAE enables analysis of the so-called copulas of fields to assess risks associated with rare events while rigorously accounting for uncertainty, such as simultaneous exceedances of high thresholds across multiple locations. The proposed approach provides a new direction for studying realistic turbulent flows, such as high-speed aerodynamics, space propulsion, and atmospheric and oceanic systems that are characterized by extreme events. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04699</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04699</id><created>2025-02-07</created><authors><author><keyname>Lan</keyname><forenames>Hui</forenames></author><author><keyname>Chang</keyname><forenames>Haoge</forenames></author><author><keyname>Dillon</keyname><forenames>Eleanor</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author></authors><title>A Meta-learner for Heterogeneous Effects in Difference-in-Differences</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We address the problem of estimating heterogeneous treatment effects in panel data, adopting the popular Difference-in-Differences (DiD) framework under the conditional parallel trends assumption. We propose a novel doubly robust meta-learner for the Conditional Average Treatment Effect on the Treated (CATT), reducing the estimation to a convex risk minimization problem involving a set of auxiliary models. Our framework allows for the flexible estimation of the CATT, when conditioning on any subset of variables of interest using generic machine learning. Leveraging Neyman orthogonality, our proposed approach is robust to estimation errors in the auxiliary models. As a generalization to our main result, we develop a meta-learning approach for the estimation of general conditional functionals under covariate shift. We also provide an extension to the instrumented DiD setting with non-compliance. Empirical results demonstrate the superiority of our approach over existing baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04709</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04709</id><created>2025-02-07</created><authors><author><keyname>Miftachov</keyname><forenames>Ratmir</forenames></author><author><keyname>Reiß</keyname><forenames>Markus</forenames></author></authors><title>Early Stopping for Regression Trees</title><categories>math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We develop early stopping rules for growing regression tree estimators. The fully data-driven stopping rule is based on monitoring the global residual norm. The best-first search and the breadth-first search algorithms together with linear interpolation give rise to generalized projection or regularization flows. A general theory of early stopping is established. Oracle inequalities for the early-stopped regression tree are derived without any smoothness assumption on the regression function, assuming the original CART splitting rule, yet with a much broader scope. The remainder terms are of smaller order than the best achievable rates for Lipschitz functions in dimension $d\ge 2$. In real and synthetic data the early stopping regression tree estimators attain the statistical performance of cost-complexity pruning while significantly reducing computational costs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04723</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04723</id><created>2025-02-07</created><authors><author><keyname>Lyu</keyname><forenames>Ziyang</forenames></author><author><keyname>Sisson</keyname><forenames>S. A.</forenames></author><author><keyname>Welsh</keyname><forenames>A. H.</forenames></author></authors><title>Asymptotics for EBLUPs within crossed mixed effect models</title><categories>stat.ME</categories><comments>33 pages</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In this article, we derive the joint asymptotic distribution of empirical best linear unbiased predictors (EBLUPs) for individual and cell-level random effects in a crossed mixed effect model. Under mild conditions (which include moment conditions instead of normality for the random effects and model errors), we demonstrate that as the sizes of rows, columns, and, when we include interactions, cells simultaneously increase to infinity, the distribution of the differences between the EBLUPs and the random effects satisfy central limit theorems. These central limit theorems mean the EBLUPs asymptotically follow the convolution of the true random effect distribution and a normal distribution. Moreover, our results enable simple asymptotic approximations and estimators for the mean squared error (MSE) of the EBLUPs, which in turn facilitates the construction of asymptotic prediction intervals for the unobserved random effects. We show in simulations that our simple estimator of the MSE of the EBLUPs works very well in finite samples. Finally, we illustrate the use of the asymptotic prediction intervals with an analysis of movie rating data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04730</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04730</id><created>2025-02-07</created><authors><author><keyname>Xie</keyname><forenames>Tianyu</forenames></author><author><keyname>Richman</keyname><forenames>Harry</forenames></author><author><keyname>Gao</keyname><forenames>Jiansi</forenames></author><author><keyname>Matsen</keyname><forenames>Frederick A.</forenames><suffix>IV</suffix></author><author><keyname>Zhang</keyname><forenames>Cheng</forenames></author></authors><title>PhyloVAE: Unsupervised Learning of Phylogenetic Trees via Variational   Autoencoders</title><categories>stat.ML cs.LG q-bio.PE</categories><comments>ICLR 2025. 22 pages, 14 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Learning informative representations of phylogenetic tree structures is essential for analyzing evolutionary relationships. Classical distance-based methods have been widely used to project phylogenetic trees into Euclidean space, but they are often sensitive to the choice of distance metric and may lack sufficient resolution. In this paper, we introduce phylogenetic variational autoencoders (PhyloVAEs), an unsupervised learning framework designed for representation learning and generative modeling of tree topologies. Leveraging an efficient encoding mechanism inspired by autoregressive tree topology generation, we develop a deep latent-variable generative model that facilitates fast, parallelized topology generation. PhyloVAE combines this generative model with a collaborative inference model based on learnable topological features, allowing for high-resolution representations of phylogenetic tree samples. Extensive experiments demonstrate PhyloVAE's robust representation learning capabilities and fast generation of phylogenetic tree topologies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04750</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04750</id><created>2025-02-07</created><authors><author><keyname>Bui</keyname><forenames>Thang D.</forenames></author><author><keyname>Ashman</keyname><forenames>Matthew</forenames></author><author><keyname>Turner</keyname><forenames>Richard E.</forenames></author></authors><title>Tighter sparse variational Gaussian processes</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sparse variational Gaussian process (GP) approximations based on inducing points have become the de facto standard for scaling GPs to large datasets, owing to their theoretical elegance, computational efficiency, and ease of implementation. This paper introduces a provably tighter variational approximation by relaxing the standard assumption that the conditional approximate posterior given the inducing points must match that in the prior. The key innovation is to modify the conditional posterior to have smaller variances than that of the prior at the training points. We derive the collapsed bound for the regression case, describe how to use the proposed approximation in large data settings, and discuss its application to handle orthogonally structured inducing points and GP latent variable models. Extensive experiments on regression benchmarks, classification, and latent variable models demonstrate that the proposed approximation consistently matches or outperforms standard sparse variational GPs while maintaining the same computational cost. An implementation will be made available in all popular GP packages. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04793</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04793</id><created>2025-02-07</created><authors><author><keyname>Jeunen</keyname><forenames>Olivier</forenames></author></authors><title>$t$-Testing the Waters: Empirically Validating Assumptions for Reliable   A/B-Testing</title><categories>stat.ME cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A/B-tests are a cornerstone of experimental design on the web, with wide-ranging applications and use-cases. The statistical $t$-test comparing differences in means is the most commonly used method for assessing treatment effects, often justified through the Central Limit Theorem (CLT). The CLT ascertains that, as the sample size grows, the sampling distribution of the Average Treatment Effect converges to normality, making the $t$-test valid for sufficiently large sample sizes. When outcome measures are skewed or non-normal, quantifying what "sufficiently large" entails is not straightforward.   To ensure that confidence intervals maintain proper coverage and that $p$-values accurately reflect the false positive rate, it is critical to validate this normality assumption. We propose a practical method to test this, by analysing repeatedly resampled A/A-tests. When the normality assumption holds, the resulting $p$-value distribution should be uniform, and this property can be tested using the Kolmogorov-Smirnov test. This provides an efficient and effective way to empirically assess whether the $t$-test's assumptions are met, and the A/B-test is valid. We demonstrate our methodology and highlight how it helps to identify scenarios prone to inflated Type-I errors. Our approach provides a practical framework to ensure and improve the reliability and robustness of A/B-testing practices. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04807</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04807</id><created>2025-02-07</created><authors><author><keyname>Bashari</keyname><forenames>Meshi</forenames></author><author><keyname>Sesia</keyname><forenames>Matteo</forenames></author><author><keyname>Romano</keyname><forenames>Yaniv</forenames></author></authors><title>Robust Conformal Outlier Detection under Contaminated Reference Data</title><categories>stat.ML cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conformal prediction is a flexible framework for calibrating machine learning predictions, providing distribution-free statistical guarantees. In outlier detection, this calibration relies on a reference set of labeled inlier data to control the type-I error rate. However, obtaining a perfectly labeled inlier reference set is often unrealistic, and a more practical scenario involves access to a contaminated reference set containing a small fraction of outliers. This paper analyzes the impact of such contamination on the validity of conformal methods. We prove that under realistic, non-adversarial settings, calibration on contaminated data yields conservative type-I error control, shedding light on the inherent robustness of conformal methods. This conservativeness, however, typically results in a loss of power. To alleviate this limitation, we propose a novel, active data-cleaning framework that leverages a limited labeling budget and an outlier detection model to selectively annotate data points in the contaminated reference set that are suspected as outliers. By removing only the annotated outliers in this ``suspicious'' subset, we can effectively enhance power while mitigating the risk of inflating the type-I error rate, as supported by our theoretical analysis. Experiments on real datasets validate the conservative behavior of conformal methods under contamination and show that the proposed data-cleaning strategy improves power without sacrificing validity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04824</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04824</id><created>2025-02-07</created><authors><author><keyname>Blake</keyname><forenames>Joshua</forenames></author><author><keyname>Birrell</keyname><forenames>Paul</forenames></author><author><keyname>Walker</keyname><forenames>A. Sarah</forenames></author><author><keyname>Pouwels</keyname><forenames>Koen B.</forenames></author><author><keyname>House</keyname><forenames>Thomas</forenames></author><author><keyname>Tom</keyname><forenames>Brian D. M.</forenames></author><author><keyname>Kypraios</keyname><forenames>Theodore</forenames></author><author><keyname>De Angelis</keyname><forenames>Daniela</forenames></author></authors><title>Estimating the duration of RT-PCR positivity for SARS-CoV-2 from doubly   interval censored data with undetected infections</title><categories>stat.ME stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Monitoring the incidence of new infections during a pandemic is critical for an effective public health response. General population prevalence surveys for SARS-CoV-2 can provide high-quality data to estimate incidence. However, estimation relies on understanding the distribution of the duration that infections remain detectable. This study addresses this need using data from the Coronavirus Infection Survey (CIS), a long-term, longitudinal, general population survey conducted in the UK. Analyzing these data presents unique challenges, such as doubly interval censoring, undetected infections, and false negatives. We propose a Bayesian nonparametric survival analysis approach, estimating a discrete-time distribution of durations and integrating prior information derived from a complementary study. Our methodology is validated through a simulation study, including its resilience to model misspecification, and then applied to the CIS dataset. This results in the first estimate of the full duration distribution in a general population, as well as methodology that could be transferred to new contexts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04832</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04832</id><created>2025-02-07</created><authors><author><keyname>Ballarin</keyname><forenames>Giovanni</forenames></author><author><keyname>Grigoryeva</keyname><forenames>Lyudmila</forenames></author><author><keyname>Ortega</keyname><forenames>Juan-Pablo</forenames></author></authors><title>Memory Capacity of Nonlinear Recurrent Networks: Is it Informative?</title><categories>cs.LG stat.ML</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The total memory capacity (MC) of linear recurrent neural networks (RNNs) has been proven to be equal to the rank of the corresponding Kalman controllability matrix, and it is almost surely maximal for connectivity and input weight matrices drawn from regular distributions. This fact questions the usefulness of this metric in distinguishing the performance of linear RNNs in the processing of stochastic signals. This note shows that the MC of random nonlinear RNNs yields arbitrary values within established upper and lower bounds depending just on the input process scale. This confirms that the existing definition of MC in linear and nonlinear cases has no practical value. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04849</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04849</id><created>2025-02-07</created><authors><author><keyname>Yu</keyname><forenames>Yifeng</forenames></author><author><keyname>Yu</keyname><forenames>Lu</forenames></author></authors><title>Advancing Wasserstein Convergence Analysis of Score-Based Models:   Insights from Discretization and Second-Order Acceleration</title><categories>stat.ML cs.LG math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Score-based diffusion models have emerged as powerful tools in generative modeling, yet their theoretical foundations remain underexplored. In this work, we focus on the Wasserstein convergence analysis of score-based diffusion models. Specifically, we investigate the impact of various discretization schemes, including Euler discretization, exponential integrators, and midpoint randomization methods. Our analysis provides a quantitative comparison of these discrete approximations, emphasizing their influence on convergence behavior. Furthermore, we explore scenarios where Hessian information is available and propose an accelerated sampler based on the local linearization method. We demonstrate that this Hessian-based approach achieves faster convergence rates of order $\widetilde{\mathcal{O}}\left(\frac{1}{\varepsilon}\right)$ significantly improving upon the standard rate $\widetilde{\mathcal{O}}\left(\frac{1}{\varepsilon^2}\right)$ of vanilla diffusion models, where $\varepsilon$ denotes the target accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04867</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04867</id><created>2025-02-07</created><authors><author><keyname>Maclaren</keyname><forenames>Oliver J.</forenames></author><author><keyname>Nicholson</keyname><forenames>Ruanui</forenames></author><author><keyname>Trent</keyname><forenames>Joel A.</forenames></author><author><keyname>Rottenberry</keyname><forenames>Joshua</forenames></author><author><keyname>Simpson</keyname><forenames>Matthew</forenames></author></authors><title>Invariant Image Reparameterisation: A Unified Approach to Structural and   Practical Identifiability and Model Reduction</title><categories>stat.AP</categories><comments>25 pages + supplementary material</comments><msc-class>62F99 (Primary) 65L09, 93B30, 62F12, 34A55 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Both structural and practical parameter non-identifiability present fundamental challenges when using mathematical models to interpret data. This issue is particularly acute in complex, applied areas such as the life sciences or engineering, where determining appropriate model complexity is challenging. While several approaches exist for diagnosing and resolving parameter non-identifiability, including symbolic methods, profile likelihood analysis, and sloppiness analysis, these approaches have distinct limitations and are rarely combined. We present an integrated approach called Invariant Image Reparameterisation (IIR) that incorporates key elements of these methods in a new way. Our approach replaces symbolic computations with numerical calculations at a single reference estimate and an invariance condition that determines when this local calculation holds globally. Parameter combinations determined by this method are naturally ordered by degree of identifiability, and this supports model reduction by replacing a practically non-identified model with a structurally non-identified approximate model. This approximate model can be further parameterised in terms of identified parameters only. By treating parameter combinations determined by our approach as interest parameters within our established likelihood-based Profile-Wise Analysis (PWA) framework, we incorporate uncertainty quantification in terms of likelihood profiles and confidence sets. We provide a Julia library on GitHub (https://github.com/omaclaren/reparam) demonstrating our methodology across a range of mathematical models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04879</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04879</id><created>2025-02-07</created><authors><author><keyname>Gauthier</keyname><forenames>Etienne</forenames></author><author><keyname>Bach</keyname><forenames>Francis</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Statistical Collusion by Collectives on Learning Platforms</title><categories>stat.ML cs.LG</categories><comments>Code available at: https://github.com/GauthierE/statistical-collusion</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As platforms increasingly rely on learning algorithms, collectives may form and seek ways to influence these platforms to align with their own interests. This can be achieved by coordinated submission of altered data. To evaluate the potential impact of such behavior, it is essential to understand the computations that collectives must perform to impact platforms in this way. In particular, collectives need to make a priori assessments of the effect of the collective before taking action, as they may face potential risks when modifying their data. Moreover they need to develop implementable coordination algorithms based on quantities that can be inferred from observed data. We develop a framework that provides a theoretical and algorithmic treatment of these issues and present experimental results in a product evaluation domain. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04889</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04889</id><created>2025-02-07</created><authors><author><keyname>Bao</keyname><forenames>Han</forenames></author><author><keyname>Sakaue</keyname><forenames>Shinsaku</forenames></author><author><keyname>Takezawa</keyname><forenames>Yuki</forenames></author></authors><title>Any-stepsize Gradient Descent for Separable Data under Fenchel--Young   Losses</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The gradient descent (GD) has been one of the most common optimizer in machine learning. In particular, the loss landscape of a neural network is typically sharpened during the initial phase of training, making the training dynamics hover on the edge of stability. This is beyond our standard understanding of GD convergence in the stable regime where arbitrarily chosen stepsize is sufficiently smaller than the edge of stability. Recently, Wu et al. (COLT2024) have showed that GD converges with arbitrary stepsize under linearly separable logistic regression. Although their analysis hinges on the self-bounding property of the logistic loss, which seems to be a cornerstone to establish a modified descent lemma, our pilot study shows that other loss functions without the self-bounding property can make GD converge with arbitrary stepsize. To further understand what property of a loss function matters in GD, we aim to show arbitrary-stepsize GD convergence for a general loss function based on the framework of \emph{Fenchel--Young losses}. We essentially leverage the classical perceptron argument to derive the convergence rate for achieving $\epsilon$-optimal loss, which is possible for a majority of Fenchel--Young losses. Among typical loss functions, the Tsallis entropy achieves the GD convergence rate $T=\Omega(\epsilon^{-1/2})$, and the R{\'e}nyi entropy achieves the far better rate $T=\Omega(\epsilon^{-1/3})$. We argue that these better rate is possible because of \emph{separation margin} of loss functions, instead of the self-bounding property. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04891</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04891</id><created>2025-02-07</created><authors><author><keyname>Rubio-Madrigal</keyname><forenames>Celia</forenames></author><author><keyname>Jamadandi</keyname><forenames>Adarsh</forenames></author><author><keyname>Burkholz</keyname><forenames>Rebekka</forenames></author></authors><title>GNNs Getting ComFy: Community and Feature Similarity Guided Rewiring</title><categories>cs.LG cs.SI stat.ML</categories><comments>Accepted at ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Maximizing the spectral gap through graph rewiring has been proposed to enhance the performance of message-passing graph neural networks (GNNs) by addressing over-squashing. However, as we show, minimizing the spectral gap can also improve generalization. To explain this, we analyze how rewiring can benefit GNNs within the context of stochastic block models. Since spectral gap optimization primarily influences community strength, it improves performance when the community structure aligns with node labels. Building on this insight, we propose three distinct rewiring strategies that explicitly target community structure, node labels, and their alignment: (a) community structure-based rewiring (ComMa), a more computationally efficient alternative to spectral gap optimization that achieves similar goals; (b) feature similarity-based rewiring (FeaSt), which focuses on maximizing global homophily; and (c) a hybrid approach (ComFy), which enhances local feature similarity while preserving community structure to optimize label-community alignment. Extensive experiments confirm the effectiveness of these strategies and support our theoretical insights. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04892</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04892</id><created>2025-02-07</created><authors><author><keyname>Park</keyname><forenames>Joonhyeong</forenames></author><author><keyname>Park</keyname><forenames>Byoungwoo</forenames></author><author><keyname>Bang</keyname><forenames>Chang-Bae</forenames></author><author><keyname>Choi</keyname><forenames>Jungwon</forenames></author><author><keyname>Chung</keyname><forenames>Hyungjin</forenames></author><author><keyname>Kim</keyname><forenames>Byung-Hoon</forenames></author><author><keyname>Lee</keyname><forenames>Juho</forenames></author></authors><title>A Foundational Brain Dynamics Model via Stochastic Optimal Control</title><categories>cs.LG q-bio.NC stat.ML</categories><comments>The first two authors contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a foundational model for brain dynamics that utilizes stochastic optimal control (SOC) and amortized inference. Our method features a continuous-discrete state space model (SSM) that can robustly handle the intricate and noisy nature of fMRI signals. To address computational limitations, we implement an approximation strategy grounded in the SOC framework. Additionally, we present a simulation-free latent dynamics approach that employs locally linear approximations, facilitating efficient and scalable inference. For effective representation learning, we derive an Evidence Lower Bound (ELBO) from the SOC formulation, which integrates smoothly with recent advancements in self-supervised learning (SSL), thereby promoting robust and transferable representations. Pre-trained on extensive datasets such as the UKB, our model attains state-of-the-art results across a variety of downstream tasks, including demographic prediction, trait analysis, disease diagnosis, and prognosis. Moreover, evaluating on external datasets such as HCP-A, ABIDE, and ADHD200 further validates its superior abilities and resilience across different demographic and clinical distributions. Our foundational model provides a scalable and efficient approach for deciphering brain dynamics, opening up numerous applications in neuroscience. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04907</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04907</id><created>2025-02-07</created><authors><author><keyname>Gachon</keyname><forenames>Erell</forenames></author><author><keyname>Bigot</keyname><forenames>Jérémie</forenames></author><author><keyname>Cazelles</keyname><forenames>Elsa</forenames></author></authors><title>Scalable and consistent embedding of probability measures into Hilbert   spaces via measure quantization</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper is focused on statistical learning from data that come as probability measures. In this setting, popular approaches consist in embedding such data into a Hilbert space with either Linearized Optimal Transport or Kernel Mean Embedding. However, the cost of computing such embeddings prohibits their direct use in large-scale settings. We study two methods based on measure quantization for approximating input probability measures with discrete measures of small-support size. The first one is based on optimal quantization of each input measure, while the second one relies on mean-measure quantization. We study the consistency of such approximations, and its implication for scalable embeddings of probability measures into a Hilbert space at a low computational cost. We finally illustrate our findings with various numerical experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04938</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04938</id><created>2025-02-07</created><authors><author><keyname>Gardini</keyname><forenames>Aldo</forenames></author><author><keyname>Greco</keyname><forenames>Fedele</forenames></author><author><keyname>Trivisano</keyname><forenames>Carlo</forenames></author></authors><title>A note on auxiliary mixture sampling for Bayesian Poisson models</title><categories>stat.ME stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bayesian hierarchical Poisson models are an essential tool for analyzing count data. However, designing efficient algorithms to sample from the posterior distribution of the target parameters remains a challenging task for this class of models. Auxiliary mixture sampling algorithms have been proposed to address this issue. They involve two steps of data augmentations: the first leverages the theory of Poisson processes, and the second approximates the residual distribution of the resulting model through a mixture of Gaussian distributions. In this way, an approximated Gibbs sampler is obtained. In this paper, we focus on the accuracy of the approximation step, highlighting scenarios where the mixture fails to accurately represent the true underlying distribution, leading to a lack of convergence in the algorithm. We outline key features to monitor, in order to assess if the approximation performs as intended. Building on this, we propose a robust version of the auxiliary mixture sampling algorithm, which can detect approximation failures and incorporate a Metropolis-Hastings step when necessary. Finally, we evaluate the proposed algorithm together with the original mixture sampling algorithms on both simulated and real datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04945</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04945</id><created>2025-02-07</created><authors><author><keyname>Yanhao</keyname><affiliation>Max</affiliation></author><author><keyname>Wei</keyname></author><author><keyname>Jiang</keyname><forenames>Zhenling</forenames></author></authors><title>Estimating Parameters of Structural Models Using Neural Networks</title><categories>econ.EM stat.CO</categories><acm-class>G.3; J.4; I.2</acm-class><journal-ref>Marketing Science 44(1):102-128 (2024)</journal-ref><doi>10.1287/mksc.2022.0360</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an alternative use of machine learning. We train neural nets to provide the parameter estimate of a given (structural) econometric model, for example, discrete choice or consumer search. Training examples consist of datasets generated by the econometric model under a range of parameter values. The neural net takes the moments of a dataset as input and tries to recognize the parameter value underlying that dataset. Besides the point estimate, the neural net can also output statistical accuracy. This neural net estimator (NNE) tends to limited-information Bayesian posterior as the number of training datasets increases. We apply NNE to a consumer search model. It gives more accurate estimates at lighter computational costs than the prevailing approach. NNE is also robust to redundant moment inputs. In general, NNE offers the most benefits in applications where other estimation approaches require very heavy simulation costs. We provide code at: https://nnehome.github.io. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04949</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04949</id><created>2025-02-07</created><authors><author><keyname>Elsemüller</keyname><forenames>Lasse</forenames></author><author><keyname>Pratz</keyname><forenames>Valentin</forenames></author><author><keyname>von Krause</keyname><forenames>Mischa</forenames></author><author><keyname>Voss</keyname><forenames>Andreas</forenames></author><author><keyname>Bürkner</keyname><forenames>Paul-Christian</forenames></author><author><keyname>Radev</keyname><forenames>Stefan T.</forenames></author></authors><title>Does Unsupervised Domain Adaptation Improve the Robustness of Amortized   Bayesian Inference? A Systematic Evaluation</title><categories>stat.ML cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Neural networks are fragile when confronted with data that significantly deviates from their training distribution. This is true in particular for simulation-based inference methods, such as neural amortized Bayesian inference (ABI), where models trained on simulated data are deployed on noisy real-world observations. Recent robust approaches employ unsupervised domain adaptation (UDA) to match the embedding spaces of simulated and observed data. However, the lack of comprehensive evaluations across different domain mismatches raises concerns about the reliability in high-stakes applications. We address this gap by systematically testing UDA approaches across a wide range of misspecification scenarios in both a controlled and a high-dimensional benchmark. We demonstrate that aligning summary spaces between domains effectively mitigates the impact of unmodeled phenomena or noise. However, the same alignment mechanism can lead to failures under prior misspecifications - a critical finding with practical consequences. Our results underscore the need for careful consideration of misspecification types when using UDA techniques to increase the robustness of ABI in practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04970</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04970</id><created>2025-02-07</created><authors><author><keyname>Langbein</keyname><forenames>Sophie Hanna</forenames></author><author><keyname>Koenen</keyname><forenames>Niklas</forenames></author><author><keyname>Wright</keyname><forenames>Marvin N.</forenames></author></authors><title>Gradient-based Explanations for Deep Learning Survival Models</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deep learning survival models often outperform classical methods in time-to-event predictions, particularly in personalized medicine, but their "black box" nature hinders broader adoption. We propose a framework for gradient-based explanation methods tailored to survival neural networks, extending their use beyond regression and classification. We analyze the implications of their theoretical assumptions for time-dependent explanations in the survival setting and propose effective visualizations incorporating the temporal dimension. Experiments on synthetic data show that gradient-based methods capture the magnitude and direction of local and global feature effects, including time dependencies. We introduce GradSHAP(t), a gradient-based counterpart to SurvSHAP(t), which outperforms SurvSHAP(t) and SurvLIME in a computational speed vs. accuracy trade-off. Finally, we apply these methods to medical data with multi-modal inputs, revealing relevant tabular features and visual patterns, as well as their temporal dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04990</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04990</id><created>2025-02-07</created><authors><author><keyname>Pichler</keyname><forenames>Clemens</forenames></author><author><keyname>Jewson</keyname><forenames>Jack</forenames></author><author><keyname>Avalos-Pacheco</keyname><forenames>Alejandra</forenames></author></authors><title>Probabilistic Programming with Sufficient Statistics for faster Bayesian   Computation</title><categories>stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Probabilistic programming methods have revolutionised Bayesian inference, making it easier than ever for practitioners to perform Markov-chain-Monte-Carlo sampling from non-conjugate posterior distributions. Here we focus on Stan, arguably the most used probabilistic programming tool for Bayesian inference (Carpenter et al., 2017), and its interface with R via the brms (Burkner, 2017) and rstanarm (Goodrich et al., 2024) packages. Although easy to implement, these tools can become computationally prohibitive when applied to datasets with many observations or models with numerous parameters. While the use of sufficient statistics is well-established in theory, it has been surprisingly overlooked in state-of-the-art Stan software. We show that when the likelihood can be written in terms of sufficient statistics, considerable computational improvements can be made to current implementations. We demonstrate how this approach provides accurate inference at a fraction of the time than state-of-the-art implementations for Gaussian linear regression models with non-conjugate priors, hierarchical random effects models, and factor analysis models. Our results also show that moderate computational gains can be achieved even in models where the likelihood can only be partially written in terms of sufficient statistics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05021</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05021</id><created>2025-02-07</created><authors><author><keyname>van Heel</keyname><forenames>Simon Donker</forenames></author><author><keyname>Lange</keyname><forenames>Rutger-Jan</forenames></author><author><keyname>van Dijk</keyname><forenames>Dick</forenames></author><author><keyname>van Os</keyname><forenames>Bram</forenames></author></authors><title>Stability and performance guarantees for misspecified multivariate   score-driven filters</title><categories>stat.ME eess.SP stat.ML</categories><comments>71 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the problem of tracking latent time-varying parameter vectors under model misspecification. We analyze implicit and explicit score-driven (ISD and ESD) filters, which update a prediction of the parameters using the gradient of the logarithmic observation density (i.e., the score). In the ESD filter, the score is computed using the predicted parameter values, whereas in the ISD filter, the score is evaluated using the new, updated parameter values. For both filter types, we derive novel sufficient conditions for the exponential stability (i.e., invertibility) of the filtered parameter path and existence of a finite mean squared error (MSE) bound with respect to the pseudo-true parameter path. In addition, we present expressions for finite-sample and asymptotic MSE bounds. Our performance guarantees rely on mild moment conditions on the data-generating process, while our stability result is entirely agnostic about the true process. As a result, our primary conditions depend only on the characteristics of the filter; hence, they are verifiable in practice. Concavity of the postulated log density combined with simple parameter restrictions is sufficient (but not necessary) for ISD-filter stability, whereas ESD-filter stability additionally requires the score to be Lipschitz continuous. Extensive simulation studies validate our theoretical findings and demonstrate the enhanced stability and improved performance of ISD over ESD filters. An empirical application to U.S. Treasury-bill rates confirms the practical relevance of our contribution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05072</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05072</id><created>2025-02-07</created><authors><author><keyname>Barnett</keyname><forenames>Helen</forenames></author><author><keyname>Boix</keyname><forenames>Oliver</forenames></author><author><keyname>Kontos</keyname><forenames>Dimitris</forenames></author><author><keyname>Jaki</keyname><forenames>Thomas</forenames></author></authors><title>Joint TITE-CRM for Dual Agent Dose Finding Studies</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Dual agent dose-finding trials study the effect of a combination of more than one agent, where the objective is to find the Maximum Tolerated Dose Combination (MTC), the combination of doses of the two agents that is associated with a pre-specified risk of being unsafe. In a Phase I/II setting, the objective is to find a dose combination that is both safe and active, the Optimal Biological Dose (OBD), that optimizes a criterion based on both safety and activity. Since Oncology treatments are typically given over multiple cycles, both the safety and activity outcome can be considered as late-onset, potentially occurring in the later cycles of treatment. This work proposes two model-based designs for dual-agent dose finding studies with late-onset activity and late-onset toxicity outcomes, the Joint TITE-POCRM and the Joint TITE-BLRM. Their performance is compared alongside a model-assisted comparator in a comprehensive simulation study motivated by a real trial example, with an extension to consider alternative sized dosing grids. It is found that both model-based methods outperform the model-assisted design. Whilst on average the two model-based designs are comparable, this comparability is not consistent across scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05074</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05074</id><created>2025-02-07</created><authors><author><keyname>Atanasov</keyname><forenames>Alexander</forenames></author><author><keyname>Bordelon</keyname><forenames>Blake</forenames></author><author><keyname>Zavatone-Veth</keyname><forenames>Jacob A.</forenames></author><author><keyname>Paquette</keyname><forenames>Courtney</forenames></author><author><keyname>Pehlevan</keyname><forenames>Cengiz</forenames></author></authors><title>Two-Point Deterministic Equivalence for Stochastic Gradient Dynamics in   Linear Models</title><categories>cond-mat.dis-nn cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We derive a novel deterministic equivalence for the two-point function of a random matrix resolvent. Using this result, we give a unified derivation of the performance of a wide variety of high-dimensional linear models trained with stochastic gradient descent. This includes high-dimensional linear regression, kernel regression, and random feature models. Our results include previously known asymptotics as well as novel ones. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05075</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05075</id><created>2025-02-07</created><authors><author><keyname>Dong</keyname><forenames>Yijun</forenames></author><author><keyname>Li</keyname><forenames>Yicheng</forenames></author><author><keyname>Li</keyname><forenames>Yunai</forenames></author><author><keyname>Lee</keyname><forenames>Jason D.</forenames></author><author><keyname>Lei</keyname><forenames>Qi</forenames></author></authors><title>Discrepancies are Virtue: Weak-to-Strong Generalization through Lens of   Intrinsic Dimension</title><categories>cs.LG cs.NA math.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weak-to-strong (W2S) generalization is a type of finetuning (FT) where a strong (large) student model is trained on pseudo-labels generated by a weak teacher. Surprisingly, W2S FT often outperforms the weak teacher. We seek to understand this phenomenon through the observation that FT often occurs in intrinsically low-dimensional spaces. Leveraging the low intrinsic dimensionality of FT, we analyze W2S in the ridgeless regression setting from a variance reduction perspective. For a strong student - weak teacher pair with sufficiently expressive low-dimensional feature subspaces $\mathcal{V}_s, \mathcal{V}_w$, we provide an exact characterization of the variance that dominates the generalization error of W2S. This unveils a virtue of discrepancy between the strong and weak models in W2S: the variance of the weak teacher is inherited by the strong student in $\mathcal{V}_s \cap \mathcal{V}_w$, while reduced by a factor of $\dim(\mathcal{V}_s)/N$ in the subspace of discrepancy $\mathcal{V}_w \setminus \mathcal{V}_s$ with $N$ pseudo-labels for W2S. Further, our analysis casts light on the sample complexities and the scaling of performance gap recovery in W2S. The analysis is supported with experiments on both synthetic regression problems and real vision tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05094</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05094</id><created>2025-02-07</created><authors><author><keyname>Blanchet</keyname><forenames>Jose</forenames></author><author><keyname>Hamoudi</keyname><forenames>Yassine</forenames></author><author><keyname>Szegedy</keyname><forenames>Mario</forenames></author><author><keyname>Wang</keyname><forenames>Guanyang</forenames></author></authors><title>Non-linear Quantum Monte Carlo</title><categories>quant-ph cs.LG cs.NA math.NA stat.CO stat.ML</categories><comments>30 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The mean of a random variable can be understood as a $\textit{linear}$ functional on the space of probability distributions. Quantum computing is known to provide a quadratic speedup over classical Monte Carlo methods for mean estimation. In this paper, we investigate whether a similar quadratic speedup is achievable for estimating $\textit{non-linear}$ functionals of probability distributions. We propose a quantum-inside-quantum Monte Carlo algorithm that achieves such a speedup for a broad class of non-linear estimation problems, including nested conditional expectations and stochastic optimization. Our algorithm improves upon the direct application of the quantum multilevel Monte Carlo algorithm introduced by An et al.. The existing lower bound indicates that our algorithm is optimal up polylogarithmic factors. A key innovation of our approach is a new sequence of multilevel Monte Carlo approximations specifically designed for quantum computing, which is central to the algorithm's improved performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05102</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05102</id><created>2025-02-07</created><authors><author><keyname>Piancastelli</keyname><forenames>Luiza</forenames></author><author><keyname>Barreto-Souza</keyname><forenames>Wagner</forenames></author></authors><title>Time Series Analysis of Rankings: A GARCH-Type Approach</title><categories>stat.ME stat.CO stat.ML</categories><comments>Paper submitted for publication</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Ranking data are frequently obtained nowadays but there are still scarce methods for treating these data when temporally observed. The present paper contributes to this topic by proposing and developing novel models for handling time series of ranking data. We introduce a class of time-varying ranking models inspired by the Generalized AutoRegressive Conditional Heteroskedasticity (GARCH) models. More specifically, the temporal dynamics are defined by the conditional distribution of the current ranking given the past rankings, which are assumed to follow a Mallows distribution, which implicitly depends on a distance. Then, autoregressive and feedback components are incorporated into the model through the conditional expectation of the associated distances. Theoretical properties of our ranking GARCH models such as stationarity and ergodicity are established. The estimation of parameters is performed via maximum likelihood estimation when data is fully observed. We develop a Monte Carlo Expectation-Maximisation algorithm to deal with cases involving missing data. Monte Carlo simulation studies are presented to study the performance of the proposed estimators under both non-missing and missing data scenarios. A real data application about the weekly ranking of professional tennis players from 2015 to 2019 is presented under our proposed ranking GARCH models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05122</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05122</id><created>2025-02-07</created><authors><author><keyname>Xi</keyname><forenames>Johnny</forenames></author><author><keyname>Dance</keyname><forenames>Hugh</forenames></author><author><keyname>Orbanz</keyname><forenames>Peter</forenames></author><author><keyname>Bloem-Reddy</keyname><forenames>Benjamin</forenames></author></authors><title>Distinguishing Cause from Effect with Causal Velocity Models</title><categories>stat.ML cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bivariate structural causal models (SCM) are often used to infer causal direction by examining their goodness-of-fit under restricted model classes. In this paper, we describe a parametrization of bivariate SCMs in terms of a causal velocity by viewing the cause variable as time in a dynamical system. The velocity implicitly defines counterfactual curves via the solution of initial value problems where the observation specifies the initial condition. Using tools from measure transport, we obtain a unique correspondence between SCMs and the score function of the generated distribution via its causal velocity. Based on this, we derive an objective function that directly regresses the velocity against the score function, the latter of which can be estimated non-parametrically from observational data. We use this to develop a method for bivariate causal discovery that extends beyond known model classes such as additive or location scale noise, and that requires no assumptions on the noise distributions. When the score is estimated well, the objective is also useful for detecting model non-identifiability and misspecification. We present positive results in simulation and benchmark experiments where many existing methods fail, and perform ablation studies to examine the method's sensitivity to accurate score estimation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05127</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05127</id><created>2025-02-07</created><authors><author><keyname>Everink</keyname><forenames>Jasper M.</forenames></author><author><keyname>Amougou</keyname><forenames>Bernardin Tamo</forenames></author><author><keyname>Pereyra</keyname><forenames>Marcelo</forenames></author></authors><title>Self-supervised Conformal Prediction for Uncertainty Quantification in   Imaging Problems</title><categories>cs.CV stat.ME</categories><msc-class>62H35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most image restoration problems are ill-conditioned or ill-posed and hence involve significant uncertainty. Quantifying this uncertainty is crucial for reliably interpreting experimental results, particularly when reconstructed images inform critical decisions and science. However, most existing image restoration methods either fail to quantify uncertainty or provide estimates that are highly inaccurate. Conformal prediction has recently emerged as a flexible framework to equip any estimator with uncertainty quantification capabilities that, by construction, have nearly exact marginal coverage. To achieve this, conformal prediction relies on abundant ground truth data for calibration. However, in image restoration problems, reliable ground truth data is often expensive or not possible to acquire. Also, reliance on ground truth data can introduce large biases in situations of distribution shift between calibration and deployment. This paper seeks to develop a more robust approach to conformal prediction for image restoration problems by proposing a self-supervised conformal prediction method that leverages Stein's Unbiased Risk Estimator (SURE) to self-calibrate itself directly from the observed noisy measurements, bypassing the need for ground truth. The method is suitable for any linear imaging inverse problem that is ill-conditioned, and it is especially powerful when used with modern self-supervised image restoration techniques that can also be trained directly from measurement data. The proposed approach is demonstrated through numerical experiments on image denoising and deblurring, where it delivers results that are remarkably accurate and comparable to those obtained by supervised conformal prediction with ground truth data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05134</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05134</id><created>2025-02-07</created><authors><author><keyname>Kızıldağ</keyname><forenames>Eren C.</forenames></author></authors><title>Information-Theoretic Guarantees for Recovering Low-Rank Tensors from   Symmetric Rank-One Measurements</title><categories>math.ST cs.IT math.IT math.PR stat.ML stat.TH</categories><comments>Algorithmic Learning Theory (ALT), 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the sample complexity of recovering tensors with low symmetric rank from symmetric rank-one measurements. This setting is particularly motivated by the study of higher-order interactions and the analysis of two-layer neural networks with polynomial activations (polynomial networks). Using a covering numbers argument, we analyze the performance of the symmetric rank minimization program and establish near-optimal sample complexity bounds when the underlying distribution is log-concave. Our measurement model involves random symmetric rank-one tensors, which lead to involved probability calculations. To address these challenges, we employ the Carbery-Wright inequality, a powerful tool for studying anti-concentration properties of random polynomials, and leverage orthogonal polynomials. Additionally, we provide a sample complexity lower bound based on Fano's inequality, and discuss broader implications of our results for two-layer polynomial networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05155</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05155</id><created>2025-02-07</created><authors><author><keyname>Tang</keyname><forenames>Shiqin</forenames></author><author><keyname>Yu</keyname><forenames>Shujian</forenames></author><author><keyname>Dong</keyname><forenames>Yining</forenames></author><author><keyname>Qin</keyname><forenames>S. Joe</forenames></author></authors><title>Deep Dynamic Probabilistic Canonical Correlation Analysis</title><categories>cs.LG stat.ML</categories><comments>accepted by ICASSP-25, code is available at   \url{https://github.com/marcusstang/D2PCCA}</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents Deep Dynamic Probabilistic Canonical Correlation Analysis (D2PCCA), a model that integrates deep learning with probabilistic modeling to analyze nonlinear dynamical systems. Building on the probabilistic extensions of Canonical Correlation Analysis (CCA), D2PCCA captures nonlinear latent dynamics and supports enhancements such as KL annealing for improved convergence and normalizing flows for a more flexible posterior approximation. D2PCCA naturally extends to multiple observed variables, making it a versatile tool for encoding prior knowledge about sequential datasets and providing a probabilistic understanding of the system's dynamics. Experimental validation on real financial datasets demonstrates the effectiveness of D2PCCA and its extensions in capturing latent dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05161</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05161</id><created>2025-02-07</created><authors><author><keyname>Antonczak</keyname><forenames>Brittany</forenames></author><author><keyname>Fay</keyname><forenames>Meg</forenames></author><author><keyname>Chawla</keyname><forenames>Aviral</forenames></author><author><keyname>Rowangould</keyname><forenames>Gregory</forenames></author></authors><title>Estimated Roadway Segment Traffic Data by Vehicle Class for the United   States: A Machine Learning Approach</title><categories>stat.AP</categories><comments>14 pages including references, 4 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Highway Performance Monitoring System, managed by the Federal Highway Administration, provides essential data on average annual daily traffic across U.S. roadways, but it has limited representation of medium- and heavy-duty vehicles on non-interstate roads. This gap limits research and policy analysis on the impacts of truck traffic, especially concerning air quality and public health. To address this, we use random forest regression to estimate medium- and heavy-duty vehicle traffic volumes in areas with sparse data. This results in a more comprehensive dataset, which enables the estimation of traffic density at the census block level as a proxy for traffic-related air pollution exposure. Our high-resolution spatial data products, rigorously validated, provide a more accurate representation of truck traffic and its environmental and health impacts. These datasets are valuable for transportation planning, public health research, and policy decisions aimed at mitigating the effects of truck traffic on vulnerable communities exposed to air pollution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:1811.12852</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>1811.12852</id><created>2018-11-30</created><updated>2025-02-06</updated><authors><author><keyname>Burnetas</keyname><forenames>Apostolos N.</forenames></author><author><keyname>Kanavetas</keyname><forenames>Odysseas</forenames></author><author><keyname>Katehakis</keyname><forenames>Michael N.</forenames></author></authors><title>Optimal Data Driven Resource Allocation under Multi-Armed Bandit   Observations</title><categories>stat.ML cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1509.02857</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the first asymptotically optimal strategy for a multi armed bandit (MAB) model under side constraints. The side constraints model situations in which bandit activations are limited by the availability of certain resources that are replenished at a constant rate. The main result involves the derivation of an asymptotic lower bound for the regret of feasible uniformly fast policies and the construction of policies that achieve this lower bound, under pertinent conditions. Further, we provide the explicit form of such policies for the case in which the unknown distributions are Normal with unknown means and known variances, for the case of Normal distributions with unknown means and unknown variances and for the case of arbitrary discrete distributions with finite support. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2202.04912</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2202.04912</id><created>2022-02-10</created><updated>2025-02-06</updated><authors><author><keyname>Qiu</keyname><forenames>Rui</forenames></author><author><keyname>Yu</keyname><forenames>Zhou</forenames></author><author><keyname>Zhu</keyname><forenames>Ruoqing</forenames></author></authors><title>Random Forest Weighted Local Fr\'echet Regression with Random Objects</title><categories>stat.ML cs.LG</categories><comments>This paper has been published in the Journal of Machine Learning   Research</comments><journal-ref>Journal of Machine Learning Research 25 (2024) 1-69</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical analysis is increasingly confronted with complex data from metric spaces. Petersen and M\"uller (2019) established a general paradigm of Fr\'echet regression with complex metric space valued responses and Euclidean predictors. However, the local approach therein involves nonparametric kernel smoothing and suffers from the curse of dimensionality. To address this issue, we in this paper propose a novel random forest weighted local Fr\'echet regression paradigm. The main mechanism of our approach relies on a locally adaptive kernel generated by random forests. Our first method uses these weights as the local average to solve the conditional Fr\'echet mean, while the second method performs local linear Fr\'echet regression, both significantly improving existing Fr\'echet regression methods. Based on the theory of infinite order U-processes and infinite order $M_{m_n}$-estimator, we establish the consistency, rate of convergence, and asymptotic normality for our local constant estimator, which covers the current large sample theory of random forests with Euclidean responses as a special case. Numerical studies show the superiority of our methods with several commonly encountered types of responses such as distribution functions, symmetric positive-definite matrices, and sphere data. The practical merits of our proposals are also demonstrated through the application to New York taxi data and human mortality data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2206.04902</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2206.04902</id><created>2022-06-10</created><updated>2024-11-13</updated><authors><author><keyname>Gruber</keyname><forenames>Luis</forenames></author><author><keyname>Kastner</keyname><forenames>Gregor</forenames></author></authors><title>Forecasting macroeconomic data with Bayesian VARs: Sparse or dense? It   depends!</title><categories>econ.EM stat.AP stat.ME</categories><journal-ref>International Journal of Forecasting (2025)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vector autogressions (VARs) are widely applied when it comes to modeling and forecasting macroeconomic variables. In high dimensions, however, they are prone to overfitting. Bayesian methods, more concretely shrinkage priors, have shown to be successful in improving prediction performance. In the present paper, we introduce the semi-global framework, in which we replace the traditional global shrinkage parameter with group-specific shrinkage parameters. We show how this framework can be applied to various shrinkage priors, such as global-local priors and stochastic search variable selection priors. We demonstrate the virtues of the proposed framework in an extensive simulation study and in an empirical application forecasting data of the US economy. Further, we shed more light on the ongoing ``Illusion of Sparsity'' debate, finding that forecasting performances under sparse/dense priors vary across evaluated economic variables and across time frames. Dynamic model averaging, however, can combine the merits of both worlds. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2211.15353</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2211.15353</id><created>2022-11-25</created><updated>2025-02-07</updated><authors><author><keyname>Letizia</keyname><forenames>Nunzio A.</forenames></author><author><keyname>Tonello</keyname><forenames>Andrea M.</forenames></author></authors><title>Copula Density Neural Estimation</title><categories>cs.LG eess.SP stat.ML</categories><comments>6 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Probability density estimation from observed data constitutes a central task in statistics. Recent advancements in machine learning offer new tools but also pose new challenges. The big data era demands analysis of long-range spatial and long-term temporal dependencies in large collections of raw data, rendering neural networks an attractive solution for density estimation. In this paper, we exploit the concept of copula to explicitly build an estimate of the probability density function associated to any observed data. In particular, we separate univariate marginal distributions from the joint dependence structure in the data, the copula itself, and we model the latter with a neural network-based method referred to as copula density neural estimation (CODINE). Results show that the novel learning approach is capable of modeling complex distributions and it can be applied for mutual information estimation and data generation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2212.09544</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2212.09544</id><created>2022-12-19</created><authors><author><keyname>Gardini</keyname><forenames>Aldo</forenames></author><author><keyname>Greco</keyname><forenames>Fedele</forenames></author><author><keyname>Trivisano</keyname><forenames>Carlo</forenames></author></authors><title>Design and Structure Dependent Priors for Scale Parameters in Latent   Gaussian Models</title><categories>stat.ME math.ST stat.TH</categories><doi>10.1214/24-BA1454</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Many common correlation structures assumed for data can be described through latent Gaussian models. When Bayesian inference is carried out, it is required to set the prior distribution for scale parameters that rules the model components, possibly allowing to incorporate prior information. This task is particularly delicate and many contributions in the literature are devoted to investigating such aspects. We focus on the fact that the scale parameter controls the prior variability of the model component in a complex way since its dispersion is also affected by the correlation structure and the design. To overcome this issue that might confound the prior elicitation step, we propose to let the user specify the marginal prior of a measure of dispersion of the model component, integrating out the scale parameter, the structure and the design. Then, we analytically derive the implied prior for the scale parameter. Results from a simulation study, aimed at showing the behavior of the estimators sampling properties under the proposed prior elicitation strategy, are discussed. Lastly, some real data applications are explored to investigate prior sensitivity and allocation of explained variance among model components. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2304.13077</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2304.13077</id><created>2023-04-25</created><updated>2025-02-07</updated><authors><author><keyname>De Vito</keyname><forenames>Roberta</forenames></author><author><keyname>Avalos-Pacheco</keyname><forenames>Alejandra</forenames></author></authors><title>Multi-study factor regression model: an application in nutritional   epidemiology</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diet is a risk factor for many diseases. In nutritional epidemiology, studying reproducible dietary patterns is critical to reveal important associations with health. However, it is challenging: diverse cultural and ethnic backgrounds may critically impact eating patterns, showing heterogeneity, leading to incorrect dietary patterns and obscuring the components shared across different groups or populations. Moreover, covariate effects generated from observed variables, such as demographics and other confounders, can further bias these dietary patterns. Identifying the shared and group-specific dietary components and covariate effects is essential to drive accurate conclusions. To address these issues, we introduce a new modeling factor regression, the Multi-Study Factor Regression (MSFR) model. The MSFR model analyzes different populations simultaneously, achieving three goals: capturing shared component(s) across populations, identifying group-specific structures, and correcting for covariate effects. We use this novel method to derive common and ethnic-specific dietary patterns in a multi-center epidemiological study in Hispanic/Latinos community. Our model improves the accuracy of common and group dietary signals and yields better prediction than other techniques, revealing significant associations with health. In summary, we provide a tool to integrate different groups, giving accurate dietary signals crucial to inform public health policy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2305.13998</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2305.13998</id><created>2023-05-23</created><updated>2024-01-23</updated><authors><author><keyname>Saves</keyname><forenames>Paul</forenames></author><author><keyname>Lafage</keyname><forenames>Remi</forenames></author><author><keyname>Bartoli</keyname><forenames>Nathalie</forenames></author><author><keyname>Diouane</keyname><forenames>Youssef</forenames></author><author><keyname>Bussemaker</keyname><forenames>Jasper</forenames></author><author><keyname>Lefebvre</keyname><forenames>Thierry</forenames></author><author><keyname>Hwang</keyname><forenames>John T.</forenames></author><author><keyname>Morlier</keyname><forenames>Joseph</forenames></author><author><keyname>Martins</keyname><forenames>Joaquim R. R. A.</forenames></author></authors><title>SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and   Mixed Variables Gaussian Processes</title><categories>cs.LG cs.MS math.OC stat.CO</categories><comments>10.1016/j.advengsoft.2023.103571</comments><journal-ref>Advances in Engineering Software Volume 188, February 2024, 103571</journal-ref><doi>10.1016/j.advengsoft.2023.103571</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multifidelity data. To the best of our knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical and mixed inputs. This open-source software is distributed under the New BSD license. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2306.16033</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2306.16033</id><created>2023-06-28</created><authors><author><keyname>Gardini</keyname><forenames>Aldo</forenames></author></authors><title>Functional and variables selection in extreme value models for regional   flood frequency analysis</title><categories>stat.ME stat.AP</categories><doi>10.1007/s10651-023-00581-8</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The problem of estimating return levels of river discharge, relevant in flood frequency analysis, is tackled by relying on the extreme value theory. The Generalized Extreme Value (GEV) distribution is assumed to model annual maxima values of river discharge registered at multiple gauging stations belonging to the same river basin. The specific features of the data from the Upper Danube basin drive the definition of the proposed statistical model. Firstly, Bayesian P-splines are considered to account for the non-linear effects of station-specific covariates on the GEV parameters. Secondly, the problem of functional and variable selection is addressed by imposing a grouped horseshoe prior on the coefficients, to encourage the shrinkage of non-relevant components to zero. A cross-validation study is organized to compare the proposed modeling solution to other models, showing its potential in reducing the uncertainty of the ungauged predictions without affecting their calibration. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2307.10272</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2307.10272</id><created>2023-07-18</created><updated>2025-02-06</updated><authors><author><keyname>Takeishi</keyname><forenames>Shota</forenames></author></authors><title>A Shrinkage Likelihood Ratio Test for High-Dimensional Subgroup Analysis   with a Logistic-Normal Mixture Model</title><categories>math.ST stat.ME stat.TH</categories><comments>38 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In subgroup analysis, testing the existence of a subgroup with a differential treatment effect serves as protection against spurious subgroup discovery. Despite its importance, this hypothesis testing possesses a complicated nature: parameter characterizing subgroup classification is not identified under the null hypothesis of no subgroup. Due to this irregularity, the existing methods have the following two limitations. First, the asymptotic null distribution of test statistics often takes an intractable form, which necessitates computationally demanding resampling methods to calculate the critical value. Second, the dimension of personal attributes characterizing subgroup membership is not allowed to be of high dimension. To solve these two problems simultaneously, this study develops a shrinkage likelihood ratio test for the existence of a subgroup using a logistic-normal mixture model. The proposed test statistics are built on a modified likelihood function that shrinks possibly high-dimensional unidentified parameters toward zero under the null hypothesis while retaining power under the alternative. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2307.12982</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2307.12982</id><created>2023-07-24</created><updated>2025-02-07</updated><authors><author><keyname>Mukherjee</keyname><forenames>Soumendu Sundar</forenames></author></authors><title>Consistent model selection in the spiked Wigner model via AIC-type   criteria</title><categories>math.ST cs.IT math.IT stat.ME stat.ML stat.TH</categories><comments>25 pages, 2 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the spiked Wigner model \[   X = \sum_{i = 1}^k \lambda_i u_i u_i^\top + \sigma G, \] where $G$ is an $N \times N$ GOE random matrix, and the eigenvalues $\lambda_i$ are all spiked, i.e. above the Baik-Ben Arous-P\'ech\'e (BBP) threshold $\sigma$. We consider AIC-type model selection criteria of the form \[   -2 \, (\text{maximised log-likelihood}) + \gamma \, (\text{number of parameters}) \] for estimating the number $k$ of spikes. For $\gamma &gt; 2$, the above criterion is strongly consistent provided $\lambda_k &gt; \lambda_{\gamma}$, where $\lambda_{\gamma}$ is a threshold strictly above the BBP threshold, whereas for $\gamma &lt; 2$, it almost surely overestimates $k$. Although AIC (which corresponds to $\gamma = 2$) is not strongly consistent, we show that taking $\gamma = 2 + \delta_N$, where $\delta_N \to 0$ and $\delta_N \gg N^{-2/3}$, results in a weakly consistent estimator of $k$. We further show that a soft minimiser of AIC, where one chooses the least complex model whose AIC score is close to the minimum AIC score, is strongly consistent. Based on a spiked (generalised) Wigner representation, we also develop similar model selection criteria for consistently estimating the number of communities in a balanced stochastic block model under some sparsity restrictions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.07399</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.07399</id><created>2023-10-11</created><updated>2024-10-03</updated><authors><author><keyname>Bou-Rabee</keyname><forenames>Nawaf</forenames></author><author><keyname>Kleppe</keyname><forenames>Tore Selland</forenames></author></authors><title>Randomized Runge-Kutta-Nystr\"om Methods for Unadjusted Hamiltonian and   Kinetic Langevin Monte Carlo</title><categories>math.NA cs.NA math.PR stat.CO stat.ME stat.ML</categories><msc-class>60J05 (Primary) 65C05, 65L05, 65P10 (Secondary)</msc-class><journal-ref>Mathematics of Computation 2025</journal-ref><doi>10.1090/mcom/4061</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce $5/2$- and $7/2$-order $L^2$-accurate randomized Runge-Kutta-Nystr\"{o}m methods, tailored for approximating Hamiltonian flows within non-reversible Markov chain Monte Carlo samplers, such as unadjusted Hamiltonian Monte Carlo and unadjusted kinetic Langevin Monte Carlo. We establish quantitative $5/2$-order $L^2$-accuracy upper bounds under gradient and Hessian Lipschitz assumptions on the potential energy function. The numerical experiments demonstrate the superior efficiency of the proposed unadjusted samplers on a variety of well-behaved, high-dimensional target distributions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2311.02655</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2311.02655</id><created>2023-11-05</created><updated>2025-02-06</updated><authors><author><keyname>Horst</keyname><forenames>Ulrich</forenames></author><author><keyname>Xu</keyname><forenames>Wei</forenames></author></authors><title>Second-Order Regular Variation and Second-Order Approximation of Hawkes   Processes</title><categories>math.PR math.FA math.ST stat.TH</categories><comments>40 pages</comments><msc-class>Primary 26A12, 40E05, secondary 60G55, 60K05</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper provides and extends second-order versions of several fundamental theorems on first-order regularly varying functions such as Karamata's theorem/representation and Tauberian's theorem. Our results are used to establish second-order approximations for the mean and variance of Hawkes processes with general kernels. Our approximations provide novel insights into the asymptotic behavior of Hawkes processes. They are also of key importance when establishing functional limit theorems for Hawkes processes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.04711</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.04711</id><created>2024-02-07</created><updated>2024-05-26</updated><authors><author><keyname>Saves</keyname><forenames>Paul</forenames></author></authors><title>High-dimensional multidisciplinary design optimization for aircraft   eco-design / Optimisation multi-disciplinaire en grande dimension pour   l'\'eco-conception avion en avant-projet</title><categories>math.OC cs.MS stat.ML</categories><comments>PhD Thesis, Universit\'e de Toulouse, Toulouse, 2024 on Gaussian   Process kernels for Bayesian optimization in high dimension with mixed and   hierarchical variables at ISAE-SUPAERO. Keywords: Gaussian process, Black-box   optimization, Bayesian inference, Multidisciplinary design optimization,   Mixed hierarchical and categorical inputs, Eco-friendly aircraft design</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The objective of this Philosophiae Doctor (Ph.D) thesis is to propose an efficient approach for optimizing a multidisciplinary black-box model when the optimization problem is constrained and involves a large number of mixed integer design variables (typically 100 variables). The targeted optimization approach, called EGO, is based on a sequential enrichment of an adaptive surrogate model and, in this context, GP surrogate models are one of the most widely used in engineering problems to approximate time-consuming high fidelity models. EGO is a heuristic BO method that performs well in terms of solution quality. However, like any other global optimization method, EGO suffers from the curse of dimensionality, meaning that its performance is satisfactory on lower dimensional problems, but deteriorates as the dimensionality of the optimization search space increases. For realistic aircraft design problems, the typical size of the design variables can even exceed 100 and, thus, trying to solve directly the problems using EGO is ruled out. The latter is especially true when the problems involve both continuous and categorical variables increasing even more the size of the search space. In this Ph.D thesis, effective parameterization tools are investigated, including techniques like partial least squares regression, to significantly reduce the number of design variables. Additionally, Bayesian optimization is adapted to handle discrete variables and high-dimensional spaces in order to reduce the number of evaluations when optimizing innovative aircraft concepts such as the "DRAGON" hybrid airplane to reduce their climate impact. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.13196</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.13196</id><created>2024-03-19</created><updated>2025-02-07</updated><authors><author><keyname>Eskandar</keyname><forenames>Masih</forenames></author><author><keyname>Imtiaz</keyname><forenames>Tooba</forenames></author><author><keyname>Wang</keyname><forenames>Zifeng</forenames></author><author><keyname>Dy</keyname><forenames>Jennifer</forenames></author></authors><title>ADAPT to Robustify Prompt Tuning Vision Transformers</title><categories>cs.LG cs.AI cs.CV stat.ML</categories><comments>Published in Transactions on Machine Learning Research (2025)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The performance of deep models, including Vision Transformers, is known to be vulnerable to adversarial attacks. Many existing defenses against these attacks, such as adversarial training, rely on full-model fine-tuning to induce robustness in the models. These defenses require storing a copy of the entire model, that can have billions of parameters, for each task. At the same time, parameter-efficient prompt tuning is used to adapt large transformer-based models to downstream tasks without the need to save large copies. In this paper, we examine parameter-efficient prompt tuning of Vision Transformers for downstream tasks under the lens of robustness. We show that previous adversarial defense methods, when applied to the prompt tuning paradigm, suffer from gradient obfuscation and are vulnerable to adaptive attacks. We introduce ADAPT, a novel framework for performing adaptive adversarial training in the prompt tuning paradigm. Our method achieves competitive robust accuracy of ~40% w.r.t. SOTA robustness methods using full-model fine-tuning, by tuning only ~1% of the number of parameters. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.13340</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.13340</id><created>2024-03-20</created><updated>2025-02-07</updated><authors><author><keyname>Jiménez-Varón</keyname><forenames>Cristian F.</forenames></author><author><keyname>Sun</keyname><forenames>Ying</forenames></author><author><keyname>Shang</keyname><forenames>Han Lin</forenames></author></authors><title>Forecasting density-valued functional panel data</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce a statistical method for modeling and forecasting functional panel data represented by multiple densities. Density functions are nonnegative and have a constrained integral and thus do not constitute a linear vector space. We implement a center log-ratio transformation to transform densities into unconstrained functions. These functions exhibit cross-sectional correlation and temporal dependence. Via a functional analysis of variance decomposition, we decompose the unconstrained functional panel data into a deterministic trend component and a time-varying residual component. To produce forecasts for the time-varying component, a functional time series forecasting method, based on the estimation of the long-run covariance, is implemented. By combining the forecasts of the time-varying residual component with the deterministic trend component, we obtain $h$-step-ahead forecast curves for multiple populations. Illustrated by age- and sex-specific life-table death counts in the United States, we apply our proposed method to generate forecasts of the life-table death counts for 51 states. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.12120</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.12120</id><created>2024-06-17</created><updated>2025-02-06</updated><authors><author><keyname>Zhao</keyname><forenames>Yulai</forenames></author><author><keyname>Uehara</keyname><forenames>Masatoshi</forenames></author><author><keyname>Scalia</keyname><forenames>Gabriele</forenames></author><author><keyname>Kung</keyname><forenames>Sunyuan</forenames></author><author><keyname>Biancalani</keyname><forenames>Tommaso</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Hajiramezanali</keyname><forenames>Ehsan</forenames></author></authors><title>Adding Conditional Control to Diffusion Models with Reinforcement   Learning</title><categories>cs.LG cs.AI stat.ML</categories><comments>ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diffusion models are powerful generative models that allow for precise control over the characteristics of the generated samples. While these diffusion models trained on large datasets have achieved success, there is often a need to introduce additional controls in downstream fine-tuning processes, treating these powerful models as pre-trained diffusion models. This work presents a novel method based on reinforcement learning (RL) to add such controls using an offline dataset comprising inputs and labels. We formulate this task as an RL problem, with the classifier learned from the offline dataset and the KL divergence against pre-trained models serving as the reward functions. Our method, $\textbf{CTRL}$ ($\textbf{C}$onditioning pre-$\textbf{T}$rained diffusion models with $\textbf{R}$einforcement $\textbf{L}$earning), produces soft-optimal policies that maximize the abovementioned reward functions. We formally demonstrate that our method enables sampling from the conditional distribution with additional controls during inference. Our RL-based approach offers several advantages over existing methods. Compared to classifier-free guidance, it improves sample efficiency and can greatly simplify dataset construction by leveraging conditional independence between the inputs and additional controls. Additionally, unlike classifier guidance, it eliminates the need to train classifiers from intermediate states to additional controls. The code is available at https://github.com/zhaoyl18/CTRL. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.14535</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.14535</id><created>2024-06-20</created><updated>2025-02-06</updated><authors><author><keyname>Deng</keyname><forenames>Shiyuan</forenames></author><author><keyname>Tang</keyname><forenames>He</forenames></author><author><keyname>Bai</keyname><forenames>Shuyang</forenames></author></authors><title>On estimation and order selection for multivariate extremes via   clustering</title><categories>stat.ME math.ST stat.TH</categories><msc-class>62G32 (Primary), 60G70 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the estimation of multivariate extreme models with a discrete spectral measure using spherical clustering techniques. The primary contribution involves devising a method for selecting the order, that is, the number of clusters. The method consistently identifies the true order, i.e., the number of spectral atoms, and enjoys intuitive implementation in practice. Specifically, we introduce an extra penalty term to the well-known simplified average silhouette width, which penalizes small cluster sizes and small dissimilarities between cluster centers. Consequently, we provide a consistent method for determining the order of a max-linear factor model, where a typical information-based approach is not viable. Our second contribution is a large-deviation-type analysis for estimating the discrete spectral measure through clustering methods, which serves as an assessment of the convergence quality of clustering-based estimation for multivariate extremes. Additionally, as a third contribution, we discuss how estimating the discrete measure can lead to parameter estimations of heavy-tailed factor models. We also present simulations and real-data studies that demonstrate order selection and factor model estimation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.06120</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.06120</id><created>2024-07-08</created><updated>2024-11-16</updated><authors><author><keyname>Dong</keyname><forenames>Yijun</forenames></author><author><keyname>Phan</keyname><forenames>Hoang</forenames></author><author><keyname>Pan</keyname><forenames>Xiang</forenames></author><author><keyname>Lei</keyname><forenames>Qi</forenames></author></authors><title>Sketchy Moment Matching: Toward Fast and Provable Data Selection for   Finetuning</title><categories>cs.LG cs.NA math.NA stat.ML</categories><comments>NeurIPS 2024</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit data selection in a modern context of finetuning from a fundamental perspective. Extending the classical wisdom of variance minimization in low dimensions to high-dimensional finetuning, our generalization analysis unveils the importance of additionally reducing bias induced by low-rank approximation. Inspired by the variance-bias tradeoff in high dimensions from the theory, we introduce Sketchy Moment Matching (SkMM), a scalable data selection scheme with two stages. (i) First, the bias is controlled using gradient sketching that explores the finetuning parameter space for an informative low-dimensional subspace $\mathcal{S}$; (ii) then the variance is reduced over $\mathcal{S}$ via moment matching between the original and selected datasets. Theoretically, we show that gradient sketching is fast and provably accurate: selecting $n$ samples by reducing variance over $\mathcal{S}$ preserves the fast-rate generalization $O(\dim(\mathcal{S})/n)$, independent of the parameter dimension. Empirically, we concretize the variance-bias balance via synthetic experiments and demonstrate the effectiveness of SkMM for finetuning in real vision tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.21314</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.21314</id><created>2024-07-30</created><updated>2025-02-07</updated><authors><author><keyname>Li</keyname><forenames>Zhuoyuan</forenames></author><author><keyname>Dong</keyname><forenames>Bin</forenames></author><author><keyname>Zhang</keyname><forenames>Pingwen</forenames></author></authors><title>State-observation augmented diffusion model for nonlinear assimilation   with unknown dynamics</title><categories>cs.LG stat.ML</categories><msc-class>49N45, 60J60, 62F15, 68T20</msc-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Data assimilation has become a key technique for combining physical models with observational data to estimate state variables. However, classical assimilation algorithms often struggle with the high nonlinearity present in both physical and observational models. To address this challenge, a novel generative model, termed the State-Observation Augmented Diffusion (SOAD) model is proposed for data-driven assimilation. The marginal posterior associated with SOAD has been derived and then proved to match the true posterior distribution under mild assumptions, suggesting its theoretical advantages over previous score-based approaches. Experimental results also indicate that SOAD may offer improved performance compared to existing data-driven methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.03733</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.03733</id><created>2024-08-07</created><authors><author><keyname>Maillard</keyname><forenames>Antoine</forenames></author><author><keyname>Troiani</keyname><forenames>Emanuele</forenames></author><author><keyname>Martin</keyname><forenames>Simon</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Zdeborová</keyname><forenames>Lenka</forenames></author></authors><title>Bayes-optimal learning of an extensive-width neural network from   quadratically many samples</title><categories>stat.ML cond-mat.dis-nn cs.IT cs.LG math.IT math.PR</categories><comments>47 pages</comments><journal-ref>Advances in Neural Information Processing Systems 37 (NeurIPS   2024)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning a target function corresponding to a single hidden layer neural network, with a quadratic activation function after the first layer, and random weights. We consider the asymptotic limit where the input dimension and the network width are proportionally large. Recent work [Cui &amp; al '23] established that linear regression provides Bayes-optimal test error to learn such a function when the number of available samples is only linear in the dimension. That work stressed the open challenge of theoretically analyzing the optimal test error in the more interesting regime where the number of samples is quadratic in the dimension. In this paper, we solve this challenge for quadratic activations and derive a closed-form expression for the Bayes-optimal test error. We also provide an algorithm, that we call GAMP-RIE, which combines approximate message passing with rotationally invariant matrix denoising, and that asymptotically achieves the optimal performance. Technically, our result is enabled by establishing a link with recent works on optimal denoising of extensive-rank matrices and on the ellipsoid fitting problem. We further show empirically that, in the absence of noise, randomly-initialized gradient descent seems to sample the space of weights, leading to zero training loss, and averaging over initialization leads to a test error equal to the Bayes-optimal one. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.09004</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.09004</id><created>2024-08-16</created><updated>2025-02-06</updated><authors><author><keyname>Subedi</keyname><forenames>Unique</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Controlling Statistical, Discretization, and Truncation Errors in   Learning Fourier Linear Operators</title><categories>stat.ML cs.LG cs.NA math.NA</categories><comments>Added Experiments</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study learning-theoretic foundations of operator learning, using the linear layer of the Fourier Neural Operator architecture as a model problem. First, we identify three main errors that occur during the learning process: statistical error due to finite sample size, truncation error from finite rank approximation of the operator, and discretization error from handling functional data on a finite grid of domain points. Finally, we analyze a Discrete Fourier Transform (DFT) based least squares estimator, establishing both upper and lower bounds on the aforementioned errors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.09576</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.09576</id><created>2024-08-18</created><updated>2025-02-07</updated><authors><author><keyname>Oubari</keyname><forenames>Fouad</forenames></author><author><keyname>Baha</keyname><forenames>Mohamed El</forenames></author><author><keyname>Meunier</keyname><forenames>Raphael</forenames></author><author><keyname>Décatoire</keyname><forenames>Rodrigue</forenames></author><author><keyname>Mougeot</keyname><forenames>Mathilde</forenames></author></authors><title>A Markov Random Field Multi-Modal Variational AutoEncoder</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Recent advancements in multimodal Variational AutoEncoders (VAEs) have highlighted their potential for modeling complex data from multiple modalities. However, many existing approaches use relatively straightforward aggregating schemes that may not fully capture the complex dynamics present between different modalities. This work introduces a novel multimodal VAE that incorporates a Markov Random Field (MRF) into both the prior and posterior distributions. This integration aims to capture complex intermodal interactions more effectively. Unlike previous models, our approach is specifically designed to model and leverage the intricacies of these relationships, enabling a more faithful representation of multimodal data. Our experiments demonstrate that our model performs competitively on the standard PolyMNIST dataset and shows superior performance in managing complex intermodal dependencies in a specially designed synthetic dataset, intended to test intricate relationships. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.02363</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.02363</id><created>2024-09-03</created><updated>2025-02-06</updated><authors><author><keyname>Maiti</keyname><forenames>Ayan</forenames></author><author><keyname>Michelle</keyname><forenames>Michelle</forenames></author><author><keyname>Yang</keyname><forenames>Haizhao</forenames></author></authors><title>Optimal Neural Network Approximation for High-Dimensional Continuous   Functions</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the authors of Shen Yang Zhang (JMLR, 2022) developed a neural network with width $36d(2d + 1)$ and depth $11$, which utilizes a special activation function called the elementary universal activation function, to achieve the super approximation property for functions in $C([a,b]^d)$. That is, the constructed network only requires a fixed number of neurons (and thus parameters) to approximate a $d$-variate continuous function on a $d$-dimensional hypercube with arbitrary accuracy. More specifically, only $\mathcal{O}(d^2)$ neurons or parameters are used. One natural question is whether we can reduce the number of these neurons or parameters in such a network. By leveraging a variant of the Kolmogorov Superposition Theorem, our analysis shows that there is a neural network generated by the elementary universal activation function with at most $10889d+10887$ unique nonzero parameters such that this super approximation property is attained. Furthermore, we present a family of continuous functions that requires at least width $d$, and thus at least $d$ neurons or parameters, to achieve arbitrary accuracy in its approximation. This suggests that the number of unique nonzero parameters is optimal in the sense that it grows linearly with the input dimension $d$, unlike some approximation methods where parameters may grow exponentially with $d$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.09800</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.09800</id><created>2024-09-15</created><updated>2025-02-06</updated><authors><author><keyname>Calvello</keyname><forenames>Edoardo</forenames></author><author><keyname>Monmarché</keyname><forenames>Pierre</forenames></author><author><keyname>Stuart</keyname><forenames>Andrew M.</forenames></author><author><keyname>Vaes</keyname><forenames>Urbain</forenames></author></authors><title>Accuracy of the Ensemble Kalman Filter in the Near-Linear Setting</title><categories>math.ST cs.NA math.DS math.NA math.OC stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The filtering distribution captures the statistics of the state of a dynamical system from partial and noisy observations. Classical particle filters provably approximate this distribution in quite general settings; however they behave poorly for high dimensional problems, suffering weight collapse. This issue is circumvented by the ensemble Kalman filter which is an equal-weight interacting particle system. However, this finite particle system is only proven to approximate the true filter in the linear Gaussian case. In practice, however, it is applied in much broader settings; as a result, establishing its approximation properties more generally is important. There has been recent progress in the theoretical analysis of the algorithm, establishing stability and error estimates in non-Gaussian settings, but the assumptions on the dynamics and observation models rule out the unbounded vector fields that arise in practice and the analysis applies only to the mean field limit of the ensemble Kalman filter. The present work establishes error bounds between the filtering distribution and the finite particle ensemble Kalman filter when the dynamics and observation vector fields may be unbounded, allowing linear growth. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.11381</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.11381</id><created>2024-09-17</created><updated>2025-02-07</updated><authors><author><keyname>Banerjee</keyname><forenames>Debapratim</forenames></author><author><keyname>Mukherjee</keyname><forenames>Soumendu Sundar</forenames></author><author><keyname>Pal</keyname><forenames>Dipranjan</forenames></author></authors><title>Edge spectra of Gaussian random symmetric matrices with correlated   entries</title><categories>math.PR math-ph math.CO math.MP math.ST stat.TH</categories><comments>27 pages, 2 figures; abstract shortened to meet arXiv requirements</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the largest eigenvalue of a Gaussian random symmetric matrix $X_n$, with zero-mean, unit variance entries satisfying the condition $\sup_{(i, j) \ne (i', j')}|\mathbb{E}[X_{ij} X_{i'j'}]| = O(n^{-(1 + \varepsilon)})$, where $\varepsilon &gt; 0$. It follows from Catalano et al. (2024) that the empirical spectral distribution of $n^{-1/2} X_n$ converges weakly almost surely to the standard semi-circle law. Using a F\"{u}redi-Koml\'{o}s-type high moment analysis, we show that the largest eigenvalue $\lambda_1(n^{-1/2} X_n)$ of $n^{-1/2} X_n$ converges almost surely to $2$. This result is essentially optimal in the sense that one cannot take $\varepsilon = 0$ and still obtain an almost sure limit of $2$. We also derive Gaussian fluctuation results for the largest eigenvalue in the case where the entries have a common non-zero mean. Let $Y_n = X_n + \frac{\lambda}{\sqrt{n}}\mathbf{1} \mathbf{1}^\top$. When $\varepsilon \ge 1$ and $\lambda \gg n^{1/4}$, we show that \[   n^{1/2}\bigg(\lambda_1(n^{-1/2} Y_n) - \lambda - \frac{1}{\lambda}\bigg) \xrightarrow{d} \sqrt{2} Z, \] where $Z$ is a standard Gaussian. On the other hand, when $0 &lt; \varepsilon &lt; 1$, we have $\mathrm{Var}(\frac{1}{n}\sum_{i, j}X_{ij}) = O(n^{1 - \varepsilon})$. Assuming that $\mathrm{Var}(\frac{1}{n}\sum_{i, j} X_{ij}) = \sigma^2 n^{1 - \varepsilon} (1 + o(1))$, if $\lambda \gg n^{\varepsilon/4}$, then we have \[   n^{\varepsilon/2}\bigg(\lambda_1(n^{-1/2} Y_n) - \lambda - \frac{1}{\lambda}\bigg) \xrightarrow{d} \sigma Z. \] While the ranges of $\lambda$ in these fluctuation results are certainly not optimal, a striking aspect is that different scalings are required in the two regimes $0 &lt; \varepsilon &lt; 1$ and $\varepsilon \ge 1$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.19200</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.19200</id><created>2024-09-27</created><updated>2025-02-06</updated><authors><author><keyname>Bai</keyname><forenames>Site</forenames></author><author><keyname>Bullins</keyname><forenames>Brian</forenames></author></authors><title>Faster Acceleration for Steepest Descent</title><categories>math.OC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances (Sherman, 2017; Sidford and Tian, 2018; Cohen et al., 2021) have overcome the fundamental barrier of dimension dependence in the iteration complexity of solving $\ell_\infty$ regression with first-order methods. Yet it remains unclear to what extent such acceleration can be achieved for general $\ell_p$ smooth functions. In this paper, we propose a new accelerated first-order method for convex optimization under non-Euclidean smoothness assumptions. In contrast to standard acceleration techniques, our approach uses primal-dual iterate sequences taken with respect to $\textit{differing}$ norms, which are then coupled using an $\textit{implicitly}$ determined interpolation parameter. For $\ell_p$ norm smooth problems in $d$ dimensions, our method provides an iteration complexity improvement of up to $O(d^{1-\frac{2}{p}})$ in terms of calls to a first-order oracle, thereby allowing us to circumvent long-standing barriers in accelerated non-Euclidean steepest descent. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.03159</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.03159</id><created>2024-10-04</created><updated>2025-02-07</updated><authors><author><keyname>Lu</keyname><forenames>Jiecheng</forenames></author><author><keyname>Han</keyname><forenames>Xu</forenames></author><author><keyname>Sun</keyname><forenames>Yan</forenames></author><author><keyname>Yang</keyname><forenames>Shihao</forenames></author></authors><title>WAVE: Weighted Autoregressive Varing Gate for Time Series Forecasting</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Weighted Autoregressive Varing gatE (WAVE) attention mechanism equipped with both Autoregressive (AR) and Moving-average (MA) components. It can adapt to various attention mechanisms, enhancing and decoupling their ability to capture long-range and local temporal patterns in time series data. In this paper, we first demonstrate that, for the time series forecasting (TSF) task, the previously overlooked decoder-only autoregressive Transformer model can achieve results comparable to the best baselines when appropriate tokenization and training methods are applied. Moreover, inspired by the ARMA model from statistics and recent advances in linear attention, we introduce the full ARMA structure into existing autoregressive attention mechanisms. By using an indirect MA weight generation method, we incorporate the MA term while maintaining the time complexity and parameter size of the underlying efficient attention models. We further explore how indirect parameter generation can produce implicit MA weights that align with the modeling requirements for local temporal impacts. Experimental results show that WAVE attention that incorporates the ARMA structure consistently improves the performance of various AR attentions on TSF tasks, achieving state-of-the-art results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.15361</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.15361</id><created>2024-10-20</created><updated>2025-02-06</updated><authors><author><keyname>Zhou</keyname><forenames>Han</forenames></author><author><keyname>Van Landeghem</keyname><forenames>Jordy</forenames></author><author><keyname>Popordanoska</keyname><forenames>Teodora</forenames></author><author><keyname>Blaschko</keyname><forenames>Matthew B.</forenames></author></authors><title>A Novel Characterization of the Population Area Under the Risk Coverage   Curve (AURC) and Rates of Finite Sample Estimators</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The selective classifier (SC) has been proposed for rank based uncertainty thresholding, which could have applications in safety critical areas such as medical diagnostics, autonomous driving, and the justice system. The Area Under the Risk-Coverage Curve (AURC) has emerged as the foremost evaluation metric for assessing the performance of SC systems. In this work, we present a formal statistical formulation of population AURC, presenting an equivalent expression that can be interpreted as a reweighted risk function. Through Monte Carlo methods, we derive empirical AURC plug-in estimators for finite sample scenarios. The weight estimators associated with these plug-in estimators are shown to be consistent, with low bias and tightly bounded mean squared error (MSE). The plug-in estimators are proven to converge at a rate of $\mathcal{O}(\sqrt{\ln(n)/n})$ demonstrating statistical consistency. We empirically validate the effectiveness of our estimators through experiments across multiple datasets, model architectures, and confidence score functions (CSFs), demonstrating consistency and effectiveness in fine-tuning AURC performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.18959</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.18959</id><created>2024-10-24</created><updated>2025-02-06</updated><authors><author><keyname>Williams</keyname><forenames>Andrew Robert</forenames></author><author><keyname>Ashok</keyname><forenames>Arjun</forenames></author><author><keyname>Marcotte</keyname><forenames>Étienne</forenames></author><author><keyname>Zantedeschi</keyname><forenames>Valentina</forenames></author><author><keyname>Subramanian</keyname><forenames>Jithendaraa</forenames></author><author><keyname>Riachi</keyname><forenames>Roland</forenames></author><author><keyname>Requeima</keyname><forenames>James</forenames></author><author><keyname>Lacoste</keyname><forenames>Alexandre</forenames></author><author><keyname>Rish</keyname><forenames>Irina</forenames></author><author><keyname>Chapados</keyname><forenames>Nicolas</forenames></author><author><keyname>Drouin</keyname><forenames>Alexandre</forenames></author></authors><title>Context is Key: A Benchmark for Forecasting with Essential Textual   Information</title><categories>cs.LG cs.AI stat.ML</categories><comments>Preprint; under review. First two authors contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forecasting is a critical task in decision-making across numerous domains. While historical numerical data provide a start, they fail to convey the complete context for reliable and accurate predictions. Human forecasters frequently rely on additional information, such as background knowledge and constraints, which can efficiently be communicated through natural language. However, in spite of recent progress with LLM-based forecasters, their ability to effectively integrate this textual information remains an open question. To address this, we introduce "Context is Key" (CiK), a time-series forecasting benchmark that pairs numerical data with diverse types of carefully crafted textual context, requiring models to integrate both modalities; crucially, every task in CiK requires understanding textual context to be solved successfully. We evaluate a range of approaches, including statistical models, time series foundation models, and LLM-based forecasters, and propose a simple yet effective LLM prompting method that outperforms all other tested methods on our benchmark. Our experiments highlight the importance of incorporating contextual information, demonstrate surprising performance when using LLM-based forecasting models, and also reveal some of their critical shortcomings. This benchmark aims to advance multimodal forecasting by promoting models that are both accurate and accessible to decision-makers with varied technical expertise. The benchmark can be visualized at https://servicenow.github.io/context-is-key-forecasting/v0/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.19217</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.19217</id><created>2024-10-24</created><authors><author><keyname>Wu</keyname><forenames>Changlong</forenames></author><author><keyname>Grama</keyname><forenames>Ananth</forenames></author><author><keyname>Szpankowski</keyname><forenames>Wojciech</forenames></author></authors><title>No Free Lunch: Fundamental Limits of Learning Non-Hallucinating   Generative Models</title><categories>cs.LG cs.AI stat.ML</categories><journal-ref>International Conference on Learning Representations (ICLR 2025).   URL: https://openreview.net/pdf?id=OwNoTs2r8e</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative models have shown impressive capabilities in synthesizing high-quality outputs across various domains. However, a persistent challenge is the occurrence of "hallucinations", where the model produces outputs that are plausible but invalid. While empirical strategies have been explored to mitigate this issue, a rigorous theoretical understanding remains elusive. In this paper, we develop a theoretical framework to analyze the learnability of non-hallucinating generative models from a learning-theoretic perspective. Our results reveal that non-hallucinating learning is statistically impossible when relying solely on the training dataset, even for a hypothesis class of size two and when the entire training set is truthful. To overcome these limitations, we show that incorporating inductive biases aligned with the actual facts into the learning process is essential. We provide a systematic approach to achieve this by restricting the facts set to a concept class of finite VC-dimension and demonstrate its effectiveness under various learning paradigms. Although our findings are primarily conceptual, they represent a first step towards a principled approach to addressing hallucinations in learning generative models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.19725</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.19725</id><created>2024-10-25</created><updated>2025-02-06</updated><authors><author><keyname>Subedi</keyname><forenames>Unique</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>On the Benefits of Active Data Collection in Operator Learning</title><categories>stat.ML cs.LG</categories><comments>Moved Proofs to the Appendix</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study active data collection strategies for operator learning when the target operator is linear and the input functions are drawn from a mean-zero stochastic process with continuous covariance kernels. With an active data collection strategy, we establish an error convergence rate in terms of the decay rate of the eigenvalues of the covariance kernel. We can achieve arbitrarily fast error convergence rates with sufficiently rapid eigenvalue decay of the covariance kernels. This contrasts with the passive (i.i.d.) data collection strategies, where the convergence rate is never faster than linear decay ($\sim n^{-1}$). In fact, for our setting, we show a \emph{non-vanishing} lower bound for any passive data collection strategy, regardless of the eigenvalues decay rate of the covariance kernel. Overall, our results show the benefit of active data collection strategies in operator learning over their passive counterparts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.20640</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.20640</id><created>2024-10-27</created><updated>2025-02-07</updated><authors><author><keyname>Rivera</keyname><forenames>Eduardo Ochoa</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Near Optimal Pure Exploration in Logistic Bandits</title><categories>stat.ML cs.LG</categories><comments>25 pages, 2 figures. arXiv admin note: text overlap with   arXiv:2006.16073 by other authors</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bandit algorithms have garnered significant attention due to their practical applications in real-world scenarios. However, beyond simple settings such as multi-arm or linear bandits, optimal algorithms remain scarce. Notably, no optimal solution exists for pure exploration problems in the context of generalized linear model (GLM) bandits. In this paper, we narrow this gap and develop the first track-and-stop algorithm for general pure exploration problems under the logistic bandit called logistic track-and-stop (Log-TS). Log-TS is an efficient algorithm that asymptotically matches an approximation for the instance-specific lower bound of the expected sample complexity up to a logarithmic factor. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.03727</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.03727</id><created>2024-12-04</created><updated>2025-02-07</updated><authors><author><keyname>Zhang</keyname><forenames>Zhiheng</forenames></author><author><keyname>Wang</keyname><forenames>Zichen</forenames></author></authors><title>Online Experimental Design With Estimation-Regret Trade-off Under   Network Interference</title><categories>cs.LG math.OC math.ST stat.TH</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network interference has attracted significant attention in the field of causal inference, encapsulating various sociological behaviors where the treatment assigned to one individual within a network may affect the outcomes of others, such as their neighbors. A key challenge in this setting is that standard causal inference methods often assume independent treatment effects among individuals, which may not hold in networked environments. To estimate interference-aware causal effects, a traditional approach is to inherit the independent settings, where practitioners randomly assign experimental participants into different groups and compare their outcomes. While effective in offline settings, this strategy becomes problematic in sequential experiments, where suboptimal decision persists, leading to substantial regret. To address this issue, we introduce a unified interference-aware framework for online experimental design. Compared to existing studies, we extend the definition of arm space by utilizing the statistical concept of exposure mapping, which allows for a more flexible and context-aware representation of treatment effects in networked settings. Crucially, we establish a Pareto-optimal trade-off between estimation accuracy and regret under the network concerning both time period and arm space, which remains superior to baseline models even without network interference. Furthermore, we propose an algorithmic implementation and discuss its generalization across different learning settings and network topology </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.04767</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.04767</id><created>2024-12-05</created><updated>2025-02-06</updated><authors><author><keyname>Tian</keyname><forenames>Bowei</forenames></author><author><keyname>Wang</keyname><forenames>Ziyao</forenames></author><author><keyname>He</keyname><forenames>Shwai</forenames></author><author><keyname>Ye</keyname><forenames>Wanghao</forenames></author><author><keyname>Sun</keyname><forenames>Guoheng</forenames></author><author><keyname>Dai</keyname><forenames>Yucong</forenames></author><author><keyname>Wu</keyname><forenames>Yongkai</forenames></author><author><keyname>Li</keyname><forenames>Ang</forenames></author></authors><title>Towards counterfactual fairness through auxiliary variables</title><categories>cs.LG cs.DS stat.ML</categories><comments>arXiv admin note: text overlap with arXiv:2307.08232 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The challenge of balancing fairness and predictive accuracy in machine learning models, especially when sensitive attributes such as race, gender, or age are considered, has motivated substantial research in recent years. Counterfactual fairness ensures that predictions remain consistent across counterfactual variations of sensitive attributes, which is a crucial concept in addressing societal biases. However, existing counterfactual fairness approaches usually overlook intrinsic information about sensitive features, limiting their ability to achieve fairness while simultaneously maintaining performance. To tackle this challenge, we introduce EXOgenous Causal reasoning (EXOC), a novel causal reasoning framework motivated by exogenous variables. It leverages auxiliary variables to uncover intrinsic properties that give rise to sensitive attributes. Our framework explicitly defines an auxiliary node and a control node that contribute to counterfactual fairness and control the information flow within the model. Our evaluation, conducted on synthetic and real-world datasets, validates EXOC's superiority, showing that it outperforms state-of-the-art approaches in achieving counterfactual fairness. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.08435</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.08435</id><created>2024-12-11</created><updated>2025-02-07</updated><authors><author><keyname>Zhao</keyname><forenames>Lifan</forenames></author><author><keyname>Shen</keyname><forenames>Yanyan</forenames></author></authors><title>Proactive Model Adaptation Against Concept Drift for Online Time Series   Forecasting</title><categories>cs.LG cs.AI cs.CE stat.ML</categories><comments>Accepted by KDD 2025</comments><doi>10.1145/3690624.3709210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time series forecasting always faces the challenge of concept drift, where data distributions evolve over time, leading to a decline in forecast model performance. Existing solutions are based on online learning, which continually organize recent time series observations as new training samples and update model parameters according to the forecasting feedback on recent data. However, they overlook a critical issue: obtaining ground-truth future values of each sample should be delayed until after the forecast horizon. This delay creates a temporal gap between the training samples and the test sample. Our empirical analysis reveals that the gap can introduce concept drift, causing forecast models to adapt to outdated concepts. In this paper, we present Proceed, a novel proactive model adaptation framework for online time series forecasting. Proceed first estimates the concept drift between the recently used training samples and the current test sample. It then employs an adaptation generator to efficiently translate the estimated drift into parameter adjustments, proactively adapting the model to the test sample. To enhance the generalization capability of the framework, Proceed is trained on synthetic diverse concept drifts. Extensive experiments on five real-world datasets across various forecast models demonstrate that Proceed brings more performance improvements than the state-of-the-art online learning methods, significantly facilitating forecast models' resilience against concept drifts. Code is available at https://github.com/SJTU-DMTai/OnlineTSF. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.20471</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.20471</id><created>2024-12-29</created><updated>2025-02-07</updated><authors><author><keyname>Cai</keyname><forenames>Yang</forenames></author><author><keyname>Mitra</keyname><forenames>Siddharth</forenames></author><author><keyname>Wang</keyname><forenames>Xiuyuan</forenames></author><author><keyname>Wibisono</keyname><forenames>Andre</forenames></author></authors><title>On the Convergence of Min-Max Langevin Dynamics and Algorithm</title><categories>cs.GT cs.LG math.OC stat.ML</categories><comments>v2: Revised introduction and presentation of results</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study zero-sum games in the space of probability distributions over the Euclidean space $\mathbb{R}^d$ with entropy regularization, in the setting when the interaction function between the players is smooth and strongly convex-strongly concave. We prove an exponential convergence guarantee for the mean-field min-max Langevin dynamics to compute the equilibrium distribution of the zero-sum game. We also study the finite-particle approximation of the mean-field min-max Langevin dynamics, both in continuous and discrete times. We prove biased convergence guarantees for the continuous-time finite-particle min-max Langevin dynamics to the stationary mean-field equilibrium distribution with an explicit bias term which does not scale with the number of particles. We also prove biased convergence guarantees for the discrete-time finite-particle min-max Langevin algorithm to the stationary mean-field equilibrium distribution with an additional bias term which scales with the step size and the number of particles. This provides an explicit iteration complexity for the average particle along the finite-particle algorithm to approximately compute the equilibrium distribution of the zero-sum game. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.01437</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.01437</id><created>2024-12-23</created><updated>2025-02-06</updated><authors><author><keyname>Murphy</keyname><forenames>Charles</forenames></author><author><keyname>Lizotte</keyname><forenames>Simon</forenames></author><author><keyname>Thibault</keyname><forenames>François</forenames></author><author><keyname>Thibeault</keyname><forenames>Vincent</forenames></author><author><keyname>Desrosiers</keyname><forenames>Patrick</forenames></author><author><keyname>Allard</keyname><forenames>Antoine</forenames></author></authors><title>On the reconstruction limits of complex networks</title><categories>stat.AP cs.IT math.IT physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network reconstruction consists in retrieving the hidden interaction structure of a system from observations. Many reconstruction algorithms have been proposed, although less research has been devoted to describe their theoretical limitations. In this work, we adopt an information-theoretic perspective and define the reconstructability: The fraction of structural information recoverable from data. The reconstructability depends on the true data generating (TDG) model which is shown to set the reconstruction limit: any algorithm can perform, on average, at best like the TDG model. We show that the reconstructability is related to various performance measures, such as the probability of error and the Jaccard similarity. In an empirical context where the TDG model is unknown, we introduce the reconstruction index as an approximation of the reconstructability. We find that performing model selection is crucial for the validity of the reconstruction index as a proxy of the reconstructability of empirical time series and networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.07025</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.07025</id><created>2025-01-12</created><updated>2025-02-07</updated><authors><author><keyname>Zhang</keyname><forenames>Yong</forenames></author><author><keyname>Gyamfi</keyname><forenames>Eric Herrison</forenames></author></authors><title>A Weighted Similarity Metric for Community Detection in Sparse Data</title><categories>stat.ME cs.SI</categories><comments>This paper has been accepted for Workshop of AI for Social Impact at   AAAI 2025</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Many Natural Language Processing (NLP) related applications involves topics and sentiments derived from short documents such as consumer reviews and social media posts. Topics and sentiments of short documents are highly sparse because a short document generally covers a few topics among hundreds of candidates. Imputation of missing data is sometimes hard to justify and also often unpractical in highly sparse data. We developed a method for calculating a weighted similarity for highly sparse data without imputation. This weighted similarity is consist of three components to capture similarities based on both existence and lack of common properties and pattern of missing values. As a case study, we used a community detection algorithm and this weighted similarity to group different shampoo brands based on sparse topic sentiments derived from short consumer reviews. Compared with traditional imputation and similarity measures, the weighted similarity shows better performance in both general community structures and average community qualities. The performance is consistent and robust across metrics and community complexities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10117</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10117</id><created>2025-01-17</created><updated>2025-02-07</updated><authors><author><keyname>Liu</keyname><forenames>Weiguang</forenames></author><author><keyname>de Paula</keyname><forenames>Áureo</forenames></author><author><keyname>Tamer</keyname><forenames>Elie</forenames></author></authors><title>Prediction Sets and Conformal Inference with Censored Outcomes</title><categories>econ.EM stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given data on a scalar random variable $Y$, a prediction set for $Y$ with miscoverage level $\alpha$ is a set of values for $Y$ that contains a randomly drawn $Y$ with probability $1 - \alpha$, where $\alpha \in (0,1)$. Among all prediction sets that satisfy this coverage property, the oracle prediction set is the one with the smallest volume. This paper provides estimation methods of such prediction sets given observed conditioning covariates when $Y$ is \textit{censored} or \textit{measured in intervals}. We first characterise the oracle prediction set under interval censoring and develop a consistent estimator for the shortest prediction {\it interval} that satisfies this coverage property.These consistency results are extended to accommodate cases where the prediction set consists of multiple disjoint intervals. We use conformal inference to construct a prediction set that achieves finite-sample validity under censoring and maintains consistency as sample size increases, using a conformity score function designed for interval data. The procedure accommodates the prediction uncertainty that is irreducible (due to the stochastic nature of outcomes), the modelling uncertainty due to partial identification and also sampling uncertainty that gets reduced as samples get larger. We conduct a set of Monte Carlo simulations and an application to data from the Current Population Survey. The results highlight the robustness and efficiency of the proposed methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10974</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10974</id><created>2025-01-19</created><updated>2025-02-06</updated><authors><author><keyname>Huang</keyname><forenames>Yu-Han</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Sequential Change Detection for Learning in Piecewise Stationary Bandit   Environments</title><categories>cs.IT cs.SY eess.SY math.IT stat.OT</categories><comments>15 pages, 2 figures. arXiv admin note: text overlap with   arXiv:2501.01291</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A finite-horizon variant of the quickest change detection problem is investigated, which is motivated by a change detection problem that arises in piecewise stationary bandits. The goal is to minimize the \emph{latency}, which is smallest threshold such that the probability that the detection delay exceeds the threshold is below a desired low level, while controlling the false alarm probability to a desired low level. When the pre- and post-change distributions are unknown, two tests are proposed as candidate solutions. These tests are shown to attain order optimality in terms of the horizon. Furthermore, the growth in their latencies with respect to the false alarm probability and late detection probability satisfies a property that is desirable in regret analysis for piecewise stationary bandits. Numerical results are provided to validate the theoretical performance results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14974</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14974</id><created>2025-01-24</created><updated>2025-02-06</updated><authors><author><keyname>Deng</keyname><forenames>Fengnan</forenames></author><author><keyname>Vidyashankar</keyname><forenames>Anand N.</forenames></author></authors><title>Private Minimum Hellinger Distance Estimation via Hellinger Distance   Differential Privacy</title><categories>math.ST cs.CR math.PR stat.ME stat.ML stat.TH</categories><msc-class>62F35, 68P27, 62E20, 60E05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective functions based on Hellinger distance yield robust and efficient estimators of model parameters. Motivated by privacy and regulatory requirements encountered in contemporary applications, we derive in this paper \emph{private minimum Hellinger distance estimators}. The estimators satisfy a new privacy constraint, namely, Hellinger differential privacy, while retaining the robustness and efficiency properties. We demonstrate that Hellinger differential privacy shares several features of standard differential privacy while allowing for sharper inference. Additionally, for computational purposes, we also develop Hellinger differentially private gradient descent and Newton-Raphson algorithms. We illustrate the behavior of our estimators in finite samples using numerical experiments and verify that they retain robustness properties under gross-error contamination. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18502</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18502</id><created>2025-01-30</created><updated>2025-02-07</updated><authors><author><keyname>Kumar</keyname><forenames>Ritesh</forenames></author><author><keyname>Vatedka</keyname><forenames>Shashank</forenames></author></authors><title>One-Bit Distributed Mean Estimation with Unknown Variance</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>21 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this work, we study the problem of distributed mean estimation with $1$-bit communication constraints when the variance is unknown. We focus on the specific case where each user has access to one i.i.d. sample drawn from a distribution that belongs to a scale-location family, and is limited to sending just a single bit of information to a central server whose goal is to estimate the mean. We propose non-adaptive and adaptive estimators that are shown to be asymptotically normal. We derive bounds on the asymptotic (in the number of users) Mean Squared Error (MSE) achieved by these estimators. For a class of symmetric log-concave distributions, we derive matching lower bounds for the MSE achieved by adaptive estimators, proving the optimality of our scheme. We show that non-adaptive estimators can be strictly suboptimal by deriving a lower bound on the MSE achieved by any non-adaptive estimator for Gaussian distributions and demonstrating a positive gap between this and the MSE achieved by our adaptive scheme. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00168</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00168</id><created>2025-01-31</created><updated>2025-02-07</updated><authors><author><keyname>Herrera-Esposito</keyname><forenames>Daniel</forenames></author><author><keyname>Burge</keyname><forenames>Johannes</forenames></author></authors><title>Supervised Quadratic Feature Analysis: An Information Geometry Approach   to Dimensionality Reduction</title><categories>stat.ML cs.LG math.DG math.ST stat.TH</categories><comments>18 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Supervised dimensionality reduction aims to map labeled data to a low-dimensional feature space while maximizing class discriminability. Despite the availability of methods for learning complex non-linear features (e.g. Deep Learning), there is an enduring demand for dimensionality reduction methods that learn linear features due to their interpretability, low computational cost, and broad applicability. However, there is a gap between methods that optimize linear separability (e.g. LDA), and more flexible but computationally expensive methods that optimize over arbitrary class boundaries (e.g. metric-learning methods). Here, we present Supervised Quadratic Feature Analysis (SQFA), a dimensionality reduction method for learning linear features that maximize the differences between class-conditional first- and second-order statistics, which allow for quadratic discrimination. SQFA exploits the information geometry of second-order statistics in the symmetric positive definite manifold. We show that SQFA features support quadratic discriminability in real-world problems. We also provide a theoretical link, based on information geometry, between SQFA and the Quadratic Discriminant Analysis (QDA) classifier. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00182</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00182</id><created>2025-01-31</created><updated>2025-02-07</updated><authors><author><keyname>Seo</keyname><forenames>Jungwon</forenames></author><author><keyname>Catak</keyname><forenames>Ferhat Ozgur</forenames></author><author><keyname>Rong</keyname><forenames>Chunming</forenames></author></authors><title>Understanding Federated Learning from IID to Non-IID dataset: An   Experimental Study</title><categories>cs.LG cs.AI stat.ML</categories><journal-ref>36th Norwegian ICT Conference for Research and Education, NIKT   2024</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As privacy concerns and data regulations grow, federated learning (FL) has emerged as a promising approach for training machine learning models across decentralized data sources without sharing raw data. However, a significant challenge in FL is that client data are often non-IID (non-independent and identically distributed), leading to reduced performance compared to centralized learning. While many methods have been proposed to address this issue, their underlying mechanisms are often viewed from different perspectives. Through a comprehensive investigation from gradient descent to FL, and from IID to non-IID data settings, we find that inconsistencies in client loss landscapes primarily cause performance degradation in non-IID scenarios. From this understanding, we observe that existing methods can be grouped into two main strategies: (i) adjusting parameter update paths and (ii) modifying client loss landscapes. These findings offer a clear perspective on addressing non-IID challenges in FL and help guide future research in the field. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00838</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00838</id><created>2025-02-02</created><authors><author><keyname>Bussemaker</keyname><forenames>Jasper H.</forenames></author><author><keyname>Saves</keyname><forenames>Paul</forenames></author><author><keyname>Bartoli</keyname><forenames>Nathalie</forenames></author><author><keyname>Lefebvre</keyname><forenames>Thierry</forenames></author><author><keyname>Lafage</keyname><forenames>Rémi</forenames></author></authors><title>System Architecture Optimization Strategies: Dealing with Expensive   Hierarchical Problems</title><categories>math.OC cs.DM stat.AP</categories><comments>J Glob Optim (2024)</comments><doi>10.1007/s10898-024-01443-8</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Choosing the right system architecture for the problem at hand is challenging due to the large design space and high uncertainty in the early stage of the design process. Formulating the architecting process as an optimization problem may mitigate some of these challenges. This work investigates strategies for solving System Architecture Optimization (SAO) problems: expensive, black-box, hierarchical, mixed-discrete, constrained, multi-objective problems that may be subject to hidden constraints. Imputation ratio, correction ratio, correction fraction, and max rate diversity metrics are defined for characterizing hierar chical design spaces. This work considers two classes of optimization algorithms for SAO: Multi-Objective Evolutionary Algorithms (MOEA) such as NSGA-II, and Bayesian Optimization (BO) algorithms. A new Gaussian process kernel is presented that enables modeling hierarchical categorical variables, extending previous work on modeling continuous and integer hierarchical variables. Next, a hierarchical sampling algorithm that uses design space hierarchy to group design vectors by active design variables is developed. Then, it is demonstrated that integrating more hierarchy information in the optimization algorithms yields better optimization results for BO algorithms. Several realistic single-objective and multi-objective test problems are used for investigations. Finally, the BO algorithm is applied to a jet engine architecture optimization problem. This work shows that the developed BO algorithm can effectively solve the problem with one order of magnitude less function evaluations than NSGA-II. The algorithms and problems used in this work are implemented in the open-source Python library SBArchOpt. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00854</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00854</id><created>2025-02-02</created><authors><author><keyname>Priem</keyname><forenames>Rémy</forenames></author><author><keyname>Diouane</keyname><forenames>Youssef</forenames></author><author><keyname>Bartoli</keyname><forenames>Nathalie</forenames></author><author><keyname>Dubreuil</keyname><forenames>Sylvain</forenames></author><author><keyname>Saves</keyname><forenames>Paul</forenames></author></authors><title>High-Dimensional Bayesian Optimization Using Both Random and Supervised   Embeddings</title><categories>math.OC cs.LG stat.ML</categories><journal-ref>AIAA Journal 2025 63:1, 162-173</journal-ref><doi>10.2514/1.J063488</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Bayesian optimization (BO) is one of the most powerful strategies to solve computationally expensive-to-evaluate blackbox optimization problems. However, BO methods are conventionally used for optimization problems of small dimension because of the curse of dimensionality. In this paper, a high-dimensionnal optimization method incorporating linear embedding subspaces of small dimension is proposed to efficiently perform the optimization. An adaptive learning strategy for these linear embeddings is carried out in conjunction with the optimization. The resulting BO method, named efficient global optimization coupled with random and supervised embedding (EGORSE), combines in an adaptive way both random and supervised linear embeddings. EGORSE has been compared to state-of-the-art algorithms and tested on academic examples with a number of design variables ranging from 10 to 600. The obtained results show the high potential of EGORSE to solve high-dimensional blackbox optimization problems, in terms of both CPU time and the limited number of calls to the expensive blackbox simulation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02496</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02496</id><created>2025-02-04</created><updated>2025-02-07</updated><authors><author><keyname>Kolb</keyname><forenames>Chris</forenames></author><author><keyname>Weber</keyname><forenames>Tobias</forenames></author><author><keyname>Bischl</keyname><forenames>Bernd</forenames></author><author><keyname>Rügamer</keyname><forenames>David</forenames></author></authors><title>Deep Weight Factorization: Sparse Learning Through the Lens of   Artificial Symmetries</title><categories>cs.LG stat.ML</categories><comments>accepted at ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sparse regularization techniques are well-established in machine learning, yet their application in neural networks remains challenging due to the non-differentiability of penalties like the $L_1$ norm, which is incompatible with stochastic gradient descent. A promising alternative is shallow weight factorization, where weights are decomposed into two factors, allowing for smooth optimization of $L_1$-penalized neural networks by adding differentiable $L_2$ regularization to the factors. In this work, we introduce deep weight factorization, extending previous shallow approaches to more than two factors. We theoretically establish equivalence of our deep factorization with non-convex sparse regularization and analyze its impact on training dynamics and optimization. Due to the limitations posed by standard training practices, we propose a tailored initialization scheme and identify important learning rate requirements necessary for training factorized networks. We demonstrate the effectiveness of our deep weight factorization through experiments on various architectures and datasets, consistently outperforming its shallow counterpart and widely used pruning methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02861</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02861</id><created>2025-02-04</created><updated>2025-02-06</updated><authors><author><keyname>Shen</keyname><forenames>Judy Hanwen</forenames></author><author><keyname>Vitercik</keyname><forenames>Ellen</forenames></author><author><keyname>Wikum</keyname><forenames>Anders</forenames></author></authors><title>Algorithms with Calibrated Machine Learning Predictions</title><categories>stat.ML cs.DS cs.LG</categories><comments>Replacement: updated acknowledgments</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The field of algorithms with predictions incorporates machine learning advice in the design of online algorithms to improve real-world performance. While this theoretical framework often assumes uniform reliability across all predictions, modern machine learning models can now provide instance-level uncertainty estimates. In this paper, we propose calibration as a principled and practical tool to bridge this gap, demonstrating the benefits of calibrated advice through two case studies: the ski rental and online job scheduling problems. For ski rental, we design an algorithm that achieves optimal prediction-dependent performance and prove that, in high-variance settings, calibrated advice offers more effective guidance than alternative methods for uncertainty quantification. For job scheduling, we demonstrate that using a calibrated predictor leads to significant performance improvements over existing methods. Evaluations on real-world data validate our theoretical findings, highlighting the practical impact of calibration for algorithms with predictions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03479</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03479</id><created>2025-01-27</created><updated>2025-02-06</updated><authors><author><keyname>Cuicizion</keyname><forenames>Eliuvish</forenames></author><author><keyname>Ri</keyname><forenames>Itsugo</forenames></author><author><keyname>Holmes</keyname><forenames>Elaine</forenames></author><author><keyname>Lyeutsaon</keyname><forenames>Sho</forenames></author><author><keyname>Chern</keyname><forenames>Jawad Hassan</forenames></author></authors><title>A Tutorial on Markov Renewal and Semi-Markov Proportional Hazards Model</title><categories>stat.AP stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Transition probability estimation plays a critical role in multi-state modeling, especially in clinical research. This paper investigates the application of semi-Markov and Markov renewal frameworks to the EBMT dataset, focusing on six clinical states encountered during hematopoietic stem cell transplantation. By comparing Aalen-Johansen (AJ) and Dabrowska-Sun-Horowitz (DSH) estimators, we demonstrate that semi-Markov models, which incorporate sojourn times, provide a more nuanced and temporally sensitive depiction of patient trajectories compared to memoryless Markov models. The DSH estimator consistently yields smoother probability curves, particularly for transitions involving prolonged states. We use empirical process theory and Burkholder-Davis-Gundy inequality to show weak convergence of the estimator. Future work includes extending the framework to accommodate advanced covariate structures and non-Markovian dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03587</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03587</id><created>2025-02-05</created><updated>2025-02-06</updated><authors><author><keyname>von Seeger</keyname><forenames>Anneke</forenames></author><author><keyname>Zou</keyname><forenames>Dongmian</forenames></author><author><keyname>Lerman</keyname><forenames>Gilad</forenames></author></authors><title>Stein Discrepancy for Unsupervised Domain Adaptation</title><categories>cs.LG stat.ML</categories><comments>24 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Unsupervised domain adaptation (UDA) leverages information from a labeled source dataset to improve accuracy on a related but unlabeled target dataset. A common approach to UDA is aligning representations from the source and target domains by minimizing the distance between their data distributions. Previous methods have employed distances such as Wasserstein distance and maximum mean discrepancy. However, these approaches are less effective when the target data is significantly scarcer than the source data. Stein discrepancy is an asymmetric distance between distributions that relies on one distribution only through its score function. In this paper, we propose a novel UDA method that uses Stein discrepancy to measure the distance between source and target domains. We develop a learning framework using both non-kernelized and kernelized Stein discrepancy. Theoretically, we derive an upper bound for the generalization error. Numerical experiments show that our method outperforms existing methods using other domain discrepancy measures when only small amounts of target data are available. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03942</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03942</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Holst</keyname><forenames>Klaus Kähler</forenames></author><author><keyname>Nordland</keyname><forenames>Andreas</forenames></author><author><keyname>Furkjær</keyname><forenames>Julie</forenames></author><author><keyname>Damgaard</keyname><forenames>Lars Holm</forenames></author><author><keyname>Pipper</keyname><forenames>Christian Bressen</forenames></author></authors><title>A retake on the analysis of scores truncated by terminal events</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of data from randomized controlled trials in vulnerable populations requires special attention when assessing treatment effect by a score measuring, e.g., disease stage or activity together with onset of prevalent terminal events. In reality, it is impossible to disentangle a disease score from the terminal event, since the score is not clinically meaningful after this event. In this work, we propose to assess treatment interventions simultaneously on disease score and the terminal event. Our proposal is based on a natural data-generating mechanism respecting that a disease score does not exist beyond the terminal event. We use modern semi-parametric statistical methods to provide robust and efficient estimation of the risk of terminal event and expected disease score conditional on no terminal event at a pre-specified landmark time. We also use the simultaneous asymptotic behavior of our estimators to develop a powerful closed testing procedure for confirmatory assessment of treatment effect on both onset of terminal event and level of disease score. A simulation study mimicking a large-scale outcome trial in chronic kidney patients as well as an analysis of that trial is provided to assess performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04168</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04168</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Ferradini</keyname><forenames>Carla</forenames></author><author><keyname>Gitton</keyname><forenames>Victor</forenames></author><author><keyname>Vilasini</keyname><forenames>V.</forenames></author></authors><title>Cyclic quantum causal modelling with a graph separation theorem</title><categories>quant-ph math.ST stat.ML stat.TH</categories><comments>41+41 pages. A companion paper by the same authors, focussing on   cyclic classical (functional) causal models has been submitted to the arXiv   concurrently with primary class [math.ST], v2 only differs from v1 in   including the arXiv number of the companion paper. Comments are welcome</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Causal modelling frameworks link observable correlations to causal explanations, which is a crucial aspect of science. These models represent causal relationships through directed graphs, with vertices and edges denoting systems and transformations within a theory. Most studies focus on acyclic causal graphs, where well-defined probability rules and powerful graph-theoretic properties like the d-separation theorem apply. However, understanding complex feedback processes and exotic fundamental scenarios with causal loops requires cyclic causal models, where such results do not generally hold. While progress has been made in classical cyclic causal models, challenges remain in uniquely fixing probability distributions and identifying graph-separation properties applicable in general cyclic models. In cyclic quantum scenarios, existing frameworks have focussed on a subset of possible cyclic causal scenarios, with graph-separation properties yet unexplored. This work proposes a framework applicable to all consistent quantum and classical cyclic causal models on finite-dimensional systems. We address these challenges by introducing a robust probability rule and a novel graph-separation property, p-separation, which we prove to be sound and complete for all such models. Our approach maps cyclic causal models to acyclic ones with post-selection, leveraging the post-selected quantum teleportation protocol. We characterize these protocols and their success probabilities along the way. We also establish connections between this formalism and other classical and quantum frameworks to inform a more unified perspective on causality. This provides a foundation for more general cyclic causal discovery algorithms and to systematically extend open problems and techniques from acyclic informational networks (e.g., certification of non-classicality) to cyclic causal structures and networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04171</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04171</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Ferradini</keyname><forenames>Carla</forenames></author><author><keyname>Gitton</keyname><forenames>Victor</forenames></author><author><keyname>Vilasini</keyname><forenames>V.</forenames></author></authors><title>Cyclic functional causal models beyond unique solvability with a graph   separation theorem</title><categories>math.ST quant-ph stat.ML stat.TH</categories><comments>33+16 pages. A companion paper by the same authors, focussing on   cyclic quantum causal models has been submitted to the arXiv concurrently   with primary class [quant-ph], v2 only differs from v1 in including the arXiv   number of the companion paper. Comments are welcome</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Functional causal models (fCMs) specify functional dependencies between random variables associated to the vertices of a graph. In directed acyclic graphs (DAGs), fCMs are well-understood: a unique probability distribution on the random variables can be easily specified, and a crucial graph-separation result called the d-separation theorem allows one to characterize conditional independences between the variables. However, fCMs on cyclic graphs pose challenges due to the absence of a systematic way to assign a unique probability distribution to the fCM's variables, the failure of the d-separation theorem, and lack of a generalization of this theorem that is applicable to all consistent cyclic fCMs. In this work, we develop a causal modeling framework applicable to all cyclic fCMs involving finite-cardinality variables, except inconsistent ones admitting no solutions. Our probability rule assigns a unique distribution even to non-uniquely solvable cyclic fCMs and reduces to the known rule for uniquely solvable fCMs. We identify a class of fCMs, called averagely uniquely solvable, that we show to be the largest class where the probabilities admit a Markov factorization. Furthermore, we introduce a new graph-separation property, p-separation, and prove this to be sound and complete for all consistent finite-cardinality cyclic fCMs while recovering the d-separation theorem for DAGs. These results are obtained by considering classical post-selected teleportation protocols inspired by analogous protocols in quantum information theory. We discuss further avenues for exploration, linking in particular problems in cyclic fCMs and in quantum causality. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04208</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04208</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Grünwald</keyname><forenames>Peter D.</forenames></author><author><keyname>Koolen</keyname><forenames>Wouter M.</forenames></author></authors><title>Supermartingales for One-Sided Tests: Sufficient Monotone Likelihood   Ratios are Sufficient</title><categories>math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The t-statistic is a widely-used scale-invariant statistic for testing the null hypothesis that the mean is zero. Martingale methods enable sequential testing with the t-statistic at every sample size, while controlling the probability of falsely rejecting the null. For one-sided sequential tests, which reject when the t-statistic is too positive, a natural question is whether they also control false rejection when the true mean is negative. We prove that this is the case using monotone likelihood ratios and sufficient statistics. We develop applications to the scale-invariant t-test, the location-invariant $\chi^2$-test and sequential linear regression with nuisance covariates. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04339</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04339</id><created>2025-02-01</created><authors><author><keyname>George</keyname><forenames>Anand Jerry</forenames></author><author><keyname>Veiga</keyname><forenames>Rodrigo</forenames></author><author><keyname>Macris</keyname><forenames>Nicolas</forenames></author></authors><title>Analysis of Diffusion Models for Manifold Data</title><categories>math.ST cond-mat.dis-nn cs.IT cs.LG math.IT math.PR stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We analyze the time reversed dynamics of generative diffusion models. If the exact empirical score function is used in a regime of large dimension and exponentially large number of samples, these models are known to undergo transitions between distinct dynamical regimes. We extend this analysis and compute the transitions for an analytically tractable manifold model where the statistical model for the data is a mixture of lower dimensional Gaussians embedded in higher dimensional space. We compute the so-called speciation and collapse transition times, as a function of the ratio of manifold-to-ambient space dimensions, and other characteristics of the data model. An important tool used in our analysis is the exact formula for the mutual information (or free energy) of Generalized Linear Models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04372</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04372</id><created>2025-02-05</created><authors><author><keyname>Genari</keyname><forenames>Juliano</forenames></author><author><keyname>Goedert</keyname><forenames>Guilherme Tegoni</forenames></author></authors><title>Mining Unstructured Medical Texts With Conformal Active Learning</title><categories>cs.CL cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The extraction of relevant data from Electronic Health Records (EHRs) is crucial to identifying symptoms and automating epidemiological surveillance processes. By harnessing the vast amount of unstructured text in EHRs, we can detect patterns that indicate the onset of disease outbreaks, enabling faster, more targeted public health responses. Our proposed framework provides a flexible and efficient solution for mining data from unstructured texts, significantly reducing the need for extensive manual labeling by specialists. Experiments show that our framework achieving strong performance with as few as 200 manually labeled texts, even for complex classification problems. Additionally, our approach can function with simple lightweight models, achieving competitive and occasionally even better results compared to more resource-intensive deep learning models. This capability not only accelerates processing times but also preserves patient privacy, as the data can be processed on weaker on-site hardware rather than being transferred to external systems. Our methodology, therefore, offers a practical, scalable, and privacy-conscious approach to real-time epidemiological monitoring, equipping health institutions to respond rapidly and effectively to emerging health threats. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04422</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04422</id><created>2025-02-06</created><authors><author><keyname>Yadav</keyname><forenames>Pooja</forenames></author><author><keyname>Srivastava</keyname><forenames>Tanuja</forenames></author></authors><title>The Maximum Likelihood Degree of Farlie Gumbel Morgenstern Bivariate   Exponential Distribution</title><categories>math.ST math.AC stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum likelihood degree of a statistical model refers to the number of solutions, where the derivative of the log-likelihood function is zero, over the complex field. This paper examines the maximum likelihood degree of the parameter in Farlie-Gumbel-Morgenstern bivariate exponential distribution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04464</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04464</id><created>2025-02-06</created><authors><author><keyname>Jadoul</keyname><forenames>Yannick</forenames></author><author><keyname>Tufarelli</keyname><forenames>Tommaso</forenames></author><author><keyname>Coissac</keyname><forenames>Chloé</forenames></author><author><keyname>Gamba</keyname><forenames>Marco</forenames></author><author><keyname>Ravignani</keyname><forenames>Andrea</forenames></author></authors><title>Hidden assumptions of integer ratio analyses in bioacoustics and music</title><categories>stat.AP</categories><comments>19 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Rhythm is ubiquitous in human culture and in nature, but hard to capture in all its complexity. A key dimension of rhythm, integer ratio categories occur when the relationship between temporal intervals can be expressed as small-integer ratios. Recent work has found integer ratio categories in most human musical cultures and some animal species' vocalizations or behavioral displays. But biological systems are noisy, and empirically measured intervals rarely form an exact small-integer ratio. Here, we mathematically assess whether the leading integer ratio analysis method makes valid statistical and biological assumptions. In particular, we (1) make the temporal properties of empirical ratios explicit, both in general and for the typical use in the literature; (2) show how the choice of ratio formula affects the probability distribution of rhythm ratios and ensuing statistical results; (3) guide the reader to carefully consider the assumptions and null hypotheses of the statistical analysis; (4) present a comprehensive methodology to statistically test integer ratios for any null hypothesis of choice. Our observations have implications for both past and future research in music cognition and animal behavior: They suggest how to interpret past findings and provide tools to choose the correct null hypotheses in future empirical work. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04491</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04491</id><created>2025-02-06</created><authors><author><keyname>Cheng</keyname><forenames>Ziheng</forenames></author><author><keyname>Xie</keyname><forenames>Tianyu</forenames></author><author><keyname>Zhang</keyname><forenames>Shiyue</forenames></author><author><keyname>Zhang</keyname><forenames>Cheng</forenames></author></authors><title>Provable Sample-Efficient Transfer Learning Conditional Diffusion Models   via Representation Learning</title><categories>cs.LG math.ST stat.ML stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While conditional diffusion models have achieved remarkable success in various applications, they require abundant data to train from scratch, which is often infeasible in practice. To address this issue, transfer learning has emerged as an essential paradigm in small data regimes. Despite its empirical success, the theoretical underpinnings of transfer learning conditional diffusion models remain unexplored. In this paper, we take the first step towards understanding the sample efficiency of transfer learning conditional diffusion models through the lens of representation learning. Inspired by practical training procedures, we assume that there exists a low-dimensional representation of conditions shared across all tasks. Our analysis shows that with a well-learned representation from source tasks, the samplecomplexity of target tasks can be reduced substantially. In addition, we investigate the practical implications of our theoretical results in several real-world applications of conditional diffusion models. Numerical experiments are also conducted to verify our results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04543</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04543</id><created>2025-02-06</created><authors><author><keyname>Lu</keyname><forenames>Zhou</forenames></author><author><keyname>Sun</keyname><forenames>Y. Jennifer</forenames></author><author><keyname>Zhang</keyname><forenames>Zhiyu</forenames></author></authors><title>Sparsity-Based Interpolation of External, Internal and Swap Regret</title><categories>stat.ML cs.LG</categories><comments>Equal contribution, alphabetical order</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Focusing on the expert problem in online learning, this paper studies the interpolation of several performance metrics via $\phi$-regret minimization, which measures the performance of an algorithm by its regret with respect to an arbitrary action modification rule $\phi$. With $d$ experts and $T\gg d$ rounds in total, we present a single algorithm achieving the instance-adaptive $\phi$-regret bound \begin{equation*} \tilde O\left(\min\left\{\sqrt{d-d^{\mathrm{unif}}_\phi+1},\sqrt{d-d^{\mathrm{self}}_\phi}\right\}\cdot\sqrt{T}\right), \end{equation*} where $d^{\mathrm{unif}}_\phi$ is the maximum amount of experts modified identically by $\phi$, and $d^{\mathrm{self}}_\phi$ is the amount of experts that $\phi$ trivially modifies to themselves. By recovering the optimal $O(\sqrt{T\log d})$ external regret bound when $d^{\mathrm{unif}}_\phi=d$, the standard $\tilde O(\sqrt{T})$ internal regret bound when $d^{\mathrm{self}}_\phi=d-1$ and the optimal $\tilde O(\sqrt{dT})$ swap regret bound in the worst case, we improve existing results in the intermediate regimes. In addition, the same algorithm achieves the optimal quantile regret bound, which corresponds to even easier settings of $\phi$ than the external regret.   Building on the classical reduction from $\phi$-regret minimization to external regret minimization on stochastic matrices, our main idea is to further convert the latter to online linear regression using Haar-wavelet-inspired matrix features. Then, we apply a particular $L_1$-version of comparator-adaptive online learning algorithms to exploit the sparsity in this regression subroutine. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04550</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04550</id><created>2025-02-06</created><authors><author><keyname>Faes</keyname><forenames>Luca</forenames></author><author><keyname>Sparacino</keyname><forenames>Laura</forenames></author><author><keyname>Mijatovic</keyname><forenames>Gorana</forenames></author><author><keyname>Antonacci</keyname><forenames>Yuri</forenames></author><author><keyname>Ricci</keyname><forenames>Leonardo</forenames></author><author><keyname>Marinazzo</keyname><forenames>Daniele</forenames></author><author><keyname>Stramaglia</keyname><forenames>Sebastiano</forenames></author></authors><title>Partial Information Rate Decomposition</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Partial Information Decomposition (PID) is a principled and flexible method to unveil complex high-order interactions in multi-unit network systems. Though being defined exclusively for random variables, PID is ubiquitously applied to multivariate time series taken as realizations of random processes with temporal statistical structure. Here, to overcome the incorrect depiction of high-order effects by PID schemes applied to dynamic networks, we introduce the framework of Partial Information Rate Decomposition (PIRD). PIRD is formalized applying lattice theory to decompose the information shared dynamically between a target random process and a set of source processes, implemented for Gaussian processes through a spectral expansion of information rates, and demonstrated in practice analyzing time series from large-scale climate oscillations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04553</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04553</id><created>2025-02-06</created><authors><author><keyname>Swanson</keyname><forenames>David</forenames></author><author><keyname>Sherry</keyname><forenames>Alexander</forenames></author><author><keyname>Tang</keyname><forenames>Chad</forenames></author></authors><title>Variance component mixture modelling for longitudinal T-cell receptor   clonal dynamics</title><categories>stat.ME</categories><msc-class>62F15 (Primary), 62H30 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Studies of T cells and their clonally unique receptors have shown promise in elucidating the association between immune response and human disease. Methods to identify T-cell receptor clones which expand or contract in response to certain therapeutic strategies have so far been limited to longitudinal pairwise comparisons of clone frequency with multiplicity adjustment. Here we develop a more general mixture model approach for arbitrary follow-up and missingness which partitions dynamic longitudinal clone frequency behavior from static. While it is common to mix on the location or scale parameter of a family of distributions, the model instead mixes on the parameterization itself, the dynamic component allowing for a variable, Gamma-distributed Poisson mean parameter over longitudinal follow-up, while the static component mean is time invariant. Leveraging conjugacy, one can integrate out the mean parameter for the dynamic and static components to yield distinct posterior predictive distributions whose expressions are a product of negative binomials and a single negative multinomial, respectively, each modified according to an offset for receptor read count normalization. An EM-algorithm is developed to estimate hyperparameters and component membership, and validity of the approach is demonstrated in simulation. The model identifies a statistically significant and clinically relevant increase in TCR clonal dynamism among metastasis-directed radiation therapy in a cohort of prostate cancer patients. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04555</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04555</id><created>2025-02-06</created><authors><author><keyname>Sparacino</keyname><forenames>Laura</forenames></author><author><keyname>Mijatovic</keyname><forenames>Gorana</forenames></author><author><keyname>Antonacci</keyname><forenames>Yuri</forenames></author><author><keyname>Ricci</keyname><forenames>Leonardo</forenames></author><author><keyname>Marinazzo</keyname><forenames>Daniele</forenames></author><author><keyname>Stramaglia</keyname><forenames>Sebastiano</forenames></author><author><keyname>Faes</keyname><forenames>Luca</forenames></author></authors><title>Decomposing Multivariate Information Rates in Networks of Random   Processes</title><categories>stat.ME cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The Partial Information Decomposition (PID) framework has emerged as a powerful tool for analyzing high-order interdependencies in complex network systems. However, its application to dynamic processes remains challenging due to the implicit assumption of memorylessness, which often falls in real-world scenarios. In this work, we introduce the framework of Partial Information Rate Decomposition (PIRD) that extends PID to random processes with temporal correlations. By leveraging mutual information rate (MIR) instead of mutual information (MI), our approach decomposes the dynamic information shared by multivariate random processes into unique, redundant, and synergistic contributions obtained aggregating information rate atoms in a principled manner. To solve PIRD, we define a pointwise redundancy rate function based on the minimum MI principle applied locally in the frequency-domain representation of the processes. The framework is validated in benchmark simulations of Gaussian systems, demonstrating its advantages over traditional PID in capturing temporal correlations and showing how the spectral representation may reveal scale-specific higher-order interaction that are obscured in the time domain. Furthermore, we apply PIRD to a physiological network comprising cerebrovascular and cardiovascular variables, revealing frequency-dependent redundant information exchange during a protocol of postural stress. Our results highlight the necessity of accounting for the full temporal statistical structure and spectral content of vector random processes to meaningfully perform information decomposition in network systems with dynamic behavior such as those typically encountered in neuroscience and physiology. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04574</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04574</id><created>2025-02-06</created><authors><author><keyname>Gong</keyname><forenames>ZhuQing</forenames></author><author><keyname>Zuo</keyname><forenames>XiNian</forenames></author></authors><title>Dark Brain Energy: Toward an Integrative Model of Spontaneous Slow   Oscillations</title><categories>q-bio.NC cs.IT math.IT stat.AP</categories><comments>38 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Neural oscillations facilitate the functioning of the human brain in spatial and temporal dimensions at various frequencies. These oscillations feature a universal frequency architecture that is governed by brain anatomy, ensuring frequency specificity remains invariant across different measurement techniques. Initial magnetic resonance imaging (MRI) methodology constrained functional MRI (fMRI) investigations to a singular frequency range, thereby neglecting the frequency characteristics inherent in blood oxygen level-dependent oscillations. With advancements in MRI technology, it has become feasible to decode intricate brain activities via multi-band frequency analysis (MBFA). During the past decade, the utilization of MBFA in fMRI studies has surged, unveiling frequency-dependent characteristics of spontaneous slow oscillations (SSOs) believed to base dark energy in the brain. There remains a dearth of conclusive insights and hypotheses pertaining to the properties and functionalities of SSOs in distinct bands. We surveyed the SSO MBFA studies during the past 15 years to delineate the attributes of SSOs and enlighten their correlated functions. We further proposed a model to elucidate the hierarchical organization of multi-band SSOs by integrating their function, aimed at bridging theoretical gaps and guiding future MBFA research endeavors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04575</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04575</id><created>2025-02-06</created><authors><author><keyname>Guo</keyname><forenames>Wei</forenames></author><author><keyname>Tao</keyname><forenames>Molei</forenames></author><author><keyname>Chen</keyname><forenames>Yongxin</forenames></author></authors><title>Complexity Analysis of Normalizing Constant Estimation: from Jarzynski   Equality to Annealed Importance Sampling and beyond</title><categories>stat.ML cs.LG cs.NA math.NA physics.comp-ph stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given an unnormalized probability density $\pi\propto\mathrm{e}^{-V}$, estimating its normalizing constant $Z=\int_{\mathbb{R}^d}\mathrm{e}^{-V(x)}\mathrm{d}x$ or free energy $F=-\log Z$ is a crucial problem in Bayesian statistics, statistical mechanics, and machine learning. It is challenging especially in high dimensions or when $\pi$ is multimodal. To mitigate the high variance of conventional importance sampling estimators, annealing-based methods such as Jarzynski equality and annealed importance sampling are commonly adopted, yet their quantitative complexity guarantees remain largely unexplored. We take a first step toward a non-asymptotic analysis of annealed importance sampling. In particular, we derive an oracle complexity of $\widetilde{O}\left(\frac{d\beta^2{\mathcal{A}}^2}{\varepsilon^4}\right)$ for estimating $Z$ within $\varepsilon$ relative error with high probability, where $\beta$ is the smoothness of $V$ and $\mathcal{A}$ denotes the action of a curve of probability measures interpolating $\pi$ and a tractable reference distribution. Our analysis, leveraging Girsanov theorem and optimal transport, does not explicitly require isoperimetric assumptions on the target distribution. Finally, to tackle the large action of the widely used geometric interpolation of probability distributions, we propose a new normalizing constant estimation algorithm based on reverse diffusion samplers and establish a framework for analyzing its complexity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04591</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04591</id><created>2025-02-06</created><authors><author><keyname>Deidda</keyname><forenames>Piero</forenames></author><author><keyname>Zhang</keyname><forenames>Kaicheng</forenames></author><author><keyname>Higham</keyname><forenames>Desmond</forenames></author><author><keyname>Tudisco</keyname><forenames>Francesco</forenames></author></authors><title>Rethinking Oversmoothing in Graph Neural Networks: A Rank-Based   Perspective</title><categories>cs.LG cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Oversmoothing is a fundamental challenge in graph neural networks (GNNs): as the number of layers increases, node embeddings become increasingly similar, and model performance drops sharply. Traditionally, oversmoothing has been quantified using metrics that measure the similarity of neighbouring node features, such as the Dirichlet energy. While these metrics are related to oversmoothing, we argue they have critical limitations and fail to reliably capture oversmoothing in realistic scenarios. For instance, they provide meaningful insights only for very deep networks and under somewhat strict conditions on the norm of network weights and feature representations. As an alternative, we propose measuring oversmoothing by examining the numerical or effective rank of the feature representations. We provide theoretical support for this approach, demonstrating that the numerical rank of feature representations converges to one for a broad family of nonlinear activation functions under the assumption of nonnegative trained weights. To the best of our knowledge, this is the first result that proves the occurrence of oversmoothing without assumptions on the boundedness of the weight matrices. Along with the theoretical findings, we provide extensive numerical evaluation across diverse graph architectures. Our results show that rank-based metrics consistently capture oversmoothing, whereas energy-based metrics often fail. Notably, we reveal that a significant drop in the rank aligns closely with performance degradation, even in scenarios where energy metrics remain unchanged. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04593</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04593</id><created>2025-02-06</created><authors><author><keyname>Rezaei</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Dieng</keyname><forenames>Adji Bousso</forenames></author></authors><title>The $\alpha$-Alternator: Dynamic Adaptation To Varying Noise Levels In   Sequences Using The Vendi Score For Improved Robustness and Performance</title><categories>cs.LG cs.AI cs.NE stat.ML</categories><comments>The codebase will be made available upon publication. This paper is   dedicated to Patrice Lumumba</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Current state-of-the-art dynamical models, such as Mamba, assume the same level of noisiness for all elements of a given sequence, which limits their performance on noisy temporal data. In this paper, we introduce the $\alpha$-Alternator, a novel generative model for time-dependent data that dynamically adapts to the complexity introduced by varying noise levels in sequences. The $\alpha$-Alternator leverages the Vendi Score (VS), a flexible similarity-based diversity metric, to adjust, at each time step $t$, the influence of the sequence element at time $t$ and the latent representation of the dynamics up to that time step on the predicted future dynamics. This influence is captured by a parameter that is learned and shared across all sequences in a given dataset. The sign of this parameter determines the direction of influence. A negative value indicates a noisy dataset, where a sequence element that increases the VS is considered noisy, and the model relies more on the latent history when processing that element. Conversely, when the parameter is positive, a sequence element that increases the VS is considered informative, and the $\alpha$-Alternator relies more on this new input than on the latent history when updating its predicted latent dynamics. The $\alpha$-Alternator is trained using a combination of observation masking and Alternator loss minimization. Masking simulates varying noise levels in sequences, enabling the model to be more robust to these fluctuations and improving its performance in trajectory prediction, imputation, and forecasting. Our experimental results demonstrate that the $\alpha$-Alternator outperforms both Alternators and state-of-the-art state-space models across neural decoding and time-series forecasting benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04654</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04654</id><created>2025-02-06</created><authors><author><keyname>Lim</keyname><forenames>Keunwoo</forenames></author><author><keyname>Ye</keyname><forenames>Ting</forenames></author><author><keyname>Han</keyname><forenames>Fang</forenames></author></authors><title>A sliced Wasserstein and diffusion approach to random coefficient models</title><categories>math.ST econ.EM stat.TH</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new minimum-distance estimator for linear random coefficient models. This estimator integrates the recently advanced sliced Wasserstein distance with the nearest neighbor methods, both of which enhance computational efficiency. We demonstrate that the proposed method is consistent in approximating the true distribution. Additionally, our formulation encourages a diffusion process-based algorithm, which holds independent interest and potential for broader applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04673</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04673</id><created>2025-02-07</created><authors><author><keyname>Neopane</keyname><forenames>Ojash</forenames></author><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author></authors><title>Optimistic Algorithms for Adaptive Estimation of the Average Treatment   Effect</title><categories>stat.ML cs.LG stat.ME</categories><comments>15 pages, 2 Figures</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Estimation and inference for the Average Treatment Effect (ATE) is a cornerstone of causal inference and often serves as the foundation for developing procedures for more complicated settings. Although traditionally analyzed in a batch setting, recent advances in martingale theory have paved the way for adaptive methods that can enhance the power of downstream inference. Despite these advances, progress in understanding and developing adaptive algorithms remains in its early stages. Existing work either focus on asymptotic analyses that overlook exploration-exploitation tradeoffs relevant in finite-sample regimes or rely on simpler but suboptimal estimators. In this work, we address these limitations by studying adaptive sampling procedures that take advantage of the asymptotically optimal Augmented Inverse Probability Weighting (AIPW) estimator. Our analysis uncovers challenges obscured by asymptotic approaches and introduces a novel algorithmic design principle reminiscent of optimism in multiarmed bandits. This principled approach enables our algorithm to achieve significant theoretical and empirical gains compared to prior methods. Our findings mark a step forward in advancing adaptive causal inference methods in theory and practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04681</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04681</id><created>2025-02-07</created><authors><author><keyname>Louit</keyname><forenames>Sydney</forenames></author><author><keyname>Clark</keyname><forenames>Evan</forenames></author><author><keyname>Gelbard</keyname><forenames>Alexander</forenames></author><author><keyname>Vivek</keyname><forenames>Niketna</forenames></author><author><keyname>Yan</keyname><forenames>Jun</forenames></author><author><keyname>Zhang</keyname><forenames>Panpan</forenames></author></authors><title>CALF-SBM: A Covariate-Assisted Latent Factor Stochastic Block Model</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel network generative model extended from the standard stochastic block model by concurrently utilizing observed node-level information and accounting for network-enabled nodal heterogeneity. The proposed model is so so-called covariate-assisted latent factor stochastic block model (CALF-SBM). The inference for the proposed model is done in a fully Bayesian framework. The primary application of CALF-SBM in the present research is focused on community detection, where a model-selection-based approach is employed to estimate the number of communities which is practically assumed unknown. To assess the performance of CALF-SBM, an extensive simulation study is carried out, including comparisons with multiple classical and modern network clustering algorithms. Lastly, the paper presents two real data applications, respectively based on an extremely new network data demonstrating collaborative relationships of otolaryngologists in the United States and a traditional aviation network data containing information about direct flights between airports in the United States and Canada. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04685</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04685</id><created>2025-02-07</created><authors><author><keyname>Zhang</keyname><forenames>Likun</forenames></author><author><keyname>Bhaganagar</keyname><forenames>Kiran</forenames></author><author><keyname>Wikle</keyname><forenames>Christopher K.</forenames></author></authors><title>Capturing Extreme Events in Turbulence using an Extreme Variational   Autoencoder (xVAE)</title><categories>physics.flu-dyn stat.AP stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Turbulent flow fields are characterized by extreme events that are statistically intermittent and carry a significant amount of energy and physical importance. To emulate these flows, we introduce the extreme variational Autoencoder (xVAE), which embeds a max-infinitely divisible process with heavy-tailed distributions into a standard VAE framework, enabling accurate modeling of extreme events. xVAEs are neural network models that reduce system dimensionality by learning non-linear latent representations of data. We demonstrate the effectiveness of xVAE in large-eddy simulation data of wildland fire plumes, where intense heat release and complex plume-atmosphere interactions generate extreme turbulence. Comparisons with the commonly used Proper Orthogonal Decomposition (POD) modes show that xVAE is more robust in capturing extreme values and provides a powerful uncertainty quantification framework using variational Bayes. Additionally, xVAE enables analysis of the so-called copulas of fields to assess risks associated with rare events while rigorously accounting for uncertainty, such as simultaneous exceedances of high thresholds across multiple locations. The proposed approach provides a new direction for studying realistic turbulent flows, such as high-speed aerodynamics, space propulsion, and atmospheric and oceanic systems that are characterized by extreme events. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04699</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04699</id><created>2025-02-07</created><authors><author><keyname>Lan</keyname><forenames>Hui</forenames></author><author><keyname>Chang</keyname><forenames>Haoge</forenames></author><author><keyname>Dillon</keyname><forenames>Eleanor</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author></authors><title>A Meta-learner for Heterogeneous Effects in Difference-in-Differences</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We address the problem of estimating heterogeneous treatment effects in panel data, adopting the popular Difference-in-Differences (DiD) framework under the conditional parallel trends assumption. We propose a novel doubly robust meta-learner for the Conditional Average Treatment Effect on the Treated (CATT), reducing the estimation to a convex risk minimization problem involving a set of auxiliary models. Our framework allows for the flexible estimation of the CATT, when conditioning on any subset of variables of interest using generic machine learning. Leveraging Neyman orthogonality, our proposed approach is robust to estimation errors in the auxiliary models. As a generalization to our main result, we develop a meta-learning approach for the estimation of general conditional functionals under covariate shift. We also provide an extension to the instrumented DiD setting with non-compliance. Empirical results demonstrate the superiority of our approach over existing baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04709</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04709</id><created>2025-02-07</created><authors><author><keyname>Miftachov</keyname><forenames>Ratmir</forenames></author><author><keyname>Reiß</keyname><forenames>Markus</forenames></author></authors><title>Early Stopping for Regression Trees</title><categories>math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We develop early stopping rules for growing regression tree estimators. The fully data-driven stopping rule is based on monitoring the global residual norm. The best-first search and the breadth-first search algorithms together with linear interpolation give rise to generalized projection or regularization flows. A general theory of early stopping is established. Oracle inequalities for the early-stopped regression tree are derived without any smoothness assumption on the regression function, assuming the original CART splitting rule, yet with a much broader scope. The remainder terms are of smaller order than the best achievable rates for Lipschitz functions in dimension $d\ge 2$. In real and synthetic data the early stopping regression tree estimators attain the statistical performance of cost-complexity pruning while significantly reducing computational costs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04723</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04723</id><created>2025-02-07</created><authors><author><keyname>Lyu</keyname><forenames>Ziyang</forenames></author><author><keyname>Sisson</keyname><forenames>S. A.</forenames></author><author><keyname>Welsh</keyname><forenames>A. H.</forenames></author></authors><title>Asymptotics for EBLUPs within crossed mixed effect models</title><categories>stat.ME</categories><comments>33 pages</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In this article, we derive the joint asymptotic distribution of empirical best linear unbiased predictors (EBLUPs) for individual and cell-level random effects in a crossed mixed effect model. Under mild conditions (which include moment conditions instead of normality for the random effects and model errors), we demonstrate that as the sizes of rows, columns, and, when we include interactions, cells simultaneously increase to infinity, the distribution of the differences between the EBLUPs and the random effects satisfy central limit theorems. These central limit theorems mean the EBLUPs asymptotically follow the convolution of the true random effect distribution and a normal distribution. Moreover, our results enable simple asymptotic approximations and estimators for the mean squared error (MSE) of the EBLUPs, which in turn facilitates the construction of asymptotic prediction intervals for the unobserved random effects. We show in simulations that our simple estimator of the MSE of the EBLUPs works very well in finite samples. Finally, we illustrate the use of the asymptotic prediction intervals with an analysis of movie rating data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04730</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04730</id><created>2025-02-07</created><authors><author><keyname>Xie</keyname><forenames>Tianyu</forenames></author><author><keyname>Richman</keyname><forenames>Harry</forenames></author><author><keyname>Gao</keyname><forenames>Jiansi</forenames></author><author><keyname>Matsen</keyname><forenames>Frederick A.</forenames><suffix>IV</suffix></author><author><keyname>Zhang</keyname><forenames>Cheng</forenames></author></authors><title>PhyloVAE: Unsupervised Learning of Phylogenetic Trees via Variational   Autoencoders</title><categories>stat.ML cs.LG q-bio.PE</categories><comments>ICLR 2025. 22 pages, 14 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Learning informative representations of phylogenetic tree structures is essential for analyzing evolutionary relationships. Classical distance-based methods have been widely used to project phylogenetic trees into Euclidean space, but they are often sensitive to the choice of distance metric and may lack sufficient resolution. In this paper, we introduce phylogenetic variational autoencoders (PhyloVAEs), an unsupervised learning framework designed for representation learning and generative modeling of tree topologies. Leveraging an efficient encoding mechanism inspired by autoregressive tree topology generation, we develop a deep latent-variable generative model that facilitates fast, parallelized topology generation. PhyloVAE combines this generative model with a collaborative inference model based on learnable topological features, allowing for high-resolution representations of phylogenetic tree samples. Extensive experiments demonstrate PhyloVAE's robust representation learning capabilities and fast generation of phylogenetic tree topologies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04750</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04750</id><created>2025-02-07</created><authors><author><keyname>Bui</keyname><forenames>Thang D.</forenames></author><author><keyname>Ashman</keyname><forenames>Matthew</forenames></author><author><keyname>Turner</keyname><forenames>Richard E.</forenames></author></authors><title>Tighter sparse variational Gaussian processes</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sparse variational Gaussian process (GP) approximations based on inducing points have become the de facto standard for scaling GPs to large datasets, owing to their theoretical elegance, computational efficiency, and ease of implementation. This paper introduces a provably tighter variational approximation by relaxing the standard assumption that the conditional approximate posterior given the inducing points must match that in the prior. The key innovation is to modify the conditional posterior to have smaller variances than that of the prior at the training points. We derive the collapsed bound for the regression case, describe how to use the proposed approximation in large data settings, and discuss its application to handle orthogonally structured inducing points and GP latent variable models. Extensive experiments on regression benchmarks, classification, and latent variable models demonstrate that the proposed approximation consistently matches or outperforms standard sparse variational GPs while maintaining the same computational cost. An implementation will be made available in all popular GP packages. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04793</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04793</id><created>2025-02-07</created><authors><author><keyname>Jeunen</keyname><forenames>Olivier</forenames></author></authors><title>$t$-Testing the Waters: Empirically Validating Assumptions for Reliable   A/B-Testing</title><categories>stat.ME cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A/B-tests are a cornerstone of experimental design on the web, with wide-ranging applications and use-cases. The statistical $t$-test comparing differences in means is the most commonly used method for assessing treatment effects, often justified through the Central Limit Theorem (CLT). The CLT ascertains that, as the sample size grows, the sampling distribution of the Average Treatment Effect converges to normality, making the $t$-test valid for sufficiently large sample sizes. When outcome measures are skewed or non-normal, quantifying what "sufficiently large" entails is not straightforward.   To ensure that confidence intervals maintain proper coverage and that $p$-values accurately reflect the false positive rate, it is critical to validate this normality assumption. We propose a practical method to test this, by analysing repeatedly resampled A/A-tests. When the normality assumption holds, the resulting $p$-value distribution should be uniform, and this property can be tested using the Kolmogorov-Smirnov test. This provides an efficient and effective way to empirically assess whether the $t$-test's assumptions are met, and the A/B-test is valid. We demonstrate our methodology and highlight how it helps to identify scenarios prone to inflated Type-I errors. Our approach provides a practical framework to ensure and improve the reliability and robustness of A/B-testing practices. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04807</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04807</id><created>2025-02-07</created><authors><author><keyname>Bashari</keyname><forenames>Meshi</forenames></author><author><keyname>Sesia</keyname><forenames>Matteo</forenames></author><author><keyname>Romano</keyname><forenames>Yaniv</forenames></author></authors><title>Robust Conformal Outlier Detection under Contaminated Reference Data</title><categories>stat.ML cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conformal prediction is a flexible framework for calibrating machine learning predictions, providing distribution-free statistical guarantees. In outlier detection, this calibration relies on a reference set of labeled inlier data to control the type-I error rate. However, obtaining a perfectly labeled inlier reference set is often unrealistic, and a more practical scenario involves access to a contaminated reference set containing a small fraction of outliers. This paper analyzes the impact of such contamination on the validity of conformal methods. We prove that under realistic, non-adversarial settings, calibration on contaminated data yields conservative type-I error control, shedding light on the inherent robustness of conformal methods. This conservativeness, however, typically results in a loss of power. To alleviate this limitation, we propose a novel, active data-cleaning framework that leverages a limited labeling budget and an outlier detection model to selectively annotate data points in the contaminated reference set that are suspected as outliers. By removing only the annotated outliers in this ``suspicious'' subset, we can effectively enhance power while mitigating the risk of inflating the type-I error rate, as supported by our theoretical analysis. Experiments on real datasets validate the conservative behavior of conformal methods under contamination and show that the proposed data-cleaning strategy improves power without sacrificing validity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04824</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04824</id><created>2025-02-07</created><authors><author><keyname>Blake</keyname><forenames>Joshua</forenames></author><author><keyname>Birrell</keyname><forenames>Paul</forenames></author><author><keyname>Walker</keyname><forenames>A. Sarah</forenames></author><author><keyname>Pouwels</keyname><forenames>Koen B.</forenames></author><author><keyname>House</keyname><forenames>Thomas</forenames></author><author><keyname>Tom</keyname><forenames>Brian D. M.</forenames></author><author><keyname>Kypraios</keyname><forenames>Theodore</forenames></author><author><keyname>De Angelis</keyname><forenames>Daniela</forenames></author></authors><title>Estimating the duration of RT-PCR positivity for SARS-CoV-2 from doubly   interval censored data with undetected infections</title><categories>stat.ME stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Monitoring the incidence of new infections during a pandemic is critical for an effective public health response. General population prevalence surveys for SARS-CoV-2 can provide high-quality data to estimate incidence. However, estimation relies on understanding the distribution of the duration that infections remain detectable. This study addresses this need using data from the Coronavirus Infection Survey (CIS), a long-term, longitudinal, general population survey conducted in the UK. Analyzing these data presents unique challenges, such as doubly interval censoring, undetected infections, and false negatives. We propose a Bayesian nonparametric survival analysis approach, estimating a discrete-time distribution of durations and integrating prior information derived from a complementary study. Our methodology is validated through a simulation study, including its resilience to model misspecification, and then applied to the CIS dataset. This results in the first estimate of the full duration distribution in a general population, as well as methodology that could be transferred to new contexts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04832</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04832</id><created>2025-02-07</created><authors><author><keyname>Ballarin</keyname><forenames>Giovanni</forenames></author><author><keyname>Grigoryeva</keyname><forenames>Lyudmila</forenames></author><author><keyname>Ortega</keyname><forenames>Juan-Pablo</forenames></author></authors><title>Memory Capacity of Nonlinear Recurrent Networks: Is it Informative?</title><categories>cs.LG stat.ML</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The total memory capacity (MC) of linear recurrent neural networks (RNNs) has been proven to be equal to the rank of the corresponding Kalman controllability matrix, and it is almost surely maximal for connectivity and input weight matrices drawn from regular distributions. This fact questions the usefulness of this metric in distinguishing the performance of linear RNNs in the processing of stochastic signals. This note shows that the MC of random nonlinear RNNs yields arbitrary values within established upper and lower bounds depending just on the input process scale. This confirms that the existing definition of MC in linear and nonlinear cases has no practical value. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04849</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04849</id><created>2025-02-07</created><authors><author><keyname>Yu</keyname><forenames>Yifeng</forenames></author><author><keyname>Yu</keyname><forenames>Lu</forenames></author></authors><title>Advancing Wasserstein Convergence Analysis of Score-Based Models:   Insights from Discretization and Second-Order Acceleration</title><categories>stat.ML cs.LG math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Score-based diffusion models have emerged as powerful tools in generative modeling, yet their theoretical foundations remain underexplored. In this work, we focus on the Wasserstein convergence analysis of score-based diffusion models. Specifically, we investigate the impact of various discretization schemes, including Euler discretization, exponential integrators, and midpoint randomization methods. Our analysis provides a quantitative comparison of these discrete approximations, emphasizing their influence on convergence behavior. Furthermore, we explore scenarios where Hessian information is available and propose an accelerated sampler based on the local linearization method. We demonstrate that this Hessian-based approach achieves faster convergence rates of order $\widetilde{\mathcal{O}}\left(\frac{1}{\varepsilon}\right)$ significantly improving upon the standard rate $\widetilde{\mathcal{O}}\left(\frac{1}{\varepsilon^2}\right)$ of vanilla diffusion models, where $\varepsilon$ denotes the target accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04867</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04867</id><created>2025-02-07</created><authors><author><keyname>Maclaren</keyname><forenames>Oliver J.</forenames></author><author><keyname>Nicholson</keyname><forenames>Ruanui</forenames></author><author><keyname>Trent</keyname><forenames>Joel A.</forenames></author><author><keyname>Rottenberry</keyname><forenames>Joshua</forenames></author><author><keyname>Simpson</keyname><forenames>Matthew</forenames></author></authors><title>Invariant Image Reparameterisation: A Unified Approach to Structural and   Practical Identifiability and Model Reduction</title><categories>stat.AP</categories><comments>25 pages + supplementary material</comments><msc-class>62F99 (Primary) 65L09, 93B30, 62F12, 34A55 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Both structural and practical parameter non-identifiability present fundamental challenges when using mathematical models to interpret data. This issue is particularly acute in complex, applied areas such as the life sciences or engineering, where determining appropriate model complexity is challenging. While several approaches exist for diagnosing and resolving parameter non-identifiability, including symbolic methods, profile likelihood analysis, and sloppiness analysis, these approaches have distinct limitations and are rarely combined. We present an integrated approach called Invariant Image Reparameterisation (IIR) that incorporates key elements of these methods in a new way. Our approach replaces symbolic computations with numerical calculations at a single reference estimate and an invariance condition that determines when this local calculation holds globally. Parameter combinations determined by this method are naturally ordered by degree of identifiability, and this supports model reduction by replacing a practically non-identified model with a structurally non-identified approximate model. This approximate model can be further parameterised in terms of identified parameters only. By treating parameter combinations determined by our approach as interest parameters within our established likelihood-based Profile-Wise Analysis (PWA) framework, we incorporate uncertainty quantification in terms of likelihood profiles and confidence sets. We provide a Julia library on GitHub (https://github.com/omaclaren/reparam) demonstrating our methodology across a range of mathematical models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04879</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04879</id><created>2025-02-07</created><authors><author><keyname>Gauthier</keyname><forenames>Etienne</forenames></author><author><keyname>Bach</keyname><forenames>Francis</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Statistical Collusion by Collectives on Learning Platforms</title><categories>stat.ML cs.LG</categories><comments>Code available at: https://github.com/GauthierE/statistical-collusion</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As platforms increasingly rely on learning algorithms, collectives may form and seek ways to influence these platforms to align with their own interests. This can be achieved by coordinated submission of altered data. To evaluate the potential impact of such behavior, it is essential to understand the computations that collectives must perform to impact platforms in this way. In particular, collectives need to make a priori assessments of the effect of the collective before taking action, as they may face potential risks when modifying their data. Moreover they need to develop implementable coordination algorithms based on quantities that can be inferred from observed data. We develop a framework that provides a theoretical and algorithmic treatment of these issues and present experimental results in a product evaluation domain. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04889</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04889</id><created>2025-02-07</created><authors><author><keyname>Bao</keyname><forenames>Han</forenames></author><author><keyname>Sakaue</keyname><forenames>Shinsaku</forenames></author><author><keyname>Takezawa</keyname><forenames>Yuki</forenames></author></authors><title>Any-stepsize Gradient Descent for Separable Data under Fenchel--Young   Losses</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The gradient descent (GD) has been one of the most common optimizer in machine learning. In particular, the loss landscape of a neural network is typically sharpened during the initial phase of training, making the training dynamics hover on the edge of stability. This is beyond our standard understanding of GD convergence in the stable regime where arbitrarily chosen stepsize is sufficiently smaller than the edge of stability. Recently, Wu et al. (COLT2024) have showed that GD converges with arbitrary stepsize under linearly separable logistic regression. Although their analysis hinges on the self-bounding property of the logistic loss, which seems to be a cornerstone to establish a modified descent lemma, our pilot study shows that other loss functions without the self-bounding property can make GD converge with arbitrary stepsize. To further understand what property of a loss function matters in GD, we aim to show arbitrary-stepsize GD convergence for a general loss function based on the framework of \emph{Fenchel--Young losses}. We essentially leverage the classical perceptron argument to derive the convergence rate for achieving $\epsilon$-optimal loss, which is possible for a majority of Fenchel--Young losses. Among typical loss functions, the Tsallis entropy achieves the GD convergence rate $T=\Omega(\epsilon^{-1/2})$, and the R{\'e}nyi entropy achieves the far better rate $T=\Omega(\epsilon^{-1/3})$. We argue that these better rate is possible because of \emph{separation margin} of loss functions, instead of the self-bounding property. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04891</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04891</id><created>2025-02-07</created><authors><author><keyname>Rubio-Madrigal</keyname><forenames>Celia</forenames></author><author><keyname>Jamadandi</keyname><forenames>Adarsh</forenames></author><author><keyname>Burkholz</keyname><forenames>Rebekka</forenames></author></authors><title>GNNs Getting ComFy: Community and Feature Similarity Guided Rewiring</title><categories>cs.LG cs.SI stat.ML</categories><comments>Accepted at ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Maximizing the spectral gap through graph rewiring has been proposed to enhance the performance of message-passing graph neural networks (GNNs) by addressing over-squashing. However, as we show, minimizing the spectral gap can also improve generalization. To explain this, we analyze how rewiring can benefit GNNs within the context of stochastic block models. Since spectral gap optimization primarily influences community strength, it improves performance when the community structure aligns with node labels. Building on this insight, we propose three distinct rewiring strategies that explicitly target community structure, node labels, and their alignment: (a) community structure-based rewiring (ComMa), a more computationally efficient alternative to spectral gap optimization that achieves similar goals; (b) feature similarity-based rewiring (FeaSt), which focuses on maximizing global homophily; and (c) a hybrid approach (ComFy), which enhances local feature similarity while preserving community structure to optimize label-community alignment. Extensive experiments confirm the effectiveness of these strategies and support our theoretical insights. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04892</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04892</id><created>2025-02-07</created><authors><author><keyname>Park</keyname><forenames>Joonhyeong</forenames></author><author><keyname>Park</keyname><forenames>Byoungwoo</forenames></author><author><keyname>Bang</keyname><forenames>Chang-Bae</forenames></author><author><keyname>Choi</keyname><forenames>Jungwon</forenames></author><author><keyname>Chung</keyname><forenames>Hyungjin</forenames></author><author><keyname>Kim</keyname><forenames>Byung-Hoon</forenames></author><author><keyname>Lee</keyname><forenames>Juho</forenames></author></authors><title>A Foundational Brain Dynamics Model via Stochastic Optimal Control</title><categories>cs.LG q-bio.NC stat.ML</categories><comments>The first two authors contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a foundational model for brain dynamics that utilizes stochastic optimal control (SOC) and amortized inference. Our method features a continuous-discrete state space model (SSM) that can robustly handle the intricate and noisy nature of fMRI signals. To address computational limitations, we implement an approximation strategy grounded in the SOC framework. Additionally, we present a simulation-free latent dynamics approach that employs locally linear approximations, facilitating efficient and scalable inference. For effective representation learning, we derive an Evidence Lower Bound (ELBO) from the SOC formulation, which integrates smoothly with recent advancements in self-supervised learning (SSL), thereby promoting robust and transferable representations. Pre-trained on extensive datasets such as the UKB, our model attains state-of-the-art results across a variety of downstream tasks, including demographic prediction, trait analysis, disease diagnosis, and prognosis. Moreover, evaluating on external datasets such as HCP-A, ABIDE, and ADHD200 further validates its superior abilities and resilience across different demographic and clinical distributions. Our foundational model provides a scalable and efficient approach for deciphering brain dynamics, opening up numerous applications in neuroscience. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04907</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04907</id><created>2025-02-07</created><authors><author><keyname>Gachon</keyname><forenames>Erell</forenames></author><author><keyname>Bigot</keyname><forenames>Jérémie</forenames></author><author><keyname>Cazelles</keyname><forenames>Elsa</forenames></author></authors><title>Scalable and consistent embedding of probability measures into Hilbert   spaces via measure quantization</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper is focused on statistical learning from data that come as probability measures. In this setting, popular approaches consist in embedding such data into a Hilbert space with either Linearized Optimal Transport or Kernel Mean Embedding. However, the cost of computing such embeddings prohibits their direct use in large-scale settings. We study two methods based on measure quantization for approximating input probability measures with discrete measures of small-support size. The first one is based on optimal quantization of each input measure, while the second one relies on mean-measure quantization. We study the consistency of such approximations, and its implication for scalable embeddings of probability measures into a Hilbert space at a low computational cost. We finally illustrate our findings with various numerical experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04938</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04938</id><created>2025-02-07</created><authors><author><keyname>Gardini</keyname><forenames>Aldo</forenames></author><author><keyname>Greco</keyname><forenames>Fedele</forenames></author><author><keyname>Trivisano</keyname><forenames>Carlo</forenames></author></authors><title>A note on auxiliary mixture sampling for Bayesian Poisson models</title><categories>stat.ME stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bayesian hierarchical Poisson models are an essential tool for analyzing count data. However, designing efficient algorithms to sample from the posterior distribution of the target parameters remains a challenging task for this class of models. Auxiliary mixture sampling algorithms have been proposed to address this issue. They involve two steps of data augmentations: the first leverages the theory of Poisson processes, and the second approximates the residual distribution of the resulting model through a mixture of Gaussian distributions. In this way, an approximated Gibbs sampler is obtained. In this paper, we focus on the accuracy of the approximation step, highlighting scenarios where the mixture fails to accurately represent the true underlying distribution, leading to a lack of convergence in the algorithm. We outline key features to monitor, in order to assess if the approximation performs as intended. Building on this, we propose a robust version of the auxiliary mixture sampling algorithm, which can detect approximation failures and incorporate a Metropolis-Hastings step when necessary. Finally, we evaluate the proposed algorithm together with the original mixture sampling algorithms on both simulated and real datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04945</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04945</id><created>2025-02-07</created><authors><author><keyname>Yanhao</keyname><affiliation>Max</affiliation></author><author><keyname>Wei</keyname></author><author><keyname>Jiang</keyname><forenames>Zhenling</forenames></author></authors><title>Estimating Parameters of Structural Models Using Neural Networks</title><categories>econ.EM stat.CO</categories><acm-class>G.3; J.4; I.2</acm-class><journal-ref>Marketing Science 44(1):102-128 (2024)</journal-ref><doi>10.1287/mksc.2022.0360</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an alternative use of machine learning. We train neural nets to provide the parameter estimate of a given (structural) econometric model, for example, discrete choice or consumer search. Training examples consist of datasets generated by the econometric model under a range of parameter values. The neural net takes the moments of a dataset as input and tries to recognize the parameter value underlying that dataset. Besides the point estimate, the neural net can also output statistical accuracy. This neural net estimator (NNE) tends to limited-information Bayesian posterior as the number of training datasets increases. We apply NNE to a consumer search model. It gives more accurate estimates at lighter computational costs than the prevailing approach. NNE is also robust to redundant moment inputs. In general, NNE offers the most benefits in applications where other estimation approaches require very heavy simulation costs. We provide code at: https://nnehome.github.io. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04949</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04949</id><created>2025-02-07</created><authors><author><keyname>Elsemüller</keyname><forenames>Lasse</forenames></author><author><keyname>Pratz</keyname><forenames>Valentin</forenames></author><author><keyname>von Krause</keyname><forenames>Mischa</forenames></author><author><keyname>Voss</keyname><forenames>Andreas</forenames></author><author><keyname>Bürkner</keyname><forenames>Paul-Christian</forenames></author><author><keyname>Radev</keyname><forenames>Stefan T.</forenames></author></authors><title>Does Unsupervised Domain Adaptation Improve the Robustness of Amortized   Bayesian Inference? A Systematic Evaluation</title><categories>stat.ML cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Neural networks are fragile when confronted with data that significantly deviates from their training distribution. This is true in particular for simulation-based inference methods, such as neural amortized Bayesian inference (ABI), where models trained on simulated data are deployed on noisy real-world observations. Recent robust approaches employ unsupervised domain adaptation (UDA) to match the embedding spaces of simulated and observed data. However, the lack of comprehensive evaluations across different domain mismatches raises concerns about the reliability in high-stakes applications. We address this gap by systematically testing UDA approaches across a wide range of misspecification scenarios in both a controlled and a high-dimensional benchmark. We demonstrate that aligning summary spaces between domains effectively mitigates the impact of unmodeled phenomena or noise. However, the same alignment mechanism can lead to failures under prior misspecifications - a critical finding with practical consequences. Our results underscore the need for careful consideration of misspecification types when using UDA techniques to increase the robustness of ABI in practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04970</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04970</id><created>2025-02-07</created><authors><author><keyname>Langbein</keyname><forenames>Sophie Hanna</forenames></author><author><keyname>Koenen</keyname><forenames>Niklas</forenames></author><author><keyname>Wright</keyname><forenames>Marvin N.</forenames></author></authors><title>Gradient-based Explanations for Deep Learning Survival Models</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deep learning survival models often outperform classical methods in time-to-event predictions, particularly in personalized medicine, but their "black box" nature hinders broader adoption. We propose a framework for gradient-based explanation methods tailored to survival neural networks, extending their use beyond regression and classification. We analyze the implications of their theoretical assumptions for time-dependent explanations in the survival setting and propose effective visualizations incorporating the temporal dimension. Experiments on synthetic data show that gradient-based methods capture the magnitude and direction of local and global feature effects, including time dependencies. We introduce GradSHAP(t), a gradient-based counterpart to SurvSHAP(t), which outperforms SurvSHAP(t) and SurvLIME in a computational speed vs. accuracy trade-off. Finally, we apply these methods to medical data with multi-modal inputs, revealing relevant tabular features and visual patterns, as well as their temporal dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04990</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04990</id><created>2025-02-07</created><authors><author><keyname>Pichler</keyname><forenames>Clemens</forenames></author><author><keyname>Jewson</keyname><forenames>Jack</forenames></author><author><keyname>Avalos-Pacheco</keyname><forenames>Alejandra</forenames></author></authors><title>Probabilistic Programming with Sufficient Statistics for faster Bayesian   Computation</title><categories>stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Probabilistic programming methods have revolutionised Bayesian inference, making it easier than ever for practitioners to perform Markov-chain-Monte-Carlo sampling from non-conjugate posterior distributions. Here we focus on Stan, arguably the most used probabilistic programming tool for Bayesian inference (Carpenter et al., 2017), and its interface with R via the brms (Burkner, 2017) and rstanarm (Goodrich et al., 2024) packages. Although easy to implement, these tools can become computationally prohibitive when applied to datasets with many observations or models with numerous parameters. While the use of sufficient statistics is well-established in theory, it has been surprisingly overlooked in state-of-the-art Stan software. We show that when the likelihood can be written in terms of sufficient statistics, considerable computational improvements can be made to current implementations. We demonstrate how this approach provides accurate inference at a fraction of the time than state-of-the-art implementations for Gaussian linear regression models with non-conjugate priors, hierarchical random effects models, and factor analysis models. Our results also show that moderate computational gains can be achieved even in models where the likelihood can only be partially written in terms of sufficient statistics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05021</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05021</id><created>2025-02-07</created><authors><author><keyname>van Heel</keyname><forenames>Simon Donker</forenames></author><author><keyname>Lange</keyname><forenames>Rutger-Jan</forenames></author><author><keyname>van Dijk</keyname><forenames>Dick</forenames></author><author><keyname>van Os</keyname><forenames>Bram</forenames></author></authors><title>Stability and performance guarantees for misspecified multivariate   score-driven filters</title><categories>stat.ME eess.SP stat.ML</categories><comments>71 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the problem of tracking latent time-varying parameter vectors under model misspecification. We analyze implicit and explicit score-driven (ISD and ESD) filters, which update a prediction of the parameters using the gradient of the logarithmic observation density (i.e., the score). In the ESD filter, the score is computed using the predicted parameter values, whereas in the ISD filter, the score is evaluated using the new, updated parameter values. For both filter types, we derive novel sufficient conditions for the exponential stability (i.e., invertibility) of the filtered parameter path and existence of a finite mean squared error (MSE) bound with respect to the pseudo-true parameter path. In addition, we present expressions for finite-sample and asymptotic MSE bounds. Our performance guarantees rely on mild moment conditions on the data-generating process, while our stability result is entirely agnostic about the true process. As a result, our primary conditions depend only on the characteristics of the filter; hence, they are verifiable in practice. Concavity of the postulated log density combined with simple parameter restrictions is sufficient (but not necessary) for ISD-filter stability, whereas ESD-filter stability additionally requires the score to be Lipschitz continuous. Extensive simulation studies validate our theoretical findings and demonstrate the enhanced stability and improved performance of ISD over ESD filters. An empirical application to U.S. Treasury-bill rates confirms the practical relevance of our contribution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05072</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05072</id><created>2025-02-07</created><authors><author><keyname>Barnett</keyname><forenames>Helen</forenames></author><author><keyname>Boix</keyname><forenames>Oliver</forenames></author><author><keyname>Kontos</keyname><forenames>Dimitris</forenames></author><author><keyname>Jaki</keyname><forenames>Thomas</forenames></author></authors><title>Joint TITE-CRM for Dual Agent Dose Finding Studies</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Dual agent dose-finding trials study the effect of a combination of more than one agent, where the objective is to find the Maximum Tolerated Dose Combination (MTC), the combination of doses of the two agents that is associated with a pre-specified risk of being unsafe. In a Phase I/II setting, the objective is to find a dose combination that is both safe and active, the Optimal Biological Dose (OBD), that optimizes a criterion based on both safety and activity. Since Oncology treatments are typically given over multiple cycles, both the safety and activity outcome can be considered as late-onset, potentially occurring in the later cycles of treatment. This work proposes two model-based designs for dual-agent dose finding studies with late-onset activity and late-onset toxicity outcomes, the Joint TITE-POCRM and the Joint TITE-BLRM. Their performance is compared alongside a model-assisted comparator in a comprehensive simulation study motivated by a real trial example, with an extension to consider alternative sized dosing grids. It is found that both model-based methods outperform the model-assisted design. Whilst on average the two model-based designs are comparable, this comparability is not consistent across scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05074</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05074</id><created>2025-02-07</created><authors><author><keyname>Atanasov</keyname><forenames>Alexander</forenames></author><author><keyname>Bordelon</keyname><forenames>Blake</forenames></author><author><keyname>Zavatone-Veth</keyname><forenames>Jacob A.</forenames></author><author><keyname>Paquette</keyname><forenames>Courtney</forenames></author><author><keyname>Pehlevan</keyname><forenames>Cengiz</forenames></author></authors><title>Two-Point Deterministic Equivalence for Stochastic Gradient Dynamics in   Linear Models</title><categories>cond-mat.dis-nn cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We derive a novel deterministic equivalence for the two-point function of a random matrix resolvent. Using this result, we give a unified derivation of the performance of a wide variety of high-dimensional linear models trained with stochastic gradient descent. This includes high-dimensional linear regression, kernel regression, and random feature models. Our results include previously known asymptotics as well as novel ones. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05075</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05075</id><created>2025-02-07</created><authors><author><keyname>Dong</keyname><forenames>Yijun</forenames></author><author><keyname>Li</keyname><forenames>Yicheng</forenames></author><author><keyname>Li</keyname><forenames>Yunai</forenames></author><author><keyname>Lee</keyname><forenames>Jason D.</forenames></author><author><keyname>Lei</keyname><forenames>Qi</forenames></author></authors><title>Discrepancies are Virtue: Weak-to-Strong Generalization through Lens of   Intrinsic Dimension</title><categories>cs.LG cs.NA math.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weak-to-strong (W2S) generalization is a type of finetuning (FT) where a strong (large) student model is trained on pseudo-labels generated by a weak teacher. Surprisingly, W2S FT often outperforms the weak teacher. We seek to understand this phenomenon through the observation that FT often occurs in intrinsically low-dimensional spaces. Leveraging the low intrinsic dimensionality of FT, we analyze W2S in the ridgeless regression setting from a variance reduction perspective. For a strong student - weak teacher pair with sufficiently expressive low-dimensional feature subspaces $\mathcal{V}_s, \mathcal{V}_w$, we provide an exact characterization of the variance that dominates the generalization error of W2S. This unveils a virtue of discrepancy between the strong and weak models in W2S: the variance of the weak teacher is inherited by the strong student in $\mathcal{V}_s \cap \mathcal{V}_w$, while reduced by a factor of $\dim(\mathcal{V}_s)/N$ in the subspace of discrepancy $\mathcal{V}_w \setminus \mathcal{V}_s$ with $N$ pseudo-labels for W2S. Further, our analysis casts light on the sample complexities and the scaling of performance gap recovery in W2S. The analysis is supported with experiments on both synthetic regression problems and real vision tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05094</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05094</id><created>2025-02-07</created><authors><author><keyname>Blanchet</keyname><forenames>Jose</forenames></author><author><keyname>Hamoudi</keyname><forenames>Yassine</forenames></author><author><keyname>Szegedy</keyname><forenames>Mario</forenames></author><author><keyname>Wang</keyname><forenames>Guanyang</forenames></author></authors><title>Non-linear Quantum Monte Carlo</title><categories>quant-ph cs.LG cs.NA math.NA stat.CO stat.ML</categories><comments>30 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The mean of a random variable can be understood as a $\textit{linear}$ functional on the space of probability distributions. Quantum computing is known to provide a quadratic speedup over classical Monte Carlo methods for mean estimation. In this paper, we investigate whether a similar quadratic speedup is achievable for estimating $\textit{non-linear}$ functionals of probability distributions. We propose a quantum-inside-quantum Monte Carlo algorithm that achieves such a speedup for a broad class of non-linear estimation problems, including nested conditional expectations and stochastic optimization. Our algorithm improves upon the direct application of the quantum multilevel Monte Carlo algorithm introduced by An et al.. The existing lower bound indicates that our algorithm is optimal up polylogarithmic factors. A key innovation of our approach is a new sequence of multilevel Monte Carlo approximations specifically designed for quantum computing, which is central to the algorithm's improved performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05102</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05102</id><created>2025-02-07</created><authors><author><keyname>Piancastelli</keyname><forenames>Luiza</forenames></author><author><keyname>Barreto-Souza</keyname><forenames>Wagner</forenames></author></authors><title>Time Series Analysis of Rankings: A GARCH-Type Approach</title><categories>stat.ME stat.CO stat.ML</categories><comments>Paper submitted for publication</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Ranking data are frequently obtained nowadays but there are still scarce methods for treating these data when temporally observed. The present paper contributes to this topic by proposing and developing novel models for handling time series of ranking data. We introduce a class of time-varying ranking models inspired by the Generalized AutoRegressive Conditional Heteroskedasticity (GARCH) models. More specifically, the temporal dynamics are defined by the conditional distribution of the current ranking given the past rankings, which are assumed to follow a Mallows distribution, which implicitly depends on a distance. Then, autoregressive and feedback components are incorporated into the model through the conditional expectation of the associated distances. Theoretical properties of our ranking GARCH models such as stationarity and ergodicity are established. The estimation of parameters is performed via maximum likelihood estimation when data is fully observed. We develop a Monte Carlo Expectation-Maximisation algorithm to deal with cases involving missing data. Monte Carlo simulation studies are presented to study the performance of the proposed estimators under both non-missing and missing data scenarios. A real data application about the weekly ranking of professional tennis players from 2015 to 2019 is presented under our proposed ranking GARCH models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05122</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05122</id><created>2025-02-07</created><authors><author><keyname>Xi</keyname><forenames>Johnny</forenames></author><author><keyname>Dance</keyname><forenames>Hugh</forenames></author><author><keyname>Orbanz</keyname><forenames>Peter</forenames></author><author><keyname>Bloem-Reddy</keyname><forenames>Benjamin</forenames></author></authors><title>Distinguishing Cause from Effect with Causal Velocity Models</title><categories>stat.ML cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bivariate structural causal models (SCM) are often used to infer causal direction by examining their goodness-of-fit under restricted model classes. In this paper, we describe a parametrization of bivariate SCMs in terms of a causal velocity by viewing the cause variable as time in a dynamical system. The velocity implicitly defines counterfactual curves via the solution of initial value problems where the observation specifies the initial condition. Using tools from measure transport, we obtain a unique correspondence between SCMs and the score function of the generated distribution via its causal velocity. Based on this, we derive an objective function that directly regresses the velocity against the score function, the latter of which can be estimated non-parametrically from observational data. We use this to develop a method for bivariate causal discovery that extends beyond known model classes such as additive or location scale noise, and that requires no assumptions on the noise distributions. When the score is estimated well, the objective is also useful for detecting model non-identifiability and misspecification. We present positive results in simulation and benchmark experiments where many existing methods fail, and perform ablation studies to examine the method's sensitivity to accurate score estimation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05127</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05127</id><created>2025-02-07</created><authors><author><keyname>Everink</keyname><forenames>Jasper M.</forenames></author><author><keyname>Amougou</keyname><forenames>Bernardin Tamo</forenames></author><author><keyname>Pereyra</keyname><forenames>Marcelo</forenames></author></authors><title>Self-supervised Conformal Prediction for Uncertainty Quantification in   Imaging Problems</title><categories>cs.CV stat.ME</categories><msc-class>62H35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most image restoration problems are ill-conditioned or ill-posed and hence involve significant uncertainty. Quantifying this uncertainty is crucial for reliably interpreting experimental results, particularly when reconstructed images inform critical decisions and science. However, most existing image restoration methods either fail to quantify uncertainty or provide estimates that are highly inaccurate. Conformal prediction has recently emerged as a flexible framework to equip any estimator with uncertainty quantification capabilities that, by construction, have nearly exact marginal coverage. To achieve this, conformal prediction relies on abundant ground truth data for calibration. However, in image restoration problems, reliable ground truth data is often expensive or not possible to acquire. Also, reliance on ground truth data can introduce large biases in situations of distribution shift between calibration and deployment. This paper seeks to develop a more robust approach to conformal prediction for image restoration problems by proposing a self-supervised conformal prediction method that leverages Stein's Unbiased Risk Estimator (SURE) to self-calibrate itself directly from the observed noisy measurements, bypassing the need for ground truth. The method is suitable for any linear imaging inverse problem that is ill-conditioned, and it is especially powerful when used with modern self-supervised image restoration techniques that can also be trained directly from measurement data. The proposed approach is demonstrated through numerical experiments on image denoising and deblurring, where it delivers results that are remarkably accurate and comparable to those obtained by supervised conformal prediction with ground truth data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05134</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05134</id><created>2025-02-07</created><authors><author><keyname>Kızıldağ</keyname><forenames>Eren C.</forenames></author></authors><title>Information-Theoretic Guarantees for Recovering Low-Rank Tensors from   Symmetric Rank-One Measurements</title><categories>math.ST cs.IT math.IT math.PR stat.ML stat.TH</categories><comments>Algorithmic Learning Theory (ALT), 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the sample complexity of recovering tensors with low symmetric rank from symmetric rank-one measurements. This setting is particularly motivated by the study of higher-order interactions and the analysis of two-layer neural networks with polynomial activations (polynomial networks). Using a covering numbers argument, we analyze the performance of the symmetric rank minimization program and establish near-optimal sample complexity bounds when the underlying distribution is log-concave. Our measurement model involves random symmetric rank-one tensors, which lead to involved probability calculations. To address these challenges, we employ the Carbery-Wright inequality, a powerful tool for studying anti-concentration properties of random polynomials, and leverage orthogonal polynomials. Additionally, we provide a sample complexity lower bound based on Fano's inequality, and discuss broader implications of our results for two-layer polynomial networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05155</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05155</id><created>2025-02-07</created><authors><author><keyname>Tang</keyname><forenames>Shiqin</forenames></author><author><keyname>Yu</keyname><forenames>Shujian</forenames></author><author><keyname>Dong</keyname><forenames>Yining</forenames></author><author><keyname>Qin</keyname><forenames>S. Joe</forenames></author></authors><title>Deep Dynamic Probabilistic Canonical Correlation Analysis</title><categories>cs.LG stat.ML</categories><comments>accepted by ICASSP-25, code is available at   \url{https://github.com/marcusstang/D2PCCA}</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents Deep Dynamic Probabilistic Canonical Correlation Analysis (D2PCCA), a model that integrates deep learning with probabilistic modeling to analyze nonlinear dynamical systems. Building on the probabilistic extensions of Canonical Correlation Analysis (CCA), D2PCCA captures nonlinear latent dynamics and supports enhancements such as KL annealing for improved convergence and normalizing flows for a more flexible posterior approximation. D2PCCA naturally extends to multiple observed variables, making it a versatile tool for encoding prior knowledge about sequential datasets and providing a probabilistic understanding of the system's dynamics. Experimental validation on real financial datasets demonstrates the effectiveness of D2PCCA and its extensions in capturing latent dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05161</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05161</id><created>2025-02-07</created><authors><author><keyname>Antonczak</keyname><forenames>Brittany</forenames></author><author><keyname>Fay</keyname><forenames>Meg</forenames></author><author><keyname>Chawla</keyname><forenames>Aviral</forenames></author><author><keyname>Rowangould</keyname><forenames>Gregory</forenames></author></authors><title>Estimated Roadway Segment Traffic Data by Vehicle Class for the United   States: A Machine Learning Approach</title><categories>stat.AP</categories><comments>14 pages including references, 4 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Highway Performance Monitoring System, managed by the Federal Highway Administration, provides essential data on average annual daily traffic across U.S. roadways, but it has limited representation of medium- and heavy-duty vehicles on non-interstate roads. This gap limits research and policy analysis on the impacts of truck traffic, especially concerning air quality and public health. To address this, we use random forest regression to estimate medium- and heavy-duty vehicle traffic volumes in areas with sparse data. This results in a more comprehensive dataset, which enables the estimation of traffic density at the census block level as a proxy for traffic-related air pollution exposure. Our high-resolution spatial data products, rigorously validated, provide a more accurate representation of truck traffic and its environmental and health impacts. These datasets are valuable for transportation planning, public health research, and policy decisions aimed at mitigating the effects of truck traffic on vulnerable communities exposed to air pollution. </abstract></arXiv></metadata></record>
