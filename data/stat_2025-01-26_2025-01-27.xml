<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:1902.01330</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>1902.01330</id><created>2019-02-04</created><updated>2025-01-07</updated><authors><author><keyname>Miller</keyname><forenames>David L.</forenames></author></authors><title>Bayesian views of generalized additive modelling</title><categories>stat.ME</categories><doi>10.1111/2041-210X.14498</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalized additive models (GAMs) are a commonly used, flexible framework applied to many problems in statistical ecology. GAMs are often considered to be a purely frequentist framework (`generalized linear models with wiggly bits'), however links between frequentist and Bayesian approaches to these models were highlighted early on in the literature. Bayesian thinking underlies many parts of the implementation in the popular R package \texttt{mgcv} as well as in GAM theory more generally. This article aims to highlight useful links (and differences) between Bayesian and frequentist approaches to smoothing, and their practical applications in ecology (with an \texttt{mgcv}-centric viewpoint). Here I give some background for these results then move onto two important topics for quantitative ecologists: term/model selection and uncertainty estimation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:1912.11209</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>1912.11209</id><created>2019-12-23</created><authors><author><keyname>Singh</keyname><forenames>Vikas</forenames></author><author><keyname>Verma</keyname><forenames>Nishchal K.</forenames></author></authors><title>An Entropy-based Variable Feature Weighted Fuzzy k-Means Algorithm for   High Dimensional Data</title><categories>cs.LG stat.ML</categories><doi>10.1007/s11042-024-20493-4</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents a new fuzzy k-means algorithm for the clustering of high dimensional data in various subspaces. Since, In the case of high dimensional data, some features might be irrelevant and relevant but may have different significance in the clustering. For a better clustering, it is crucial to incorporate the contribution of these features in the clustering process. To combine these features, in this paper, we have proposed a new fuzzy k-means clustering algorithm in which the objective function of the fuzzy k-means is modified using two different entropy term. The first entropy term helps to minimize the within-cluster dispersion and maximize the negative entropy to determine clusters to contribute to the association of data points. The second entropy term helps to control the weight of the features because different features have different contributing weights in the clustering process for obtaining the better partition of the data. The efficacy of the proposed method is presented in terms of various clustering measures on multiple datasets and compared with various state-of-the-art methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2206.06991</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2206.06991</id><created>2022-06-14</created><updated>2025-01-24</updated><authors><author><keyname>Legramanti</keyname><forenames>Sirio</forenames></author><author><keyname>Durante</keyname><forenames>Daniele</forenames></author><author><keyname>Alquier</keyname><forenames>Pierre</forenames></author></authors><title>Concentration of discrepancy-based approximate Bayesian computation via   Rademacher complexity</title><categories>stat.ME math.ST stat.CO stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been increasing interest on summary-free solutions for approximate Bayesian computation (ABC) which replace distances among summaries with discrepancies between the empirical distributions of the observed data and the synthetic samples generated under the proposed parameter values. The success of these strategies has motivated theoretical studies on the limiting properties of the induced posteriors. However, there is still the lack of a theoretical framework for summary-free ABC that (i) is unified, instead of discrepancy-specific, (ii) does not require to constrain the analysis to data generating processes and statistical models meeting specific regularity conditions, but rather facilitates the derivation of limiting properties that hold uniformly, and (iii) relies on verifiable assumptions that provide explicit concentration bounds clarifying which factors govern the limiting behavior of the ABC posterior. We address this gap via a novel theoretical framework that introduces the concept of Rademacher complexity in the analysis of the limiting properties for discrepancy-based ABC posteriors, including in non-i.i.d. and misspecified settings. This yields a unified theory that relies on constructive arguments and provides more informative asymptotic results and uniform concentration bounds, even in settings not covered by current studies. These advancements are obtained by relating the asymptotic properties of summary-free ABC posteriors to the behavior of the Rademacher complexity associated with the chosen discrepancy in the family of integral probability semimetrics (IPS). The IPS class extends summary-based distances, and includes the Wasserstein distance and maximum mean discrepancy, among others. As clarified in specialized theoretical analyses of popular IPS discrepancies and via illustrative simulations, this perspective improves the understanding of summary-free ABC. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2206.08178</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2206.08178</id><created>2022-06-16</created><updated>2022-06-23</updated><authors><author><keyname>Olaniyi</keyname><forenames>Babaniyi Yusuf</forenames></author><author><keyname>del Río</keyname><forenames>Ana Fernández</forenames></author><author><keyname>Periáñez</keyname><forenames>África</forenames></author><author><keyname>Bellhouse</keyname><forenames>Lauren</forenames></author></authors><title>User Engagement in Mobile Health Applications</title><categories>stat.ML cs.CY cs.LG stat.AP</categories><comments>Accepted at KDD 2022 Health Day, will be appear in the KDD2022   proceedings as a full paper</comments><journal-ref>KDD '22: Proceedings of the 28th ACM SIGKDD Conference on   Knowledge Discovery and Data Mining, 4704-4712, 2022</journal-ref><doi>10.1145/3534678.3542681</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Mobile health apps are revolutionizing the healthcare ecosystem by improving communication, efficiency, and quality of service. In low- and middle-income countries, they also play a unique role as a source of information about health outcomes and behaviors of patients and healthcare workers, while providing a suitable channel to deliver both personalized and collective policy interventions. We propose a framework to study user engagement with mobile health, focusing on healthcare workers and digital health apps designed to support them in resource-poor settings. The behavioral logs produced by these apps can be transformed into daily time series characterizing each user's activity. We use probabilistic and survival analysis to build multiple personalized measures of meaningful engagement, which could serve to tailor content and digital interventions suiting each health worker's specific needs. Special attention is given to the problem of detecting churn, understood as a marker of complete disengagement. We discuss the application of our methods to the Indian and Ethiopian users of the Safe Delivery App, a capacity-building tool for skilled birth attendants. This work represents an important step towards a full characterization of user engagement in mobile health applications, which can significantly enhance the abilities of health workers and, ultimately, save lives. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2206.14674</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2206.14674</id><created>2022-06-29</created><updated>2025-01-24</updated><authors><author><keyname>Lyons</keyname><forenames>Terry</forenames></author><author><keyname>McLeod</keyname><forenames>Andrew D.</forenames></author></authors><title>Signature Methods in Machine Learning</title><categories>stat.ML cs.LG cs.NA math.CA math.NA math.ST stat.ME stat.TH</categories><comments>Version accepted for publication in EMS Surveys in Mathematical   Sciences</comments><msc-class>60L10, 93C15, 68Q32, 34F05</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Signature-based techniques give mathematical insight into the interactions between complex streams of evolving data. These insights can be quite naturally translated into numerical approaches to understanding streamed data, and perhaps because of their mathematical precision, have proved useful in analysing streamed data in situations where the data is irregular, and not stationary, and the dimension of the data and the sample sizes are both moderate. Understanding streamed multi-modal data is exponential: a word in $n$ letters from an alphabet of size $d$ can be any one of $d^n$ messages. Signatures remove the exponential amount of noise that arises from sampling irregularity, but an exponential amount of information still remain. This survey aims to stay in the domain where that exponential scaling can be managed directly. Scalability issues are an important challenge in many problems but would require another survey article and further ideas. This survey describes a range of contexts where the data sets are small enough to remove the possibility of massive machine learning, and the existence of small sets of context free and principled features can be used effectively. The mathematical nature of the tools can make their use intimidating to non-mathematicians. The examples presented in this article are intended to bridge this communication gap and provide tractable working examples drawn from the machine learning context. Notebooks are available online for several of these examples. This survey builds on the earlier paper of Ilya Chevryev and Andrey Kormilitzin which had broadly similar aims at an earlier point in the development of this machinery. This article illustrates how the theoretical insights offered by signatures are simply realised in the analysis of application data in a way that is largely agnostic to the data type. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2207.08868</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2207.08868</id><created>2022-07-18</created><updated>2025-01-23</updated><authors><author><keyname>Xu</keyname><forenames>Mengshan</forenames></author><author><keyname>Otsu</keyname><forenames>Taisuke</forenames></author></authors><title>Isotonic propensity score matching</title><categories>econ.EM stat.ME</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We propose a one-to-many matching estimator of the average treatment effect based on propensity scores estimated by isotonic regression. This approach is predicated on the assumption of monotonicity in the propensity score function, a condition that can be justified in many economic applications. We show that the nature of the isotonic estimator can help us to fix many problems of existing matching methods, including efficiency, choice of the number of matches, choice of tuning parameters, robustness to propensity score misspecification, and bootstrap validity. As a by-product, a uniformly consistent isotonic estimator is developed for our proposed matching method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2211.05408</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2211.05408</id><created>2022-11-10</created><updated>2025-01-24</updated><authors><author><keyname>Kanagawa</keyname><forenames>Heishiro</forenames></author><author><keyname>Barp</keyname><forenames>Alessandro</forenames></author><author><keyname>Gretton</keyname><forenames>Arthur</forenames></author><author><keyname>Mackey</keyname><forenames>Lester</forenames></author></authors><title>Controlling Moments with Kernel Stein Discrepancies</title><categories>stat.ML cs.LG stat.CO</categories><comments>102 pages, 10 figures, Update key citations</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Kernel Stein discrepancies (KSDs) measure the quality of a distributional approximation and can be computed even when the target density has an intractable normalizing constant. Notable applications include the diagnosis of approximate MCMC samplers and goodness-of-fit tests for unnormalized statistical models. The present work analyzes the convergence control properties of KSDs. We first show that standard KSDs used for weak convergence control fail to control moment convergence. To address this limitation, we next provide sufficient conditions under which alternative diffusion KSDs control both moment and weak convergence. As an immediate consequence we develop, for each $q &gt; 0$, the first KSDs known to exactly characterize $q$-Wasserstein convergence. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.06254</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.06254</id><created>2023-12-11</created><updated>2025-01-24</updated><authors><author><keyname>Böther</keyname><forenames>Maximilian</forenames></author><author><keyname>Robroek</keyname><forenames>Ties</forenames></author><author><keyname>Gsteiger</keyname><forenames>Viktor</forenames></author><author><keyname>Holzinger</keyname><forenames>Robin</forenames></author><author><keyname>Ma</keyname><forenames>Xianzhe</forenames></author><author><keyname>Tözün</keyname><forenames>Pınar</forenames></author><author><keyname>Klimovic</keyname><forenames>Ana</forenames></author></authors><title>Modyn: Data-Centric Machine Learning Pipeline Orchestration</title><categories>cs.LG cs.AI cs.DB cs.DC stat.ML</categories><comments>final version published at SIGMOD'25; 30 pages</comments><doi>10.1145/3709705</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In real-world machine learning (ML) pipelines, datasets are continuously growing. Models must incorporate this new training data to improve generalization and adapt to potential distribution shifts. The cost of model retraining is proportional to how frequently the model is retrained and how much data it is trained on, which makes the naive approach of retraining from scratch each time impractical.   We present Modyn, a data-centric end-to-end machine learning platform. Modyn's ML pipeline abstraction enables users to declaratively describe policies for continuously training a model on a growing dataset. Modyn pipelines allow users to apply data selection policies (to reduce the number of data points) and triggering policies (to reduce the number of trainings). Modyn executes and orchestrates these continuous ML training pipelines. The system is open-source and comes with an ecosystem of benchmark datasets, models, and tooling. We formally discuss how to measure the performance of ML pipelines by introducing the concept of composite models, enabling fair comparison of pipelines with different data selection and triggering policies. We empirically analyze how various data selection and triggering policies impact model accuracy, and also show that Modyn enables high throughput training with sample-level data selection. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.06415</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.06415</id><created>2023-12-11</created><updated>2024-04-19</updated><authors><author><keyname>Hagar</keyname><forenames>Luke</forenames></author><author><keyname>Stevens</keyname><forenames>Nathaniel T.</forenames></author></authors><title>Bioequivalence Design with Sampling Distribution Segments</title><categories>stat.ME</categories><doi>10.1002/sim.10321</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In bioequivalence design, power analyses dictate how much data must be collected to detect the absence of clinically important effects. Power is computed as a tail probability in the sampling distribution of the pertinent test statistics. When these test statistics cannot be constructed from pivotal quantities, their sampling distributions are approximated via repetitive, time-intensive computer simulation. We propose a novel simulation-based method to quickly approximate the power curve for many such bioequivalence tests by efficiently exploring segments (as opposed to the entirety) of the relevant sampling distributions. Despite not estimating the entire sampling distribution, this approach prompts unbiased sample size recommendations. We illustrate this method using two-group bioequivalence tests with unequal variances and overview its broader applicability in clinical design. All methods proposed in this work can be implemented using the developed dent package in R. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.00324</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.00324</id><created>2023-12-30</created><updated>2025-01-24</updated><authors><author><keyname>Pesonen</keyname><forenames>Henri</forenames></author><author><keyname>Corander</keyname><forenames>Jukka</forenames></author></authors><title>Stratified distance space improves the efficiency of sequential samplers   for approximate Bayesian computation</title><categories>stat.ME stat.CO</categories><comments>16 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximate Bayesian computation (ABC) methods are standard tools for inferring parameters of complex models when the likelihood function is analytically intractable. A popular approach to improving the poor acceptance rate of the basic rejection sampling ABC algorithm is to use sequential Monte Carlo (ABC SMC) to produce a sequence of proposal distributions adapting towards the posterior, instead of generating values from the prior distribution of the model parameters. Proposal distribution for the subsequent iteration is typically obtained from a weighted set of samples, often called particles, of the current iteration of this sequence. Current methods for constructing these proposal distributions treat all the particles equivalently, regardless of the corresponding value generated by the sampler, which may lead to inefficiency when propagating the information across iterations of the algorithm. To improve sampler efficiency, we introduce a modified approach called stratified distance ABC SMC. Our algorithm stratifies particles based on their distance between the corresponding synthetic and observed data, and then constructs distinct proposal distributions for all the strata. Taking into account the distribution of distances across the particle space leads to substantially improved acceptance rate of the rejection sampling. We further show that efficiency can be gained by introducing a novel stopping rule for the sequential process based on the stratified posterior samples and demonstrate these advances by several examples. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.02708</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.02708</id><created>2024-01-05</created><authors><author><keyname>Zhang</keyname><forenames>Liwen</forenames></author><author><keyname>Zhong</keyname><forenames>Lianzhen</forenames></author><author><keyname>Yang</keyname><forenames>Fan</forenames></author><author><keyname>Dong</keyname><forenames>Di</forenames></author><author><keyname>Hui</keyname><forenames>Hui</forenames></author><author><keyname>Tian</keyname><forenames>Jie</forenames></author></authors><title>TripleSurv: Triplet Time-adaptive Coordinate Loss for Survival Analysis</title><categories>cs.LG cs.AI stat.ML</categories><comments>9 pages,6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A core challenge in survival analysis is to model the distribution of censored time-to-event data, where the event of interest may be a death, failure, or occurrence of a specific event. Previous studies have showed that ranking and maximum likelihood estimation (MLE)loss functions are widely-used for survival analysis. However, ranking loss only focus on the ranking of survival time and does not consider potential effect of samples for exact survival time values. Furthermore, the MLE is unbounded and easily subject to outliers (e.g., censored data), which may cause poor performance of modeling. To handle the complexities of learning process and exploit valuable survival time values, we propose a time-adaptive coordinate loss function, TripleSurv, to achieve adaptive adjustments by introducing the differences in the survival time between sample pairs into the ranking, which can encourage the model to quantitatively rank relative risk of pairs, ultimately enhancing the accuracy of predictions. Most importantly, the TripleSurv is proficient in quantifying the relative risk between samples by ranking ordering of pairs, and consider the time interval as a trade-off to calibrate the robustness of model over sample distribution. Our TripleSurv is evaluated on three real-world survival datasets and a public synthetic dataset. The results show that our method outperforms the state-of-the-art methods and exhibits good model performance and robustness on modeling various sophisticated data distributions with different censor rates. Our code will be available upon acceptance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.14669</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.14669</id><created>2024-01-26</created><updated>2025-01-24</updated><authors><author><keyname>Fritz</keyname><forenames>Tobias</forenames></author><author><keyname>Klingler</keyname><forenames>Andreas</forenames></author><author><keyname>McNeely</keyname><forenames>Drew</forenames></author><author><keyname>Shah-Mohammed</keyname><forenames>Areeb</forenames></author><author><keyname>Wang</keyname><forenames>Yuwen</forenames></author></authors><title>Hidden Markov Models and the Bayes Filter in Categorical Probability</title><categories>math.ST cs.SY eess.SY math.CT stat.TH</categories><comments>v3: shortened to 55 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use Markov categories to generalize the basic theory of Markov chains and hidden Markov models to an abstract setting. This comprises characterizations of hidden Markov models in terms of conditional independences and algorithms for Bayesian filtering and smoothing applicable in all Markov categories with conditionals. When instantiated in appropriate Markov categories, these algorithms specialize to existing ones such as the Kalman filter, forward-backward algorithm, and the Rauch-Tung-Striebel smoother. We also prove that the sequence of outputs of our abstract Bayes filter is itself a Markov chain with a concrete formula for its transition maps.   There are two main features of this categorical framework. The first is its abstract generality, as manifested in our unified account of hidden Markov models and algorithms for filtering and smoothing in discrete probability, Gaussian probability, measure-theoretic probability, possibilistic nondeterminism and others at the same time. The second feature is the intuitive visual representation of information flow in terms of string diagrams. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.15705</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.15705</id><created>2024-02-23</created><updated>2025-01-23</updated><authors><author><keyname>Lee</keyname><forenames>Jin Hyung</forenames></author><author><keyname>Lee</keyname><forenames>Ben Seiyon</forenames></author></authors><title>A Scalable Variational Bayes Approach to Fit High-dimensional Spatial   Generalized Linear Mixed Models</title><categories>stat.ME</categories><comments>26 Pages for the main paper, 60 pages for the supplemental   information, 5 tables, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian and discrete non-Gaussian spatial datasets are common across fields like public health, ecology, geosciences, and social sciences. Bayesian spatial generalized linear mixed models (SGLMMs) are a flexible class of models for analyzing such data, but they struggle to scale to large datasets. Many scalable Bayesian methods, built upon basis representations or sparse covariance matrices, still rely on posterior sampling via Markov chain Monte Carlo (MCMC). Variational Bayes (VB) methods have been applied to SGLMMs, but only for small areal datasets. We propose two computationally efficient VB approaches for analyzing moderately sized and massive (millions of locations) Gaussian and discrete non-Gaussian spatial data in the continuous spatial domain. Our methods leverage semi-parametric approximations of latent spatial processes and parallel computing to ensure computational efficiency. The proposed methods deliver inferential and predictive performance comparable to gold-standard MCMC methods while achieving computational speedups of up to 3600 times. In most cases, our VB approaches outperform state-of-the-art alternatives such as INLA and Hamiltonian Monte Carlo. We validate our methods through a comparative numerical study and applications to real-world datasets. These VB approaches can enable practitioners to model millions of discrete non-Gaussian spatial observations on standard laptops, significantly expanding access to advanced spatial modeling tools. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.18379</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.18379</id><created>2024-05-28</created><updated>2025-01-24</updated><authors><author><keyname>Zrnic</keyname><forenames>Tijana</forenames></author></authors><title>A Note on the Prediction-Powered Bootstrap</title><categories>stat.ML cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce PPBoot: a bootstrap-based method for prediction-powered inference. PPBoot is applicable to arbitrary estimation problems and is very simple to implement, essentially only requiring one application of the bootstrap. Through a series of examples, we demonstrate that PPBoot often performs nearly identically to (and sometimes better than) the earlier PPI(++) method based on asymptotic normality$\unicode{x2013}$when the latter is applicable$\unicode{x2013}$without requiring any asymptotic characterizations. Given its versatility, PPBoot could simplify and expand the scope of application of prediction-powered inference to problems where central limit theorems are hard to prove. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.19019</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.19019</id><created>2024-05-29</created><authors><author><keyname>Chatzopoulos</keyname><forenames>Matthaios</forenames></author><author><keyname>Koutsourelakis</keyname><forenames>Phaedon-Stelios</forenames></author></authors><title>Physics-Aware Neural Implicit Solvers for multiscale, parametric PDEs   with applications in heterogeneous media</title><categories>stat.ML cs.LG</categories><journal-ref>Computer Methods in Applied Mechanics and Engineering, Volume 432,   Part A, 2024, 117342, ISSN 0045-7825</journal-ref><doi>10.1016/j.cma.2024.117342</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We propose Physics-Aware Neural Implicit Solvers (PANIS), a novel, data-driven framework for learning surrogates for parametrized Partial Differential Equations (PDEs). It consists of a probabilistic, learning objective in which weighted residuals are used to probe the PDE and provide a source of {\em virtual} data i.e. the actual PDE never needs to be solved. This is combined with a physics-aware implicit solver that consists of a much coarser, discretized version of the original PDE, which provides the requisite information bottleneck for high-dimensional problems and enables generalization in out-of-distribution settings (e.g. different boundary conditions). We demonstrate its capability in the context of random heterogeneous materials where the input parameters represent the material microstructure. We extend the framework to multiscale problems and show that a surrogate can be learned for the effective (homogenized) solution without ever solving the reference problem. We further demonstrate how the proposed framework can accommodate and generalize several existing learning objectives and architectures while yielding probabilistic surrogates that can quantify predictive uncertainty. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.02124</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.02124</id><created>2024-06-04</created><updated>2025-01-24</updated><authors><author><keyname>Eberl</keyname><forenames>Andreas</forenames></author><author><keyname>Klar</keyname><forenames>Bernhard</forenames></author></authors><title>Defining Dispersion: A Fundamental Order for Univariate Discrete   Distributions</title><categories>stat.ME</categories><msc-class>60E15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The measurement of dispersion is one of the most fundamental and ubiquitous statistical concepts, in both applied and theoretical contexts. For dispersion measures, such as the standard deviation, to effectively capture the variability of a given distribution, they must, by definition, preserve some stochastic order of dispersion. The so-called dispersive order is the most basic order that serves as a foundation underneath the concept of dispersion measures. However, this order is incompatible with almost all discrete distributions, including lattice and most empirical distributions. As a result, popular measures may fail to accurately capture the dispersion of such distributions.   In this paper, discrete adaptations of the dispersive order are defined and analyzed. They are shown to be a compromise between being equivalent to the original dispersive order on their joint area of applicability and other crucial properties. Moreover, they share many characteristic properties with the dispersive order, validating their role as a foundation for measuring discrete dispersion in a manner closely aligned with the continuous setting. Their behaviour on well-known families of lattice distribution is generally as expected when parameter differences are sufficiently large. Most popular dispersion measures preserve both discrete dispersive orders, rigorously ensuring that they are also meaningful in discrete settings. However, the interquantile range fails to preserve either discrete order, indicating that it is unsuitable for measuring the dispersion of discrete distributions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.09116</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.09116</id><created>2024-06-13</created><updated>2025-01-24</updated><authors><author><keyname>Negri</keyname><forenames>Marcello Massimo</forenames></author><author><keyname>Aellen</keyname><forenames>Jonathan</forenames></author><author><keyname>Roth</keyname><forenames>Volker</forenames></author></authors><title>Injective flows for star-like manifolds</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Normalizing Flows (NFs) are powerful and efficient models for density estimation. When modeling densities on manifolds, NFs can be generalized to injective flows but the Jacobian determinant becomes computationally prohibitive. Current approaches either consider bounds on the log-likelihood or rely on some approximations of the Jacobian determinant. In contrast, we propose injective flows for star-like manifolds and show that for such manifolds we can compute the Jacobian determinant exactly and efficiently, with the same cost as NFs. This aspect is particularly relevant for variational inference settings, where no samples are available and only some unnormalized target is known. Among many, we showcase the relevance of modeling densities on star-like manifolds in two settings. Firstly, we introduce a novel Objective Bayesian approach for penalized likelihood models by interpreting level-sets of the penalty as star-like manifolds. Secondly, we consider probabilistic mixing models and introduce a general method for variational inference by defining the posterior of mixture weights on the probability simplex. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.09293</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.09293</id><created>2024-07-12</created><updated>2025-01-24</updated><authors><author><keyname>Riley</keyname><forenames>Richard D</forenames></author><author><keyname>Collins</keyname><forenames>Gary S</forenames></author><author><keyname>Whittle</keyname><forenames>Rebecca</forenames></author><author><keyname>Archer</keyname><forenames>Lucinda</forenames></author><author><keyname>Snell</keyname><forenames>Kym IE</forenames></author><author><keyname>Dhiman</keyname><forenames>Paula</forenames></author><author><keyname>Kirton</keyname><forenames>Laura</forenames></author><author><keyname>Legha</keyname><forenames>Amardeep</forenames></author><author><keyname>Liu</keyname><forenames>Xiaoxuan</forenames></author><author><keyname>Denniston</keyname><forenames>Alastair</forenames></author><author><keyname>Harrell</keyname><forenames>Frank E</forenames><suffix>Jr</suffix></author><author><keyname>Wynants</keyname><forenames>Laure</forenames></author><author><keyname>Martin</keyname><forenames>Glen P</forenames></author><author><keyname>Ensor</keyname><forenames>Joie</forenames></author></authors><title>A decomposition of Fisher's information to inform sample size for   developing fair and precise clinical prediction models -- part 1: binary   outcomes</title><categories>stat.ME</categories><comments>36 pages, 6 figures, 1 table</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  When developing a clinical prediction model, the sample size of the development dataset is a key consideration. Small sample sizes lead to greater concerns of overfitting, instability, poor performance and lack of fairness. Previous research has outlined minimum sample size calculations to minimise overfitting and precisely estimate the overall risk. However even when meeting these criteria, the uncertainty (instability) in individual-level risk estimates may be considerable. In this article we propose how to examine and calculate the sample size required for developing a model with acceptably precise individual-level risk estimates to inform decisions and improve fairness. We outline a five-step process to be used before data collection or when an existing dataset is available. It requires researchers to specify the overall risk in the target population, the (anticipated) distribution of key predictors in the model, and an assumed 'core model' either specified directly (i.e., a logistic regression equation is provided) or based on specified C-statistic and relative effects of (standardised) predictors. We produce closed-form solutions that decompose the variance of an individual's risk estimate into Fisher's unit information matrix, predictor values and total sample size; this allows researchers to quickly calculate and examine individual-level uncertainty interval widths and classification instability for specified sample sizes. Such information can be presented to key stakeholders (e.g., health professionals, patients, funders) using prediction and classification instability plots to help identify the (target) sample size required to improve trust, reliability and fairness in individual predictions. Our proposal is implemented in software module pmstabilityss. We provide real examples and emphasise the importance of clinical context including any risk thresholds for decision making. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.15018</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.15018</id><created>2024-10-19</created><updated>2025-01-24</updated><authors><author><keyname>Mariani</keyname><forenames>Arturo</forenames></author><author><keyname>Senocrate</keyname><forenames>Federico</forenames></author><author><keyname>Mikiel-Hunter</keyname><forenames>Jason</forenames></author><author><keyname>McAlpine</keyname><forenames>David</forenames></author><author><keyname>Beiderbeck</keyname><forenames>Barbara</forenames></author><author><keyname>Pecka</keyname><forenames>Michael</forenames></author><author><keyname>Lin</keyname><forenames>Kevin</forenames></author><author><keyname>Kreuz</keyname><forenames>Thomas</forenames></author></authors><title>Latency correction in sparse neuronal spike trains with overlapping   global events</title><categories>q-bio.NC physics.bio-ph physics.data-an physics.med-ph stat.AP</categories><comments>21 pages, 14 Figures</comments><msc-class>62-04, 65-04, 92-04, 92C05, 92C42</msc-class><acm-class>G.2.3; G.3; G.4; I.5.3; J.3</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Background: In Kreuz et al., J Neurosci Methods 381, 109703 (2022) two methods were proposed that perform latency correction, i.e., optimize the spike time alignment of sparse neuronal spike trains with well defined global spiking events. The first one based on direct shifts is fast but uses only partial latency information, while the other one makes use of the full information but relies on the computationally costly simulated annealing. Both methods reach their limits and can become unreliable when successive global events are not sufficiently separated or even overlap.   New Method: Here we propose an iterative scheme that combines the advantages of the two original methods by using in each step as much of the latency information as possible and by employing a very fast extrapolation direct shift method instead of the much slower simulated annealing.   Results: We illustrate the effectiveness and the improved performance, measured in terms of the relative shift error, of the new iterative scheme not only on simulated data with known ground truths but also on single-unit recordings from two medial superior olive neurons of a gerbil.   Comparison with Existing Method(s): The iterative scheme outperforms the existing approaches on both the simulated and the experimental data. Due to its low computational demands, and in contrast to simulated annealing, it can also be applied to very large datasets.   Conclusions: The new method generalizes and improves on the original method both in terms of accuracy and speed. Importantly, it is the only method that allows to disentangle global events with overlap. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.22948</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.22948</id><created>2024-10-30</created><updated>2025-01-24</updated><authors><author><keyname>Rønning</keyname><forenames>Ola</forenames></author><author><keyname>Nalisnick</keyname><forenames>Eric</forenames></author><author><keyname>Ley</keyname><forenames>Christophe</forenames></author><author><keyname>Smyth</keyname><forenames>Padhraic</forenames></author><author><keyname>Hamelryck</keyname><forenames>Thomas</forenames></author></authors><title>ELBOing Stein: Variational Bayes with Stein Mixture Inference</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Stein variational gradient descent (SVGD) [Liu and Wang, 2016] performs approximate Bayesian inference by representing the posterior with a set of particles. However, SVGD suffers from variance collapse, i.e. poor predictions due to underestimating uncertainty [Ba et al., 2021], even for moderately-dimensional models such as small Bayesian neural networks (BNNs). To address this issue, we generalize SVGD by letting each particle parameterize a component distribution in a mixture model. Our method, Stein Mixture Inference (SMI), optimizes a lower bound to the evidence (ELBO) and introduces user-specified guides parameterized by particles. SMI extends the Nonlinear SVGD framework [Wang and Liu, 2019] to the case of variational Bayes. SMI effectively avoids variance collapse, judging by a previously described test developed for this purpose, and performs well on standard data sets. In addition, SMI requires considerably fewer particles than SVGD to accurately estimate uncertainty for small BNNs. The synergistic combination of NSVGD, ELBO optimization and user-specified guides establishes a promising approach towards variational Bayesian inference in the case of tall and wide data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.05601</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.05601</id><created>2024-11-08</created><updated>2025-01-24</updated><authors><author><keyname>Hecq</keyname><forenames>Alain</forenames></author><author><keyname>Ricardo</keyname><forenames>Ivan</forenames></author><author><keyname>Wilms</keyname><forenames>Ines</forenames></author></authors><title>Detecting Cointegrating Relations in Non-stationary Matrix-Valued Time   Series</title><categories>econ.EM stat.ME</categories><comments>10 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper proposes a Matrix Error Correction Model to identify cointegration relations in matrix-valued time series. We hereby allow separate cointegrating relations along the rows and columns of the matrix-valued time series and use information criteria to select the cointegration ranks. Through Monte Carlo simulations and a macroeconomic application, we demonstrate that our approach provides a reliable estimation of the number of cointegrating relationships. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.14288</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.14288</id><created>2024-11-21</created><updated>2025-01-23</updated><authors><author><keyname>Behboodi</keyname><forenames>Arash</forenames></author><author><keyname>Cesa</keyname><forenames>Gabriele</forenames></author></authors><title>On the Sample Complexity of One Hidden Layer Networks with Equivariance,   Locality and Weight Sharing</title><categories>cs.LG math.ST stat.ML stat.TH</categories><comments>48 pages, TMLR accepted</comments><msc-class>68T07</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Weight sharing, equivariance, and local filters, as in convolutional neural networks, are believed to contribute to the sample efficiency of neural networks. However, it is not clear how each one of these design choices contributes to the generalization error. Through the lens of statistical learning theory, we aim to provide insight into this question by characterizing the relative impact of each choice on the sample complexity. We obtain lower and upper sample complexity bounds for a class of single hidden layer networks. For a large class of activation functions, the bounds depend merely on the norm of filters and are dimension-independent. We also provide bounds for max-pooling and an extension to multi-layer networks, both with mild dimension dependence. We provide a few takeaways from the theoretical results. It can be shown that depending on the weight-sharing mechanism, the non-equivariant weight-sharing can yield a similar generalization bound as the equivariant one. We show that locality has generalization benefits, however the uncertainty principle implies a trade-off between locality and expressivity. We conduct extensive experiments and highlight some consistent trends for these models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.18180</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.18180</id><created>2024-12-24</created><updated>2025-01-24</updated><authors><author><keyname>Nanmo</keyname><forenames>Hisayoshi</forenames></author><author><keyname>Kuroki</keyname><forenames>Manabu</forenames></author></authors><title>PCM Selector: Penalized Covariate-Mediator Selection Operator for   Evaluating Linear Causal Effects</title><categories>stat.ME cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  For a data-generating process for random variables that can be described with a linear structural equation model, we consider a situation in which (i) a set of covariates satisfying the back-door criterion cannot be observed or (ii) such a set can be observed, but standard statistical estimation methods cannot be applied to estimate causal effects because of multicollinearity/high-dimensional data problems. We propose a novel two-stage penalized regression approach, the penalized covariate-mediator selection operator (PCM Selector), to estimate the causal effects in such scenarios. Unlike existing penalized regression analyses, when a set of intermediate variables is available, PCM Selector provides a consistent or less biased estimator of the causal effect. In addition, PCM Selector provides a variable selection procedure for intermediate variables to obtain better estimation accuracy of the causal effects than does the back-door criterion. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.19366</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.19366</id><created>2024-12-26</created><updated>2025-01-23</updated><authors><author><keyname>Álvarez-López</keyname><forenames>Antonio</forenames></author><author><keyname>Geshkovski</keyname><forenames>Borjan</forenames></author><author><keyname>Ruiz-Balet</keyname><forenames>Domènec</forenames></author></authors><title>Constructive approximate transport maps with normalizing flows</title><categories>math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an approximate controllability problem for the continuity equation and its application to constructing transport maps with normalizing flows. Specifically, we construct time-dependent controls $\theta=(w, a, b)$ in the vector field $x\mapsto w(a^\top x + b)_+$ to approximately transport a known base density $\rho_{\mathrm{B}}$ to a target density $\rho_*$. The approximation error is measured in relative entropy, and $\theta$ are constructed piecewise constant, with bounds on the number of switches being provided. Our main result relies on an assumption on the relative tail decay of $\rho_*$ and $\rho_{\mathrm{B}}$, and provides hints on characterizing the reachable space of the continuity equation in relative entropy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10321</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10321</id><created>2025-01-17</created><updated>2025-01-24</updated><authors><author><keyname>Saveliev</keyname><forenames>Evgeny</forenames></author><author><keyname>Liu</keyname><forenames>Jiashuo</forenames></author><author><keyname>Seedat</keyname><forenames>Nabeel</forenames></author><author><keyname>Boyd</keyname><forenames>Anders</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Towards Human-Guided, Data-Centric LLM Co-Pilots</title><categories>cs.LG stat.ML</categories><comments>Saveliev, Liu &amp; Seedat contributed equally</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Machine learning (ML) has the potential to revolutionize various domains, but its adoption is often hindered by the disconnect between the needs of domain experts and translating these needs into robust and valid ML tools. Despite recent advances in LLM-based co-pilots to democratize ML for non-technical domain experts, these systems remain predominantly focused on model-centric aspects while overlooking critical data-centric challenges. This limitation is problematic in complex real-world settings where raw data often contains complex issues, such as missing values, label noise, and domain-specific nuances requiring tailored handling. To address this we introduce CliMB-DC, a human-guided, data-centric framework for LLM co-pilots that combines advanced data-centric tools with LLM-driven reasoning to enable robust, context-aware data processing. At its core, CliMB-DC introduces a novel, multi-agent reasoning system that combines a strategic coordinator for dynamic planning and adaptation with a specialized worker agent for precise execution. Domain expertise is then systematically incorporated to guide the reasoning process using a human-in-the-loop approach. To guide development, we formalize a taxonomy of key data-centric challenges that co-pilots must address. Thereafter, to address the dimensions of the taxonomy, we integrate state-of-the-art data-centric tools into an extensible, open-source architecture, facilitating the addition of new tools from the research community. Empirically, using real-world healthcare datasets we demonstrate CliMB-DC's ability to transform uncurated datasets into ML-ready formats, significantly outperforming existing co-pilot baselines for handling data-centric challenges. CliMB-DC promises to empower domain experts from diverse domains -- healthcare, finance, social sciences and more -- to actively participate in driving real-world impact using ML. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.11139</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.11139</id><created>2025-01-19</created><updated>2025-01-23</updated><authors><author><keyname>Jin</keyname><forenames>Dian</forenames></author><author><keyname>Zhang</keyname><forenames>Yuqian</forenames></author><author><keyname>Zhang</keyname><forenames>Qiaosheng</forenames></author></authors><title>Community Detection for Contextual-LSBM: Theoretical Limitations of   Misclassification Rate and Efficient Algorithms</title><categories>stat.ML cs.LG</categories><comments>online version for Isit-25 submission</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The integration of network information and node attribute information has recently gained significant attention in the community detection literature. In this work, we consider community detection in the Contextual Labeled Stochastic Block Model (CLSBM), where the network follows an LSBM and node attributes follow a Gaussian Mixture Model (GMM). Our primary focus is the misclassification rate, which measures the expected number of nodes misclassified by community detection algorithms. We first establish a lower bound on the optimal misclassification rate that holds for any algorithm. When we specialize our setting to the LSBM (which preserves only network information) or the GMM (which preserves only node attribute information), our lower bound recovers prior results. Moreover, we present an efficient spectral-based algorithm tailored for the CLSBM and derive an upper bound on its misclassification rate. Although the algorithm does not attain the lower bound, it serves as a reliable starting point for designing more accurate community detection algorithms (as many algorithms use spectral method as an initial step, followed by refinement procedures to enhance accuracy). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.13932</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.13932</id><created>2025-01-08</created><authors><author><keyname>Granados</keyname><forenames>Abraham</forenames></author><author><keyname>Bañales</keyname><forenames>Isaías</forenames></author></authors><title>Understanding the Hamiltonian Monte Carlo through its Physics   Fundamentals and Examples</title><categories>stat.CO stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Hamiltonian Monte Carlo (HMC) algorithm is a powerful Markov Chain Monte Carlo (MCMC) method that uses Hamiltonian dynamics to generate samples from a target distribution. To fully exploit its potential, we must understand how Hamiltonian dynamics work and why they can be used in a MCMC algorithm. This work elucidates the Monte Carlo Hamiltonian, providing comprehensive explanations of the underlying physical concepts. It is intended for readers with a solid foundation in mathematics who may lack familiarity with specific physical concepts, such as those related to Hamiltonian dynamics. Additionally, we provide Python code for the HMC algorithm, examples and comparisons with the Random Walk Metropolis-Hastings (RWMH) and t-walk algorithms to highlight HMC's strengths and weaknesses when applied to Bayesian Inference. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14075</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14075</id><created>2025-01-23</created><authors><author><keyname>Lando</keyname><forenames>Tommaso</forenames></author><author><keyname>Benjrada</keyname><forenames>Mohammed Es-Salih</forenames></author></authors><title>A new class of tests for convex-ordered families based on expected order   statistics</title><categories>math.ST stat.TH</categories><msc-class>62Gxx</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Consider a pair of cumulative distribution functions $F$ and $G$, where $F$ is unknown and $G$ is a known reference distribution. Given a sample from $F$, we propose tests to detect the convexity or the concavity of $G^{-1}\circ F$ versus equality in distribution (up to location and scale transformations). This framework encompasses well-known cases, including increasing hazard rate distributions, as well as some other relevant families that have garnered attention more recently, for which no tests are currently available. We introduce test statistics based on the estimated probability that the random variable of interest does not exceed a given expected order statistic, which, in turn, is estimated via L-estimation. The tests are unbiased, consistent, and exhibit monotone power with respect to the convex transform order. To ensure consistency, we extend the strong law of large numbers for L-estimators to random variables without finite means, making the tests suitable for heavy-tailed distributions. Unlike other approaches, these tests are broadly applicable, regardless of the choice of $G$ and without support restrictions. The performance of the method under various conditions is demonstrated via simulations, and its applicability is illustrated through a concrete example. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14080</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14080</id><created>2025-01-23</created><authors><author><keyname>Lang</keyname><forenames>Quanjun</forenames></author><author><keyname>Lu</keyname><forenames>Jianfeng</forenames></author></authors><title>A Unified Blockwise Measurement Design for Learning Quantum Channels and   Lindbladians via Low-Rank Matrix Sensing</title><categories>quant-ph stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Quantum superoperator learning is a pivotal task in quantum information science, enabling accurate reconstruction of unknown quantum operations from measurement data. We propose a robust approach based on the matrix sensing techniques for quantum superoperator learning that extends beyond the positive semidefinite case, encompassing both quantum channels and Lindbladians. We first introduce a randomized measurement design using a near-optimal number of measurements. By leveraging the restricted isometry property (RIP), we provide theoretical guarantees for the identifiability and recovery of low-rank superoperators in the presence of noise. Additionally, we propose a blockwise measurement design that restricts the tomography to the sub-blocks, significantly enhancing performance while maintaining a comparable scale of measurements. We also provide a performance guarantee for this setup. Our approach employs alternating least squares (ALS) with acceleration for optimization in matrix sensing. Numerical experiments validate the efficiency and scalability of the proposed methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14090</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14090</id><created>2025-01-23</created><authors><author><keyname>Li</keyname><forenames>Bolian</forenames></author><author><keyname>Zhang</keyname><forenames>Ruqi</forenames></author></authors><title>Making Reliable and Flexible Decisions in Long-tailed Classification</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Long-tailed classification is challenging due to its heavy imbalance in class probabilities. While existing methods often focus on overall accuracy or accuracy for tail classes, they overlook a critical aspect: certain types of errors can carry greater risks than others in real-world long-tailed problems. For example, misclassifying patients (a tail class) as healthy individuals (a head class) entails far more serious consequences than the reverse scenario. To address this critical issue, we introduce Making Reliable and Flexible Decisions in Long-tailed Classification (RF-DLC), a novel framework aimed at reliable predictions in long-tailed problems. Leveraging Bayesian Decision Theory, we introduce an integrated gain to seamlessly combine long-tailed data distributions and the decision-making procedure. We further propose an efficient variational optimization strategy for the decision risk objective. Our method adapts readily to diverse utility matrices, which can be designed for specific tasks, ensuring its flexibility for different problem settings. In empirical evaluation, we design a new metric, False Head Rate, to quantify tail-sensitivity risk, along with comprehensive experiments on multiple real-world tasks, including large-scale image classification and uncertainty quantification, to demonstrate the reliability and flexibility of our method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14095</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14095</id><created>2025-01-23</created><authors><author><keyname>Ramsay</keyname><forenames>Kelly</forenames></author><author><keyname>Spicker</keyname><forenames>Dylan</forenames></author></authors><title>Improved subsample-and-aggregate via the private modified winsorized   mean</title><categories>stat.ME cs.LG</categories><comments>40 pages, 2 figures</comments><msc-class>62G35, 68P27</msc-class><acm-class>G.3.7; C.2.0</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We develop a univariate, differentially private mean estimator, called the private modified winsorized mean designed to be used as the aggregator in subsample-and-aggregate. We demonstrate, via real data analysis, that common differentially private multivariate mean estimators may not perform well as the aggregator, even with a dataset with 8000 observations, motivating our developments. We show that the modified winsorized mean is minimax optimal for several, large classes of distributions, even under adversarial contamination. We also demonstrate that, empirically, the modified winsorized mean performs well compared to other private mean estimates. We consider the modified winsorized mean as the aggregator in subsample-and-aggregate, deriving a finite sample deviations bound for a subsample-and-aggregate estimate generated with the new aggregator. This result yields two important insights: (i) the optimal choice of subsamples depends on the bias of the estimator computed on the subsamples, and (ii) the rate of convergence of the subsample-and-aggregate estimator depends on the robustness of the estimator computed on the subsamples. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14097</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14097</id><created>2025-01-23</created><authors><author><keyname>Morsomme</keyname><forenames>Raphael</forenames></author><author><keyname>Liang</keyname><forenames>C. Jason</forenames></author><author><keyname>Mateja</keyname><forenames>Allyson</forenames></author><author><keyname>Follmann</keyname><forenames>Dean A.</forenames></author><author><keyname>O'Brien</keyname><forenames>Meagan P.</forenames></author><author><keyname>Wang</keyname><forenames>Chenguang</forenames></author><author><keyname>Fintzi</keyname><forenames>Jonathan</forenames></author></authors><title>Assessing treatment efficacy for interval-censored endpoints using   multistate semi-Markov models fit to multiple data streams</title><categories>stat.ME stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce a computationally efficient and general approach for utilizing multiple, possibly interval-censored, data streams to study complex biomedical endpoints using multistate semi-Markov models. Our motivating application is the REGEN-2069 trial, which investigated the protective efficacy (PE) of the monoclonal antibody combination REGEN-COV against SARS-CoV-2 when administered prophylactically to individuals in households at high risk of secondary transmission. Using data on symptom onset, episodic RT-qPCR sampling, and serological testing, we estimate the PE of REGEN-COV for asymptomatic infection, its effect on seroconversion following infection, and the duration of viral shedding. We find that REGEN-COV reduced the risk of asymptomatic infection and the duration of viral shedding, and led to lower rates of seroconversion among asymptomatically infected participants. Our algorithm for fitting semi-Markov models to interval-censored data employs a Monte Carlo expectation maximization (MCEM) algorithm combined with importance sampling to efficiently address the intractability of the marginal likelihood when data are intermittently observed. Our algorithm provide substantial computational improvements over existing methods and allows us to fit semi-parametric models despite complex coarsening of the data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14107</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14107</id><created>2025-01-23</created><authors><author><keyname>Chen</keyname><forenames>Jianhong</forenames></author><author><keyname>Yang</keyname><forenames>Shihao</forenames></author></authors><title>EFiGP: Eigen-Fourier Physics-Informed Gaussian Process for Inference of   Dynamic Systems</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Parameter estimation and trajectory reconstruction for data-driven dynamical systems governed by ordinary differential equations (ODEs) are essential tasks in fields such as biology, engineering, and physics. These inverse problems -- estimating ODE parameters from observational data -- are particularly challenging when the data are noisy, sparse, and the dynamics are nonlinear. We propose the Eigen-Fourier Physics-Informed Gaussian Process (EFiGP), an algorithm that integrates Fourier transformation and eigen-decomposition into a physics-informed Gaussian Process framework. This approach eliminates the need for numerical integration, significantly enhancing computational efficiency and accuracy. Built on a principled Bayesian framework, EFiGP incorporates the ODE system through probabilistic conditioning, enforcing governing equations in the Fourier domain while truncating high-frequency terms to achieve denoising and computational savings. The use of eigen-decomposition further simplifies Gaussian Process covariance operations, enabling efficient recovery of trajectories and parameters even in dense-grid settings. We validate the practical effectiveness of EFiGP on three benchmark examples, demonstrating its potential for reliable and interpretable modeling of complex dynamical systems while addressing key challenges in trajectory recovery and computational cost. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14118</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14118</id><created>2025-01-23</created><authors><author><keyname>Mulkin</keyname><forenames>Olivier</forenames></author><author><keyname>Heleno</keyname><forenames>Miguel</forenames></author><author><keyname>Ludkovski</keyname><forenames>Mike</forenames></author></authors><title>Selecting Critical Scenarios of DER Adoption in Distribution Grids Using   Bayesian Optimization</title><categories>cs.LG stat.AP stat.ML</categories><comments>10 pages, 2 tables, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a new methodology to select scenarios of DER adoption most critical for distribution grids. Anticipating risks of future voltage and line flow violations due to additional PV adopters is central for utility investment planning but continues to rely on deterministic or ad hoc scenario selection. We propose a highly efficient search framework based on multi-objective Bayesian Optimization. We treat underlying grid stress metrics as computationally expensive black-box functions, approximated via Gaussian Process surrogates and design an acquisition function based on probability of scenarios being Pareto-critical across a collection of line- and bus-based violation objectives. Our approach provides a statistical guarantee and offers an order of magnitude speed-up relative to a conservative exhaustive search. Case studies on realistic feeders with 200-400 buses demonstrate the effectiveness and accuracy of our approach. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14139</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14139</id><created>2025-01-23</created><authors><author><keyname>Lawson</keyname><forenames>John R.</forenames></author></authors><title>A Probabilistic WxChallenge Proposal</title><categories>stat.AP</categories><comments>Manuscript is 10 pages and 2 figures. Originally submitted and   rejected from Bulletin of American Meteorological Society. Future   resubmission is expected</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The national forecasting competition WxChallenge, brainchild of Brad Illston at the University of Oklahoma in 2005, has become a cherished institution played across the United States each year. Participants include students, faculty, alumni, and industry professionals. However, forecasts are given as scalar values without expression of uncertainty, probabilities being a keystone of meteorological forecasting today, and previous attempts to add probabilistic elements to WxChallenge have failed partly due to challenges in making probability forecasting accessible to all, and inability to combine scores with different units while also appropriately rewarding forecasts using proper scoring rules. Much of the competition's maintenance relies on dedicated volunteers, highlighting need for more automation. Hence I propose three new features: (1) automated forecast problems based on morning ensemble guidance, forming prediction baselines, thresholds over which the players demonstrate skill in their later forecast; (2) a spread betting game, where the players allocate 100 confidence credits to the over-under for exceeding a percentile (e.g., 50pc) threshold of a variable (e.g., maximum temperature) derived from the ensemble baseline; and (3) a game where players distribute 100 confidence credits across bins of a continuous variable (e.g., accumulated precipitation) approximating a probability mass function. Forecasts are evaluated using Shannon information gained over the baseline forecast, yielding additive units of bits that allow score combinations of different variables and units. Information gain parallels the Brier Score and is likewise a sound measure of skill due its punishment of hedging. This proposal objective is to augment WxChallenge with two new probabilistic games that are accessible, scientifically sound, enjoyable, and optional. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14142</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14142</id><created>2025-01-23</created><authors><author><keyname>Goldwasser</keyname><forenames>Jeremy</forenames></author><author><keyname>Fithian</keyname><forenames>Will</forenames></author><author><keyname>Hooker</keyname><forenames>Giles</forenames></author></authors><title>Gaussian Rank Verification</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Statistical experiments often seek to identify random variables with the largest population means. This inferential task, known as rank verification, has been well-studied on Gaussian data with equal variances. This work provides the first treatment of the unequal variances case, utilizing ideas from the selective inference literature. We design a hypothesis test that verifies the rank of the largest observed value without losing power due to multiple testing corrections. This test is subsequently extended for two procedures: Identifying some number of correctly-ordered Gaussian means, and validating the top-K set. The testing procedures are validated on NHANES survey data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14152</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14152</id><created>2025-01-23</created><authors><author><keyname>Bertsimas</keyname><forenames>Dimitris</forenames></author><author><keyname>Everest</keyname><forenames>Lisa</forenames></author><author><keyname>Stoumpou</keyname><forenames>Vasiliki</forenames></author></authors><title>Multimodal Prescriptive Deep Learning</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce a multimodal deep learning framework, Prescriptive Neural Networks (PNNs), that combines ideas from optimization and machine learning, and is, to the best of our knowledge, the first prescriptive method to handle multimodal data. The PNN is a feedforward neural network trained on embeddings to output an outcome-optimizing prescription. In two real-world multimodal datasets, we demonstrate that PNNs prescribe treatments that are able to significantly improve estimated outcomes in transcatheter aortic valve replacement (TAVR) procedures by reducing estimated postoperative complication rates by 32% and in liver trauma injuries by reducing estimated mortality rates by over 40%. In four real-world, unimodal tabular datasets, we demonstrate that PNNs outperform or perform comparably to other well-known, state-of-the-art prescriptive models; importantly, on tabular datasets, we also recover interpretability through knowledge distillation, fitting interpretable Optimal Classification Tree models onto the PNN prescriptions as classification targets, which is critical for many real-world applications. Finally, we demonstrate that our multimodal PNN models achieve stability across randomized data splits comparable to other prescriptive methods and produce realistic prescriptions across the different datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14168</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14168</id><created>2025-01-23</created><authors><author><keyname>Yan</keyname><forenames>Guowei</forenames></author><author><keyname>Zhao</keyname><forenames>Ping</forenames></author><author><keyname>Feng</keyname><forenames>Long</forenames></author></authors><title>Inverse Norm Weighted Maxsum Test for High Dimensional Location   Parameters</title><categories>stat.ME</categories><comments>arXiv admin note: text overlap with arXiv:2402.01381</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the context of high-dimensional data, we investigate the one-sample location testing problem. We introduce a max-type test based on the weighted spatial sign, which exhibits exceptional performance, particularly in the presence of sparse alternatives. Notably, we find that the inverse norm test significantly enhances the power of the test compared to several existing max-type tests. Next, we prove the asymptotic independence between the newly proposed max-type test statistic and the sum-type test statistic based on the weighted spatial sign. Then, we propose an innovative max-sum type testing procedure that integrates both test statistics. This novel procedure demonstrates remarkable robustness and effectiveness across a wide range of signal sparsity levels and heavy-tailed distributions. Through extensive simulation studies, we highlight the superior performance of the proposed method, showcasing its robustness and efficiency compared to traditional alternatives in various high-dimensional settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14197</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14197</id><created>2025-01-23</created><authors><author><keyname>Hao</keyname><forenames>Yitong</forenames></author><author><keyname>He</keyname><forenames>Enbo</forenames></author><author><keyname>Zhang</keyname><forenames>Yue</forenames></author><author><keyname>Yin</keyname><forenames>Guisheng</forenames></author></authors><title>Bi-directional Curriculum Learning for Graph Anomaly Detection: Dual   Focus on Homogeneity and Heterogeneity</title><categories>cs.LG cs.SI stat.ML</categories><comments>8pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph anomaly detection (GAD) aims to identify nodes from a graph that are significantly different from normal patterns. Most previous studies are model-driven, focusing on enhancing the detection effect by improving the model structure. However, these approaches often treat all nodes equally, neglecting the different contributions of various nodes to the training. Therefore, we introduce graph curriculum learning as a simple and effective plug-and-play module to optimize GAD methods. The existing graph curriculum learning mainly focuses on the homogeneity of graphs and treats nodes with high homogeneity as easy nodes. In fact, GAD models can handle not only graph homogeneity but also heterogeneity, which leads to the unsuitability of these existing methods. To address this problem, we propose an innovative Bi-directional Curriculum Learning strategy (BCL), which considers nodes with higher and lower similarity to neighbor nodes as simple nodes in the direction of focusing on homogeneity and focusing on heterogeneity, respectively, and prioritizes their training. Extensive experiments show that BCL can be quickly integrated into existing detection processes and significantly improves the performance of ten GAD anomaly detection models on seven commonly used datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14248</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14248</id><created>2025-01-24</created><authors><author><keyname>Deng</keyname><forenames>Fei</forenames><affiliation>Department of Chemical Biology, Ernest Mario School of Pharmacy, Rutgers University, Piscataway, NJ</affiliation></author><author><keyname>Feng</keyname><forenames>Catherine H</forenames><affiliation>Department of Chemical Biology, Ernest Mario School of Pharmacy, Rutgers University, Piscataway, NJ</affiliation><affiliation>Harvard University, Cambridge, MA</affiliation></author><author><keyname>Gao</keyname><forenames>Nan</forenames><affiliation>Department of Biological Sciences, School of Arts &amp; Sciences, Rutgers University, Newark, NJ</affiliation><affiliation>Department of Pharmacology, Physiology, and Neuroscience, New Jersey Medical School, Rutgers University, Newark, NJ</affiliation></author><author><keyname>Zhang</keyname><forenames>Lanjing</forenames><affiliation>Department of Chemical Biology, Ernest Mario School of Pharmacy, Rutgers University, Piscataway, NJ</affiliation><affiliation>Department of Pharmacology, Physiology, and Neuroscience, New Jersey Medical School, Rutgers University, Newark, NJ</affiliation><affiliation>Department of Pathology, Princeton Medical Center, Plainsboro, NJ</affiliation><affiliation>Rutgers Cancer Institute of New Jersey, New Brunswick, NJ</affiliation></author></authors><title>Normalization and selecting non-differentially expressed genes improve   machine learning modelling of cross-platform transcriptomic data</title><categories>q-bio.QM q-bio.GN stat.CO stat.ME</categories><comments>35 pages, 5 figures, 2 tables</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Normalization is a critical step in quantitative analyses of biological processes. Recent works show that cross-platform integration and normalization enable machine learning (ML) training on RNA microarray and RNA-seq data, but no independent datasets were used in their studies. Therefore, it is unclear how to improve ML modelling performance on independent RNA array and RNA-seq based datasets. Inspired by the house-keeping genes that are commonly used in experimental biology, this study tests the hypothesis that non-differentially expressed genes (NDEG) may improve normalization of transcriptomic data and subsequently cross-platform modelling performance of ML models. Microarray and RNA-seq datasets of the TCGA breast cancer were used as independent training and test datasets, respectively, to classify the molecular subtypes of breast cancer. NDEG (p&gt;0.85) and differentially expressed genes (DEG, p&lt;0.05) were selected based on the p values of ANOVA analysis and used for subsequent data normalization and classification, respectively. Models trained based on data from one platform were used for testing on the other platform. Our data show that NDEG and DEG gene selection could effectively improve the model classification performance. Normalization methods based on parametric statistical analysis were inferior to those based on nonparametric statistics. In this study, the LOG_QN and LOG_QNZ normalization methods combined with the neural network classification model seem to achieve better performance. Therefore, NDEG-based normalization appears useful for cross-platform testing on completely independent datasets. However, more studies are required to examine whether NDEG-based normalization can improve ML classification performance in other datasets and other omic data types. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14253</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14253</id><created>2025-01-24</created><authors><author><keyname>Tanaka</keyname><forenames>Tomonari</forenames></author><author><keyname>Hanada</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Yang</keyname><forenames>Hanting</forenames></author><author><keyname>Aoyama</keyname><forenames>Tatsuya</forenames></author><author><keyname>Inatsu</keyname><forenames>Yu</forenames></author><author><keyname>Akahane</keyname><forenames>Satoshi</forenames></author><author><keyname>Okura</keyname><forenames>Yoshito</forenames></author><author><keyname>Hashimoto</keyname><forenames>Noriaki</forenames></author><author><keyname>Murayama</keyname><forenames>Taro</forenames></author><author><keyname>Lee</keyname><forenames>Hanju</forenames></author><author><keyname>Kojima</keyname><forenames>Shinya</forenames></author><author><keyname>Takeuchi</keyname><forenames>Ichiro</forenames></author></authors><title>Distributionally Robust Coreset Selection under Covariate Shift</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Coreset selection, which involves selecting a small subset from an existing training dataset, is an approach to reducing training data, and various approaches have been proposed for this method. In practical situations where these methods are employed, it is often the case that the data distributions differ between the development phase and the deployment phase, with the latter being unknown. Thus, it is challenging to select an effective subset of training data that performs well across all deployment scenarios. We therefore propose Distributionally Robust Coreset Selection (DRCS). DRCS theoretically derives an estimate of the upper bound for the worst-case test error, assuming that the future covariate distribution may deviate within a defined range from the training distribution. Furthermore, by selecting instances in a way that suppresses the estimate of the upper bound for the worst-case test error, DRCS achieves distributionally robust training instance selection. This study is primarily applicable to convex training computation, but we demonstrate that it can also be applied to deep learning under appropriate approximations. In this paper, we focus on covariate shift, a type of data distribution shift, and demonstrate the effectiveness of DRCS through experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14291</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14291</id><created>2025-01-24</created><authors><author><keyname>Zhou</keyname><forenames>Feng</forenames></author><author><keyname>Kong</keyname><forenames>Quyu</forenames></author><author><keyname>Zhang</keyname><forenames>Yixuan</forenames></author></authors><title>Advances in Temporal Point Processes: Bayesian, Deep, and LLM Approaches</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Temporal point processes (TPPs) are stochastic process models used to characterize event sequences occurring in continuous time. Traditional statistical TPPs have a long-standing history, with numerous models proposed and successfully applied across diverse domains. In recent years, advances in deep learning have spurred the development of neural TPPs, enabling greater flexibility and expressiveness in capturing complex temporal dynamics. The emergence of large language models (LLMs) has further sparked excitement, offering new possibilities for modeling and analyzing event sequences by leveraging their rich contextual understanding. This survey presents a comprehensive review of recent research on TPPs from three perspectives: Bayesian, deep learning, and LLM approaches. We begin with a review of the fundamental concepts of TPPs, followed by an in-depth discussion of model design and parameter estimation techniques in these three frameworks. We also revisit classic application areas of TPPs to highlight their practical relevance. Finally, we outline challenges and promising directions for future research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14329</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14329</id><created>2025-01-24</created><authors><author><keyname>Gershoff</keyname><forenames>Matthew</forenames></author></authors><title>K-Anonymous A/B Testing</title><categories>stat.AP</categories><comments>13 pages, 7 tables</comments><msc-class>63</msc-class><acm-class>G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A core principle of Privacy by Design (PbD) is minimizing the data that is stored or shared about each individual respondent. PbD principles are mandated by the GDPR (see Article 5c and Article 25), as well as informing aspects of California Privacy Rights Act (CPRA). This paper describes a simple and effective approach that can be used in many a/b testing and similar contexts to help meet these PbD goals. Specifically, the method presented describes an approach to run OLS regression on k-anonymized data. To help illustrate the general utility of this approach, descriptions of two important use cases are offered: 1) calculating partial f-tests as a simple way to both check for a/b test interactions and to test for heterogeneity of treatment effects; and 2) regression adjustment using an approach similar to the popular CUPED method, as a variance reduction method for a/b tests. Using this method has advantages for privacy and compliance, as well as often reducing data storage and processing costs, by storing, sharing, or analyzing only aggregate level rather than individual level data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14351</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14351</id><created>2025-01-24</created><authors><author><keyname>Ma</keyname><forenames>Jian</forenames></author></authors><title>Facies Classification with Copula Entropy</title><categories>cs.LG physics.geo-ph stat.AP</categories><comments>12 pages, 5 figures, 3 tables. arXiv admin note: text overlap with   arXiv:2310.16633</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose to apply copula entropy (CE) to facies classification. In our method, the correlations between geological variables and facies classes are measured with CE and then the variables associated with large negative CEs are selected for classification. We verified the proposed method on a typical facies dataset for facies classification and the experimental results show that the proposed method can select less geological variables for facies classification without sacrificing classification performance. The geological variables such selected are also interpretable to geologists with geological meanings due to the rigorous definition of CE. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14430</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14430</id><created>2025-01-24</created><authors><author><keyname>Zhiyanov</keyname><forenames>Anton</forenames></author><author><keyname>Shklyaev</keyname><forenames>Alexander</forenames></author><author><keyname>Galatenko</keyname><forenames>Alexey</forenames></author><author><keyname>Galatenko</keyname><forenames>Vladimir</forenames></author><author><keyname>Tonevitsky</keyname><forenames>Alexander</forenames></author></authors><title>Statistical Verification of Linear Classifiers</title><categories>stat.ML cs.LG math.PR math.ST stat.AP stat.TH</categories><comments>16 pages, 3 figures</comments><msc-class>62P10</msc-class><acm-class>G.3</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a homogeneity test closely related to the concept of linear separability between two samples. Using the test one can answer the question whether a linear classifier is merely ``random'' or effectively captures differences between two classes. We focus on establishing upper bounds for the test's \emph{p}-value when applied to two-dimensional samples. Specifically, for normally distributed samples we experimentally demonstrate that the upper bound is highly accurate. Using this bound, we evaluate classifiers designed to detect ER-positive breast cancer recurrence based on gene pair expression. Our findings confirm significance of IGFBP6 and ELOVL5 genes in this process. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14437</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14437</id><created>2025-01-24</created><authors><author><keyname>Helbic</keyname><forenames>Marco</forenames></author><author><keyname>Hagenauer</keyname><forenames>Julian</forenames></author><author><keyname>Burov</keyname><forenames>Angel</forenames></author><author><keyname>Dzhambov</keyname><forenames>Angel M.</forenames></author></authors><title>Traffic noise assessment in urban Bulgaria using explainable machine   learning</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Fine-grained noise maps are vital for epidemiological studies on traffic noise. However, detailed information on traffic noise is often limited, especially in Eastern Europe. Rigid linear noise land-use regressions are typically employed to estimate noise levels; however, machine learning likely offers more accurate noise predictions. We innovated by comparing the predictive accuracies of supervised machine learning models to estimate traffic noise levels across the five largest Bulgarian cities. In situ A-weighted equivalent continuous sound levels were obtained from 232 fixed-site monitors across these cities. We included transport- and land-use-related predictors using 50-1,000 m buffers. Extreme gradient boosting (XGB) had the highest ten-fold cross-validated fit (R2=0.680) and the lowest root mean square error (RMSE=4.739), insignificantly besting the random forest-based model (R2=0.667, RMSE=4.895). Support vector regression (R2=0.633, RMSE=5.358), elastic net (R2=0.568, RMSE=5.625), and linear regression (R2=0.548, RMSE=5.569) performed significantly worse. Shapley values for the XGB showed that the length of major roads within 100 m buffers, footways within 50 m buffers, residential roads within 50 m buffers, and the number of buildings within 50 m buffers were important non-linear predictors. Our spatially resolved noise maps revealed striking geographic noise variations and that, on average, 96.8% of the urban population experiences harmful noise levels. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14466</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14466</id><created>2025-01-24</created><authors><author><keyname>Yuksel</keyname><forenames>Goksenin</forenames></author><author><keyname>Kamps</keyname><forenames>Jaap</forenames></author></authors><title>On Correlating Factors for Domain Adaptation Performance</title><categories>cs.IR stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Dense retrievers have demonstrated significant potential for neural information retrieval; however, they lack robustness to domain shifts, limiting their efficacy in zero-shot settings across diverse domains. In this paper, we set out to analyze the possible factors that lead to successful domain adaptation of dense retrievers. We include domain similarity proxies between generated queries to test and source domains. Furthermore, we conduct a case study comparing two powerful domain adaptation techniques. We find that generated query type distribution is an important factor, and generating queries that share a similar domain to the test documents improves the performance of domain adaptation methods. This study further emphasizes the importance of domain-tailored generated queries. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14482</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14482</id><created>2025-01-24</created><authors><author><keyname>Riley</keyname><forenames>Richard D</forenames></author><author><keyname>Collins</keyname><forenames>Gary S</forenames></author><author><keyname>Archer</keyname><forenames>Lucinda</forenames></author><author><keyname>Whittle</keyname><forenames>Rebecca</forenames></author><author><keyname>Legha</keyname><forenames>Amardeep</forenames></author><author><keyname>Kirton</keyname><forenames>Laura</forenames></author><author><keyname>Dhiman</keyname><forenames>Paula</forenames></author><author><keyname>Sadatsafavi</keyname><forenames>Mohsen</forenames></author><author><keyname>Adderley</keyname><forenames>Nicola J</forenames></author><author><keyname>Alderman</keyname><forenames>Joseph</forenames></author><author><keyname>Martin</keyname><forenames>Glen P</forenames></author><author><keyname>Ensor</keyname><forenames>Joie</forenames></author></authors><title>A decomposition of Fisher's information to inform sample size for   developing fair and precise clinical prediction models -- Part 2:   time-to-event outcomes</title><categories>stat.ME</categories><comments>arXiv admin note: text overlap with arXiv:2407.09293</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Background: When developing a clinical prediction model using time-to-event data, previous research focuses on the sample size to minimise overfitting and precisely estimate the overall risk. However, instability of individual-level risk estimates may still be large. Methods: We propose a decomposition of Fisher's information matrix to examine and calculate the sample size required for developing a model that aims for precise and fair risk estimates. We propose a six-step process which can be used before data collection or when an existing dataset is available. Steps (1) to (5) require researchers to specify the overall risk in the target population at a key time-point of interest; an assumed pragmatic 'core model' in the form of an exponential regression model; the (anticipated) joint distribution of core predictors included in that model; and the distribution of any censoring. Results: We derive closed-form solutions that decompose the variance of an individual's estimated event rate into Fisher's unit information matrix, predictor values and total sample size; this allows researchers to calculate and examine uncertainty distributions around individual risk estimates and misclassification probabilities for specified sample sizes. We provide an illustrative example in breast cancer and emphasise the importance of clinical context, including risk thresholds for decision making, and examine fairness concerns for pre- and post-menopausal women. Lastly, in two empirical evaluations, we provide reassurance that uncertainty interval widths based on our approach are close to using more flexible models. Conclusions: Our approach allows users to identify the (target) sample size required to develop a prediction model for time-to-event outcomes, via the pmstabilityss module. It aims to facilitate models with improved trust, reliability and fairness in individual-level predictions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14485</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14485</id><created>2025-01-24</created><authors><author><keyname>Norkin</keyname><forenames>Vladimir</forenames></author><author><keyname>Pichler</keyname><forenames>Alois</forenames></author></authors><title>Models Parametric Analysis via Adaptive Kernel Learning</title><categories>math.OC math.ST stat.TH</categories><comments>To appear as a paper in Springer's series "Studies in Systems,   Decision and Control"</comments><msc-class>62G08, 62M45, 68Q32</msc-class><acm-class>I.2.6</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Any applied mathematical model contains parameters. The paper proposes to use kernel learning for the parametric analysis of the model. The approach consists in setting a distribution on the parameter space, obtaining a finite training sample from this distribution, solving the problem for each parameter value from this sample, and constructing a kernel approximation of the parametric dependence on the entire set of parameter values. The kernel approximation is obtained by minimizing the approximation error on the training sample and adjusting kernel parameters (width) on the same or another independent sample of parameters. This approach to learning complex dependencies is called kernel learning (or kernel SVM). Traditionally, kernel learning is considered in the so-called Reproducing Kernel Hilbert Space (RKHS) with a fixed kernel. The novelty of our approach is that we consider the kernel learning in a broad subspace of square-integrable functions with a corresponding L_2-norm regularization. This subspace contains linear combinations of kernel functions with kernels of different shapes at different data points. The approach essentially uses a derived analytical representation of the L2-norm for kernel functions. Thus the approach substantially extends the flexibility of the traditional kernel SVM for account of the number of adjusted parameters and minimization of the training error not only over weights of kernels but also over their shapes. The important issue of selecting the optimal regularization parameter is resolved by minimizing the test error over this parameter. Numerical illustrations are provided. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14544</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14544</id><created>2025-01-24</created><authors><author><keyname>Wen</keyname><forenames>Haifeng</forenames></author><author><keyname>Xing</keyname><forenames>Hong</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author></authors><title>Distributed Conformal Prediction via Message Passing</title><categories>cs.LG cs.AI stat.ML</categories><comments>16 pages, 11 figures, submitted for posssible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Post-hoc calibration of pre-trained models is critical for ensuring reliable inference, especially in safety-critical domains such as healthcare. Conformal Prediction (CP) offers a robust post-hoc calibration framework, providing distribution-free statistical coverage guarantees for prediction sets by leveraging held-out datasets. In this work, we address a decentralized setting where each device has limited calibration data and can communicate only with its neighbors over an arbitrary graph topology. We propose two message-passing-based approaches for achieving reliable inference via CP: quantile-based distributed conformal prediction (Q-DCP) and histogram-based distributed conformal prediction (H-DCP). Q-DCP employs distributed quantile regression enhanced with tailored smoothing and regularization terms to accelerate convergence, while H-DCP uses a consensus-based histogram estimation approach. Through extensive experiments, we investigate the trade-offs between hyperparameter tuning requirements, communication overhead, coverage guarantees, and prediction set sizes across different network topologies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14570</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14570</id><created>2025-01-24</created><authors><author><keyname>Meehinkong</keyname><forenames>Panisara</forenames></author><author><keyname>Ponnoprat</keyname><forenames>Donlapark</forenames></author></authors><title>coverforest: Conformal Predictions with Random Forest in Python</title><categories>stat.ML cs.LG stat.CO</categories><comments>In peer review</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Conformal prediction provides a framework for uncertainty quantification, specifically in the forms of prediction intervals and sets with distribution-free guaranteed coverage. While recent cross-conformal techniques such as CV+ and Jackknife+-after-bootstrap achieve better data efficiency than traditional split conformal methods, they incur substantial computational costs due to required pairwise comparisons between training and test samples' out-of-bag scores. Observing that these methods naturally extend from ensemble models, particularly random forests, we leverage existing optimized random forest implementations to enable efficient cross-conformal predictions.   We present coverforest, a Python package that implements efficient conformal prediction methods specifically optimized for random forests. coverforest supports both regression and classification tasks through various conformal prediction methods, including split conformal, CV+, Jackknife+-after-bootstrap, and adaptive prediction sets. Our package leverages parallel computing and Cython optimizations to speed up out-of-bag calculations. Our experiments demonstrate that coverforest's predictions achieve the desired level of coverage. In addition, its training and prediction times can be faster than an existing implementation by 2--9 times. The source code for the coverforest is hosted on GitHub at https://github.com/donlapark/coverforest. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14594</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14594</id><created>2025-01-24</created><authors><author><keyname>Bercu</keyname><forenames>Bernard</forenames></author></authors><title>On the multidimensional elephant random walk with stops</title><categories>math.PR math.ST stat.TH</categories><msc-class>60G50, 60G42, 60F05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to investigate the asymptotic behavior of the multidimensional elephant random walk with stops (MERWS). In contrast with the standard elephant random walk, the elephant is allowed to stay on his own position. We prove that the Gram matrix associated with the MERWS, properly normalized, converges almost surely to the product of a deterministic matrix, related to the axes on which the MERWS moves uniformly, and a Mittag-Leffler distribution. It allows us to extend all the results previously established for the one-dimensional elephant random walk with stops. More precisely, in the diffusive and critical regimes, we prove the almost sure convergence of the MERWS. In the superdiffusive regime, we establish the almost sure convergence of the MERWS, properly normalized, to a nondegenerate random vector. We also study the self-normalized asymptotic normality of the MERWS. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14602</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14602</id><created>2025-01-24</created><authors><author><keyname>Yu</keyname><forenames>Haoyang</forenames></author><author><keyname>Ma</keyname><forenames>Wei</forenames></author><author><keyname>Liu</keyname><forenames>Hanzhong</forenames></author></authors><title>Minimax Optimal Design with Spillover and Carryover Effects</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In various applications, the potential outcome of a unit may be influenced by the treatments received by other units, a phenomenon known as interference, as well as by prior treatments, referred to as carryover effects. These phenomena violate the stable unit treatment value assumption and pose significant challenges in causal inference. To address these complexities, we propose a minimax optimal experimental design that simultaneously accounts for both spillover and carryover effects, enhancing the precision of estimates for direct and spillover effects. This method is particularly applicable to multi-unit experiments, reducing sample size requirements and experimental costs. We also investigate the asymptotic properties of the Horvitz--Thompson estimators of direct and spillover effects, demonstrating their consistency and asymptotic normality under the minimax optimal design. To facilitate valid inferences, we propose conservative variance estimators. Furthermore, we tackle the challenges associated with potential misspecifications in the order of carryover effects. Our approach is validated by comprehensive numerical studies that demonstrate superior performance compared to existing experimental designs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14616</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14616</id><created>2025-01-24</created><authors><author><keyname>Liu</keyname><forenames>Yen-Chun</forenames></author><author><keyname>Mak</keyname><forenames>Simon</forenames></author></authors><title>QuIP: Experimental design for expensive simulators with many Qualitative   factors via Integer Programming</title><categories>stat.AP cs.RO</categories><comments>40 pages, 6 figures, submitted to JCGS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need to explore and/or optimize expensive simulators with many qualitative factors arises in broad scientific and engineering problems. Our motivating application lies in path planning - the exploration of feasible paths for navigation, which plays an important role in robotics, surgical planning and assembly planning. Here, the feasibility of a path is evaluated via expensive virtual experiments, and its parameter space is typically discrete and high-dimensional. A carefully selected experimental design is thus essential for timely decision-making. We propose here a novel framework, called QuIP, for experimental design of Qualitative factors via Integer Programming under a Gaussian process surrogate model with an exchangeable covariance function. For initial design, we show that its asymptotic D-optimal design can be formulated as a variant of the well-known assignment problem in operations research, which can be efficiently solved to global optimality using state-of-the-art integer programming solvers. For sequential design (specifically, for active learning or black-box optimization), we show that its design criterion can similarly be formulated as an assignment problem, thus enabling efficient and reliable optimization with existing solvers. We then demonstrate the effectiveness of QuIP over existing methods in a suite of path planning experiments and an application to rover trajectory optimization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14635</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14635</id><created>2025-01-24</created><authors><author><keyname>Kim</keyname><forenames>Kaheon</forenames></author><author><keyname>Yao</keyname><forenames>Rentian</forenames></author><author><keyname>Zhu</keyname><forenames>Changbo</forenames></author><author><keyname>Chen</keyname><forenames>Xiaohui</forenames></author></authors><title>Optimal Transport Barycenter via Nonconvex-Concave Minimax Optimization</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The optimal transport barycenter (a.k.a. Wasserstein barycenter) is a fundamental notion of averaging that extends from the Euclidean space to the Wasserstein space of probability distributions. Computation of the unregularized barycenter for discretized probability distributions on point clouds is a challenging task when the domain dimension $d &gt; 1$. Most practical algorithms for approximating the barycenter problem are based on entropic regularization. In this paper, we introduce a nearly linear time $O(m \log{m})$ and linear space complexity $O(m)$ primal-dual algorithm, the Wasserstein-Descent $\dot{\mathbb{H}}^1$-Ascent (WDHA) algorithm, for computing the exact barycenter when the input probability density functions are discretized on an $m$-point grid. The key success of the WDHA algorithm hinges on alternating between two different yet closely related Wasserstein and Sobolev optimization geometries for the primal barycenter and dual Kantorovich potential subproblems. Under reasonable assumptions, we establish the convergence rate and iteration complexity of WDHA to its stationary point when the step size is appropriately chosen. Superior computational efficacy, scalability, and accuracy over the existing Sinkhorn-type algorithms are demonstrated on high-resolution (e.g., $1024 \times 1024$ images) 2D synthetic and real data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14698</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14698</id><created>2025-01-24</created><authors><author><keyname>Wang</keyname><forenames>Qi</forenames></author><author><keyname>Parker</keyname><forenames>Paul A.</forenames></author><author><keyname>Lund</keyname><forenames>Robert B.</forenames></author></authors><title>Hierarchical Count Echo State Network Models with Application to   Graduate Student Enrollments</title><categories>stat.ME stat.AP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Poisson autoregressive count models have evolved into a time series staple for correlated count data. This paper proposes an alternative to Poisson autoregressions: count echo state networks. Echo state networks can be statistically analyzed in frequentist manners via optimizing penalized likelihoods, or in Bayesian manners via MCMC sampling. This paper develops Poisson echo state techniques for count data and applies them to a massive count data set containing the number of graduate students from 1,758 United States universities during the years 1972-2021 inclusive. Negative binomial models are also implemented to better handle overdispersion in the counts. Performance of the proposed models are compared via their forecasting performance as judged by several methods. In the end, a hierarchical negative binomial based echo state network is judged as the superior model. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14710</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14710</id><created>2025-01-24</created><authors><author><keyname>Leininger</keyname><forenames>Charlotte</forenames></author><author><keyname>Rittel</keyname><forenames>Simon</forenames></author><author><keyname>Bothmann</keyname><forenames>Ludwig</forenames></author></authors><title>Overcoming Fairness Trade-offs via Pre-processing: A Causal Perspective</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Training machine learning models for fair decisions faces two key challenges: The \emph{fairness-accuracy trade-off} results from enforcing fairness which weakens its predictive performance in contrast to an unconstrained model. The incompatibility of different fairness metrics poses another trade-off -- also known as the \emph{impossibility theorem}. Recent work identifies the bias within the observed data as a possible root cause and shows that fairness and predictive performance are in fact in accord when predictive performance is measured on unbiased data. We offer a causal explanation for these findings using the framework of the FiND (fictitious and normatively desired) world, a "fair" world, where protected attributes have no causal effects on the target variable. We show theoretically that (i) classical fairness metrics deemed to be incompatible are naturally satisfied in the FiND world, while (ii) fairness aligns with high predictive performance. We extend our analysis by suggesting how one can benefit from these theoretical insights in practice, using causal pre-processing methods that approximate the FiND world. Additionally, we propose a method for evaluating the approximation of the FiND world via pre-processing in practical use cases where we do not have access to the FiND world. In simulations and empirical studies, we demonstrate that these pre-processing methods are successful in approximating the FiND world and resolve both trade-offs. Our results provide actionable solutions for practitioners to achieve fairness and high predictive performance simultaneously. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14724</identifier><datestamp>2025-01-27</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14724</id><created>2025-01-24</created><authors><author><keyname>Terjék</keyname><forenames>Dávid</forenames></author><author><keyname>González-Sánchez</keyname><forenames>Diego</forenames></author></authors><title>MLPs at the EOC: Concentration of the NTK</title><categories>cs.LG stat.ML</categories><comments>36 pages, 1 figure</comments><msc-class>68T07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the concentration of the Neural Tangent Kernel (NTK) $K_\theta : \mathbb{R}^{m_0} \times \mathbb{R}^{m_0} \to \mathbb{R}^{m_l \times m_l}$ of $l$-layer Multilayer Perceptrons (MLPs) $N : \mathbb{R}^{m_0} \times \Theta \to \mathbb{R}^{m_l}$ equipped with activation functions $\phi(s) = a s + b \vert s \vert$ for some $a,b \in \mathbb{R}$ with the parameter $\theta \in \Theta$ being initialized at the Edge Of Chaos (EOC). Without relying on the gradient independence assumption that has only been shown to hold asymptotically in the infinitely wide limit, we prove that an approximate version of gradient independence holds at finite width. Showing that the NTK entries $K_\theta(x_{i_1},x_{i_2})$ for $i_1,i_2 \in [1:n]$ over a dataset $\{x_1,\cdots,x_n\} \subset \mathbb{R}^{m_0}$ concentrate simultaneously via maximal inequalities, we prove that the NTK matrix $K(\theta) = [\frac{1}{n} K_\theta(x_{i_1},x_{i_2}) : i_1,i_2 \in [1:n]] \in \mathbb{R}^{nm_l \times nm_l}$ concentrates around its infinitely wide limit $\overset{\scriptscriptstyle\infty}{K} \in \mathbb{R}^{nm_l \times nm_l}$ without the need for linear overparameterization. Our results imply that in order to accurately approximate the limit, hidden layer widths have to grow quadratically as $m_k = k^2 m$ for some $m \in \mathbb{N}+1$ for sufficient concentration. For such MLPs, we obtain the concentration bound $\mathbb{P}( \Vert K(\theta) - \overset{\scriptscriptstyle\infty}{K} \Vert \leq O((\Delta_\phi^{-2} + m_l^{\frac{1}{2}} l) \kappa_\phi^2 m^{-\frac{1}{2}})) \geq 1-O(m^{-1})$ modulo logarithmic terms, where we denoted $\Delta_\phi = \frac{b^2}{a^2+b^2}$ and $\kappa_\phi = \frac{\vert a \vert + \vert b \vert}{\sqrt{a^2 + b^2}}$. This reveals in particular that the absolute value ($\Delta_\phi=1$, $\kappa_\phi=1$) beats the ReLU ($\Delta_\phi=\frac{1}{2}$, $\kappa_\phi=\sqrt{2}$) in terms of the concentration of the NTK. </abstract></arXiv></metadata></record>
