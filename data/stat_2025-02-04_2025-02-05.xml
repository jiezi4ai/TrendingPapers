<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:1910.13398</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>1910.13398</id><created>2019-10-29</created><updated>2025-01-31</updated><authors><author><keyname>Lin</keyname><forenames>Wu</forenames></author><author><keyname>Khan</keyname><forenames>Mohammad Emtiyaz</forenames></author><author><keyname>Schmidt</keyname><forenames>Mark</forenames></author></authors><title>Stein's Lemma for the Reparameterization Trick with Exponential Family   Mixtures</title><categories>stat.ML cs.LG</categories><comments>fixed some typos in the main text</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Stein's method (Stein, 1973; 1981) is a powerful tool for statistical applications and has significantly impacted machine learning. Stein's lemma plays an essential role in Stein's method. Previous applications of Stein's lemma either required strong technical assumptions or were limited to Gaussian distributions with restricted covariance structures. In this work, we extend Stein's lemma to exponential-family mixture distributions, including Gaussian distributions with full covariance structures. Our generalization enables us to establish a connection between Stein's lemma and the reparameterization trick to derive gradients of expectations of a large class of functions under weak assumptions. Using this connection, we can derive many new reparameterizable gradient identities that go beyond the reach of existing works. For example, we give gradient identities when the expectation is taken with respect to Student's t-distribution, skew Gaussian, exponentially modified Gaussian, and normal inverse Gaussian. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2002.05308</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2002.05308</id><created>2020-02-12</created><updated>2025-02-01</updated><authors><author><keyname>Kato</keyname><forenames>Masahiro</forenames></author><author><keyname>Ishihara</keyname><forenames>Takuya</forenames></author><author><keyname>Honda</keyname><forenames>Junya</forenames></author><author><keyname>Narita</keyname><forenames>Yusuke</forenames></author></authors><title>Efficient Adaptive Experimental Design for Average Treatment Effect   Estimation</title><categories>stat.ML cs.LG econ.EM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study how to efficiently estimate average treatment effects (ATEs) using adaptive experiments. In adaptive experiments, experimenters sequentially assign treatments to experimental units while updating treatment assignment probabilities based on past data. We start by defining the efficient treatment-assignment probability, which minimizes the semiparametric efficiency bound for ATE estimation. Our proposed experimental design estimates and uses the efficient treatment-assignment probability to assign treatments. At the end of the proposed design, the experimenter estimates the ATE using a newly proposed Adaptive Augmented Inverse Probability Weighting (A2IPW) estimator. We show that the asymptotic variance of the A2IPW estimator using data from the proposed design achieves the minimized semiparametric efficiency bound. We also analyze the estimator's finite-sample properties and develop nonparametric and nonasymptotic confidence intervals that are valid at any round of the proposed design. These anytime valid confidence intervals allow us to conduct rate-optimal sequential hypothesis testing, allowing for early stopping and reducing necessary sample size. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2103.01648</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2103.01648</id><created>2021-03-02</created><updated>2022-04-25</updated><authors><author><keyname>González</keyname><forenames>Mario</forenames></author><author><keyname>Almansa</keyname><forenames>Andrés</forenames></author><author><keyname>Tan</keyname><forenames>Pauline</forenames></author></authors><title>Solving Inverse Problems by Joint Posterior Maximization with   Autoencoding Prior</title><categories>stat.ML cs.CV cs.LG eess.IV math.OC</categories><comments>arXiv admin note: text overlap with arXiv:1911.06379</comments><report-no>https://hal.science/hal-03151455/</report-no><journal-ref>SIAM Journal on Imaging Sciences, Vol. 15, Iss. 2 (2022)   10.1137/21M140225X</journal-ref><doi>10.1137/21M140225X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we address the problem of solving ill-posed inverse problems in imaging where the prior is a variational autoencoder (VAE). Specifically we consider the decoupled case where the prior is trained once and can be reused for many different log-concave degradation models without retraining. Whereas previous MAP-based approaches to this problem lead to highly non-convex optimization algorithms, our approach computes the joint (space-latent) MAP that naturally leads to alternate optimization algorithms and to the use of a stochastic encoder to accelerate computations. The resulting technique (JPMAP) performs Joint Posterior Maximization using an Autoencoding Prior. We show theoretical and experimental evidence that the proposed objective function is quite close to bi-convex. Indeed it satisfies a weak bi-convexity property which is sufficient to guarantee that our optimization scheme converges to a stationary point. We also highlight the importance of correctly training the VAE using a denoising criterion, in order to ensure that the encoder generalizes well to out-of-distribution images, without affecting the quality of the generative model. This simple modification is key to providing robustness to the whole procedure. Finally we show how our joint MAP methodology relates to more common MAP approaches, and we propose a continuation scheme that makes use of our JPMAP algorithm to provide more robust MAP estimates. Experimental results also show the higher quality of the solutions obtained by our JPMAP approach with respect to other non-convex MAP approaches which more often get stuck in spurious local optima. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2103.04715</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2103.04715</id><created>2021-03-08</created><updated>2022-01-12</updated><authors><author><keyname>Laumont</keyname><forenames>Rémi</forenames></author><author><keyname>de Bortoli</keyname><forenames>Valentin</forenames></author><author><keyname>Almansa</keyname><forenames>Andrés</forenames></author><author><keyname>Delon</keyname><forenames>Julie</forenames></author><author><keyname>Durmus</keyname><forenames>Alain</forenames></author><author><keyname>Pereyra</keyname><forenames>Marcelo</forenames></author></authors><title>Bayesian imaging using Plug &amp; Play priors: when Langevin meets Tweedie</title><categories>stat.ME cs.CV eess.IV math.ST stat.ML stat.TH</categories><report-no>https://hal.science/hal-03161400/</report-no><msc-class>65K10, 65K05, 65D18, 62F15, 62C10, 68Q25, 68U10, 90C26</msc-class><journal-ref>SIAM Journal on Imaging Sciences, Volume 15, Issue 2 (2022)</journal-ref><doi>10.1137/21M1406349</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the seminal work of Venkatakrishnan et al. in 2013, Plug &amp; Play (PnP) methods have become ubiquitous in Bayesian imaging. These methods derive Minimum Mean Square Error (MMSE) or Maximum A Posteriori (MAP) estimators for inverse problems in imaging by combining an explicit likelihood function with a prior that is implicitly defined by an image denoising algorithm. The PnP algorithms proposed in the literature mainly differ in the iterative schemes they use for optimisation or for sampling. In the case of optimisation schemes, some recent works guarantee the convergence to a fixed point, albeit not necessarily a MAP estimate. In the case of sampling schemes, to the best of our knowledge, there is no known proof of convergence. There also remain important open questions regarding whether the underlying Bayesian models and estimators are well defined, well-posed, and have the basic regularity properties required to support these numerical schemes. To address these limitations, this paper develops theory, methods, and provably convergent algorithms for performing Bayesian inference with PnP priors. We introduce two algorithms: 1) PnP-ULA (Unadjusted Langevin Algorithm) for Monte Carlo sampling and MMSE inference; and 2) PnP-SGD (Stochastic Gradient Descent) for MAP inference. Using recent results on the quantitative convergence of Markov chains, we establish detailed convergence guarantees for these two algorithms under realistic assumptions on the denoising operators used, with special attention to denoisers based on deep neural networks. We also show that these algorithms approximately target a decision-theoretically optimal Bayesian model that is well-posed. The proposed algorithms are demonstrated on several canonical problems such as image deblurring, inpainting, and denoising, where they are used for point estimation as well as for uncertainty visualisation and quantification. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2109.11647</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2109.11647</id><created>2021-09-23</created><updated>2025-02-01</updated><authors><author><keyname>Munro</keyname><forenames>Evan</forenames></author><author><keyname>Kuang</keyname><forenames>Xu</forenames></author><author><keyname>Wager</keyname><forenames>Stefan</forenames></author></authors><title>Treatment Effects in Market Equilibrium</title><categories>econ.EM stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Policy-relevant treatment effect estimation in a marketplace setting requires taking into account both the direct benefit of the treatment and any spillovers induced by changes to the market equilibrium. The standard way to address these challenges is to evaluate interventions via cluster-randomized experiments, where each cluster corresponds to an isolated market. This approach, however, cannot be used when we only have access to a single market (or a small number of markets). Here, we show how to identify and estimate policy-relevant treatment effects using a unit-level randomized trial run within a single large market. A standard Bernoulli-randomized trial allows consistent estimation of direct effects, and of treatment heterogeneity measures that can be used for welfare-improving targeting. Estimating spillovers - as well as providing confidence intervals for the direct effect - requires estimates of price elasticities, which we provide using an augmented experimental design. Our results rely on all spillovers being mediated via the (observed) prices of a finite number of traded goods, and the market power of any single unit decaying as the market gets large. We illustrate our results using a simulation calibrated to a conditional cash transfer experiment in the Philippines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2111.14207</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2111.14207</id><created>2021-11-28</created><authors><author><keyname>Wiemann</keyname><forenames>Paul F. V.</forenames></author><author><keyname>Kneib</keyname><forenames>Thomas</forenames></author><author><keyname>Hambuckers</keyname><forenames>Julien</forenames></author></authors><title>Using the Softplus Function to Construct Alternative Link Functions in   Generalized Linear Models and Beyond</title><categories>stat.ME</categories><doi>10.1007/s00362-023-01509-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Response functions linking regression predictors to properties of the response distribution are fundamental components in many statistical models. However, the choice of these functions is typically based on the domain of the modeled quantities and is not further scrutinized. For example, the exponential response function is usually assumed for parameters restricted to be positive although it implies a multiplicative model which may not necessarily be desired. Consequently, applied researchers might easily face misleading results when relying on defaults without further investigation. As an alternative to the exponential response function, we propose the use of the softplus function to construct alternative link functions for parameters restricted to be positive. As a major advantage, we can construct differentiable link functions corresponding closely to the identity function for positive values of the regression predictor, which implies an quasi-additive model and thus allows for an additive interpretation of the estimated effects by practitioners. We demonstrate the applicability of the softplus response function using both simulations and real data. In four applications featuring count data regression and Bayesian distributional regression, we contrast our approach to the commonly used exponential response function. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2201.06133</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2201.06133</id><created>2022-01-16</created><authors><author><keyname>Laumont</keyname><forenames>Rémi</forenames></author><author><keyname>de Bortoli</keyname><forenames>Valentin</forenames></author><author><keyname>Almansa</keyname><forenames>Andrés</forenames></author><author><keyname>Delon</keyname><forenames>Julie</forenames></author><author><keyname>Durmus</keyname><forenames>Alain</forenames></author><author><keyname>Pereyra</keyname><forenames>Marcelo</forenames></author></authors><title>On Maximum-a-Posteriori estimation with Plug &amp; Play priors and   stochastic gradient descent</title><categories>stat.ML cs.CV cs.LG eess.IV math.OC</categories><report-no>https://hal.science/hal-03348735/</report-no><msc-class>65K10 (Primary) 65K05, 62F15, 62C10, 68Q25, 68U10, 90C26 (Secondary)   65K10, 65K05, 62F15, 62C10, 68Q25, 68U10, 90C26</msc-class><journal-ref>Journal of Mathematical Imaging and Vision, Volume 65, Pages   140-163 (2023)</journal-ref><doi>10.1007/s10851-022-01134-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian methods to solve imaging inverse problems usually combine an explicit data likelihood function with a prior distribution that explicitly models expected properties of the solution. Many kinds of priors have been explored in the literature, from simple ones expressing local properties to more involved ones exploiting image redundancy at a non-local scale. In a departure from explicit modelling, several recent works have proposed and studied the use of implicit priors defined by an image denoising algorithm. This approach, commonly known as Plug &amp; Play (PnP) regularisation, can deliver remarkably accurate results, particularly when combined with state-of-the-art denoisers based on convolutional neural networks. However, the theoretical analysis of PnP Bayesian models and algorithms is difficult and works on the topic often rely on unrealistic assumptions on the properties of the image denoiser. This papers studies maximum-a-posteriori (MAP) estimation for Bayesian models with PnP priors. We first consider questions related to existence, stability and well-posedness, and then present a convergence proof for MAP computation by PnP stochastic gradient descent (PnP-SGD) under realistic assumptions on the denoiser used. We report a range of imaging experiments demonstrating PnP-SGD as well as comparisons with other PnP schemes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2207.01789</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2207.01789</id><created>2022-07-04</created><updated>2024-07-08</updated><authors><author><keyname>Zhang</keyname><forenames>Richard Y.</forenames></author></authors><title>Improved Global Guarantees for the Nonconvex Burer--Monteiro   Factorization via Rank Overparameterization</title><categories>math.OC cs.LG stat.ML</categories><journal-ref>Mathematical Programming, 2024</journal-ref><doi>10.1007/s10107-024-02160-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider minimizing a twice-differentiable, $L$-smooth, and $\mu$-strongly convex objective $\phi$ over an $n\times n$ positive semidefinite matrix $M\succeq0$, under the assumption that the minimizer $M^{\star}$ has low rank $r^{\star}\ll n$. Following the Burer--Monteiro approach, we instead minimize the nonconvex objective $f(X)=\phi(XX^{T})$ over a factor matrix $X$ of size $n\times r$. This substantially reduces the number of variables from $O(n^{2})$ to as few as $O(n)$ and also enforces positive semidefiniteness for free, but at the cost of giving up the convexity of the original problem. In this paper, we prove that if the search rank $r\ge r^{\star}$ is overparameterized by a \emph{constant factor} with respect to the true rank $r^{\star}$, namely as in $r&gt;\frac{1}{4}(L/\mu-1)^{2}r^{\star}$, then despite nonconvexity, local optimization is guaranteed to globally converge from any initial point to the global optimum. This significantly improves upon a previous rank overparameterization threshold of $r\ge n$, which we show is sharp in the absence of smoothness and strong convexity, but would increase the number of variables back up to $O(n^{2})$. Conversely, without rank overparameterization, we prove that such a global guarantee is possible if and only if $\phi$ is almost perfectly conditioned, with a condition number of $L/\mu&lt;3$. Therefore, we conclude that a small amount of overparameterization can lead to large improvements in theoretical guarantees for the nonconvex Burer--Monteiro factorization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2207.05671</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2207.05671</id><created>2022-07-12</created><updated>2023-02-01</updated><authors><author><keyname>Wickett</keyname><forenames>Eugene</forenames></author><author><keyname>Plumlee</keyname><forenames>Matthew</forenames></author><author><keyname>Smilowitz</keyname><forenames>Karen</forenames></author><author><keyname>Phanouvong</keyname><forenames>Souly</forenames></author><author><keyname>Pribluda</keyname><forenames>Victor</forenames></author></authors><title>Inferring sources of substandard and falsified products in   pharmaceutical supply chains</title><categories>stat.AP</categories><journal-ref>IISE.Trans. 56(3) (2023) 241-256</journal-ref><doi>10.1080/24725854.2023.2174277</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Substandard and falsified pharmaceuticals, prevalent in low- and middle-income countries, substantially increase levels of morbidity, mortality and drug resistance. Regulatory agencies combat this problem using post-market surveillance by collecting and testing samples where consumers purchase products. Existing analysis tools for post-market surveillance data focus attention on the locations of positive samples. This paper looks to expand such analysis through underutilized supply-chain information to provide inference on sources of substandard and falsified products. We first establish the presence of unidentifiability issues when integrating this supply-chain information with surveillance data. We then develop a Bayesian methodology for evaluating substandard and falsified sources that extracts utility from supply-chain information and mitigates unidentifiability while accounting for multiple sources of uncertainty. Using de-identified surveillance data, we show the proposed methodology to be effective in providing valuable inference. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2207.09560</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2207.09560</id><created>2022-07-19</created><updated>2025-02-01</updated><authors><author><keyname>Bennouna</keyname><forenames>Amine</forenames></author><author><keyname>Van Parys</keyname><forenames>Bart</forenames></author><author><keyname>Lucas</keyname><forenames>Ryan</forenames></author></authors><title>Holistic Robust Data-Driven Decisions</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The design of data-driven formulations for machine learning and decision-making with good out-of-sample performance is a key challenge. The observation that good in-sample performance does not guarantee good out-of-sample performance is generally known as overfitting. Practical overfitting can typically not be attributed to a single cause but is caused by several factors simultaneously. We consider here three overfitting sources: (i) statistical error as a result of working with finite sample data, (ii) data noise, which occurs when the data points are measured only with finite precision, and finally, (iii) data misspecification in which a small fraction of all data may be wholly corrupted. Although existing data-driven formulations may be robust against one of these three sources in isolation, they do not provide holistic protection against all overfitting sources simultaneously. We design a novel data-driven formulation that guarantees such holistic protection and is computationally viable. Our distributionally robust optimization formulation can be interpreted as a novel combination of a Kullback-Leibler and L\'evy-Prokhorov robust optimization formulation. In the context of classification and regression problems, we show that several popular regularized and robust formulations naturally reduce to a particular case of our proposed novel formulation. Finally, we apply the proposed HR formulation to two real-life applications and study it alongside several benchmarks: (1) training neural networks on healthcare data, where we analyze various robustness and generalization properties in the presence of noise, labeling errors, and scarce data, (2) a portfolio selection problem with real stock data, and analyze the risk/return tradeoff under the natural severe distribution shift of the application. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2210.05026</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2210.05026</id><created>2022-10-10</created><updated>2025-02-01</updated><authors><author><keyname>Cattaneo</keyname><forenames>Matias D.</forenames></author><author><keyname>Feng</keyname><forenames>Yingjie</forenames></author><author><keyname>Palomba</keyname><forenames>Filippo</forenames></author><author><keyname>Titiunik</keyname><forenames>Rocio</forenames></author></authors><title>Uncertainty Quantification in Synthetic Controls with Staggered   Treatment Adoption</title><categories>econ.EM stat.AP stat.CO stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose principled prediction intervals to quantify the uncertainty of a large class of synthetic control predictions (or estimators) in settings with staggered treatment adoption, offering precise non-asymptotic coverage probability guarantees. From a methodological perspective, we provide a detailed discussion of different causal quantities to be predicted, which we call causal predictands, allowing for multiple treated units with treatment adoption at possibly different points in time. From a theoretical perspective, our uncertainty quantification methods improve on prior literature by (i) covering a large class of causal predictands in staggered adoption settings, (ii) allowing for synthetic control methods with possibly nonlinear constraints, (iii) proposing scalable robust conic optimization methods and principled data-driven tuning parameter selection, and (iv) offering valid uniform inference across post-treatment periods. We illustrate our methodology with an empirical application studying the effects of economic liberalization on real GDP per capita for Sub-Saharan African countries. Companion software packages are provided in Python, R, and Stata. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2212.13686</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2212.13686</id><created>2022-12-27</created><updated>2025-02-02</updated><authors><author><keyname>Chang</keyname><forenames>Jinyuan</forenames></author><author><keyname>Jiang</keyname><forenames>Qing</forenames></author><author><keyname>McElroy</keyname><forenames>Tucker S.</forenames></author><author><keyname>Shao</keyname><forenames>Xiaofeng</forenames></author></authors><title>Statistical inference for high-dimensional spectral density matrix</title><categories>math.ST stat.ME stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spectral density matrix is a fundamental object of interest in time series analysis, and it encodes both contemporary and dynamic linear relationships between component processes of the multivariate system. In this paper we develop novel inference procedures for the spectral density matrix in the high-dimensional setting. Specifically, we introduce a new global testing procedure to test the nullity of the cross-spectral density for a given set of frequencies and across pairs of component indices. For the first time, both Gaussian approximation and parametric bootstrap methodologies are employed to conduct inference for a high-dimensional parameter formulated in the frequency domain, and new technical tools are developed to provide asymptotic guarantees of the size accuracy and power for global testing. We further propose a multiple testing procedure for simultaneously testing the nullity of the cross-spectral density at a given set of frequencies. The method is shown to control the false discovery rate. Both numerical simulations and a real data illustration demonstrate the usefulness of the proposed testing methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2301.01085</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2301.01085</id><created>2023-01-03</created><updated>2025-01-31</updated><authors><author><keyname>Bellégo</keyname><forenames>Christophe</forenames></author><author><keyname>Benatia</keyname><forenames>David</forenames></author><author><keyname>Dortet-Bernardet</keyname><forenames>Vincent</forenames></author></authors><title>The Chained Difference-in-Differences</title><categories>econ.EM stat.AP</categories><comments>49 pages manuscript, 34 pages appendix</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper studies the identification, estimation, and inference of long-term (binary) treatment effect parameters when balanced panel data is not available, or consists of only a subset of the available data. We develop a new estimator: the chained difference-in-differences, which leverages the overlapping structure of many unbalanced panel data sets. This approach consists in aggregating a collection of short-term treatment effects estimated on multiple incomplete panels. Our estimator accommodates (1) multiple time periods, (2) variation in treatment timing, (3) treatment effect heterogeneity, (4) general missing data patterns, and (5) sample selection on observables. We establish the asymptotic properties of the proposed estimator and discuss identification and efficiency gains in comparison to existing methods. Finally, we illustrate its relevance through (i) numerical simulations, and (ii) an application about the effects of an innovation policy in France. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2302.12103</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2302.12103</id><created>2023-02-23</created><authors><author><keyname>Ragni</keyname><forenames>Alessandra</forenames></author><author><keyname>Masci</keyname><forenames>Chiara</forenames></author><author><keyname>Ieva</keyname><forenames>Francesca</forenames></author><author><keyname>Paganoni</keyname><forenames>Anna Maria</forenames></author></authors><title>Clustering Hierarchies via a Semi-Parametric Generalized Linear Mixed   Model: a statistical significance-based approach</title><categories>stat.ME stat.AP stat.CO</categories><doi>10.1093/jrsssa/qnaf007</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We introduce a novel statistical significance-based approach for clustering hierarchical data using semi-parametric linear mixed-effects models designed for responses with laws in the exponential family (e.g., Poisson and Bernoulli). Within the family of semi-parametric mixed-effects models, a latent clustering structure of the highest-level units can be identified by assuming the random effects to follow a discrete distribution with an unknown number of support points. We achieve this by computing {\alpha}-level confidence regions of the estimated support point and identifying statistically different clusters. At each iteration of a tailored Expectation Maximization algorithm, the two closest estimated support points for which the confidence regions overlap collapse. Unlike the related state-of-the-art methods that rely on arbitrary thresholds to determine the merging of close discrete masses, the proposed approach relies on conventional statistical confidence levels, thereby avoiding the use of discretionary tuning parameters. To demonstrate the effectiveness of our approach, we apply it to data from the Programme for International Student Assessment (PISA - OECD) to cluster countries based on the rate of innumeracy levels in schools. Additionally, a simulation study and comparison with classical parametric and state-of-the-art models are provided and discussed. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2303.03080</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2303.03080</id><created>2023-03-06</created><updated>2025-02-01</updated><authors><author><keyname>Botha</keyname><forenames>Arno</forenames></author><author><keyname>Oberholzer</keyname><forenames>Esmerelda</forenames></author><author><keyname>Larney</keyname><forenames>Janette</forenames></author><author><keyname>de Jongh</keyname><forenames>Riaan</forenames></author></authors><title>Defining and comparing SICR-events for classifying impaired loans under   IFRS 9</title><categories>q-fin.RM q-fin.ST stat.AP</categories><comments>36 pages (including appendix), 9199 words, 13 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The IFRS 9 accounting standard requires the prediction of credit deterioration in financial instruments, i.e., significant increases in credit risk (SICR). However, the definition of such a SICR-event is inherently ambiguous, given its current reliance on evaluating the change in the estimated probability of default (PD) against some arbitrary threshold. We examine the shortcomings of this PD-comparison approach and propose an alternative framework for generating SICR-definitions based on three parameters: delinquency, stickiness, and the outcome period. Having varied these framework parameters, we obtain 27 unique SICR-definitions and fit logistic regression models accordingly using rich South African mortgage and macroeconomic data. For each definition and corresponding model, the resulting SICR-rates are analysed at the portfolio-level on their stability over time and their responsiveness to economic downturns. At the account-level, we compare both the accuracy and dynamicity of the SICR-predictions, and discover several interesting trends and trade-offs. These results can help any bank with appropriately setting the three framework parameters in defining SICR-events for prediction purposes. We demonstrate this process by comparing the best-performing SICR-model to the PD-comparison approach, and show the latter's inferiority as an early-warning system. Our work can therefore guide the formulation, modelling, and testing of any SICR-definition, thereby promoting the timeous recognition of credit losses; the main imperative of IFRS 9. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2306.03303</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2306.03303</id><created>2023-06-05</created><updated>2025-02-02</updated><authors><author><keyname>Cuchiero</keyname><forenames>Christa</forenames></author><author><keyname>Schmocker</keyname><forenames>Philipp</forenames></author><author><keyname>Teichmann</keyname><forenames>Josef</forenames></author></authors><title>Global universal approximation of functional input maps on weighted   spaces</title><categories>stat.ML cs.LG math.FA math.PR q-fin.MF</categories><comments>67 pages, 4 figures</comments><msc-class>26A16, 26E20, 41A65, 41A81, 46E40, 60L10, 68T07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce so-called functional input neural networks defined on a possibly infinite dimensional weighted space with values also in a possibly infinite dimensional output space. To this end, we use an additive family to map the input weighted space to the hidden layer, on which a non-linear scalar activation function is applied to each neuron, and finally return the output via some linear readouts. Relying on Stone-Weierstrass theorems on weighted spaces, we can prove a global universal approximation result on weighted spaces for continuous functions going beyond the usual approximation on compact sets. This then applies in particular to approximation of (non-anticipative) path space functionals via functional input neural networks. As a further application of the weighted Stone-Weierstrass theorem we prove a global universal approximation result for linear functions of the signature. We also introduce the viewpoint of Gaussian process regression in this setting and emphasize that the reproducing kernel Hilbert space of the signature kernels are Cameron-Martin spaces of certain Gaussian processes. This paves a way towards uncertainty quantification for signature kernel regression. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2306.12968</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2306.12968</id><created>2023-06-18</created><updated>2025-02-02</updated><authors><author><keyname>Ariu</keyname><forenames>Kaito</forenames></author><author><keyname>Proutiere</keyname><forenames>Alexandre</forenames></author><author><keyname>Yun</keyname><forenames>Se-Young</forenames></author></authors><title>Revisiting Instance-Optimal Cluster Recovery in the Labeled Stochastic   Block Model</title><categories>cs.SI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the problem of recovering hidden communities in the Labeled Stochastic Block Model (LSBM) with a finite number of clusters whose sizes grow linearly with the total number of nodes. We derive the necessary and sufficient conditions under which the expected number of misclassified nodes is less than $ s $, for any number $ s = o(n) $. To achieve this, we propose IAC (Instance-Adaptive Clustering), the first algorithm whose performance matches the instance-specific lower bounds both in expectation and with high probability. IAC is a novel two-phase algorithm that consists of a one-shot spectral clustering step followed by iterative likelihood-based cluster assignment improvements. This approach is based on the instance-specific lower bound and notably does not require any knowledge of the model parameters, including the number of clusters. By performing the spectral clustering only once, IAC maintains an overall computational complexity of $ \mathcal{O}(n\, \text{polylog}(n)) $, making it scalable and practical for large-scale problems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2308.12540</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2308.12540</id><created>2023-08-23</created><authors><author><keyname>Zhou</keyname><forenames>Yidong</forenames></author><author><keyname>Müller</keyname><forenames>Hans-Georg</forenames></author></authors><title>Wasserstein Regression with Empirical Measures and Density Estimation   for Sparse Data</title><categories>stat.ME</categories><comments>27 pages, 5 figures, 2 tables</comments><doi>10.1093/biomtc/ujae127</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of modeling the relationship between univariate distributions and one or more explanatory variables has found increasing interest. Traditional functional data methods cannot be applied directly to distributional data because of their inherent constraints. Modeling distributions as elements of the Wasserstein space, a geodesic metric space equipped with the Wasserstein metric that is related to optimal transport, is attractive for statistical applications. Existing approaches proceed by substituting proxy estimated distributions for the typically unknown response distributions. These estimates are obtained from available data but are problematic when for some of the distributions only few data are available. Such situations are common in practice and cannot be addressed with available approaches, especially when one aims at density estimates. We show how this and other problems associated with density estimation such as tuning parameter selection and bias issues can be side-stepped when covariates are available. We also introduce a novel version of distribution-response regression that is based on empirical measures. By avoiding the preprocessing step of recovering complete individual response distributions, the proposed approach is applicable when the sample size available for some of the distributions is small. In this case, one can still obtain consistent distribution estimates even for distributions with only few data by gaining strength across the entire sample of distributions, while traditional approaches where distributions or densities are estimated individually fail, since sparsely sampled densities cannot be consistently estimated. The proposed model is demonstrated to outperform existing approaches through simulations. Its efficacy is corroborated in two case studies on Environmental Influences on Child Health Outcomes (ECHO) data and eBay auction data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2309.05496</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2309.05496</id><created>2023-09-11</created><updated>2025-01-31</updated><authors><author><keyname>Marques</keyname><forenames>Isa</forenames></author><author><keyname>Wiemann</keyname><forenames>Paul F. V.</forenames></author></authors><title>Bayesian spatial+: A joint model perspective</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial confounding is a common issue in spatial regression models, occurring when spatially indexed covariates that model the mean of the response are correlated with a spatial effect included in the model. This dependence, particularly at high spatial frequencies combined with smoothing, can introduce bias in the regression coefficient estimates. The spatial+ framework is a widely used two-stage frequentist approach to mitigate spatial confounding by explicitly modeling and removing the spatial structure in the confounding covariate, replacing it with residuals in the second-stage model for the response. However, frequentist spatial+ does not propagate uncertainty from the first-stage estimation to the second stage, and inference can be cumbersome in a frequentist setting. In contrast, a Bayesian joint modeling framework inherently propagates uncertainty between stages and allows for direct inference on the model parameters. Despite its advantages, the original spatial+ method does not ensure the residuals and spatial effects in the second-stage model are free of shared high spatial frequencies without additional assumptions. To address this, we propose a novel joint prior for the smoothness parameters of the spatial effects that mitigates this issue while preserving the predictive power of the model. We demonstrate the efficacy of our approach through simulation studies and real-world applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.12140</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.12140</id><created>2023-10-18</created><updated>2025-02-01</updated><authors><author><keyname>Zhang</keyname><forenames>Tianyu</forenames></author><author><keyname>Lei</keyname><forenames>Jing</forenames></author></authors><title>Online Estimation with Rolling Validation: Adaptive Nonparametric   Estimation with Streaming Data</title><categories>math.ST stat.ME stat.ML stat.TH</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Online nonparametric estimators are gaining popularity due to their efficient computation and competitive generalization abilities. An important example includes variants of stochastic gradient descent. These algorithms often take one sample point at a time and incrementally update the parameter estimate of interest. In this work, we consider model selection/hyperparameter tuning for such online algorithms. We propose a weighted rolling validation procedure, an online variant of leave-one-out cross-validation, that costs minimal extra computation for many typical stochastic gradient descent estimators and maintains their online nature. Similar to batch cross-validation, it can boost base estimators to achieve better heuristic performance and adaptive convergence rate. Our analysis is straightforward, relying mainly on some general statistical stability assumptions. The simulation study underscores the significance of diverging weights in practice and demonstrates its favorable sensitivity even when there is only a slim difference between candidate estimators. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2311.13327</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2311.13327</id><created>2023-11-22</created><updated>2025-02-03</updated><authors><author><keyname>Dimitriadis</keyname><forenames>Timo</forenames></author><author><keyname>Hoga</keyname><forenames>Yannick</forenames></author></authors><title>Regressions under Adverse Conditions</title><categories>econ.EM stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new regression method that relates the mean of an outcome variable to covariates, under the "adverse condition" that a distress variable falls in its tail. This allows to tailor classical mean regressions to adverse scenarios, which receive increasing interest in economics and finance, among many others. In the terminology of the systemic risk literature, our method can be interpreted as a regression for the Marginal Expected Shortfall. We propose a two-step procedure to estimate the new models, show consistency and asymptotic normality of the estimator, and propose feasible inference under weak conditions that allow for cross-sectional and time series applications. Simulations verify the accuracy of the asymptotic approximations of the two-step estimator. Two empirical applications show that our regressions under adverse conditions are a valuable tool in such diverse fields as the study of the relation between systemic risk and asset price bubbles, and dissecting macroeconomic growth vulnerabilities into individual components. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.05678</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.05678</id><created>2023-12-09</created><updated>2025-02-01</updated><authors><author><keyname>Wickett</keyname><forenames>Eugene</forenames></author><author><keyname>Plumlee</keyname><forenames>Matthew</forenames></author><author><keyname>Smilowitz</keyname><forenames>Karen</forenames></author><author><keyname>Phanouvong</keyname><forenames>Souly</forenames></author><author><keyname>Nwogu</keyname><forenames>Timothy</forenames></author></authors><title>Measuring sampling plan utility in post-marketing surveillance of   medical products</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Ensuring product quality is critical to combating the global challenge of substandard and falsified medical products. Post-marketing surveillance is a central quality-assurance activity in which products from consumer-facing locations are collected and tested. Regulators in low-resource settings use post-marketing surveillance to evaluate product quality across locations and determine corrective actions. Part of post-marketing surveillance is developing a sampling plan, which specifies where to test and the number of tests to conduct at a location. With limited resources, it is important to base decisions on the utility of the samples tested. We propose a Bayesian approach to generate a comprehensive utility metric for sampling plans. This sampling plan utility integrates regulatory risk assessments with prior testing data, available supply-chain information, and valuations of regulatory objectives. We develop an efficient method for calculating sampling plan utility. We illustrate the value of the utility metric with a case study based on de-identified post-marketing surveillance data from a low-resource setting. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.12747</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.12747</id><created>2023-12-19</created><updated>2025-02-02</updated><authors><author><keyname>Mills</keyname><forenames>Edmund</forenames></author><author><keyname>Su</keyname><forenames>Shiye</forenames></author><author><keyname>Russell</keyname><forenames>Stuart</forenames></author><author><keyname>Emmons</keyname><forenames>Scott</forenames></author></authors><title>ALMANACS: A Simulatability Benchmark for Language Model Explainability</title><categories>cs.LG cs.AI cs.CL stat.ML</categories><comments>Code is available at   https://github.com/edmundmills/ALMANACS}{https://github.com/edmundmills/ALMANACS</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  How do we measure the efficacy of language model explainability methods? While many explainability methods have been developed, they are typically evaluated on bespoke tasks, preventing an apples-to-apples comparison. To help fill this gap, we present ALMANACS, a language model explainability benchmark. ALMANACS scores explainability methods on simulatability, i.e., how well the explanations improve behavior prediction on new inputs. The ALMANACS scenarios span twelve safety-relevant topics such as ethical reasoning and advanced AI behaviors; they have idiosyncratic premises to invoke model-specific behavior; and they have a train-test distributional shift to encourage faithful explanations. By using another language model to predict behavior based on the explanations, ALMANACS is a fully automated benchmark. While not a replacement for human evaluations, we aim for ALMANACS to be a complementary, automated tool that allows for fast, scalable evaluation. Using ALMANACS, we evaluate counterfactual, rationalization, attention, and Integrated Gradients explanations. Our results are sobering: when averaged across all topics, no explanation method outperforms the explanation-free control. We conclude that despite modest successes in prior work, developing an explanation method that aids simulatability in ALMANACS remains an open challenge. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.15595</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.15595</id><created>2023-12-24</created><updated>2025-01-31</updated><authors><author><keyname>Wei</keyname><forenames>Haoyu</forenames></author><author><keyname>Wan</keyname><forenames>Runzhe</forenames></author><author><keyname>Shi</keyname><forenames>Lei</forenames></author><author><keyname>Song</keyname><forenames>Rui</forenames></author></authors><title>Zero-Inflated Bandits</title><categories>stat.ML cs.LG econ.EM</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Many real-world bandit applications are characterized by sparse rewards, which can significantly hinder learning efficiency. Leveraging problem-specific structures for careful distribution modeling is recognized as essential for improving estimation efficiency in statistics. However, this approach remains under-explored in the context of bandits. To address this gap, we initiate the study of zero-inflated bandits, where the reward is modeled using a classic semi-parametric distribution known as the zero-inflated distribution. We develop algorithms based on the Upper Confidence Bound and Thompson Sampling frameworks for this specific structure. The superior empirical performance of these methods is demonstrated through extensive numerical studies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.11352</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.11352</id><created>2024-01-20</created><updated>2025-02-02</updated><authors><author><keyname>Zhang</keyname><forenames>Zhiwei</forenames></author></authors><title>A Connection Between Covariate Adjustment and Stratified Randomization   in Randomized Clinical Trials</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The statistical efficiency of randomized clinical trials can be improved by incorporating information from baseline covariates (i.e., pre-treatment patient characteristics). This can be done in the design stage using stratified (permutated block) randomization or in the analysis stage through covariate adjustment. This article makes a connection between covariate adjustment and stratified randomization in a general framework where all regular, asymptotically linear estimators are identified as augmented estimators. From a geometric perspective, covariate adjustment can be viewed as an attempt to approximate the optimal augmentation function, and stratified randomization improves a given approximation by moving it closer to the optimal augmentation function. The efficiency benefit of stratified randomization is asymptotically equivalent to attaching an optimal augmentation term based on the stratification factor. In designing a trial with stratified randomization, it is not essential to include all important covariates in the stratification, because their prognostic information can be incorporated through covariate adjustment. Under stratified randomization, adjusting for the stratification factor only in data analysis is not expected to improve efficiency, and the key to efficient estimation is incorporating prognostic information from all important covariates. These observations are confirmed in a simulation study and illustrated using real clinical trial data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.01454</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.01454</id><created>2024-02-02</created><updated>2025-01-31</updated><authors><author><keyname>Takayama</keyname><forenames>Masayuki</forenames></author><author><keyname>Okuda</keyname><forenames>Tadahisa</forenames></author><author><keyname>Pham</keyname><forenames>Thong</forenames></author><author><keyname>Ikenoue</keyname><forenames>Tatsuyoshi</forenames></author><author><keyname>Fukuma</keyname><forenames>Shingo</forenames></author><author><keyname>Shimizu</keyname><forenames>Shohei</forenames></author><author><keyname>Sannai</keyname><forenames>Akiyoshi</forenames></author></authors><title>Integrating Large Language Models in Causal Discovery: A Statistical   Causal Approach</title><categories>cs.LG cs.AI stat.ME stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is important for creating consistent, meaningful causal models, despite the challenges in the systematic acquisition of background knowledge. To overcome these challenges, this paper proposes a novel method for causal inference, in which SCD and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through ``statistical causal prompting (SCP)'' for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that the results of LLM-KBCI and SCD augmented with LLM-KBCI approach the ground truths, more than the SCD result without prior knowledge. It has also been revealed that the SCD result can be further improved if the LLM undergoes SCP. Furthermore, with an unpublished real-world dataset, we have demonstrated that the background knowledge provided by the LLM can improve the SCD on this dataset, even if this dataset has never been included in the training data of the LLM. For future practical application of this proposed method across important domains such as healthcare, we also thoroughly discuss the limitations, risks of critical errors, expected improvement of techniques around LLMs, and realistic integration of expert checks of the results into this automatic process, with SCP simulations under various conditions both in successful and failure scenarios. The careful and appropriate application of the proposed approach in this work, with improvement and customization for each domain, can thus address challenges such as dataset biases and limitations, illustrating the potential of LLMs to improve data-driven causal inference across diverse scientific domains. The code used in this work is publicly available at: www.github.com/mas-takayama/LLM-and-SCD </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.04530</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.04530</id><created>2024-02-06</created><updated>2025-02-02</updated><authors><author><keyname>Wiens</keyname><forenames>Douglas P.</forenames></author></authors><title>To ignore dependencies is perhaps not a sin</title><categories>math.ST stat.TH</categories><msc-class>Primary 62G35, Secondary 62K05</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a result according to which certain functions of covariance matrices are maximized at scalar multiples of the identity matrix. In a statistical context in which such functions measure loss, this says that the least favourable form of dependence is in fact independence, so that a procedure optimal for i.i.d.\ data can be minimax. In particular, the ordinary least squares (\textsc{ols}) estimate of a correctly specified regression response is minimax among generalized least squares (\textsc{gls}) estimates, when the maximum is taken over certain classes of error covariance structures and the loss function possesses a natural monotonicity property. An implication is that it can be not only safe, but optimal to ignore such departures from the usual assumption of i.i.d.\ errors. We then consider regression models in which the response function is possibly misspecified, and show that \textsc{ols} is minimax if the design is uniform on its support, but that this often fails otherwise. We go on to investigate the interplay between minimax \textsc{gls} procedures and minimax designs, leading us to extend, to robustness against dependencies, an existing observation -- that robustness against model misspecifications is increased by splitting replicates into clusters of observations at nearby locations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.06049</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.06049</id><created>2024-02-05</created><updated>2025-02-01</updated><authors><author><keyname>Flamino</keyname><forenames>James</forenames></author><author><keyname>Modi</keyname><forenames>Mohammed Shahid</forenames></author><author><keyname>Szymanski</keyname><forenames>Boleslaw K.</forenames></author><author><keyname>Cross</keyname><forenames>Brendan</forenames></author><author><keyname>Mikolajczyk</keyname><forenames>Colton</forenames></author></authors><title>Limits of Large Language Models in Debating Humans</title><categories>cs.AI cs.CL cs.HC stat.AP</categories><comments>23 pages, 4 figures, 3 tables, 42 pages of supplemental materials, 9   supplemental figures, 24 supplemental tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) have shown remarkable promise in communicating with humans. Their potential use as artificial partners with humans in sociological experiments involving conversation is an exciting prospect. But how viable is it? Here, we rigorously test the limits of agents that debate using LLMs in a preregistered study that runs multiple debate-based opinion consensus games. Each game starts with six humans, six agents, or three humans and three agents. We found that agents can blend in and concentrate on a debate's topic better than humans, improving the productivity of all players. Yet, humans perceive agents as less convincing and confident than other humans, and several behavioral metrics of humans and agents we collected deviate measurably from each other. We observed that agents are already decent debaters, but their behavior generates a pattern distinctly different from the human-generated data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.11179</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.11179</id><created>2024-02-16</created><authors><author><keyname>Hauth</keyname><forenames>Jeremiah</forenames></author><author><keyname>Safta</keyname><forenames>Cosmin</forenames></author><author><keyname>Huan</keyname><forenames>Xun</forenames></author><author><keyname>Patel</keyname><forenames>Ravi G.</forenames></author><author><keyname>Jones</keyname><forenames>Reese E.</forenames></author></authors><title>Uncertainty Quantification of Graph Convolution Neural Network Models of   Evolving Processes</title><categories>cs.LG math.ST physics.comp-ph stat.TH</categories><comments>27 pages, 20 figures</comments><journal-ref>Computer Methods in Applied Mechanics and Engineering 429 (2024)   117195</journal-ref><doi>10.1016/j.cma.2024.117195</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The application of neural network models to scientific machine learning tasks has proliferated in recent years. In particular, neural network models have proved to be adept at modeling processes with spatial-temporal complexity. Nevertheless, these highly parameterized models have garnered skepticism in their ability to produce outputs with quantified error bounds over the regimes of interest. Hence there is a need to find uncertainty quantification methods that are suitable for neural networks. In this work we present comparisons of the parametric uncertainty quantification of neural networks modeling complex spatial-temporal processes with Hamiltonian Monte Carlo and Stein variational gradient descent and its projected variant. Specifically we apply these methods to graph convolutional neural network models of evolving systems modeled with recurrent neural network and neural ordinary differential equations architectures. We show that Stein variational inference is a viable alternative to Monte Carlo methods with some clear advantages for complex neural network models. For our exemplars, Stein variational interference gave similar uncertainty profiles through time compared to Hamiltonian Monte Carlo, albeit with generally more generous variance.Projected Stein variational gradient descent also produced similar uncertainty profiles to the non-projected counterpart, but large reductions in the active weight space were confounded by the stability of the neural network predictions and the convoluted likelihood landscape. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.01737</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.01737</id><created>2024-03-04</created><updated>2025-02-01</updated><authors><author><keyname>Castillo</keyname><forenames>Ismaël</forenames></author><author><keyname>Randrianarisoa</keyname><forenames>Thibault</forenames></author></authors><title>Deep Horseshoe Gaussian Processes</title><categories>math.ST stat.ML stat.TH</categories><comments>54 pages (25-page supplement included), one figure</comments><msc-class>62G20</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deep Gaussian processes have recently been proposed as natural objects to fit, similarly to deep neural networks, possibly complex features present in modern data samples, such as compositional structures. Adopting a Bayesian nonparametric approach, it is natural to use deep Gaussian processes as prior distributions, and use the corresponding posterior distributions for statistical inference. We introduce the deep Horseshoe Gaussian process Deep-HGP, a new simple prior based on deep Gaussian processes with a squared-exponential kernel, that in particular enables data-driven choices of the key lengthscale parameters. For nonparametric regression with random design, we show that the associated posterior distribution recovers the unknown true regression curve optimally in terms of quadratic loss, up to a logarithmic factor, in an adaptive way. The convergence rates are simultaneously adaptive to both the smoothness of the regression function and to its structure in terms of compositions. The dependence of the rates in terms of dimension are explicit, allowing in particular for input spaces of dimension increasing with the number of observations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.10771</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.10771</id><created>2024-03-15</created><updated>2025-02-01</updated><authors><author><keyname>Cao</keyname><forenames>Junyu</forenames></author><author><keyname>Bayati</keyname><forenames>Mohsen</forenames></author></authors><title>A Probabilistic Approach for Model Alignment with Human Comparisons</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A growing trend involves integrating human knowledge into learning frameworks, leveraging subtle human feedback to refine AI models. While these approaches have shown promising results in practice, the theoretical understanding of when and why such approaches are effective remains limited. This work takes steps toward developing a theoretical framework for analyzing the conditions under which human comparisons can enhance the traditional supervised learning process. Specifically, this paper studies the effective use of noisy-labeled data and human comparison data to address challenges arising from noisy environment and high-dimensional models. We propose a two-stage "Supervised Learning+Learning from Human Feedback" (SL+LHF) framework that connects machine learning with human feedback through a probabilistic bisection approach. The two-stage framework first learns low-dimensional representations from noisy-labeled data via an SL procedure and then uses human comparisons to improve the model alignment. To examine the efficacy of the alignment phase, we introduce a concept, termed the "label-noise-to-comparison-accuracy" (LNCA) ratio. This paper identifies from a theoretical perspective the conditions under which the "SL+LHF" framework outperforms the pure SL approach; we then leverage this LNCA ratio to highlight the advantage of incorporating human evaluators in reducing sample complexity. We validate that the LNCA ratio meets the proposed conditions for its use through a case study conducted via Amazon Mechanical Turk (MTurk). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.18072</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.18072</id><created>2024-03-26</created><updated>2025-02-01</updated><authors><author><keyname>Zhong</keyname><forenames>Shijie</forenames></author><author><keyname>Shen</keyname><forenames>Wanggang</forenames></author><author><keyname>Catanach</keyname><forenames>Tommie</forenames></author><author><keyname>Huan</keyname><forenames>Xun</forenames></author></authors><title>Goal-Oriented Bayesian Optimal Experimental Design for Nonlinear Models   using Markov Chain Monte Carlo</title><categories>stat.CO cs.LG stat.ME stat.ML</categories><comments>28 pages, 19 figures</comments><msc-class>62K05, 62F15, 62B15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal experimental design (OED) provides a systematic approach to quantify and maximize the value of experimental data. Under a Bayesian approach, conventional OED maximizes the expected information gain (EIG) on model parameters. However, we are often interested in not the parameters themselves, but predictive quantities of interest (QoIs) that depend on the parameters in a nonlinear manner. We present a computational framework of predictive goal-oriented OED (GO-OED) suitable for nonlinear observation and prediction models, which seeks the experimental design providing the greatest EIG on the QoIs. In particular, we propose a nested Monte Carlo estimator for the QoI EIG, featuring Markov chain Monte Carlo for posterior sampling and kernel density estimation for evaluating the posterior-predictive density and its Kullback-Leibler divergence from the prior-predictive. The GO-OED design is then found by maximizing the EIG over the design space using Bayesian optimization. We demonstrate the effectiveness of the overall nonlinear GO-OED method, and illustrate its differences versus conventional non-GO-OED, through various test problems and an application of sensor placement for source inversion in a convection-diffusion field. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.13056</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.13056</id><created>2024-04-08</created><authors><author><keyname>Dong</keyname><forenames>Jiayuan</forenames></author><author><keyname>Jacobsen</keyname><forenames>Christian</forenames></author><author><keyname>Khalloufi</keyname><forenames>Mehdi</forenames></author><author><keyname>Akram</keyname><forenames>Maryam</forenames></author><author><keyname>Liu</keyname><forenames>Wanjiao</forenames></author><author><keyname>Duraisamy</keyname><forenames>Karthik</forenames></author><author><keyname>Huan</keyname><forenames>Xun</forenames></author></authors><title>Variational Bayesian Optimal Experimental Design with Normalizing Flows</title><categories>cs.LG cs.CE stat.CO stat.ME stat.ML</categories><msc-class>62K05, 94A17, 62C10, 62F15</msc-class><journal-ref>Computer Methods in Applied Mechanics and Engineering 433 (2025)   117457</journal-ref><doi>10.1016/j.cma.2024.117457</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian optimal experimental design (OED) seeks experiments that maximize the expected information gain (EIG) in model parameters. Directly estimating the EIG using nested Monte Carlo is computationally expensive and requires an explicit likelihood. Variational OED (vOED), in contrast, estimates a lower bound of the EIG without likelihood evaluations by approximating the posterior distributions with variational forms, and then tightens the bound by optimizing its variational parameters. We introduce the use of normalizing flows (NFs) for representing variational distributions in vOED; we call this approach vOED-NFs. Specifically, we adopt NFs with a conditional invertible neural network architecture built from compositions of coupling layers, and enhanced with a summary network for data dimension reduction. We present Monte Carlo estimators to the lower bound along with gradient expressions to enable a gradient-based simultaneous optimization of the variational parameters and the design variables. The vOED-NFs algorithm is then validated in two benchmark problems, and demonstrated on a partial differential equation-governed application of cathodic electrophoretic deposition and an implicit likelihood case with stochastic modeling of aphid population. The findings suggest that a composition of 4--5 coupling layers is able to achieve lower EIG estimation bias, under a fixed budget of forward model runs, compared to previous approaches. The resulting NFs produce approximate posteriors that agree well with the true posteriors, able to capture non-Gaussian and multi-modal features effectively. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.16209</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.16209</id><created>2024-04-24</created><updated>2025-02-01</updated><authors><author><keyname>Fotheringham</keyname><forenames>A. Stewart</forenames></author><author><keyname>Kao</keyname><forenames>Chen-Lun</forenames></author><author><keyname>Yu</keyname><forenames>Hanchen</forenames></author><author><keyname>Bardin</keyname><forenames>Sarah</forenames></author><author><keyname>Oshan</keyname><forenames>Taylor</forenames></author><author><keyname>Li</keyname><forenames>Ziqi</forenames></author><author><keyname>Sachdeva</keyname><forenames>Mehak</forenames></author><author><keyname>Luo</keyname><forenames>Wei</forenames></author></authors><title>Exploring Spatial Context: A Comprehensive Bibliography of GWR and MGWR</title><categories>stat.ME stat.AP stat.CO</categories><comments>446 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Local spatial models such as Geographically Weighted Regression (GWR) and Multiscale Geographically Weighted Regression (MGWR) serve as instrumental tools to capture intrinsic contextual effects through the estimates of the local intercepts and behavioral contextual effects through estimates of the local slope parameters. GWR and MGWR provide simple implementation yet powerful frameworks that could be extended to various disciplines that handle spatial data. This bibliography aims to serve as a comprehensive compilation of peer-reviewed papers that have utilized GWR or MGWR as a primary analytical method to conduct spatial analyses and acts as a useful guide to anyone searching the literature for previous examples of local statistical modeling in a wide variety of application fields. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.17769</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.17769</id><created>2024-04-26</created><updated>2025-02-01</updated><authors><author><keyname>Xu</keyname><forenames>Yunpeng</forenames></author><author><keyname>Ying</keyname><forenames>Mufang</forenames></author><author><keyname>Guo</keyname><forenames>Wenge</forenames></author><author><keyname>Wei</keyname><forenames>Zhi</forenames></author></authors><title>Two-stage Risk Control with Application to Ranked Retrieval</title><categories>cs.IR stat.ME stat.ML</categories><comments>20 pages, 3 figures, 2 tables; 7 supplementary pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Practical machine learning systems often operate in multiple sequential stages, as seen in ranking and recommendation systems, which typically include a retrieval phase followed by a ranking phase. Effectively assessing prediction uncertainty and ensuring effective risk control in such systems pose significant challenges due to their inherent complexity. To address these challenges, we developed two-stage risk control methods based on the recently proposed learn-then-test (LTT) and conformal risk control (CRC) frameworks. Unlike the methods in prior work that address multiple risks, our approach leverages the sequential nature of the problem, resulting in reduced computational burden. We provide theoretical guarantees for our proposed methods and design novel loss functions tailored for ranked retrieval tasks. The effectiveness of our approach is validated through experiments on two large-scale, widely-used datasets: MSLR-Web and Yahoo LTRC. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.19261</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.19261</id><created>2024-04-30</created><updated>2025-01-31</updated><authors><author><keyname>Agarwala</keyname><forenames>Atish</forenames></author><author><keyname>Pennington</keyname><forenames>Jeffrey</forenames></author></authors><title>High dimensional analysis reveals conservative sharpening and a   stochastic edge of stability</title><categories>cs.LG math.OC math.ST physics.data-an stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent empirical and theoretical work has shown that the dynamics of the large eigenvalues of the training loss Hessian have some remarkably robust features across models and datasets in the full batch regime. There is often an early period of progressive sharpening where the large eigenvalues increase, followed by stabilization at a predictable value known as the edge of stability. Previous work showed that in the stochastic setting, the eigenvalues increase more slowly - a phenomenon we call conservative sharpening. We provide a theoretical analysis of a simple high-dimensional model which shows the origin of this slowdown. We also show that there is an alternative stochastic edge of stability which arises at small batch size that is sensitive to the trace of the Neural Tangent Kernel rather than the large Hessian eigenvalues. We conduct an experimental study which highlights the qualitative differences from the full batch phenomenology, and suggests that controlling the stochastic edge of stability can help optimization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.05025</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.05025</id><created>2024-05-08</created><authors><author><keyname>Poinsot</keyname><forenames>Audrey</forenames></author><author><keyname>Leite</keyname><forenames>Alessandro</forenames></author><author><keyname>Chesneau</keyname><forenames>Nicolas</forenames></author><author><keyname>Sébag</keyname><forenames>Michèle</forenames></author><author><keyname>Schoenauer</keyname><forenames>Marc</forenames></author></authors><title>Learning Structural Causal Models through Deep Generative Models:   Methods, Guarantees, and Challenges</title><categories>stat.ML cs.LG</categories><comments>Accepted to the 33rd International Joint Conference on Artificial   Intelligence</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper provides a comprehensive review of deep structural causal models (DSCMs), particularly focusing on their ability to answer counterfactual queries using observational data within known causal structures. It delves into the characteristics of DSCMs by analyzing the hypotheses, guarantees, and applications inherent to the underlying deep learning components and structural causal models, fostering a finer understanding of their capabilities and limitations in addressing different counterfactual queries. Furthermore, it highlights the challenges and open questions in the field of deep structural causal modeling. It sets the stages for researchers to identify future work directions and for practitioners to get an overview in order to find out the most appropriate methods for their needs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.09831</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.09831</id><created>2024-05-16</created><updated>2025-02-03</updated><authors><author><keyname>Lee</keyname><forenames>Joongkyu</forenames></author><author><keyname>Oh</keyname><forenames>Min-hwan</forenames></author></authors><title>Nearly Minimax Optimal Regret for Multinomial Logistic Bandit</title><categories>stat.ML cs.LG</categories><comments>Accepted in NeurIPS 2024</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In this paper, we study the contextual multinomial logit (MNL) bandit problem in which a learning agent sequentially selects an assortment based on contextual information, and user feedback follows an MNL choice model. There has been a significant discrepancy between lower and upper regret bounds, particularly regarding the maximum assortment size $K$. Additionally, the variation in reward structures between these bounds complicates the quest for optimality. Under uniform rewards, where all items have the same expected reward, we establish a regret lower bound of $\Omega(d\sqrt{\smash[b]{T/K}})$ and propose a constant-time algorithm, OFU-MNL+, that achieves a matching upper bound of $\tilde{O}(d\sqrt{\smash[b]{T/K}})$. We also provide instance-dependent minimax regret bounds under uniform rewards. Under non-uniform rewards, we prove a lower bound of $\Omega(d\sqrt{T})$ and an upper bound of $\tilde{O}(d\sqrt{T})$, also achievable by OFU-MNL+. Our empirical studies support these theoretical findings. To the best of our knowledge, this is the first work in the contextual MNL bandit literature to prove minimax optimality -- for either uniform or non-uniform reward setting -- and to propose a computationally efficient algorithm that achieves this optimality up to logarithmic factors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.13888</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.13888</id><created>2024-05-22</created><updated>2025-02-03</updated><authors><author><keyname>Yao</keyname><forenames>Dingling</forenames></author><author><keyname>Muller</keyname><forenames>Caroline</forenames></author><author><keyname>Locatello</keyname><forenames>Francesco</forenames></author></authors><title>Marrying Causal Representation Learning with Dynamical Systems for   Science</title><categories>cs.LG stat.ML</categories><comments>NeurIPS 2024 Camera Ready</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Causal representation learning promises to extend causal models to hidden causal variables from raw entangled measurements. However, most progress has focused on proving identifiability results in different settings, and we are not aware of any successful real-world application. At the same time, the field of dynamical systems benefited from deep learning and scaled to countless applications but does not allow parameter identification. In this paper, we draw a clear connection between the two and their key assumptions, allowing us to apply identifiable methods developed in causal representation learning to dynamical systems. At the same time, we can leverage scalable differentiable solvers developed for differential equations to build models that are both identifiable and practical. Overall, we learn explicitly controllable models that isolate the trajectory-specific parameters for further downstream tasks such as out-of-distribution classification or treatment effect estimation. We experiment with a wind simulator with partially known factors of variation. We also apply the resulting model to real-world climate data and successfully answer downstream causal questions in line with existing literature on climate change. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.14741</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.14741</id><created>2024-05-23</created><updated>2025-02-01</updated><authors><author><keyname>Qian</keyname><forenames>Huajie</forenames></author><author><keyname>Ying</keyname><forenames>Donghao</forenames></author><author><keyname>Lam</keyname><forenames>Henry</forenames></author><author><keyname>Yin</keyname><forenames>Wotao</forenames></author></authors><title>Subsampled Ensemble Can Improve Generalization Tail Exponentially</title><categories>math.OC cs.LG stat.ML</categories><comments>42 pages, 18 figures</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Ensemble learning is a popular technique to improve the accuracy of machine learning models. It traditionally hinges on the rationale that aggregating multiple weak models can lead to better models with lower variance and hence higher stability, especially for discontinuous base learners. In this paper, we provide a new perspective on ensembling. By selecting the best model trained on subsamples via majority voting, we can attain exponentially decaying tails for the excess risk, even if the base learner suffers from slow (i.e., polynomial) decay rates. This tail enhancement power of ensembling is agnostic to the underlying base learner and is stronger than variance reduction in the sense of exhibiting rate improvement. We demonstrate how our ensemble methods can substantially improve out-of-sample performances in a range of numerical examples involving heavy-tailed data or intrinsically slow rates. Code for the proposed methods is available at https://github.com/mickeyhqian/VoteEnsemble. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.15141</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.15141</id><created>2024-05-23</created><updated>2025-02-02</updated><authors><author><keyname>Di Noia</keyname><forenames>Antonio</forenames></author><author><keyname>Ruggeri</keyname><forenames>Fabrizio</forenames></author><author><keyname>Mira</keyname><forenames>Antonietta</forenames></author></authors><title>Likelihood distortion and Bayesian local robustness</title><categories>math.ST stat.ME stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust Bayesian analysis has been mainly devoted to detecting and measuring robustness w.r.t. the prior distribution. Many contributions in the literature aim to define suitable classes of priors which allow the computation of variations of quantities of interest while the prior changes within those classes. The literature has devoted much less attention to the robustness of Bayesian methods w.r.t. the likelihood function due to mathematical and computational complexity, and because it is often arguably considered a more objective choice compared to the prior. In this contribution, we propose a new approach to Bayesian local robustness, mainly focusing on robustness w.r.t. the likelihood function. Successively, we extend it to account for robustness w.r.t. the prior, as well as the prior and the likelihood jointly. This approach is based on the notion of distortion function introduced in the literature on risk theory. The novel robustness measure is a local sensitivity measure that turns out to be very tractable and easy to compute for several classes of distortion functions. Asymptotic properties are derived, and numerical experiments illustrate the theory and its applicability for modelling purposes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.15441</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.15441</id><created>2024-05-24</created><updated>2025-02-02</updated><authors><author><keyname>Wang</keyname><forenames>Jie</forenames></author><author><keyname>Boedihardjo</keyname><forenames>March</forenames></author><author><keyname>Xie</keyname><forenames>Yao</forenames></author></authors><title>Statistical and Computational Guarantees of Kernel Max-Sliced   Wasserstein Distances</title><categories>stat.ML cs.CC cs.LG</categories><comments>34 pages, 8 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal transport has been very successful for various machine learning tasks; however, it is known to suffer from the curse of dimensionality. Hence, dimensionality reduction is desirable when applied to high-dimensional data with low-dimensional structures. The kernel max-sliced (KMS) Wasserstein distance is developed for this purpose by finding an optimal nonlinear mapping that reduces data into $1$ dimension before computing the Wasserstein distance. However, its theoretical properties have not yet been fully developed. In this paper, we provide sharp finite-sample guarantees under milder technical assumptions compared with state-of-the-art for the KMS $p$-Wasserstein distance between two empirical distributions with $n$ samples for general $p\in[1,\infty)$. Algorithm-wise, we show that computing the KMS $2$-Wasserstein distance is NP-hard, and then we further propose a semidefinite relaxation (SDR) formulation (which can be solved efficiently in polynomial time) and provide a relaxation gap for the obtained solution. We provide numerical examples to demonstrate the good performance of our scheme for high-dimensional two-sample testing. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.15643</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.15643</id><created>2024-05-24</created><updated>2025-02-03</updated><authors><author><keyname>Schneider</keyname><forenames>Fabian</forenames></author><author><keyname>Duong</keyname><forenames>Duc-Lam</forenames></author><author><keyname>Lassas</keyname><forenames>Matti</forenames></author><author><keyname>de Hoop</keyname><forenames>Maarten V.</forenames></author><author><keyname>Helin</keyname><forenames>Tapio</forenames></author></authors><title>Scalable diffusion posterior sampling in infinite-dimensional inverse   problems</title><categories>stat.ML cs.LG cs.NA math.AP math.NA math.PR</categories><comments>27 pages, 9 figures</comments><msc-class>62F15, 65N21, 68Q32, 60Hxx, 60Jxx, 68T07, 92C55,</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Score-based diffusion models (SDMs) have emerged as a powerful tool for sampling from the posterior distribution in Bayesian inverse problems. However, existing methods often require multiple evaluations of the forward mapping to generate a single sample, resulting in significant computational costs for large-scale inverse problems. To address this issue, we propose a scalable diffusion posterior sampling (SDPS) method to bypass forward mapping evaluations during sampling by shifting computational effort to an offline training phase, where a task-dependent score is learned based on the forward mapping. Crucially, the conditional posterior score is then derived from this trained score using affine transformations, ensuring no conditional score approximation is needed. The approach is shown to generalize to infinite-dimensional diffusion models and is validated through rigorous convergence analysis and high-dimensional CT imaging experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.16644</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.16644</id><created>2024-05-26</created><updated>2025-02-02</updated><authors><author><keyname>Samsonov</keyname><forenames>Sergey</forenames></author><author><keyname>Moulines</keyname><forenames>Eric</forenames></author><author><keyname>Shao</keyname><forenames>Qi-Man</forenames></author><author><keyname>Zhang</keyname><forenames>Zhuo-Song</forenames></author><author><keyname>Naumov</keyname><forenames>Alexey</forenames></author></authors><title>Gaussian Approximation and Multiplier Bootstrap for Polyak-Ruppert   Averaged Linear Stochastic Approximation with Applications to TD Learning</title><categories>stat.ML cs.LG math.OC math.PR math.ST stat.TH</categories><comments>NeurIPS-2024, camera-ready version. Some typos fixed compared to the   previous version</comments><msc-class>60F05, 62L20, 62E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we obtain the Berry-Esseen bound for multivariate normal approximation for the Polyak-Ruppert averaged iterates of the linear stochastic approximation (LSA) algorithm with decreasing step size. Moreover, we prove the non-asymptotic validity of the confidence intervals for parameter estimation with LSA based on multiplier bootstrap. This procedure updates the LSA estimate together with a set of randomly perturbed LSA estimates upon the arrival of subsequent observations. We illustrate our findings in the setting of temporal difference learning with linear function approximation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.19317</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.19317</id><created>2024-05-29</created><updated>2025-02-02</updated><authors><author><keyname>Kato</keyname><forenames>Masahiro</forenames></author></authors><title>Generalized Neyman Allocation for Locally Minimax Optimal Best-Arm   Identification</title><categories>cs.LG cs.AI econ.EM stat.ME stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study investigates an asymptotically locally minimax optimal algorithm for fixed-budget best-arm identification (BAI). We propose the Generalized Neyman Allocation (GNA) algorithm and demonstrate that its worst-case upper bound on the probability of misidentifying the best arm aligns with the worst-case lower bound under the small-gap regime, where the gap between the expected outcomes of the best and suboptimal arms is small. Our lower and upper bounds are tight, matching exactly including constant terms within the small-gap regime. The GNA algorithm generalizes the Neyman allocation for two-armed bandits (Neyman, 1934; Kaufmann et al., 2016) and refines existing BAI algorithms, such as those proposed by Glynn &amp; Juneja (2004). By proposing an asymptotically minimax optimal algorithm, we address the longstanding open issue in BAI (Kaufmann, 2020) and treatment choice (Kasy &amp; Sautmann, 202) by restricting a class of distributions to the small-gap regimes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.02054</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.02054</id><created>2024-06-04</created><updated>2025-02-03</updated><authors><author><keyname>Guibert</keyname><forenames>Quentin</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Pincemin</keyname><forenames>Gaëlle</forenames><affiliation>LSAF</affiliation></author><author><keyname>Planchet</keyname><forenames>Frédéric</forenames><affiliation>LSAF</affiliation></author></authors><title>Impacts of Climate Change on Mortality: An extrapolation of temperature   effects based on time series data in France</title><categories>stat.AP</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most contemporary mortality models rely on extrapolating trends or past events. However, population dynamics will be significantly impacted by climate change, notably the influence of temperatures on mortality. In this paper, we introduce a novel approach to incorporate temperature effects on projected mortality using a multi-population mortality model. This method combines a stochastic mortality model with a climate epidemiology model, predicting mortality variations due to daily temperature fluctuations, be it excesses or insufficiencies. The significance of this approach lies in its ability to disrupt mortality projections by utilizing temperature forecasts from climate models and to assess the impact of this unaccounted risk factor in conventional mortality models. We illustrate this proposed mortality model using French data stratified by gender, focusing on past temperatures and mortality. Utilizing climate model predictions across various IPCC scenarios, we investigate gains and loss in life expectancy linked to temperatures and the additional mortality induced by extreme heatwaves, and quantify them by assessing this new risk factor in prediction intervals. Furthermore, we analyze the geographical differences across the Metropolitan France. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.05189</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.05189</id><created>2024-06-07</created><updated>2025-01-31</updated><authors><author><keyname>Lam</keyname><forenames>Jorden</forenames></author><author><keyname>Xu</keyname><forenames>Kunpeng</forenames></author></authors><title>Analyzing the factors that are involved in length of inpatient stay at   the hospital for diabetes patients</title><categories>stat.AP cs.AI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The paper investigates the escalating concerns surrounding the surge in diabetes cases, exacerbated by the COVID-19 pandemic, and the subsequent strain on medical resources. The research aims to construct a predictive model quantifying factors influencing inpatient hospital stay durations for diabetes patients, offering insights to hospital administrators for improved patient management strategies. The literature review highlights the increasing prevalence of diabetes, emphasizing the need for continued attention and analysis of urban-rural disparities in healthcare access. International studies underscore the financial implications and healthcare burden associated with diabetes-related hospitalizations and complications, emphasizing the significance of effective management strategies. The methodology involves a quantitative approach, utilizing a dataset comprising 10,000 observations of diabetic inpatient encounters in U.S. hospitals from 1999 to 2008. Predictive modeling techniques, particularly Generalized Linear Models (GLM), are employed to develop a model predicting hospital stay durations based on patient demographics, admission types, medical history, and treatment regimen. The results highlight the influence of age, medical history, and treatment regimen on hospital stay durations for diabetes patients. Despite model limitations, such as heteroscedasticity and deviations from normality in residual analysis, the findings offer valuable insights for hospital administrators in patient management. The paper concludes with recommendations for future research to address model limitations and explore the implications of predictive models on healthcare management strategies, ensuring equitable patient care and resource allocation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.10499</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.10499</id><created>2024-06-15</created><updated>2025-02-01</updated><authors><author><keyname>Luo</keyname><forenames>Fangzhi</forenames></author><author><keyname>Tan</keyname><forenames>Jianbin</forenames></author><author><keyname>Zhang</keyname><forenames>Donglan</forenames></author><author><keyname>Huang</keyname><forenames>Hui</forenames></author><author><keyname>Shen</keyname><forenames>Ye</forenames></author></authors><title>Functional Clustering for Longitudinal Associations between Social   Determinants of Health and Stroke Mortality in the US</title><categories>stat.ME stat.AP</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Understanding the longitudinally changing associations between Social Determinants of Health (SDOH) and stroke mortality is essential for effective stroke management. Previous studies have uncovered significant regional disparities in the relationships between SDOH and stroke mortality. However, existing studies have not utilized longitudinal associations to develop data-driven methods for regional division in stroke control. To fill this gap, we propose a novel clustering method to analyze SDOH -- stroke mortality associations in US counties. To enhance the interpretability of the clustering outcomes, we introduce a novel regularized expectation-maximization algorithm equipped with various sparsity-and-smoothness-pursued penalties, aiming at simultaneous clustering and variable selection in longitudinal associations. As a result, we can identify crucial SDOH that contribute to longitudinal changes in stroke mortality. This facilitates the clustering of US counties into different regions based on the relationships between these SDOH and stroke mortality. The effectiveness of our proposed method is demonstrated through extensive numerical studies. By applying our method to longitudinal data on SDOH and stroke mortality at the county level, we identify 18 important SDOH for stroke mortality and divide the US counties into two clusters based on these selected SDOH. Our findings unveil complex regional heterogeneity in the longitudinal associations between SDOH and stroke mortality, providing valuable insights into region-specific SDOH adjustments for mitigating stroke mortality. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.13876</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.13876</id><created>2024-06-19</created><updated>2025-02-02</updated><authors><author><keyname>Xin</keyname><forenames>Huqin</forenames></author><author><keyname>Zhao</keyname><forenames>Sihai Dave</forenames></author></authors><title>An Empirical Bayes Jackknife Regression Framework for Covariance Matrix   Estimation</title><categories>stat.ME</categories><comments>13 pages, 3 figures</comments><msc-class>62C25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Covariance matrix estimation, a classical statistical topic, poses significant challenges when the sample size is comparable to or smaller than the number of features. In this paper, we frame covariance matrix estimation as a compound decision problem and apply an optimal decision rule to estimate covariance parameters. To approximate this rule, we introduce an algorithm that integrates jackknife techniques with machine learning regression methods. This algorithm exhibits adaptability across diverse scenarios without relying on assumptions about data distribution. Simulation results and gene network inference from an RNA-seq experiment in mice demonstrate that our approach either matches or surpasses several state-of-the-art methods </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.14426</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.14426</id><created>2024-06-20</created><updated>2025-02-01</updated><authors><author><keyname>Klein</keyname><forenames>Leon</forenames></author><author><keyname>Noé</keyname><forenames>Frank</forenames></author></authors><title>Transferable Boltzmann Generators</title><categories>stat.ML cs.LG physics.chem-ph physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generation of equilibrium samples of molecular systems has been a long-standing problem in statistical physics. Boltzmann Generators are a generative machine learning method that addresses this issue by learning a transformation via a normalizing flow from a simple prior distribution to the target Boltzmann distribution of interest. Recently, flow matching has been employed to train Boltzmann Generators for small molecular systems in Cartesian coordinates. We extend this work and propose a first framework for Boltzmann Generators that are transferable across chemical space, such that they predict zero-shot Boltzmann distributions for test molecules without being retrained for these systems. These transferable Boltzmann Generators allow approximate sampling from the target distribution of unseen systems, as well as efficient reweighting to the target Boltzmann distribution. The transferability of the proposed framework is evaluated on dipeptides, where we show that it generalizes efficiently to unseen systems. Furthermore, we demonstrate that our proposed architecture enhances the efficiency of Boltzmann Generators trained on single molecular systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.05108</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.05108</id><created>2024-07-06</created><authors><author><keyname>Lyu</keyname><forenames>Shen-Huan</forenames></author><author><keyname>Wu</keyname><forenames>Jin-Hui</forenames></author><author><keyname>Zheng</keyname><forenames>Qin-Cheng</forenames></author><author><keyname>Ye</keyname><forenames>Baoliu</forenames></author></authors><title>The Role of Depth, Width, and Tree Size in Expressiveness of Deep Forest</title><categories>cs.LG stat.ML</categories><journal-ref>Proceedings of the 27th European Conference on Artificial   Intelligence, pp. 2042-2049, Santiago de Compostela, Spain, 2024</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random forests are classical ensemble algorithms that construct multiple randomized decision trees and aggregate their predictions using naive averaging. \citet{zhou2019deep} further propose a deep forest algorithm with multi-layer forests, which outperforms random forests in various tasks. The performance of deep forests is related to three hyperparameters in practice: depth, width, and tree size, but little has been known about its theoretical explanation. This work provides the first upper and lower bounds on the approximation complexity of deep forests concerning the three hyperparameters. Our results confirm the distinctive role of depth, which can exponentially enhance the expressiveness of deep forests compared with width and tree size. Experiments confirm the theoretical findings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.13195</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.13195</id><created>2024-07-18</created><updated>2025-01-31</updated><authors><author><keyname>Li</keyname><forenames>Yingru</forenames></author><author><keyname>Xu</keyname><forenames>Jiawei</forenames></author><author><keyname>Wang</keyname><forenames>Baoxiang</forenames></author><author><keyname>Luo</keyname><forenames>Zhi-Quan</forenames></author></authors><title>Scalable Thompson Sampling via Ensemble++ Agent</title><categories>cs.LG cs.AI cs.HC cs.IT math.IT stat.ML</categories><comments>47 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Thompson Sampling is a principled method for balancing exploration and exploitation, but its real-world adoption is impeded by the high computational overhead of posterior maintenance in large-scale or non-conjugate settings. Ensemble-based approaches offer partial remedies, but often require a large ensemble size. This paper proposes the Ensemble++, a scalable agent that sidesteps these limitations by a shared-factor ensemble update architecture and a random linear combination scheme. We theoretically justify that in linear bandits, Ensemble++ agent only needs an ensemble size of $\Theta(d \log T)$ to achieve regret guarantees comparable to exact Thompson Sampling. Further, to handle nonlinear rewards and complex environments. we introduce a neural extension that replaces fixed features with a learnable representation, preserving the same underlying objective via gradient-based updates. Empirical results confirm that Ensemble++ agent excel in both sample efficiency and computational scalability across linear and nonlinear environments, including GPT-based contextual bandits. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.14369</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.14369</id><created>2024-07-19</created><updated>2025-02-02</updated><authors><author><keyname>Baumer</keyname><forenames>Benjamin S.</forenames></author><author><keyname>Sierra</keyname><forenames>Biviana Marcela Suarez</forenames></author></authors><title>tidychangepoint: a unified framework for analyzing changepoint detection   in univariate time series</title><categories>stat.ME stat.CO</categories><msc-class>62P99</msc-class><acm-class>G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present tidychangepoint, a new R package for changepoint detection analysis. Most R packages for segmenting univariate time series focus on providing one or two algorithms for changepoint detection that work with a small set of models and penalized objective functions, and all of them return a custom, nonstandard object type. This makes comparing results across various algorithms, models, and penalized objective functions unnecessarily difficult. tidychangepoint solves this problem by wrapping functions from a variety of existing packages and storing the results in a common S3 class called tidycpt. The package then provides functionality for easily extracting comparable numeric or graphical information from a tidycpt object, all in a tidyverse-compliant framework. tidychangepoint is versatile: it supports both deterministic algorithms like PELT (from changepoint), and also flexible, randomized, genetic algorithms (via GA) that -- via new functionality built into tidychangepoint -- can be used with any compliant model-fitting function and any penalized objective function. By bringing all of these disparate tools together in a cohesive fashion, tidychangepoint facilitates comparative analysis of changepoint detection algorithms and models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.15401</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.15401</id><created>2024-07-22</created><updated>2025-02-02</updated><authors><author><keyname>de Beer</keyname><forenames>Alex</forenames></author><author><keyname>Power</keyname><forenames>Andrew</forenames></author><author><keyname>Wong</keyname><forenames>Daniel</forenames></author><author><keyname>Dekkers</keyname><forenames>Ken</forenames></author><author><keyname>Gravatt</keyname><forenames>Michael</forenames></author><author><keyname>Bjarkason</keyname><forenames>Elvar K.</forenames></author><author><keyname>O'Sullivan</keyname><forenames>John P.</forenames></author><author><keyname>O'Sullivan</keyname><forenames>Michael J.</forenames></author><author><keyname>Maclaren</keyname><forenames>Oliver J.</forenames></author><author><keyname>Nicholson</keyname><forenames>Ruanui</forenames></author></authors><title>Data Space Inversion for Efficient Predictions and Uncertainty   Quantification for Geothermal Models</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The ability to make accurate predictions with quantified uncertainty provides a crucial foundation for the successful management of a geothermal reservoir. Conventional approaches for making predictions using geothermal reservoir models involve estimating unknown model parameters using field data, then propagating the uncertainty in these estimates through to the predictive quantities of interest. However, the unknown parameters are not always of direct interest; instead, the predictions are of primary importance. Data space inversion (DSI) is an alternative methodology that allows for the efficient estimation of predictive quantities of interest, with quantified uncertainty, that avoids the need to estimate model parameters entirely. In this paper, we illustrate the applicability of DSI to geothermal reservoir modelling. We first review the processes of model calibration, prediction and uncertainty quantification from a Bayesian perspective, and introduce data space inversion as a simple, efficient technique for approximating the posterior predictive distribution. We then introduce a modification of the typical DSI algorithm that allows us to sample directly and efficiently from the DSI approximation to the posterior predictive distribution, and apply the algorithm to two model problems in geothermal reservoir modelling. We evaluate the accuracy and efficiency of our DSI algorithm relative to other common methods for uncertainty quantification and study how the number of reservoir model simulations affects the resulting approximation to the posterior predictive distribution. Our results demonstrate that data space inversion is a robust and efficient technique for making predictions with quantified uncertainty using geothermal reservoir models, providing a useful alternative to more conventional approaches. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.16212</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.16212</id><created>2024-07-23</created><authors><author><keyname>Huan</keyname><forenames>Xun</forenames></author><author><keyname>Jagalur</keyname><forenames>Jayanth</forenames></author><author><keyname>Marzouk</keyname><forenames>Youssef</forenames></author></authors><title>Optimal experimental design: Formulations and computations</title><categories>stat.ME cs.NA math.NA stat.CO</categories><comments>Appears in Acta Numerica 2024. This version contains an evolving set   of post-publication additions and corrections</comments><journal-ref>Acta Numerica 33 (2024) 715-840</journal-ref><doi>10.1017/S0962492924000023</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Questions of `how best to acquire data' are essential to modeling and prediction in the natural and social sciences, engineering applications, and beyond. Optimal experimental design (OED) formalizes these questions and creates computational methods to answer them. This article presents a systematic survey of modern OED, from its foundations in classical design theory to current research involving OED for complex models. We begin by reviewing criteria used to formulate an OED problem and thus to encode the goal of performing an experiment. We emphasize the flexibility of the Bayesian and decision-theoretic approach, which encompasses information-based criteria that are well-suited to nonlinear and non-Gaussian statistical models. We then discuss methods for estimating or bounding the values of these design criteria; this endeavor can be quite challenging due to strong nonlinearities, high parameter dimension, large per-sample costs, or settings where the model is implicit. A complementary set of computational issues involves optimization methods used to find a design; we discuss such methods in the discrete (combinatorial) setting of observation selection and in settings where an exact design can be continuously parameterized. Finally we present emerging methods for sequential OED that build non-myopic design policies, rather than explicit designs; these methods naturally adapt to the outcomes of past experiments in proposing new experiments, while seeking coordination among all experiments to be performed. Throughout, we highlight important open questions and challenges. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.02295</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.02295</id><created>2024-08-05</created><updated>2025-02-03</updated><authors><author><keyname>Kim</keyname><forenames>Seyeon</forenames></author><author><keyname>Lee</keyname><forenames>Joonhun</forenames></author><author><keyname>Cho</keyname><forenames>Namhoon</forenames></author><author><keyname>Han</keyname><forenames>Sungjun</forenames></author><author><keyname>Hwang</keyname><forenames>Wooseop</forenames></author></authors><title>Generalized Gaussian Temporal Difference Error for Uncertainty-aware   Reinforcement Learning</title><categories>cs.LG cs.AI math.PR stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Conventional uncertainty-aware temporal difference (TD) learning often assumes a zero-mean Gaussian distribution for TD errors, leading to inaccurate error representations and compromised uncertainty estimation. We introduce a novel framework for generalized Gaussian error modeling in deep reinforcement learning to enhance the flexibility of error distribution modeling by incorporating additional higher-order moment, particularly kurtosis, thereby improving the estimation and mitigation of data-dependent aleatoric uncertainty. We examine the influence of the shape parameter of the generalized Gaussian distribution (GGD) on aleatoric uncertainty and provide a closed-form expression that demonstrates an inverse relationship between uncertainty and the shape parameter. Additionally, we propose a theoretically grounded weighting scheme to address epistemic uncertainty by fully leveraging the GGD. We refine batch inverse variance weighting with bias reduction and kurtosis considerations, enhancing robustness. Experiments with policy gradient algorithms demonstrate significant performance gains. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.14618</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.14618</id><created>2024-08-26</created><updated>2025-02-03</updated><authors><author><keyname>Deitmar</keyname><forenames>Ben</forenames></author></authors><title>Marchenko-Pastur law for Daniell smoothed periodograms without   simultaneous diagonalizability</title><categories>math.ST stat.TH</categories><comments>40 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The eigenvectors of a spectral density matrix $F(\theta)$ to a stationary Gaussian process $(X_t)_{t \in \mathbb{Z}}$ depend explicitly on the frequency $\theta \in [0,2\pi]$. The most commonly used estimator of the spectral density matrix $F(\theta)$ is the smoothed periodogram, which takes the form $ZZ^*$ for random matrices $Z$ with non-zero covariance between rows and columns. When the covariance matrices of the columns are not simultaneously diagonalizable, this covariance structure is non-separable and such matrices $ZZ^*$ are out of reach for the current state of random matrix theory. In this paper, we derive a Marchenko-Pastur law in this non-simultaneously diagonalizable case. The Marchenko-Pastur law emerges when the dimension $d$ of the process and the smoothing span $m$ of the smoothed periodogram grow at the same rate, which is slower than the number of observations $n$.   On the technical level we prove a trace moment bound for matrices $YY^T$, where $Y$ is a matrix with correlated Gaussian entries. This allows for sub-polynomial error bounds in settings where the error $Y$ has correlations between different points in time as well as between features. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.02521</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.02521</id><created>2024-09-04</created><updated>2025-02-01</updated><authors><author><keyname>Filipovic</keyname><forenames>Damir</forenames></author><author><keyname>Schneider</keyname><forenames>Paul</forenames></author></authors><title>Fundamental properties of linear factor models</title><categories>q-fin.ST stat.AP</categories><msc-class>62P20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study conditional linear factor models in the context of asset pricing panels. Our analysis focuses on conditional means and covariances to characterize the cross-sectional and inter-temporal properties of returns and factors as well as their interrelationships. We also review the conditions outlined in Kozak and Nagel (2024) and show how the conditional mean-variance efficient portfolio of an unbalanced panel can be spanned by low-dimensional factor portfolios, even without assuming invertibility of the conditional covariance matrices. Our analysis provides a comprehensive foundation for the specification and estimation of conditional linear factor models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.13876</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.13876</id><created>2024-09-20</created><updated>2025-02-03</updated><authors><author><keyname>Hamelijnck</keyname><forenames>Oliver</forenames></author><author><keyname>Solin</keyname><forenames>Arno</forenames></author><author><keyname>Damoulas</keyname><forenames>Theodoros</forenames></author></authors><title>Physics-Informed Variational State-Space Gaussian Processes</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Differential equations are important mechanistic models that are integral to many scientific and engineering applications. With the abundance of available data there has been a growing interest in data-driven physics-informed models. Gaussian processes (GPs) are particularly suited to this task as they can model complex, non-linear phenomena whilst incorporating prior knowledge and quantifying uncertainty. Current approaches have found some success but are limited as they either achieve poor computational scalings or focus only on the temporal setting. This work addresses these issues by introducing a variational spatio-temporal state-space GP that handles linear and non-linear physical constraints while achieving efficient linear-in-time computation costs. We demonstrate our methods in a range of synthetic and real-world settings and outperform the current state-of-the-art in both predictive and computational performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.19208</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.19208</id><created>2024-09-27</created><updated>2025-02-01</updated><authors><author><keyname>Chakraborty</keyname><forenames>Anirban</forenames></author><author><keyname>Katzfuss</keyname><forenames>Matthias</forenames></author></authors><title>Learning non-Gaussian spatial distributions via Bayesian transport maps   with parametric shrinkage</title><categories>stat.CO stat.AP stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications, including climate-model analysis and stochastic weather generators, require learning or emulating the distribution of a high-dimensional and non-Gaussian spatial field based on relatively few training samples. To address this challenge, a recently proposed Bayesian transport map (BTM) approach consists of a triangular transport map with nonparametric Gaussian-process (GP) components, which is trained to transform the distribution of interest distribution to a Gaussian reference distribution. To improve the performance of this existing BTM, we propose to shrink the map components toward a ``base'' parametric Gaussian family combined with a Vecchia approximation for scalability. The resulting ShrinkTM approach is more accurate than the existing BTM, especially for small numbers of training samples. It can even outperform the ``base'' family when trained on a single sample of the spatial field. We demonstrate the advantage of ShrinkTM though numerical experiments on simulated data and on climate-model output. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.01196</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.01196</id><created>2024-10-01</created><updated>2025-02-02</updated><authors><author><keyname>Miller</keyname><forenames>John Joshua</forenames></author><author><keyname>Mak</keyname><forenames>Simon</forenames></author><author><keyname>Sun</keyname><forenames>Benny</forenames></author><author><keyname>Narayanan</keyname><forenames>Sai Ranjeet</forenames></author><author><keyname>Yang</keyname><forenames>Suo</forenames></author><author><keyname>Sun</keyname><forenames>Zongxuan</forenames></author><author><keyname>Kim</keyname><forenames>Kenneth S.</forenames></author><author><keyname>Kweon</keyname><forenames>Chol-Bum Mike</forenames></author></authors><title>Expected Diverse Utility (EDU): Diverse Bayesian Optimization of   Expensive Computer Simulators</title><categories>stat.AP cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The optimization of expensive black-box simulators arises in a myriad of modern scientific and engineering applications. Bayesian optimization provides an appealing solution, by leveraging a fitted surrogate model to guide the selection of subsequent simulator evaluations. In practice, however, the objective is often not to obtain a single good solution, but rather a ``basket'' of good solutions from which users can choose for downstream decision-making. This need arises in our motivating application for real-time control of internal combustion engines for flight propulsion, where a diverse set of control strategies is essential for stable flight control. There has been little work on this front for Bayesian optimization. We thus propose a new Expected Diverse Utility (EDU) method that searches for diverse ``$\epsilon$-optimal'' solutions: locally-optimal solutions within a tolerance level $\epsilon &gt; 0$ from a global optimum. We show that EDU yields a closed-form acquisition function under a Gaussian process surrogate model, which facilitates efficient sequential queries via automatic differentiation. This closed form further reveals a novel exploration-exploitation-diversity trade-off, which incorporates the desired diversity property within the well-known exploration-exploitation trade-off. We demonstrate the improvement of EDU over existing methods in a suite of numerical experiments, then explore the EDU in two applications on rover trajectory optimization and engine control for flight propulsion. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.02199</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.02199</id><created>2024-10-03</created><updated>2025-02-02</updated><authors><author><keyname>Hashimoto</keyname><forenames>Yuka</forenames></author><author><keyname>Iwata</keyname><forenames>Tomoharu</forenames></author></authors><title>Deep Koopman-layered Model with Universal Property Based on Toeplitz   Matrices</title><categories>cs.LG math.DS math.FA stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose deep Koopman-layered models with learnable parameters in the form of Toeplitz matrices for analyzing the transition of the dynamics of time-series data. The proposed model has both theoretical solidness and flexibility. By virtue of the universal property of Toeplitz matrices and the reproducing property underlying the model, we can show its universality and generalization property. In addition, the flexibility of the proposed model enables the model to fit time-series data coming from nonautonomous dynamical systems. When training the model, we apply Krylov subspace methods for efficient computations. In this sense, the proposed model establishes a new connection between Koopman operators and numerical linear algebraic methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.02724</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.02724</id><created>2024-10-03</created><updated>2025-02-02</updated><authors><author><keyname>Zekri</keyname><forenames>Oussama</forenames></author><author><keyname>Odonnat</keyname><forenames>Ambroise</forenames></author><author><keyname>Benechehab</keyname><forenames>Abdelhakim</forenames></author><author><keyname>Bleistein</keyname><forenames>Linus</forenames></author><author><keyname>Boullé</keyname><forenames>Nicolas</forenames></author><author><keyname>Redko</keyname><forenames>Ievgen</forenames></author></authors><title>Large Language Models as Markov Chains</title><categories>stat.ML cs.AI cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large language models (LLMs) are remarkably efficient across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the LLMs' generalization capabilities remains elusive. In our paper, we approach this task by drawing an equivalence between autoregressive transformer-based language models and Markov chains defined on a finite state space. This allows us to study the multi-step inference mechanism of LLMs from first principles. We relate the obtained results to the pathological behavior observed with LLMs such as repetitions and incoherent replies with high temperature. Finally, we leverage the proposed formalization to derive pre-training and in-context learning generalization bounds for LLMs under realistic data and model assumptions. Experiments with the most recent Llama and Gemma herds of models show that our theory correctly captures their behavior in practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.10546</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.10546</id><created>2024-10-14</created><updated>2025-01-31</updated><authors><author><keyname>Alain</keyname><forenames>Mathieu</forenames></author><author><keyname>Takao</keyname><forenames>So</forenames></author><author><keyname>Dong</keyname><forenames>Xiaowen</forenames></author><author><keyname>Rieck</keyname><forenames>Bastian</forenames></author><author><keyname>Noutahi</keyname><forenames>Emmanuel</forenames></author></authors><title>Graph Classification Gaussian Processes via Hodgelet Spectral Features</title><categories>cs.LG stat.ML</categories><comments>NeurIPS 2024 Workshop on Bayesian Decision-Making and Uncertainty   (Spotlight)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The problem of classifying graphs is ubiquitous in machine learning. While it is standard to apply graph neural networks or graph kernel methods, Gaussian processes can be employed by transforming spatial features from the graph domain into spectral features in the Euclidean domain, and using them as the input points of classical kernels. However, this approach currently only takes into account features on vertices, whereas some graph datasets also support features on edges. In this work, we present a Gaussian process-based classification algorithm that can leverage one or both vertex and edges features. Furthermore, we take advantage of the Hodge decomposition to better capture the intricate richness of vertex and edge features, which can be beneficial on diverse tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.15729</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.15729</id><created>2024-10-21</created><updated>2025-02-02</updated><authors><author><keyname>Montreuil</keyname><forenames>Yannis</forenames></author><author><keyname>Yeo</keyname><forenames>Shu Heng</forenames></author><author><keyname>Carlier</keyname><forenames>Axel</forenames></author><author><keyname>Ng</keyname><forenames>Lai Xing</forenames></author><author><keyname>Ooi</keyname><forenames>Wei Tsang</forenames></author></authors><title>A Two-Stage Learning-to-Defer Approach for Multi-Task Learning</title><categories>stat.ML cs.HC cs.LG</categories><comments>32 pages, 17 main paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Two-Stage Learning-to-Defer framework has been extensively studied for classification and, more recently, regression tasks. However, many contemporary applications involve both classification and regression in an interdependent manner. In this work, we introduce a novel Two-Stage Learning-to-Defer framework for multi-task learning that jointly addresses these tasks. Our approach leverages a two-stage surrogate loss family, which we prove to be both ($\mathcal{G}, \mathcal{R}$)-consistent and Bayes-consistent, providing strong theoretical guarantees of convergence to the Bayes-optimal rejector. We establish consistency bounds explicitly linked to the cross-entropy surrogate family and the $L_1$-norm of the agents' costs, extending the theoretical minimizability gap analysis to the two-stage setting with multiple experts. We validate our framework on two challenging tasks: object detection, where classification and regression are tightly coupled, and existing methods fail, and electronic health record analysis, in which we highlight the suboptimality of current learning-to-defer approaches. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.18268</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.18268</id><created>2024-10-23</created><updated>2025-01-31</updated><authors><author><keyname>Adrian</keyname><forenames>Melissa</forenames></author><author><keyname>Soloff</keyname><forenames>Jake A.</forenames></author><author><keyname>Willett</keyname><forenames>Rebecca</forenames></author></authors><title>Stabilizing black-box model selection with the inflated argmax</title><categories>stat.ML cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Model selection is the process of choosing from a class of candidate models given data. For instance, methods such as the LASSO and sparse identification of nonlinear dynamics (SINDy) formulate model selection as finding a sparse solution to a linear system of equations determined by training data. However, absent strong assumptions, such methods are highly unstable: if a single data point is removed from the training set, a different model may be selected. In this paper, we present a new approach to stabilizing model selection with theoretical stability guarantees that leverages a combination of bagging and an ''inflated'' argmax operation. Our method selects a small collection of models that all fit the data, and it is stable in that, with high probability, the removal of any training point will result in a collection of selected models that overlaps with the original collection. We illustrate this method in (a) a simulation in which strongly correlated covariates make standard LASSO model selection highly unstable, (b) a Lotka-Volterra model selection problem focused on identifying how competition in an ecosystem influences species' abundances, and (c) a graph subset selection problem using cell-signaling data from proteomics. In these settings, the proposed method yields stable, compact, and accurate collections of selected models, outperforming a variety of benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.19019</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.19019</id><created>2024-10-24</created><updated>2025-02-02</updated><authors><author><keyname>Attia</keyname><forenames>Iman Mohammed</forenames></author></authors><title>Median Based Unit Weibull (MBUW): a new unit distribution Properties</title><categories>stat.ME math.PR</categories><comments>arXiv admin note: text overlap with arXiv:2410.04132 , this a new   update as the new update contains real data anlysis ( 6 data sets ) to   illustrate the benefit of the distribution</comments><msc-class>60E05, 62E15, 62E10, 62F10, 62F03</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A new 2 parameter unit Weibull distribution is defined on the unit interval (0,1). The methodology of deducing its PDF, some of its properties and related functions are discussed. The paper is supplied by many figures illustrating the new distribution and how this can make it illegible to fit a wide range of skewed data. The new distribution holds a name (Attia) as a nickname. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.24050</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.24050</id><created>2024-10-31</created><updated>2025-02-02</updated><authors><author><keyname>Odonnat</keyname><forenames>Ambroise</forenames></author><author><keyname>Bouaziz</keyname><forenames>Wassim</forenames></author><author><keyname>Cabannes</keyname><forenames>Vivien</forenames></author></authors><title>Clustering Head: A Visual Case Study of the Training Dynamics in   Transformers</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper introduces the sparse modular addition task and examines how transformers learn it. We focus on transformers with embeddings in $\R^2$ and introduce a visual sandbox that provides comprehensive visualizations of each layer throughout the training process. We reveal a type of circuit, called "clustering heads," which learns the problem's invariants. We analyze the training dynamics of these circuits, highlighting two-stage learning, loss spikes due to high curvature or normalization layers, and the effects of initialization and curriculum learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.00945</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.00945</id><created>2024-11-01</created><updated>2025-02-03</updated><authors><author><keyname>Bayati</keyname><forenames>Mohsen</forenames></author><author><keyname>Luo</keyname><forenames>Yuwei</forenames></author><author><keyname>Overman</keyname><forenames>William</forenames></author><author><keyname>Shirani</keyname><forenames>Sadegh</forenames></author><author><keyname>Xiong</keyname><forenames>Ruoxuan</forenames></author></authors><title>Higher-Order Causal Message Passing for Experimentation with Complex   Interference</title><categories>cs.LG econ.EM stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate estimation of treatment effects is essential for decision-making across various scientific fields. This task, however, becomes challenging in areas like social sciences and online marketplaces, where treating one experimental unit can influence outcomes for others through direct or indirect interactions. Such interference can lead to biased treatment effect estimates, particularly when the structure of these interactions is unknown. We address this challenge by introducing a new class of estimators based on causal message-passing, specifically designed for settings with pervasive, unknown interference. Our estimator draws on information from the sample mean and variance of unit outcomes and treatments over time, enabling efficient use of observed data to estimate the evolution of the system state. Concretely, we construct non-linear features from the moments of unit outcomes and treatments and then learn a function that maps these features to future mean and variance of unit outcomes. This allows for the estimation of the treatment effect over time. Extensive simulations across multiple domains, using synthetic and real network data, demonstrate the efficacy of our approach in estimating total treatment effect dynamics, even in cases where interference exhibits non-monotonic behavior in the probability of treatment. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.01375</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.01375</id><created>2024-11-02</created><updated>2025-02-02</updated><authors><author><keyname>Arnal</keyname><forenames>Charles</forenames></author><author><keyname>Berenfeld</keyname><forenames>Clement</forenames></author><author><keyname>Rosenberg</keyname><forenames>Simon</forenames></author><author><keyname>Cabannes</keyname><forenames>Vivien</forenames></author></authors><title>Learning with Hidden Factorial Structure</title><categories>stat.ML cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Statistical learning in high-dimensional spaces is challenging without a strong underlying data structure. Recent advances with foundational models suggest that text and image data contain such hidden structures, which help mitigate the curse of dimensionality. Inspired by results from nonparametric statistics, we hypothesize that this phenomenon can be partially explained in terms of decomposition of complex tasks into simpler subtasks. In this paper, we present a controlled experimental framework to test whether neural networks can indeed exploit such "hidden factorial structures". We find that they do leverage these latent patterns to learn discrete distributions more efficiently. We also study the interplay between our structural assumptions and the models' capacity for generalization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.05391</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.05391</id><created>2024-11-08</created><updated>2025-02-03</updated><authors><author><keyname>Hoessly</keyname><forenames>Linard</forenames></author></authors><title>On the Impossibility of Equating the Youden Index with Tjur's $R^2$-like   metrics in $2\times 2$ Tables</title><categories>stat.OT math.PR math.ST stat.TH</categories><msc-class>62C99, 62H17, 62P10, , 62J12</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In 2017, Hughes claimed an equivalence between Tjurs $R^2$ coefficient of discrimination and the Youden index for assessing diagnostic test performance on $2\times 2$ contingency tables. We prove an impossibility result when averaging over binary outcomes (0s and 1s) under any continuous real-valued scoring rule. Our finding clarifies the limitations of such a possible equivalence and highlights the distinct roles these metrics play in diagnostic test assessment. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.10394</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.10394</id><created>2024-11-15</created><updated>2025-02-02</updated><authors><author><keyname>Améndola</keyname><forenames>Carlos</forenames></author><author><keyname>Ferry</keyname><forenames>Kamillo</forenames></author></authors><title>Tropical combinatorics of max-linear Bayesian networks</title><categories>math.CO math.AG math.ST stat.TH</categories><comments>22 pages, 8 figures, 1 table. v2: added clarifying remarks in Section   3</comments><msc-class>05C12, 14T90, 52B11, 62R01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A polytrope is a tropical polyhedron that is also classically convex. We study the tropical combinatorial types of polytropes associated to weighted directed acyclic graphs (DAGs). This family of polytropes arises in algebraic statistics when describing the model class of max-linear Bayesian networks. We show how the edge weights of a network directly relate to the facet structure of the corresponding polytrope. We also give a classification of polytropes from weighted DAGs at different levels of equivalence. These results give insight on the statistical problem of identifiability for a max-linear Bayesian network. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.00798</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.00798</id><created>2024-12-01</created><updated>2025-02-03</updated><authors><author><keyname>Song</keyname><forenames>Seockbean</forenames></author><author><keyname>Yoon</keyname><forenames>Youngsik</forenames></author><author><keyname>Wang</keyname><forenames>Siwei</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Ok</keyname><forenames>Jungseul</forenames></author></authors><title>Combinatorial Rising Bandit</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Combinatorial online learning is a fundamental task to decide the optimal combination of base arms in sequential interactions with systems providing uncertain rewards, which is applicable to diverse domains such as robotics, social advertising, network routing and recommendation systems. In real-world scenarios, we often observe rising rewards, where the selection of a base arm not only provides an instantaneous reward but also contributes to the enhancement of future rewards, {\it e.g.}, robots enhancing proficiency through practice and social influence strengthening in the history of successful recommendations. To address this, we introduce the problem of combinatorial rising bandit to minimize policy regret and propose a provably efficient algorithm, called Combinatorial Rising Upper Confidence Bound (CRUCB), of which regret upper bound is close to a regret lower bound. To the best of our knowledge, previous studies do not provide a sub-linear regret lower bound, making it impossible to assess the efficiency of their algorithms. However, we provide the sub-linear regret lower bound for combinatorial rising bandit and show that CRUCB is provably efficient by showing that the regret upper bound is close to the regret lower bound. In addition, we empirically demonstrate the effectiveness and superiority of CRUCB not only in synthetic environments but also in realistic applications of deep reinforcement learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.02409</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.02409</id><created>2025-01-04</created><updated>2025-02-01</updated><authors><author><keyname>Lin</keyname><forenames>Zaikang</forenames></author><author><keyname>Chang</keyname><forenames>Sei</forenames></author><author><keyname>Zweig</keyname><forenames>Aaron</forenames></author><author><keyname>Kang</keyname><forenames>Minseo</forenames></author><author><keyname>Azizi</keyname><forenames>Elham</forenames></author><author><keyname>Knowles</keyname><forenames>David A.</forenames></author></authors><title>Interpretable Neural ODEs for Gene Regulatory Network Discovery under   Perturbations</title><categories>cs.LG cs.AI cs.CE q-bio.MN stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern high-throughput biological datasets with thousands of perturbations provide the opportunity for large-scale discovery of causal graphs that represent the regulatory interactions between genes. Differentiable causal graphical models have been proposed to infer a gene regulatory network (GRN) from large scale interventional datasets, capturing the causal gene regulatory relationships from genetic perturbations. However, existing models are limited in their expressivity and scalability while failing to address the dynamic nature of biological processes such as cellular differentiation. We propose PerturbODE, a novel framework that incorporates biologically informative neural ordinary differential equations (neural ODEs) to model cell state trajectories under perturbations and derive the causal GRN from the neural ODE's parameters. We demonstrate PerturbODE's efficacy in trajectory prediction and GRN inference across simulated and real over-expression datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.11421</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.11421</id><created>2025-01-20</created><updated>2025-02-02</updated><authors><author><keyname>Chandran</keyname><forenames>G Dhinesh</forenames></author><author><keyname>Kota</keyname><forenames>Srinivas Reddy</forenames></author><author><keyname>Bhashyam</keyname><forenames>Srikrishna</forenames></author></authors><title>Online Clustering with Bandit Information</title><categories>cs.LG cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of online clustering within the multi-armed bandit framework under the fixed confidence setting. In this multi-armed bandit problem, we have $M$ arms, each providing i.i.d. samples that follow a multivariate Gaussian distribution with an {\em unknown} mean and a known unit covariance. The arms are grouped into $K$ clusters based on the distance between their means using the Single Linkage (SLINK) clustering algorithm on the means of the arms. Since the true means are unknown, the objective is to obtain the above clustering of the arms with the minimum number of samples drawn from the arms, subject to an upper bound on the error probability. We introduce a novel algorithm, Average Tracking Bandit Online Clustering (ATBOC), and prove that this algorithm is order optimal, meaning that the upper bound on its expected sample complexity for given error probability $\delta$ is within a factor of 2 of an instance-dependent lower bound as $\delta \rightarrow 0$. Furthermore, we propose a computationally more efficient algorithm, Lower and Upper Confidence Bound-based Bandit Online Clustering (LUCBBOC), inspired by the LUCB algorithm for best arm identification. Simulation results demonstrate that the performance of LUCBBOC is comparable to that of ATBOC. We numerically assess the effectiveness of the proposed algorithms through numerical experiments on both synthetic datasets and the real-world MovieLens dataset. To the best of our knowledge, this is the first work on bandit online clustering that allows arms with different means in a cluster and $K$ greater than 2. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14142</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14142</id><created>2025-01-23</created><updated>2025-02-01</updated><authors><author><keyname>Goldwasser</keyname><forenames>Jeremy</forenames></author><author><keyname>Fithian</keyname><forenames>Will</forenames></author><author><keyname>Hooker</keyname><forenames>Giles</forenames></author></authors><title>Gaussian Rank Verification</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Statistical experiments often seek to identify random variables with the largest population means. This inferential task, known as rank verification, has been well-studied on Gaussian data with equal variances. This work provides the first treatment of the unequal variances case, utilizing ideas from the selective inference literature. We design a hypothesis test that verifies the rank of the largest observed value without losing power due to multiple testing corrections. This test is subsequently extended for two procedures: Identifying some number of correctly-ordered Gaussian means, and validating the top-K set. The testing procedures are validated on NHANES survey data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.15194</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.15194</id><created>2025-01-25</created><updated>2025-02-02</updated><authors><author><keyname>Yao</keyname><forenames>Zhihao</forenames></author><author><keyname>Yin</keyname><forenames>Jixuan</forenames></author><author><keyname>Li</keyname><forenames>Bo</forenames></author></authors><title>Reliable Pseudo-labeling via Optimal Transport with Attention for Short   Text Clustering</title><categories>cs.LG stat.CO stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Short text clustering has gained significant attention in the data mining community. However, the limited valuable information contained in short texts often leads to low-discriminative representations, increasing the difficulty of clustering. This paper proposes a novel short text clustering framework, called Reliable \textbf{P}seudo-labeling via \textbf{O}ptimal \textbf{T}ransport with \textbf{A}ttention for Short Text Clustering (\textbf{POTA}), that generate reliable pseudo-labels to aid discriminative representation learning for clustering. Specially, \textbf{POTA} first implements an instance-level attention mechanism to capture the semantic relationships among samples, which are then incorporated as a semantic consistency regularization term into an optimal transport problem. By solving this OT problem, we can yield reliable pseudo-labels that simultaneously account for sample-to-sample semantic consistency and sample-to-cluster global structure information. Additionally, the proposed OT can adaptively estimate cluster distributions, making \textbf{POTA} well-suited for varying degrees of imbalanced datasets. Then, we utilize the pseudo-labels to guide contrastive learning to generate discriminative representations and achieve efficient clustering. Extensive experiments demonstrate \textbf{POTA} outperforms state-of-the-art methods. The code is available at: \href{https://github.com/YZH0905/POTA-STC/tree/main}{https://github.com/YZH0905/POTA-STC/tree/main}. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.16226</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.16226</id><created>2025-01-27</created><updated>2025-02-01</updated><authors><author><keyname>Takanami</keyname><forenames>Kaito</forenames></author><author><keyname>Takahashi</keyname><forenames>Takashi</forenames></author><author><keyname>Sakata</keyname><forenames>Ayaka</forenames></author></authors><title>The Effect of Optimal Self-Distillation in Noisy Gaussian Mixture Model</title><categories>stat.ML cond-mat.dis-nn cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Self-distillation (SD), a technique where a model refines itself from its own predictions, has garnered attention as a simple yet powerful approach in machine learning. Despite its widespread use, the mechanisms underlying its effectiveness remain unclear. In this study, we investigate the efficacy of hyperparameter-tuned multi-stage SD in binary classification tasks with noisy labeled Gaussian mixture data, utilizing a replica theory. Our findings reveals that the primary driver of SD's performance improvement is denoising through hard pseudo-labels, with the most notable gains observed in moderately sized datasets. We also demonstrate the efficacy of practical heuristics, such as early stopping for extracting meaningful signal and bias fixation for imbalanced data. These results provide both theoretical guarantees and practical insights, advancing our understanding and application of SD in noisy settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18106</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18106</id><created>2025-01-29</created><updated>2025-02-01</updated><authors><author><keyname>Newman</keyname><forenames>Ken B.</forenames></author><author><keyname>Villa</keyname><forenames>Cristiano</forenames></author><author><keyname>King</keyname><forenames>Ruth</forenames></author></authors><title>Logistic regression models: practical induced prior specification</title><categories>stat.ME stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bayesian inference for statistical models with a hierarchical structure is often characterized by specification of priors for parameters at different levels of the hierarchy. When higher level parameters are functions of the lower level parameters, specifying a prior on the lower level parameters leads to induced priors on the higher level parameters. However, what are deemed uninformative priors for lower level parameters can induce strikingly non-vague priors for higher level parameters. Depending on the sample size and specific model parameterization, these priors can then have unintended effects on the posterior distribution of the higher level parameters.   Here we focus on Bayesian inference for the Bernoulli distribution parameter $\theta$ which is modeled as a function of covariates via a logistic regression, where the coefficients are the lower level parameters for which priors are specified. A specific area of interest and application is the modeling of survival probabilities in capture-recapture data and occupancy and detection probabilities in presence-absence data. In particular we propose alternative priors for the coefficients that yield specific induced priors for $\theta$. We address three induced prior cases. The simplest is when the induced prior for $\theta$ is Uniform(0,1). The second case is when the induced prior for $\theta$ is an arbitrary Beta($\alpha$, $\beta$) distribution. The third case is one where the intercept in the logistic model is to be treated distinct from the partial slope coefficients; e.g., $E[\theta]$ equals a specified value on (0,1) when all covariates equal 0. Simulation studies were carried out to evaluate performance of these priors and the methods were applied to a real presence/absence data set and occupancy modelling. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18360</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18360</id><created>2025-01-30</created><updated>2025-02-02</updated><authors><author><keyname>Chatterjee</keyname><forenames>Sourav</forenames></author><author><keyname>Hastie</keyname><forenames>Trevor</forenames></author><author><keyname>Tibshirani</keyname><forenames>Robert</forenames></author></authors><title>Univariate-Guided Sparse Regression</title><categories>stat.ME</categories><msc-class>62J07</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we introduce ``UniLasso'' -- a novel statistical method for regression. This two-stage approach preserves the signs of the univariate coefficients and leverages their magnitude. Both of these properties are attractive for stability and interpretation of the model. Through comprehensive simulations and applications to real-world datasets, we demonstrate that UniLasso outperforms Lasso in various settings, particularly in terms of sparsity and model interpretability. We prove asymptotic support recovery and mean-squared error consistency under a set of conditions different from the well-known irrepresentability conditions for the Lasso. Extensions to generalized linear models (GLMs) and Cox regression are also discussed. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18798</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18798</id><created>2025-01-30</created><authors><author><keyname>Liu</keyname><forenames>Yi</forenames></author><author><keyname>Levis</keyname><forenames>Alexander W.</forenames></author><author><keyname>Zhu</keyname><forenames>Ke</forenames></author><author><keyname>Yang</keyname><forenames>Shu</forenames></author><author><keyname>Gilbert</keyname><forenames>Peter B.</forenames></author><author><keyname>Han</keyname><forenames>Larry</forenames></author></authors><title>Targeted Data Fusion for Causal Survival Analysis Under Distribution   Shift</title><categories>stat.ME math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Causal inference across multiple data sources has the potential to improve the generalizability, transportability, and replicability of scientific findings. However, data integration methods for time-to-event outcomes -- common in medical contexts such as clinical trials -- remain underdeveloped. Existing data fusion methods focus on binary or continuous outcomes, neglecting the distinct challenges of survival analysis, including right-censoring and the unification of discrete and continuous time frameworks. To address these gaps, we propose two novel approaches for multi-source causal survival analysis. First, considering a target site-specific causal effect, we introduce a semiparametric efficient estimator for scenarios where data-sharing is feasible. Second, we develop a federated learning framework tailored to privacy-constrained environments. This framework dynamically adjusts source site-specific contributions, downweighting biased sources and upweighting less biased ones relative to the target population. Both approaches incorporate nonparametric machine learning models to enhance robustness and efficiency, with theoretical guarantees applicable to both continuous and discrete time-to-event outcomes. We demonstrate the practical utility of our methods through extensive simulations and an application to two randomized trials of a monoclonal neutralizing antibody for HIV-1 prevention: HVTN 704/HPTN 085 (cisgender men and transgender persons in the Americas and Switzerland) and HVTN 703/HPTN 081 (women in sub-Saharan Africa). The results highlight the potential of our approaches to efficiently estimate causal effects while addressing heterogeneity across data sources and adhering to privacy and robustness constraints. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.19047</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.19047</id><created>2025-01-31</created><updated>2025-02-03</updated><authors><author><keyname>Pavlovic</keyname><forenames>Maja</forenames></author></authors><title>Understanding Model Calibration -- A gentle introduction and visual   exploration of calibration and the expected calibration error (ECE)</title><categories>stat.ME cs.AI cs.CV cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some issues with a measure that is still widely used to evaluate calibration. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00038</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00038</id><created>2025-01-26</created><authors><author><keyname>Meyer</keyname><forenames>François G.</forenames></author></authors><title>The Best Soules Basis for the Estimation of a Spectral Barycentre   Network</title><categories>cs.SI cs.LG physics.data-an stat.ML</categories><comments>20 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The main contribution of this work is a fast algorithm to compute the barycentre of a set of networks based on a Laplacian spectral pseudo-distance. The core engine for the reconstruction of the barycentre is an algorithm that explores the large library of Soules bases, and returns a basis that yields a sparse approximation of the sample mean adjacency matrix. We prove that when the networks are random realizations of stochastic block models, then our algorithm reconstructs the population mean adjacency matrix. In addition to the theoretical analysis of the estimator of the barycentre network, we perform Monte Carlo simulations to validate the theoretical properties of the estimator. This work is significant because it opens the door to the design of new spectral-based network synthesis that have theoretical guarantees. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00088</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00088</id><created>2025-01-31</created><authors><author><keyname>Salih</keyname><forenames>Ahmed M.</forenames></author></authors><title>Re-Visiting Explainable AI Evaluation Metrics to Identify The Most   Informative Features</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Functionality or proxy-based approach is one of the used approaches to evaluate the quality of explainable artificial intelligence methods. It uses statistical methods, definitions and new developed metrics for the evaluation without human intervention. Among them, Selectivity or RemOve And Retrain (ROAR), and Permutation Importance (PI) are the most commonly used metrics to evaluate the quality of explainable artificial intelligence methods to highlight the most significant features in machine learning models. They state that the model performance should experience a sharp reduction if the most informative feature is removed from the model or permuted. However, the efficiency of both metrics is significantly affected by multicollinearity, number of significant features in the model and the accuracy of the model. This paper shows with empirical examples that both metrics suffer from the aforementioned limitations. Accordingly, we propose expected accuracy interval (EAI), a metric to predict the upper and lower bounds of the the accuracy of the model when ROAR or IP is implemented. The proposed metric found to be very useful especially with collinear features. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00092</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00092</id><created>2025-01-31</created><authors><author><keyname>Hug</keyname><forenames>Daniel</forenames></author><author><keyname>Klatt</keyname><forenames>Michael A.</forenames></author><author><keyname>Pabst</keyname><forenames>Dominik</forenames></author></authors><title>Minkowski tensors for voxelized data: robust, asymptotically unbiased   estimators</title><categories>math.ST cond-mat.dis-nn math.MG math.PR stat.TH</categories><msc-class>94A08, 68U10, 60D05, 53C65, 28A75, 62H35, 52A22</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Minkowski tensors, also known as tensor valuations, provide robust $n$-point information for a wide range of random spatial structures. Local estimators for voxelized data, however, are unavoidably biased even in the limit of infinitely high resolution. Here, we substantially improve a recently proposed, asymptotically unbiased algorithm to estimate Minkowski tensors for voxelized data. Our improved algorithm is more robust and efficient. Moreover we generalize the theoretical foundations for an asymptotically bias-free estimation of the interfacial tensors to the case of finite unions of compact sets with positive reach, which is relevant for many applications like rough surfaces or composite materials. As a realistic test case, we consider, among others, random (beta) polytopes. We first derive explicit expressions of the expected Minkowski tensors, which we then compare to our simulation results. We obtain precise estimates with relative errors of a few percent for practically relevant resolutions. Finally, we apply our methods to real data of metallic grains and nanorough surfaces, and we provide an open-source python package, which works in any dimension. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00108</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00108</id><created>2025-01-31</created><authors><author><keyname>Suk</keyname><forenames>Joe</forenames></author><author><keyname>Kim</keyname><forenames>Jung-hun</forenames></author></authors><title>Tracking Most Significant Shifts in Infinite-Armed Bandits</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study an infinite-armed bandit problem where actions' mean rewards are initially sampled from a reservoir distribution. Most prior works in this setting focused on stationary rewards (Berry et al., 1997; Wang et al., 2008; Bonald and Proutiere, 2013; Carpentier and Valko, 2015) with the more challenging adversarial/non-stationary variant only recently studied in the context of rotting/decreasing rewards (Kim et al., 2022; 2024). Furthermore, optimal regret upper bounds were only achieved using parameter knowledge of non-stationarity and only known for certain regimes of regularity of the reservoir. This work shows the first parameter-free optimal regret bounds for all regimes while also relaxing distributional assumptions on the reservoir.   We first introduce a blackbox scheme to convert a finite-armed MAB algorithm designed for near-stationary environments into a parameter-free algorithm for the infinite-armed non-stationary problem with optimal regret guarantees. We next study a natural notion of significant shift for this problem inspired by recent developments in finite-armed MAB (Suk &amp; Kpotufe, 2022). We show that tighter regret bounds in terms of significant shifts can be adaptively attained by employing a randomized variant of elimination within our blackbox scheme. Our enhanced rates only depend on the rotting non-stationarity and thus exhibit an interesting phenomenon for this problem where rising rewards do not factor into the difficulty of non-stationarity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00120</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00120</id><created>2025-01-31</created><authors><author><keyname>Ziersen</keyname><forenames>Simon Christoffer</forenames></author><author><keyname>Martinussen</keyname><forenames>Torben</forenames></author></authors><title>Causal effect on the number of life years lost due to a specific event:   Average treatment effect and variable importance</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Competing risk is a common phenomenon when dealing with time-to-event outcomes in biostatistical applications. An attractive estimand in this setting is the "number of life-years lost due to a specific cause of death", Andersen et al. (2013). It provides a direct interpretation on the time-scale on which the data is observed. In this paper, we introduce the causal effect on the number of life years lost due to a specific event, and we give assumptions under which the average treatment effect (ATE) and the conditional average treatment effect (CATE) are identified from the observed data. Semiparametric estimators for ATE and a partially linear projection of CATE, serving as a variable importance measure, are proposed. These estimators leverage machine learning for nuisance parameters and are model-agnostic, asymptotically normal, and efficient. We give conditions under which the estimators are asymptotically normal, and their performance are investigated in a simulation study. Lastly, the methods are implemented in a study concerning the response to different antidepressants using data from the Danish national registers. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00126</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00126</id><created>2025-01-31</created><authors><author><keyname>Li</keyname><forenames>Aihua</forenames></author><author><keyname>Tokdar</keyname><forenames>Surya T.</forenames></author><author><keyname>Xu</keyname><forenames>Jason</forenames></author></authors><title>A Bayesian decision-theoretic approach to sparse estimation</title><categories>stat.ME math.ST stat.TH</categories><comments>Submitted to Biometrika</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We extend the work of Hahn and Carvalho (2015) and develop a doubly-regularized sparse regression estimator by synthesizing Bayesian regularization with penalized least squares within a decision-theoretic framework. In contrast to existing Bayesian decision-theoretic formulation chiefly reliant upon the symmetric 0-1 loss, the new method -- which we call Bayesian Decoupling -- employs a family of penalized loss functions indexed by a sparsity-tuning parameter. We propose a class of reweighted l1 penalties, with two specific instances that achieve simultaneous bias reduction and convexity. The design of the penalties incorporates considerations of signal sizes, as enabled by the Bayesian paradigm. The tuning parameter is selected using a posterior benchmarking criterion, which quantifies the drop in predictive power relative to the posterior mean which is the optimal Bayes estimator under the squared error loss. Additionally, in contrast to the widely used median probability model technique which selects variables by thresholding posterior inclusion probabilities at the fixed threshold of 1/2, Bayesian Decoupling enables the use of a data-driven threshold which automatically adapts to estimated signal sizes and offers far better performance in high-dimensional settings with highly correlated predictors. Our numerical results in such settings show that certain combinations of priors and loss functions significantly improve the solution path compared to existing methods, prioritizing true signals early along the path before false signals are selected. Consequently, Bayesian Decoupling produces estimates with better prediction and selection performance. Finally, a real data application illustrates the practical advantages of our approaches which select sparser models with larger coefficient estimates. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00128</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00128</id><created>2025-01-31</created><authors><author><keyname>Valachovic</keyname><forenames>Edward</forenames></author></authors><title>An Extension of the Iterated Moving Average</title><categories>stat.ME stat.AP</categories><comments>16 pages, 7 figures</comments><msc-class>62M10</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This work introduces an extension of the iterated moving average filter, called the Extended Kolmogorov-Zurbenko (EKZ) filter for time series and spatio-temporal analysis. The iterated application of a central simple moving average (SMA) filter, also known as a Kolmogorov-Zurbenko (KZ) filter, is a low-pass filter defined by the length of the moving average window and the number of iterations. These two arguments determine the filter properties such as the energy transfer function and cut-off frequency. However, the existing KZ filter is only defined for positive odd integer widow lengths. Therefore, for any finite time series dataset there is only a relatively small selection of possible window lengths, determined by the length of the dataset, with which to apply a KZ filter. This inflexibility impedes use of KZ filters for a wide variety of applications such as time series component separation, filtration, signal reconstruction, energy transfer function design, modeling, and forecasting. The proposed EKZ filter extends the KZ and SMA filters by permitting a widened range of argument selection for the filter window length providing the choice of an infinite number of filters that may be applied to a dataset, affording enhanced control over the filter characteristics and greater practical application. Simulations and real data application examples are provided. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00168</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00168</id><created>2025-01-31</created><authors><author><keyname>Herrera-Esposito</keyname><forenames>Daniel</forenames></author><author><keyname>Burge</keyname><forenames>Johannes</forenames></author></authors><title>Supervised Quadratic Feature Analysis: An Information Geometry Approach   to Dimensionality Reduction</title><categories>stat.ML cs.LG math.DG math.ST stat.TH</categories><comments>18 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Supervised dimensionality reduction aims to map labeled data to a low-dimensional feature space while maximizing class discriminability. Despite the availability of methods for learning complex non-linear features (e.g. Deep Learning), there is an enduring demand for dimensionality reduction methods that learn linear features due to their interpretability, low computational cost, and broad applicability. However, there is a gap between methods that optimize linear separability (e.g. LDA), and more flexible but computationally expensive methods that optimize over arbitrary class boundaries (e.g. metric-learning methods). Here, we present Supervised Quadratic Feature Analysis (SQFA), a dimensionality reduction method for learning linear features that maximize the differences between class-conditional first- and second-order statistics, which allow for quadratic discrimination. SQFA exploits the information geometry of second-order statistics in the symmetric positive definite manifold. We show that SQFA features support quadratic discriminability in real-world problems. We also provide a theoretical link, based on information geometry, between SQFA and the Quadratic Discriminant Analysis (QDA) classifier. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00171</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00171</id><created>2025-01-31</created><authors><author><keyname>Zhu</keyname><forenames>Yu</forenames></author><author><keyname>Li</keyname><forenames>Zehang Richard</forenames></author></authors><title>Flexible Bayesian Tensor Decomposition for Verbal Autopsy Data</title><categories>stat.AP stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cause-of-death data is fundamental for understanding population health trends and inequalities as well as designing and evaluating public health interventions. A significant proportion of global deaths, particularly in low- and middle-income countries (LMICs), do not have medically certified causes assigned. In such settings, verbal autopsy (VA) is a widely adopted approach to estimate disease burdens by interviewing caregivers of the deceased. Recently, latent class models have been developed to model the joint distribution of symptoms and perform probabilistic cause-of-death assignment. A large number of latent classes are usually needed in order to characterize the complex dependence among symptoms, making the estimated symptom profiles challenging to summarize and interpret. In this paper, we propose a flexible Bayesian tensor decomposition framework that balances the predictive accuracy of the cause-of-death assignment task and the interpretability of the latent structures. The key to our approach is to partition symptoms into groups and model the joint distributions of group-level symptom sub-profiles. The proposed methods achieve better predictive accuracy than existing VA methods and provide a more parsimonious representation of the symptom distributions. We show our methods provide new insights into the clustering patterns of both symptoms and causes using the PHMRC gold-standard VA dataset. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00172</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00172</id><created>2025-01-31</created><authors><author><keyname>Huang</keyname><forenames>Jizhou</forenames></author><author><keyname>Juba</keyname><forenames>Brendan</forenames></author></authors><title>Distribution-Specific Agnostic Conditional Classification With   Halfspaces</title><categories>cs.LG cs.CC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study ``selective'' or ``conditional'' classification problems under an agnostic setting. Classification tasks commonly focus on modeling the relationship between features and categories that captures the vast majority of data. In contrast to common machine learning frameworks, conditional classification intends to model such relationships only on a subset of the data defined by some selection rule. Most work on conditional classification either solves the problem in a realizable setting or does not guarantee the error is bounded compared to an optimal solution. In this work, we consider selective/conditional classification by sparse linear classifiers for subsets defined by halfspaces, and give both positive as well as negative results for Gaussian feature distributions. On the positive side, we present the first PAC-learning algorithm for homogeneous halfspace selectors with error guarantee $\bigO*{\sqrt{\mathrm{opt}}}$, where $\mathrm{opt}$ is the smallest conditional classification error over the given class of classifiers and homogeneous halfspaces. On the negative side, we find that, under cryptographic assumptions, approximating the conditional classification loss within a small additive error is computationally hard even under Gaussian distribution. We prove that approximating conditional classification is at least as hard as approximating agnostic classification in both additive and multiplicative form. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00178</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00178</id><created>2025-01-31</created><authors><author><keyname>Ciuperca</keyname><forenames>Gabriela</forenames></author></authors><title>Right-censored models on massive data</title><categories>math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article considers the automatic selection problem of the relevant explanatory variables in a right-censored model on a massive database. We propose and study four aggregated censored adaptive LASSO estimators constructed by dividing the observations in such a way as to keep the consistency of the estimator of the survival curve. We show that these estimators have the same theoretical oracle properties as the one built on the full database. Moreover, by Monte Carlo simulations we obtain that their calculation time is smaller than that of the full database. The simulations confirm also the theoretical properties. For optimal tuning parameter selection, we propose a BIC-type criterion. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00180</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00180</id><created>2025-01-31</created><authors><author><keyname>Benita</keyname><forenames>Roi</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author><author><keyname>Keshet</keyname><forenames>Joseph</forenames></author></authors><title>Designing Scheduling for Diffusion Models via Spectral Analysis</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Diffusion models (DMs) have emerged as powerful tools for modeling complex data distributions and generating realistic new samples. Over the years, advanced architectures and sampling methods have been developed to make these models practically usable. However, certain synthesis process decisions still rely on heuristics without a solid theoretical foundation. In our work, we offer a novel analysis of the DM's inference process, introducing a comprehensive frequency response perspective. Specifically, by relying on Gaussianity and shift-invariance assumptions, we present the inference process as a closed-form spectral transfer function, capturing how the generated signal evolves in response to the initial noise. We demonstrate how the proposed analysis can be leveraged for optimizing the noise schedule, ensuring the best alignment with the original dataset's characteristics. Our results lead to scheduling curves that are dependent on the frequency content of the data, offering a theoretical justification for some of the heuristics taken by practitioners. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00182</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00182</id><created>2025-01-31</created><authors><author><keyname>Seo</keyname><forenames>Jungwon</forenames></author><author><keyname>Catak</keyname><forenames>Ferhat Ozgur</forenames></author><author><keyname>Rong</keyname><forenames>Chunming</forenames></author></authors><title>Understanding Federated Learning from IID to Non-IID dataset: An   Experimental Study</title><categories>cs.LG cs.AI stat.ML</categories><journal-ref>36th Norwegian ICT Conference for Research and Education, NIKT   2024</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As privacy concerns and data regulations grow, federated learning (FL) has emerged as a promising approach for training machine learning models across decentralized data sources without sharing raw data. However, a significant challenge in FL is that client data are often non-IID (non-independent and identically distributed), leading to reduced performance compared to centralized learning. While many methods have been proposed to address this issue, their underlying mechanisms are often viewed from different perspectives. Through a comprehensive investigation from gradient descent to FL, and from IID to non-IID data settings, we find that inconsistencies in client loss landscapes primarily cause performance degradation in non-IID scenarios. From this understanding, we observe that existing methods can be grouped into two main strategies: (i) adjusting parameter update paths and (ii) modifying client loss landscapes. These findings offer a clear perspective on addressing non-IID challenges in FL and help guide future research in the field. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00190</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00190</id><created>2025-01-31</created><authors><author><keyname>Bui</keyname><forenames>Thu</forenames></author><author><keyname>Schönlieb</keyname><forenames>Carola-Bibiane</forenames></author><author><keyname>Ribeiro</keyname><forenames>Bruno</forenames></author><author><keyname>Bevilacqua</keyname><forenames>Beatrice</forenames></author><author><keyname>Eliasof</keyname><forenames>Moshe</forenames></author></authors><title>On the Effectiveness of Random Weights in Graph Neural Networks</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Graph Neural Networks (GNNs) have achieved remarkable success across diverse tasks on graph-structured data, primarily through the use of learned weights in message passing layers. In this paper, we demonstrate that random weights can be surprisingly effective, achieving performance comparable to end-to-end training counterparts, across various tasks and datasets. Specifically, we show that by replacing learnable weights with random weights, GNNs can retain strong predictive power, while significantly reducing training time by up to 6$\times$ and memory usage by up to 3$\times$. Moreover, the random weights combined with our construction yield random graph propagation operators, which we show to reduce the problem of feature rank collapse in GNNs. These understandings and empirical results highlight random weights as a lightweight and efficient alternative, offering a compelling perspective on the design and training of GNN architectures. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00192</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00192</id><created>2025-01-31</created><authors><author><keyname>Zhou</keyname><forenames>Ruyu</forenames></author><author><keyname>Liu</keyname><forenames>Fang</forenames></author></authors><title>PRECISE: PRivacy-loss-Efficient and Consistent Inference based on   poSterior quantilEs</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Differential privacy (DP) is a mathematical framework for releasing information with formal privacy guarantees. Despite the existence of various DP procedures for performing a wide range of statistical analysis and machine learning tasks, methods of good utility are still lacking in valid statistical inference with DP guarantees. We address this gap by introducing the notion of valid Privacy-Preserving Interval Estimation (PPIE) and proposing PRivacy-loss-Efficient and Consistent Inference based on poSterior quantilEs (PRECISE). PRECISE is a general-purpose Bayesian approach for constructing privacy-preserving posterior intervals. We establish the Mean-Squared-Error (MSE) consistency for our proposed private posterior quantiles converging to the population posterior quantile as sample size or privacy loss increases. We conduct extensive experiments to compare the utilities of PRECISE with common existing privacy-preserving inferential approaches in various inferential tasks, data types and sizes,and privacy loss levels. The results demonstrated a significant advantage of PRECISE with its nominal coverage and substantially narrower intervals than the existing methods, which are prone to either under-coverage or impractically wide intervals. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00193</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00193</id><created>2025-01-31</created><authors><author><keyname>Egger</keyname><forenames>Maximilian</forenames></author><author><keyname>Bakshi</keyname><forenames>Mayank</forenames></author><author><keyname>Bitar</keyname><forenames>Rawad</forenames></author></authors><title>Byzantine-Resilient Zero-Order Optimization for Communication-Efficient   Heterogeneous Federated Learning</title><categories>cs.LG cs.CR cs.DC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce CyBeR-0, a Byzantine-resilient federated zero-order optimization method that is robust under Byzantine attacks and provides significant savings in uplink and downlink communication costs. We introduce transformed robust aggregation to give convergence guarantees for general non-convex objectives under client data heterogeneity. Empirical evaluations for standard learning tasks and fine-tuning large language models show that CyBeR-0 exhibits stable performance with only a few scalars per-round communication cost and reduced memory requirements. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00197</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00197</id><created>2025-01-31</created><authors><author><keyname>Chang</keyname><forenames>Yingshan</forenames></author><author><keyname>Bisk</keyname><forenames>Yonatan</forenames></author></authors><title>Model Successor Functions</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The notion of generalization has moved away from the classical one defined in statistical learning theory towards an emphasis on out-of-domain generalization (OODG). Recently, there is a growing focus on inductive generalization, where a progression of difficulty implicitly governs the direction of domain shifts. In inductive generalization, it is often assumed that the training data lie in the easier side, while the testing data lie in the harder side. The challenge is that training data are always finite, but a learner is expected to infer an inductive principle that could be applied in an unbounded manner. This emerging regime has appeared in the literature under different names, such as length/logical/algorithmic extrapolation, but a formal definition is lacking. This work provides such a formalization that centers on the concept of model successors. Then we outline directions to adapt well-established techniques towards the learning of model successors. This work calls for restructuring of the research discussion around inductive generalization from fragmented task-centric communities to a more unified effort, focused on universal properties of learning and computation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00200</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00200</id><created>2025-01-31</created><authors><author><keyname>Pimentel</keyname><forenames>Noel</forenames></author><author><keyname>Schuler</keyname><forenames>Alejandro</forenames></author><author><keyname>van der Laan</keyname><forenames>Mark</forenames></author></authors><title>Score-Preserving Targeted Maximum Likelihood Estimation</title><categories>stat.ME math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Targeted maximum likelihood estimators (TMLEs) are asymptotically optimal among regular, asymptotically linear estimators. In small samples, however, we may be far from "asymptopia" and not reap the benefits of optimality. Here we propose a variant (score-preserving TMLE; SP-TMLE) that leverages an initial estimator defined as the solution of a large number of possibly data-dependent score equations. Instead of targeting only the efficient influence function in the TMLE update to knock out the plug-in bias, we also target the already-solved scores. Solving additional scores reduces the remainder term in the von-Mises expansion of our estimator because these scores may come close to spanning higher-order influence functions. The result is an estimator with better finite-sample performance. We demonstrate our approach in simulation studies leveraging the (relaxed) highly adaptive lasso (HAL) as our initial estimator. These simulations show that in small samples SP-TMLE has reduced bias relative to plug-in HAL and reduced variance relative to vanilla TMLE, blending the advantages of the two approaches. We also observe improved estimation of standard errors in small samples. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00206</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00206</id><created>2025-01-31</created><authors><author><keyname>Egger</keyname><forenames>Maximilian</forenames></author><author><keyname>Bitar</keyname><forenames>Rawad</forenames></author><author><keyname>Wachter-Zeh</keyname><forenames>Antonia</forenames></author><author><keyname>Weinberger</keyname><forenames>Nir</forenames></author><author><keyname>Gündüz</keyname><forenames>Deniz</forenames></author></authors><title>BICompFL: Stochastic Federated Learning with Bi-Directional Compression</title><categories>cs.LG cs.DC cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the prominent communication bottleneck in federated learning (FL). We specifically consider stochastic FL, in which models or compressed model updates are specified by distributions rather than deterministic parameters. Stochastic FL offers a principled approach to compression, and has been shown to reduce the communication load under perfect downlink transmission from the federator to the clients. However, in practice, both the uplink and downlink communications are constrained. We show that bi-directional compression for stochastic FL has inherent challenges, which we address by introducing BICompFL. Our BICompFL is experimentally shown to reduce the communication cost by an order of magnitude compared to multiple benchmarks, while maintaining state-of-the-art accuracies. Theoretically, we study the communication cost of BICompFL through a new analysis of an importance-sampling based technique, which exposes the interplay between uplink and downlink communication costs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00214</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00214</id><created>2025-01-31</created><authors><author><keyname>Donohue</keyname><forenames>Michael C.</forenames></author><author><keyname>Insel</keyname><forenames>Philip S.</forenames></author><author><keyname>Langford</keyname><forenames>Oliver</forenames></author></authors><title>Proportional effect models for continuous outcomes are biased</title><categories>stat.ME</categories><comments>10 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Longitudinal models that assume Gaussian residuals with proportional treatment group means provide direct estimates of the proportional treatment effect. However, we demonstrate that these models are biased and sensitive to the labeling of treatment groups. Typically, this bias favors the active group and inflates statistical power. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00234</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00234</id><created>2025-01-31</created><authors><author><keyname>Ren</keyname><forenames>Yinuo</forenames></author><author><keyname>Chen</keyname><forenames>Haoxuan</forenames></author><author><keyname>Zhu</keyname><forenames>Yuchen</forenames></author><author><keyname>Guo</keyname><forenames>Wei</forenames></author><author><keyname>Chen</keyname><forenames>Yongxin</forenames></author><author><keyname>Rotskoff</keyname><forenames>Grant M.</forenames></author><author><keyname>Tao</keyname><forenames>Molei</forenames></author><author><keyname>Ying</keyname><forenames>Lexing</forenames></author></authors><title>Fast Solvers for Discrete Diffusion Models: Theory and Applications of   High-Order Algorithms</title><categories>cs.LG cs.CV cs.NA math.NA physics.comp-ph stat.ML</categories><comments>38 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Discrete diffusion models have emerged as a powerful generative modeling framework for discrete data with successful applications spanning from text generation to image synthesis. However, their deployment faces challenges due to the high dimensionality of the state space, necessitating the development of efficient inference algorithms. Current inference approaches mainly fall into two categories: exact simulation and approximate methods such as $\tau$-leaping. While exact methods suffer from unpredictable inference time and redundant function evaluations, $\tau$-leaping is limited by its first-order accuracy. In this work, we advance the latter category by tailoring the first extension of high-order numerical inference schemes to discrete diffusion models, enabling larger step sizes while reducing error. We rigorously analyze the proposed schemes and establish the second-order accuracy of the $\theta$-trapezoidal method in KL divergence. Empirical evaluations on GPT-2 level text and ImageNet-level image generation tasks demonstrate that our method achieves superior sample quality compared to existing approaches under equivalent computational constraints. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00240</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00240</id><created>2025-01-31</created><authors><author><keyname>Zhang</keyname><forenames>Yasi</forenames></author><author><keyname>Leong</keyname><forenames>Oscar</forenames></author></authors><title>Learning Difference-of-Convex Regularizers for Inverse Problems: A   Flexible Framework with Theoretical Guarantees</title><categories>stat.ML cs.LG eess.IV math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning effective regularization is crucial for solving ill-posed inverse problems, which arise in a wide range of scientific and engineering applications. While data-driven methods that parameterize regularizers using deep neural networks have demonstrated strong empirical performance, they often result in highly nonconvex formulations that lack theoretical guarantees. Recent work has shown that incorporating structured nonconvexity into neural network-based regularizers, such as weak convexity, can strike a balance between empirical performance and theoretical tractability. In this paper, we demonstrate that a broader class of nonconvex functions, difference-of-convex (DC) functions, can yield improved empirical performance while retaining strong convergence guarantees. The DC structure enables the use of well-established optimization algorithms, such as the Difference-of-Convex Algorithm (DCA) and a Proximal Subgradient Method (PSM), which extend beyond standard gradient descent. Furthermore, we provide theoretical insights into the conditions under which optimal regularizers can be expressed as DC functions. Extensive experiments on computed tomography (CT) reconstruction tasks show that our approach achieves strong performance across sparse and limited-view settings, consistently outperforming other weakly supervised learned regularizers. Our code is available at \url{https://github.com/YasminZhang/ADCR}. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00251</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00251</id><created>2025-01-31</created><authors><author><keyname>Zhao</keyname><forenames>Anqi</forenames></author><author><keyname>Ding</keyname><forenames>Peng</forenames></author><author><keyname>Li</keyname><forenames>Fan</forenames></author></authors><title>Interacted two-stage least squares with treatment effect heterogeneity</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Treatment effect heterogeneity with respect to covariates is common in instrumental variable (IV) analyses. An intuitive approach, which we term the interacted two-stage least squares (2SLS), is to postulate a linear working model of the outcome on the treatment, covariates, and treatment-covariate interactions, and instrument it by the IV, covariates, and IV-covariate interactions. We clarify the causal interpretation of the interacted 2SLS under the local average treatment effect (LATE) framework when the IV is valid conditional on covariates. Our contributions are threefold. First, we show that the interacted 2SLS with centered covariates is consistent for estimating the LATE if either of the following conditions holds: (i) the treatment-covariate interactions are linear in the covariates; (ii) the linear outcome model underlying the interacted 2SLS is correct. Second, we show that the coefficients of the treatment-covariate interactions from the interacted 2SLS are consistent for estimating treatment effect heterogeneity with regard to covariates among compliers if either condition (i) or condition (ii) holds. Moreover, we connect the 2SLS estimator with the reweighting perspective in Abadie (2003) and establish the necessity of condition (i) in the absence of additional assumptions on potential outcomes. Third, leveraging the consistency guarantees of the interacted 2SLS for categorical covariates, we propose a stratification strategy based on the IV propensity score to approximate the LATE and treatment effect heterogeneity with regard to the IV propensity score when neither condition (i) nor condition (ii) holds. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00270</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00270</id><created>2025-01-31</created><authors><author><keyname>Chen</keyname><forenames>Zhiliang</forenames></author><author><keyname>Lau</keyname><forenames>Gregory Kang Ruey</forenames></author><author><keyname>Foo</keyname><forenames>Chuan-Sheng</forenames></author><author><keyname>Low</keyname><forenames>Bryan Kian Hsiang</forenames></author></authors><title>DUET: Optimizing Training Data Mixtures via Feedback from Unseen   Evaluation Tasks</title><categories>cs.LG cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The performance of a machine learning (ML) model depends heavily on the relevance of its training data to the domain of the downstream evaluation task. However, in practice, the data involved in an unseen evaluation task is often not known to us (e.g., conversations between an LLM and a user are end-to-end encrypted). So, it is not obvious what data would be relevant for training/fine-tuning the ML model to maximize its task performance. Instead, one can only deploy the ML model in the unseen evaluation task to gather multiple rounds of coarse feedback on how well the model has performed. This paper presents a novel global-to-local algorithm called DUET that can exploit the feedback loop by interleaving a data selection method with Bayesian optimization. As a result, DUET can efficiently refine the training data mixture from a pool of data domains to maximize the model's performance on the unseen evaluation task and its convergence to the optimal data mixture can be theoretically guaranteed by analyzing its cumulative regret. Empirical evaluation on image and LLM evaluation tasks shows that DUET finds better training data mixtures than conventional baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00277</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00277</id><created>2025-01-31</created><authors><author><keyname>Feng</keyname><forenames>Shengyu</forenames></author><author><keyname>Yang</keyname><forenames>Yiming</forenames></author></authors><title>Regularized Langevin Dynamics for Combinatorial Optimization</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a simple yet effective sampling framework for combinatorial optimization (CO). Our method builds on discrete Langevin dynamics (LD), an efficient gradient-guided generative algorithm. However, we observed that directly applying LD often leads to limited exploration. To overcome this limitation, we propose the Regularized Langevin Dynamics (RLD), which enforces an expected distance between the sampled and current solutions, effectively avoiding local minima. We develop two CO solvers on top of RLD, one based on simulated annealing (SA) and the other one based on neural network (NN). Empirical results on three classical CO problems demonstrate that both of our methods can achieve comparable or better performance against the previous state-of-the-art (SOTA) SA and NN-based solvers. In particular, our SA algorithm reduces the running time of the previous SOTA SA method by up to 80\%, while achieving equal or superior performance. In summary, RLD offers a promising framework for enhancing both traditional heuristics and NN models to solve CO problems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00279</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00279</id><created>2025-01-31</created><authors><author><keyname>Pham</keyname><forenames>Khiem</forenames></author><author><keyname>Herrmann</keyname><forenames>Charles</forenames></author><author><keyname>Zabih</keyname><forenames>Ramin</forenames></author></authors><title>Improving realistic semi-supervised learning with doubly robust   estimation</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  A major challenge in Semi-Supervised Learning (SSL) is the limited information available about the class distribution in the unlabeled data. In many real-world applications this arises from the prevalence of long-tailed distributions, where the standard pseudo-label approach to SSL is biased towards the labeled class distribution and thus performs poorly on unlabeled data. Existing methods typically assume that the unlabeled class distribution is either known a priori, which is unrealistic in most situations, or estimate it on-the-fly using the pseudo-labels themselves. We propose to explicitly estimate the unlabeled class distribution, which is a finite-dimensional parameter, \emph{as an initial step}, using a doubly robust estimator with a strong theoretical guarantee; this estimate can then be integrated into existing methods to pseudo-label the unlabeled data during training more accurately. Experimental results demonstrate that incorporating our techniques into common pseudo-labeling approaches improves their performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00298</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00298</id><created>2025-01-31</created><authors><author><keyname>Moreno</keyname><forenames>Alexander</forenames></author><author><keyname>Xiao</keyname><forenames>Justin</forenames></author><author><keyname>Mei</keyname><forenames>Jonathan</forenames></author></authors><title>The Price of Linear Time: Error Analysis of Structured Kernel   Interpolation</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Structured Kernel Interpolation (SKI) (Wilson et al. 2015) helps scale Gaussian Processes (GPs) by approximating the kernel matrix via interpolation at inducing points, achieving linear computational complexity. However, it lacks rigorous theoretical error analysis. This paper bridges the gap: we prove error bounds for the SKI Gram matrix and examine the error's effect on hyperparameter estimation and posterior inference. We further provide a practical guide to selecting the number of inducing points under convolutional cubic interpolation: they should grow as $n^{d/3}$ for error control. Crucially, we identify two dimensionality regimes governing the trade-off between SKI Gram matrix spectral norm error and computational complexity. For $d \leq 3$, any error tolerance can achieve linear time for sufficiently large sample size. For $d &gt; 3$, the error must increase with sample size to maintain linear time. Our analysis provides key insights into SKI's scalability-accuracy trade-offs, establishing precise conditions for achieving linear-time GP inference with controlled approximation error. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00300</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00300</id><created>2025-01-31</created><authors><author><keyname>Jahan</keyname><forenames>Israt</forenames></author><author><keyname>Schreck</keyname><forenames>John S.</forenames></author><author><keyname>Gagne</keyname><forenames>David John</forenames></author><author><keyname>Becker</keyname><forenames>Charlie</forenames></author><author><keyname>Astitha</keyname><forenames>Marina</forenames></author></authors><title>Uncertainty Quantification of Wind Gust Predictions in the Northeast US:   An Evidential Neural Network and Explainable Artificial Intelligence Approach</title><categories>cs.LG physics.ao-ph stat.ML</categories><comments>Main body 27 pages with 12 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Machine learning has shown promise in reducing bias in numerical weather model predictions of wind gusts. Yet, they underperform to predict high gusts even with additional observations due to the right-skewed distribution of gusts. Uncertainty quantification (UQ) addresses this by identifying when predictions are reliable or needs cautious interpretation. Using data from 61 extratropical storms in the Northeastern USA, we introduce evidential neural network (ENN) as a novel approach for UQ in gust predictions, leveraging atmospheric variables from the Weather Research and Forecasting (WRF) model as features and gust observations as targets. Explainable artificial intelligence (XAI) techniques demonstrated that key predictive features also contributed to higher uncertainty. Estimated uncertainty correlated with storm intensity and spatial gust gradients. ENN allowed constructing gust prediction intervals without requiring an ensemble. From an operational perspective, providing gust forecasts with quantified uncertainty enhances stakeholders' confidence in risk assessment and response planning for extreme gust events. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00302</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00302</id><created>2025-01-31</created><authors><author><keyname>He</keyname><forenames>Yixuan</forenames></author><author><keyname>Sandel</keyname><forenames>Aaron</forenames></author><author><keyname>Wipf</keyname><forenames>David</forenames></author><author><keyname>Cucuringu</keyname><forenames>Mihai</forenames></author><author><keyname>Mitani</keyname><forenames>John</forenames></author><author><keyname>Reinert</keyname><forenames>Gesine</forenames></author></authors><title>Learning to Fuse Temporal Proximity Networks: A Case Study in Chimpanzee   Social Interactions</title><categories>stat.ML cs.AI cs.LG math.OC math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  How can we identify groups of primate individuals which could be conjectured to drive social structure? To address this question, one of us has collected a time series of data for social interactions between chimpanzees. Here we use a network representation, leading to the task of combining these data into a time series of a single weighted network per time stamp, where different proximities should be given different weights reflecting their relative importance. We optimize these proximity-type weights in a principled way, using an innovative loss function which rewards structural consistency across time. The approach is empirically validated by carefully designed synthetic data. Using statistical tests, we provide a way of identifying groups of individuals that stay related for a significant length of time. Applying the approach to the chimpanzee data set, we detect cliques in the animal social network time series, which can be validated by real-world intuition from prior research and qualitative observations by chimpanzee experts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00309</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00309</id><created>2025-01-31</created><authors><author><keyname>Shi</keyname><forenames>Jianwei</forenames></author><author><keyname>Abdulah</keyname><forenames>Sameh</forenames></author><author><keyname>Sun</keyname><forenames>Ying</forenames></author><author><keyname>Genton</keyname><forenames>Marc G.</forenames></author></authors><title>Decentralized Inference for Distributed Geospatial Data Using Low-Rank   Models</title><categories>stat.ML cs.LG stat.CO stat.ME</categories><comments>84 pages</comments><msc-class>62M30</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Advancements in information technology have enabled the creation of massive spatial datasets, driving the need for scalable and efficient computational methodologies. While offering viable solutions, centralized frameworks are limited by vulnerabilities such as single-point failures and communication bottlenecks. This paper presents a decentralized framework tailored for parameter inference in spatial low-rank models to address these challenges. A key obstacle arises from the spatial dependence among observations, which prevents the log-likelihood from being expressed as a summation-a critical requirement for decentralized optimization approaches. To overcome this challenge, we propose a novel objective function leveraging the evidence lower bound, which facilitates the use of decentralized optimization techniques. Our approach employs a block descent method integrated with multi-consensus and dynamic consensus averaging for effective parameter optimization. We prove the convexity of the new objective function in the vicinity of the true parameters, ensuring the convergence of the proposed method. Additionally, we present the first theoretical results establishing the consistency and asymptotic normality of the estimator within the context of spatial low-rank models. Extensive simulations and real-world data experiments corroborate these theoretical findings, showcasing the robustness and scalability of the framework. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00330</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00330</id><created>2025-02-01</created><authors><author><keyname>Wan</keyname><forenames>Xingchen</forenames></author><author><keyname>Zhou</keyname><forenames>Han</forenames></author><author><keyname>Sun</keyname><forenames>Ruoxi</forenames></author><author><keyname>Nakhost</keyname><forenames>Hootan</forenames></author><author><keyname>Jiang</keyname><forenames>Ke</forenames></author><author><keyname>Arık</keyname><forenames>Sercan Ö.</forenames></author></authors><title>From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative   Optimization and Generation</title><categories>cs.LG cs.AI stat.ML</categories><comments>Expanded version of the ICLR 2025 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in long-context large language models (LLMs) have led to the emerging paradigm of many-shot in-context learning (ICL), where it is observed that scaling many more demonstrating examples beyond the conventional few-shot setup in the context can lead to performance benefits. However, despite its promise, it is unclear what aspects dominate the benefits and whether simply scaling to more examples is the most effective way of improving many-shot ICL. In this work, we first provide an analysis of the factors driving many-shot ICL, and we find that 1) many-shot performance can still be attributed to often a few disproportionately influential examples and 2) identifying such influential examples ("optimize") and using them as demonstrations to regenerate new examples ("generate") can lead to further improvements. Inspired by the findings, we propose BRIDGE, an algorithm that alternates between the optimize step with Bayesian optimization to discover the influential sets of examples and the generate step to reuse this set to expand the reasoning paths of the examples back to the many-shot regime automatically. On Gemini, Claude, and Mistral LLMs of different sizes, we show that BRIDGE to significant improvements across a diverse set of tasks, including symbolic reasoning, numerical reasoning, and code generation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00336</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00336</id><created>2025-02-01</created><authors><author><keyname>George</keyname><forenames>Anand Jerry</forenames></author><author><keyname>Veiga</keyname><forenames>Rodrigo</forenames></author><author><keyname>Macris</keyname><forenames>Nicolas</forenames></author></authors><title>Denoising Score Matching with Random Features: Insights on Diffusion   Models from Precise Learning Curves</title><categories>cs.LG stat.ML</categories><comments>8 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We derive asymptotically precise expressions for test and train errors of denoising score matching (DSM) in generative diffusion models. The score function is parameterized by random features neural networks, with the target distribution being $d$-dimensional standard Gaussian. We operate in a regime where the dimension $d$, number of data samples $n$, and number of features $p$ tend to infinity while keeping the ratios $\psi_n=\frac{n}{d}$ and $\psi_p=\frac{p}{d}$ fixed. By characterizing the test and train errors, we identify regimes of generalization and memorization in diffusion models. Furthermore, our work sheds light on the conditions enhancing either generalization or memorization. Consistent with prior empirical observations, our findings indicate that the model complexity ($p$) and the number of noise samples per data sample ($m$) used during DSM significantly influence generalization and memorization behaviors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00349</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00349</id><created>2025-02-01</created><authors><author><keyname>Sebastian</keyname><forenames>Iona Ann</forenames></author><author><keyname>Sunoj</keyname><forenames>S. M.</forenames></author></authors><title>Fractional Cumulative Residual Entropy in the Quantile Framework and its   Applications in the Financial Data</title><categories>math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fractional cumulative residual entropy (FCRE) is a powerful tool for the analysis of complex systems. Most of the theoretical results and applications related to the FCRE of the lifetime random variable are based on the distribution function approach. However, there are situations in which the distribution function may not be available in explicit form but has a closed-form quantile function (QF), an alternative method of representing a probability distribution. Motivated by this, in the present study we introduce a quantile-based FCRE, its dynamic version and their various properties and examine their usefulness in different applied fields. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00355</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00355</id><created>2025-02-01</created><authors><author><keyname>George</keyname><forenames>Anand Jerry</forenames></author><author><keyname>Macris</keyname><forenames>Nicolas</forenames></author></authors><title>Sampling in High-Dimensions using Stochastic Interpolants and   Forward-Backward Stochastic Differential Equations</title><categories>cs.LG stat.ML</categories><comments>8 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a class of diffusion-based algorithms to draw samples from high-dimensional probability distributions given their unnormalized densities. Ideally, our methods can transport samples from a Gaussian distribution to a specified target distribution in finite time. Our approach relies on the stochastic interpolants framework to define a time-indexed collection of probability densities that bridge a Gaussian distribution to the target distribution. Subsequently, we derive a diffusion process that obeys the aforementioned probability density at each time instant. Obtaining such a diffusion process involves solving certain Hamilton-Jacobi-Bellman PDEs. We solve these PDEs using the theory of forward-backward stochastic differential equations (FBSDE) together with machine learning-based methods. Through numerical experiments, we demonstrate that our algorithm can effectively draw samples from distributions that conventional methods struggle to handle. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00380</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00380</id><created>2025-02-01</created><authors><author><keyname>Belucci</keyname><forenames>Bruno</forenames></author><author><keyname>Lounici</keyname><forenames>Karim</forenames></author><author><keyname>Meziani</keyname><forenames>Katia</forenames></author></authors><title>CoHiRF: A Scalable and Interpretable Clustering Framework for   High-Dimensional Data</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Clustering high-dimensional data poses significant challenges due to the curse of dimensionality, scalability issues, and the presence of noisy and irrelevant features. We propose Consensus Hierarchical Random Feature (CoHiRF), a novel clustering method designed to address these challenges effectively. CoHiRF leverages random feature selection to mitigate noise and dimensionality effects, repeatedly applies K-Means clustering in reduced feature spaces, and combines results through a unanimous consensus criterion. This iterative approach constructs a cluster assignment matrix, where each row records the cluster assignments of a sample across repetitions, enabling the identification of stable clusters by comparing identical rows. Clusters are organized hierarchically, enabling the interpretation of the hierarchy to gain insights into the dataset. CoHiRF is computationally efficient with a running time comparable to K-Means, scalable to massive datasets, and exhibits robust performance against state-of-the-art methods such as SC-SRGF, HDBSCAN, and OPTICS. Experimental results on synthetic and real-world datasets confirm the method's ability to reveal meaningful patterns while maintaining scalability, making it a powerful tool for high-dimensional data analysis. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00401</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00401</id><created>2025-02-01</created><authors><author><keyname>Grover</keyname><forenames>Karish</forenames></author><author><keyname>Yu</keyname><forenames>Haiyang</forenames></author><author><keyname>Song</keyname><forenames>Xiang</forenames></author><author><keyname>Zhu</keyname><forenames>Qi</forenames></author><author><keyname>Xie</keyname><forenames>Han</forenames></author><author><keyname>Ioannidis</keyname><forenames>Vassilis N.</forenames></author><author><keyname>Faloutsos</keyname><forenames>Christos</forenames></author></authors><title>Spectro-Riemannian Graph Neural Networks</title><categories>cs.LG cs.AI stat.ML</categories><comments>ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Can integrating spectral and curvature signals unlock new potential in graph representation learning? Non-Euclidean geometries, particularly Riemannian manifolds such as hyperbolic (negative curvature) and spherical (positive curvature), offer powerful inductive biases for embedding complex graph structures like scale-free, hierarchical, and cyclic patterns. Meanwhile, spectral filtering excels at processing signal variations across graphs, making it effective in homophilic and heterophilic settings. Leveraging both can significantly enhance the learned representations. To this end, we propose Spectro-Riemannian Graph Neural Networks (CUSP) - the first graph representation learning paradigm that unifies both CUrvature (geometric) and SPectral insights. CUSP is a mixed-curvature spectral GNN that learns spectral filters to optimize node embeddings in products of constant-curvature manifolds (hyperbolic, spherical, and Euclidean). Specifically, CUSP introduces three novel components: (a) Cusp Laplacian, an extension of the traditional graph Laplacian based on Ollivier-Ricci curvature, designed to capture the curvature signals better; (b) Cusp Filtering, which employs multiple Riemannian graph filters to obtain cues from various bands in the eigenspectrum; and (c) Cusp Pooling, a hierarchical attention mechanism combined with a curvature-based positional encoding to assess the relative importance of differently curved substructures in our graph. Empirical evaluation across eight homophilic and heterophilic datasets demonstrates the superiority of CUSP in node classification and link prediction tasks, with a gain of up to 5.3% over state-of-the-art models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00423</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00423</id><created>2025-02-01</created><authors><author><keyname>Chen</keyname><forenames>Elynn</forenames></author><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Jing</keyname><forenames>Wenbo</forenames></author><author><keyname>Liu</keyname><forenames>Xiao</forenames></author></authors><title>Stochastic Linear Bandits with Latent Heterogeneity</title><categories>cs.LG stat.ME stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper addresses the critical challenge of latent heterogeneity in online decision-making, where individual responses to business actions vary due to unobserved characteristics. While existing approaches in data-driven decision-making have focused on observable heterogeneity through contextual features, they fall short when heterogeneity stems from unobservable factors such as lifestyle preferences and personal experiences. We propose a novel latent heterogeneous bandit framework that explicitly models this unobserved heterogeneity in customer responses, with promotion targeting as our primary example. Our methodology introduces an innovative algorithm that simultaneously learns latent group memberships and group-specific reward functions. Through theoretical analysis and empirical validation using data from a mobile commerce platform, we establish high-probability bounds for parameter estimation, convergence rates for group classification, and comprehensive regret bounds. Notably, our theoretical analysis reveals two distinct types of regret measures: a ``strong regret'' against an oracle with perfect knowledge of customer memberships, which remains non-sub-linear due to inherent classification uncertainty, and a ``regular regret'' against an oracle aware only of deterministic components, for which our algorithm achieves a sub-linear rate that is minimax optimal in horizon length and dimension. We further demonstrate that existing bandit algorithms ignoring latent heterogeneity incur constant average regret that accumulates linearly over time. Our framework provides practitioners with new tools for decision-making under latent heterogeneity and extends to various business applications, including personalized pricing, resource allocation, and inventory management. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00450</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00450</id><created>2025-02-01</created><authors><author><keyname>Kaplan</keyname><forenames>David M.</forenames></author><author><keyname>Liu</keyname><forenames>Xin</forenames></author></authors><title>Confidence intervals for intentionally biased estimators</title><categories>econ.EM math.ST stat.ME stat.TH</categories><comments>accepted manuscript (post-peer review, pre-copyedited,   author-produced version)</comments><msc-class>62F25, 62G15</msc-class><journal-ref>Econometric Reviews 43 (2024) 197-214</journal-ref><doi>10.1080/07474938.2024.2312288</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose and study three confidence intervals (CIs) centered at an estimator that is intentionally biased to reduce mean squared error. The first CI simply uses an unbiased estimator's standard error; compared to centering at the unbiased estimator, this CI has higher coverage probability for confidence levels above 91.7%, even if the biased and unbiased estimators have equal mean squared error. The second CI trades some of this "excess" coverage for shorter length. The third CI is centered at a convex combination of the two estimators to further reduce length. Practically, these CIs apply broadly and are simple to compute. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00463</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00463</id><created>2025-02-01</created><authors><author><keyname>Liu</keyname><forenames>Zhiyu</forenames></author><author><keyname>Han</keyname><forenames>Zhi</forenames></author><author><keyname>Tang</keyname><forenames>Yandong</forenames></author><author><keyname>Zhang</keyname><forenames>Hai</forenames></author><author><keyname>Tang</keyname><forenames>Shaojie</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author></authors><title>Efficient Over-parameterized Matrix Sensing from Noisy Measurements via   Alternating Preconditioned Gradient Descent</title><categories>cs.LG math.OC stat.ML</categories><comments>18 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the noisy matrix sensing problem in the over-parameterization setting, where the estimated rank $r$ is larger than the true rank $r_\star$. Specifically, our main objective is to recover a matrix $ X_\star \in \mathbb{R}^{n_1 \times n_2} $ with rank $ r_\star $ from noisy measurements using an over-parameterized factorized form $ LR^\top $, where $ L \in \mathbb{R}^{n_1 \times r}, \, R \in \mathbb{R}^{n_2 \times r} $ and $ \min\{n_1, n_2\} \ge r &gt; r_\star $, with the true rank $ r_\star $ being unknown. Recently, preconditioning methods have been proposed to accelerate the convergence of matrix sensing problem compared to vanilla gradient descent, incorporating preconditioning terms $ (L^\top L + \lambda I)^{-1} $ and $ (R^\top R + \lambda I)^{-1} $ into the original gradient. However, these methods require careful tuning of the damping parameter $\lambda$ and are sensitive to initial points and step size. To address these limitations, we propose the alternating preconditioned gradient descent (APGD) algorithm, which alternately updates the two factor matrices, eliminating the need for the damping parameter and enabling faster convergence with larger step sizes. We theoretically prove that APGD achieves near-optimal error convergence at a linear rate, starting from arbitrary random initializations. Through extensive experiments, we validate our theoretical results and demonstrate that APGD outperforms other methods, achieving the fastest convergence rate. Notably, both our theoretical analysis and experimental results illustrate that APGD does not rely on the initialization procedure, making it more practical and versatile. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00465</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00465</id><created>2025-02-01</created><authors><author><keyname>Lyu</keyname><forenames>Shen-Huan</forenames></author><author><keyname>He</keyname><forenames>Yi-Xiao</forenames></author><author><keyname>Wang</keyname><forenames>Yanyan</forenames></author><author><keyname>Qu</keyname><forenames>Zhihao</forenames></author><author><keyname>Tang</keyname><forenames>Bin</forenames></author><author><keyname>Ye</keyname><forenames>Baoliu</forenames></author></authors><title>Enhance Learning Efficiency of Oblique Decision Tree via Feature   Concatenation</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Oblique Decision Tree (ODT) separates the feature space by linear projections, as opposed to the conventional Decision Tree (DT) that forces axis-parallel splits. ODT has been proven to have a stronger representation ability than DT, as it provides a way to create shallower tree structures while still approximating complex decision boundaries. However, its learning efficiency is still insufficient, since the linear projections cannot be transmitted to the child nodes, resulting in a waste of model parameters. In this work, we propose an enhanced ODT method with Feature Concatenation (\texttt{FC-ODT}), which enables in-model feature transformation to transmit the projections along the decision paths. Theoretically, we prove that our method enjoys a faster consistency rate w.r.t. the tree depth, indicating that our method possesses a significant advantage in generalization performance, especially for shallow trees. Experiments show that \texttt{FC-ODT} can outperform the other state-of-the-art decision trees with a limited tree depth. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00470</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00470</id><created>2025-02-01</created><authors><author><keyname>Wu</keyname><forenames>Runxiong</forenames></author><author><keyname>Liu</keyname><forenames>Dong</forenames></author><author><keyname>Wang</keyname><forenames>Xueqin</forenames></author><author><keyname>Wang</keyname><forenames>Andi</forenames></author></authors><title>Distributed Primal-Dual Algorithms: Unification, Connections, and   Insights</title><categories>math.OC cs.LG stat.ML</categories><comments>15 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study primal-dual algorithms for general empirical risk minimization problems in distributed settings, focusing on two prominent classes of algorithms. The first class is the communication-efficient distributed dual coordinate ascent (CoCoA), derived from the coordinate ascent method for solving the dual problem. The second class is the alternating direction method of multipliers (ADMM), including consensus ADMM, linearized ADMM, and proximal ADMM. We demonstrate that both classes of algorithms can be transformed into a unified update form that involves only primal and dual variables. This discovery reveals key connections between the two classes of algorithms: CoCoA can be interpreted as a special case of proximal ADMM for solving the dual problem, while consensus ADMM is closely related to a proximal ADMM algorithm. This discovery provides the insight that by adjusting the augmented Lagrangian parameter, we can easily enable the ADMM variants to outperform the CoCoA variants. We further explore linearized versions of ADMM and analyze the effects of tuning parameters on these ADMM variants in the distributed setting. Our theoretical findings are supported by extensive simulation studies and real-world data analysis. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00475</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00475</id><created>2025-02-01</created><authors><author><keyname>Pitarakis</keyname><forenames>Jean-Yves</forenames></author></authors><title>Serial-Dependence and Persistence Robust Inference in Predictive   Regressions</title><categories>econ.EM stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper introduces a new method for testing the statistical significance of estimated parameters in predictive regressions. The approach features a new family of test statistics that are robust to the degree of persistence of the predictors. Importantly, the method accounts for serial correlation and conditional heteroskedasticity without requiring any corrections or adjustments. This is achieved through a mechanism embedded within the test statistics that effectively decouples serial dependence present in the data. The limiting null distributions of these test statistics are shown to follow a chi-square distribution, and their asymptotic power under local alternatives is derived. A comprehensive set of simulation experiments illustrates their finite sample size and power properties. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00501</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00501</id><created>2025-02-01</created><authors><author><keyname>Yang</keyname><forenames>Tianyu</forenames></author><author><keyname>Noor-E-Alam</keyname><forenames>Md.</forenames></author></authors><title>Optimizing Feature Selection in Causal Inference: A Three-Stage   Computational Framework for Unbiased Estimation</title><categories>stat.ME cs.AI cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Feature selection is an important but challenging task in causal inference for obtaining unbiased estimates of causal quantities. Properly selected features in causal inference not only significantly reduce the time required to implement a matching algorithm but, more importantly, can also reduce the bias and variance when estimating causal quantities. When feature selection techniques are applied in causal inference, the crucial criterion is to select variables that, when used for matching, can achieve an unbiased and robust estimation of causal quantities. Recent research suggests that balancing only on treatment-associated variables introduces bias while balancing on spurious variables increases variance. To address this issue, we propose an enhanced three-stage framework that shows a significant improvement in selecting the desired subset of variables compared to the existing state-of-the-art feature selection framework for causal inference, resulting in lower bias and variance in estimating the causal quantity. We evaluated our proposed framework using a state-of-the-art synthetic data across various settings and observed superior performance within a feasible computation time, ensuring scalability for large-scale datasets. Finally, to demonstrate the applicability of our proposed methodology using large-scale real-world data, we evaluated an important US healthcare policy related to the opioid epidemic crisis: whether opioid use disorder has a causal relationship with suicidal behavior. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00514</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00514</id><created>2025-02-01</created><authors><author><keyname>Du</keyname><forenames>Hang</forenames></author><author><keyname>Gong</keyname><forenames>Shuyang</forenames></author><author><keyname>Xu</keyname><forenames>Jiaming</forenames></author></authors><title>A Proof of The Changepoint Detection Threshold Conjecture in   Preferential Attachment Models</title><categories>math.PR math.CO math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of detecting and estimating a changepoint in the attachment function of a network evolving according to a preferential attachment model on $n$ vertices, using only a single final snapshot of the network. Bet et al.~\cite{bet2023detecting} show that a simple test based on thresholding the number of vertices with minimum degrees can detect the changepoint when the change occurs at time $n-\Omega(\sqrt{n})$. They further make the striking conjecture that detection becomes impossible for any test if the change occurs at time $n-o(\sqrt{n}).$ Kaddouri et al.~\cite{kaddouri2024impossibility} make a step forward by proving the detection is impossible if the change occurs at time $n-o(n^{1/3}).$ In this paper, we resolve the conjecture affirmatively, proving that detection is indeed impossible if the change occurs at time $n-o(\sqrt{n}).$ Furthermore, we establish that estimating the changepoint with an error smaller than $o(\sqrt{n})$ is also impossible, thereby confirming that the estimator proposed in Bhamidi et al.~\cite{bhamidi2018change} is order-optimal. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00520</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00520</id><created>2025-02-01</created><authors><author><keyname>Han</keyname><forenames>Jiale</forenames></author><author><keyname>Dai</keyname><forenames>Xiaowu</forenames></author><author><keyname>Zhu</keyname><forenames>Yuhua</forenames></author></authors><title>Variance Reduction via Resampling and Experience Replay</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Experience replay is a foundational technique in reinforcement learning that enhances learning stability by storing past experiences in a replay buffer and reusing them during training. Despite its practical success, its theoretical properties remain underexplored. In this paper, we present a theoretical framework that models experience replay using resampled $U$- and $V$-statistics, providing rigorous variance reduction guarantees. We apply this framework to policy evaluation tasks using the Least-Squares Temporal Difference (LSTD) algorithm and a Partial Differential Equation (PDE)-based model-free algorithm, demonstrating significant improvements in stability and efficiency, particularly in data-scarce scenarios. Beyond policy evaluation, we extend the framework to kernel ridge regression, showing that the experience replay-based method reduces the computational cost from the traditional $O(n^3)$ in time to as low as $O(n^2)$ in time while simultaneously reducing variance. Extensive numerical experiments validate our theoretical findings, demonstrating the broad applicability and effectiveness of experience replay in diverse machine learning tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00523</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00523</id><created>2025-02-01</created><authors><author><keyname>Liang</keyname><forenames>Shuyi</forenames></author><author><keyname>Emura</keyname><forenames>Takeshi</forenames></author><author><keyname>Ma</keyname><forenames>Chang-Xing</forenames></author><author><keyname>Xin</keyname><forenames>Yijing</forenames></author><author><keyname>Huang</keyname><forenames>Xin-Wei</forenames></author></authors><title>Testing the Homogeneity of Two Proportions for Correlated Bilateral Data   via the Clayton Copula</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Handling highly dependent data is crucial in clinical trials, particularly in fields related to ophthalmology. Incorrectly specifying the dependency structure can lead to biased inferences. Traditionally, models rely on three fixed dependence structures, which lack flexibility and interpretation. In this article, we propose a framework using a more general model -- copulas -- to better account for dependency. We assess the performance of three different test statistics within the Clayton copula setting to demonstrate the framework's feasibility. Simulation results indicate that this method controls type I error rates and achieves reasonable power, providing a solid benchmark for future research and broader applications. Additionally, we present analyses of two real-world datasets as case studies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00534</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00534</id><created>2025-02-01</created><authors><author><keyname>Chai</keyname><forenames>Jinhang</forenames></author><author><keyname>Chen</keyname><forenames>Elynn</forenames></author><author><keyname>Yang</keyname><forenames>Lin</forenames></author></authors><title>Transition Transfer $Q$-Learning for Composite Markov Decision Processes</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  To bridge the gap between empirical success and theoretical understanding in transfer reinforcement learning (RL), we study a principled approach with provable performance guarantees. We introduce a novel composite MDP framework where high-dimensional transition dynamics are modeled as the sum of a low-rank component representing shared structure and a sparse component capturing task-specific variations. This relaxes the common assumption of purely low-rank transition models, allowing for more realistic scenarios where tasks share core dynamics but maintain individual variations. We introduce UCB-TQL (Upper Confidence Bound Transfer Q-Learning), designed for transfer RL scenarios where multiple tasks share core linear MDP dynamics but diverge along sparse dimensions. When applying UCB-TQL to a target task after training on a source task with sufficient trajectories, we achieve a regret bound of $\tilde{O}(\sqrt{eH^5N})$ that scales independently of the ambient dimension. Here, $N$ represents the number of trajectories in the target task, while $e$ quantifies the sparse differences between tasks. This result demonstrates substantial improvement over single task RL by effectively leveraging their structural similarities. Our theoretical analysis provides rigorous guarantees for how UCB-TQL simultaneously exploits shared dynamics while adapting to task-specific variations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00557</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00557</id><created>2025-02-01</created><authors><author><keyname>Bach</keyname><forenames>Francis</forenames></author><author><keyname>Saremi</keyname><forenames>Saeed</forenames></author></authors><title>Sampling Binary Data by Denoising through Score Functions</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Gaussian smoothing combined with a probabilistic framework for denoising via the empirical Bayes formalism, i.e., the Tweedie-Miyasawa formula (TMF), are the two key ingredients in the success of score-based generative models in Euclidean spaces. Smoothing holds the key for easing the problem of learning and sampling in high dimensions, denoising is needed for recovering the original signal, and TMF ties these together via the score function of noisy data. In this work, we extend this paradigm to the problem of learning and sampling the distribution of binary data on the Boolean hypercube by adopting Bernoulli noise, instead of Gaussian noise, as a smoothing device. We first derive a TMF-like expression for the optimal denoiser for the Hamming loss, where a score function naturally appears. Sampling noisy binary data is then achieved using a Langevin-like sampler which we theoretically analyze for different noise levels. At high Bernoulli noise levels sampling becomes easy, akin to log-concave sampling in Euclidean spaces. In addition, we extend the sequential multi-measurement sampling of Saremi et al. (2024) to the binary setting where we can bring the "effective noise" down by sampling multiple noisy measurements at a fixed noise level, without the need for continuous-time stochastic processes. We validate our formalism and theoretical findings by experiments on synthetic data and binarized images. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00579</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00579</id><created>2025-02-01</created><authors><author><keyname>Kim</keyname><forenames>Jongwook</forenames></author><author><keyname>Huang</keyname><forenames>Chunfeng</forenames></author><author><keyname>Bussberg</keyname><forenames>Nicholas</forenames></author></authors><title>Intrinsic Random Functions and Parametric Covariance Models of   Spatio-Temporal Random Processes on the Sphere</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Identifying an appropriate covariance function is one of the primary interests in spatial and spatio-temporal statistics because it allows researchers to analyze the dependence structure of the random process. For this purpose, spatial homogeneity and temporal stationarity are widely used assumptions, and many parametric covariance models have been developed under these assumptions. However, these are strong and unrealistic conditions in many cases. In addition, on the sphere, although different statistical approaches from those on Euclidean space should be applied to build a proper covariance model considering its unique characteristics, relevant studies are rare. In this research, we introduce novel parameterized models of the covariance function for spatially non-homogeneous and temporally non-stationary random processes on the sphere. To alleviate the spatial homogeneity assumption and temporal stationarity, and to consider the spherical domain and time domain together, this research will apply the theories of Intrinsic Random Functions (IRF). We also provide a methodology to estimate the associated parameters for the model. Finally, through a simulation study and analysis of a real-world data set about global temperature anomaly, we demonstrate validity of the suggested covariance model with its advantage of interpretability. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00590</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00590</id><created>2025-02-01</created><authors><author><keyname>Mehta</keyname><forenames>Prashant</forenames></author><author><keyname>Meyn</keyname><forenames>Sean</forenames></author></authors><title>Functional role of synchronization: A mean-field control perspective</title><categories>math.OC stat.ML</categories><comments>To appear, J Syst Sci Complex</comments><msc-class>49N80, 68T05</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The broad goal of the research surveyed in this article is to develop methods for understanding the aggregate behavior of interconnected dynamical systems, as found in mathematical physics, neuroscience, economics, power systems and neural networks. Questions concern prediction of emergent (often unanticipated) phenomena, methods to formulate distributed control schemes to influence this behavior, and these topics prompt many other questions in the domain of learning. The area of mean field games, pioneered by Peter Caines, are well suited to addressing these topics. The approach is surveyed in the present paper within the context of controlled coupled oscillators. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00606</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00606</id><created>2025-02-01</created><authors><author><keyname>Alam</keyname><forenames>Entejar</forenames></author><author><keyname>Linero</keyname><forenames>Antonio R.</forenames></author></authors><title>A Unified Bayesian Nonparametric Framework for Ordinal, Survival, and   Density Regression Using the Complementary Log-Log Link</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we develop applications of the complementary log-log (cloglog) link to problems in Bayesian nonparametrics. Although less commonly used than the probit or logit links, we find that the cloglog link is computationally and theoretically well-suited to several commonly used Bayesian nonparametric methods. Our starting point is a Bayesian nonparametric model for ordinal regression. We first review how the cloglog link uniquely sits at the intersection of the cumulative link and continuation ratio approaches to ordinal regression. Then, we develop a convenient computational method for fitting these ordinal models using Bayesian additive regression trees. Next, we use our ordinal regression model to build a Bayesian nonparametric stick-breaking process and show that, under a proportional hazards assumption, our stick-breaking process can be used to construct a weight-dependent Dirichlet process mixture model. Again, Bayesian additive regression trees lead to convenient computations. We then extend these models to allow for Bayesian nonparametric survival analysis in both discrete and continuous time. Our models have desirable theoretical properties, and we illustrate this analyzing the posterior contraction rate of our ordinal models. Finally, we demonstrate the practical utility of our cloglog models through a series of illustrative examples. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00607</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00607</id><created>2025-02-01</created><authors><author><keyname>Dughmi</keyname><forenames>Shaddin</forenames></author></authors><title>PAC Learning is just Bipartite Matching (Sort of)</title><categories>cs.LG cs.DS stat.ML</categories><comments>Position paper</comments><acm-class>F.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main goal of this article is to convince you, the reader, that supervised learning in the Probably Approximately Correct (PAC) model is closely related to -- of all things -- bipartite matching! En-route from PAC learning to bipartite matching, I will overview a particular transductive model of learning, and associated one-inclusion graphs, which can be viewed as a generalization of some of the hat puzzles that are popular in recreational mathematics. Whereas this transductive model is far from new, it has recently seen a resurgence of interest as a tool for tackling deep questions in learning theory. A secondary purpose of this article could be as a (biased) tutorial on the connections between the PAC and transductive models of learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00639</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00639</id><created>2025-02-01</created><authors><author><keyname>Ren</keyname><forenames>Tao</forenames></author><author><keyname>Zhang</keyname><forenames>Zishi</forenames></author><author><keyname>Li</keyname><forenames>Zehao</forenames></author><author><keyname>Jiang</keyname><forenames>Jingyang</forenames></author><author><keyname>Qin</keyname><forenames>Shentao</forenames></author><author><keyname>Li</keyname><forenames>Guanghao</forenames></author><author><keyname>Li</keyname><forenames>Yan</forenames></author><author><keyname>Zheng</keyname><forenames>Yi</forenames></author><author><keyname>Li</keyname><forenames>Xinping</forenames></author><author><keyname>Zhan</keyname><forenames>Min</forenames></author><author><keyname>Peng</keyname><forenames>Yijie</forenames></author></authors><title>Zeroth-order Informed Fine-Tuning for Diffusion Model: A Recursive   Likelihood Ratio Optimizer</title><categories>cs.CV cs.AI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The probabilistic diffusion model (DM), generating content by inferencing through a recursive chain structure, has emerged as a powerful framework for visual generation. After pre-training on enormous unlabeled data, the model needs to be properly aligned to meet requirements for downstream applications. How to efficiently align the foundation DM is a crucial task. Contemporary methods are either based on Reinforcement Learning (RL) or truncated Backpropagation (BP). However, RL and truncated BP suffer from low sample efficiency and biased gradient estimation respectively, resulting in limited improvement or, even worse, complete training failure. To overcome the challenges, we propose the Recursive Likelihood Ratio (RLR) optimizer, a zeroth-order informed fine-tuning paradigm for DM. The zeroth-order gradient estimator enables the computation graph rearrangement within the recursive diffusive chain, making the RLR's gradient estimator an unbiased one with the lower variance than other methods. We provide theoretical guarantees for the performance of the RLR. Extensive experiments are conducted on image and video generation tasks to validate the superiority of the RLR. Furthermore, we propose a novel prompt technique that is natural for the RLR to achieve a synergistic effect. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00644</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00644</id><created>2025-02-01</created><authors><author><keyname>Chen</keyname><forenames>Yitong</forenames></author><author><keyname>Dong</keyname><forenames>Wentao</forenames></author><author><keyname>Yu</keyname><forenames>Chengcheng</forenames></author><author><keyname>Yuan</keyname><forenames>Quan</forenames></author><author><keyname>Yang</keyname><forenames>Chao</forenames></author></authors><title>A Two-Stage Trip Inference Model of Purposes and Socio-Economic   Attributes of Regular Public Transit Users</title><categories>stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data-driven research is becoming a new paradigm in transportation, but the natural lack of individual socio-economic attributes in transportation data makes research such as activity purpose inference and mobility pattern identification lack convincingness and verifiability. In this paper, a two-stage trip purpose and socio-economic attributes inference model is proposed based on travel resident survey and smart card data. In the first stage, the trip purpose of each trip is inferred by a combination of rule-based and XGBoost models. In the second stage, based on the trip purpose, a machine-learning model is built to inference the socio-economic attributes of individuals. A teacher-student model based on self-training is then applied on the models above to transfer them to smart card data. The impact of independent variables of socio-economic attributes inference model is also investigated. The results show that models for inferring trip purposes and socio-economic attributes have overall accuracies of 92.7% and 76.3%, respectively. Travel time, arrival time, departure time and purpose of the first two trips are most important factors on age and job status, while the land price of jobs-housing are significant to the inference of individual incomes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00657</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00657</id><created>2025-02-01</created><authors><author><keyname>Haldar</keyname><forenames>Rajdeep</forenames></author><author><keyname>Wang</keyname><forenames>Ziyi</forenames></author><author><keyname>Song</keyname><forenames>Qifan</forenames></author><author><keyname>Lin</keyname><forenames>Guang</forenames></author><author><keyname>Xing</keyname><forenames>Yue</forenames></author></authors><title>LLM Safety Alignment is Divergence Estimation in Disguise</title><categories>cs.LG cs.AI cs.CY stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a theoretical framework demonstrating that popular Large Language Model (LLM) alignment methods, including Reinforcement Learning from Human Feedback (RLHF) and alternatives, fundamentally function as divergence estimators between aligned (preferred or safe) and unaligned (less-preferred or harmful) distributions. This explains the separation phenomenon between safe and harmful prompts in the model hidden representation after alignment. Inspired by the theoretical results, we identify that some alignment methods are better than others in terms of separation and, introduce a new method, KLDO, and further demonstrate the implication of our theories. We advocate for compliance-refusal datasets over preference datasets to enhance safety alignment, supported by both theoretical reasoning and empirical evidence. Additionally, to quantify safety separation, we leverage a distance metric in the representation space and statistically validate its efficacy as a statistical significant indicator of LLM resilience against jailbreak attacks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00658</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00658</id><created>2025-02-01</created><authors><author><keyname>Salvaña</keyname><forenames>Mary Lai O.</forenames></author></authors><title>Multi-Hazard Bayesian Hierarchical Model for Damage Prediction</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A fundamental theoretical limitation undermines current disaster risk models: existing approaches suffer from two critical constraints. First, conventional damage prediction models remain predominantly deterministic, relying on fixed parameters established through expert judgment rather than learned from data. Second, probabilistic frameworks are fundamentally restricted by their underlying assumption of hazard independence, which directly contradicts the observed reality of cascading and compound disasters. By relying on fixed expert parameters and treating hazards as independent phenomena, these models dangerously misrepresent the true risk landscape. This work addresses this challenge by developing the Multi-Hazard Bayesian Hierarchical Model (MH-BHM), which reconceptualizes the classical risk equation beyond its deterministic origins. The model's core theoretical contribution lies in reformulating a classical risk formula as a fully probabilistic model that naturally accommodates hazard interactions through its hierarchical structure while preserving the traditional hazard-exposure-vulnerability framework. Using tropical cyclone damage data (1952-2020) from the Philippines as a test case, with out-of-sample validation on recent events (2020-2022), the model demonstrates significant empirical advantages. Key findings include a reduction in damage prediction error by 61% compared to a single-hazard model, and 80% compared to a benchmark deterministic model. This corresponds to an improvement in damage estimation accuracy of USD 0.8 billion and USD 2 billion, respectively. The improved accuracy enables more effective disaster risk management across multiple domains, from optimized insurance pricing and national resource allocation to local adaptation strategies, fundamentally improving society's capacity to prepare for and respond to disasters. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00666</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00666</id><created>2025-02-01</created><authors><author><keyname>Chen</keyname><forenames>Mingyu</forenames></author><author><keyname>Chen</keyname><forenames>Yiding</forenames></author><author><keyname>Sun</keyname><forenames>Wen</forenames></author><author><keyname>Zhang</keyname><forenames>Xuezhou</forenames></author></authors><title>Avoiding $\mathbf{exp(R_{max})}$ scaling in RLHF through   Preference-based Exploration</title><categories>cs.LG cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal technique for large language model (LLM) alignment. This paper studies the setting of online RLHF and focus on improving sample efficiency. All existing algorithms in online RLHF, whether doing passive exploration or active exploration, suffer from a sample complexity that scales exponentially with the scale of the reward function. This fundamental limitation hinders their effectiveness in scenarios with heavily skewed preferences, e.g. questions with a unique correct solution. To address this, we introduce Self-Exploring Preference-Incentive Online Preference Optimization (SE-POPO), an online RLHF algorithm that for the first time achieves a sample complexity that scales polynomially with the reward scale, answering an open problem raised by Xie et al. (2024).. Theoretically, we demonstrate that the sample complexity of SE-POPO dominates that of existing exploration algorithms. Empirically, our systematic evaluation confirms that SE-POPO is more sample-efficient than both exploratory and non-exploratory baselines, in two primary application scenarios of RLHF as well as on public benchmarks, marking a significant step forward in RLHF algorithm design. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00713</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00713</id><created>2025-02-02</created><authors><author><keyname>Sechidis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Zhang</keyname><forenames>Cong</forenames></author><author><keyname>Sun</keyname><forenames>Sophie</forenames></author><author><keyname>Chen</keyname><forenames>Yao</forenames></author><author><keyname>Spector</keyname><forenames>Asher</forenames></author><author><keyname>Bornkamp</keyname><forenames>Björn</forenames></author></authors><title>Using Individualized Treatment Effects to Assess Treatment Effect   Heterogeneity</title><categories>stat.AP stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assessing treatment effect heterogeneity (TEH) in clinical trials is crucial, as it provides insights into the variability of treatment responses among patients, influencing important decisions related to drug development. Furthermore, it can lead to personalized medicine by tailoring treatments to individual patient characteristics. This paper introduces novel methodologies for assessing treatment effects using the individual treatment effect as a basis. To estimate this effect, we use a Double Robust (DR) learner to infer a pseudo-outcome that reflects the causal contrast. This pseudo-outcome is then used to perform three objectives: (1) a global test for heterogeneity, (2) ranking covariates based on their influence on effect modification, and (3) providing estimates of the individualized treatment effect. We compare our DR-learner with various alternatives and competing methods in a simulation study, and also use it to assess heterogeneity in a pooled analysis of five Phase III trials in psoriatic arthritis. By integrating these methods with the recently proposed WATCH workflow (Workflow to Assess Treatment Effect Heterogeneity in Drug Development for Clinical Trial Sponsors), we provide a robust framework for analyzing TEH, offering insights that enable more informed decision-making in this challenging area. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00724</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00724</id><created>2025-02-02</created><authors><author><keyname>Habi</keyname><forenames>Hai Victor</forenames></author><author><keyname>Messer</keyname><forenames>Hagit</forenames></author><author><keyname>Bresler</keyname><forenames>Yoram</forenames></author></authors><title>Learned Bayesian Cram\'er-Rao Bound for Unknown Measurement Models Using   Score Neural Networks</title><categories>eess.SP cs.AI cs.LG stat.ML</categories><comments>28 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bayesian Cram\'er-Rao bound (BCRB) is a crucial tool in signal processing for assessing the fundamental limitations of any estimation problem as well as benchmarking within a Bayesian frameworks. However, the BCRB cannot be computed without full knowledge of the prior and the measurement distributions. In this work, we propose a fully learned Bayesian Cram\'er-Rao bound (LBCRB) that learns both the prior and the measurement distributions. Specifically, we suggest two approaches to obtain the LBCRB: the Posterior Approach and the Measurement-Prior Approach. The Posterior Approach provides a simple method to obtain the LBCRB, whereas the Measurement-Prior Approach enables us to incorporate domain knowledge to improve the sample complexity and {interpretability}. To achieve this, we introduce a Physics-encoded score neural network which enables us to easily incorporate such domain knowledge into a neural network. We {study the learning} errors of the two suggested approaches theoretically, and validate them numerically. We demonstrate the two approaches on several signal processing examples, including a linear measurement problem with unknown mixing and Gaussian noise covariance matrices, frequency estimation, and quantized measurement. In addition, we test our approach on a nonlinear signal processing problem of frequency estimation with real-world underwater ambient noise. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00737</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00737</id><created>2025-02-02</created><authors><author><keyname>Le</keyname><forenames>Tam</forenames></author><author><keyname>Nguyen</keyname><forenames>Truyen</forenames></author><author><keyname>Hino</keyname><forenames>Hideitsu</forenames></author><author><keyname>Fukumizu</keyname><forenames>Kenji</forenames></author></authors><title>Scalable Sobolev IPM for Probability Measures on a Graph</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We investigate the Sobolev IPM problem for probability measures supported on a graph metric space. Sobolev IPM is an important instance of integral probability metrics (IPM), and is obtained by constraining a critic function within a unit ball defined by the Sobolev norm. In particular, it has been used to compare probability measures and is crucial for several theoretical works in machine learning. However, to our knowledge, there are no efficient algorithmic approaches to compute Sobolev IPM effectively, which hinders its practical applications. In this work, we establish a relation between Sobolev norm and weighted $L^p$-norm, and leverage it to propose a \emph{novel regularization} for Sobolev IPM. By exploiting the graph structure, we demonstrate that the regularized Sobolev IPM provides a \emph{closed-form} expression for fast computation. This advancement addresses long-standing computational challenges, and paves the way to apply Sobolev IPM for practical applications, even in large-scale settings. Additionally, the regularized Sobolev IPM is negative definite. Utilizing this property, we design positive-definite kernels upon the regularized Sobolev IPM, and provide preliminary evidences of their advantages on document classification and topological data analysis for measures on a graph. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00739</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00739</id><created>2025-02-02</created><authors><author><keyname>Le</keyname><forenames>Tam</forenames></author><author><keyname>Nguyen</keyname><forenames>Truyen</forenames></author><author><keyname>Hino</keyname><forenames>Hideitsu</forenames></author><author><keyname>Fukumizu</keyname><forenames>Kenji</forenames></author></authors><title>Orlicz-Sobolev Transport for Unbalanced Measures on a Graph</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Moving beyond $L^p$ geometric structure, Orlicz-Wasserstein (OW) leverages a specific class of convex functions for Orlicz geometric structure. While OW remarkably helps to advance certain machine learning approaches, it has a high computational complexity due to its two-level optimization formula. Recently, Le et al. (2024) exploits graph structure to propose generalized Sobolev transport (GST), i.e., a scalable variant for OW. However, GST assumes that input measures have the same mass. Unlike optimal transport (OT), it is nontrivial to incorporate a mass constraint to extend GST for measures on a graph, possibly having different total mass. In this work, we propose to take a step back by considering the entropy partial transport (EPT) for nonnegative measures on a graph. By leveraging Caffarelli &amp; McCann (2010)'s observations, EPT can be reformulated as a standard complete OT between two corresponding balanced measures. Consequently, we develop a novel EPT with Orlicz geometric structure, namely Orlicz-EPT, for unbalanced measures on a graph. Especially, by exploiting the dual EPT formulation and geometric structures of the graph-based Orlicz-Sobolev space, we derive a novel regularization to propose Orlicz-Sobolev transport (OST). The resulting distance can be efficiently computed by simply solving a univariate optimization problem, unlike the high-computational two-level optimization problem for Orlicz-EPT. Additionally, we derive geometric structures for the OST and draw its relations to other transport distances. We empirically show that OST is several-order faster than Orlicz-EPT. We further illustrate preliminary evidences on the advantages of OST for document classification, and several tasks in topological data analysis. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00751</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00751</id><created>2025-02-02</created><authors><author><keyname>Leung</keyname><forenames>Man Fung</forenames></author><author><keyname>Chan</keyname><forenames>Kin Wai</forenames></author><author><keyname>Shao</keyname><forenames>Xiaofeng</forenames></author></authors><title>Online Generalized Method of Moments for Time Series</title><categories>stat.ME econ.EM</categories><comments>44 pages (26 main), 10 figures, 3 tables</comments><msc-class>62L12, 62M15 (Primary) 62J20 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online learning has gained popularity in recent years due to the urgent need to analyse large-scale streaming data, which can be collected in perpetuity and serially dependent. This motivates us to develop the online generalized method of moments (OGMM), an explicitly updated estimation and inference framework in the time series setting. The OGMM inherits many properties of offline GMM, such as its broad applicability to many problems in econometrics and statistics, natural accommodation for over-identification, and achievement of semiparametric efficiency under temporal dependence. As an online method, the key gain relative to offline GMM is the vast improvement in time complexity and memory requirement.   Building on the OGMM framework, we propose improved versions of online Sargan--Hansen and structural stability tests following recent work in econometrics and statistics. Through Monte Carlo simulations, we observe encouraging finite-sample performance in online instrumental variables regression, online over-identifying restrictions test, online quantile regression, and online anomaly detection. Interesting applications of OGMM to stochastic volatility modelling and inertial sensor calibration are presented to demonstrate the effectiveness of OGMM. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00775</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00775</id><created>2025-02-02</created><authors><author><keyname>Maranjyan</keyname><forenames>Artavazd</forenames></author><author><keyname>Saad</keyname><forenames>El Mehdi</forenames></author><author><keyname>Richtárik</keyname><forenames>Peter</forenames></author><author><keyname>Orabona</keyname><forenames>Francesco</forenames></author></authors><title>ATA: Adaptive Task Allocation for Efficient Resource Management in   Distributed Machine Learning</title><categories>cs.LG cs.DC math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Asynchronous methods are fundamental for parallelizing computations in distributed machine learning. They aim to accelerate training by fully utilizing all available resources. However, their greedy approach can lead to inefficiencies using more computation than required, especially when computation times vary across devices. If the computation times were known in advance, training could be fast and resource-efficient by assigning more tasks to faster workers. The challenge lies in achieving this optimal allocation without prior knowledge of the computation time distributions. In this paper, we propose ATA (Adaptive Task Allocation), a method that adapts to heterogeneous and random distributions of worker computation times. Through rigorous theoretical analysis, we show that ATA identifies the optimal task allocation and performs comparably to methods with prior knowledge of computation times. Experimental results further demonstrate that ATA is resource-efficient, significantly reducing costs compared to the greedy approach, which can be arbitrarily expensive depending on the number of workers. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00812</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00812</id><created>2025-02-02</created><authors><author><keyname>Mano</keyname><forenames>Shuhei</forenames></author></authors><title>Direct sampling from conditional distributions by sequential maximum   likelihood estimations</title><categories>math.ST stat.TH</categories><comments>25 pages, 1 figure</comments><msc-class>33C90, 33F99, 62H17, 62R01</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We can directly sample from the conditional distribution of any log-affine model. The algorithm is a Markov chain on a bounded integer lattice, and its transition probability is the ratio of the UMVUE (uniformly minimum variance unbiased estimator) of the expected counts to the total number of counts. The computation of the UMVUE accounts for most of the computational cost, which makes the implementation challenging. Here, we investigated an approximate algorithm that replaces the UMVUE with the MLE (maximum likelihood estimator). Although it is generally not exact, it is efficient and easy to implement; no prior study is required, such as about the connection matrices of the holonomic ideal in the original algorithm. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00818</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00818</id><created>2025-02-02</created><authors><author><keyname>Wu</keyname><forenames>Junxi</forenames></author><author><keyname>Hu</keyname><forenames>Dongjian</forenames></author><author><keyname>Bao</keyname><forenames>Yajie</forenames></author><author><keyname>Xia</keyname><forenames>Shu-Tao</forenames></author><author><keyname>Zou</keyname><forenames>Changliang</forenames></author></authors><title>Error-quantified Conformal Inference for Time Series</title><categories>stat.ML cs.LG</categories><comments>ICLR 2025 camera version</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Uncertainty quantification in time series prediction is challenging due to the temporal dependence and distribution shift on sequential data. Conformal inference provides a pivotal and flexible instrument for assessing the uncertainty of machine learning models through prediction sets. Recently, a series of online conformal inference methods updated thresholds of prediction sets by performing online gradient descent on a sequence of quantile loss functions. A drawback of such methods is that they only use the information of revealed non-conformity scores via miscoverage indicators but ignore error quantification, namely the distance between the non-conformity score and the current threshold. To accurately leverage the dynamic of miscoverage error, we propose \textit{Error-quantified Conformal Inference} (ECI) by smoothing the quantile loss function. ECI introduces a continuous and adaptive feedback scale with the miscoverage error, rather than simple binary feedback in existing methods. We establish a long-term coverage guarantee for ECI under arbitrary dependence and distribution shift. The extensive experimental results show that ECI can achieve valid miscoverage control and output tighter prediction sets than other baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00846</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00846</id><created>2025-02-02</created><authors><author><keyname>Mildner</keyname><forenames>Terje</forenames></author><author><keyname>Hamelijnck</keyname><forenames>Oliver</forenames></author><author><keyname>Giampouras</keyname><forenames>Paris</forenames></author><author><keyname>Damoulas</keyname><forenames>Theodoros</forenames></author></authors><title>Federated Generalised Variational Inference: A Robust Probabilistic   Federated Learning Framework</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce FedGVI, a probabilistic Federated Learning (FL) framework that is provably robust to both prior and likelihood misspecification. FedGVI addresses limitations in both frequentist and Bayesian FL by providing unbiased predictions under model misspecification, with calibrated uncertainty quantification. Our approach generalises previous FL approaches, specifically Partitioned Variational Inference (Ashman et al., 2022), by allowing robust and conjugate updates, decreasing computational complexity at the clients. We offer theoretical analysis in terms of fixed-point convergence, optimality of the cavity distribution, and provable robustness. Additionally, we empirically demonstrate the effectiveness of FedGVI in terms of improved robustness and predictive performance on multiple synthetic and real world classification data sets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00851</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00851</id><created>2025-02-02</created><authors><author><keyname>Herdiana</keyname><forenames>Indra</forenames></author><author><keyname>Kamal</keyname><forenames>M Alfin</forenames></author><author><keyname>Triyani</keyname></author><author><keyname>Estri</keyname><forenames>Mutia Nur</forenames></author><author><keyname>Renny</keyname></author></authors><title>A More Precise Elbow Method for Optimum K-means Clustering</title><categories>stat.ME</categories><comments>22 pages</comments><msc-class>91C20, 62-08, 62A09</msc-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  K-means clustering is an unsupervised clustering method that requires an initial decision of number of clusters. One method to determine the number of clusters is the elbow method, a heuristic method that relies on visual representation. The method uses the number based on the elbow point, the point closest to 90 degrees that indicates the most optimum number of clusters. This research improves the elbow method such that it becomes an objective method. We use the analytical geometric formula to calculate an angle between lines and real analysis principle of derivative to simplify the elbow point determination. We also consider every possibility of the elbow method graph behaviour such that the algorithm is universally applicable. The result is that the elbow point can be measured precisely with a simple algorithm that does not involve complex functions or calculations. This improved method gives an alternative of more reliable cluster determination method that contributes to more optimum k-means clustering. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00864</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00864</id><created>2025-02-02</created><authors><author><keyname>Vicentini</keyname><forenames>Carlo</forenames></author><author><keyname>Jermyn</keyname><forenames>Ian Hyla</forenames></author></authors><title>Prior selection for the precision parameter of Dirichlet Process   Mixtures</title><categories>stat.ME stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a Dirichlet process mixture model (DPM) with random precision parameter $\alpha$, inducing $K_n$ clusters over $n$ observations through its latent random partition. Our goal is to specify the prior distribution $p\left(\alpha\mid\boldsymbol{\eta}\right)$, including its fixed parameter vector $\boldsymbol{\eta}$, in a way that is meaningful.   Existing approaches can be broadly categorised into three groups. Those in the first group rely on the linkage between $p\left(\alpha\mid\boldsymbol{\eta}\right)$ and $p\left(K_n\right)$ to draw conclusions on how to best choose $\boldsymbol{\eta}$ to reflect one's prior knowledge of $K_{n}$; we call them sample-size-dependent. Those in the second and third group consist instead of using quasi-degenerate or improper priors, respectively.   In this article, we show how all three methods have limitations, especially for large $n$. We enrich the first group by working out and testing Jeffreys' prior in the context of the DPM framework, and by evaluating its behaviour. Then we propose an alternative methodology which does not depend on $K_n$ or on the size of the available sample, but rather on the relationship between the largest stick lengths in the stick-breaking construction of the DPM; and which reflects those prior beliefs in $p\left(\alpha\mid\boldsymbol{\eta}\right)$. We conclude with an example where existing sample-size-dependent approaches fail, while our sample-size-independent approach continues to be feasible. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00882</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00882</id><created>2025-02-02</created><authors><author><keyname>Goldshlager</keyname><forenames>Gil</forenames></author><author><keyname>Hu</keyname><forenames>Jiang</forenames></author><author><keyname>Lin</keyname><forenames>Lin</forenames></author></authors><title>Worth Their Weight: Randomized and Regularized Block Kaczmarz Algorithms   without Preprocessing</title><categories>cs.LG cs.NA math.NA math.OC stat.ML</categories><comments>25 pages, 8 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Due to the ever growing amounts of data leveraged for machine learning and scientific computing, it is increasingly important to develop algorithms that sample only a small portion of the data at a time. In the case of linear least-squares, the randomized block Kaczmarz method (RBK) is an appealing example of such an algorithm, but its convergence is only understood under sampling distributions that require potentially prohibitively expensive preprocessing steps. To address this limitation, we analyze RBK when the data is sampled uniformly, showing that its iterates converge in a Monte Carlo sense to a $\textit{weighted}$ least-squares solution. Unfortunately, for general problems the condition number of the weight matrix and the variance of the iterates can become arbitrarily large. We resolve these issues by incorporating regularization into the RBK iterations. Numerical experiments, including examples arising from natural gradient optimization, suggest that the regularized algorithm, ReBlocK, outperforms minibatch stochastic gradient descent for realistic problems that exhibit fast singular value decay. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00885</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00885</id><created>2025-02-02</created><authors><author><keyname>Dang</keyname><forenames>Thanh</forenames></author><author><keyname>Barsbey</keyname><forenames>Melih</forenames></author><author><keyname>Sonet</keyname><forenames>A K M Rokonuzzaman</forenames></author><author><keyname>Gurbuzbalaban</keyname><forenames>Mert</forenames></author><author><keyname>Simsekli</keyname><forenames>Umut</forenames></author><author><keyname>Zhu</keyname><forenames>Lingjiong</forenames></author></authors><title>Algorithmic Stability of Stochastic Gradient Descent with Momentum under   Heavy-Tailed Noise</title><categories>stat.ML cs.LG math.OC math.PR</categories><comments>64 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the generalization properties of optimization algorithms under heavy-tailed noise has gained growing attention. However, the existing theoretical results mainly focus on stochastic gradient descent (SGD) and the analysis of heavy-tailed optimizers beyond SGD is still missing. In this work, we establish generalization bounds for SGD with momentum (SGDm) under heavy-tailed gradient noise. We first consider the continuous-time limit of SGDm, i.e., a Levy-driven stochastic differential equation (SDE), and establish quantitative Wasserstein algorithmic stability bounds for a class of potentially non-convex loss functions. Our bounds reveal a remarkable observation: For quadratic loss functions, we show that SGDm admits a worse generalization bound in the presence of heavy-tailed noise, indicating that the interaction of momentum and heavy tails can be harmful for generalization. We then extend our analysis to discrete-time and develop a uniform-in-time discretization error bound, which, to our knowledge, is the first result of its kind for SDEs with degenerate noise. This result shows that, with appropriately chosen step-sizes, the discrete dynamics retain the generalization properties of the limiting SDE. We illustrate our theory on both synthetic quadratic problems and neural networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00899</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00899</id><created>2025-02-02</created><authors><author><keyname>Makni</keyname><forenames>Mehdi</forenames></author><author><keyname>Behdin</keyname><forenames>Kayhan</forenames></author><author><keyname>Xu</keyname><forenames>Zheng</forenames></author><author><keyname>Ponomareva</keyname><forenames>Natalia</forenames></author><author><keyname>Mazumder</keyname><forenames>Rahul</forenames></author></authors><title>HASSLE-free: A unified Framework for Sparse plus Low-Rank Matrix   Decomposition for LLMs</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The impressive capabilities of large foundation models come at a cost of substantial computing resources to serve them. Compressing these pre-trained models is of practical interest as it can democratize deploying them to the machine learning community at large by lowering the costs associated with inference. A promising compression scheme is to decompose foundation models' dense weights into a sum of sparse plus low-rank matrices. In this paper, we design a unified framework coined HASSLE-free for (semi-structured) sparse plus low-rank matrix decomposition of foundation models. Our framework introduces the local layer-wise reconstruction error objective for this decomposition, we demonstrate that prior work solves a relaxation of this optimization problem; and we provide efficient and scalable methods to minimize the exact introduced optimization problem. HASSLE-free substantially outperforms state-of-the-art methods in terms of the introduced objective and a wide range of LLM evaluation benchmarks. For the Llama3-8B model with a 2:4 sparsity component plus a 64-rank component decomposition, a compression scheme for which recent work shows important inference acceleration on GPUs, HASSLE-free reduces the test perplexity by 12% for the WikiText-2 dataset and reduces the gap (compared to the dense model) of the average of eight popular zero-shot tasks by 15% compared to existing methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00904</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00904</id><created>2025-02-02</created><authors><author><keyname>Gao</keyname><forenames>Yuxiong</forenames></author><author><keyname>Li</keyname><forenames>Wentao</forenames></author><author><keyname>Chen</keyname><forenames>Rong</forenames></author></authors><title>Parameter Estimation of State Space Models Using Particle Importance   Sampling</title><categories>stat.ME stat.CO</categories><comments>Accepted by AISTATS2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  State-space models have been used in many applications, including econometrics, engineering, medical research, etc. The maximum likelihood estimation (MLE) of the static parameter of general state-space models is not straightforward because the likelihood function is intractable. It is popular to use the sequential Monte Carlo(SMC) method to perform gradient ascent optimisation in either offline or online fashion. One problem with existing online SMC methods for MLE is that the score estimators are inconsistent, i.e. the bias does not vanish with increasing particle size. In this paper, two SMC algorithms are proposed based on an importance sampling weight function to use each set of generated particles more efficiently. The first one is an offline algorithm that locally approximates the likelihood function using importance sampling, where the locality is adapted by the effective sample size (ESS). The second one is a semi-online algorithm that has a computational cost linear in the particle size and uses score estimators that are consistent. We study its consistency and asymptotic normality. Their computational superiority is illustrated in numerical studies for long time series. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00924</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00924</id><created>2025-02-02</created><authors><author><keyname>Zhang</keyname><forenames>Yichi</forenames></author><author><keyname>Lu</keyname><forenames>Haidong</forenames></author></authors><title>Generalized Simple Graphical Rules for Assessing Selection Bias</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Selection bias is a major obstacle toward valid causal inference in epidemiology. Over the past decade, several simple graphical rules based on causal diagrams have been proposed as the sufficient identification conditions for addressing selection bias and recovering causal effects. However, these simple graphical rules are usually coupled with specific identification strategies and estimators. In this article, we show two important cases of selection bias that cannot be addressed by these simple rules and their estimators: one case where selection is a descendant of a collider of the treatment and the outcome, and the other case where selection is affected by the mediator. To address selection bias in these two cases, we construct identification formulas by the g-computation and the inverse probability weighting (IPW) methods based on single-world intervention graphs (SWIGs). They are generalized to recover the average treatment effect by adjusting for post-treatment upstream causes of selection. We propose two IPW estimators and their variance estimators to recover the average treatment effect in the presence of selection bias in these two cases. We conduct simulation studies to verify the performance of the estimators when the traditional crude selected-sample analysis returns erroneous contradictory conclusions to the truth. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00945</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00945</id><created>2025-02-02</created><authors><author><keyname>Faes</keyname><forenames>Luca</forenames></author><author><keyname>Mijatovic</keyname><forenames>Gorana</forenames></author><author><keyname>Sparacino</keyname><forenames>Laura</forenames></author><author><keyname>Porta</keyname><forenames>Alberto</forenames></author></authors><title>Predictive Information Decomposition as a Tool to Quantify Emergent   Dynamical Behaviors In Physiological Networks</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Objective: This work introduces a framework for multivariate time series analysis aimed at detecting and quantifying collective emerging behaviors in the dynamics of physiological networks. Methods: Given a network system mapped by a vector random process, we compute the predictive information (PI) between the present and past network states and dissect it into amounts quantifying the unique, redundant and synergistic information shared by the present of the network and the past of each unit. Emergence is then quantified as the prevalence of the synergistic over the redundant contribution. The framework is implemented in practice using vector autoregressive (VAR) models. Results: Validation in simulated VAR processes documents that emerging behaviors arise in networks where multiple causal interactions coexist with internal dynamics. The application to cardiovascular and respiratory networks mapping the beat-to-beat variability of heart rate, arterial pressure and respiration measured at rest and during postural stress reveals the presence of statistically significant net synergy, as well as its modulation with sympathetic nervous system activation. Conclusion: Causal emergence can be efficiently assessed decomposing the PI of network systems via VAR models applied to multivariate time series. This approach evidences the synergy/redundancy balance as a hallmark of integrated short-term autonomic control in cardiovascular and respiratory networks. Significance: Measures of causal emergence provide a practical tool to quantify the mechanisms of causal influence that determine the dynamic state of cardiovascular and neural network systems across distinct physiopathological conditions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00947</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00947</id><created>2025-02-02</created><authors><author><keyname>Vishwanath</keyname><forenames>Siddharth</forenames></author><author><keyname>Arias-Castro</keyname><forenames>Ery</forenames></author></authors><title>Minimax Optimality of Classical Scaling Under General Noise Conditions</title><categories>math.ST cs.LG stat.ML stat.TH</categories><comments>45 pages, 4 figures</comments><msc-class>62R07, 94A16, 62G05, 62C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish the consistency of classical scaling under a broad class of noise models, encompassing many commonly studied cases in literature. Our approach requires only finite fourth moments of the noise, significantly weakening standard assumptions. We derive convergence rates for classical scaling and establish matching minimax lower bounds, demonstrating that classical scaling achieves minimax optimality in recovering the true configuration even when the input dissimilarities are corrupted by noise. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00962</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00962</id><created>2025-02-02</created><authors><author><keyname>Cruzado</keyname><forenames>Omar Briceno</forenames></author></authors><title>Standardized Measurement Approach (SMA) vs Advanced Measurement   Approaches (AMA): A Critical Review of Approaches in Operational Risk</title><categories>q-fin.RM math.PR stat.CO stat.ME</categories><comments>in Spanish language. This is the first version of the paper. JEL   classification: G28, G21, C18, C4</comments><msc-class>91B06, 62P99, 62M05</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The Basel Committee on Banking Supervision proposed replacing all approaches for operational risk capital, including the Advanced Measurement Approach (AMA), with a simplified formula called the Standardized Measurement Approach (SMA). This paper examines and criticizes the weaknesses and failures of SMA, such as instability, insensitivity to risk, superadditivity, and the implicit relationship between the SMA capital model and systemic risk in the banking sector. Furthermore, it discusses the issues of the proposed Operational Risk Capital (OpCar) model by the Basel Committee, a precursor to SMA. The paper concludes by advocating for the maintenance of the AMA internal model framework and suggests a series of standardization recommendations to unify internal operational risk modeling. The findings and viewpoints presented in this paper have been discussed and supported by numerous operational risk professionals and academics from various regions of the world. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00983</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00983</id><created>2025-02-02</created><authors><author><keyname>Zhang</keyname><forenames>Zhengzhe</forenames></author><author><keyname>Meng</keyname><forenames>Wenjia</forenames></author><author><keyname>Sun</keyname><forenames>Haoliang</forenames></author><author><keyname>Pan</keyname><forenames>Gang</forenames></author></authors><title>CausalCOMRL: Context-Based Offline Meta-Reinforcement Learning with   Causal Representation</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Context-based offline meta-reinforcement learning (OMRL) methods have achieved appealing success by leveraging pre-collected offline datasets to develop task representations that guide policy learning. However, current context-based OMRL methods often introduce spurious correlations, where task components are incorrectly correlated due to confounders. These correlations can degrade policy performance when the confounders in the test task differ from those in the training task. To address this problem, we propose CausalCOMRL, a context-based OMRL method that integrates causal representation learning. This approach uncovers causal relationships among the task components and incorporates the causal relationships into task representations, enhancing the generalizability of RL agents. We further improve the distinction of task representations from different tasks by using mutual information optimization and contrastive learning. Utilizing these causal task representations, we employ SAC to optimize policies on meta-RL benchmarks. Experimental results show that CausalCOMRL achieves better performance than other methods on most benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01010</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01010</id><created>2025-02-02</created><authors><author><keyname>Gao</keyname><forenames>Jie</forenames></author><author><keyname>Xie</keyname><forenames>Liyan</forenames></author><author><keyname>Li</keyname><forenames>Zhaoyuan</forenames></author></authors><title>Online Correlation Change Detection for Large-Dimensional Data with An   Application to Forecasting of El Ni\~no Events</title><categories>stat.ME</categories><msc-class>62L10, 62P99</msc-class><acm-class>G.3; J.2</acm-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We consider detecting change points in the correlation structure of streaming large-dimensional data with minimum assumptions posed on the underlying data distribution. Depending on the $\ell_1$ and $\ell_{\infty}$ norms of the squared difference of vectorized pre-change and post-change correlation matrices, detection statistics are constructed for dense and sparse settings, respectively. The proposed detection procedures possess the bless-dimension property, as a novel algorithm for threshold selection is designed based on sign-flip permutation. Theoretical evaluations of the proposed methods are conducted in terms of average run length and expected detection delay. Numerical studies are conducted to examine the finite sample performances of the proposed methods. Our methods are effective because the average detection delays have slopes similar to that of the optimal exact CUSUM test. Moreover, a combined $\ell_1$ and $\ell_{\infty}$ norm approach is proposed and has expected performance for transitions from sparse to dense settings. Our method is applied to forecast El Ni{\~n}o events and achieves state-of-the-art hit rates greater than 0.86, while false alarm rates are 0. This application illustrates the efficiency and effectiveness of our proposed methodology in detecting fundamental changes with minimal delay. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01012</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01012</id><created>2025-02-02</created><authors><author><keyname>Zhu</keyname><forenames>Haonan</forenames></author><author><keyname>Silva</keyname><forenames>Mary</forenames></author><author><keyname>Cadena</keyname><forenames>Jose</forenames></author><author><keyname>Soper</keyname><forenames>Braden</forenames></author><author><keyname>Lisicki</keyname><forenames>Michał</forenames></author><author><keyname>Peetoom</keyname><forenames>Braian</forenames></author><author><keyname>Baranzini</keyname><forenames>Sergio E.</forenames></author><author><keyname>Sundaram</keyname><forenames>Shivshankar</forenames></author><author><keyname>Ray</keyname><forenames>Priyadip</forenames></author><author><keyname>Drocco</keyname><forenames>Jeff</forenames></author></authors><title>Deep Active Learning based Experimental Design to Uncover Synergistic   Genetic Interactions for Host Targeted Therapeutics</title><categories>cs.LG q-bio.QM stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent technological advances have introduced new high-throughput methods for studying host-virus interactions, but testing synergistic interactions between host gene pairs during infection remains relatively slow and labor intensive. Identification of multiple gene knockdowns that effectively inhibit viral replication requires a search over the combinatorial space of all possible target gene pairs and is infeasible via brute-force experiments. Although active learning methods for sequential experimental design have shown promise, existing approaches have generally been restricted to single-gene knockdowns or small-scale double knockdown datasets. In this study, we present an integrated Deep Active Learning (DeepAL) framework that incorporates information from a biological knowledge graph (SPOKE, the Scalable Precision Medicine Open Knowledge Engine) to efficiently search the configuration space of a large dataset of all pairwise knockdowns of 356 human genes in HIV infection. Through graph representation learning, the framework is able to generate task-specific representations of genes while also balancing the exploration-exploitation trade-off to pinpoint highly effective double-knockdown pairs. We additionally present an ensemble method for uncertainty quantification and an interpretation of the gene pairs selected by our algorithm via pathway analysis. To our knowledge, this is the first work to show promising results on double-gene knockdown experimental data of appreciable scale (356 by 356 matrix). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01027</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01027</id><created>2025-02-02</created><authors><author><keyname>Montreuil</keyname><forenames>Yannis</forenames></author><author><keyname>Carlier</keyname><forenames>Axel</forenames></author><author><keyname>Ng</keyname><forenames>Lai Xing</forenames></author><author><keyname>Ooi</keyname><forenames>Wei Tsang</forenames></author></authors><title>Adversarial Robustness in Two-Stage Learning-to-Defer: Algorithms and   Guarantees</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Learning-to-Defer (L2D) facilitates optimal task allocation between AI systems and decision-makers. Despite its potential, we show that current two-stage L2D frameworks are highly vulnerable to adversarial attacks, which can misdirect queries or overwhelm decision agents, significantly degrading system performance. This paper conducts the first comprehensive analysis of adversarial robustness in two-stage L2D frameworks. We introduce two novel attack strategies -- untargeted and targeted -- that exploit inherent structural vulnerabilities in these systems. To mitigate these threats, we propose SARD, a robust, convex, deferral algorithm rooted in Bayes and $(\mathcal{R},\mathcal{G})$-consistency. Our approach guarantees optimal task allocation under adversarial perturbations for all surrogates in the cross-entropy family. Extensive experiments on classification, regression, and multi-task benchmarks validate the robustness of SARD. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01032</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01032</id><created>2025-02-02</created><authors><author><keyname>Belrose</keyname><forenames>Nora</forenames></author><author><keyname>Rigg</keyname><forenames>Alice</forenames></author></authors><title>Converting MLPs into Polynomials in Closed Form</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent work has shown that purely quadratic functions can replace MLPs in transformers with no significant loss in performance, while enabling new methods of interpretability based on linear algebra. In this work, we theoretically derive closed-form least-squares optimal approximations of feedforward networks (multilayer perceptrons and gated linear units) using polynomial functions of arbitrary degree. When the $R^2$ is high, this allows us to interpret MLPs and GLUs by visualizing the eigendecomposition of the coefficients of their linear and quadratic approximants. We also show that these approximants can be used to create SVD-based adversarial examples. By tracing the $R^2$ of linear and quadratic approximants across training time, we find new evidence that networks start out simple, and get progressively more complex. Even at the end of training, however, our quadratic approximants explain over 95% of the variance in network outputs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01049</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01049</id><created>2025-02-02</created><authors><author><keyname>Odiathevar</keyname><forenames>Murugaraj</forenames></author><author><keyname>Yup</keyname><forenames>Kim Chung</forenames></author></authors><title>Simulating Application Behavior for Network Monitoring and Security</title><categories>cs.NI stat.AP</categories><comments>4 pages, 6 figures, source code, github</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Existing network simulations often rely on simplistic models that send packets at random intervals, failing to capture the critical role of application-level behaviour. This paper presents a statistical approach that extracts and models application behaviour using probability density functions to generate realistic network simulations. By convolving learned application patterns, the framework produces dynamic, scalable traffic representations that closely mimic real-world networks. The method enables rigorous testing of network monitoring tools and anomaly detection systems by dynamically adjusting application behaviour. It is lightweight, capable of running multiple emulated applications on a single machine, and scalable for analysing large networks where real data collection is impractical. To encourage adoption and further testing, the full code is provided as open-source, allowing researchers and practitioners to replicate and extend the framework for diverse network environments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01062</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01062</id><created>2025-02-03</created><authors><author><keyname>Ma</keyname><forenames>Xinwei</forenames></author><author><keyname>Wang</keyname><forenames>Jingshen</forenames></author><author><keyname>Wei</keyname><forenames>Waverly</forenames></author></authors><title>Covariate Adjusted Response Adaptive Design with Delayed Outcomes</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Covariate-adjusted response adaptive (CARA) designs have gained widespread adoption for their clear benefits in enhancing experimental efficiency and participant welfare. These designs dynamically adjust treatment allocations during interim analyses based on participant responses and covariates collected during the experiment. However, delayed responses can significantly compromise the effectiveness of CARA designs, as they hinder timely adjustments to treatment assignments when certain participant outcomes are not immediately observed. In this manuscript, we propose a fully forward-looking CARA design that dynamically updates treatment assignments throughout the experiment as response delay mechanisms are progressively estimated. Our design strategy is informed by novel semiparametric efficiency calculations that explicitly account for outcome delays in a multi-stage adaptive experiment. Through both theoretical investigations and simulation studies, we demonstrate that our proposed design offers a robust solution for handling delayed outcomes in CARA designs, yielding significant improvements in both statistical power and participant welfare. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01106</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01106</id><created>2025-02-03</created><authors><author><keyname>Shirani</keyname><forenames>Sadegh</forenames></author><author><keyname>Luo</keyname><forenames>Yuwei</forenames></author><author><keyname>Overman</keyname><forenames>William</forenames></author><author><keyname>Xiong</keyname><forenames>Ruoxuan</forenames></author><author><keyname>Bayati</keyname><forenames>Mohsen</forenames></author></authors><title>Can We Validate Counterfactual Estimations in the Presence of General   Network Interference?</title><categories>cs.LG econ.EM stat.ME stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In experimental settings with network interference, a unit's treatment can influence outcomes of other units, challenging both causal effect estimation and its validation. Classic validation approaches fail as outcomes are only observable under one treatment scenario and exhibit complex correlation patterns due to interference. To address these challenges, we introduce a new framework enabling cross-validation for counterfactual estimation. At its core is our distribution-preserving network bootstrap method -- a theoretically-grounded approach inspired by approximate message passing. This method creates multiple subpopulations while preserving the underlying distribution of network effects. We extend recent causal message-passing developments by incorporating heterogeneous unit-level characteristics and varying local interactions, ensuring reliable finite-sample performance through non-asymptotic analysis. We also develop and publicly release a comprehensive benchmark toolbox with diverse experimental environments, from networks of interacting AI agents to opinion formation in real-world communities and ride-sharing applications. These environments provide known ground truth values while maintaining realistic complexities, enabling systematic examination of causal inference methods. Extensive evaluation across these environments demonstrates our method's robustness to diverse forms of network interference. Our work provides researchers with both a practical estimation framework and a standardized platform for testing future methodological developments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01131</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01131</id><created>2025-02-03</created><authors><author><keyname>Munoz</keyname><forenames>Daniel</forenames></author></authors><title>Simple Linear Neuron Boosting</title><categories>cs.LG stat.ML</categories><comments>12 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Given a differentiable network architecture and loss function, we revisit optimizing the network's neurons in function space using Boosted Backpropagation (Grubb &amp; Bagnell, 2010), in contrast to optimizing in parameter space. From this perspective, we reduce descent in the space of linear functions that optimizes the network's backpropagated-errors to a preconditioned gradient descent algorithm. We show that this preconditioned update rule is equivalent to reparameterizing the network to whiten each neuron's features, with the benefit that the normalization occurs outside of inference. In practice, we use this equivalence to construct an online estimator for approximating the preconditioner and we propose an online, matrix-free learning algorithm with adaptive step sizes. The algorithm is applicable whenever autodifferentiation is available, including convolutional networks and transformers, and it is simple to implement for both the local and distributed training settings. We demonstrate fast convergence both in terms of epochs and wall clock time on a variety of tasks and networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01164</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01164</id><created>2025-02-03</created><authors><author><keyname>Lin</keyname><forenames>Sirui</forenames></author><author><keyname>Gao</keyname><forenames>Zijun</forenames></author><author><keyname>Blanchet</keyname><forenames>Jose</forenames></author><author><keyname>Glynn</keyname><forenames>Peter</forenames></author></authors><title>Tightening Causal Bounds via Covariate-Aware Optimal Transport</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Causal estimands can vary significantly depending on the relationship between outcomes in treatment and control groups, potentially leading to wide partial identification (PI) intervals that impede decision making. Incorporating covariates can substantially tighten these bounds, but requires determining the range of PI over probability models consistent with the joint distributions of observed covariates and outcomes in treatment and control groups. This problem is known to be equivalent to a conditional optimal transport (COT) optimization task, which is more challenging than standard optimal transport (OT) due to the additional conditioning constraints. In this work, we study a tight relaxation of COT that effectively reduces it to standard OT, leveraging its well-established computational and theoretical foundations. Our relaxation incorporates covariate information and ensures narrower PI intervals for any value of the penalty parameter, while becoming asymptotically exact as a penalty increases to infinity. This approach preserves the benefits of covariate adjustment in PI and results in a data-driven estimator for the PI set that is easy to implement using existing OT packages. We analyze the convergence rate of our estimator and demonstrate the effectiveness of our approach through extensive simulations, highlighting its practical use and superior performance compared to existing methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01168</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01168</id><created>2025-02-03</created><authors><author><keyname>Lalanne</keyname><forenames>Clément</forenames><affiliation>IMT, ANITI</affiliation></author><author><keyname>Iutzeler</keyname><forenames>Franck</forenames><affiliation>IMT, ANITI</affiliation></author><author><keyname>Loubes</keyname><forenames>Jean-Michel</forenames><affiliation>IMT, ANITI</affiliation></author><author><keyname>Chhor</keyname><forenames>Julien</forenames><affiliation>TSE-R</affiliation></author></authors><title>On the Private Estimation of Smooth Transport Maps</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating optimal transport maps between two distributions from respective samples is an important element for many machine learning methods. To do so, rather than extending discrete transport maps, it has been shown that estimating the Brenier potential of the transport problem and obtaining a transport map through its gradient is near minimax optimal for smooth problems. In this paper, we investigate the private estimation of such potentials and transport maps with respect to the distribution samples.We propose a differentially private transport map estimator achieving an $L^2$ error of at most $n^{-1} \vee n^{-\frac{2 \alpha}{2 \alpha - 2 + d}} \vee (n\epsilon)^{-\frac{2 \alpha}{2 \alpha + d}} $ up to poly-logarithmic terms where $n$ is the sample size, $\epsilon$ is the desired level of privacy, $\alpha$ is the smoothness of the true transport map, and $d$ is the dimension of the feature space. We also provide a lower bound for the problem. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01188</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01188</id><created>2025-02-03</created><authors><author><keyname>Zahid</keyname><forenames>Anam</forenames></author><author><keyname>Ali</keyname><forenames>Abdur Rehman</forenames></author><author><keyname>Raza</keyname><forenames>Shaina</forenames></author><author><keyname>Shahnawaz</keyname><forenames>Rai</forenames></author><author><keyname>Kamiran</keyname><forenames>Faisal</forenames></author><author><keyname>Karim</keyname><forenames>Asim</forenames></author></authors><title>FairUDT: Fairness-aware Uplift Decision Trees</title><categories>cs.LG stat.ML</categories><comments>Published in Knowledge-based Systems (2025)</comments><journal-ref>Knowledge-based Systems 311 (2025) 113068</journal-ref><doi>10.1016/j.knosys.2025.113068</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Training data used for developing machine learning classifiers can exhibit biases against specific protected attributes. Such biases typically originate from historical discrimination or certain underlying patterns that disproportionately under-represent minority groups, such as those identified by their gender, religion, or race. In this paper, we propose a novel approach, FairUDT, a fairness-aware Uplift-based Decision Tree for discrimination identification. FairUDT demonstrates how the integration of uplift modeling with decision trees can be adapted to include fair splitting criteria. Additionally, we introduce a modified leaf relabeling approach for removing discrimination. We divide our dataset into favored and deprived groups based on a binary sensitive attribute, with the favored dataset serving as the treatment group and the deprived dataset as the control group. By applying FairUDT and our leaf relabeling approach to preprocess three benchmark datasets, we achieve an acceptable accuracy-discrimination tradeoff. We also show that FairUDT is inherently interpretable and can be utilized in discrimination detection tasks. The code for this project is available https://github.com/ara-25/FairUDT </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01203</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01203</id><created>2025-02-03</created><authors><author><keyname>Aminian</keyname><forenames>Gholamali</forenames></author><author><keyname>Asadi</keyname><forenames>Amir R.</forenames></author><author><keyname>Shenfeld</keyname><forenames>Idan</forenames></author><author><keyname>Mroueh</keyname><forenames>Youssef</forenames></author></authors><title>Theoretical Analysis of KL-regularized RLHF with Multiple Reference   Models</title><categories>cs.LG stat.ML</categories><comments>Under review</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent methods for aligning large language models (LLMs) with human feedback predominantly rely on a single reference model, which limits diversity, model overfitting, and underutilizes the wide range of available pre-trained models. Incorporating multiple reference models has the potential to address these limitations by broadening perspectives, reducing bias, and leveraging the strengths of diverse open-source LLMs. However, integrating multiple reference models into reinforcement learning with human feedback (RLHF) frameworks poses significant theoretical challenges, particularly in reverse KL-regularization, where achieving exact solutions has remained an open problem. This paper presents the first \emph{exact solution} to the multiple reference model problem in reverse KL-regularized RLHF. We introduce a comprehensive theoretical framework that includes rigorous statistical analysis and provides sample complexity guarantees. Additionally, we extend our analysis to forward KL-regularized RLHF, offering new insights into sample complexity requirements in multiple reference scenarios. Our contributions lay the foundation for more advanced and adaptable LLM alignment techniques, enabling the effective use of multiple reference models. This work paves the way for developing alignment frameworks that are both theoretically sound and better suited to the challenges of modern AI ecosystems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01211</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01211</id><created>2025-02-03</created><authors><author><keyname>Bothmann</keyname><forenames>Ludwig</forenames></author><author><keyname>Boustani</keyname><forenames>Philip A.</forenames></author><author><keyname>Alvarez</keyname><forenames>Jose M.</forenames></author><author><keyname>Casalicchio</keyname><forenames>Giuseppe</forenames></author><author><keyname>Bischl</keyname><forenames>Bernd</forenames></author><author><keyname>Dandl</keyname><forenames>Susanne</forenames></author></authors><title>Privilege Scores</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bias-transforming methods of fairness-aware machine learning aim to correct a non-neutral status quo with respect to a protected attribute (PA). Current methods, however, lack an explicit formulation of what drives non-neutrality. We introduce privilege scores (PS) to measure PA-related privilege by comparing the model predictions in the real world with those in a fair world in which the influence of the PA is removed. At the individual level, PS can identify individuals who qualify for affirmative action; at the global level, PS can inform bias-transforming policies. After presenting estimation methods for PS, we propose privilege score contributions (PSCs), an interpretation method that attributes the origin of privilege to mediating features and direct effects. We provide confidence intervals for both PS and PSCs. Experiments on simulated and real-world data demonstrate the broad applicability of our methods and provide novel insights into gender and racial privilege in mortgage and college admissions applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01226</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01226</id><created>2025-02-03</created><authors><author><keyname>Sandberg</keyname><forenames>Jack</forenames></author><author><keyname>Chehreghani</keyname><forenames>Morteza Haghir</forenames></author></authors><title>Efficient Prior Selection in Gaussian Process Bandits with Thompson   Sampling</title><categories>cs.LG stat.ML</categories><comments>16 pages, 12 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Gaussian process (GP) bandits provide a powerful framework for solving blackbox optimization of unknown functions. The characteristics of the unknown function depends heavily on the assumed GP prior. Most work in the literature assume that this prior is known but in practice this seldom holds. Instead, practitioners often rely on maximum likelihood estimation to select the hyperparameters of the prior - which lacks theoretical guarantees. In this work, we propose two algorithms for joint prior selection and regret minimization in GP bandits based on GP Thompson sampling (GP-TS): Prior-Elimination GP-TS (PE-GP-TS) and HyperPrior GP-TS (HP-GP-TS). We theoretically analyze the algorithms and establish upper bounds for their respective regret. In addition, we demonstrate the effectiveness of our algorithms compared to the alternatives through experiments with synthetic and real-world data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01235</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01235</id><created>2025-02-03</created><authors><author><keyname>Zhang</keyname><forenames>Yuanhe</forenames></author><author><keyname>Liu</keyname><forenames>Fanghui</forenames></author><author><keyname>Chen</keyname><forenames>Yudong</forenames></author></authors><title>One-step full gradient suffices for low-rank fine-tuning, provably and   efficiently</title><categories>stat.ML cs.AI cs.LG</categories><comments>86 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper studies how to improve the performance of Low-Rank Adaption (LoRA) as guided by our theoretical analysis. Our first set of theoretical results show that for random initialization and linear models, \textit{i)} LoRA will align to the certain singular subspace of one-step gradient of full fine-tuning; \textit{ii)} preconditioners improve convergence in the high-rank case. These insights motivate us to focus on preconditioned LoRA using a specific spectral initialization strategy for aligning with certain subspaces. For both linear and nonlinear models, we prove that alignment and generalization guarantees can be directly achieved at initialization, and the subsequent linear convergence can be also built. Our analysis leads to the \emph{LoRA-One} algorithm (using \emph{One}-step gradient and preconditioning), a theoretically grounded algorithm that achieves significant empirical improvement over vanilla LoRA and its variants on several benchmarks. Our theoretical analysis, based on decoupling the learning dynamics and characterizing how spectral initialization contributes to feature learning, may be of independent interest for understanding matrix sensing and deep learning theory. The source code can be found in the https://github.com/YuanheZ/LoRA-One. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01244</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01244</id><created>2025-02-03</created><authors><author><keyname>Clerico</keyname><forenames>Eugenio</forenames></author><author><keyname>Flynn</keyname><forenames>Hamish E</forenames></author><author><keyname>Rebeschini</keyname><forenames>Patrick</forenames></author></authors><title>Uniform mean estimation for monotonic processes</title><categories>math.ST stat.TH</categories><comments>10 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the problem of deriving uniform confidence bands for the mean of a monotonic stochastic process, such as the cumulative distribution function (CDF) of a random variable, based on a sequence of i.i.d.~observations. Our approach leverages the coin-betting framework, and inherits several favourable characteristics of coin-betting methods. In particular, for each point in the domain of the mean function, we obtain anytime-valid confidence intervals that are numerically tight and adapt to the variance of the observations. To derive uniform confidence bands, we employ a continuous union bound that crucially leverages monotonicity. In the case of CDF estimation, we also exploit the fact that the empirical CDF is piece-wise constant to obtain simple confidence bands that can be easily computed. In simulations, we find that our confidence bands for the CDF achieve state-of-the-art performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01254</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01254</id><created>2025-02-03</created><authors><author><keyname>Beare</keyname><forenames>Brendan K.</forenames></author><author><keyname>Kaji</keyname><forenames>Tetsuya</forenames></author></authors><title>Necessary and sufficient conditions for convergence in distribution of   quantile and P-P processes in $L^1(0,1)$</title><categories>math.ST stat.TH</categories><comments>44 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish a necessary and sufficient condition for the quantile process based on iid sampling to converge in distribution in $L^1(0,1)$. The condition is that the quantile function is locally absolutely continuous on the open unit interval and satisfies a slight strengthening of square integrability. We further establish a necessary and sufficient condition for the P-P process based on iid sampling from two populations to converge in distribution in $L^1(0,1)$. The condition is that the P-P curve is locally absolutely continuous on the open unit interval. If either process converges in distribution then it may be approximated using the bootstrap. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01255</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01255</id><created>2025-02-03</created><authors><author><keyname>Gupta</keyname><forenames>Neetu</forenames></author><author><keyname>Neogy</keyname><forenames>S. K.</forenames></author><author><keyname>Azhad</keyname><forenames>Qazi J.</forenames></author><author><keyname>Devi</keyname><forenames>Bhagwati</forenames></author></authors><title>Inference of Half Logistic Geometric Distribution Based on Generalized   Order Statistics</title><categories>stat.ME math.ST stat.TH</categories><comments>20 pages, 3 figures, 7 tables, preprint</comments><acm-class>G.3</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As the unification of various models of ordered quantities, generalized order statistics act as a simplistic approach introduced in \cite{kamps1995concept}. In this present study, results pertaining to the expressions of marginal and joint moment generating functions from half logistic geometric distribution are presented based on generalized order statistics framework. We also consider the estimation problem of $\theta$ and provides a Bayesian framework. The two widely and popular methods called Markov chain Monte Carlo and Lindley approximations are used for obtaining the Bayes estimators.The results are derived under symmetric and asymmetric loss functions. Analysis of the special cases of generalized order statistics, \textit{i.e.,} order statistics is also presented. To have an insight into the practical applicability of the proposed results, two real data sets, one from the field of Demography and, other from reliability have been taken for analysis. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01271</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01271</id><created>2025-02-03</created><authors><author><keyname>Idowu</keyname><forenames>Victory</forenames></author></authors><title>On tail dependence parameters for non-continuous and autocorrelated   margins</title><categories>math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Tail dependence plays an essential role in the characterization of joint extreme events in multivariate data. However, most standard tail dependence parameters assume continuous margins. This note presents a form of tail dependence suitable for non-continuous and discrete margins. We derive a representation of tail dependence based on the volume of a copula and prove its properties. We utilize a bivariate regular variation to show that our new metric is consistent with the standard tail dependence parameters on continuous margins. We further define tail dependence on autocorrelated margins where the tail dependence parameter examine lagged correlation on the sample. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01276</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01276</id><created>2025-02-03</created><authors><author><keyname>Wever</keyname><forenames>Marcel</forenames></author><author><keyname>Muschalik</keyname><forenames>Maximilian</forenames></author><author><keyname>Fumagalli</keyname><forenames>Fabian</forenames></author><author><keyname>Lindauer</keyname><forenames>Marius</forenames></author></authors><title>HyperSHAP: Shapley Values and Interactions for Hyperparameter Importance</title><categories>cs.LG cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Hyperparameter optimization (HPO) is a crucial step in achieving strong predictive performance. However, the impact of individual hyperparameters on model generalization is highly context-dependent, prohibiting a one-size-fits-all solution and requiring opaque automated machine learning (AutoML) systems to find optimal configurations. The black-box nature of most AutoML systems undermines user trust and discourages adoption. To address this, we propose a game-theoretic explainability framework for HPO that is based on Shapley values and interactions. Our approach provides an additive decomposition of a performance measure across hyperparameters, enabling local and global explanations of hyperparameter importance and interactions. The framework, named HyperSHAP, offers insights into ablations, the tunability of learning algorithms, and optimizer behavior across different hyperparameter spaces. We evaluate HyperSHAP on various HPO benchmarks by analyzing the interaction structure of the HPO problem. Our results show that while higher-order interactions exist, most performance improvements can be explained by focusing on lower-order representations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01282</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01282</id><created>2025-02-03</created><authors><author><keyname>Ámon</keyname><forenames>Attila Miklós</forenames></author><author><keyname>Fenech</keyname><forenames>Kristian</forenames></author><author><keyname>Kovács</keyname><forenames>Péter</forenames></author><author><keyname>Dózsa</keyname><forenames>Tamás</forenames></author></authors><title>Rational Gaussian wavelets and corresponding model driven neural   networks</title><categories>stat.ML cs.AI cs.LG</categories><comments>Submitted to IEEE Transactions on Signal Processing, 2024 (under   review)</comments><msc-class>65D15</msc-class><acm-class>G.1.2</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we consider the continuous wavelet transform using Gaussian wavelets multiplied by an appropriate rational term. The zeros and poles of this rational modifier act as free parameters and their choice highly influences the shape of the mother wavelet. This allows the proposed construction to approximate signals with complex morphology using only a few wavelet coefficients. We show that the proposed rational Gaussian wavelets are admissible and provide numerical approximations of the wavelet coefficients using variable projection operators. In addition, we show how the proposed variable projection based rational Gaussian wavelet transform can be used in neural networks to obtain a highly interpretable feature learning layer. We demonstrate the effectiveness of the proposed scheme through a biomedical application, namely, the detection of ventricular ectopic beats (VEBs) in real ECG measurements. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.01313</identifier><datestamp>2025-02-04</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.01313</id><created>2025-02-03</created><authors><author><keyname>Geary</keyname><forenames>Jack</forenames></author><author><keyname>Gouk</keyname><forenames>Henry</forenames></author></authors><title>Strategic Classification with Randomised Classifiers</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the problem of strategic classification, where a learner must build a model to classify agents based on features that have been strategically modified. Previous work in this area has concentrated on the case when the learner is restricted to deterministic classifiers. In contrast, we perform a theoretical analysis of an extension to this setting that allows the learner to produce a randomised classifier. We show that, under certain conditions, the optimal randomised classifier can achieve better accuracy than the optimal deterministic classifier, but under no conditions can it be worse. When a finite set of training data is available, we show that the excess risk of Strategic Empirical Risk Minimisation over the class of randomised classifiers is bounded in a similar manner as the deterministic case. In both the deterministic and randomised cases, the risk of the classifier produced by the learner converges to that of the corresponding optimal classifier as the volume of available training data grows. Moreover, this convergence happens at the same rate as in the i.i.d. case. Our findings are compared with previous theoretical work analysing the problem of strategic classification. We conclude that randomisation has the potential to alleviate some issues that could be faced in practice without introducing any substantial downsides. </abstract></arXiv></metadata></record>
