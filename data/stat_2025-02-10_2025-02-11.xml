<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:1811.12852</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>1811.12852</id><created>2018-11-30</created><updated>2025-02-06</updated><authors><author><keyname>Burnetas</keyname><forenames>Apostolos N.</forenames></author><author><keyname>Kanavetas</keyname><forenames>Odysseas</forenames></author><author><keyname>Katehakis</keyname><forenames>Michael N.</forenames></author></authors><title>Optimal Data Driven Resource Allocation under Multi-Armed Bandit   Observations</title><categories>stat.ML cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1509.02857</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the first asymptotically optimal strategy for a multi armed bandit (MAB) model under side constraints. The side constraints model situations in which bandit activations are limited by the availability of certain resources that are replenished at a constant rate. The main result involves the derivation of an asymptotic lower bound for the regret of feasible uniformly fast policies and the construction of policies that achieve this lower bound, under pertinent conditions. Further, we provide the explicit form of such policies for the case in which the unknown distributions are Normal with unknown means and known variances, for the case of Normal distributions with unknown means and unknown variances and for the case of arbitrary discrete distributions with finite support. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:1912.11209</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>1912.11209</id><created>2019-12-23</created><authors><author><keyname>Singh</keyname><forenames>Vikas</forenames></author><author><keyname>Verma</keyname><forenames>Nishchal K.</forenames></author></authors><title>An Entropy-based Variable Feature Weighted Fuzzy k-Means Algorithm for   High Dimensional Data</title><categories>cs.LG stat.ML</categories><journal-ref>Multimedia Tools and Applications, 2025/1/21, 1-18</journal-ref><doi>10.1007/s11042-024-20493-4</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents a new fuzzy k-means algorithm for the clustering of high dimensional data in various subspaces. Since, In the case of high dimensional data, some features might be irrelevant and relevant but may have different significance in the clustering. For a better clustering, it is crucial to incorporate the contribution of these features in the clustering process. To combine these features, in this paper, we have proposed a new fuzzy k-means clustering algorithm in which the objective function of the fuzzy k-means is modified using two different entropy term. The first entropy term helps to minimize the within-cluster dispersion and maximize the negative entropy to determine clusters to contribute to the association of data points. The second entropy term helps to control the weight of the features because different features have different contributing weights in the clustering process for obtaining the better partition of the data. The efficacy of the proposed method is presented in terms of various clustering measures on multiple datasets and compared with various state-of-the-art methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2002.05308</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2002.05308</id><created>2020-02-12</created><updated>2025-02-09</updated><authors><author><keyname>Kato</keyname><forenames>Masahiro</forenames></author><author><keyname>Ishihara</keyname><forenames>Takuya</forenames></author><author><keyname>Honda</keyname><forenames>Junya</forenames></author><author><keyname>Narita</keyname><forenames>Yusuke</forenames></author></authors><title>Efficient Adaptive Experimental Design for Average Treatment Effect   Estimation</title><categories>stat.ML cs.LG econ.EM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study how to efficiently estimate average treatment effects (ATEs) using adaptive experiments. In adaptive experiments, experimenters sequentially assign treatments to experimental units while updating treatment assignment probabilities based on past data. We start by defining the efficient treatment-assignment probability, which minimizes the semiparametric efficiency bound for ATE estimation. Our proposed experimental design estimates and uses the efficient treatment-assignment probability to assign treatments. At the end of the proposed design, the experimenter estimates the ATE using a newly proposed Adaptive Augmented Inverse Probability Weighting (A2IPW) estimator. We show that the asymptotic variance of the A2IPW estimator using data from the proposed design achieves the minimized semiparametric efficiency bound. We also analyze the estimator's finite-sample properties and develop nonparametric and nonasymptotic confidence intervals that are valid at any round of the proposed design. These anytime valid confidence intervals allow us to conduct rate-optimal sequential hypothesis testing, allowing for early stopping and reducing necessary sample size. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2112.03969</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2112.03969</id><created>2021-12-07</created><updated>2025-02-10</updated><authors><author><keyname>Lindqvist</keyname><forenames>Jakob</forenames></author><author><keyname>Särkkä</keyname><forenames>Simo</forenames></author><author><keyname>García-Fernández</keyname><forenames>Ángel F.</forenames></author><author><keyname>Raitoharju</keyname><forenames>Matti</forenames></author><author><keyname>Svensson</keyname><forenames>Lennart</forenames></author></authors><title>Posterior linearisation smoothing with robust iterations</title><categories>math.OC stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper considers the problem of iterative Bayesian smoothing in nonlinear state-space models with additive noise using Gaussian approximations. Iterative methods are known to improve smoothed estimates but are not guaranteed to converge, motivating the development of methods with better convergence properties. The aim of this article is to extend Levenberg-Marquardt (LM) and line-search versions of the classical iterated extended Kalman smoother (IEKS) to the iterated posterior linearisation smoother (IPLS). The IEKS has previously been shown to be equivalent to the Gauss-Newton (GN) method. We derive a similar GN interpretation for the IPLS and use this to develop extensions to the IPLS, with improved convergence properties. We show that an LM extension for the IPLS can be achieved with a simple modification of the smoothing iterations, enabling algorithms with efficient implementations. We also derive the Armijo--Wolfe step length conditions for the IPLS enabling an efficient inexact line-search method. Our numerical experiments show the benefits of these extensions in highly nonlinear scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2202.04912</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2202.04912</id><created>2022-02-10</created><updated>2025-02-06</updated><authors><author><keyname>Qiu</keyname><forenames>Rui</forenames></author><author><keyname>Yu</keyname><forenames>Zhou</forenames></author><author><keyname>Zhu</keyname><forenames>Ruoqing</forenames></author></authors><title>Random Forest Weighted Local Fr\'echet Regression with Random Objects</title><categories>stat.ML cs.LG</categories><comments>This paper has been published in the Journal of Machine Learning   Research</comments><journal-ref>Journal of Machine Learning Research 25 (2024) 1-69</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical analysis is increasingly confronted with complex data from metric spaces. Petersen and M\"uller (2019) established a general paradigm of Fr\'echet regression with complex metric space valued responses and Euclidean predictors. However, the local approach therein involves nonparametric kernel smoothing and suffers from the curse of dimensionality. To address this issue, we in this paper propose a novel random forest weighted local Fr\'echet regression paradigm. The main mechanism of our approach relies on a locally adaptive kernel generated by random forests. Our first method uses these weights as the local average to solve the conditional Fr\'echet mean, while the second method performs local linear Fr\'echet regression, both significantly improving existing Fr\'echet regression methods. Based on the theory of infinite order U-processes and infinite order $M_{m_n}$-estimator, we establish the consistency, rate of convergence, and asymptotic normality for our local constant estimator, which covers the current large sample theory of random forests with Euclidean responses as a special case. Numerical studies show the superiority of our methods with several commonly encountered types of responses such as distribution functions, symmetric positive-definite matrices, and sphere data. The practical merits of our proposals are also demonstrated through the application to New York taxi data and human mortality data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2202.10078</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2202.10078</id><created>2022-02-21</created><authors><author><keyname>Esstafa</keyname><forenames>Youssef</forenames></author><author><keyname>Kokonendji</keyname><forenames>Célestin C.</forenames></author><author><keyname>Somé</keyname><forenames>Sobom M.</forenames></author></authors><title>Asymptotic properties of the normalized discrete associated-kernel   estimator for probability mass function</title><categories>math.ST stat.TH</categories><comments>20 pages, 3 figures</comments><msc-class>62G07, 62G20, 62G99</msc-class><journal-ref>Journal of Nonparametric Statistics, 35, 355-372, 2023</journal-ref><doi>10.1080/10485252.2022.2151597</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Discrete kernel smoothing is now gaining importance in nonparametric statistics. In this paper, we investigate some asymptotic properties of the normalized discrete associated-kernel estimator of a probability mass function. We show, under some regularity and non-restrictive assumptions on the associated-kernel, that the normalizing random variable converges in mean square to 1. We then derive the consistency and the asymptotic normality of the proposed estimator. Various families of discrete kernels already exhibited satisfy the conditions, including the refined CoM-Poisson which is underdispersed and of second-order. Finally, the first-order binomial kernel is discussed and, surprisingly, its normalized estimator has a suitable asymptotic behaviour through simulations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2203.11461</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2203.11461</id><created>2022-03-22</created><updated>2025-02-10</updated><authors><author><keyname>Liang</keyname><forenames>Ziyi</forenames></author><author><keyname>Cai</keyname><forenames>T. Tony</forenames></author><author><keyname>Sun</keyname><forenames>Wenguang</forenames></author><author><keyname>Xia</keyname><forenames>Yin</forenames></author></authors><title>A Locally Adaptive Algorithm for Multiple Testing with Network Structure</title><categories>stat.ME stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Incorporating auxiliary information alongside primary data can significantly enhance the accuracy of simultaneous inference. However, existing multiple testing methods face challenges in efficiently incorporating complex side information, especially when it differs in dimension or structure from the primary data, such as network side information. This paper introduces a locally adaptive structure learning algorithm (LASLA), a flexible framework designed to integrate a broad range of auxiliary information into the inference process. Although LASLA is specifically motivated by the challenges posed by network-structured data, it also proves highly effective with other types of side information, such as spatial locations and multiple auxiliary sequences. LASLA employs a $p$-value weighting approach, leveraging structural insights to derive data-driven weights that prioritize the importance of different hypotheses. Our theoretical analysis demonstrates that LASLA asymptotically controls the false discovery rate (FDR) under independent or weakly dependent $p$-values, and achieves enhanced power in scenarios where the auxiliary data provides valuable side information. Simulation studies are conducted to evaluate LASLA's numerical performance, and its efficacy is further illustrated through two real-world applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2205.02128</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2205.02128</id><created>2022-05-04</created><updated>2025-02-08</updated><authors><author><keyname>Block</keyname><forenames>Adam</forenames></author><author><keyname>Jia</keyname><forenames>Zeyu</forenames></author><author><keyname>Polyanskiy</keyname><forenames>Yury</forenames></author><author><keyname>Rakhlin</keyname><forenames>Alexander</forenames></author></authors><title>Rate of convergence of the smoothed empirical Wasserstein distance</title><categories>math.PR cs.IT math.IT math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Consider an empirical measure $\mathbb{P}_n$ induced by $n$ iid samples from a $d$-dimensional $K$-subgaussian distribution $\mathbb{P}$ and let $\gamma = N(0,\sigma^2 I_d)$ be the isotropic Gaussian measure. We study the speed of convergence of the smoothed Wasserstein distance $W_2(\mathbb{P}_n * \gamma, \mathbb{P}*\gamma) = n^{-\alpha + o(1)}$ with $*$ being the convolution of measures. For $K&lt;\sigma$ and in any dimension $d\ge 1$ we show that $\alpha = {1\over2}$. For $K&gt;\sigma$ in dimension $d=1$ we show that the rate is slower and is given by $\alpha = {(\sigma^2 + K^2)^2\over 4 (\sigma^4 + K^4)} &lt; 1/2$. This resolves several open problems in [GGNWP20], and in particular precisely identifies the amount of smoothing $\sigma$ needed to obtain a parametric rate. In addition, for any $d$-dimensional $K$-subgaussian distribution $\mathbb{P}$, we also establish that $D_{KL}(\mathbb{P}_n * \gamma \|\mathbb{P}*\gamma)$ has rate $O(1/n)$ for $K&lt;\sigma$ but only slows down to $O({(\log n)^{d+1}\over n})$ for $K&gt;\sigma$. The surprising difference of the behavior of $W_2^2$ and KL implies the failure of $T_{2}$-transportation inequality when $\sigma &lt; K$. Consequently, it follows that for $K&gt;\sigma$ the log-Sobolev inequality (LSI) for the Gaussian mixture $\mathbb{P} * N(0, \sigma^{2})$ cannot hold. This closes an open problem in [WW+16], who established the LSI under the condition $K&lt;\sigma$ and asked if their bound can be improved. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2206.04902</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2206.04902</id><created>2022-06-10</created><updated>2024-11-13</updated><authors><author><keyname>Gruber</keyname><forenames>Luis</forenames></author><author><keyname>Kastner</keyname><forenames>Gregor</forenames></author></authors><title>Forecasting macroeconomic data with Bayesian VARs: Sparse or dense? It   depends!</title><categories>econ.EM stat.AP stat.ME</categories><journal-ref>International Journal of Forecasting (2025)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vector autogressions (VARs) are widely applied when it comes to modeling and forecasting macroeconomic variables. In high dimensions, however, they are prone to overfitting. Bayesian methods, more concretely shrinkage priors, have shown to be successful in improving prediction performance. In the present paper, we introduce the semi-global framework, in which we replace the traditional global shrinkage parameter with group-specific shrinkage parameters. We show how this framework can be applied to various shrinkage priors, such as global-local priors and stochastic search variable selection priors. We demonstrate the virtues of the proposed framework in an extensive simulation study and in an empirical application forecasting data of the US economy. Further, we shed more light on the ongoing ``Illusion of Sparsity'' debate, finding that forecasting performances under sparse/dense priors vary across evaluated economic variables and across time frames. Dynamic model averaging, however, can combine the merits of both worlds. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2207.06030</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2207.06030</id><created>2022-07-13</created><updated>2025-02-09</updated><authors><author><keyname>Liu</keyname><forenames>Xuefeng</forenames></author><author><keyname>Xia</keyname><forenames>Fangfang</forenames></author><author><keyname>Stevens</keyname><forenames>Rick L.</forenames></author><author><keyname>Chen</keyname><forenames>Yuxin</forenames></author></authors><title>Contextual Active Model Selection</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While training models and labeling data are resource-intensive, a wealth of pre-trained models and unlabeled data exists. To effectively utilize these resources, we present an approach to actively select pre-trained models while minimizing labeling costs. We frame this as an online contextual active model selection problem: At each round, the learner receives an unlabeled data point as a context. The objective is to adaptively select the best model to make a prediction while limiting label requests. To tackle this problem, we propose CAMS, a contextual active model selection algorithm that relies on two novel components: (1) a contextual model selection mechanism, which leverages context information to make informed decisions about which model is likely to perform best for a given context, and (2) an active query component, which strategically chooses when to request labels for data points, minimizing the overall labeling cost. We provide rigorous theoretical analysis for the regret and query complexity under both adversarial and stochastic settings. Furthermore, we demonstrate the effectiveness of our algorithm on a diverse collection of benchmark classification tasks. Notably, CAMS requires substantially less labeling effort (less than 10%) compared to existing methods on CIFAR10 and DRIFT benchmarks, while achieving similar or better accuracy. Our code is publicly available at: https://github.com/xuefeng-cs/Contextual-Active-Model-Selection. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2209.04716</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2209.04716</id><created>2022-09-10</created><updated>2023-11-29</updated><authors><author><keyname>Lotspeich</keyname><forenames>Sarah C.</forenames></author><author><keyname>Garcia</keyname><forenames>Tanya P.</forenames></author></authors><title>Extrapolation before imputation reduces bias when imputing censored   covariates</title><categories>stat.ME</categories><comments>16 pages main text (incl. 2 tables and 3 figures); Supplemental   Materials, R code, and R package available on GitHub (linked in main text)</comments><msc-class>62J05</msc-class><journal-ref>Journal of Computational and Graphical Statistics (2025)</journal-ref><doi>10.1080/10618600.2024.2444323</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Modeling symptom progression to identify informative subjects for a new Huntington's disease clinical trial is problematic since time to diagnosis, a key covariate, can be heavily censored. Imputation is an appealing strategy where censored covariates are replaced with their conditional means, but existing methods saw over 200% bias under heavy censoring. Calculating these conditional means well requires estimating and then integrating over the survival function of the censored covariate from the censored value to infinity. To estimate the survival function flexibly, existing methods use the semiparametric Cox model with Breslow's estimator, leaving the integrand for the conditional means (the estimated survival function) undefined beyond the observed data. The integral is then estimated up to the largest observed covariate value, and this approximation can cut off the tail of the survival function and lead to severe bias, particularly under heavy censoring. We propose a hybrid approach that splices together the semiparametric survival estimator with a parametric extension, making it possible to approximate the integral up to infinity. In simulation studies, our proposed approach of extrapolation then imputation substantially reduces the bias seen with existing imputation methods, even when the parametric extension was misspecified. We further demonstrate how imputing with corrected conditional means helps to prioritize patients for future clinical trials. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2210.09228</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2210.09228</id><created>2022-10-17</created><updated>2025-02-08</updated><authors><author><keyname>Ren</keyname><forenames>Kui</forenames></author><author><keyname>Zhang</keyname><forenames>Lu</forenames></author></authors><title>A Model-Consistent Data-Driven Computational Strategy for PDE Joint   Inversion Problems</title><categories>math.NA cs.LG cs.NA math.OC stat.ME</categories><msc-class>35R30, 49M41, 65M32, 65N21, 68Q32, 68T07, 78A46</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of simultaneously reconstructing multiple physical coefficients in partial differential equations (PDEs) from observed data is ubiquitous in applications. In this work, we propose an integrated data-driven and model-based iterative reconstruction framework for such joint inversion problems where additional data on the unknown coefficients are supplemented for better reconstructions. Our method couples the supplementary data with the PDE model to make the data-driven modeling process consistent with the model-based reconstruction procedure. We characterize the impact of learning uncertainty on the joint inversion results for two typical inverse problems. Numerical evidence is provided to demonstrate the feasibility of using data-driven models to improve the joint inversion of multiple coefficients in PDEs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2211.15353</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2211.15353</id><created>2022-11-25</created><updated>2025-02-07</updated><authors><author><keyname>Letizia</keyname><forenames>Nunzio A.</forenames></author><author><keyname>Tonello</keyname><forenames>Andrea M.</forenames></author></authors><title>Copula Density Neural Estimation</title><categories>cs.LG eess.SP stat.ML</categories><comments>6 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Probability density estimation from observed data constitutes a central task in statistics. Recent advancements in machine learning offer new tools but also pose new challenges. The big data era demands analysis of long-range spatial and long-term temporal dependencies in large collections of raw data, rendering neural networks an attractive solution for density estimation. In this paper, we exploit the concept of copula to explicitly build an estimate of the probability density function associated to any observed data. In particular, we separate univariate marginal distributions from the joint dependence structure in the data, the copula itself, and we model the latter with a neural network-based method referred to as copula density neural estimation (CODINE). Results show that the novel learning approach is capable of modeling complex distributions and it can be applied for mutual information estimation and data generation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2212.09544</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2212.09544</id><created>2022-12-19</created><authors><author><keyname>Gardini</keyname><forenames>Aldo</forenames></author><author><keyname>Greco</keyname><forenames>Fedele</forenames></author><author><keyname>Trivisano</keyname><forenames>Carlo</forenames></author></authors><title>Design and Structure Dependent Priors for Scale Parameters in Latent   Gaussian Models</title><categories>stat.ME math.ST stat.TH</categories><doi>10.1214/24-BA1454</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Many common correlation structures assumed for data can be described through latent Gaussian models. When Bayesian inference is carried out, it is required to set the prior distribution for scale parameters that rules the model components, possibly allowing to incorporate prior information. This task is particularly delicate and many contributions in the literature are devoted to investigating such aspects. We focus on the fact that the scale parameter controls the prior variability of the model component in a complex way since its dispersion is also affected by the correlation structure and the design. To overcome this issue that might confound the prior elicitation step, we propose to let the user specify the marginal prior of a measure of dispersion of the model component, integrating out the scale parameter, the structure and the design. Then, we analytically derive the implied prior for the scale parameter. Results from a simulation study, aimed at showing the behavior of the estimators sampling properties under the proposed prior elicitation strategy, are discussed. Lastly, some real data applications are explored to investigate prior sensitivity and allocation of explained variance among model components. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2302.00516</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2302.00516</id><created>2023-02-01</created><updated>2023-09-26</updated><authors><author><keyname>Lotspeich</keyname><forenames>Sarah C.</forenames></author><author><keyname>Richardson</keyname><forenames>Brian D.</forenames></author><author><keyname>Baldoni</keyname><forenames>Pedro L.</forenames></author><author><keyname>Enders</keyname><forenames>Kimberly P.</forenames></author><author><keyname>Hudgens</keyname><forenames>Michael G.</forenames></author></authors><title>Quantifying the HIV reservoir with dilution assays and deep viral   sequencing</title><categories>stat.ME stat.AP</categories><comments>Main text is 13 pages, including 2 figures and 2 tables. Supporting   information follows the main text</comments><report-no>80(1)</report-no><journal-ref>Biometrics (2024)</journal-ref><doi>10.1093/biomtc/ujad018</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  People living with HIV on antiretroviral therapy often have undetectable virus levels by standard assays, but "latent" HIV still persists in viral reservoirs. Eliminating these reservoirs is the goal of HIV cure research. The quantitative viral outgrowth assay (QVOA) is commonly used to estimate the reservoir size, i.e., the infectious units per million (IUPM) of HIV-persistent resting CD4+ T cells. A new variation of the QVOA, the Ultra Deep Sequencing Assay of the outgrowth virus (UDSA), was recently developed that further quantifies the number of viral lineages within a subset of infected wells. Performing the UDSA on a subset of wells provides additional information that can improve IUPM estimation. This paper considers statistical inference about the IUPM from combined dilution assay (QVOA) and deep viral sequencing (UDSA) data, even when some deep sequencing data are missing. Methods are proposed to accommodate assays with wells sequenced at multiple dilution levels and with imperfect sensitivity and specificity, and a novel bias-corrected estimator is included for small samples. The proposed methods are evaluated in a simulation study, applied to data from the University of North Carolina HIV Cure Center, and implemented in the open-source R package SLDeepAssay. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2304.13077</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2304.13077</id><created>2023-04-25</created><updated>2025-02-07</updated><authors><author><keyname>De Vito</keyname><forenames>Roberta</forenames></author><author><keyname>Avalos-Pacheco</keyname><forenames>Alejandra</forenames></author></authors><title>Multi-study factor regression model: an application in nutritional   epidemiology</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diet is a risk factor for many diseases. In nutritional epidemiology, studying reproducible dietary patterns is critical to reveal important associations with health. However, it is challenging: diverse cultural and ethnic backgrounds may critically impact eating patterns, showing heterogeneity, leading to incorrect dietary patterns and obscuring the components shared across different groups or populations. Moreover, covariate effects generated from observed variables, such as demographics and other confounders, can further bias these dietary patterns. Identifying the shared and group-specific dietary components and covariate effects is essential to drive accurate conclusions. To address these issues, we introduce a new modeling factor regression, the Multi-Study Factor Regression (MSFR) model. The MSFR model analyzes different populations simultaneously, achieving three goals: capturing shared component(s) across populations, identifying group-specific structures, and correcting for covariate effects. We use this novel method to derive common and ethnic-specific dietary patterns in a multi-center epidemiological study in Hispanic/Latinos community. Our model improves the accuracy of common and group dietary signals and yields better prediction than other techniques, revealing significant associations with health. In summary, we provide a tool to integrate different groups, giving accurate dietary signals crucial to inform public health policy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2305.13998</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2305.13998</id><created>2023-05-23</created><updated>2024-01-23</updated><authors><author><keyname>Saves</keyname><forenames>Paul</forenames></author><author><keyname>Lafage</keyname><forenames>Remi</forenames></author><author><keyname>Bartoli</keyname><forenames>Nathalie</forenames></author><author><keyname>Diouane</keyname><forenames>Youssef</forenames></author><author><keyname>Bussemaker</keyname><forenames>Jasper</forenames></author><author><keyname>Lefebvre</keyname><forenames>Thierry</forenames></author><author><keyname>Hwang</keyname><forenames>John T.</forenames></author><author><keyname>Morlier</keyname><forenames>Joseph</forenames></author><author><keyname>Martins</keyname><forenames>Joaquim R. R. A.</forenames></author></authors><title>SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and   Mixed Variables Gaussian Processes</title><categories>cs.LG cs.MS math.OC stat.CO</categories><comments>10.1016/j.advengsoft.2023.103571</comments><journal-ref>Advances in Engineering Software Volume 188, February 2024, 103571</journal-ref><doi>10.1016/j.advengsoft.2023.103571</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multifidelity data. To the best of our knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical and mixed inputs. This open-source software is distributed under the New BSD license. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2305.16272</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2305.16272</id><created>2023-05-25</created><updated>2025-02-10</updated><authors><author><keyname>Dorner</keyname><forenames>Florian E.</forenames></author><author><keyname>Konstantinov</keyname><forenames>Nikola</forenames></author><author><keyname>Pashaliev</keyname><forenames>Georgi</forenames></author><author><keyname>Vechev</keyname><forenames>Martin</forenames></author></authors><title>Incentivizing Honesty among Competitors in Collaborative Learning and   Optimization</title><categories>cs.LG cs.GT stat.ML</categories><comments>Updated experimental results after fixing a mistake in the code.   Previous version published in NeurIPS 2023; 37 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative learning techniques have the potential to enable training machine learning models that are superior to models trained on a single entity's data. However, in many cases, potential participants in such collaborative schemes are competitors on a downstream task, such as firms that each aim to attract customers by providing the best recommendations. This can incentivize dishonest updates that damage other participants' models, potentially undermining the benefits of collaboration. In this work, we formulate a game that models such interactions and study two learning tasks within this framework: single-round mean estimation and multi-round SGD on strongly-convex objectives. For a natural class of player actions, we show that rational clients are incentivized to strongly manipulate their updates, preventing learning. We then propose mechanisms that incentivize honest communication and ensure learning quality comparable to full cooperation. Lastly, we empirically demonstrate the effectiveness of our incentive scheme on a standard non-convex federated learning benchmark. Our work shows that explicitly modeling the incentives and actions of dishonest clients, rather than assuming them malicious, can enable strong robustness guarantees for collaborative learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2306.10947</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2306.10947</id><created>2023-06-19</created><updated>2025-02-10</updated><authors><author><keyname>Masegosa</keyname><forenames>Andrés R.</forenames></author><author><keyname>Ortega</keyname><forenames>Luis A.</forenames></author></authors><title>PAC-Chernoff Bounds: Understanding Generalization in the Interpolation   Regime</title><categories>cs.LG math.ST stat.ML stat.TH</categories><comments>60 pages, 12 figures, published at JAIR 2025</comments><journal-ref>Journal of Artificial Intelligence Research 82 (2025) 503-562</journal-ref><doi>10.1613/jair.1.17036</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper introduces a distribution-dependent PAC-Chernoff bound that exhibits perfect tightness for interpolators, even within over-parameterized model classes. This bound, which relies on basic principles of Large Deviation Theory, defines a natural measure of the smoothness of a model, characterized by simple real-valued functions. Building upon this bound and the new concept of smoothness, we present an unified theoretical framework revealing why certain interpolators show an exceptional generalization, while others falter. We theoretically show how a wide spectrum of modern learning methodologies, encompassing techniques such as $\ell_2$-norm, distance-from-initialization and input-gradient regularization, in combination with data augmentation, invariant architectures, and over-parameterization, collectively guide the optimizer toward smoother interpolators, which, according to our theoretical framework, are the ones exhibiting superior generalization performance. This study shows that distribution-dependent bounds serve as a powerful tool to understand the complex dynamics behind the generalization capabilities of over-parameterized interpolators. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2306.15048</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2306.15048</id><created>2023-06-26</created><updated>2025-02-10</updated><authors><author><keyname>Kaji</keyname><forenames>Tetsuya</forenames></author><author><keyname>Cao</keyname><forenames>Jianfei</forenames></author></authors><title>Assessing Heterogeneity of Treatment Effects</title><categories>econ.EM stat.AP stat.ME</categories><comments>35 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Treatment effect heterogeneity is of major interest in economics, but its assessment is often hindered by the fundamental lack of identification of the individual treatment effects. For example, we may want to assess the effect of a poverty reduction measure at different levels of poverty, but the causal effects on wealth at different wealth levels are not identified. Or, we may be interested in the proportion of workers who benefit from the minimum wage increase, but the proportion is not identified in the absence of counterfactuals. This paper derives bounds useful in such situations, which only depend on the marginal distributions of the outcomes. The bounds are nonparametrically sharp, making clear the maximum extent to which the data can speak about the heterogeneity of the treatment effects. An application to microfinance shows that the bounds can be informative even when the average treatment effects are not significant. Another application to the welfare reform identifies a nonnegligible portion of workers who increased and decreased working hours due to the reform. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2306.16033</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2306.16033</id><created>2023-06-28</created><authors><author><keyname>Gardini</keyname><forenames>Aldo</forenames></author></authors><title>Functional and variables selection in extreme value models for regional   flood frequency analysis</title><categories>stat.ME stat.AP</categories><doi>10.1007/s10651-023-00581-8</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The problem of estimating return levels of river discharge, relevant in flood frequency analysis, is tackled by relying on the extreme value theory. The Generalized Extreme Value (GEV) distribution is assumed to model annual maxima values of river discharge registered at multiple gauging stations belonging to the same river basin. The specific features of the data from the Upper Danube basin drive the definition of the proposed statistical model. Firstly, Bayesian P-splines are considered to account for the non-linear effects of station-specific covariates on the GEV parameters. Secondly, the problem of functional and variable selection is addressed by imposing a grouped horseshoe prior on the coefficients, to encourage the shrinkage of non-relevant components to zero. A cross-validation study is organized to compare the proposed modeling solution to other models, showing its potential in reducing the uncertainty of the ungauged predictions without affecting their calibration. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2307.10272</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2307.10272</id><created>2023-07-18</created><updated>2025-02-06</updated><authors><author><keyname>Takeishi</keyname><forenames>Shota</forenames></author></authors><title>A Shrinkage Likelihood Ratio Test for High-Dimensional Subgroup Analysis   with a Logistic-Normal Mixture Model</title><categories>math.ST stat.ME stat.TH</categories><comments>38 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In subgroup analysis, testing the existence of a subgroup with a differential treatment effect serves as protection against spurious subgroup discovery. Despite its importance, this hypothesis testing possesses a complicated nature: parameter characterizing subgroup classification is not identified under the null hypothesis of no subgroup. Due to this irregularity, the existing methods have the following two limitations. First, the asymptotic null distribution of test statistics often takes an intractable form, which necessitates computationally demanding resampling methods to calculate the critical value. Second, the dimension of personal attributes characterizing subgroup membership is not allowed to be of high dimension. To solve these two problems simultaneously, this study develops a shrinkage likelihood ratio test for the existence of a subgroup using a logistic-normal mixture model. The proposed test statistics are built on a modified likelihood function that shrinks possibly high-dimensional unidentified parameters toward zero under the null hypothesis while retaining power under the alternative. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2307.12557</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2307.12557</id><created>2023-07-24</created><updated>2025-02-10</updated><authors><author><keyname>Baghel</keyname><forenames>Shanya</forenames></author><author><keyname>Mondal</keyname><forenames>Shuvashree</forenames></author></authors><title>Robust Bayesian inference for nondestructive one-shot device testing   data under competing risk using Hamiltonian Monte Carlo method</title><categories>stat.ME math.ST stat.TH</categories><msc-class>62F10, 62F12, 62NO2</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The prevalence of one-shot devices is quite prolific in engineering and medical domains. Unlike typical one-shot devices, nondestructive one-shot devices (NOSD) may survive multiple tests and offer additional data for reliability estimation. This study aims to implement the Bayesian approach of the lifetime prognosis of NOSD when failures are subject to multiple risks. With small deviations from the assumed model conditions, conventional likelihood-based Bayesian estimation may result in misleading statistical inference, raising the need for a robust Bayesian method. This work develops Bayesian estimation by exploiting a robustified posterior based on the density power divergence measure for NOSD test data. Further, the testing of the hypothesis is carried out by applying a proposed Bayes factor derived from the robustified posterior. A flexible Hamiltonian Monte Carlo approach is applied to generate posterior samples. Additionally, we assess the extent of resistance of the proposed methods to small deviations from the assumed model conditions by applying the influence function (IF) approach. In testing of hypothesis, IF reflects how outliers impact the decision-making through Bayes factor under null hypothesis. Finally, this analytical development is validated through a simulation study and a data analysis based on cancer data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2307.12982</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2307.12982</id><created>2023-07-24</created><updated>2025-02-07</updated><authors><author><keyname>Mukherjee</keyname><forenames>Soumendu Sundar</forenames></author></authors><title>Consistent model selection in the spiked Wigner model via AIC-type   criteria</title><categories>math.ST cs.IT math.IT stat.ME stat.ML stat.TH</categories><comments>25 pages, 2 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the spiked Wigner model \[   X = \sum_{i = 1}^k \lambda_i u_i u_i^\top + \sigma G, \] where $G$ is an $N \times N$ GOE random matrix, and the eigenvalues $\lambda_i$ are all spiked, i.e. above the Baik-Ben Arous-P\'ech\'e (BBP) threshold $\sigma$. We consider AIC-type model selection criteria of the form \[   -2 \, (\text{maximised log-likelihood}) + \gamma \, (\text{number of parameters}) \] for estimating the number $k$ of spikes. For $\gamma &gt; 2$, the above criterion is strongly consistent provided $\lambda_k &gt; \lambda_{\gamma}$, where $\lambda_{\gamma}$ is a threshold strictly above the BBP threshold, whereas for $\gamma &lt; 2$, it almost surely overestimates $k$. Although AIC (which corresponds to $\gamma = 2$) is not strongly consistent, we show that taking $\gamma = 2 + \delta_N$, where $\delta_N \to 0$ and $\delta_N \gg N^{-2/3}$, results in a weakly consistent estimator of $k$. We further show that a soft minimiser of AIC, where one chooses the least complex model whose AIC score is close to the minimum AIC score, is strongly consistent. Based on a spiked (generalised) Wigner representation, we also develop similar model selection criteria for consistently estimating the number of communities in a balanced stochastic block model under some sparsity restrictions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2309.16448</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2309.16448</id><created>2023-09-28</created><authors><author><keyname>Žukovič</keyname><forenames>Milan</forenames></author><author><keyname>Hristopulos</keyname><forenames>Dionissios T.</forenames></author></authors><title>A parsimonious, computationally efficient machine learning method for   spatial regression</title><categories>stat.ML cs.LG</categories><comments>42 pages, 15 figures</comments><journal-ref>Stoch Environ Res Risk Assess (2024)</journal-ref><doi>10.1007/s00477-023-02656-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the modified planar rotator method (MPRS), a physically inspired machine learning method for spatial/temporal regression. MPRS is a non-parametric model which incorporates spatial or temporal correlations via short-range, distance-dependent ``interactions'' without assuming a specific form for the underlying probability distribution. Predictions are obtained by means of a fully autonomous learning algorithm which employs equilibrium conditional Monte Carlo simulations. MPRS is able to handle scattered data and arbitrary spatial dimensions. We report tests on various synthetic and real-word data in one, two and three dimensions which demonstrate that the MPRS prediction performance (without parameter tuning) is competitive with standard interpolation methods such as ordinary kriging and inverse distance weighting. In particular, MPRS is a particularly effective gap-filling method for rough and non-Gaussian data (e.g., daily precipitation time series). MPRS shows superior computational efficiency and scalability for large samples. Massive data sets involving millions of nodes can be processed in a few seconds on a standard personal computer. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.07399</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.07399</id><created>2023-10-11</created><updated>2024-10-03</updated><authors><author><keyname>Bou-Rabee</keyname><forenames>Nawaf</forenames></author><author><keyname>Kleppe</keyname><forenames>Tore Selland</forenames></author></authors><title>Randomized Runge-Kutta-Nystr\"om Methods for Unadjusted Hamiltonian and   Kinetic Langevin Monte Carlo</title><categories>math.NA cs.NA math.PR stat.CO stat.ME stat.ML</categories><msc-class>60J05 (Primary) 65C05, 65L05, 65P10 (Secondary)</msc-class><journal-ref>Mathematics of Computation 2025</journal-ref><doi>10.1090/mcom/4061</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce $5/2$- and $7/2$-order $L^2$-accurate randomized Runge-Kutta-Nystr\"{o}m methods, tailored for approximating Hamiltonian flows within non-reversible Markov chain Monte Carlo samplers, such as unadjusted Hamiltonian Monte Carlo and unadjusted kinetic Langevin Monte Carlo. We establish quantitative $5/2$-order $L^2$-accuracy upper bounds under gradient and Hessian Lipschitz assumptions on the potential energy function. The numerical experiments demonstrate the superior efficiency of the proposed unadjusted samplers on a variety of well-behaved, high-dimensional target distributions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.11799</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.11799</id><created>2023-10-18</created><updated>2025-02-10</updated><authors><author><keyname>Sattler</keyname><forenames>Paavo</forenames></author><author><keyname>Dobler</keyname><forenames>Dennis</forenames></author></authors><title>Testing for patterns and structures in covariance and correlation   matrices</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Covariance matrices of random vectors contain information that is crucial for modelling. Specific structures and patterns of the covariances (or correlations) may be used to justify parametric models, e.g., autoregressive models. Until now, there have been only a few approaches for testing such covariance structures and most of them can only be used for one particular structure. In the present paper, we propose a systematic and unified testing procedure working among others for the large class of linear covariance structures. Our approach requires only weak distributional assumptions. It covers common structures such as diagonal matrices, Toeplitz matrices and compound symmetry, as well as the more involved autoregressive matrices. We exemplify the approach for all these structures. We prove the correctness of these tests for large sample sizes and use bootstrap techniques for a better small-sample approximation. Moreover, the proposed tests invite adaptations to other covariance patterns by choosing the hypothesis matrix appropriately. With the help of a simulation study, we also assess the small sample properties of the tests. Finally, we illustrate the procedure in an application to a real data set. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2311.02655</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2311.02655</id><created>2023-11-05</created><updated>2025-02-06</updated><authors><author><keyname>Horst</keyname><forenames>Ulrich</forenames></author><author><keyname>Xu</keyname><forenames>Wei</forenames></author></authors><title>Second-Order Regular Variation and Second-Order Approximation of Hawkes   Processes</title><categories>math.PR math.FA math.ST stat.TH</categories><comments>40 pages</comments><msc-class>Primary 26A12, 40E05, secondary 60G55, 60K05</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper provides and extends second-order versions of several fundamental theorems on first-order regularly varying functions such as Karamata's theorem/representation and Tauberian's theorem. Our results are used to establish second-order approximations for the mean and variance of Hawkes processes with general kernels. Our approximations provide novel insights into the asymptotic behavior of Hawkes processes. They are also of key importance when establishing functional limit theorems for Hawkes processes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2311.10282</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2311.10282</id><created>2023-11-16</created><updated>2025-02-10</updated><authors><author><keyname>Arsenteva</keyname><forenames>Polina</forenames></author><author><keyname>Benadjaoud</keyname><forenames>Mohamed Amine</forenames></author><author><keyname>Cardot</keyname><forenames>Hervé</forenames></author></authors><title>Joint clustering with alignment for temporal data in a   one-point-per-experiment setting</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Temporal data, obtained in the setting where it is only possible to observe one time point per experiment, is widely used in different research fields, yet remains insufficiently addressed from the statistical point of view. Such data often contain observations of a large number of entities, in which case it is of interest to identify a small number of representative behavior types. In this paper, we propose a new method that simultaneously performs clustering and alignment of temporal objects inferred from these data, providing insight into the relationships between entities. Simulations confirm the ability of the proposed approach to leverage multiple properties of the complex data we target such as accessible uncertainties, correlations and a small number of time points. We illustrate it on real data encoding cellular response to a radiation treatment with high energy, supported with the results of an enrichment analysis. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.00770</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.00770</id><created>2023-12-01</created><updated>2025-02-10</updated><authors><author><keyname>Loe</keyname><forenames>Abigail</forenames></author><author><keyname>Murray</keyname><forenames>Susan</forenames></author><author><keyname>Wu</keyname><forenames>Zhenke</forenames></author></authors><title>Random Forest for Dynamic Risk Prediction or Recurrent Events: A   Pseudo-Observation Approach</title><categories>stat.AP stat.ME</categories><comments>To be published in an upcoming edition of the journal Biostatistics</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Recurrent events are common in clinical, healthcare, social and behavioral studies. A recent analysis framework for potentially censored recurrent event data is to construct a censored longitudinal data set consisting of times to the first recurrent event in multiple prespecified follow-up windows of length $\tau$. With the staggering number of potential predictors being generated from genetic, -omic, and electronic health records sources, machine learning approaches such as the random forest are growing in popularity, as they can incorporate information from highly correlated predictors with non-standard relationships. In this paper, we bridge this gap by developing a random forest approach for dynamically predicting probabilities of remaining event-free during a subsequent $\tau$-duration follow-up period from a reconstructed censored longitudinal data set. We demonstrate the increased ability of our random forest algorithm for predicting the probability of remaining event-free over a $\tau$-duration follow-up period when compared to the recurrent event modeling framework of Xia et al. (2020) in settings where association between predictors and recurrent event outcomes is complex in nature. The proposed random forest algorithm is demonstrated using recurrent exacerbation data from the Azithromycin for the Prevention of Exacerbations of Chronic Obstructive Pulmonary Disease (Albert et al., 2011). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.04711</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.04711</id><created>2024-02-07</created><updated>2024-05-26</updated><authors><author><keyname>Saves</keyname><forenames>Paul</forenames></author></authors><title>High-dimensional multidisciplinary design optimization for aircraft   eco-design / Optimisation multi-disciplinaire en grande dimension pour   l'\'eco-conception avion en avant-projet</title><categories>math.OC cs.MS stat.ML</categories><comments>PhD Thesis, Universit\'e de Toulouse, Toulouse, 2024 on Gaussian   Process kernels for Bayesian optimization in high dimension with mixed and   hierarchical variables at ISAE-SUPAERO. Keywords: Gaussian process, Black-box   optimization, Bayesian inference, Multidisciplinary design optimization,   Mixed hierarchical and categorical inputs, Eco-friendly aircraft design</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The objective of this Philosophiae Doctor (Ph.D) thesis is to propose an efficient approach for optimizing a multidisciplinary black-box model when the optimization problem is constrained and involves a large number of mixed integer design variables (typically 100 variables). The targeted optimization approach, called EGO, is based on a sequential enrichment of an adaptive surrogate model and, in this context, GP surrogate models are one of the most widely used in engineering problems to approximate time-consuming high fidelity models. EGO is a heuristic BO method that performs well in terms of solution quality. However, like any other global optimization method, EGO suffers from the curse of dimensionality, meaning that its performance is satisfactory on lower dimensional problems, but deteriorates as the dimensionality of the optimization search space increases. For realistic aircraft design problems, the typical size of the design variables can even exceed 100 and, thus, trying to solve directly the problems using EGO is ruled out. The latter is especially true when the problems involve both continuous and categorical variables increasing even more the size of the search space. In this Ph.D thesis, effective parameterization tools are investigated, including techniques like partial least squares regression, to significantly reduce the number of design variables. Additionally, Bayesian optimization is adapted to handle discrete variables and high-dimensional spaces in order to reduce the number of evaluations when optimizing innovative aircraft concepts such as the "DRAGON" hybrid airplane to reduce their climate impact. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.14029</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.14029</id><created>2024-02-19</created><updated>2025-02-08</updated><authors><author><keyname>Otsuka</keyname><forenames>Hikari</forenames></author><author><keyname>Chijiwa</keyname><forenames>Daiki</forenames></author><author><keyname>García-Arias</keyname><forenames>Ángel López</forenames></author><author><keyname>Okoshi</keyname><forenames>Yasuyuki</forenames></author><author><keyname>Kawamura</keyname><forenames>Kazushi</forenames></author><author><keyname>Van Chu</keyname><forenames>Thiem</forenames></author><author><keyname>Fujiki</keyname><forenames>Daichi</forenames></author><author><keyname>Takeuchi</keyname><forenames>Susumu</forenames></author><author><keyname>Motomura</keyname><forenames>Masato</forenames></author></authors><title>Partially Frozen Random Networks Contain Compact Strong Lottery Tickets</title><categories>cs.LG cs.AI stat.ML</categories><comments>Accepted at TMLR</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Randomly initialized dense networks contain subnetworks that achieve high accuracy without weight learning--strong lottery tickets (SLTs). Recently, Gadhikar et al. (2023) demonstrated that SLTs could also be found within a randomly pruned source network. This phenomenon can be exploited to further compress the small memory size required by SLTs. However, their method is limited to SLTs that are even sparser than the source, leading to worse accuracy due to unintentionally high sparsity. This paper proposes a method for reducing the SLT memory size without restricting the sparsity of the SLTs that can be found. A random subset of the initial weights is frozen by either permanently pruning them or locking them as a fixed part of the SLT, resulting in a smaller model size. Experimental results show that Edge-Popup (Ramanujan et al., 2020; Sreenivasan et al., 2022) finds SLTs with better accuracy-to-model size trade-off within frozen networks than within dense or randomly pruned source networks. In particular, freezing $70\%$ of a ResNet on ImageNet provides $3.3 \times$ compression compared to the SLT found within a dense counterpart, raises accuracy by up to $14.12$ points compared to the SLT found within a randomly pruned counterpart, and offers a better accuracy-model size trade-off than both. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.17067</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.17067</id><created>2024-02-26</created><updated>2025-02-08</updated><authors><author><keyname>Liang</keyname><forenames>Jiaming</forenames></author><author><keyname>Mitra</keyname><forenames>Siddharth</forenames></author><author><keyname>Wibisono</keyname><forenames>Andre</forenames></author></authors><title>Characterizing Dependence of Samples along the Langevin Dynamics and   Algorithms via Contraction of $\Phi$-Mutual Information</title><categories>math.ST cs.IT math.IT stat.ML stat.TH</categories><comments>v2: Generalizes results to $\Phi$-mutual information and to Proximal   Sampler. 50 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mixing time of a Markov chain determines how fast the iterates of the Markov chain converge to the stationary distribution; however, it does not control the dependencies between samples along the Markov chain. In this paper, we study the question of how fast the samples become approximately independent along popular Markov chains for continuous-space sampling: the Langevin dynamics in continuous time, and the Unadjusted Langevin Algorithm and the Proximal Sampler in discrete time. We measure the dependence between samples via $\Phi$-mutual information, which is a broad generalization of the standard mutual information, and which is equal to $0$ if and only if the the samples are independent. We show that along these Markov chains, the $\Phi$-mutual information between the first and the $k$-th iterate decreases to $0$ exponentially fast in $k$ when the target distribution is strongly log-concave. Our proof technique is based on showing the Strong Data Processing Inequalities (SDPIs) hold along the Markov chains. To prove fast mixing of the Markov chains, we only need to show the SDPIs hold for the stationary distribution. In contrast, to prove the contraction of $\Phi$-mutual information, we need to show the SDPIs hold along the entire trajectories of the Markov chains; we prove this when the iterates along the Markov chains satisfy the corresponding $\Phi$-Sobolev inequality, which is implied by the strong log-concavity of the target distribution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.07236</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.07236</id><created>2024-03-11</created><updated>2025-02-07</updated><authors><author><keyname>Moon</keyname><forenames>Sarah</forenames></author></authors><title>Partial Identification of Individual-Level Parameters Using Aggregate   Data in a Nonparametric Model</title><categories>econ.EM stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  I develop a methodology to partially identify linear combinations of conditional mean outcomes when the researcher only has access to aggregate data. Unlike the existing literature, I only allow for marginal, not joint, distributions of covariates in my model of aggregate data. Identified sets are very wide in an empirical illustration, suggesting that in order to obtain useful results when only using aggregate data, researchers may have no option other than to impose strong assumptions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.12540</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.12540</id><created>2024-03-19</created><updated>2025-02-08</updated><authors><author><keyname>Qing</keyname><forenames>Huan</forenames></author></authors><title>Community detection by spectral methods in multi-layer networks</title><categories>cs.SI stat.ML</categories><journal-ref>Applied Soft Computing 2025</journal-ref><doi>10.1016/j.asoc.2025.112769</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection in multi-layer networks is a crucial problem in network analysis. In this paper, we analyze the performance of two spectral clustering algorithms for community detection within the framework of the multi-layer degree-corrected stochastic block model (MLDCSBM) framework. One algorithm is based on the sum of adjacency matrices, while the other utilizes the debiased sum of squared adjacency matrices. We also provide their accelerated versions through subsampling to handle large-scale multi-layer networks. We establish consistency results for community detection of the two proposed methods under MLDCSBM as the size of the network and/or the number of layers increases. Our theorems demonstrate the advantages of utilizing multiple layers for community detection. Our analysis also indicates that spectral clustering with the debiased sum of squared adjacency matrices is generally superior to spectral clustering with the sum of adjacency matrices. Furthermore, we provide a strategy to estimate the number of communities in multi-layer networks by maximizing the averaged modularity. Substantial numerical simulations demonstrate the superiority of our algorithm employing the debiased sum of squared adjacency matrices over existing methods for community detection in multi-layer networks, the high computational efficiency of our accelerated algorithms for large-scale multi-layer networks, and the high accuracy of our strategy in estimating the number of communities. Finally, the analysis of several real-world multi-layer networks yields meaningful insights. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.12789</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.12789</id><created>2024-03-19</created><updated>2025-02-10</updated><authors><author><keyname>Pan</keyname><forenames>Ruyi</forenames></author><author><keyname>Nieto-Barajas</keyname><forenames>Luis E.</forenames></author><author><keyname>Craiu</keyname><forenames>Radu</forenames></author></authors><title>Multivariate temporal dependence via mixtures of rotated copulas</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Parametric copula families have been known to flexibly capture various dependence patterns, e.g., either positive or negative dependence in either the lower or upper tails of bivariate distributions. In this paper, our objective is to construct a model that is adaptable enough to capture several of these features simultaneously in $m$ dimensions. We propose a mixture of $2^m$ rotations of a parametric copula that can achieve this goal. We illustrate the construction using the Clayton family but the concept is general and can be applied to other families. In order to include dynamic dependence regimes, the approach is extended to a time-dependent sequence of mixture copulas in which the mixture probabilities are allowed to evolve in time via a moving average and seasonal types of relationship. The properties of the proposed model and its performance are examined using simulated and real data sets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.13196</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.13196</id><created>2024-03-19</created><updated>2025-02-07</updated><authors><author><keyname>Eskandar</keyname><forenames>Masih</forenames></author><author><keyname>Imtiaz</keyname><forenames>Tooba</forenames></author><author><keyname>Wang</keyname><forenames>Zifeng</forenames></author><author><keyname>Dy</keyname><forenames>Jennifer</forenames></author></authors><title>ADAPT to Robustify Prompt Tuning Vision Transformers</title><categories>cs.LG cs.AI cs.CV stat.ML</categories><comments>Published in Transactions on Machine Learning Research (2025)</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The performance of deep models, including Vision Transformers, is known to be vulnerable to adversarial attacks. Many existing defenses against these attacks, such as adversarial training, rely on full-model fine-tuning to induce robustness in the models. These defenses require storing a copy of the entire model, that can have billions of parameters, for each task. At the same time, parameter-efficient prompt tuning is used to adapt large transformer-based models to downstream tasks without the need to save large copies. In this paper, we examine parameter-efficient prompt tuning of Vision Transformers for downstream tasks under the lens of robustness. We show that previous adversarial defense methods, when applied to the prompt tuning paradigm, suffer from gradient obfuscation and are vulnerable to adaptive attacks. We introduce ADAPT, a novel framework for performing adaptive adversarial training in the prompt tuning paradigm. Our method achieves competitive robust accuracy of ~40% w.r.t. SOTA robustness methods using full-model fine-tuning, by tuning only ~1% of the number of parameters. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.13340</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.13340</id><created>2024-03-20</created><updated>2025-02-07</updated><authors><author><keyname>Jiménez-Varón</keyname><forenames>Cristian F.</forenames></author><author><keyname>Sun</keyname><forenames>Ying</forenames></author><author><keyname>Shang</keyname><forenames>Han Lin</forenames></author></authors><title>Forecasting density-valued functional panel data</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce a statistical method for modeling and forecasting functional panel data represented by multiple densities. Density functions are nonnegative and have a constrained integral and thus do not constitute a linear vector space. We implement a center log-ratio transformation to transform densities into unconstrained functions. These functions exhibit cross-sectional correlation and temporal dependence. Via a functional analysis of variance decomposition, we decompose the unconstrained functional panel data into a deterministic trend component and a time-varying residual component. To produce forecasts for the time-varying component, a functional time series forecasting method, based on the estimation of the long-run covariance, is implemented. By combining the forecasts of the time-varying residual component with the deterministic trend component, we obtain $h$-step-ahead forecast curves for multiple populations. Illustrated by age- and sex-specific life-table death counts in the United States, we apply our proposed method to generate forecasts of the life-table death counts for 51 states. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.03586</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.03586</id><created>2024-04-04</created><updated>2025-02-07</updated><authors><author><keyname>Chang</keyname><forenames>Tyler</forenames></author><author><keyname>Gillette</keyname><forenames>Andrew</forenames></author><author><keyname>Maulik</keyname><forenames>Romit</forenames></author></authors><title>Leveraging Interpolation Models and Error Bounds for Verifiable   Scientific Machine Learning</title><categories>cs.LG stat.ML</categories><journal-ref>Journal of Computational Physics, Vol. 524, March 2025, 113726</journal-ref><doi>10.1016/j.jcp.2025.113726</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Effective verification and validation techniques for modern scientific machine learning workflows are challenging to devise. Statistical methods are abundant and easily deployed, but often rely on speculative assumptions about the data and methods involved. Error bounds for classical interpolation techniques can provide mathematically rigorous estimates of accuracy, but often are difficult or impractical to determine computationally. In this work, we present a best-of-both-worlds approach to verifiable scientific machine learning by demonstrating that (1) multiple standard interpolation techniques have informative error bounds that can be computed or estimated efficiently; (2) comparative performance among distinct interpolants can aid in validation goals; (3) deploying interpolation methods on latent spaces generated by deep learning techniques enables some interpretability for black-box models. We present a detailed case study of our approach for predicting lift-drag ratios from airfoil images. Code developed for this work is available in a public Github repository. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.17442</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.17442</id><created>2024-04-26</created><updated>2025-02-10</updated><authors><author><keyname>Dupuis</keyname><forenames>Benjamin</forenames></author><author><keyname>Viallard</keyname><forenames>Paul</forenames></author><author><keyname>Deligiannidis</keyname><forenames>George</forenames></author><author><keyname>Simsekli</keyname><forenames>Umut</forenames></author></authors><title>Uniform Generalization Bounds on Data-Dependent Hypothesis Sets via   PAC-Bayesian Theory on Random Sets</title><categories>stat.ML cs.LG</categories><journal-ref>Journal of Machine Learning Research (2024)</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose data-dependent uniform generalization bounds by approaching the problem from a PAC-Bayesian perspective. We first apply the PAC-Bayesian framework on "random sets" in a rigorous way, where the training algorithm is assumed to output a data-dependent hypothesis set after observing the training data. This approach allows us to prove data-dependent bounds, which can be applicable in numerous contexts. To highlight the power of our approach, we consider two main applications. First, we propose a PAC-Bayesian formulation of the recently developed fractal-dimension-based generalization bounds. The derived results are shown to be tighter and they unify the existing results around one simple proof technique. Second, we prove uniform bounds over the trajectories of continuous Langevin dynamics and stochastic gradient Langevin dynamics. These results provide novel information about the generalization properties of noisy algorithms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.17561</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.17561</id><created>2024-04-26</created><updated>2025-02-10</updated><authors><author><keyname>Liang</keyname><forenames>Ziyi</forenames></author><author><keyname>Xie</keyname><forenames>Tianmin</forenames></author><author><keyname>Tong</keyname><forenames>Xin</forenames></author><author><keyname>Sesia</keyname><forenames>Matteo</forenames></author></authors><title>Structured Conformal Inference for Matrix Completion with Applications   to Group Recommender Systems</title><categories>stat.ME stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We develop a conformal inference method to construct a joint confidence region for a given group of missing entries within a sparsely observed matrix, focusing primarily on entries from the same column. Our method is model-agnostic and can be combined with any ``black-box'' matrix completion algorithm to provide reliable uncertainty estimation for group-level recommendations. For example, in the context of movie recommendations, it is useful to quantify the uncertainty in the ratings assigned by all members of a group to the same movie, enabling more informed decision-making when individual preferences may conflict. Unlike existing conformal techniques, which estimate uncertainty for one individual at a time, our method provides stronger group-level guarantees by assembling a structured calibration dataset that mimics the dependencies expected in the test group. To achieve this, we introduce a generalized weighted conformalization framework that addresses the lack of exchangeability arising from structured calibration, introducing several innovations to overcome associated computational challenges. We demonstrate the practicality and effectiveness of our approach through extensive numerical experiments and an analysis of the MovieLens 100K dataset. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.18786</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.18786</id><created>2024-04-29</created><updated>2025-02-10</updated><authors><author><keyname>Aronow</keyname><forenames>P. M.</forenames></author><author><keyname>Chang</keyname><forenames>Haoge</forenames></author><author><keyname>Lopatto</keyname><forenames>Patrick</forenames></author></authors><title>Randomization-based confidence sets for the local average treatment   effect</title><categories>math.ST stat.ME stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of generating confidence sets in randomized experiments with noncompliance. We show that a refinement of a randomization-based procedure proposed by Imbens and Rosenbaum (2005) has desirable properties. Namely, we show that using a studentized Anderson--Rubin-type statistic as a test statistic yields confidence sets that are finite-sample exact under treatment effect homogeneity, and remain asymptotically valid for the Local Average Treatment Effect when the treatment effect is heterogeneous. We provide a uniform analysis of this procedure and efficient algorithms to construct the confidence set. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.19756</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.19756</id><created>2024-04-30</created><updated>2025-02-09</updated><authors><author><keyname>Liu</keyname><forenames>Ziming</forenames></author><author><keyname>Wang</keyname><forenames>Yixuan</forenames></author><author><keyname>Vaidya</keyname><forenames>Sachin</forenames></author><author><keyname>Ruehle</keyname><forenames>Fabian</forenames></author><author><keyname>Halverson</keyname><forenames>James</forenames></author><author><keyname>Soljačić</keyname><forenames>Marin</forenames></author><author><keyname>Hou</keyname><forenames>Thomas Y.</forenames></author><author><keyname>Tegmark</keyname><forenames>Max</forenames></author></authors><title>KAN: Kolmogorov-Arnold Networks</title><categories>cs.LG cond-mat.dis-nn cs.AI stat.ML</categories><comments>Accepted by International Conference on Learning Representations   (ICLR) 2025 (conference version: https://openreview.net/forum?id=Ozo7qJ5vZi).   Codes are available at https://github.com/KindXiaoming/pykan</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes ("neurons"), KANs have learnable activation functions on edges ("weights"). KANs have no linear weights at all -- every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability. For accuracy, much smaller KANs can achieve comparable or better accuracy than much larger MLPs in data fitting and PDE solving. Theoretically and empirically, KANs possess faster neural scaling laws than MLPs. For interpretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful collaborators helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today's deep learning models which rely heavily on MLPs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.03083</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.03083</id><created>2024-05-05</created><updated>2025-02-10</updated><authors><author><keyname>Kim</keyname><forenames>Kwangho</forenames></author><author><keyname>Kim</keyname><forenames>Jisu</forenames></author><author><keyname>Kennedy</keyname><forenames>Edward H.</forenames></author></authors><title>Causal K-Means Clustering</title><categories>stat.ME cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Causal effects are often characterized with population summaries. These might provide an incomplete picture when there are heterogeneous treatment effects across subgroups. Since the subgroup structure is typically unknown, it is more challenging to identify and evaluate subgroup effects than population effects. We propose a new solution to this problem: Causal k-Means Clustering, which harnesses the widely-used k-means clustering algorithm to uncover the unknown subgroup structure. Our problem differs significantly from the conventional clustering setup since the variables to be clustered are unknown counterfactual functions. We present a plug-in estimator which is simple and readily implementable using off-the-shelf algorithms, and study its rate of convergence. We also develop a new bias-corrected estimator based on nonparametric efficiency theory and double machine learning, and show that this estimator achieves fast root-n rates and asymptotic normality in large nonparametric models. Our proposed methods are especially useful for modern outcome-wide studies with multiple treatment levels. Further, our framework is extensible to clustering with generic pseudo-outcomes, such as partially observed outcomes or otherwise unknown functions. Finally, we explore finite sample properties via simulation, and illustrate the proposed methods in a study of treatment programs for adolescent substance abuse. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.07292</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.07292</id><created>2024-05-12</created><updated>2025-02-08</updated><authors><author><keyname>Jat</keyname><forenames>Rajveer</forenames></author><author><keyname>Padha</keyname><forenames>Daanish</forenames></author></authors><title>Kernel Three Pass Regression Filter</title><categories>econ.EM q-fin.ST stat.ME</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We forecast a single time series using a high-dimensional set of predictors. When these predictors share common underlying dynamics, an approximate latent factor model provides a powerful characterization of their co-movements Bai(2003). These latent factors succinctly summarize the data and can also be used for prediction, alleviating the curse of dimensionality in high-dimensional prediction exercises, see Stock &amp; Watson (2002a). However, forecasting using these latent factors suffers from two potential drawbacks. First, not all pervasive factors among the set of predictors may be relevant, and using all of them can lead to inefficient forecasts. The second shortcoming is the assumption of linear dependence of predictors on the underlying factors. The first issue can be addressed by using some form of supervision, which leads to the omission of irrelevant information. One example is the three-pass regression filter proposed by Kelly &amp; Pruitt (2015). We extend their framework to cases where the form of dependence might be nonlinear by developing a new estimator, which we refer to as the Kernel Three-Pass Regression Filter (K3PRF). This alleviates the aforementioned second shortcoming. The estimator is computationally efficient and performs well empirically. The short-term performance matches or exceeds that of established models, while the long-term performance shows significant improvement. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.07757</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.07757</id><created>2024-05-13</created><updated>2025-02-10</updated><authors><author><keyname>Moen</keyname><forenames>Per August Jarval</forenames></author></authors><title>Minimax rates in variance and covariance changepoint testing</title><categories>math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the detection of a change in the covariance matrix of $n$ independent sub-Gaussian random variables of dimension $p$. Our first contribution is to show that $\log\log(8n)$ is the exact minimax testing rate for a change in variance when $p=1$, thereby giving a complete characterization of the problem for univariate data. Our second contribution is to derive a lower bound on the minimax testing rate under the operator norm, taking a certain notion of sparsity into account. In the low- to moderate-dimensional region of the parameter space, we are able to match the lower bound from above with an optimal test based on sparse eigenvalues. In the remaining region of the parameter space, where the dimensionality is high, the minimax lower bound implies that changepoint testing is very difficult. As our third contribution, we propose a computationally feasible variant of the optimal multivariate test for a change in covariance, which is also adaptive to the nominal noise level and the sparsity level of the change. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.14051</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.14051</id><created>2024-05-22</created><updated>2025-02-09</updated><authors><author><keyname>Ni</keyname><forenames>Yijin</forenames></author><author><keyname>Huo</keyname><forenames>Xiaoming</forenames></author></authors><title>A Uniform Concentration Inequality for Kernel-Based Two-Sample   Statistics</title><categories>cs.LG math.ST stat.TH</categories><comments>70 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many contemporary statistical and machine learning methods, one needs to optimize an objective function that depends on the discrepancy between two probability distributions. The discrepancy can be referred to as a metric for distributions. Widely adopted examples of such a metric include Energy Distance (ED), distance Covariance (dCov), Maximum Mean Discrepancy (MMD), and the Hilbert-Schmidt Independence Criterion (HSIC). We show that these metrics can be unified under a general framework of kernel-based two-sample statistics.   This paper establishes a novel uniform concentration inequality for the aforementioned kernel-based statistics. Our results provide upper bounds for estimation errors in the associated optimization problems, thereby offering both finite-sample and asymptotic performance guarantees. As illustrative applications, we demonstrate how these bounds facilitate the derivation of error bounds for procedures such as distance covariance-based dimension reduction, distance covariance-based independent component analysis, MMD-based fairness-constrained inference, MMD-based generative model search, and MMD-based generative adversarial networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.15459</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.15459</id><created>2024-05-24</created><updated>2025-02-10</updated><authors><author><keyname>Arnaboldi</keyname><forenames>Luca</forenames></author><author><keyname>Dandi</keyname><forenames>Yatin</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Pesce</keyname><forenames>Luca</forenames></author><author><keyname>Stephan</keyname><forenames>Ludovic</forenames></author></authors><title>Repetita Iuvant: Data Repetition Allows SGD to Learn High-Dimensional   Multi-Index Functions</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Neural networks can identify low-dimensional relevant structures within high-dimensional noisy data, yet our mathematical understanding of how they do so remains scarce. Here, we investigate the training dynamics of two-layer shallow neural networks trained with gradient-based algorithms, and discuss how they learn pertinent features in multi-index models, that is target functions with low-dimensional relevant directions. In the high-dimensional regime, where the input dimension $d$ diverges, we show that a simple modification of the idealized single-pass gradient descent training scenario, where data can now be repeated or iterated upon twice, drastically improves its computational efficiency. In particular, it surpasses the limitations previously believed to be dictated by the Information and Leap exponents associated with the target function to be learned. Our results highlight the ability of networks to learn relevant structures from data alone without any pre-processing. More precisely, we show that (almost) all directions are learned with at most $O(d \log d)$ steps. Among the exceptions is a set of hard functions that includes sparse parities. In the presence of coupling between directions, however, these can be learned sequentially through a hierarchical mechanism that generalizes the notion of staircase functions. Our results are proven by a rigorous study of the evolution of the relevant statistics for high-dimensional dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.18929</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.18929</id><created>2024-05-29</created><updated>2025-02-08</updated><authors><author><keyname>Takahashi</keyname><forenames>Hiroshi</forenames></author><author><keyname>Iwata</keyname><forenames>Tomoharu</forenames></author><author><keyname>Kumagai</keyname><forenames>Atsutoshi</forenames></author><author><keyname>Yamanaka</keyname><forenames>Yuuki</forenames></author></authors><title>Deep Positive-Unlabeled Anomaly Detection for Contaminated Unlabeled   Data</title><categories>stat.ML cs.AI cs.LG</categories><comments>Under revirew. Code is available at   https://github.com/takahashihiroshi/pusvdd</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Semi-supervised anomaly detection, which aims to improve the anomaly detection performance by using a small amount of labeled anomaly data in addition to unlabeled data, has attracted attention. Existing semi-supervised approaches assume that most unlabeled data are normal, and train anomaly detectors by minimizing the anomaly scores for the unlabeled data while maximizing those for the labeled anomaly data. However, in practice, the unlabeled data are often contaminated with anomalies. This weakens the effect of maximizing the anomaly scores for anomalies, and prevents us from improving the detection performance. To solve this problem, we propose the deep positive-unlabeled anomaly detection framework, which integrates positive-unlabeled learning with deep anomaly detection models such as autoencoders and deep support vector data descriptions. Our approach enables the approximation of anomaly scores for normal data using the unlabeled data and the labeled anomaly data. Therefore, without labeled normal data, our approach can train anomaly detectors by minimizing the anomaly scores for normal data while maximizing those for the labeled anomaly data. Experiments on various datasets show that our approach achieves better detection performance than existing approaches. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.00368</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.00368</id><created>2024-06-01</created><updated>2025-02-08</updated><authors><author><keyname>Iakovlev</keyname><forenames>Valerii</forenames></author><author><keyname>Lähdesmäki</keyname><forenames>Harri</forenames></author></authors><title>Learning Spatiotemporal Dynamical Systems from Point Process   Observations</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Spatiotemporal dynamics models are fundamental for various domains, from heat propagation in materials to oceanic and atmospheric flows. However, currently available neural network-based spatiotemporal modeling approaches fall short when faced with data that is collected randomly over time and space, as is often the case with sensor networks in real-world applications like crowdsourced earthquake detection or pollution monitoring. In response, we developed a new method that can effectively learn spatiotemporal dynamics from such point process observations. Our model integrates techniques from neural differential equations, neural point processes, implicit neural representations and amortized variational inference to model both the dynamics of the system and the probabilistic locations and timings of observations. It outperforms existing methods on challenging spatiotemporal datasets by offering substantial improvements in predictive accuracy and computational efficiency, making it a useful tool for modeling and understanding complex dynamical systems observed under realistic, unconstrained conditions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.01561</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.01561</id><created>2024-06-03</created><updated>2025-02-08</updated><authors><author><keyname>Zhou</keyname><forenames>Mingyuan</forenames></author><author><keyname>Wang</keyname><forenames>Zhendong</forenames></author><author><keyname>Zheng</keyname><forenames>Huangjie</forenames></author><author><keyname>Huang</keyname><forenames>Hai</forenames></author></authors><title>Guided Score identity Distillation for Data-Free One-Step Text-to-Image   Generation</title><categories>cs.CV cs.AI cs.CL cs.LG stat.ML</categories><comments>ICLR 2025; fixed typos in Table 1; Code and model checkpoints   available at https://github.com/mingyuanzhou/SiD-LSG; More efficient code   using AMP is coming soon</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diffusion-based text-to-image generation models trained on extensive text-image pairs have demonstrated the ability to produce photorealistic images aligned with textual descriptions. However, a significant limitation of these models is their slow sample generation process, which requires iterative refinement through the same network. To overcome this, we introduce a data-free guided distillation method that enables the efficient distillation of pretrained Stable Diffusion models without access to the real training data, often restricted due to legal, privacy, or cost concerns. This method enhances Score identity Distillation (SiD) with Long and Short Classifier-Free Guidance (LSG), an innovative strategy that applies Classifier-Free Guidance (CFG) not only to the evaluation of the pretrained diffusion model but also to the training and evaluation of the fake score network. We optimize a model-based explicit score matching loss using a score-identity-based approximation alongside our proposed guidance strategies for practical computation. By exclusively training with synthetic images generated by its one-step generator, our data-free distillation method rapidly improves FID and CLIP scores, achieving state-of-the-art FID performance while maintaining a competitive CLIP score. Notably, the one-step distillation of Stable Diffusion 1.5 achieves an FID of 8.15 on the COCO-2014 validation set, a record low value under the data-free setting. Our code and checkpoints are available at https://github.com/mingyuanzhou/SiD-LSG. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.12120</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.12120</id><created>2024-06-17</created><updated>2025-02-06</updated><authors><author><keyname>Zhao</keyname><forenames>Yulai</forenames></author><author><keyname>Uehara</keyname><forenames>Masatoshi</forenames></author><author><keyname>Scalia</keyname><forenames>Gabriele</forenames></author><author><keyname>Kung</keyname><forenames>Sunyuan</forenames></author><author><keyname>Biancalani</keyname><forenames>Tommaso</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Hajiramezanali</keyname><forenames>Ehsan</forenames></author></authors><title>Adding Conditional Control to Diffusion Models with Reinforcement   Learning</title><categories>cs.LG cs.AI stat.ML</categories><comments>ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diffusion models are powerful generative models that allow for precise control over the characteristics of the generated samples. While these diffusion models trained on large datasets have achieved success, there is often a need to introduce additional controls in downstream fine-tuning processes, treating these powerful models as pre-trained diffusion models. This work presents a novel method based on reinforcement learning (RL) to add such controls using an offline dataset comprising inputs and labels. We formulate this task as an RL problem, with the classifier learned from the offline dataset and the KL divergence against pre-trained models serving as the reward functions. Our method, $\textbf{CTRL}$ ($\textbf{C}$onditioning pre-$\textbf{T}$rained diffusion models with $\textbf{R}$einforcement $\textbf{L}$earning), produces soft-optimal policies that maximize the abovementioned reward functions. We formally demonstrate that our method enables sampling from the conditional distribution with additional controls during inference. Our RL-based approach offers several advantages over existing methods. Compared to classifier-free guidance, it improves sample efficiency and can greatly simplify dataset construction by leveraging conditional independence between the inputs and additional controls. Additionally, unlike classifier guidance, it eliminates the need to train classifiers from intermediate states to additional controls. The code is available at https://github.com/zhaoyl18/CTRL. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.14302</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.14302</id><created>2024-06-20</created><updated>2025-02-08</updated><authors><author><keyname>Reizinger</keyname><forenames>Patrik</forenames></author><author><keyname>Guo</keyname><forenames>Siyuan</forenames></author><author><keyname>Huszár</keyname><forenames>Ferenc</forenames></author><author><keyname>Schölkopf</keyname><forenames>Bernhard</forenames></author><author><keyname>Brendel</keyname><forenames>Wieland</forenames></author></authors><title>Identifiable Exchangeable Mechanisms for Causal Structure and   Representation Learning</title><categories>stat.ML cs.AI cs.LG</categories><comments>ICLR2025 camera ready</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Identifying latent representations or causal structures is important for good generalization and downstream task performance. However, both fields have been developed rather independently. We observe that several methods in both representation and causal structure learning rely on the same data-generating process (DGP), namely, exchangeable but not i.i.d. (independent and identically distributed) data. We provide a unified framework, termed Identifiable Exchangeable Mechanisms (IEM), for representation and structure learning under the lens of exchangeability. IEM provides new insights that let us relax the necessary conditions for causal structure identification in exchangeable non--i.i.d. data. We also demonstrate the existence of a duality condition in identifiable representation learning, leading to new identifiability results. We hope this work will pave the way for further research in causal representation learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.14535</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.14535</id><created>2024-06-20</created><updated>2025-02-06</updated><authors><author><keyname>Deng</keyname><forenames>Shiyuan</forenames></author><author><keyname>Tang</keyname><forenames>He</forenames></author><author><keyname>Bai</keyname><forenames>Shuyang</forenames></author></authors><title>On estimation and order selection for multivariate extremes via   clustering</title><categories>stat.ME math.ST stat.TH</categories><msc-class>62G32 (Primary), 60G70 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the estimation of multivariate extreme models with a discrete spectral measure using spherical clustering techniques. The primary contribution involves devising a method for selecting the order, that is, the number of clusters. The method consistently identifies the true order, i.e., the number of spectral atoms, and enjoys intuitive implementation in practice. Specifically, we introduce an extra penalty term to the well-known simplified average silhouette width, which penalizes small cluster sizes and small dissimilarities between cluster centers. Consequently, we provide a consistent method for determining the order of a max-linear factor model, where a typical information-based approach is not viable. Our second contribution is a large-deviation-type analysis for estimating the discrete spectral measure through clustering methods, which serves as an assessment of the convergence quality of clustering-based estimation for multivariate extremes. Additionally, as a third contribution, we discuss how estimating the discrete measure can lead to parameter estimations of heavy-tailed factor models. We also present simulations and real-data studies that demonstrate order selection and factor model estimation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.19903</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.19903</id><created>2024-06-28</created><updated>2025-02-10</updated><authors><author><keyname>Goold</keyname><forenames>Conor</forenames></author></authors><title>Joint estimation of insurance loss development factors using Bayesian   hidden Markov models</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Loss development modelling is the actuarial practice of predicting the total 'ultimate' losses incurred on a set of policies once all claims are reported and settled. This poses a challenging prediction task as losses frequently take years to fully emerge from reported claims, and not all claims might yet be reported. Loss development models frequently estimate a set of 'link ratios' from insurance loss triangles, which are multiplicative factors transforming losses at one time point to ultimate. However, link ratios estimated using classical methods typically underestimate ultimate losses and cannot be extrapolated outside the domains of the triangle, requiring extension by 'tail factors' from another model. Although flexible, this two-step process relies on subjective decision points that might bias inference. Methods that jointly estimate 'body' link ratios and smooth tail factors offer an attractive alternative. This paper proposes a novel application of Bayesian hidden Markov models to loss development modelling, where discrete, latent states representing body and tail processes are automatically learned from the data. The hidden Markov development model is found to perform comparably to, and frequently better than, the two-step approach, as well as a latent change-point model, on numerical examples and industry datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.03133</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.03133</id><created>2024-05-24</created><updated>2025-02-08</updated><authors><author><keyname>Yuan</keyname><forenames>Yingfang</forenames></author><author><keyname>Chen</keyname><forenames>Kefan</forenames></author><author><keyname>Rizvi</keyname><forenames>Mehdi</forenames></author><author><keyname>Baillie</keyname><forenames>Lynne</forenames></author><author><keyname>Pang</keyname><forenames>Wei</forenames></author></authors><title>Quantifying the Cross-sectoral Intersecting Discrepancies within   Multiple Groups Using Latent Class Analysis Towards Fairness</title><categories>cs.CY cs.AI cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The growing interest in fair AI development is evident. The ''Leave No One Behind'' initiative urges us to address multiple and intersecting forms of inequality in accessing services, resources, and opportunities, emphasising the significance of fairness in AI. This is particularly relevant as an increasing number of AI tools are applied to decision-making processes, such as resource allocation and service scheme development, across various sectors such as health, energy, and housing. Therefore, exploring joint inequalities in these sectors is significant and valuable for thoroughly understanding overall inequality and unfairness. This research introduces an innovative approach to quantify cross-sectoral intersecting discrepancies among user-defined groups using latent class analysis. These discrepancies can be used to approximate inequality and provide valuable insights to fairness issues. We validate our approach using both proprietary and public datasets, including both EVENS and Census 2021 (England &amp; Wales) datasets, to examine cross-sectoral intersecting discrepancies among different ethnic groups. We also verify the reliability of the quantified discrepancy by conducting a correlation analysis with a government public metric. Our findings reveal significant discrepancies both among minority ethnic groups and between minority ethnic groups and non-minority ethnic groups, emphasising the need for targeted interventions in policy-making processes. Furthermore, we demonstrate how the proposed approach can provide valuable insights into ensuring fairness in machine learning systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.05237</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.05237</id><created>2024-07-06</created><updated>2025-02-10</updated><authors><author><keyname>Kong</keyname><forenames>Weiwei</forenames></author><author><keyname>Ribero</keyname><forenames>Mónica</forenames></author></authors><title>Privacy of the last iterate in cyclically-sampled DP-SGD on nonconvex   composite losses</title><categories>cs.LG cs.CR cs.DS math.OC stat.ML</categories><msc-class>65K10 (Primary), 60G15, 68P27</msc-class><acm-class>G.3; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differentially-private stochastic gradient descent (DP-SGD) is a family of iterative machine learning training algorithms that privatize gradients to generate a sequence of differentially-private (DP) model parameters. It is also the standard tool used to train DP models in practice, even though most users are only interested in protecting the privacy of the final model. Tight DP accounting for the last iterate would minimize the amount of noise required while maintaining the same privacy guarantee and potentially increasing model utility. However, last-iterate accounting is challenging, and existing works require strong assumptions not satisfied by most implementations. These include assuming (i) the global sensitivity constant is known - to avoid gradient clipping; (ii) the loss function is Lipschitz or convex; and (iii) input batches are sampled randomly.   In this work, we forego any unrealistic assumptions and provide privacy bounds for the most commonly used variant of DP-SGD, in which data is traversed cyclically, gradients are clipped, and only the last model is released. More specifically, we establish new Renyi differential privacy (RDP) upper bounds for the last iterate under realistic assumptions of small stepsize and Lipschitz smoothness of the loss function. Our general bounds also recover the special-case convex bounds when the weak-convexity parameter of the objective function approaches zero and no clipping is performed. The approach itself leverages optimal transport techniques for last iterate bounds, which is a nontrivial task when the data is traversed cyclically and the loss function is nonconvex. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.06120</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.06120</id><created>2024-07-08</created><updated>2024-11-16</updated><authors><author><keyname>Dong</keyname><forenames>Yijun</forenames></author><author><keyname>Phan</keyname><forenames>Hoang</forenames></author><author><keyname>Pan</keyname><forenames>Xiang</forenames></author><author><keyname>Lei</keyname><forenames>Qi</forenames></author></authors><title>Sketchy Moment Matching: Toward Fast and Provable Data Selection for   Finetuning</title><categories>cs.LG cs.NA math.NA stat.ML</categories><comments>NeurIPS 2024</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit data selection in a modern context of finetuning from a fundamental perspective. Extending the classical wisdom of variance minimization in low dimensions to high-dimensional finetuning, our generalization analysis unveils the importance of additionally reducing bias induced by low-rank approximation. Inspired by the variance-bias tradeoff in high dimensions from the theory, we introduce Sketchy Moment Matching (SkMM), a scalable data selection scheme with two stages. (i) First, the bias is controlled using gradient sketching that explores the finetuning parameter space for an informative low-dimensional subspace $\mathcal{S}$; (ii) then the variance is reduced over $\mathcal{S}$ via moment matching between the original and selected datasets. Theoretically, we show that gradient sketching is fast and provably accurate: selecting $n$ samples by reducing variance over $\mathcal{S}$ preserves the fast-rate generalization $O(\dim(\mathcal{S})/n)$, independent of the parameter dimension. Empirically, we concretize the variance-bias balance via synthetic experiments and demonstrate the effectiveness of SkMM for finetuning in real vision tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.10207</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.10207</id><created>2024-07-14</created><updated>2025-02-08</updated><authors><author><keyname>Huang</keyname><forenames>Jiawei</forenames></author><author><keyname>Thoma</keyname><forenames>Vinzenz</forenames></author><author><keyname>Shen</keyname><forenames>Zebang</forenames></author><author><keyname>Nax</keyname><forenames>Heinrich H.</forenames></author><author><keyname>He</keyname><forenames>Niao</forenames></author></authors><title>Learning to Steer Markovian Agents under Model Uncertainty</title><categories>cs.LG cs.AI cs.MA stat.ML</categories><comments>35 Pages; ICLR 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing incentives for an adapting population is a ubiquitous problem in a wide array of economic applications and beyond. In this work, we study how to design additional rewards to steer multi-agent systems towards desired policies \emph{without} prior knowledge of the agents' underlying learning dynamics. Motivated by the limitation of existing works, we consider a new and general category of learning dynamics called \emph{Markovian agents}. We introduce a model-based non-episodic Reinforcement Learning (RL) formulation for our steering problem. Importantly, we focus on learning a \emph{history-dependent} steering strategy to handle the inherent model uncertainty about the agents' learning dynamics. We introduce a novel objective function to encode the desiderata of achieving a good steering outcome with reasonable cost. Theoretically, we identify conditions for the existence of steering strategies to guide agents to the desired policies. Complementing our theoretical contributions, we provide empirical algorithms to approximately solve our objective, which effectively tackles the challenge in learning history-dependent strategies. We demonstrate the efficacy of our algorithms through empirical evaluations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.11762</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.11762</id><created>2024-07-16</created><updated>2025-02-10</updated><authors><author><keyname>Egger</keyname><forenames>Maximilian</forenames></author><author><keyname>Bitar</keyname><forenames>Rawad</forenames></author><author><keyname>Ayache</keyname><forenames>Ghadir</forenames></author><author><keyname>Wachter-Zeh</keyname><forenames>Antonia</forenames></author><author><keyname>Rouayheb</keyname><forenames>Salim El</forenames></author></authors><title>Self-Regulating Random Walks for Resilient Decentralized Learning on   Graphs</title><categories>cs.LG cs.DC cs.IT math.IT stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the setting of multiple random walks (RWs) on a graph executing a certain computational task. For instance, in decentralized learning via RWs, a model is updated at each iteration based on the local data of the visited node and then passed to a randomly chosen neighbor. RWs can fail due to node or link failures. The goal is to maintain a desired number of RWs to ensure failure resilience. Achieving this is challenging due to the lack of a central entity to track which RWs have failed to replace them with new ones by forking (duplicating) surviving ones. Without duplications, the number of RWs will eventually go to zero, causing a catastrophic failure of the system. We propose two decentralized algorithms called DecAFork and DecAFork+ that can maintain the number of RWs in the graph around a desired value even in the presence of arbitrary RW failures. Nodes continuously estimate the number of surviving RWs by estimating their return time distribution and fork the RWs when failures are likely to happen. DecAFork+ additionally allows terminations to avoid overloading the network by forking too many RWs. We present extensive numerical simulations that show the performance of DecAFork and DecAFork+ regarding fast detection and reaction to failures compared to a baseline, and establish theoretical guarantees on the performance of both algorithms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.16975</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.16975</id><created>2024-07-23</created><updated>2025-02-10</updated><authors><author><keyname>Dong</keyname><forenames>Xinshuai</forenames></author><author><keyname>Ng</keyname><forenames>Ignavier</forenames></author><author><keyname>Huang</keyname><forenames>Biwei</forenames></author><author><keyname>Sun</keyname><forenames>Yuewen</forenames></author><author><keyname>Jin</keyname><forenames>Songyao</forenames></author><author><keyname>Legaspi</keyname><forenames>Roberto</forenames></author><author><keyname>Spirtes</keyname><forenames>Peter</forenames></author><author><keyname>Zhang</keyname><forenames>Kun</forenames></author></authors><title>On the Parameter Identifiability of Partially Observed Linear Causal   Models</title><categories>cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear causal models are important tools for modeling causal dependencies and yet in practice, only a subset of the variables can be observed. In this paper, we examine the parameter identifiability of these models by investigating whether the edge coefficients can be recovered given the causal structure and partially observed data. Our setting is more general than that of prior research - we allow all variables, including both observed and latent ones, to be flexibly related, and we consider the coefficients of all edges, whereas most existing works focus only on the edges between observed variables. Theoretically, we identify three types of indeterminacy for the parameters in partially observed linear causal models. We then provide graphical conditions that are sufficient for all parameters to be identifiable and show that some of them are provably necessary. Methodologically, we propose a novel likelihood-based parameter estimation method that addresses the variance indeterminacy of latent variables in a specific way and can asymptotically recover the underlying parameters up to trivial indeterminacy. Empirical studies on both synthetic and real-world datasets validate our identifiability theory and the effectiveness of the proposed method in the finite-sample regime. Code: https://github.com/dongxinshuai/scm-identify. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.18609</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.18609</id><created>2024-07-26</created><updated>2025-02-10</updated><authors><author><keyname>Shariatian</keyname><forenames>Dario</forenames></author><author><keyname>Simsekli</keyname><forenames>Umut</forenames></author><author><keyname>Durmus</keyname><forenames>Alain</forenames></author></authors><title>Denoising L\'evy Probabilistic Models</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Exploring noise distributions beyond Gaussian in diffusion models remains an open challenge. While Gaussian-based models succeed within a unified SDE framework, recent studies suggest that heavy-tailed noise distributions, like $\alpha$-stable distributions, may better handle mode collapse and effectively manage datasets exhibiting class imbalance, heavy tails, or prominent outliers. Recently, Yoon et al.\ (NeurIPS 2023), presented the L\'evy-It\^o model (LIM), directly extending the SDE-based framework to a class of heavy-tailed SDEs, where the injected noise followed an $\alpha$-stable distribution, a rich class of heavy-tailed distributions. However, the LIM framework relies on highly involved mathematical techniques with limited flexibility, potentially hindering broader adoption and further development. In this study, instead of starting from the SDE formulation, we extend the denoising diffusion probabilistic model (DDPM) by replacing the Gaussian noise with $\alpha$-stable noise. By using only elementary proof techniques, the proposed approach, Denoising L\'evy Probabilistic Models (DLPM), boils down to vanilla DDPM with minor modifications. As opposed to the Gaussian case, DLPM and LIM yield different training algorithms and different backward processes, leading to distinct sampling algorithms. These fundamental differences translate favorably for DLPM as compared to LIM: our experiments show improvements in coverage of data distribution tails, better robustness to unbalanced datasets, and improved computation times requiring smaller number of backward steps. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.21314</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.21314</id><created>2024-07-30</created><updated>2025-02-07</updated><authors><author><keyname>Li</keyname><forenames>Zhuoyuan</forenames></author><author><keyname>Dong</keyname><forenames>Bin</forenames></author><author><keyname>Zhang</keyname><forenames>Pingwen</forenames></author></authors><title>State-observation augmented diffusion model for nonlinear assimilation   with unknown dynamics</title><categories>cs.LG stat.ML</categories><msc-class>49N45, 60J60, 62F15, 68T20</msc-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Data assimilation has become a key technique for combining physical models with observational data to estimate state variables. However, classical assimilation algorithms often struggle with the high nonlinearity present in both physical and observational models. To address this challenge, a novel generative model, termed the State-Observation Augmented Diffusion (SOAD) model is proposed for data-driven assimilation. The marginal posterior associated with SOAD has been derived and then proved to match the true posterior distribution under mild assumptions, suggesting its theoretical advantages over previous score-based approaches. Experimental results also indicate that SOAD may offer improved performance compared to existing data-driven methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.01022</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.01022</id><created>2024-08-02</created><updated>2025-02-10</updated><authors><author><keyname>Kawashima</keyname><forenames>Takahiro</forenames></author><author><keyname>Hino</keyname><forenames>Hideitsu</forenames></author></authors><title>A Family of Distributions of Random Subsets for Controlling Positive and   Negative Dependence</title><categories>stat.ML cs.LG</categories><comments>Accepted by AISTATS2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Positive and negative dependence are fundamental concepts that characterize the attractive and repulsive behavior of random subsets. Although some probabilistic models are known to exhibit positive or negative dependence, it is challenging to seamlessly bridge them with a practicable probabilistic model. In this study, we introduce a new family of distributions, named the discrete kernel point process (DKPP), which includes determinantal point processes and parts of Boltzmann machines. We also develop some computational methods for probabilistic operations and inference with DKPPs, such as calculating marginal and conditional probabilities and learning the parameters. Our numerical experiments demonstrate the controllability of positive and negative dependence and the effectiveness of the computational methods for DKPPs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.03733</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.03733</id><created>2024-08-07</created><authors><author><keyname>Maillard</keyname><forenames>Antoine</forenames></author><author><keyname>Troiani</keyname><forenames>Emanuele</forenames></author><author><keyname>Martin</keyname><forenames>Simon</forenames></author><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>Zdeborová</keyname><forenames>Lenka</forenames></author></authors><title>Bayes-optimal learning of an extensive-width neural network from   quadratically many samples</title><categories>stat.ML cond-mat.dis-nn cs.IT cs.LG math.IT math.PR</categories><comments>47 pages</comments><journal-ref>Advances in Neural Information Processing Systems 37 (NeurIPS   2024)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning a target function corresponding to a single hidden layer neural network, with a quadratic activation function after the first layer, and random weights. We consider the asymptotic limit where the input dimension and the network width are proportionally large. Recent work [Cui &amp; al '23] established that linear regression provides Bayes-optimal test error to learn such a function when the number of available samples is only linear in the dimension. That work stressed the open challenge of theoretically analyzing the optimal test error in the more interesting regime where the number of samples is quadratic in the dimension. In this paper, we solve this challenge for quadratic activations and derive a closed-form expression for the Bayes-optimal test error. We also provide an algorithm, that we call GAMP-RIE, which combines approximate message passing with rotationally invariant matrix denoising, and that asymptotically achieves the optimal performance. Technically, our result is enabled by establishing a link with recent works on optimal denoising of extensive-rank matrices and on the ellipsoid fitting problem. We further show empirically that, in the absence of noise, randomly-initialized gradient descent seems to sample the space of weights, leading to zero training loss, and averaging over initialization leads to a test error equal to the Bayes-optimal one. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.09004</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.09004</id><created>2024-08-16</created><updated>2025-02-06</updated><authors><author><keyname>Subedi</keyname><forenames>Unique</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Controlling Statistical, Discretization, and Truncation Errors in   Learning Fourier Linear Operators</title><categories>stat.ML cs.LG cs.NA math.NA</categories><comments>Added Experiments</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study learning-theoretic foundations of operator learning, using the linear layer of the Fourier Neural Operator architecture as a model problem. First, we identify three main errors that occur during the learning process: statistical error due to finite sample size, truncation error from finite rank approximation of the operator, and discretization error from handling functional data on a finite grid of domain points. Finally, we analyze a Discrete Fourier Transform (DFT) based least squares estimator, establishing both upper and lower bounds on the aforementioned errors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.09576</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.09576</id><created>2024-08-18</created><updated>2025-02-07</updated><authors><author><keyname>Oubari</keyname><forenames>Fouad</forenames></author><author><keyname>Baha</keyname><forenames>Mohamed El</forenames></author><author><keyname>Meunier</keyname><forenames>Raphael</forenames></author><author><keyname>Décatoire</keyname><forenames>Rodrigue</forenames></author><author><keyname>Mougeot</keyname><forenames>Mathilde</forenames></author></authors><title>A Markov Random Field Multi-Modal Variational AutoEncoder</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Recent advancements in multimodal Variational AutoEncoders (VAEs) have highlighted their potential for modeling complex data from multiple modalities. However, many existing approaches use relatively straightforward aggregating schemes that may not fully capture the complex dynamics present between different modalities. This work introduces a novel multimodal VAE that incorporates a Markov Random Field (MRF) into both the prior and posterior distributions. This integration aims to capture complex intermodal interactions more effectively. Unlike previous models, our approach is specifically designed to model and leverage the intricacies of these relationships, enabling a more faithful representation of multimodal data. Our experiments demonstrate that our model performs competitively on the standard PolyMNIST dataset and shows superior performance in managing complex intermodal dependencies in a specially designed synthetic dataset, intended to test intricate relationships. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.14618</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.14618</id><created>2024-08-26</created><updated>2025-02-10</updated><authors><author><keyname>Deitmar</keyname><forenames>Ben</forenames></author></authors><title>Marchenko-Pastur law for Daniell smoothed periodograms without   simultaneous diagonalizability</title><categories>math.ST stat.TH</categories><comments>57 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The eigenvectors of a spectral density matrix $F(\theta)$ to a stationary Gaussian process $(X_t)_{t \in \mathbb{Z}}$ depend explicitly on the frequency $\theta \in [0,2\pi]$. The most commonly used estimator of the spectral density matrix $F(\theta)$ is the Daniell smoothed periodogram, which takes the form $BB^*$ for random matrices $B$ with non-zero covariance between rows and columns. When the covariance matrices of the columns are not simultaneously diagonalizable, this covariance structure is non-separable and such matrices $BB^*$ are out of reach for the current state of random matrix theory. In this paper, we derive a Marchenko-Pastur law in this non-simultaneously diagonalizable case. The Marchenko-Pastur law emerges when the dimension $d$ of the process and the smoothing span $m$ of the smoothed periodogram grow at the same rate, which is slower than the number of observations $n$.   On the technical level we prove a trace moment bound for matrices $YY^T$, where $Y$ is a matrix with correlated Gaussian entries. This allows for sub-polynomial error bounds in settings where the error $Y$ has correlations between different points in time as well as between features.   In the regime $d \asymp m \asymp n^{\alpha}$ for $\alpha \in (\frac{1}{2},1)$ we expand our result to the non-Gaussian case. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.16429</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.16429</id><created>2024-08-29</created><updated>2025-02-10</updated><authors><author><keyname>Heins</keyname><forenames>Conor</forenames></author><author><keyname>Wu</keyname><forenames>Hao</forenames></author><author><keyname>Markovic</keyname><forenames>Dimitrije</forenames></author><author><keyname>Tschantz</keyname><forenames>Alexander</forenames></author><author><keyname>Beck</keyname><forenames>Jeff</forenames></author><author><keyname>Buckley</keyname><forenames>Christopher</forenames></author></authors><title>Gradient-free variational learning with conditional mixture networks</title><categories>cs.LG cs.AI stat.ML</categories><comments>16 pages main text (3 figures), including references. 9 pages   supplementary material (5 figures). Accepted at NeurIPS Bayesian Decision   Making and Uncertainty Workshop (2024): https://neurips.cc/virtual/2024/98879</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Balancing computational efficiency with robust predictive performance is crucial in supervised learning, especially for critical applications. Standard deep learning models, while accurate and scalable, often lack probabilistic features like calibrated predictions and uncertainty quantification. Bayesian methods address these issues but can be computationally expensive as model and data complexity increase. Previous work shows that fast variational methods can reduce the compute requirements of Bayesian methods by eliminating the need for gradient computation or sampling, but are often limited to simple models. We introduce CAVI-CMN, a fast, gradient-free variational method for training conditional mixture networks (CMNs), a probabilistic variant of the mixture-of-experts (MoE) model. CMNs are composed of linear experts and a softmax gating network. By exploiting conditional conjugacy and P\'olya-Gamma augmentation, we furnish Gaussian likelihoods for the weights of both the linear layers and the gating network. This enables efficient variational updates using coordinate ascent variational inference (CAVI), avoiding traditional gradient-based optimization. We validate this approach by training two-layer CMNs on standard classification benchmarks from the UCI repository. CAVI-CMN achieves competitive and often superior predictive accuracy compared to maximum likelihood estimation (MLE) with backpropagation, while maintaining competitive runtime and full posterior distributions over all model parameters. Moreover, as input size or the number of experts increases, computation time scales competitively with MLE and other gradient-based solutions like black-box variational inference (BBVI), making CAVI-CMN a promising tool for deep, fast, and gradient-free Bayesian networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.02363</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.02363</id><created>2024-09-03</created><updated>2025-02-06</updated><authors><author><keyname>Maiti</keyname><forenames>Ayan</forenames></author><author><keyname>Michelle</keyname><forenames>Michelle</forenames></author><author><keyname>Yang</keyname><forenames>Haizhao</forenames></author></authors><title>Optimal Neural Network Approximation for High-Dimensional Continuous   Functions</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the authors of Shen Yang Zhang (JMLR, 2022) developed a neural network with width $36d(2d + 1)$ and depth $11$, which utilizes a special activation function called the elementary universal activation function, to achieve the super approximation property for functions in $C([a,b]^d)$. That is, the constructed network only requires a fixed number of neurons (and thus parameters) to approximate a $d$-variate continuous function on a $d$-dimensional hypercube with arbitrary accuracy. More specifically, only $\mathcal{O}(d^2)$ neurons or parameters are used. One natural question is whether we can reduce the number of these neurons or parameters in such a network. By leveraging a variant of the Kolmogorov Superposition Theorem, our analysis shows that there is a neural network generated by the elementary universal activation function with at most $10889d+10887$ unique nonzero parameters such that this super approximation property is attained. Furthermore, we present a family of continuous functions that requires at least width $d$, and thus at least $d$ neurons or parameters, to achieve arbitrary accuracy in its approximation. This suggests that the number of unique nonzero parameters is optimal in the sense that it grows linearly with the input dimension $d$, unlike some approximation methods where parameters may grow exponentially with $d$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.07263</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.07263</id><created>2024-09-11</created><updated>2025-02-07</updated><authors><author><keyname>Lastra</keyname><forenames>Katerine Zuniga</forenames></author><author><keyname>Pumi</keyname><forenames>Guilherme</forenames></author><author><keyname>Prass</keyname><forenames>Taiane Schaedler</forenames></author></authors><title>Order selection in GARMA models for count time series: a Bayesian   perspective</title><categories>stat.ME</categories><msc-class>62M10, 62F15, 62J02, 62F10</msc-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Estimation in GARMA models has traditionally been carried out under the frequentist approach. To date, Bayesian approaches for such estimation have been relatively limited. In the context of GARMA models for count time series, Bayesian estimation achieves satisfactory results in terms of point estimation. Model selection in this context often relies on the use of information criteria. Despite its prominence in the literature, the use of information criteria for model selection in GARMA models for count time series have been shown to present poor performance in simulations, especially in terms of their ability to correctly identify models, even under large sample sizes. In this study, we study the problem of order selection in GARMA models for count time series, adopting a Bayesian perspective through the application of the Reversible Jump Markov Chain Monte Carlo approach. Monte Carlo simulation studies are conducted to assess the finite sample performance of the developed ideas, including point and interval inference, sensitivity analysis, effects of burn-in and thinning, as well as the choice of related priors and hyperparameters. Two real-data applications are presented, one considering automobile production in Brazil and the other considering bus exportation in Brazil before and after the COVID-19 pandemic, showcasing the method's capabilities and further exploring its flexibility. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.09800</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.09800</id><created>2024-09-15</created><updated>2025-02-06</updated><authors><author><keyname>Calvello</keyname><forenames>Edoardo</forenames></author><author><keyname>Monmarché</keyname><forenames>Pierre</forenames></author><author><keyname>Stuart</keyname><forenames>Andrew M.</forenames></author><author><keyname>Vaes</keyname><forenames>Urbain</forenames></author></authors><title>Accuracy of the Ensemble Kalman Filter in the Near-Linear Setting</title><categories>math.ST cs.NA math.DS math.NA math.OC stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The filtering distribution captures the statistics of the state of a dynamical system from partial and noisy observations. Classical particle filters provably approximate this distribution in quite general settings; however they behave poorly for high dimensional problems, suffering weight collapse. This issue is circumvented by the ensemble Kalman filter which is an equal-weight interacting particle system. However, this finite particle system is only proven to approximate the true filter in the linear Gaussian case. In practice, however, it is applied in much broader settings; as a result, establishing its approximation properties more generally is important. There has been recent progress in the theoretical analysis of the algorithm, establishing stability and error estimates in non-Gaussian settings, but the assumptions on the dynamics and observation models rule out the unbounded vector fields that arise in practice and the analysis applies only to the mean field limit of the ensemble Kalman filter. The present work establishes error bounds between the filtering distribution and the finite particle ensemble Kalman filter when the dynamics and observation vector fields may be unbounded, allowing linear growth. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.11381</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.11381</id><created>2024-09-17</created><updated>2025-02-07</updated><authors><author><keyname>Banerjee</keyname><forenames>Debapratim</forenames></author><author><keyname>Mukherjee</keyname><forenames>Soumendu Sundar</forenames></author><author><keyname>Pal</keyname><forenames>Dipranjan</forenames></author></authors><title>Edge spectra of Gaussian random symmetric matrices with correlated   entries</title><categories>math.PR math-ph math.CO math.MP math.ST stat.TH</categories><comments>27 pages, 2 figures; abstract shortened to meet arXiv requirements</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the largest eigenvalue of a Gaussian random symmetric matrix $X_n$, with zero-mean, unit variance entries satisfying the condition $\sup_{(i, j) \ne (i', j')}|\mathbb{E}[X_{ij} X_{i'j'}]| = O(n^{-(1 + \varepsilon)})$, where $\varepsilon &gt; 0$. It follows from Catalano et al. (2024) that the empirical spectral distribution of $n^{-1/2} X_n$ converges weakly almost surely to the standard semi-circle law. Using a F\"{u}redi-Koml\'{o}s-type high moment analysis, we show that the largest eigenvalue $\lambda_1(n^{-1/2} X_n)$ of $n^{-1/2} X_n$ converges almost surely to $2$. This result is essentially optimal in the sense that one cannot take $\varepsilon = 0$ and still obtain an almost sure limit of $2$. We also derive Gaussian fluctuation results for the largest eigenvalue in the case where the entries have a common non-zero mean. Let $Y_n = X_n + \frac{\lambda}{\sqrt{n}}\mathbf{1} \mathbf{1}^\top$. When $\varepsilon \ge 1$ and $\lambda \gg n^{1/4}$, we show that \[   n^{1/2}\bigg(\lambda_1(n^{-1/2} Y_n) - \lambda - \frac{1}{\lambda}\bigg) \xrightarrow{d} \sqrt{2} Z, \] where $Z$ is a standard Gaussian. On the other hand, when $0 &lt; \varepsilon &lt; 1$, we have $\mathrm{Var}(\frac{1}{n}\sum_{i, j}X_{ij}) = O(n^{1 - \varepsilon})$. Assuming that $\mathrm{Var}(\frac{1}{n}\sum_{i, j} X_{ij}) = \sigma^2 n^{1 - \varepsilon} (1 + o(1))$, if $\lambda \gg n^{\varepsilon/4}$, then we have \[   n^{\varepsilon/2}\bigg(\lambda_1(n^{-1/2} Y_n) - \lambda - \frac{1}{\lambda}\bigg) \xrightarrow{d} \sigma Z. \] While the ranges of $\lambda$ in these fluctuation results are certainly not optimal, a striking aspect is that different scalings are required in the two regimes $0 &lt; \varepsilon &lt; 1$ and $\varepsilon \ge 1$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.12124</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.12124</id><created>2024-09-18</created><updated>2025-02-10</updated><authors><author><keyname>Zhang</keyname><forenames>Anqi</forenames></author><author><keyname>Geisler</keyname><forenames>Wilson S.</forenames></author></authors><title>Optimal Visual Search with Highly Heuristic Decision Rules</title><categories>q-bio.NC cs.CV stat.AP</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Visual search is a fundamental natural task for humans and other animals. We investigated the decision processes humans use in covert (single-fixation) search with briefly presented displays having well-separated potential target locations. Performance was compared with the Bayesian-optimal decision process under the assumption that the information from the different potential target locations is statistically independent. Surprisingly, humans performed slightly better than optimal, despite humans' substantial loss of sensitivity in the fovea (foveal neglect), and the implausibility of the human brain replicating the optimal computations. We show that three factors can quantitatively explain these seemingly paradoxical results. Most importantly, simple and fixed heuristic decision rules reach near optimal search performance. Secondly, foveal neglect primarily affects only the central potential target location. Finally, spatially correlated neural noise can cause search performance to exceed that predicted for independent noise. These findings have broad implications for understanding visual search tasks and other identification tasks in humans and other animals. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.16843</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.16843</id><created>2024-09-25</created><updated>2025-02-08</updated><authors><author><keyname>Zhong</keyname><forenames>Yiming</forenames></author><author><keyname>Ren</keyname><forenames>Yinuo</forenames></author><author><keyname>Cao</keyname><forenames>Guangyao</forenames></author><author><keyname>Li</keyname><forenames>Feng</forenames></author><author><keyname>Qi</keyname><forenames>Haobo</forenames></author></authors><title>Optimal starting point for time series forecasting</title><categories>stat.AP cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent advances on time series forecasting mainly focus on improving the forecasting models themselves. However, when the time series data suffer from potential structural breaks or concept drifts, the forecasting performance might be significantly reduced. In this paper, we introduce a novel approach called Optimal Starting Point Time Series Forecast (OSP-TSP) for optimal forecasting, which can be combined with existing time series forecasting models. By adjusting the sequence length via leveraging the XGBoost and LightGBM models, the proposed approach can determine the optimal starting point (OSP) of the time series and then enhance the prediction performances of the base forecasting models. To illustrate the effectiveness of the proposed approach, comprehensive empirical analysis have been conducted on the M4 dataset and other real world datasets. Empirical results indicate that predictions based on the OSP-TSP approach consistently outperform those using the complete time series dataset. Moreover, comparison results reveals that combining our approach with existing forecasting models can achieve better prediction accuracy, which also reflect the advantages of the proposed approach. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.19200</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.19200</id><created>2024-09-27</created><updated>2025-02-06</updated><authors><author><keyname>Bai</keyname><forenames>Site</forenames></author><author><keyname>Bullins</keyname><forenames>Brian</forenames></author></authors><title>Faster Acceleration for Steepest Descent</title><categories>math.OC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances (Sherman, 2017; Sidford and Tian, 2018; Cohen et al., 2021) have overcome the fundamental barrier of dimension dependence in the iteration complexity of solving $\ell_\infty$ regression with first-order methods. Yet it remains unclear to what extent such acceleration can be achieved for general $\ell_p$ smooth functions. In this paper, we propose a new accelerated first-order method for convex optimization under non-Euclidean smoothness assumptions. In contrast to standard acceleration techniques, our approach uses primal-dual iterate sequences taken with respect to $\textit{differing}$ norms, which are then coupled using an $\textit{implicitly}$ determined interpolation parameter. For $\ell_p$ norm smooth problems in $d$ dimensions, our method provides an iteration complexity improvement of up to $O(d^{1-\frac{2}{p}})$ in terms of calls to a first-order oracle, thereby allowing us to circumvent long-standing barriers in accelerated non-Euclidean steepest descent. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.00759</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.00759</id><created>2024-10-01</created><updated>2025-02-08</updated><authors><author><keyname>Ferracci</keyname><forenames>Tommaso</forenames></author><author><keyname>Goldmann</keyname><forenames>Leonie Tabea</forenames></author><author><keyname>Hinel</keyname><forenames>Anton</forenames></author><author><keyname>Passino</keyname><forenames>Francesco Sanna</forenames></author></authors><title>Targeted synthetic data generation for tabular data via hardness   characterization</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Data augmentation via synthetic data generation has been shown to be effective in improving model performance and robustness in the context of scarce or low-quality data. Using the data valuation framework to statistically identify beneficial and detrimental observations, we introduce a simple augmentation pipeline that generates only high-value training points based on hardness characterization, in a computationally efficient manner. We first empirically demonstrate via benchmarks on real data that Shapley-based data valuation methods perform comparably with learning-based methods in hardness characterization tasks, while offering significant computational advantages. Then, we show that synthetic data generators trained on the hardest points outperform non-targeted data augmentation on a number of tabular datasets. Our approach improves the quality of out-of-sample predictions and it is computationally more efficient compared to non-targeted methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.03159</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.03159</id><created>2024-10-04</created><updated>2025-02-07</updated><authors><author><keyname>Lu</keyname><forenames>Jiecheng</forenames></author><author><keyname>Han</keyname><forenames>Xu</forenames></author><author><keyname>Sun</keyname><forenames>Yan</forenames></author><author><keyname>Yang</keyname><forenames>Shihao</forenames></author></authors><title>WAVE: Weighted Autoregressive Varing Gate for Time Series Forecasting</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Weighted Autoregressive Varing gatE (WAVE) attention mechanism equipped with both Autoregressive (AR) and Moving-average (MA) components. It can adapt to various attention mechanisms, enhancing and decoupling their ability to capture long-range and local temporal patterns in time series data. In this paper, we first demonstrate that, for the time series forecasting (TSF) task, the previously overlooked decoder-only autoregressive Transformer model can achieve results comparable to the best baselines when appropriate tokenization and training methods are applied. Moreover, inspired by the ARMA model from statistics and recent advances in linear attention, we introduce the full ARMA structure into existing autoregressive attention mechanisms. By using an indirect MA weight generation method, we incorporate the MA term while maintaining the time complexity and parameter size of the underlying efficient attention models. We further explore how indirect parameter generation can produce implicit MA weights that align with the modeling requirements for local temporal impacts. Experimental results show that WAVE attention that incorporates the ARMA structure consistently improves the performance of various AR attentions on TSF tasks, achieving state-of-the-art results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.03833</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.03833</id><created>2024-10-04</created><updated>2025-02-07</updated><authors><author><keyname>Ding</keyname><forenames>Meng</forenames></author><author><keyname>Sharma</keyname><forenames>Rohan</forenames></author><author><keyname>Chen</keyname><forenames>Changyou</forenames></author><author><keyname>Xu</keyname><forenames>Jinhui</forenames></author><author><keyname>Ji</keyname><forenames>Kaiyi</forenames></author></authors><title>Understanding Fine-tuning in Approximate Unlearning: A Theoretical   Perspective</title><categories>cs.LG stat.ML</categories><comments>23 pages,5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine Unlearning has emerged as a significant area of research, focusing on `removing' specific subsets of data from a trained model. Fine-tuning (FT) methods have become one of the fundamental approaches for approximating unlearning, as they effectively retain model performance. However, it is consistently observed that naive FT methods struggle to forget the targeted data. In this paper, we present the first theoretical analysis of FT methods for machine unlearning within a linear regression framework, providing a deeper exploration of this phenomenon. Our analysis reveals that while FT models can achieve zero remaining loss, they fail to forget the forgetting data, as the pretrained model retains its influence and the fine-tuning process does not adequately mitigate it. To address this, we propose a novel Retention-Based Masking (RBM) strategy that constructs a weight saliency map based on the remaining dataset, unlike existing methods that focus on the forgetting dataset. Our theoretical analysis demonstrates that RBM not only significantly improves unlearning accuracy (UA) but also ensures higher retaining accuracy (RA) by preserving overlapping features shared between the forgetting and remaining datasets. Experiments on synthetic and real-world datasets validate our theoretical insights, showing that RBM outperforms existing masking approaches in balancing UA, RA, and disparity metrics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.04120</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.04120</id><created>2024-10-05</created><updated>2025-02-07</updated><authors><author><keyname>Jones</keyname><forenames>Charles</forenames></author><author><keyname>Ribeiro</keyname><forenames>Fabio de Sousa</forenames></author><author><keyname>Roschewitz</keyname><forenames>Mélanie</forenames></author><author><keyname>Castro</keyname><forenames>Daniel C.</forenames></author><author><keyname>Glocker</keyname><forenames>Ben</forenames></author></authors><title>Rethinking Fair Representation Learning for Performance-Sensitive Tasks</title><categories>cs.LG cs.CY stat.ML</categories><comments>Accepted for publication in ICLR 2025:   https://openreview.net/forum?id=pBZntPrdrI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the prominent class of fair representation learning methods for bias mitigation. Using causal reasoning to define and formalise different sources of dataset bias, we reveal important implicit assumptions inherent to these methods. We prove fundamental limitations on fair representation learning when evaluation data is drawn from the same distribution as training data and run experiments across a range of medical modalities to examine the performance of fair representation learning under distribution shifts. Our results explain apparent contradictions in the existing literature and reveal how rarely considered causal and statistical aspects of the underlying data affect the validity of fair representation learning. We raise doubts about current evaluation practices and the applicability of fair representation learning methods in performance-sensitive settings. We argue that fine-grained analysis of dataset biases should play a key role in the field moving forward. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.12936</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.12936</id><created>2024-10-16</created><updated>2025-02-10</updated><authors><author><keyname>Ma</keyname><forenames>Guoxuan</forenames></author><author><keyname>Zhao</keyname><forenames>Lili</forenames></author><author><keyname>Kang</keyname><forenames>Jian</forenames></author></authors><title>Development of COVID-19 Booster Vaccine Policy by Microsimulation and   Q-learning</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The COVID-19 pandemic highlighted the urgent need for effective vaccine policies, but traditional clinical trials often lack sufficient data to capture the diverse population characteristics necessary for comprehensive public health strategies. Ethical concerns around randomized trials during a pandemic further complicate policy development for public health. Reinforcement Learning (RL) offers a promising alternative for vaccine policy development. However, direct online RL exploration in real-world scenarios can result in suboptimal and potentially harmful decisions. This study proposes a novel framework combining tabular Q-learning with microsimulation (i.e., a Recurrent Neural Network (RNN) environment simulator) to address these challenges in public health vaccine policymaking, which enables effective vaccine policy learning without real-world interaction, addressing both ethical and exploration challenges. The RNN environment simulator captures temporal associations between infection and patient characteristics, generating realistic simulation data. Our tabular Q-learning model produces an interpretable policy table that balances the risks of severe infection against vaccination side effects. Applied to COVID-19 booster policies, the learned Q-learning-based policy outperforms current practices, offering a path toward more effective vaccination strategies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.13772</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.13772</id><created>2024-10-17</created><updated>2024-10-20</updated><authors><author><keyname>Gerogiannis</keyname><forenames>Argyrios</forenames></author><author><keyname>Huang</keyname><forenames>Yu-Han</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Is Prior-Free Black-Box Non-Stationary Reinforcement Learning Feasible?</title><categories>cs.LG cs.AI stat.ML</categories><comments>Corrected minor typos in the proof of Theorem 2 on pages 25 and 26</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study the problem of Non-Stationary Reinforcement Learning (NS-RL) without prior knowledge about the system's non-stationarity. A state-of-the-art, black-box algorithm, known as MASTER, is considered, with a focus on identifying the conditions under which it can achieve its stated goals. Specifically, we prove that MASTER's non-stationarity detection mechanism is not triggered for practical choices of horizon, leading to performance akin to a random restarting algorithm. Moreover, we show that the regret bound for MASTER, while being order optimal, stays above the worst-case linear regret until unreasonably large values of the horizon. To validate these observations, MASTER is tested for the special case of piecewise stationary multi-armed bandits, along with methods that employ random restarting, and others that use quickest change detection to restart. A simple, order optimal random restarting algorithm, that has prior knowledge of the non-stationarity is proposed as a baseline. The behavior of the MASTER algorithm is validated in simulations, and it is shown that methods employing quickest change detection are more robust and consistently outperform MASTER and other random restarting approaches. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.14183</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.14183</id><created>2024-10-18</created><updated>2025-02-08</updated><authors><author><keyname>Jin</keyname><forenames>Yanhao</forenames></author><author><keyname>Balasubramanian</keyname><forenames>Krishnakumar</forenames></author><author><keyname>Lai</keyname><forenames>Lifeng</forenames></author></authors><title>In-context Learning for Mixture of Linear Regressions: Existence,   Generalization and Training Dynamics</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the in-context learning capabilities of transformers for the $d$-dimensional mixture of linear regression model, providing theoretical insights into their existence, generalization bounds, and training dynamics. Specifically, we prove that there exists a transformer capable of achieving a prediction error of order $\mathcal{O}(\sqrt{d/n})$ with high probability, where $n$ represents the training prompt size in the high signal-to-noise ratio (SNR) regime. Moreover, we derive in-context excess risk bounds of order $\mathcal{O}(L/\sqrt{B})$ for the case of two mixtures, where $B$ denotes the number of training prompts, and $L$ represents the number of attention layers. The dependence of $L$ on the SNR is explicitly characterized, differing between low and high SNR settings. We further analyze the training dynamics of transformers with single linear self-attention layers, demonstrating that, with appropriately initialized parameters, gradient flow optimization over the population mean square loss converges to a global optimum. Extensive simulations suggest that transformers perform well on this task, potentially outperforming other baselines, such as the Expectation-Maximization algorithm. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.15361</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.15361</id><created>2024-10-20</created><updated>2025-02-06</updated><authors><author><keyname>Zhou</keyname><forenames>Han</forenames></author><author><keyname>Van Landeghem</keyname><forenames>Jordy</forenames></author><author><keyname>Popordanoska</keyname><forenames>Teodora</forenames></author><author><keyname>Blaschko</keyname><forenames>Matthew B.</forenames></author></authors><title>A Novel Characterization of the Population Area Under the Risk Coverage   Curve (AURC) and Rates of Finite Sample Estimators</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The selective classifier (SC) has been proposed for rank based uncertainty thresholding, which could have applications in safety critical areas such as medical diagnostics, autonomous driving, and the justice system. The Area Under the Risk-Coverage Curve (AURC) has emerged as the foremost evaluation metric for assessing the performance of SC systems. In this work, we present a formal statistical formulation of population AURC, presenting an equivalent expression that can be interpreted as a reweighted risk function. Through Monte Carlo methods, we derive empirical AURC plug-in estimators for finite sample scenarios. The weight estimators associated with these plug-in estimators are shown to be consistent, with low bias and tightly bounded mean squared error (MSE). The plug-in estimators are proven to converge at a rate of $\mathcal{O}(\sqrt{\ln(n)/n})$ demonstrating statistical consistency. We empirically validate the effectiveness of our estimators through experiments across multiple datasets, model architectures, and confidence score functions (CSFs), demonstrating consistency and effectiveness in fine-tuning AURC performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.15557</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.15557</id><created>2024-10-20</created><updated>2025-02-09</updated><authors><author><keyname>Li</keyname><forenames>Yining</forenames></author><author><keyname>Ju</keyname><forenames>Peizhong</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author></authors><title>How to Find the Exact Pareto Front for Multi-Objective MDPs?</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multi-Objective Markov Decision Processes (MO-MDPs) are receiving increasing attention, as real-world decision-making problems often involve conflicting objectives that cannot be addressed by a single-objective MDP. The Pareto front identifies the set of policies that cannot be dominated, providing a foundation for finding Pareto optimal solutions that can efficiently adapt to various preferences. However, finding the Pareto front is a highly challenging problem. Most existing methods either (i) rely on traversing the continuous preference space, which is impractical and results in approximations that are difficult to evaluate against the true Pareto front, or (ii) focus solely on deterministic Pareto optimal policies, from which there are no known techniques to characterize the full Pareto front. Moreover, finding the structure of the Pareto front itself remains unclear even in the context of dynamic programming, where the MDP is fully known in advance. In this work, we address the challenge of efficiently discovering the Pareto front. By investigating the geometric structure of the Pareto front in MO-MDPs, we uncover a key property: the Pareto front is on the boundary of a convex polytope whose vertices all correspond to deterministic policies, and neighboring vertices of the Pareto front differ by only one state-action pair of the deterministic policy, almost surely. This insight transforms the global comparison across all policies into a localized search among deterministic policies that differ by only one state-action pair, drastically reducing the complexity of searching for the exact Pareto front. We develop an efficient algorithm that identifies the vertices of the Pareto front by solving a single-objective MDP only once and then traversing the edges of the Pareto front, making it more efficient than existing methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.16692</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.16692</id><created>2024-10-22</created><updated>2025-02-07</updated><authors><author><keyname>Cai</keyname><forenames>Xu</forenames></author><author><keyname>Scarlett</keyname><forenames>Jonathan</forenames></author></authors><title>Lower Bounds for Time-Varying Kernelized Bandits</title><categories>stat.ML cs.IT cs.LG math.IT</categories><comments>AISTATS 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The optimization of black-box functions with noisy observations is a fundamental problem with widespread applications, and has been widely studied under the assumption that the function lies in a reproducing kernel Hilbert space (RKHS). This problem has been studied extensively in the stationary setting, and near-optimal regret bounds are known via developments in both upper and lower bounds. In this paper, we consider non-stationary scenarios, which are crucial for certain applications but are currently less well-understood. Specifically, we provide the first algorithm-independent lower bounds, where the time variations are subject satisfying a total variation budget according to some function norm. Under $\ell_{\infty}$-norm variations, our bounds are found to be close to an existing upper bound (Hong et al., 2023). Under RKHS norm variations, the upper and lower bounds are still reasonably close but with more of a gap, raising the interesting open question of whether non-minor improvements in the upper bound are possible. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.17793</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.17793</id><created>2024-10-23</created><updated>2025-02-08</updated><authors><author><keyname>Puerto</keyname><forenames>Justo</forenames></author><author><keyname>Torrejon</keyname><forenames>Alberto</forenames></author></authors><title>A fresh view on Least Quantile of Squares Regression based on new   optimization approaches</title><categories>stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Regression analysis is an important instrument to determine the effect of the explanatory variables on response variables. When outliers and bias errors are present, the standard weighted least squares estimator may perform poorly. For this reason, many alternative robust techniques have been studied in literature. In these terms, the Least Squares Quantile (LQS), and in particular the Least Squares Median, are among the regression estimators that exhibit better robustness properties. However, the accurate computation of this estimators is computationally demanding, resulting in a difficult estimator to obtain. In this paper, new novel approaches to compute a global optimal solution for the LQS estimator based on single-level and bilevel optimization methods are proposed. An extensive computational study is provided to support the efficiency of the methods considered, and an ad hoc procedure to address the scalability of the problem to larger instances is proposed. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.17998</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.17998</id><created>2024-10-23</created><updated>2025-02-08</updated><authors><author><keyname>Chun</keyname><forenames>Chanwoo</forenames></author><author><keyname>Chung</keyname><forenames>SueYeon</forenames></author><author><keyname>Lee</keyname><forenames>Daniel D.</forenames></author></authors><title>Estimating the Spectral Moments of the Kernel Integral Operator from   Finite Sample Matrices</title><categories>cs.LG math.SP math.ST stat.ML stat.TH</categories><comments>Accepted for publication in the Proceedings of the 28th International   Conference on Artificial Intelligence and Statistics (AISTATS) 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analyzing the structure of sampled features from an input data distribution is challenging when constrained by limited measurements in both the number of inputs and features. Traditional approaches often rely on the eigenvalue spectrum of the sample covariance matrix derived from finite measurement matrices; however, these spectra are sensitive to the size of the measurement matrix, leading to biased insights. In this paper, we introduce a novel algorithm that provides unbiased estimates of the spectral moments of the kernel integral operator in the limit of infinite inputs and features from finitely sampled measurement matrices. Our method, based on dynamic programming, is efficient and capable of estimating the moments of the operator spectrum. We demonstrate the accuracy of our estimator on radial basis function (RBF) kernels, highlighting its consistency with the theoretical spectra. Furthermore, we showcase the practical utility and robustness of our method in understanding the geometry of learned representations in neural networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.18959</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.18959</id><created>2024-10-24</created><updated>2025-02-06</updated><authors><author><keyname>Williams</keyname><forenames>Andrew Robert</forenames></author><author><keyname>Ashok</keyname><forenames>Arjun</forenames></author><author><keyname>Marcotte</keyname><forenames>Étienne</forenames></author><author><keyname>Zantedeschi</keyname><forenames>Valentina</forenames></author><author><keyname>Subramanian</keyname><forenames>Jithendaraa</forenames></author><author><keyname>Riachi</keyname><forenames>Roland</forenames></author><author><keyname>Requeima</keyname><forenames>James</forenames></author><author><keyname>Lacoste</keyname><forenames>Alexandre</forenames></author><author><keyname>Rish</keyname><forenames>Irina</forenames></author><author><keyname>Chapados</keyname><forenames>Nicolas</forenames></author><author><keyname>Drouin</keyname><forenames>Alexandre</forenames></author></authors><title>Context is Key: A Benchmark for Forecasting with Essential Textual   Information</title><categories>cs.LG cs.AI stat.ML</categories><comments>Preprint; under review. First two authors contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Forecasting is a critical task in decision-making across numerous domains. While historical numerical data provide a start, they fail to convey the complete context for reliable and accurate predictions. Human forecasters frequently rely on additional information, such as background knowledge and constraints, which can efficiently be communicated through natural language. However, in spite of recent progress with LLM-based forecasters, their ability to effectively integrate this textual information remains an open question. To address this, we introduce "Context is Key" (CiK), a time-series forecasting benchmark that pairs numerical data with diverse types of carefully crafted textual context, requiring models to integrate both modalities; crucially, every task in CiK requires understanding textual context to be solved successfully. We evaluate a range of approaches, including statistical models, time series foundation models, and LLM-based forecasters, and propose a simple yet effective LLM prompting method that outperforms all other tested methods on our benchmark. Our experiments highlight the importance of incorporating contextual information, demonstrate surprising performance when using LLM-based forecasting models, and also reveal some of their critical shortcomings. This benchmark aims to advance multimodal forecasting by promoting models that are both accurate and accessible to decision-makers with varied technical expertise. The benchmark can be visualized at https://servicenow.github.io/context-is-key-forecasting/v0/. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.19217</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.19217</id><created>2024-10-24</created><authors><author><keyname>Wu</keyname><forenames>Changlong</forenames></author><author><keyname>Grama</keyname><forenames>Ananth</forenames></author><author><keyname>Szpankowski</keyname><forenames>Wojciech</forenames></author></authors><title>No Free Lunch: Fundamental Limits of Learning Non-Hallucinating   Generative Models</title><categories>cs.LG cs.AI stat.ML</categories><journal-ref>International Conference on Learning Representations (ICLR 2025).   URL: https://openreview.net/pdf?id=OwNoTs2r8e</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generative models have shown impressive capabilities in synthesizing high-quality outputs across various domains. However, a persistent challenge is the occurrence of "hallucinations", where the model produces outputs that are plausible but invalid. While empirical strategies have been explored to mitigate this issue, a rigorous theoretical understanding remains elusive. In this paper, we develop a theoretical framework to analyze the learnability of non-hallucinating generative models from a learning-theoretic perspective. Our results reveal that non-hallucinating learning is statistically impossible when relying solely on the training dataset, even for a hypothesis class of size two and when the entire training set is truthful. To overcome these limitations, we show that incorporating inductive biases aligned with the actual facts into the learning process is essential. We provide a systematic approach to achieve this by restricting the facts set to a concept class of finite VC-dimension and demonstrate its effectiveness under various learning paradigms. Although our findings are primarily conceptual, they represent a first step towards a principled approach to addressing hallucinations in learning generative models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.19725</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.19725</id><created>2024-10-25</created><updated>2025-02-06</updated><authors><author><keyname>Subedi</keyname><forenames>Unique</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>On the Benefits of Active Data Collection in Operator Learning</title><categories>stat.ML cs.LG</categories><comments>Moved Proofs to the Appendix</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study active data collection strategies for operator learning when the target operator is linear and the input functions are drawn from a mean-zero stochastic process with continuous covariance kernels. With an active data collection strategy, we establish an error convergence rate in terms of the decay rate of the eigenvalues of the covariance kernel. We can achieve arbitrarily fast error convergence rates with sufficiently rapid eigenvalue decay of the covariance kernels. This contrasts with the passive (i.i.d.) data collection strategies, where the convergence rate is never faster than linear decay ($\sim n^{-1}$). In fact, for our setting, we show a \emph{non-vanishing} lower bound for any passive data collection strategy, regardless of the eigenvalues decay rate of the covariance kernel. Overall, our results show the benefit of active data collection strategies in operator learning over their passive counterparts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.20640</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.20640</id><created>2024-10-27</created><updated>2025-02-07</updated><authors><author><keyname>Rivera</keyname><forenames>Eduardo Ochoa</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Near Optimal Pure Exploration in Logistic Bandits</title><categories>stat.ML cs.LG</categories><comments>25 pages, 2 figures. arXiv admin note: text overlap with   arXiv:2006.16073 by other authors</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bandit algorithms have garnered significant attention due to their practical applications in real-world scenarios. However, beyond simple settings such as multi-arm or linear bandits, optimal algorithms remain scarce. Notably, no optimal solution exists for pure exploration problems in the context of generalized linear model (GLM) bandits. In this paper, we narrow this gap and develop the first track-and-stop algorithm for general pure exploration problems under the logistic bandit called logistic track-and-stop (Log-TS). Log-TS is an efficient algorithm that asymptotically matches an approximation for the instance-specific lower bound of the expected sample complexity up to a logarithmic factor. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.21464</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.21464</id><created>2024-10-28</created><updated>2025-02-09</updated><authors><author><keyname>Brennan</keyname><forenames>Jennifer</forenames></author><author><keyname>Lahaie</keyname><forenames>Sébastien</forenames></author><author><keyname>Javanmard</keyname><forenames>Adel</forenames></author><author><keyname>Doudchenko</keyname><forenames>Nick</forenames></author><author><keyname>Pouget-Abadie</keyname><forenames>Jean</forenames></author></authors><title>Integer Programming for Generalized Causal Bootstrap Designs</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In experimental causal inference, we distinguish between two sources of uncertainty: design uncertainty, due to the treatment assignment mechanism, and sampling uncertainty, when the sample is drawn from a super-population. This distinction matters in settings with small fixed samples and heterogeneous treatment effects, as in geographical experiments. The standard bootstrap procedure most often used by practitioners primarily estimates sampling uncertainty, and the causal bootstrap procedure, which accounts for design uncertainty, was developed for the completely randomized design and the difference-in-means estimator, whereas non-standard designs and estimators are often used in these low-power regimes. We address this gap by proposing an integer program which computes numerically the worst-case copula used as an input to the causal bootstrap method, in a wide range of settings. Specifically, we prove the asymptotic validity of our approach for unconfounded, conditionally unconfounded, and and individualistic with bounded confoundedness assignments, as well as generalizing to any linear-in-treatment and quadratic-in-treatment estimators. We demonstrate the refined confidence intervals achieved through simulations of small geographical experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.01696</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.01696</id><created>2024-11-03</created><updated>2025-02-08</updated><authors><author><keyname>Noorani</keyname><forenames>Sima</forenames></author><author><keyname>Romero</keyname><forenames>Orlando</forenames></author><author><keyname>Fabbro</keyname><forenames>Nicolo Dal</forenames></author><author><keyname>Hassani</keyname><forenames>Hamed</forenames></author><author><keyname>Pappas</keyname><forenames>George J.</forenames></author></authors><title>Conformal Risk Minimization with Variance Reduction</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Conformal prediction (CP) is a distribution-free framework for achieving probabilistic guarantees on black-box models. CP is generally applied to a model post-training. Recent research efforts, on the other hand, have focused on optimizing CP efficiency during training. We formalize this concept as the problem of conformal risk minimization (CRM). In this direction, conformal training (ConfTr) by Stutz et al.(2022) is a technique that seeks to minimize the expected prediction set size of a model by simulating CP in-between training updates. Despite its potential, we identify a strong source of sample inefficiency in ConfTr that leads to overly noisy estimated gradients, introducing training instability and limiting practical use. To address this challenge, we propose variance-reduced conformal training (VR-ConfTr), a CRM method that incorporates a variance reduction technique in the gradient estimation of the ConfTr objective function. Through extensive experiments on various benchmark datasets, we demonstrate that VR-ConfTr consistently achieves faster convergence and smaller prediction sets compared to baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.10400</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.10400</id><created>2024-11-15</created><updated>2025-02-09</updated><authors><author><keyname>Brill</keyname><forenames>Ryan S.</forenames></author><author><keyname>Wyner</keyname><forenames>Abraham J.</forenames></author></authors><title>The Loser's Curse Revisited: A Case Study of the Critical Role of   Specifying a Utility Function</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A longstanding question in the judgment and decision making literature is whether experts, even in high-stakes environments, exhibit the same cognitive biases observed in controlled experiments with inexperienced participants. In their seminal work, Massey and Thaler (2013) provide a notable example of bias and irrationality in expert decision making: general managers' behavior in the National Football League draft pick trade market. They argue that general managers systematically overvalue top draft picks, which generate less surplus value on average than later first-round picks, a phenomenon known as the loser's curse. Their conclusion hinges on the assumption that general managers should use expected surplus value as their utility function for evaluating draft picks. This assumption, however, is neither explicitly justified nor necessarily aligned with the strategic complexities of constructing a National Football League roster. In this paper, we challenge their framework by considering alternative utility functions, particularly those that emphasize the acquisition of transformational players--those capable of dramatically increasing a team's chances of winning the Super Bowl. Under a decision rule that prioritizes the probability of acquiring elite players, which we construct from a novel Bayesian hierarchical Beta regression model, general managers' draft trade behavior appears rational rather than systematically flawed. More broadly, our findings highlight the critical role of carefully specifying a utility function when evaluating the quality of decisions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.10438</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.10438</id><created>2024-11-15</created><updated>2025-02-10</updated><authors><author><keyname>Yuan</keyname><forenames>Huizhuo</forenames></author><author><keyname>Liu</keyname><forenames>Yifeng</forenames></author><author><keyname>Wu</keyname><forenames>Shuang</forenames></author><author><keyname>Zhou</keyname><forenames>Xun</forenames></author><author><keyname>Gu</keyname><forenames>Quanquan</forenames></author></authors><title>MARS: Unleashing the Power of Variance Reduction for Training Large   Models</title><categories>cs.LG math.OC stat.ML</categories><comments>47 pages, 18 figures, 12 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training deep neural networks--and more recently, large models demands efficient and scalable optimizers. Adaptive gradient algorithms like Adam, AdamW, and their variants have been central to this task. Despite the development of numerous variance reduction algorithms in the past decade aimed at accelerating stochastic optimization in both convex and nonconvex settings, variance reduction has not found widespread success in training deep neural networks or large language models. Consequently, it has remained a less favored approach in modern AI. In this paper, to unleash the power of variance reduction for efficient training of large models, we propose a unified optimization framework, MARS (Make vAriance Reduction Shine), which reconciles preconditioned gradient methods with variance reduction via a scaled stochastic recursive momentum technique. Within our framework, we introduce three instances of MARS that leverage preconditioned gradient updates based on AdamW, Lion, and Shampoo, respectively. We also draw a connection between our algorithms and existing optimizers. Experimental results on training GPT-2 models indicate that MARS consistently outperforms AdamW by a large margin. The implementation of MARS is available at https://github.com/AGI-Arena/MARS. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.12502</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.12502</id><created>2024-11-19</created><updated>2025-02-09</updated><authors><author><keyname>Jenson</keyname><forenames>Daniel</forenames></author><author><keyname>Navott</keyname><forenames>Jhonathan</forenames></author><author><keyname>Zhang</keyname><forenames>Mengyan</forenames></author><author><keyname>Sharma</keyname><forenames>Makkunda</forenames></author><author><keyname>Semenova</keyname><forenames>Elizaveta</forenames></author><author><keyname>Flaxman</keyname><forenames>Seth</forenames></author></authors><title>Transformer Neural Processes - Kernel Regression</title><categories>cs.LG cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Neural Processes (NPs) are a rapidly evolving class of models designed to directly model the posterior predictive distribution of stochastic processes. Originally developed as a scalable alternative to Gaussian Processes (GPs), which are limited by $\mathcal{O}(n^3)$ runtime complexity, the most accurate modern NPs can often rival GPs but still suffer from an $\mathcal{O}(n^2)$ bottleneck due to their attention mechanism. We introduce the Transformer Neural Process - Kernel Regression (TNP-KR), a scalable NP featuring: (1) a Kernel Regression Block (KRBlock), a simple, extensible, and parameter efficient transformer block with complexity $\mathcal{O}(n_c^2 + n_c n_t)$, where $n_c$ and $n_t$ are the number of context and test points, respectively; (2) a kernel-based attention bias; and (3) two novel attention mechanisms: scan attention (SA), a memory-efficient scan-based attention that when paired with a kernel-based bias can make TNP-KR translation invariant, and deep kernel attention (DKA), a Performer-style attention that implicitly incoporates a distance bias and further reduces complexity to $\mathcal{O}(n_c)$. These enhancements enable both TNP-KR variants to perform inference with 100K context points on over 1M test points in under a minute on a single 24GB GPU. On benchmarks spanning meta regression, Bayesian optimization, image completion, and epidemiology, TNP-KR with DKA outperforms its Performer counterpart on nearly every benchmark, while TNP-KR with SA achieves state-of-the-art results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.14548</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.14548</id><created>2024-11-21</created><updated>2025-02-10</updated><authors><author><keyname>Nguyen</keyname><forenames>Thuan</forenames></author><author><keyname>Zhang</keyname><forenames>Jiangshan</forenames></author><author><keyname>Jiang</keyname><forenames>Jiming</forenames></author></authors><title>A Random-Effects Approach to Generalized Linear Mixed Model Analysis of   Incomplete Longitudinal Data</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a random-effects approach to missing values for generalized linear mixed model (GLMM) analysis. The method converts a GLMM with missing covariates to another GLMM without missing covariates. The standard GLMM analysis tools for longitudinal data then apply. The method applies, in particular, to the cases of linear mixed models and logistic regression. Performance of the method is evaluated empirically, and compared with alternative approaches, including the popular MICE procedure of multiple imputation. Theoretical justification of the method is given, and explained, for the patterns observed in the simulation studies. Two real-data examples from healthcare studies are discussed. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.18481</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.18481</id><created>2024-11-27</created><updated>2025-02-09</updated><authors><author><keyname>Bhusal</keyname><forenames>Aneel</forenames></author><author><keyname>Little</keyname><forenames>Todd D.</forenames></author></authors><title>Bhirkuti's Test of Bias Acceptance: Examining in Psychometric   Simulations</title><categories>stat.ME stat.AP stat.CO stat.OT</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This study introduces Bhirkuti's Test of Bias Acceptance, a systematic graphical framework for evaluating bias and determining its acceptability under varying experimental conditions. Absolute Relative Bias (ARB), while useful for understanding bias, is sensitive to outliers and population parameter magnitudes, often overstating bias for small values and understating it for larger ones. Similarly, Relative Efficiency (RE) can be influenced by variance differences and outliers, occasionally producing counterintuitive values exceeding 100%, which complicates interpretation. By addressing the limitations of traditional metrics such as Absolute Relative Bias (ARB) and Relative Efficiency (RE), the proposed graphical methodology framework leverages ridgeline plots and standardized estimate to provide a comprehensive visualization of parameter estimate distributions. Ridgeline plots done this way offer a robust alternative by visualizing full distributions, highlighting variability, trends, outliers, descriptives and facilitating more informed decision-making. This study employs multivariate Latent Growth Models (LGM) and Monte Carlo simulations to examine the performance of growth curve modeling under planned missing data designs, focusing on parameter estimate recovery and efficiency. By combining innovative visualization techniques with rigorous simulation methods, Bhirkuti's Test of Bias Acceptance provides two methods of versatile and interpretable toolset for advancing quantitative research in bias evaluation and efficiency assessment. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.18989</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.18989</id><created>2024-11-28</created><updated>2025-02-09</updated><authors><author><keyname>Wang</keyname><forenames>Zhanfeng</forenames></author><author><keyname>Li</keyname><forenames>Xinyu</forenames></author><author><keyname>Ding</keyname><forenames>Hao</forenames></author><author><keyname>Shi</keyname><forenames>Jian Qing</forenames></author></authors><title>Intrinsic Gaussian Process Regression Modeling for Manifold-valued   Response Variable</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extrinsic Gaussian process regression methods, such as wrapped Gaussian process, have been developed to analyze manifold data. However, there is a lack of intrinsic Gaussian process methods for studying complex data with manifold-valued response variables. In this paper, we first apply the parallel transport operator on Riemannian manifold to propose an intrinsic covariance structure that addresses a critical aspect of constructing a well-defined Gaussian process regression model. We then propose a novel intrinsic Gaussian process regression model for manifold-valued data, which can be applied to data situated not only on Euclidean submanifolds but also on manifolds without a natural ambient space. We establish the asymptotic properties of the proposed models, including information consistency and posterior consistency, and we also show that the posterior distribution of the regression function is invariant to the choice of orthonormal frames for the coordinate representations of the covariance function. Numerical studies, including simulation and real examples, indicate that the proposed methods work well. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.19908</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.19908</id><created>2024-11-29</created><updated>2025-02-07</updated><authors><author><keyname>Gronsbell</keyname><forenames>Jessica</forenames></author><author><keyname>Gao</keyname><forenames>Jianhui</forenames></author><author><keyname>Shi</keyname><forenames>Yaqi</forenames></author><author><keyname>McCaw</keyname><forenames>Zachary R.</forenames></author><author><keyname>Cheng</keyname><forenames>David</forenames></author></authors><title>Another look at inference after prediction</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  From structural biology to epidemiology, predictions from machine learning (ML) models increasingly complement costly gold-standard data to enable faster, more affordable, and scalable scientific inquiry. In response, prediction-based (PB) inference has emerged to accommodate statistical analysis using a large volume of predictions together with a small amount of gold-standard data. The goals of PB inference are two-fold: (i) to mitigate bias from errors in predictions and (ii) to improve efficiency relative to traditional inference using only the gold-standard data. Motwani and Witten (2023) recently revisited two key PB inference approaches and found that only one method, Prediction-powered Inference (PPI) proposed by Angelopoulos et al. (2023), achieves (i). In this paper, we find that PPI does not achieve (ii). We revisit the double sampling literature and show that, with a simple modification, PPI can be adjusted to provide theoretically justified improvements in efficiency. We also contextualize PB inference with economics and statistics literature dating back to the 1960s to highlight the utility of classical methods in this contemporary problem. Our extensive theoretical analyses, along with an analysis of UK Biobank data, indicate that our proposal effectively mitigates bias and improves efficiency, making it preferable for use in practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.03727</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.03727</id><created>2024-12-04</created><updated>2025-02-10</updated><authors><author><keyname>Zhang</keyname><forenames>Zhiheng</forenames></author><author><keyname>Wang</keyname><forenames>Zichen</forenames></author></authors><title>Online Experimental Design With Estimation-Regret Trade-off Under   Network Interference</title><categories>cs.LG math.OC math.ST stat.TH</categories><comments>36 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network interference has attracted significant attention in the field of causal inference, encapsulating various sociological behaviors where the treatment assigned to one individual within a network may affect the outcomes of others, such as their neighbors. A key challenge in this setting is that standard causal inference methods often assume independent treatment effects among individuals, which may not hold in networked environments. To estimate interference-aware causal effects, a traditional approach is to inherit the independent settings, where practitioners randomly assign experimental participants into different groups and compare their outcomes. While effective in offline settings, this strategy becomes problematic in sequential experiments, where suboptimal decision persists, leading to substantial regret. To address this issue, we introduce a unified interference-aware framework for online experimental design. Compared to existing studies, we extend the definition of arm space by utilizing the statistical concept of exposure mapping, which allows for a more flexible and context-aware representation of treatment effects in networked settings. Crucially, we establish a Pareto-optimal trade-off between estimation accuracy and regret under the network concerning both time period and arm space, which remains superior to baseline models even without network interference. Furthermore, we propose an algorithmic implementation and discuss its generalization across different learning settings and network topology. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.04346</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.04346</id><created>2024-12-05</created><updated>2025-02-07</updated><authors><author><keyname>Xue</keyname><forenames>Songkai</forenames></author><author><keyname>Sun</keyname><forenames>Yuekai</forenames></author></authors><title>Distributionally Robust Performative Prediction</title><categories>cs.LG stat.ML</categories><comments>In Proceedings of the 38th Conference on Neural Information   Processing Systems (NeurIPS 2024)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performative prediction aims to model scenarios where predictive outcomes subsequently influence the very systems they target. The pursuit of a performative optimum (PO) -- minimizing performative risk -- is generally reliant on modeling of the distribution map, which characterizes how a deployed ML model alters the data distribution. Unfortunately, inevitable misspecification of the distribution map can lead to a poor approximation of the true PO. To address this issue, we introduce a novel framework of distributionally robust performative prediction and study a new solution concept termed as distributionally robust performative optimum (DRPO). We show provable guarantees for DRPO as a robust approximation to the true PO when the nominal distribution map is different from the actual one. Moreover, distributionally robust performative prediction can be reformulated as an augmented performative prediction problem, enabling efficient optimization. The experimental results demonstrate that DRPO offers potential advantages over traditional PO approach when the distribution map is misspecified at either micro- or macro-level. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.04767</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.04767</id><created>2024-12-05</created><updated>2025-02-06</updated><authors><author><keyname>Tian</keyname><forenames>Bowei</forenames></author><author><keyname>Wang</keyname><forenames>Ziyao</forenames></author><author><keyname>He</keyname><forenames>Shwai</forenames></author><author><keyname>Ye</keyname><forenames>Wanghao</forenames></author><author><keyname>Sun</keyname><forenames>Guoheng</forenames></author><author><keyname>Dai</keyname><forenames>Yucong</forenames></author><author><keyname>Wu</keyname><forenames>Yongkai</forenames></author><author><keyname>Li</keyname><forenames>Ang</forenames></author></authors><title>Towards counterfactual fairness through auxiliary variables</title><categories>cs.LG cs.DS stat.ML</categories><comments>arXiv admin note: text overlap with arXiv:2307.08232 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The challenge of balancing fairness and predictive accuracy in machine learning models, especially when sensitive attributes such as race, gender, or age are considered, has motivated substantial research in recent years. Counterfactual fairness ensures that predictions remain consistent across counterfactual variations of sensitive attributes, which is a crucial concept in addressing societal biases. However, existing counterfactual fairness approaches usually overlook intrinsic information about sensitive features, limiting their ability to achieve fairness while simultaneously maintaining performance. To tackle this challenge, we introduce EXOgenous Causal reasoning (EXOC), a novel causal reasoning framework motivated by exogenous variables. It leverages auxiliary variables to uncover intrinsic properties that give rise to sensitive attributes. Our framework explicitly defines an auxiliary node and a control node that contribute to counterfactual fairness and control the information flow within the model. Our evaluation, conducted on synthetic and real-world datasets, validates EXOC's superiority, showing that it outperforms state-of-the-art approaches in achieving counterfactual fairness. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.05506</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.05506</id><created>2024-12-06</created><updated>2025-02-10</updated><authors><author><keyname>Wang</keyname><forenames>Zebin</forenames></author><author><keyname>Han</keyname><forenames>Yi</forenames></author><author><keyname>Fang</keyname><forenames>Ethan X.</forenames></author><author><keyname>Wang</keyname><forenames>Lan</forenames></author><author><keyname>Lu</keyname><forenames>Junwei</forenames></author></authors><title>Confidence Diagram of Nonparametric Ranking for Uncertainty Assessment   in Large Language Models Evaluation</title><categories>stat.ML cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the inference for the ranking of large language models (LLMs). Alignment arises as a significant challenge to mitigate hallucinations in the use of LLMs. Ranking LLMs has proven to be an effective tool to improve alignment based on the best-of-$N$ policy. In this paper, we propose a new inferential framework for hypothesis testing among the ranking for language models. Our framework is based on a nonparametric contextual ranking framework designed to assess large language models' domain-specific expertise, leveraging nonparametric scoring methods to account for their sensitivity to the prompts. To characterize the combinatorial complexity of the ranking, we introduce a novel concept of confidence diagram, which leverages a Hasse diagram to represent the entire confidence set of rankings by a single directed graph. We show the validity of the proposed confidence diagram by advancing the Gaussian multiplier bootstrap theory to accommodate the supremum of independent empirical processes that are not necessarily identically distributed. Extensive numerical experiments conducted on both synthetic and real data demonstrate that our approach offers valuable insight into the evaluation for the performance of different LLMs across various medical domains. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.06070</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.06070</id><created>2024-12-08</created><updated>2025-02-09</updated><authors><author><keyname>Louzi</keyname><forenames>Azar</forenames></author></authors><title>Stochastic Gradient Descent Revisited</title><categories>math.OC math.PR stat.ML</categories><comments>45 pages</comments><msc-class>90C15, 90C26, 90C60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic gradient descent (SGD) has been a go-to algorithm for nonconvex stochastic optimization problems arising in machine learning. Its theory however often requires a strong framework to guarantee convergence properties. We hereby present a full scope convergence study of biased nonconvex SGD, including weak convergence, function-value convergence and global convergence, and also provide subsequent convergence rates and complexities, all under relatively mild conditions in comparison with literature. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.07972</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.07972</id><created>2024-12-10</created><updated>2025-02-09</updated><authors><author><keyname>Aranguri</keyname><forenames>Santiago</forenames></author><author><keyname>Insulla</keyname><forenames>Francesco</forenames></author></authors><title>Phase-aware Training Schedule Simplifies Learning in Flow-Based   Generative Models</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We analyze the training of a two-layer autoencoder used to parameterize a flow-based generative model for sampling from a high-dimensional Gaussian mixture. Previous work shows that the phase where the relative probability between the modes is learned disappears as the dimension goes to infinity without an appropriate time schedule. We introduce a time dilation that solves this problem. This enables us to characterize the learned velocity field, finding a first phase where the probability of each mode is learned and a second phase where the variance of each mode is learned. We find that the autoencoder representing the velocity field learns to simplify by estimating only the parameters relevant to each phase. Turning to real data, we propose a method that, for a given feature, finds intervals of time where training improves accuracy the most on that feature. Since practitioners take a uniform distribution over training times, our method enables more efficient training. We provide preliminary experiments validating this approach. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.08435</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.08435</id><created>2024-12-11</created><updated>2025-02-07</updated><authors><author><keyname>Zhao</keyname><forenames>Lifan</forenames></author><author><keyname>Shen</keyname><forenames>Yanyan</forenames></author></authors><title>Proactive Model Adaptation Against Concept Drift for Online Time Series   Forecasting</title><categories>cs.LG cs.AI cs.CE stat.ML</categories><comments>Accepted by KDD 2025</comments><doi>10.1145/3690624.3709210</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time series forecasting always faces the challenge of concept drift, where data distributions evolve over time, leading to a decline in forecast model performance. Existing solutions are based on online learning, which continually organize recent time series observations as new training samples and update model parameters according to the forecasting feedback on recent data. However, they overlook a critical issue: obtaining ground-truth future values of each sample should be delayed until after the forecast horizon. This delay creates a temporal gap between the training samples and the test sample. Our empirical analysis reveals that the gap can introduce concept drift, causing forecast models to adapt to outdated concepts. In this paper, we present Proceed, a novel proactive model adaptation framework for online time series forecasting. Proceed first estimates the concept drift between the recently used training samples and the current test sample. It then employs an adaptation generator to efficiently translate the estimated drift into parameter adjustments, proactively adapting the model to the test sample. To enhance the generalization capability of the framework, Proceed is trained on synthetic diverse concept drifts. Extensive experiments on five real-world datasets across various forecast models demonstrate that Proceed brings more performance improvements than the state-of-the-art online learning methods, significantly facilitating forecast models' resilience against concept drifts. Code is available at https://github.com/SJTU-DMTai/OnlineTSF. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.09430</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.09430</id><created>2024-12-12</created><updated>2025-02-10</updated><authors><author><keyname>Krüger</keyname><forenames>Fabian</forenames></author></authors><title>A Kernel Score Perspective on Forecast Disagreement and the Linear Pool</title><categories>econ.EM stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The variance of a linearly combined forecast distribution (or linear pool) consists of two components: The average variance of the component distributions (`average uncertainty'), and the average squared difference between the components' means and the pool's mean (`disagreement'). This paper shows that similar decompositions hold for a class of uncertainty measures that can be constructed as entropy functions of kernel scores. The latter are a rich family of scoring rules that covers point and distribution forecasts for univariate and multivariate, discrete and continuous settings. We further show that the disagreement term is useful for understanding the ex-post performance of the linear pool (as compared to the component distributions), and motivates using the linear pool instead of other forecast combination techniques. From a practical perspective, the results in this paper suggest principled measures of forecast disagreement in a wide range of applied settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.20471</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.20471</id><created>2024-12-29</created><updated>2025-02-07</updated><authors><author><keyname>Cai</keyname><forenames>Yang</forenames></author><author><keyname>Mitra</keyname><forenames>Siddharth</forenames></author><author><keyname>Wang</keyname><forenames>Xiuyuan</forenames></author><author><keyname>Wibisono</keyname><forenames>Andre</forenames></author></authors><title>On the Convergence of Min-Max Langevin Dynamics and Algorithm</title><categories>cs.GT cs.LG math.OC stat.ML</categories><comments>v2: Revised introduction and presentation of results</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study zero-sum games in the space of probability distributions over the Euclidean space $\mathbb{R}^d$ with entropy regularization, in the setting when the interaction function between the players is smooth and strongly convex-strongly concave. We prove an exponential convergence guarantee for the mean-field min-max Langevin dynamics to compute the equilibrium distribution of the zero-sum game. We also study the finite-particle approximation of the mean-field min-max Langevin dynamics, both in continuous and discrete times. We prove biased convergence guarantees for the continuous-time finite-particle min-max Langevin dynamics to the stationary mean-field equilibrium distribution with an explicit bias term which does not scale with the number of particles. We also prove biased convergence guarantees for the discrete-time finite-particle min-max Langevin algorithm to the stationary mean-field equilibrium distribution with an additional bias term which scales with the step size and the number of particles. This provides an explicit iteration complexity for the average particle along the finite-particle algorithm to approximately compute the equilibrium distribution of the zero-sum game. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.01437</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.01437</id><created>2024-12-23</created><updated>2025-02-06</updated><authors><author><keyname>Murphy</keyname><forenames>Charles</forenames></author><author><keyname>Lizotte</keyname><forenames>Simon</forenames></author><author><keyname>Thibault</keyname><forenames>François</forenames></author><author><keyname>Thibeault</keyname><forenames>Vincent</forenames></author><author><keyname>Desrosiers</keyname><forenames>Patrick</forenames></author><author><keyname>Allard</keyname><forenames>Antoine</forenames></author></authors><title>On the reconstruction limits of complex networks</title><categories>stat.AP cs.IT math.IT physics.data-an</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network reconstruction consists in retrieving the hidden interaction structure of a system from observations. Many reconstruction algorithms have been proposed, although less research has been devoted to describe their theoretical limitations. In this work, we adopt an information-theoretic perspective and define the reconstructability: The fraction of structural information recoverable from data. The reconstructability depends on the true data generating (TDG) model which is shown to set the reconstruction limit: any algorithm can perform, on average, at best like the TDG model. We show that the reconstructability is related to various performance measures, such as the probability of error and the Jaccard similarity. In an empirical context where the TDG model is unknown, we introduce the reconstruction index as an approximation of the reconstructability. We find that performing model selection is crucial for the validity of the reconstruction index as a proxy of the reconstructability of empirical time series and networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.04937</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.04937</id><created>2025-01-08</created><updated>2025-02-07</updated><authors><author><keyname>Shah</keyname><forenames>Jaimin</forenames></author><author><keyname>Cardone</keyname><forenames>Martina</forenames></author><author><keyname>Rush</keyname><forenames>Cynthia</forenames></author><author><keyname>Dytso</keyname><forenames>Alex</forenames></author></authors><title>Generalized Linear Models with 1-Bit Measurements: Asymptotics of the   Maximum Likelihood Estimator</title><categories>math.ST cs.SY eess.AS eess.SP eess.SY stat.TH</categories><comments>ICASSP 2025</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This work establishes regularity conditions for consistency and asymptotic normality of the multiple parameter maximum likelihood estimator(MLE) from censored data, where the censoring mechanism is in the form of $1$-bit measurements. The underlying distribution of the uncensored data is assumed to belong to the exponential family, with natural parameters expressed as a linear combination of the predictors, known as generalized linear model (GLM). As part of the analysis, the Fisher information matrix is also derived for both censored and uncensored data, which helps to quantify the impact of censoring and assess the performance of the MLE. The choice of GLM allows one to consider a variety of practical examples where 1-bit estimation is of interest. In particular, it is shown how the derived results can be used to analyze two practically relevant scenarios: the Gaussian model with both unknown mean and variance, and the Poisson model with an unknown mean. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.05803</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.05803</id><created>2025-01-10</created><updated>2025-02-10</updated><authors><author><keyname>Kim</keyname><forenames>Sunwoo</forenames></author><author><keyname>Kim</keyname><forenames>Minkyu</forenames></author><author><keyname>Park</keyname><forenames>Dongmin</forenames></author></authors><title>Test-time Alignment of Diffusion Models without Reward Over-optimization</title><categories>cs.LG cs.AI cs.CV math.ST stat.TH</categories><comments>ICLR 2025</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Diffusion models excel in generative tasks, but aligning them with specific objectives while maintaining their versatility remains challenging. Existing fine-tuning methods often suffer from reward over-optimization, while approximate guidance approaches fail to optimize target rewards effectively. Addressing these limitations, we propose a training-free, test-time method based on Sequential Monte Carlo (SMC) to sample from the reward-aligned target distribution. Our approach, tailored for diffusion sampling and incorporating tempering techniques, achieves comparable or superior target rewards to fine-tuning methods while preserving diversity and cross-reward generalization. We demonstrate its effectiveness in single-reward optimization, multi-objective scenarios, and online black-box optimization. This work offers a robust solution for aligning diffusion models with diverse downstream objectives without compromising their general capabilities. Code is available at https://github.com/krafton-ai/DAS. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.07025</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.07025</id><created>2025-01-12</created><updated>2025-02-07</updated><authors><author><keyname>Zhang</keyname><forenames>Yong</forenames></author><author><keyname>Gyamfi</keyname><forenames>Eric Herrison</forenames></author></authors><title>A Weighted Similarity Metric for Community Detection in Sparse Data</title><categories>stat.ME cs.SI</categories><comments>This paper has been accepted for Workshop of AI for Social Impact at   AAAI 2025</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Many Natural Language Processing (NLP) related applications involves topics and sentiments derived from short documents such as consumer reviews and social media posts. Topics and sentiments of short documents are highly sparse because a short document generally covers a few topics among hundreds of candidates. Imputation of missing data is sometimes hard to justify and also often unpractical in highly sparse data. We developed a method for calculating a weighted similarity for highly sparse data without imputation. This weighted similarity is consist of three components to capture similarities based on both existence and lack of common properties and pattern of missing values. As a case study, we used a community detection algorithm and this weighted similarity to group different shampoo brands based on sparse topic sentiments derived from short consumer reviews. Compared with traditional imputation and similarity measures, the weighted similarity shows better performance in both general community structures and average community qualities. The performance is consistent and robust across metrics and community complexities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10117</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10117</id><created>2025-01-17</created><updated>2025-02-07</updated><authors><author><keyname>Liu</keyname><forenames>Weiguang</forenames></author><author><keyname>de Paula</keyname><forenames>Áureo</forenames></author><author><keyname>Tamer</keyname><forenames>Elie</forenames></author></authors><title>Prediction Sets and Conformal Inference with Censored Outcomes</title><categories>econ.EM stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given data on a scalar random variable $Y$, a prediction set for $Y$ with miscoverage level $\alpha$ is a set of values for $Y$ that contains a randomly drawn $Y$ with probability $1 - \alpha$, where $\alpha \in (0,1)$. Among all prediction sets that satisfy this coverage property, the oracle prediction set is the one with the smallest volume. This paper provides estimation methods of such prediction sets given observed conditioning covariates when $Y$ is \textit{censored} or \textit{measured in intervals}. We first characterise the oracle prediction set under interval censoring and develop a consistent estimator for the shortest prediction {\it interval} that satisfies this coverage property.These consistency results are extended to accommodate cases where the prediction set consists of multiple disjoint intervals. We use conformal inference to construct a prediction set that achieves finite-sample validity under censoring and maintains consistency as sample size increases, using a conformity score function designed for interval data. The procedure accommodates the prediction uncertainty that is irreducible (due to the stochastic nature of outcomes), the modelling uncertainty due to partial identification and also sampling uncertainty that gets reduced as samples get larger. We conduct a set of Monte Carlo simulations and an application to data from the Current Population Survey. The results highlight the robustness and efficiency of the proposed methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10139</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10139</id><created>2025-01-17</created><updated>2025-02-09</updated><authors><author><keyname>Kaur</keyname><forenames>Jivat Neet</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author><author><keyname>Alaa</keyname><forenames>Ahmed</forenames></author></authors><title>Conformal Prediction Sets with Improved Conditional Coverage using Trust   Scores</title><categories>cs.LG cs.AI stat.ME stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Standard conformal prediction offers a marginal guarantee on coverage, but for prediction sets to be truly useful, they should ideally ensure coverage conditional on each test point. Unfortunately, it is impossible to achieve exact, distribution-free conditional coverage in finite samples. In this work, we propose an alternative conformal prediction algorithm that targets coverage where it matters most--in instances where a classifier is overconfident in its incorrect predictions. We start by dissecting miscoverage events in marginally-valid conformal prediction, and show that miscoverage rates vary based on the classifier's confidence and its deviation from the Bayes optimal classifier. Motivated by this insight, we develop a variant of conformal prediction that targets coverage conditional on a reduced set of two variables: the classifier's confidence in a prediction and a nonparametric trust score that measures its deviation from the Bayes classifier. Empirical evaluation on multiple image datasets shows that our method generally improves conditional coverage properties compared to standard conformal prediction, including class-conditional coverage, coverage over arbitrary subgroups, and coverage over demographic groups. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10585</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10585</id><created>2025-01-17</created><updated>2025-02-08</updated><authors><author><keyname>Martin</keyname><forenames>Ryan</forenames></author></authors><title>An efficient Monte Carlo method for valid prior-free possibilistic   statistical inference</title><categories>stat.CO stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Inferential models (IMs) offer prior-free, Bayesian-like, posterior degrees of belief designed for statistical inference, which feature a frequentist-like calibration property that ensures reliability of said inferences. The catch is that IMs' degrees of belief are possibilistic rather than probabilistic and, since the familiar Monte Carlo methods approximate probabilistic quantities, there are computational challenges associated with putting the IM framework into practice. The present paper addresses this shortcoming by developing a new Monte Carlo-based tool designed specifically to approximate the IM's possibilistic output. The proposal is based on a characterization of the possibilistic IM's credal set, which identifies the "best probabilistic approximation" of the IM as a mixture distribution that can be readily approximated and sampled from; these samples can then be transformed into a possibilistic approximation of the IM. Numerical results are presented highlighting the proposed approximation's accuracy and computational efficiency. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10974</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10974</id><created>2025-01-19</created><updated>2025-02-06</updated><authors><author><keyname>Huang</keyname><forenames>Yu-Han</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>Sequential Change Detection for Learning in Piecewise Stationary Bandit   Environments</title><categories>cs.IT cs.SY eess.SY math.IT stat.OT</categories><comments>15 pages, 2 figures. arXiv admin note: text overlap with   arXiv:2501.01291</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A finite-horizon variant of the quickest change detection problem is investigated, which is motivated by a change detection problem that arises in piecewise stationary bandits. The goal is to minimize the \emph{latency}, which is smallest threshold such that the probability that the detection delay exceeds the threshold is below a desired low level, while controlling the false alarm probability to a desired low level. When the pre- and post-change distributions are unknown, two tests are proposed as candidate solutions. These tests are shown to attain order optimality in terms of the horizon. Furthermore, the growth in their latencies with respect to the false alarm probability and late detection probability satisfies a property that is desirable in regret analysis for piecewise stationary bandits. Numerical results are provided to validate the theoretical performance results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.11622</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.11622</id><created>2025-01-20</created><updated>2025-02-08</updated><authors><author><keyname>Liu</keyname><forenames>Lu</forenames></author><author><keyname>Tang</keyname><forenames>Yang</forenames></author><author><keyname>Zhang</keyname><forenames>Kexuan</forenames></author><author><keyname>Sun</keyname><forenames>Qiyu</forenames></author></authors><title>Causal Learning for Heterogeneous Subgroups Based on Nonlinear Causal   Kernel Clustering</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the challenge posed by multi-source and heterogeneous data collected from diverse environments, causal relationships among features can exhibit variations influenced by different time spans, regions, or strategies. This diversity makes a single causal model inadequate for accurately representing complex causal relationships in all observational data, a crucial consideration in causal learning. To address this challenge, the nonlinear Causal Kernel Clustering method is introduced for heterogeneous subgroup causal learning, highlighting variations in causal relationships across diverse subgroups. The main component for clustering heterogeneous subgroups lies in the construction of the $u$-centered sample mapping function with the property of unbiased estimation, which assesses the differences in potential nonlinear causal relationships in various samples and supported by causal identifiability theory. Experimental results indicate that the method performs well in identifying heterogeneous subgroups and enhancing causal learning, leading to a reduction in prediction error. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14926</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14926</id><created>2025-01-24</created><updated>2025-02-07</updated><authors><author><keyname>Braun</keyname><forenames>Dan</forenames></author><author><keyname>Bushnaq</keyname><forenames>Lucius</forenames></author><author><keyname>Heimersheim</keyname><forenames>Stefan</forenames></author><author><keyname>Mendel</keyname><forenames>Jake</forenames></author><author><keyname>Sharkey</keyname><forenames>Lee</forenames></author></authors><title>Interpretability in Parameter Space: Minimizing Mechanistic Description   Length with Attribution-based Parameter Decomposition</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Mechanistic interpretability aims to understand the internal mechanisms learned by neural networks. Despite recent progress toward this goal, it remains unclear how best to decompose neural network parameters into mechanistic components. We introduce Attribution-based Parameter Decomposition (APD), a method that directly decomposes a neural network's parameters into components that (i) are faithful to the parameters of the original network, (ii) require a minimal number of components to process any input, and (iii) are maximally simple. Our approach thus optimizes for a minimal length description of the network's mechanisms. We demonstrate APD's effectiveness by successfully identifying ground truth mechanisms in multiple toy experimental settings: Recovering features from superposition; separating compressed computations; and identifying cross-layer distributed representations. While challenges remain to scaling APD to non-toy models, our results suggest solutions to several open problems in mechanistic interpretability, including identifying minimal circuits in superposition, offering a conceptual foundation for 'features', and providing an architecture-agnostic framework for neural network decomposition. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14974</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14974</id><created>2025-01-24</created><updated>2025-02-06</updated><authors><author><keyname>Deng</keyname><forenames>Fengnan</forenames></author><author><keyname>Vidyashankar</keyname><forenames>Anand N.</forenames></author></authors><title>Private Minimum Hellinger Distance Estimation via Hellinger Distance   Differential Privacy</title><categories>math.ST cs.CR math.PR stat.ME stat.ML stat.TH</categories><msc-class>62F35, 68P27, 62E20, 60E05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective functions based on Hellinger distance yield robust and efficient estimators of model parameters. Motivated by privacy and regulatory requirements encountered in contemporary applications, we derive in this paper \emph{private minimum Hellinger distance estimators}. The estimators satisfy a new privacy constraint, namely, Hellinger differential privacy, while retaining the robustness and efficiency properties. We demonstrate that Hellinger differential privacy shares several features of standard differential privacy while allowing for sharper inference. Additionally, for computational purposes, we also develop Hellinger differentially private gradient descent and Newton-Raphson algorithms. We illustrate the behavior of our estimators in finite samples using numerical experiments and verify that they retain robustness properties under gross-error contamination. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.17722</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.17722</id><created>2025-01-29</created><updated>2025-02-10</updated><authors><author><keyname>Gribkova</keyname><forenames>Nadezhda</forenames></author><author><keyname>Wang</keyname><forenames>Mengqi</forenames></author><author><keyname>Zitikis</keyname><forenames>Ričardas</forenames></author></authors><title>Fundamentals of non-parametric statistical inference for integrated   quantiles</title><categories>math.ST stat.TH</categories><comments>79 pages, 6 figures, 1 table</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present a general non-parametric statistical inference theory for integrals of quantiles without assuming any specific sampling design or dependence structure. Technical considerations are accompanied by examples and discussions, including those pertaining to the bias of empirical estimators. To illustrate how the general results can be adapted to specific situations, we derive - at a stroke and under minimal conditions - consistency and asymptotic normality of the empirical tail-value-at-risk, Lorenz and Gini curves at any probability level in the case of the simple random sampling, thus facilitating a comparison of our results with what is already known in the literature. Results, notes and references concerning dependent (i.e., time series) data are also offered. As a by-product, our general results provide new and unified proofs of large-sample properties of a number of classical statistical estimators, such as trimmed means, and give additional insights into the origins of, and the reasons for, various necessary and sufficient conditions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18502</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18502</id><created>2025-01-30</created><updated>2025-02-07</updated><authors><author><keyname>Kumar</keyname><forenames>Ritesh</forenames></author><author><keyname>Vatedka</keyname><forenames>Shashank</forenames></author></authors><title>One-Bit Distributed Mean Estimation with Unknown Variance</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>21 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this work, we study the problem of distributed mean estimation with $1$-bit communication constraints when the variance is unknown. We focus on the specific case where each user has access to one i.i.d. sample drawn from a distribution that belongs to a scale-location family, and is limited to sending just a single bit of information to a central server whose goal is to estimate the mean. We propose non-adaptive and adaptive estimators that are shown to be asymptotically normal. We derive bounds on the asymptotic (in the number of users) Mean Squared Error (MSE) achieved by these estimators. For a class of symmetric log-concave distributions, we derive matching lower bounds for the MSE achieved by adaptive estimators, proving the optimality of our scheme. We show that non-adaptive estimators can be strictly suboptimal by deriving a lower bound on the MSE achieved by any non-adaptive estimator for Gaussian distributions and demonstrating a positive gap between this and the MSE achieved by our adaptive scheme. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00168</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00168</id><created>2025-01-31</created><updated>2025-02-07</updated><authors><author><keyname>Herrera-Esposito</keyname><forenames>Daniel</forenames></author><author><keyname>Burge</keyname><forenames>Johannes</forenames></author></authors><title>Supervised Quadratic Feature Analysis: An Information Geometry Approach   to Dimensionality Reduction</title><categories>stat.ML cs.LG math.DG math.ST stat.TH</categories><comments>18 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Supervised dimensionality reduction aims to map labeled data to a low-dimensional feature space while maximizing class discriminability. Despite the availability of methods for learning complex non-linear features (e.g. Deep Learning), there is an enduring demand for dimensionality reduction methods that learn linear features due to their interpretability, low computational cost, and broad applicability. However, there is a gap between methods that optimize linear separability (e.g. LDA), and more flexible but computationally expensive methods that optimize over arbitrary class boundaries (e.g. metric-learning methods). Here, we present Supervised Quadratic Feature Analysis (SQFA), a dimensionality reduction method for learning linear features that maximize the differences between class-conditional first- and second-order statistics, which allow for quadratic discrimination. SQFA exploits the information geometry of second-order statistics in the symmetric positive definite manifold. We show that SQFA features support quadratic discriminability in real-world problems. We also provide a theoretical link, based on information geometry, between SQFA and the Quadratic Discriminant Analysis (QDA) classifier. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00182</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00182</id><created>2025-01-31</created><updated>2025-02-07</updated><authors><author><keyname>Seo</keyname><forenames>Jungwon</forenames></author><author><keyname>Catak</keyname><forenames>Ferhat Ozgur</forenames></author><author><keyname>Rong</keyname><forenames>Chunming</forenames></author></authors><title>Understanding Federated Learning from IID to Non-IID dataset: An   Experimental Study</title><categories>cs.LG cs.AI stat.ML</categories><journal-ref>36th Norwegian ICT Conference for Research and Education, NIKT   2024</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As privacy concerns and data regulations grow, federated learning (FL) has emerged as a promising approach for training machine learning models across decentralized data sources without sharing raw data. However, a significant challenge in FL is that client data are often non-IID (non-independent and identically distributed), leading to reduced performance compared to centralized learning. While many methods have been proposed to address this issue, their underlying mechanisms are often viewed from different perspectives. Through a comprehensive investigation from gradient descent to FL, and from IID to non-IID data settings, we find that inconsistencies in client loss landscapes primarily cause performance degradation in non-IID scenarios. From this understanding, we observe that existing methods can be grouped into two main strategies: (i) adjusting parameter update paths and (ii) modifying client loss landscapes. These findings offer a clear perspective on addressing non-IID challenges in FL and help guide future research in the field. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00251</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00251</id><created>2025-01-31</created><updated>2025-02-09</updated><authors><author><keyname>Zhao</keyname><forenames>Anqi</forenames></author><author><keyname>Ding</keyname><forenames>Peng</forenames></author><author><keyname>Li</keyname><forenames>Fan</forenames></author></authors><title>Interacted two-stage least squares with treatment effect heterogeneity</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Treatment effect heterogeneity with respect to covariates is common in instrumental variable (IV) analyses. An intuitive approach, which we term the interacted two-stage least squares (2SLS), is to postulate a linear working model of the outcome on the treatment, covariates, and treatment-covariate interactions, and instrument it by the IV, covariates, and IV-covariate interactions. We clarify the causal interpretation of the interacted 2SLS under the local average treatment effect (LATE) framework when the IV is valid conditional on covariates. Our contributions are threefold. First, we show that the interacted 2SLS with centered covariates is consistent for estimating the LATE if either of the following conditions holds: (i) the IV-covariate interactions are linear in the covariates; (ii) the linear outcome model underlying the interacted 2SLS is correct. Second, we show that the coefficients of the treatment-covariate interactions from the interacted 2SLS are consistent for estimating treatment effect heterogeneity with regard to covariates among compliers if either condition (i) or condition (ii) holds. Moreover, we connect the 2SLS estimator with the weighting perspective in Abadie (2003) and establish the necessity of condition (i) in the absence of additional assumptions on potential outcomes. Third, leveraging the consistency guarantees of the interacted 2SLS for categorical covariates, we propose a stratification strategy based on the IV propensity score to approximate the LATE and treatment effect heterogeneity with regard to the IV propensity score when neither condition (i) nor condition (ii) holds. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00309</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00309</id><created>2025-01-31</created><updated>2025-02-10</updated><authors><author><keyname>Shi</keyname><forenames>Jianwei</forenames></author><author><keyname>Abdulah</keyname><forenames>Sameh</forenames></author><author><keyname>Sun</keyname><forenames>Ying</forenames></author><author><keyname>Genton</keyname><forenames>Marc G.</forenames></author></authors><title>Decentralized Inference for Spatial Data Using Low-Rank Models</title><categories>stat.ML cs.LG stat.CO stat.ME</categories><comments>84 pages</comments><msc-class>62M30</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Advancements in information technology have enabled the creation of massive spatial datasets, driving the need for scalable and efficient computational methodologies. While offering viable solutions, centralized frameworks are limited by vulnerabilities such as single-point failures and communication bottlenecks. This paper presents a decentralized framework tailored for parameter inference in spatial low-rank models to address these challenges. A key obstacle arises from the spatial dependence among observations, which prevents the log-likelihood from being expressed as a summation-a critical requirement for decentralized optimization approaches. To overcome this challenge, we propose a novel objective function leveraging the evidence lower bound, which facilitates the use of decentralized optimization techniques. Our approach employs a block descent method integrated with multi-consensus and dynamic consensus averaging for effective parameter optimization. We prove the convexity of the new objective function in the vicinity of the true parameters, ensuring the convergence of the proposed method. Additionally, we present the first theoretical results establishing the consistency and asymptotic normality of the estimator within the context of spatial low-rank models. Extensive simulations and real-world data experiments corroborate these theoretical findings, showcasing the robustness and scalability of the framework. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00666</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00666</id><created>2025-02-01</created><updated>2025-02-09</updated><authors><author><keyname>Chen</keyname><forenames>Mingyu</forenames></author><author><keyname>Chen</keyname><forenames>Yiding</forenames></author><author><keyname>Sun</keyname><forenames>Wen</forenames></author><author><keyname>Zhang</keyname><forenames>Xuezhou</forenames></author></authors><title>Avoiding $\mathbf{exp(R_{max})}$ scaling in RLHF through   Preference-based Exploration</title><categories>cs.LG cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal technique for large language model (LLM) alignment. This paper studies the setting of online RLHF and focus on improving sample efficiency. All existing algorithms in online RLHF, whether doing passive exploration or active exploration, suffer from a sample complexity that scales exponentially with the scale of the reward function. This fundamental limitation hinders their effectiveness in scenarios with heavily skewed preferences, e.g. questions with a unique correct solution. To address this, we introduce Self-Exploring Preference-Incentive Online Preference Optimization (SE-POPO), an online RLHF algorithm that for the first time achieves a sample complexity that scales polynomially with the reward scale, answering an open problem raised by Xie et al. (2024).. Theoretically, we demonstrate that the sample complexity of SE-POPO dominates that of existing exploration algorithms. Empirically, our systematic evaluation confirms that SE-POPO is more sample-efficient than both exploratory and non-exploratory baselines, in two primary application scenarios of RLHF as well as on public benchmarks, marking a significant step forward in RLHF algorithm design. The code is available at https://github.com/MYC000801/SE-POPO. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00724</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00724</id><created>2025-02-02</created><updated>2025-02-09</updated><authors><author><keyname>Habi</keyname><forenames>Hai Victor</forenames></author><author><keyname>Messer</keyname><forenames>Hagit</forenames></author><author><keyname>Bresler</keyname><forenames>Yoram</forenames></author></authors><title>Learned Bayesian Cram\'er-Rao Bound for Unknown Measurement Models Using   Score Neural Networks</title><categories>eess.SP cs.AI cs.LG stat.ML</categories><comments>28 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bayesian Cram\'er-Rao bound (BCRB) is a crucial tool in signal processing for assessing the fundamental limitations of any estimation problem as well as benchmarking within a Bayesian frameworks. However, the BCRB cannot be computed without full knowledge of the prior and the measurement distributions. In this work, we propose a fully learned Bayesian Cram\'er-Rao bound (LBCRB) that learns both the prior and the measurement distributions. Specifically, we suggest two approaches to obtain the LBCRB: the Posterior Approach and the Measurement-Prior Approach. The Posterior Approach provides a simple method to obtain the LBCRB, whereas the Measurement-Prior Approach enables us to incorporate domain knowledge to improve the sample complexity and {interpretability}. To achieve this, we introduce a Physics-encoded score neural network which enables us to easily incorporate such domain knowledge into a neural network. We {study the learning} errors of the two suggested approaches theoretically, and validate them numerically. We demonstrate the two approaches on several signal processing examples, including a linear measurement problem with unknown mixing and Gaussian noise covariance matrices, frequency estimation, and quantized measurement. In addition, we test our approach on a nonlinear signal processing problem of frequency estimation with real-world underwater ambient noise. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00838</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00838</id><created>2025-02-02</created><authors><author><keyname>Bussemaker</keyname><forenames>Jasper H.</forenames></author><author><keyname>Saves</keyname><forenames>Paul</forenames></author><author><keyname>Bartoli</keyname><forenames>Nathalie</forenames></author><author><keyname>Lefebvre</keyname><forenames>Thierry</forenames></author><author><keyname>Lafage</keyname><forenames>Rémi</forenames></author></authors><title>System Architecture Optimization Strategies: Dealing with Expensive   Hierarchical Problems</title><categories>math.OC cs.DM stat.AP</categories><comments>J Glob Optim (2024)</comments><doi>10.1007/s10898-024-01443-8</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Choosing the right system architecture for the problem at hand is challenging due to the large design space and high uncertainty in the early stage of the design process. Formulating the architecting process as an optimization problem may mitigate some of these challenges. This work investigates strategies for solving System Architecture Optimization (SAO) problems: expensive, black-box, hierarchical, mixed-discrete, constrained, multi-objective problems that may be subject to hidden constraints. Imputation ratio, correction ratio, correction fraction, and max rate diversity metrics are defined for characterizing hierar chical design spaces. This work considers two classes of optimization algorithms for SAO: Multi-Objective Evolutionary Algorithms (MOEA) such as NSGA-II, and Bayesian Optimization (BO) algorithms. A new Gaussian process kernel is presented that enables modeling hierarchical categorical variables, extending previous work on modeling continuous and integer hierarchical variables. Next, a hierarchical sampling algorithm that uses design space hierarchy to group design vectors by active design variables is developed. Then, it is demonstrated that integrating more hierarchy information in the optimization algorithms yields better optimization results for BO algorithms. Several realistic single-objective and multi-objective test problems are used for investigations. Finally, the BO algorithm is applied to a jet engine architecture optimization problem. This work shows that the developed BO algorithm can effectively solve the problem with one order of magnitude less function evaluations than NSGA-II. The algorithms and problems used in this work are implemented in the open-source Python library SBArchOpt. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00851</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00851</id><created>2025-02-02</created><updated>2025-02-09</updated><authors><author><keyname>Herdiana</keyname><forenames>Indra</forenames></author><author><keyname>Kamal</keyname><forenames>M Alfin</forenames></author><author><keyname>Triyani</keyname></author><author><keyname>Estri</keyname><forenames>Mutia Nur</forenames></author><author><keyname>Renny</keyname></author></authors><title>A More Precise Elbow Method for Optimum K-means Clustering</title><categories>stat.ME</categories><comments>22 pages</comments><msc-class>91C20, 62-08, 62A09</msc-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  K-means clustering is an unsupervised clustering method that requires an initial decision of number of clusters. One method to determine the number of clusters is the elbow method, a heuristic method that relies on visual representation. The method uses the number based on the elbow point, the point closest to 90 degrees that indicates the most optimum number of clusters. This research improves the elbow method such that it becomes an objective method. We use the analytical geometric formula to calculate an angle between lines and real analysis principle of derivative to simplify the elbow point determination. We also consider every possibility of the elbow method graph behaviour such that the algorithm is universally applicable. The result is that the elbow point can be measured precisely with a simple algorithm that does not involve complex functions or calculations. This improved method gives an alternative of more reliable cluster determination method that contributes to more optimum k-means clustering. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00854</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00854</id><created>2025-02-02</created><authors><author><keyname>Priem</keyname><forenames>Rémy</forenames></author><author><keyname>Diouane</keyname><forenames>Youssef</forenames></author><author><keyname>Bartoli</keyname><forenames>Nathalie</forenames></author><author><keyname>Dubreuil</keyname><forenames>Sylvain</forenames></author><author><keyname>Saves</keyname><forenames>Paul</forenames></author></authors><title>High-Dimensional Bayesian Optimization Using Both Random and Supervised   Embeddings</title><categories>math.OC cs.LG stat.ML</categories><journal-ref>AIAA Journal 2025 63:1, 162-173</journal-ref><doi>10.2514/1.J063488</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Bayesian optimization (BO) is one of the most powerful strategies to solve computationally expensive-to-evaluate blackbox optimization problems. However, BO methods are conventionally used for optimization problems of small dimension because of the curse of dimensionality. In this paper, a high-dimensionnal optimization method incorporating linear embedding subspaces of small dimension is proposed to efficiently perform the optimization. An adaptive learning strategy for these linear embeddings is carried out in conjunction with the optimization. The resulting BO method, named efficient global optimization coupled with random and supervised embedding (EGORSE), combines in an adaptive way both random and supervised linear embeddings. EGORSE has been compared to state-of-the-art algorithms and tested on academic examples with a number of design variables ranging from 10 to 600. The obtained results show the high potential of EGORSE to solve high-dimensional blackbox optimization problems, in terms of both CPU time and the limited number of calls to the expensive blackbox simulation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00924</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00924</id><created>2025-02-02</created><updated>2025-02-07</updated><authors><author><keyname>Zhang</keyname><forenames>Yichi</forenames></author><author><keyname>Lu</keyname><forenames>Haidong</forenames></author></authors><title>Generalized Simple Graphical Rules for Assessing Selection Bias</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Selection bias is a major obstacle toward valid causal inference in epidemiology. Over the past decade, several simple graphical rules based on causal diagrams have been proposed as the sufficient identification conditions for addressing selection bias and recovering causal effects. However, these simple graphical rules are usually coupled with specific identification strategies and estimators. In this article, we show two important cases of selection bias that cannot be addressed by these simple rules and their estimators: one case where selection is a descendant of a collider of the treatment and the outcome, and the other case where selection is affected by the mediator. To address selection bias in these two cases, we construct identification formulas by the g-computation and the inverse probability weighting (IPW) methods based on single-world intervention graphs (SWIGs). They are generalized to recover the average treatment effect by adjusting for post-treatment upstream causes of selection. We propose two IPW estimators and their variance estimators to recover the average treatment effect in the presence of selection bias in these two cases. We conduct simulation studies to verify the performance of the estimators when the traditional crude selected-sample analysis returns erroneous contradictory conclusions to the truth. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02496</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02496</id><created>2025-02-04</created><updated>2025-02-07</updated><authors><author><keyname>Kolb</keyname><forenames>Chris</forenames></author><author><keyname>Weber</keyname><forenames>Tobias</forenames></author><author><keyname>Bischl</keyname><forenames>Bernd</forenames></author><author><keyname>Rügamer</keyname><forenames>David</forenames></author></authors><title>Deep Weight Factorization: Sparse Learning Through the Lens of   Artificial Symmetries</title><categories>cs.LG stat.ML</categories><comments>accepted at ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sparse regularization techniques are well-established in machine learning, yet their application in neural networks remains challenging due to the non-differentiability of penalties like the $L_1$ norm, which is incompatible with stochastic gradient descent. A promising alternative is shallow weight factorization, where weights are decomposed into two factors, allowing for smooth optimization of $L_1$-penalized neural networks by adding differentiable $L_2$ regularization to the factors. In this work, we introduce deep weight factorization, extending previous shallow approaches to more than two factors. We theoretically establish equivalence of our deep factorization with non-convex sparse regularization and analyze its impact on training dynamics and optimization. Due to the limitations posed by standard training practices, we propose a tailored initialization scheme and identify important learning rate requirements necessary for training factorized networks. We demonstrate the effectiveness of our deep weight factorization through experiments on various architectures and datasets, consistently outperforming its shallow counterpart and widely used pruning methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02861</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02861</id><created>2025-02-04</created><updated>2025-02-06</updated><authors><author><keyname>Shen</keyname><forenames>Judy Hanwen</forenames></author><author><keyname>Vitercik</keyname><forenames>Ellen</forenames></author><author><keyname>Wikum</keyname><forenames>Anders</forenames></author></authors><title>Algorithms with Calibrated Machine Learning Predictions</title><categories>stat.ML cs.DS cs.LG</categories><comments>Replacement: updated acknowledgments</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The field of algorithms with predictions incorporates machine learning advice in the design of online algorithms to improve real-world performance. While this theoretical framework often assumes uniform reliability across all predictions, modern machine learning models can now provide instance-level uncertainty estimates. In this paper, we propose calibration as a principled and practical tool to bridge this gap, demonstrating the benefits of calibrated advice through two case studies: the ski rental and online job scheduling problems. For ski rental, we design an algorithm that achieves optimal prediction-dependent performance and prove that, in high-variance settings, calibrated advice offers more effective guidance than alternative methods for uncertainty quantification. For job scheduling, we demonstrate that using a calibrated predictor leads to significant performance improvements over existing methods. Evaluations on real-world data validate our theoretical findings, highlighting the practical impact of calibration for algorithms with predictions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03479</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03479</id><created>2025-01-27</created><updated>2025-02-06</updated><authors><author><keyname>Cuicizion</keyname><forenames>Eliuvish</forenames></author><author><keyname>Ri</keyname><forenames>Itsugo</forenames></author><author><keyname>Holmes</keyname><forenames>Elaine</forenames></author><author><keyname>Lyeutsaon</keyname><forenames>Sho</forenames></author><author><keyname>Chern</keyname><forenames>Jawad Hassan</forenames></author></authors><title>A Tutorial on Markov Renewal and Semi-Markov Proportional Hazards Model</title><categories>stat.AP stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Transition probability estimation plays a critical role in multi-state modeling, especially in clinical research. This paper investigates the application of semi-Markov and Markov renewal frameworks to the EBMT dataset, focusing on six clinical states encountered during hematopoietic stem cell transplantation. By comparing Aalen-Johansen (AJ) and Dabrowska-Sun-Horowitz (DSH) estimators, we demonstrate that semi-Markov models, which incorporate sojourn times, provide a more nuanced and temporally sensitive depiction of patient trajectories compared to memoryless Markov models. The DSH estimator consistently yields smoother probability curves, particularly for transitions involving prolonged states. We use empirical process theory and Burkholder-Davis-Gundy inequality to show weak convergence of the estimator. Future work includes extending the framework to accommodate advanced covariate structures and non-Markovian dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03587</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03587</id><created>2025-02-05</created><updated>2025-02-06</updated><authors><author><keyname>von Seeger</keyname><forenames>Anneke</forenames></author><author><keyname>Zou</keyname><forenames>Dongmian</forenames></author><author><keyname>Lerman</keyname><forenames>Gilad</forenames></author></authors><title>Stein Discrepancy for Unsupervised Domain Adaptation</title><categories>cs.LG stat.ML</categories><comments>24 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Unsupervised domain adaptation (UDA) leverages information from a labeled source dataset to improve accuracy on a related but unlabeled target dataset. A common approach to UDA is aligning representations from the source and target domains by minimizing the distance between their data distributions. Previous methods have employed distances such as Wasserstein distance and maximum mean discrepancy. However, these approaches are less effective when the target data is significantly scarcer than the source data. Stein discrepancy is an asymmetric distance between distributions that relies on one distribution only through its score function. In this paper, we propose a novel UDA method that uses Stein discrepancy to measure the distance between source and target domains. We develop a learning framework using both non-kernelized and kernelized Stein discrepancy. Theoretically, we derive an upper bound for the generalization error. Numerical experiments show that our method outperforms existing methods using other domain discrepancy measures when only small amounts of target data are available. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03942</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03942</id><created>2025-02-06</created><updated>2025-02-10</updated><authors><author><keyname>Holst</keyname><forenames>Klaus Kähler</forenames></author><author><keyname>Nordland</keyname><forenames>Andreas</forenames></author><author><keyname>Furberg</keyname><forenames>Julie</forenames></author><author><keyname>Damgaard</keyname><forenames>Lars Holm</forenames></author><author><keyname>Pipper</keyname><forenames>Christian Bressen</forenames></author></authors><title>A retake on the analysis of scores truncated by terminal events</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of data from randomized controlled trials in vulnerable populations requires special attention when assessing treatment effect by a score measuring, e.g., disease stage or activity together with onset of prevalent terminal events. In reality, it is impossible to disentangle a disease score from the terminal event, since the score is not clinically meaningful after this event. In this work, we propose to assess treatment interventions simultaneously on disease score and the terminal event. Our proposal is based on a natural data-generating mechanism respecting that a disease score does not exist beyond the terminal event. We use modern semi-parametric statistical methods to provide robust and efficient estimation of the risk of terminal event and expected disease score conditional on no terminal event at a pre-specified landmark time. We also use the simultaneous asymptotic behavior of our estimators to develop a powerful closed testing procedure for confirmatory assessment of treatment effect on both onset of terminal event and level of disease score. A simulation study mimicking a large-scale outcome trial in chronic kidney patients as well as an analysis of that trial is provided to assess performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04168</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04168</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Ferradini</keyname><forenames>Carla</forenames></author><author><keyname>Gitton</keyname><forenames>Victor</forenames></author><author><keyname>Vilasini</keyname><forenames>V.</forenames></author></authors><title>Cyclic quantum causal modelling with a graph separation theorem</title><categories>quant-ph math.ST stat.ML stat.TH</categories><comments>41+41 pages. A companion paper by the same authors, focussing on   cyclic classical (functional) causal models has been submitted to the arXiv   concurrently with primary class [math.ST], v2 only differs from v1 in   including the arXiv number of the companion paper. Comments are welcome</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Causal modelling frameworks link observable correlations to causal explanations, which is a crucial aspect of science. These models represent causal relationships through directed graphs, with vertices and edges denoting systems and transformations within a theory. Most studies focus on acyclic causal graphs, where well-defined probability rules and powerful graph-theoretic properties like the d-separation theorem apply. However, understanding complex feedback processes and exotic fundamental scenarios with causal loops requires cyclic causal models, where such results do not generally hold. While progress has been made in classical cyclic causal models, challenges remain in uniquely fixing probability distributions and identifying graph-separation properties applicable in general cyclic models. In cyclic quantum scenarios, existing frameworks have focussed on a subset of possible cyclic causal scenarios, with graph-separation properties yet unexplored. This work proposes a framework applicable to all consistent quantum and classical cyclic causal models on finite-dimensional systems. We address these challenges by introducing a robust probability rule and a novel graph-separation property, p-separation, which we prove to be sound and complete for all such models. Our approach maps cyclic causal models to acyclic ones with post-selection, leveraging the post-selected quantum teleportation protocol. We characterize these protocols and their success probabilities along the way. We also establish connections between this formalism and other classical and quantum frameworks to inform a more unified perspective on causality. This provides a foundation for more general cyclic causal discovery algorithms and to systematically extend open problems and techniques from acyclic informational networks (e.g., certification of non-classicality) to cyclic causal structures and networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04171</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04171</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Ferradini</keyname><forenames>Carla</forenames></author><author><keyname>Gitton</keyname><forenames>Victor</forenames></author><author><keyname>Vilasini</keyname><forenames>V.</forenames></author></authors><title>Cyclic functional causal models beyond unique solvability with a graph   separation theorem</title><categories>math.ST quant-ph stat.ML stat.TH</categories><comments>33+16 pages. A companion paper by the same authors, focussing on   cyclic quantum causal models has been submitted to the arXiv concurrently   with primary class [quant-ph], v2 only differs from v1 in including the arXiv   number of the companion paper. Comments are welcome</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Functional causal models (fCMs) specify functional dependencies between random variables associated to the vertices of a graph. In directed acyclic graphs (DAGs), fCMs are well-understood: a unique probability distribution on the random variables can be easily specified, and a crucial graph-separation result called the d-separation theorem allows one to characterize conditional independences between the variables. However, fCMs on cyclic graphs pose challenges due to the absence of a systematic way to assign a unique probability distribution to the fCM's variables, the failure of the d-separation theorem, and lack of a generalization of this theorem that is applicable to all consistent cyclic fCMs. In this work, we develop a causal modeling framework applicable to all cyclic fCMs involving finite-cardinality variables, except inconsistent ones admitting no solutions. Our probability rule assigns a unique distribution even to non-uniquely solvable cyclic fCMs and reduces to the known rule for uniquely solvable fCMs. We identify a class of fCMs, called averagely uniquely solvable, that we show to be the largest class where the probabilities admit a Markov factorization. Furthermore, we introduce a new graph-separation property, p-separation, and prove this to be sound and complete for all consistent finite-cardinality cyclic fCMs while recovering the d-separation theorem for DAGs. These results are obtained by considering classical post-selected teleportation protocols inspired by analogous protocols in quantum information theory. We discuss further avenues for exploration, linking in particular problems in cyclic fCMs and in quantum causality. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04208</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04208</id><created>2025-02-06</created><updated>2025-02-07</updated><authors><author><keyname>Grünwald</keyname><forenames>Peter D.</forenames></author><author><keyname>Koolen</keyname><forenames>Wouter M.</forenames></author></authors><title>Supermartingales for One-Sided Tests: Sufficient Monotone Likelihood   Ratios are Sufficient</title><categories>math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The t-statistic is a widely-used scale-invariant statistic for testing the null hypothesis that the mean is zero. Martingale methods enable sequential testing with the t-statistic at every sample size, while controlling the probability of falsely rejecting the null. For one-sided sequential tests, which reject when the t-statistic is too positive, a natural question is whether they also control false rejection when the true mean is negative. We prove that this is the case using monotone likelihood ratios and sufficient statistics. We develop applications to the scale-invariant t-test, the location-invariant $\chi^2$-test and sequential linear regression with nuisance covariates. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04339</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04339</id><created>2025-02-01</created><authors><author><keyname>George</keyname><forenames>Anand Jerry</forenames></author><author><keyname>Veiga</keyname><forenames>Rodrigo</forenames></author><author><keyname>Macris</keyname><forenames>Nicolas</forenames></author></authors><title>Analysis of Diffusion Models for Manifold Data</title><categories>math.ST cond-mat.dis-nn cs.IT cs.LG math.IT math.PR stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We analyze the time reversed dynamics of generative diffusion models. If the exact empirical score function is used in a regime of large dimension and exponentially large number of samples, these models are known to undergo transitions between distinct dynamical regimes. We extend this analysis and compute the transitions for an analytically tractable manifold model where the statistical model for the data is a mixture of lower dimensional Gaussians embedded in higher dimensional space. We compute the so-called speciation and collapse transition times, as a function of the ratio of manifold-to-ambient space dimensions, and other characteristics of the data model. An important tool used in our analysis is the exact formula for the mutual information (or free energy) of Generalized Linear Models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04372</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04372</id><created>2025-02-05</created><authors><author><keyname>Genari</keyname><forenames>Juliano</forenames></author><author><keyname>Goedert</keyname><forenames>Guilherme Tegoni</forenames></author></authors><title>Mining Unstructured Medical Texts With Conformal Active Learning</title><categories>cs.CL cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The extraction of relevant data from Electronic Health Records (EHRs) is crucial to identifying symptoms and automating epidemiological surveillance processes. By harnessing the vast amount of unstructured text in EHRs, we can detect patterns that indicate the onset of disease outbreaks, enabling faster, more targeted public health responses. Our proposed framework provides a flexible and efficient solution for mining data from unstructured texts, significantly reducing the need for extensive manual labeling by specialists. Experiments show that our framework achieving strong performance with as few as 200 manually labeled texts, even for complex classification problems. Additionally, our approach can function with simple lightweight models, achieving competitive and occasionally even better results compared to more resource-intensive deep learning models. This capability not only accelerates processing times but also preserves patient privacy, as the data can be processed on weaker on-site hardware rather than being transferred to external systems. Our methodology, therefore, offers a practical, scalable, and privacy-conscious approach to real-time epidemiological monitoring, equipping health institutions to respond rapidly and effectively to emerging health threats. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04422</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04422</id><created>2025-02-06</created><authors><author><keyname>Yadav</keyname><forenames>Pooja</forenames></author><author><keyname>Srivastava</keyname><forenames>Tanuja</forenames></author></authors><title>The Maximum Likelihood Degree of Farlie Gumbel Morgenstern Bivariate   Exponential Distribution</title><categories>math.ST math.AC stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum likelihood degree of a statistical model refers to the number of solutions, where the derivative of the log-likelihood function is zero, over the complex field. This paper examines the maximum likelihood degree of the parameter in Farlie-Gumbel-Morgenstern bivariate exponential distribution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04464</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04464</id><created>2025-02-06</created><authors><author><keyname>Jadoul</keyname><forenames>Yannick</forenames></author><author><keyname>Tufarelli</keyname><forenames>Tommaso</forenames></author><author><keyname>Coissac</keyname><forenames>Chloé</forenames></author><author><keyname>Gamba</keyname><forenames>Marco</forenames></author><author><keyname>Ravignani</keyname><forenames>Andrea</forenames></author></authors><title>Hidden assumptions of integer ratio analyses in bioacoustics and music</title><categories>stat.AP</categories><comments>19 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Rhythm is ubiquitous in human culture and in nature, but hard to capture in all its complexity. A key dimension of rhythm, integer ratio categories occur when the relationship between temporal intervals can be expressed as small-integer ratios. Recent work has found integer ratio categories in most human musical cultures and some animal species' vocalizations or behavioral displays. But biological systems are noisy, and empirically measured intervals rarely form an exact small-integer ratio. Here, we mathematically assess whether the leading integer ratio analysis method makes valid statistical and biological assumptions. In particular, we (1) make the temporal properties of empirical ratios explicit, both in general and for the typical use in the literature; (2) show how the choice of ratio formula affects the probability distribution of rhythm ratios and ensuing statistical results; (3) guide the reader to carefully consider the assumptions and null hypotheses of the statistical analysis; (4) present a comprehensive methodology to statistically test integer ratios for any null hypothesis of choice. Our observations have implications for both past and future research in music cognition and animal behavior: They suggest how to interpret past findings and provide tools to choose the correct null hypotheses in future empirical work. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04491</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04491</id><created>2025-02-06</created><authors><author><keyname>Cheng</keyname><forenames>Ziheng</forenames></author><author><keyname>Xie</keyname><forenames>Tianyu</forenames></author><author><keyname>Zhang</keyname><forenames>Shiyue</forenames></author><author><keyname>Zhang</keyname><forenames>Cheng</forenames></author></authors><title>Provable Sample-Efficient Transfer Learning Conditional Diffusion Models   via Representation Learning</title><categories>cs.LG math.ST stat.ML stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While conditional diffusion models have achieved remarkable success in various applications, they require abundant data to train from scratch, which is often infeasible in practice. To address this issue, transfer learning has emerged as an essential paradigm in small data regimes. Despite its empirical success, the theoretical underpinnings of transfer learning conditional diffusion models remain unexplored. In this paper, we take the first step towards understanding the sample efficiency of transfer learning conditional diffusion models through the lens of representation learning. Inspired by practical training procedures, we assume that there exists a low-dimensional representation of conditions shared across all tasks. Our analysis shows that with a well-learned representation from source tasks, the samplecomplexity of target tasks can be reduced substantially. In addition, we investigate the practical implications of our theoretical results in several real-world applications of conditional diffusion models. Numerical experiments are also conducted to verify our results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04543</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04543</id><created>2025-02-06</created><authors><author><keyname>Lu</keyname><forenames>Zhou</forenames></author><author><keyname>Sun</keyname><forenames>Y. Jennifer</forenames></author><author><keyname>Zhang</keyname><forenames>Zhiyu</forenames></author></authors><title>Sparsity-Based Interpolation of External, Internal and Swap Regret</title><categories>stat.ML cs.LG</categories><comments>Equal contribution, alphabetical order</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Focusing on the expert problem in online learning, this paper studies the interpolation of several performance metrics via $\phi$-regret minimization, which measures the performance of an algorithm by its regret with respect to an arbitrary action modification rule $\phi$. With $d$ experts and $T\gg d$ rounds in total, we present a single algorithm achieving the instance-adaptive $\phi$-regret bound \begin{equation*} \tilde O\left(\min\left\{\sqrt{d-d^{\mathrm{unif}}_\phi+1},\sqrt{d-d^{\mathrm{self}}_\phi}\right\}\cdot\sqrt{T}\right), \end{equation*} where $d^{\mathrm{unif}}_\phi$ is the maximum amount of experts modified identically by $\phi$, and $d^{\mathrm{self}}_\phi$ is the amount of experts that $\phi$ trivially modifies to themselves. By recovering the optimal $O(\sqrt{T\log d})$ external regret bound when $d^{\mathrm{unif}}_\phi=d$, the standard $\tilde O(\sqrt{T})$ internal regret bound when $d^{\mathrm{self}}_\phi=d-1$ and the optimal $\tilde O(\sqrt{dT})$ swap regret bound in the worst case, we improve existing results in the intermediate regimes. In addition, the same algorithm achieves the optimal quantile regret bound, which corresponds to even easier settings of $\phi$ than the external regret.   Building on the classical reduction from $\phi$-regret minimization to external regret minimization on stochastic matrices, our main idea is to further convert the latter to online linear regression using Haar-wavelet-inspired matrix features. Then, we apply a particular $L_1$-version of comparator-adaptive online learning algorithms to exploit the sparsity in this regression subroutine. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04550</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04550</id><created>2025-02-06</created><authors><author><keyname>Faes</keyname><forenames>Luca</forenames></author><author><keyname>Sparacino</keyname><forenames>Laura</forenames></author><author><keyname>Mijatovic</keyname><forenames>Gorana</forenames></author><author><keyname>Antonacci</keyname><forenames>Yuri</forenames></author><author><keyname>Ricci</keyname><forenames>Leonardo</forenames></author><author><keyname>Marinazzo</keyname><forenames>Daniele</forenames></author><author><keyname>Stramaglia</keyname><forenames>Sebastiano</forenames></author></authors><title>Partial Information Rate Decomposition</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Partial Information Decomposition (PID) is a principled and flexible method to unveil complex high-order interactions in multi-unit network systems. Though being defined exclusively for random variables, PID is ubiquitously applied to multivariate time series taken as realizations of random processes with temporal statistical structure. Here, to overcome the incorrect depiction of high-order effects by PID schemes applied to dynamic networks, we introduce the framework of Partial Information Rate Decomposition (PIRD). PIRD is formalized applying lattice theory to decompose the information shared dynamically between a target random process and a set of source processes, implemented for Gaussian processes through a spectral expansion of information rates, and demonstrated in practice analyzing time series from large-scale climate oscillations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04553</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04553</id><created>2025-02-06</created><authors><author><keyname>Swanson</keyname><forenames>David</forenames></author><author><keyname>Sherry</keyname><forenames>Alexander</forenames></author><author><keyname>Tang</keyname><forenames>Chad</forenames></author></authors><title>Variance component mixture modelling for longitudinal T-cell receptor   clonal dynamics</title><categories>stat.ME</categories><msc-class>62F15 (Primary), 62H30 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Studies of T cells and their clonally unique receptors have shown promise in elucidating the association between immune response and human disease. Methods to identify T-cell receptor clones which expand or contract in response to certain therapeutic strategies have so far been limited to longitudinal pairwise comparisons of clone frequency with multiplicity adjustment. Here we develop a more general mixture model approach for arbitrary follow-up and missingness which partitions dynamic longitudinal clone frequency behavior from static. While it is common to mix on the location or scale parameter of a family of distributions, the model instead mixes on the parameterization itself, the dynamic component allowing for a variable, Gamma-distributed Poisson mean parameter over longitudinal follow-up, while the static component mean is time invariant. Leveraging conjugacy, one can integrate out the mean parameter for the dynamic and static components to yield distinct posterior predictive distributions whose expressions are a product of negative binomials and a single negative multinomial, respectively, each modified according to an offset for receptor read count normalization. An EM-algorithm is developed to estimate hyperparameters and component membership, and validity of the approach is demonstrated in simulation. The model identifies a statistically significant and clinically relevant increase in TCR clonal dynamism among metastasis-directed radiation therapy in a cohort of prostate cancer patients. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04555</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04555</id><created>2025-02-06</created><authors><author><keyname>Sparacino</keyname><forenames>Laura</forenames></author><author><keyname>Mijatovic</keyname><forenames>Gorana</forenames></author><author><keyname>Antonacci</keyname><forenames>Yuri</forenames></author><author><keyname>Ricci</keyname><forenames>Leonardo</forenames></author><author><keyname>Marinazzo</keyname><forenames>Daniele</forenames></author><author><keyname>Stramaglia</keyname><forenames>Sebastiano</forenames></author><author><keyname>Faes</keyname><forenames>Luca</forenames></author></authors><title>Decomposing Multivariate Information Rates in Networks of Random   Processes</title><categories>stat.ME cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The Partial Information Decomposition (PID) framework has emerged as a powerful tool for analyzing high-order interdependencies in complex network systems. However, its application to dynamic processes remains challenging due to the implicit assumption of memorylessness, which often falls in real-world scenarios. In this work, we introduce the framework of Partial Information Rate Decomposition (PIRD) that extends PID to random processes with temporal correlations. By leveraging mutual information rate (MIR) instead of mutual information (MI), our approach decomposes the dynamic information shared by multivariate random processes into unique, redundant, and synergistic contributions obtained aggregating information rate atoms in a principled manner. To solve PIRD, we define a pointwise redundancy rate function based on the minimum MI principle applied locally in the frequency-domain representation of the processes. The framework is validated in benchmark simulations of Gaussian systems, demonstrating its advantages over traditional PID in capturing temporal correlations and showing how the spectral representation may reveal scale-specific higher-order interaction that are obscured in the time domain. Furthermore, we apply PIRD to a physiological network comprising cerebrovascular and cardiovascular variables, revealing frequency-dependent redundant information exchange during a protocol of postural stress. Our results highlight the necessity of accounting for the full temporal statistical structure and spectral content of vector random processes to meaningfully perform information decomposition in network systems with dynamic behavior such as those typically encountered in neuroscience and physiology. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04574</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04574</id><created>2025-02-06</created><authors><author><keyname>Gong</keyname><forenames>ZhuQing</forenames></author><author><keyname>Zuo</keyname><forenames>XiNian</forenames></author></authors><title>Dark Brain Energy: Toward an Integrative Model of Spontaneous Slow   Oscillations</title><categories>q-bio.NC cs.IT math.IT stat.AP</categories><comments>38 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Neural oscillations facilitate the functioning of the human brain in spatial and temporal dimensions at various frequencies. These oscillations feature a universal frequency architecture that is governed by brain anatomy, ensuring frequency specificity remains invariant across different measurement techniques. Initial magnetic resonance imaging (MRI) methodology constrained functional MRI (fMRI) investigations to a singular frequency range, thereby neglecting the frequency characteristics inherent in blood oxygen level-dependent oscillations. With advancements in MRI technology, it has become feasible to decode intricate brain activities via multi-band frequency analysis (MBFA). During the past decade, the utilization of MBFA in fMRI studies has surged, unveiling frequency-dependent characteristics of spontaneous slow oscillations (SSOs) believed to base dark energy in the brain. There remains a dearth of conclusive insights and hypotheses pertaining to the properties and functionalities of SSOs in distinct bands. We surveyed the SSO MBFA studies during the past 15 years to delineate the attributes of SSOs and enlighten their correlated functions. We further proposed a model to elucidate the hierarchical organization of multi-band SSOs by integrating their function, aimed at bridging theoretical gaps and guiding future MBFA research endeavors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04575</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04575</id><created>2025-02-06</created><authors><author><keyname>Guo</keyname><forenames>Wei</forenames></author><author><keyname>Tao</keyname><forenames>Molei</forenames></author><author><keyname>Chen</keyname><forenames>Yongxin</forenames></author></authors><title>Complexity Analysis of Normalizing Constant Estimation: from Jarzynski   Equality to Annealed Importance Sampling and beyond</title><categories>stat.ML cs.LG cs.NA math.NA physics.comp-ph stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Given an unnormalized probability density $\pi\propto\mathrm{e}^{-V}$, estimating its normalizing constant $Z=\int_{\mathbb{R}^d}\mathrm{e}^{-V(x)}\mathrm{d}x$ or free energy $F=-\log Z$ is a crucial problem in Bayesian statistics, statistical mechanics, and machine learning. It is challenging especially in high dimensions or when $\pi$ is multimodal. To mitigate the high variance of conventional importance sampling estimators, annealing-based methods such as Jarzynski equality and annealed importance sampling are commonly adopted, yet their quantitative complexity guarantees remain largely unexplored. We take a first step toward a non-asymptotic analysis of annealed importance sampling. In particular, we derive an oracle complexity of $\widetilde{O}\left(\frac{d\beta^2{\mathcal{A}}^2}{\varepsilon^4}\right)$ for estimating $Z$ within $\varepsilon$ relative error with high probability, where $\beta$ is the smoothness of $V$ and $\mathcal{A}$ denotes the action of a curve of probability measures interpolating $\pi$ and a tractable reference distribution. Our analysis, leveraging Girsanov theorem and optimal transport, does not explicitly require isoperimetric assumptions on the target distribution. Finally, to tackle the large action of the widely used geometric interpolation of probability distributions, we propose a new normalizing constant estimation algorithm based on reverse diffusion samplers and establish a framework for analyzing its complexity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04591</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04591</id><created>2025-02-06</created><authors><author><keyname>Deidda</keyname><forenames>Piero</forenames></author><author><keyname>Zhang</keyname><forenames>Kaicheng</forenames></author><author><keyname>Higham</keyname><forenames>Desmond</forenames></author><author><keyname>Tudisco</keyname><forenames>Francesco</forenames></author></authors><title>Rethinking Oversmoothing in Graph Neural Networks: A Rank-Based   Perspective</title><categories>cs.LG cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Oversmoothing is a fundamental challenge in graph neural networks (GNNs): as the number of layers increases, node embeddings become increasingly similar, and model performance drops sharply. Traditionally, oversmoothing has been quantified using metrics that measure the similarity of neighbouring node features, such as the Dirichlet energy. While these metrics are related to oversmoothing, we argue they have critical limitations and fail to reliably capture oversmoothing in realistic scenarios. For instance, they provide meaningful insights only for very deep networks and under somewhat strict conditions on the norm of network weights and feature representations. As an alternative, we propose measuring oversmoothing by examining the numerical or effective rank of the feature representations. We provide theoretical support for this approach, demonstrating that the numerical rank of feature representations converges to one for a broad family of nonlinear activation functions under the assumption of nonnegative trained weights. To the best of our knowledge, this is the first result that proves the occurrence of oversmoothing without assumptions on the boundedness of the weight matrices. Along with the theoretical findings, we provide extensive numerical evaluation across diverse graph architectures. Our results show that rank-based metrics consistently capture oversmoothing, whereas energy-based metrics often fail. Notably, we reveal that a significant drop in the rank aligns closely with performance degradation, even in scenarios where energy metrics remain unchanged. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04593</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04593</id><created>2025-02-06</created><authors><author><keyname>Rezaei</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Dieng</keyname><forenames>Adji Bousso</forenames></author></authors><title>The $\alpha$-Alternator: Dynamic Adaptation To Varying Noise Levels In   Sequences Using The Vendi Score For Improved Robustness and Performance</title><categories>cs.LG cs.AI cs.NE stat.ML</categories><comments>The codebase will be made available upon publication. This paper is   dedicated to Patrice Lumumba</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Current state-of-the-art dynamical models, such as Mamba, assume the same level of noisiness for all elements of a given sequence, which limits their performance on noisy temporal data. In this paper, we introduce the $\alpha$-Alternator, a novel generative model for time-dependent data that dynamically adapts to the complexity introduced by varying noise levels in sequences. The $\alpha$-Alternator leverages the Vendi Score (VS), a flexible similarity-based diversity metric, to adjust, at each time step $t$, the influence of the sequence element at time $t$ and the latent representation of the dynamics up to that time step on the predicted future dynamics. This influence is captured by a parameter that is learned and shared across all sequences in a given dataset. The sign of this parameter determines the direction of influence. A negative value indicates a noisy dataset, where a sequence element that increases the VS is considered noisy, and the model relies more on the latent history when processing that element. Conversely, when the parameter is positive, a sequence element that increases the VS is considered informative, and the $\alpha$-Alternator relies more on this new input than on the latent history when updating its predicted latent dynamics. The $\alpha$-Alternator is trained using a combination of observation masking and Alternator loss minimization. Masking simulates varying noise levels in sequences, enabling the model to be more robust to these fluctuations and improving its performance in trajectory prediction, imputation, and forecasting. Our experimental results demonstrate that the $\alpha$-Alternator outperforms both Alternators and state-of-the-art state-space models across neural decoding and time-series forecasting benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04654</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04654</id><created>2025-02-06</created><authors><author><keyname>Lim</keyname><forenames>Keunwoo</forenames></author><author><keyname>Ye</keyname><forenames>Ting</forenames></author><author><keyname>Han</keyname><forenames>Fang</forenames></author></authors><title>A sliced Wasserstein and diffusion approach to random coefficient models</title><categories>math.ST econ.EM stat.TH</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new minimum-distance estimator for linear random coefficient models. This estimator integrates the recently advanced sliced Wasserstein distance with the nearest neighbor methods, both of which enhance computational efficiency. We demonstrate that the proposed method is consistent in approximating the true distribution. Additionally, our formulation encourages a diffusion process-based algorithm, which holds independent interest and potential for broader applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04673</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04673</id><created>2025-02-07</created><authors><author><keyname>Neopane</keyname><forenames>Ojash</forenames></author><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author><author><keyname>Singh</keyname><forenames>Aarti</forenames></author></authors><title>Optimistic Algorithms for Adaptive Estimation of the Average Treatment   Effect</title><categories>stat.ML cs.LG stat.ME</categories><comments>15 pages, 2 Figures</comments><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Estimation and inference for the Average Treatment Effect (ATE) is a cornerstone of causal inference and often serves as the foundation for developing procedures for more complicated settings. Although traditionally analyzed in a batch setting, recent advances in martingale theory have paved the way for adaptive methods that can enhance the power of downstream inference. Despite these advances, progress in understanding and developing adaptive algorithms remains in its early stages. Existing work either focus on asymptotic analyses that overlook exploration-exploitation tradeoffs relevant in finite-sample regimes or rely on simpler but suboptimal estimators. In this work, we address these limitations by studying adaptive sampling procedures that take advantage of the asymptotically optimal Augmented Inverse Probability Weighting (AIPW) estimator. Our analysis uncovers challenges obscured by asymptotic approaches and introduces a novel algorithmic design principle reminiscent of optimism in multiarmed bandits. This principled approach enables our algorithm to achieve significant theoretical and empirical gains compared to prior methods. Our findings mark a step forward in advancing adaptive causal inference methods in theory and practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04681</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04681</id><created>2025-02-07</created><authors><author><keyname>Louit</keyname><forenames>Sydney</forenames></author><author><keyname>Clark</keyname><forenames>Evan</forenames></author><author><keyname>Gelbard</keyname><forenames>Alexander</forenames></author><author><keyname>Vivek</keyname><forenames>Niketna</forenames></author><author><keyname>Yan</keyname><forenames>Jun</forenames></author><author><keyname>Zhang</keyname><forenames>Panpan</forenames></author></authors><title>CALF-SBM: A Covariate-Assisted Latent Factor Stochastic Block Model</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel network generative model extended from the standard stochastic block model by concurrently utilizing observed node-level information and accounting for network-enabled nodal heterogeneity. The proposed model is so so-called covariate-assisted latent factor stochastic block model (CALF-SBM). The inference for the proposed model is done in a fully Bayesian framework. The primary application of CALF-SBM in the present research is focused on community detection, where a model-selection-based approach is employed to estimate the number of communities which is practically assumed unknown. To assess the performance of CALF-SBM, an extensive simulation study is carried out, including comparisons with multiple classical and modern network clustering algorithms. Lastly, the paper presents two real data applications, respectively based on an extremely new network data demonstrating collaborative relationships of otolaryngologists in the United States and a traditional aviation network data containing information about direct flights between airports in the United States and Canada. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04685</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04685</id><created>2025-02-07</created><authors><author><keyname>Zhang</keyname><forenames>Likun</forenames></author><author><keyname>Bhaganagar</keyname><forenames>Kiran</forenames></author><author><keyname>Wikle</keyname><forenames>Christopher K.</forenames></author></authors><title>Capturing Extreme Events in Turbulence using an Extreme Variational   Autoencoder (xVAE)</title><categories>physics.flu-dyn stat.AP stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Turbulent flow fields are characterized by extreme events that are statistically intermittent and carry a significant amount of energy and physical importance. To emulate these flows, we introduce the extreme variational Autoencoder (xVAE), which embeds a max-infinitely divisible process with heavy-tailed distributions into a standard VAE framework, enabling accurate modeling of extreme events. xVAEs are neural network models that reduce system dimensionality by learning non-linear latent representations of data. We demonstrate the effectiveness of xVAE in large-eddy simulation data of wildland fire plumes, where intense heat release and complex plume-atmosphere interactions generate extreme turbulence. Comparisons with the commonly used Proper Orthogonal Decomposition (POD) modes show that xVAE is more robust in capturing extreme values and provides a powerful uncertainty quantification framework using variational Bayes. Additionally, xVAE enables analysis of the so-called copulas of fields to assess risks associated with rare events while rigorously accounting for uncertainty, such as simultaneous exceedances of high thresholds across multiple locations. The proposed approach provides a new direction for studying realistic turbulent flows, such as high-speed aerodynamics, space propulsion, and atmospheric and oceanic systems that are characterized by extreme events. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04699</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04699</id><created>2025-02-07</created><authors><author><keyname>Lan</keyname><forenames>Hui</forenames></author><author><keyname>Chang</keyname><forenames>Haoge</forenames></author><author><keyname>Dillon</keyname><forenames>Eleanor</forenames></author><author><keyname>Syrgkanis</keyname><forenames>Vasilis</forenames></author></authors><title>A Meta-learner for Heterogeneous Effects in Difference-in-Differences</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We address the problem of estimating heterogeneous treatment effects in panel data, adopting the popular Difference-in-Differences (DiD) framework under the conditional parallel trends assumption. We propose a novel doubly robust meta-learner for the Conditional Average Treatment Effect on the Treated (CATT), reducing the estimation to a convex risk minimization problem involving a set of auxiliary models. Our framework allows for the flexible estimation of the CATT, when conditioning on any subset of variables of interest using generic machine learning. Leveraging Neyman orthogonality, our proposed approach is robust to estimation errors in the auxiliary models. As a generalization to our main result, we develop a meta-learning approach for the estimation of general conditional functionals under covariate shift. We also provide an extension to the instrumented DiD setting with non-compliance. Empirical results demonstrate the superiority of our approach over existing baselines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04709</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04709</id><created>2025-02-07</created><authors><author><keyname>Miftachov</keyname><forenames>Ratmir</forenames></author><author><keyname>Reiß</keyname><forenames>Markus</forenames></author></authors><title>Early Stopping for Regression Trees</title><categories>math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We develop early stopping rules for growing regression tree estimators. The fully data-driven stopping rule is based on monitoring the global residual norm. The best-first search and the breadth-first search algorithms together with linear interpolation give rise to generalized projection or regularization flows. A general theory of early stopping is established. Oracle inequalities for the early-stopped regression tree are derived without any smoothness assumption on the regression function, assuming the original CART splitting rule, yet with a much broader scope. The remainder terms are of smaller order than the best achievable rates for Lipschitz functions in dimension $d\ge 2$. In real and synthetic data the early stopping regression tree estimators attain the statistical performance of cost-complexity pruning while significantly reducing computational costs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04723</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04723</id><created>2025-02-07</created><authors><author><keyname>Lyu</keyname><forenames>Ziyang</forenames></author><author><keyname>Sisson</keyname><forenames>S. A.</forenames></author><author><keyname>Welsh</keyname><forenames>A. H.</forenames></author></authors><title>Asymptotics for EBLUPs within crossed mixed effect models</title><categories>stat.ME</categories><comments>33 pages</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  In this article, we derive the joint asymptotic distribution of empirical best linear unbiased predictors (EBLUPs) for individual and cell-level random effects in a crossed mixed effect model. Under mild conditions (which include moment conditions instead of normality for the random effects and model errors), we demonstrate that as the sizes of rows, columns, and, when we include interactions, cells simultaneously increase to infinity, the distribution of the differences between the EBLUPs and the random effects satisfy central limit theorems. These central limit theorems mean the EBLUPs asymptotically follow the convolution of the true random effect distribution and a normal distribution. Moreover, our results enable simple asymptotic approximations and estimators for the mean squared error (MSE) of the EBLUPs, which in turn facilitates the construction of asymptotic prediction intervals for the unobserved random effects. We show in simulations that our simple estimator of the MSE of the EBLUPs works very well in finite samples. Finally, we illustrate the use of the asymptotic prediction intervals with an analysis of movie rating data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04730</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04730</id><created>2025-02-07</created><authors><author><keyname>Xie</keyname><forenames>Tianyu</forenames></author><author><keyname>Richman</keyname><forenames>Harry</forenames></author><author><keyname>Gao</keyname><forenames>Jiansi</forenames></author><author><keyname>Matsen</keyname><forenames>Frederick A.</forenames><suffix>IV</suffix></author><author><keyname>Zhang</keyname><forenames>Cheng</forenames></author></authors><title>PhyloVAE: Unsupervised Learning of Phylogenetic Trees via Variational   Autoencoders</title><categories>stat.ML cs.LG q-bio.PE</categories><comments>ICLR 2025. 22 pages, 14 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Learning informative representations of phylogenetic tree structures is essential for analyzing evolutionary relationships. Classical distance-based methods have been widely used to project phylogenetic trees into Euclidean space, but they are often sensitive to the choice of distance metric and may lack sufficient resolution. In this paper, we introduce phylogenetic variational autoencoders (PhyloVAEs), an unsupervised learning framework designed for representation learning and generative modeling of tree topologies. Leveraging an efficient encoding mechanism inspired by autoregressive tree topology generation, we develop a deep latent-variable generative model that facilitates fast, parallelized topology generation. PhyloVAE combines this generative model with a collaborative inference model based on learnable topological features, allowing for high-resolution representations of phylogenetic tree samples. Extensive experiments demonstrate PhyloVAE's robust representation learning capabilities and fast generation of phylogenetic tree topologies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04750</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04750</id><created>2025-02-07</created><authors><author><keyname>Bui</keyname><forenames>Thang D.</forenames></author><author><keyname>Ashman</keyname><forenames>Matthew</forenames></author><author><keyname>Turner</keyname><forenames>Richard E.</forenames></author></authors><title>Tighter sparse variational Gaussian processes</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sparse variational Gaussian process (GP) approximations based on inducing points have become the de facto standard for scaling GPs to large datasets, owing to their theoretical elegance, computational efficiency, and ease of implementation. This paper introduces a provably tighter variational approximation by relaxing the standard assumption that the conditional approximate posterior given the inducing points must match that in the prior. The key innovation is to modify the conditional posterior to have smaller variances than that of the prior at the training points. We derive the collapsed bound for the regression case, describe how to use the proposed approximation in large data settings, and discuss its application to handle orthogonally structured inducing points and GP latent variable models. Extensive experiments on regression benchmarks, classification, and latent variable models demonstrate that the proposed approximation consistently matches or outperforms standard sparse variational GPs while maintaining the same computational cost. An implementation will be made available in all popular GP packages. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04793</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04793</id><created>2025-02-07</created><authors><author><keyname>Jeunen</keyname><forenames>Olivier</forenames></author></authors><title>$t$-Testing the Waters: Empirically Validating Assumptions for Reliable   A/B-Testing</title><categories>stat.ME cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A/B-tests are a cornerstone of experimental design on the web, with wide-ranging applications and use-cases. The statistical $t$-test comparing differences in means is the most commonly used method for assessing treatment effects, often justified through the Central Limit Theorem (CLT). The CLT ascertains that, as the sample size grows, the sampling distribution of the Average Treatment Effect converges to normality, making the $t$-test valid for sufficiently large sample sizes. When outcome measures are skewed or non-normal, quantifying what "sufficiently large" entails is not straightforward.   To ensure that confidence intervals maintain proper coverage and that $p$-values accurately reflect the false positive rate, it is critical to validate this normality assumption. We propose a practical method to test this, by analysing repeatedly resampled A/A-tests. When the normality assumption holds, the resulting $p$-value distribution should be uniform, and this property can be tested using the Kolmogorov-Smirnov test. This provides an efficient and effective way to empirically assess whether the $t$-test's assumptions are met, and the A/B-test is valid. We demonstrate our methodology and highlight how it helps to identify scenarios prone to inflated Type-I errors. Our approach provides a practical framework to ensure and improve the reliability and robustness of A/B-testing practices. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04807</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04807</id><created>2025-02-07</created><authors><author><keyname>Bashari</keyname><forenames>Meshi</forenames></author><author><keyname>Sesia</keyname><forenames>Matteo</forenames></author><author><keyname>Romano</keyname><forenames>Yaniv</forenames></author></authors><title>Robust Conformal Outlier Detection under Contaminated Reference Data</title><categories>stat.ML cs.LG stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conformal prediction is a flexible framework for calibrating machine learning predictions, providing distribution-free statistical guarantees. In outlier detection, this calibration relies on a reference set of labeled inlier data to control the type-I error rate. However, obtaining a perfectly labeled inlier reference set is often unrealistic, and a more practical scenario involves access to a contaminated reference set containing a small fraction of outliers. This paper analyzes the impact of such contamination on the validity of conformal methods. We prove that under realistic, non-adversarial settings, calibration on contaminated data yields conservative type-I error control, shedding light on the inherent robustness of conformal methods. This conservativeness, however, typically results in a loss of power. To alleviate this limitation, we propose a novel, active data-cleaning framework that leverages a limited labeling budget and an outlier detection model to selectively annotate data points in the contaminated reference set that are suspected as outliers. By removing only the annotated outliers in this ``suspicious'' subset, we can effectively enhance power while mitigating the risk of inflating the type-I error rate, as supported by our theoretical analysis. Experiments on real datasets validate the conservative behavior of conformal methods under contamination and show that the proposed data-cleaning strategy improves power without sacrificing validity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04824</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04824</id><created>2025-02-07</created><authors><author><keyname>Blake</keyname><forenames>Joshua</forenames></author><author><keyname>Birrell</keyname><forenames>Paul</forenames></author><author><keyname>Walker</keyname><forenames>A. Sarah</forenames></author><author><keyname>Pouwels</keyname><forenames>Koen B.</forenames></author><author><keyname>House</keyname><forenames>Thomas</forenames></author><author><keyname>Tom</keyname><forenames>Brian D. M.</forenames></author><author><keyname>Kypraios</keyname><forenames>Theodore</forenames></author><author><keyname>De Angelis</keyname><forenames>Daniela</forenames></author></authors><title>Estimating the duration of RT-PCR positivity for SARS-CoV-2 from doubly   interval censored data with undetected infections</title><categories>stat.ME stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Monitoring the incidence of new infections during a pandemic is critical for an effective public health response. General population prevalence surveys for SARS-CoV-2 can provide high-quality data to estimate incidence. However, estimation relies on understanding the distribution of the duration that infections remain detectable. This study addresses this need using data from the Coronavirus Infection Survey (CIS), a long-term, longitudinal, general population survey conducted in the UK. Analyzing these data presents unique challenges, such as doubly interval censoring, undetected infections, and false negatives. We propose a Bayesian nonparametric survival analysis approach, estimating a discrete-time distribution of durations and integrating prior information derived from a complementary study. Our methodology is validated through a simulation study, including its resilience to model misspecification, and then applied to the CIS dataset. This results in the first estimate of the full duration distribution in a general population, as well as methodology that could be transferred to new contexts. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04832</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04832</id><created>2025-02-07</created><authors><author><keyname>Ballarin</keyname><forenames>Giovanni</forenames></author><author><keyname>Grigoryeva</keyname><forenames>Lyudmila</forenames></author><author><keyname>Ortega</keyname><forenames>Juan-Pablo</forenames></author></authors><title>Memory Capacity of Nonlinear Recurrent Networks: Is it Informative?</title><categories>cs.LG stat.ML</categories><comments>8 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The total memory capacity (MC) of linear recurrent neural networks (RNNs) has been proven to be equal to the rank of the corresponding Kalman controllability matrix, and it is almost surely maximal for connectivity and input weight matrices drawn from regular distributions. This fact questions the usefulness of this metric in distinguishing the performance of linear RNNs in the processing of stochastic signals. This note shows that the MC of random nonlinear RNNs yields arbitrary values within established upper and lower bounds depending just on the input process scale. This confirms that the existing definition of MC in linear and nonlinear cases has no practical value. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04849</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04849</id><created>2025-02-07</created><authors><author><keyname>Yu</keyname><forenames>Yifeng</forenames></author><author><keyname>Yu</keyname><forenames>Lu</forenames></author></authors><title>Advancing Wasserstein Convergence Analysis of Score-Based Models:   Insights from Discretization and Second-Order Acceleration</title><categories>stat.ML cs.LG math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Score-based diffusion models have emerged as powerful tools in generative modeling, yet their theoretical foundations remain underexplored. In this work, we focus on the Wasserstein convergence analysis of score-based diffusion models. Specifically, we investigate the impact of various discretization schemes, including Euler discretization, exponential integrators, and midpoint randomization methods. Our analysis provides a quantitative comparison of these discrete approximations, emphasizing their influence on convergence behavior. Furthermore, we explore scenarios where Hessian information is available and propose an accelerated sampler based on the local linearization method. We demonstrate that this Hessian-based approach achieves faster convergence rates of order $\widetilde{\mathcal{O}}\left(\frac{1}{\varepsilon}\right)$ significantly improving upon the standard rate $\widetilde{\mathcal{O}}\left(\frac{1}{\varepsilon^2}\right)$ of vanilla diffusion models, where $\varepsilon$ denotes the target accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04867</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04867</id><created>2025-02-07</created><authors><author><keyname>Maclaren</keyname><forenames>Oliver J.</forenames></author><author><keyname>Nicholson</keyname><forenames>Ruanui</forenames></author><author><keyname>Trent</keyname><forenames>Joel A.</forenames></author><author><keyname>Rottenberry</keyname><forenames>Joshua</forenames></author><author><keyname>Simpson</keyname><forenames>Matthew</forenames></author></authors><title>Invariant Image Reparameterisation: A Unified Approach to Structural and   Practical Identifiability and Model Reduction</title><categories>stat.AP</categories><comments>25 pages + supplementary material</comments><msc-class>62F99 (Primary) 65L09, 93B30, 62F12, 34A55 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Both structural and practical parameter non-identifiability present fundamental challenges when using mathematical models to interpret data. This issue is particularly acute in complex, applied areas such as the life sciences or engineering, where determining appropriate model complexity is challenging. While several approaches exist for diagnosing and resolving parameter non-identifiability, including symbolic methods, profile likelihood analysis, and sloppiness analysis, these approaches have distinct limitations and are rarely combined. We present an integrated approach called Invariant Image Reparameterisation (IIR) that incorporates key elements of these methods in a new way. Our approach replaces symbolic computations with numerical calculations at a single reference estimate and an invariance condition that determines when this local calculation holds globally. Parameter combinations determined by this method are naturally ordered by degree of identifiability, and this supports model reduction by replacing a practically non-identified model with a structurally non-identified approximate model. This approximate model can be further parameterised in terms of identified parameters only. By treating parameter combinations determined by our approach as interest parameters within our established likelihood-based Profile-Wise Analysis (PWA) framework, we incorporate uncertainty quantification in terms of likelihood profiles and confidence sets. We provide a Julia library on GitHub (https://github.com/omaclaren/reparam) demonstrating our methodology across a range of mathematical models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04879</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04879</id><created>2025-02-07</created><authors><author><keyname>Gauthier</keyname><forenames>Etienne</forenames></author><author><keyname>Bach</keyname><forenames>Francis</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Statistical Collusion by Collectives on Learning Platforms</title><categories>stat.ML cs.LG</categories><comments>Code available at: https://github.com/GauthierE/statistical-collusion</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As platforms increasingly rely on learning algorithms, collectives may form and seek ways to influence these platforms to align with their own interests. This can be achieved by coordinated submission of altered data. To evaluate the potential impact of such behavior, it is essential to understand the computations that collectives must perform to impact platforms in this way. In particular, collectives need to make a priori assessments of the effect of the collective before taking action, as they may face potential risks when modifying their data. Moreover they need to develop implementable coordination algorithms based on quantities that can be inferred from observed data. We develop a framework that provides a theoretical and algorithmic treatment of these issues and present experimental results in a product evaluation domain. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04889</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04889</id><created>2025-02-07</created><authors><author><keyname>Bao</keyname><forenames>Han</forenames></author><author><keyname>Sakaue</keyname><forenames>Shinsaku</forenames></author><author><keyname>Takezawa</keyname><forenames>Yuki</forenames></author></authors><title>Any-stepsize Gradient Descent for Separable Data under Fenchel--Young   Losses</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The gradient descent (GD) has been one of the most common optimizer in machine learning. In particular, the loss landscape of a neural network is typically sharpened during the initial phase of training, making the training dynamics hover on the edge of stability. This is beyond our standard understanding of GD convergence in the stable regime where arbitrarily chosen stepsize is sufficiently smaller than the edge of stability. Recently, Wu et al. (COLT2024) have showed that GD converges with arbitrary stepsize under linearly separable logistic regression. Although their analysis hinges on the self-bounding property of the logistic loss, which seems to be a cornerstone to establish a modified descent lemma, our pilot study shows that other loss functions without the self-bounding property can make GD converge with arbitrary stepsize. To further understand what property of a loss function matters in GD, we aim to show arbitrary-stepsize GD convergence for a general loss function based on the framework of \emph{Fenchel--Young losses}. We essentially leverage the classical perceptron argument to derive the convergence rate for achieving $\epsilon$-optimal loss, which is possible for a majority of Fenchel--Young losses. Among typical loss functions, the Tsallis entropy achieves the GD convergence rate $T=\Omega(\epsilon^{-1/2})$, and the R{\'e}nyi entropy achieves the far better rate $T=\Omega(\epsilon^{-1/3})$. We argue that these better rate is possible because of \emph{separation margin} of loss functions, instead of the self-bounding property. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04891</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04891</id><created>2025-02-07</created><authors><author><keyname>Rubio-Madrigal</keyname><forenames>Celia</forenames></author><author><keyname>Jamadandi</keyname><forenames>Adarsh</forenames></author><author><keyname>Burkholz</keyname><forenames>Rebekka</forenames></author></authors><title>GNNs Getting ComFy: Community and Feature Similarity Guided Rewiring</title><categories>cs.LG cs.SI stat.ML</categories><comments>Accepted at ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Maximizing the spectral gap through graph rewiring has been proposed to enhance the performance of message-passing graph neural networks (GNNs) by addressing over-squashing. However, as we show, minimizing the spectral gap can also improve generalization. To explain this, we analyze how rewiring can benefit GNNs within the context of stochastic block models. Since spectral gap optimization primarily influences community strength, it improves performance when the community structure aligns with node labels. Building on this insight, we propose three distinct rewiring strategies that explicitly target community structure, node labels, and their alignment: (a) community structure-based rewiring (ComMa), a more computationally efficient alternative to spectral gap optimization that achieves similar goals; (b) feature similarity-based rewiring (FeaSt), which focuses on maximizing global homophily; and (c) a hybrid approach (ComFy), which enhances local feature similarity while preserving community structure to optimize label-community alignment. Extensive experiments confirm the effectiveness of these strategies and support our theoretical insights. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04892</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04892</id><created>2025-02-07</created><authors><author><keyname>Park</keyname><forenames>Joonhyeong</forenames></author><author><keyname>Park</keyname><forenames>Byoungwoo</forenames></author><author><keyname>Bang</keyname><forenames>Chang-Bae</forenames></author><author><keyname>Choi</keyname><forenames>Jungwon</forenames></author><author><keyname>Chung</keyname><forenames>Hyungjin</forenames></author><author><keyname>Kim</keyname><forenames>Byung-Hoon</forenames></author><author><keyname>Lee</keyname><forenames>Juho</forenames></author></authors><title>A Foundational Brain Dynamics Model via Stochastic Optimal Control</title><categories>cs.LG q-bio.NC stat.ML</categories><comments>The first two authors contributed equally</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a foundational model for brain dynamics that utilizes stochastic optimal control (SOC) and amortized inference. Our method features a continuous-discrete state space model (SSM) that can robustly handle the intricate and noisy nature of fMRI signals. To address computational limitations, we implement an approximation strategy grounded in the SOC framework. Additionally, we present a simulation-free latent dynamics approach that employs locally linear approximations, facilitating efficient and scalable inference. For effective representation learning, we derive an Evidence Lower Bound (ELBO) from the SOC formulation, which integrates smoothly with recent advancements in self-supervised learning (SSL), thereby promoting robust and transferable representations. Pre-trained on extensive datasets such as the UKB, our model attains state-of-the-art results across a variety of downstream tasks, including demographic prediction, trait analysis, disease diagnosis, and prognosis. Moreover, evaluating on external datasets such as HCP-A, ABIDE, and ADHD200 further validates its superior abilities and resilience across different demographic and clinical distributions. Our foundational model provides a scalable and efficient approach for deciphering brain dynamics, opening up numerous applications in neuroscience. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04907</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04907</id><created>2025-02-07</created><authors><author><keyname>Gachon</keyname><forenames>Erell</forenames></author><author><keyname>Bigot</keyname><forenames>Jérémie</forenames></author><author><keyname>Cazelles</keyname><forenames>Elsa</forenames></author></authors><title>Scalable and consistent embedding of probability measures into Hilbert   spaces via measure quantization</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper is focused on statistical learning from data that come as probability measures. In this setting, popular approaches consist in embedding such data into a Hilbert space with either Linearized Optimal Transport or Kernel Mean Embedding. However, the cost of computing such embeddings prohibits their direct use in large-scale settings. We study two methods based on measure quantization for approximating input probability measures with discrete measures of small-support size. The first one is based on optimal quantization of each input measure, while the second one relies on mean-measure quantization. We study the consistency of such approximations, and its implication for scalable embeddings of probability measures into a Hilbert space at a low computational cost. We finally illustrate our findings with various numerical experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04938</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04938</id><created>2025-02-07</created><authors><author><keyname>Gardini</keyname><forenames>Aldo</forenames></author><author><keyname>Greco</keyname><forenames>Fedele</forenames></author><author><keyname>Trivisano</keyname><forenames>Carlo</forenames></author></authors><title>A note on auxiliary mixture sampling for Bayesian Poisson models</title><categories>stat.ME stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bayesian hierarchical Poisson models are an essential tool for analyzing count data. However, designing efficient algorithms to sample from the posterior distribution of the target parameters remains a challenging task for this class of models. Auxiliary mixture sampling algorithms have been proposed to address this issue. They involve two steps of data augmentations: the first leverages the theory of Poisson processes, and the second approximates the residual distribution of the resulting model through a mixture of Gaussian distributions. In this way, an approximated Gibbs sampler is obtained. In this paper, we focus on the accuracy of the approximation step, highlighting scenarios where the mixture fails to accurately represent the true underlying distribution, leading to a lack of convergence in the algorithm. We outline key features to monitor, in order to assess if the approximation performs as intended. Building on this, we propose a robust version of the auxiliary mixture sampling algorithm, which can detect approximation failures and incorporate a Metropolis-Hastings step when necessary. Finally, we evaluate the proposed algorithm together with the original mixture sampling algorithms on both simulated and real datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04945</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04945</id><created>2025-02-07</created><authors><author><keyname>Yanhao</keyname><affiliation>Max</affiliation></author><author><keyname>Wei</keyname></author><author><keyname>Jiang</keyname><forenames>Zhenling</forenames></author></authors><title>Estimating Parameters of Structural Models Using Neural Networks</title><categories>econ.EM stat.CO</categories><acm-class>G.3; J.4; I.2</acm-class><journal-ref>Marketing Science 44(1):102-128 (2024)</journal-ref><doi>10.1287/mksc.2022.0360</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an alternative use of machine learning. We train neural nets to provide the parameter estimate of a given (structural) econometric model, for example, discrete choice or consumer search. Training examples consist of datasets generated by the econometric model under a range of parameter values. The neural net takes the moments of a dataset as input and tries to recognize the parameter value underlying that dataset. Besides the point estimate, the neural net can also output statistical accuracy. This neural net estimator (NNE) tends to limited-information Bayesian posterior as the number of training datasets increases. We apply NNE to a consumer search model. It gives more accurate estimates at lighter computational costs than the prevailing approach. NNE is also robust to redundant moment inputs. In general, NNE offers the most benefits in applications where other estimation approaches require very heavy simulation costs. We provide code at: https://nnehome.github.io. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04949</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04949</id><created>2025-02-07</created><authors><author><keyname>Elsemüller</keyname><forenames>Lasse</forenames></author><author><keyname>Pratz</keyname><forenames>Valentin</forenames></author><author><keyname>von Krause</keyname><forenames>Mischa</forenames></author><author><keyname>Voss</keyname><forenames>Andreas</forenames></author><author><keyname>Bürkner</keyname><forenames>Paul-Christian</forenames></author><author><keyname>Radev</keyname><forenames>Stefan T.</forenames></author></authors><title>Does Unsupervised Domain Adaptation Improve the Robustness of Amortized   Bayesian Inference? A Systematic Evaluation</title><categories>stat.ML cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Neural networks are fragile when confronted with data that significantly deviates from their training distribution. This is true in particular for simulation-based inference methods, such as neural amortized Bayesian inference (ABI), where models trained on simulated data are deployed on noisy real-world observations. Recent robust approaches employ unsupervised domain adaptation (UDA) to match the embedding spaces of simulated and observed data. However, the lack of comprehensive evaluations across different domain mismatches raises concerns about the reliability in high-stakes applications. We address this gap by systematically testing UDA approaches across a wide range of misspecification scenarios in both a controlled and a high-dimensional benchmark. We demonstrate that aligning summary spaces between domains effectively mitigates the impact of unmodeled phenomena or noise. However, the same alignment mechanism can lead to failures under prior misspecifications - a critical finding with practical consequences. Our results underscore the need for careful consideration of misspecification types when using UDA techniques to increase the robustness of ABI in practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04970</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04970</id><created>2025-02-07</created><authors><author><keyname>Langbein</keyname><forenames>Sophie Hanna</forenames></author><author><keyname>Koenen</keyname><forenames>Niklas</forenames></author><author><keyname>Wright</keyname><forenames>Marvin N.</forenames></author></authors><title>Gradient-based Explanations for Deep Learning Survival Models</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Deep learning survival models often outperform classical methods in time-to-event predictions, particularly in personalized medicine, but their "black box" nature hinders broader adoption. We propose a framework for gradient-based explanation methods tailored to survival neural networks, extending their use beyond regression and classification. We analyze the implications of their theoretical assumptions for time-dependent explanations in the survival setting and propose effective visualizations incorporating the temporal dimension. Experiments on synthetic data show that gradient-based methods capture the magnitude and direction of local and global feature effects, including time dependencies. We introduce GradSHAP(t), a gradient-based counterpart to SurvSHAP(t), which outperforms SurvSHAP(t) and SurvLIME in a computational speed vs. accuracy trade-off. Finally, we apply these methods to medical data with multi-modal inputs, revealing relevant tabular features and visual patterns, as well as their temporal dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04990</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04990</id><created>2025-02-07</created><authors><author><keyname>Pichler</keyname><forenames>Clemens</forenames></author><author><keyname>Jewson</keyname><forenames>Jack</forenames></author><author><keyname>Avalos-Pacheco</keyname><forenames>Alejandra</forenames></author></authors><title>Probabilistic Programming with Sufficient Statistics for faster Bayesian   Computation</title><categories>stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Probabilistic programming methods have revolutionised Bayesian inference, making it easier than ever for practitioners to perform Markov-chain-Monte-Carlo sampling from non-conjugate posterior distributions. Here we focus on Stan, arguably the most used probabilistic programming tool for Bayesian inference (Carpenter et al., 2017), and its interface with R via the brms (Burkner, 2017) and rstanarm (Goodrich et al., 2024) packages. Although easy to implement, these tools can become computationally prohibitive when applied to datasets with many observations or models with numerous parameters. While the use of sufficient statistics is well-established in theory, it has been surprisingly overlooked in state-of-the-art Stan software. We show that when the likelihood can be written in terms of sufficient statistics, considerable computational improvements can be made to current implementations. We demonstrate how this approach provides accurate inference at a fraction of the time than state-of-the-art implementations for Gaussian linear regression models with non-conjugate priors, hierarchical random effects models, and factor analysis models. Our results also show that moderate computational gains can be achieved even in models where the likelihood can only be partially written in terms of sufficient statistics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05021</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05021</id><created>2025-02-07</created><authors><author><keyname>van Heel</keyname><forenames>Simon Donker</forenames></author><author><keyname>Lange</keyname><forenames>Rutger-Jan</forenames></author><author><keyname>van Dijk</keyname><forenames>Dick</forenames></author><author><keyname>van Os</keyname><forenames>Bram</forenames></author></authors><title>Stability and performance guarantees for misspecified multivariate   score-driven filters</title><categories>stat.ME eess.SP stat.ML</categories><comments>71 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the problem of tracking latent time-varying parameter vectors under model misspecification. We analyze implicit and explicit score-driven (ISD and ESD) filters, which update a prediction of the parameters using the gradient of the logarithmic observation density (i.e., the score). In the ESD filter, the score is computed using the predicted parameter values, whereas in the ISD filter, the score is evaluated using the new, updated parameter values. For both filter types, we derive novel sufficient conditions for the exponential stability (i.e., invertibility) of the filtered parameter path and existence of a finite mean squared error (MSE) bound with respect to the pseudo-true parameter path. In addition, we present expressions for finite-sample and asymptotic MSE bounds. Our performance guarantees rely on mild moment conditions on the data-generating process, while our stability result is entirely agnostic about the true process. As a result, our primary conditions depend only on the characteristics of the filter; hence, they are verifiable in practice. Concavity of the postulated log density combined with simple parameter restrictions is sufficient (but not necessary) for ISD-filter stability, whereas ESD-filter stability additionally requires the score to be Lipschitz continuous. Extensive simulation studies validate our theoretical findings and demonstrate the enhanced stability and improved performance of ISD over ESD filters. An empirical application to U.S. Treasury-bill rates confirms the practical relevance of our contribution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05072</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05072</id><created>2025-02-07</created><authors><author><keyname>Barnett</keyname><forenames>Helen</forenames></author><author><keyname>Boix</keyname><forenames>Oliver</forenames></author><author><keyname>Kontos</keyname><forenames>Dimitris</forenames></author><author><keyname>Jaki</keyname><forenames>Thomas</forenames></author></authors><title>Joint TITE-CRM for Dual Agent Dose Finding Studies</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Dual agent dose-finding trials study the effect of a combination of more than one agent, where the objective is to find the Maximum Tolerated Dose Combination (MTC), the combination of doses of the two agents that is associated with a pre-specified risk of being unsafe. In a Phase I/II setting, the objective is to find a dose combination that is both safe and active, the Optimal Biological Dose (OBD), that optimizes a criterion based on both safety and activity. Since Oncology treatments are typically given over multiple cycles, both the safety and activity outcome can be considered as late-onset, potentially occurring in the later cycles of treatment. This work proposes two model-based designs for dual-agent dose finding studies with late-onset activity and late-onset toxicity outcomes, the Joint TITE-POCRM and the Joint TITE-BLRM. Their performance is compared alongside a model-assisted comparator in a comprehensive simulation study motivated by a real trial example, with an extension to consider alternative sized dosing grids. It is found that both model-based methods outperform the model-assisted design. Whilst on average the two model-based designs are comparable, this comparability is not consistent across scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05074</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05074</id><created>2025-02-07</created><authors><author><keyname>Atanasov</keyname><forenames>Alexander</forenames></author><author><keyname>Bordelon</keyname><forenames>Blake</forenames></author><author><keyname>Zavatone-Veth</keyname><forenames>Jacob A.</forenames></author><author><keyname>Paquette</keyname><forenames>Courtney</forenames></author><author><keyname>Pehlevan</keyname><forenames>Cengiz</forenames></author></authors><title>Two-Point Deterministic Equivalence for Stochastic Gradient Dynamics in   Linear Models</title><categories>cond-mat.dis-nn cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We derive a novel deterministic equivalence for the two-point function of a random matrix resolvent. Using this result, we give a unified derivation of the performance of a wide variety of high-dimensional linear models trained with stochastic gradient descent. This includes high-dimensional linear regression, kernel regression, and random feature models. Our results include previously known asymptotics as well as novel ones. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05075</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05075</id><created>2025-02-07</created><authors><author><keyname>Dong</keyname><forenames>Yijun</forenames></author><author><keyname>Li</keyname><forenames>Yicheng</forenames></author><author><keyname>Li</keyname><forenames>Yunai</forenames></author><author><keyname>Lee</keyname><forenames>Jason D.</forenames></author><author><keyname>Lei</keyname><forenames>Qi</forenames></author></authors><title>Discrepancies are Virtue: Weak-to-Strong Generalization through Lens of   Intrinsic Dimension</title><categories>cs.LG cs.NA math.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weak-to-strong (W2S) generalization is a type of finetuning (FT) where a strong (large) student model is trained on pseudo-labels generated by a weak teacher. Surprisingly, W2S FT often outperforms the weak teacher. We seek to understand this phenomenon through the observation that FT often occurs in intrinsically low-dimensional spaces. Leveraging the low intrinsic dimensionality of FT, we analyze W2S in the ridgeless regression setting from a variance reduction perspective. For a strong student - weak teacher pair with sufficiently expressive low-dimensional feature subspaces $\mathcal{V}_s, \mathcal{V}_w$, we provide an exact characterization of the variance that dominates the generalization error of W2S. This unveils a virtue of discrepancy between the strong and weak models in W2S: the variance of the weak teacher is inherited by the strong student in $\mathcal{V}_s \cap \mathcal{V}_w$, while reduced by a factor of $\dim(\mathcal{V}_s)/N$ in the subspace of discrepancy $\mathcal{V}_w \setminus \mathcal{V}_s$ with $N$ pseudo-labels for W2S. Further, our analysis casts light on the sample complexities and the scaling of performance gap recovery in W2S. The analysis is supported with experiments on both synthetic regression problems and real vision tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05094</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05094</id><created>2025-02-07</created><authors><author><keyname>Blanchet</keyname><forenames>Jose</forenames></author><author><keyname>Hamoudi</keyname><forenames>Yassine</forenames></author><author><keyname>Szegedy</keyname><forenames>Mario</forenames></author><author><keyname>Wang</keyname><forenames>Guanyang</forenames></author></authors><title>Non-linear Quantum Monte Carlo</title><categories>quant-ph cs.LG cs.NA math.NA stat.CO stat.ML</categories><comments>30 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The mean of a random variable can be understood as a $\textit{linear}$ functional on the space of probability distributions. Quantum computing is known to provide a quadratic speedup over classical Monte Carlo methods for mean estimation. In this paper, we investigate whether a similar quadratic speedup is achievable for estimating $\textit{non-linear}$ functionals of probability distributions. We propose a quantum-inside-quantum Monte Carlo algorithm that achieves such a speedup for a broad class of non-linear estimation problems, including nested conditional expectations and stochastic optimization. Our algorithm improves upon the direct application of the quantum multilevel Monte Carlo algorithm introduced by An et al.. The existing lower bound indicates that our algorithm is optimal up polylogarithmic factors. A key innovation of our approach is a new sequence of multilevel Monte Carlo approximations specifically designed for quantum computing, which is central to the algorithm's improved performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05102</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05102</id><created>2025-02-07</created><authors><author><keyname>Piancastelli</keyname><forenames>Luiza</forenames></author><author><keyname>Barreto-Souza</keyname><forenames>Wagner</forenames></author></authors><title>Time Series Analysis of Rankings: A GARCH-Type Approach</title><categories>stat.ME stat.CO stat.ML</categories><comments>Paper submitted for publication</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Ranking data are frequently obtained nowadays but there are still scarce methods for treating these data when temporally observed. The present paper contributes to this topic by proposing and developing novel models for handling time series of ranking data. We introduce a class of time-varying ranking models inspired by the Generalized AutoRegressive Conditional Heteroskedasticity (GARCH) models. More specifically, the temporal dynamics are defined by the conditional distribution of the current ranking given the past rankings, which are assumed to follow a Mallows distribution, which implicitly depends on a distance. Then, autoregressive and feedback components are incorporated into the model through the conditional expectation of the associated distances. Theoretical properties of our ranking GARCH models such as stationarity and ergodicity are established. The estimation of parameters is performed via maximum likelihood estimation when data is fully observed. We develop a Monte Carlo Expectation-Maximisation algorithm to deal with cases involving missing data. Monte Carlo simulation studies are presented to study the performance of the proposed estimators under both non-missing and missing data scenarios. A real data application about the weekly ranking of professional tennis players from 2015 to 2019 is presented under our proposed ranking GARCH models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05122</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05122</id><created>2025-02-07</created><authors><author><keyname>Xi</keyname><forenames>Johnny</forenames></author><author><keyname>Dance</keyname><forenames>Hugh</forenames></author><author><keyname>Orbanz</keyname><forenames>Peter</forenames></author><author><keyname>Bloem-Reddy</keyname><forenames>Benjamin</forenames></author></authors><title>Distinguishing Cause from Effect with Causal Velocity Models</title><categories>stat.ML cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bivariate structural causal models (SCM) are often used to infer causal direction by examining their goodness-of-fit under restricted model classes. In this paper, we describe a parametrization of bivariate SCMs in terms of a causal velocity by viewing the cause variable as time in a dynamical system. The velocity implicitly defines counterfactual curves via the solution of initial value problems where the observation specifies the initial condition. Using tools from measure transport, we obtain a unique correspondence between SCMs and the score function of the generated distribution via its causal velocity. Based on this, we derive an objective function that directly regresses the velocity against the score function, the latter of which can be estimated non-parametrically from observational data. We use this to develop a method for bivariate causal discovery that extends beyond known model classes such as additive or location scale noise, and that requires no assumptions on the noise distributions. When the score is estimated well, the objective is also useful for detecting model non-identifiability and misspecification. We present positive results in simulation and benchmark experiments where many existing methods fail, and perform ablation studies to examine the method's sensitivity to accurate score estimation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05127</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05127</id><created>2025-02-07</created><authors><author><keyname>Everink</keyname><forenames>Jasper M.</forenames></author><author><keyname>Amougou</keyname><forenames>Bernardin Tamo</forenames></author><author><keyname>Pereyra</keyname><forenames>Marcelo</forenames></author></authors><title>Self-supervised Conformal Prediction for Uncertainty Quantification in   Imaging Problems</title><categories>cs.CV stat.ME</categories><msc-class>62H35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most image restoration problems are ill-conditioned or ill-posed and hence involve significant uncertainty. Quantifying this uncertainty is crucial for reliably interpreting experimental results, particularly when reconstructed images inform critical decisions and science. However, most existing image restoration methods either fail to quantify uncertainty or provide estimates that are highly inaccurate. Conformal prediction has recently emerged as a flexible framework to equip any estimator with uncertainty quantification capabilities that, by construction, have nearly exact marginal coverage. To achieve this, conformal prediction relies on abundant ground truth data for calibration. However, in image restoration problems, reliable ground truth data is often expensive or not possible to acquire. Also, reliance on ground truth data can introduce large biases in situations of distribution shift between calibration and deployment. This paper seeks to develop a more robust approach to conformal prediction for image restoration problems by proposing a self-supervised conformal prediction method that leverages Stein's Unbiased Risk Estimator (SURE) to self-calibrate itself directly from the observed noisy measurements, bypassing the need for ground truth. The method is suitable for any linear imaging inverse problem that is ill-conditioned, and it is especially powerful when used with modern self-supervised image restoration techniques that can also be trained directly from measurement data. The proposed approach is demonstrated through numerical experiments on image denoising and deblurring, where it delivers results that are remarkably accurate and comparable to those obtained by supervised conformal prediction with ground truth data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05134</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05134</id><created>2025-02-07</created><authors><author><keyname>Kızıldağ</keyname><forenames>Eren C.</forenames></author></authors><title>Information-Theoretic Guarantees for Recovering Low-Rank Tensors from   Symmetric Rank-One Measurements</title><categories>math.ST cs.IT math.IT math.PR stat.ML stat.TH</categories><comments>Algorithmic Learning Theory (ALT), 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the sample complexity of recovering tensors with low symmetric rank from symmetric rank-one measurements. This setting is particularly motivated by the study of higher-order interactions and the analysis of two-layer neural networks with polynomial activations (polynomial networks). Using a covering numbers argument, we analyze the performance of the symmetric rank minimization program and establish near-optimal sample complexity bounds when the underlying distribution is log-concave. Our measurement model involves random symmetric rank-one tensors, which lead to involved probability calculations. To address these challenges, we employ the Carbery-Wright inequality, a powerful tool for studying anti-concentration properties of random polynomials, and leverage orthogonal polynomials. Additionally, we provide a sample complexity lower bound based on Fano's inequality, and discuss broader implications of our results for two-layer polynomial networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05155</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05155</id><created>2025-02-07</created><authors><author><keyname>Tang</keyname><forenames>Shiqin</forenames></author><author><keyname>Yu</keyname><forenames>Shujian</forenames></author><author><keyname>Dong</keyname><forenames>Yining</forenames></author><author><keyname>Qin</keyname><forenames>S. Joe</forenames></author></authors><title>Deep Dynamic Probabilistic Canonical Correlation Analysis</title><categories>cs.LG stat.ML</categories><comments>accepted by ICASSP-25, code is available at   \url{https://github.com/marcusstang/D2PCCA}</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents Deep Dynamic Probabilistic Canonical Correlation Analysis (D2PCCA), a model that integrates deep learning with probabilistic modeling to analyze nonlinear dynamical systems. Building on the probabilistic extensions of Canonical Correlation Analysis (CCA), D2PCCA captures nonlinear latent dynamics and supports enhancements such as KL annealing for improved convergence and normalizing flows for a more flexible posterior approximation. D2PCCA naturally extends to multiple observed variables, making it a versatile tool for encoding prior knowledge about sequential datasets and providing a probabilistic understanding of the system's dynamics. Experimental validation on real financial datasets demonstrates the effectiveness of D2PCCA and its extensions in capturing latent dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05161</identifier><datestamp>2025-02-10</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05161</id><created>2025-02-07</created><authors><author><keyname>Antonczak</keyname><forenames>Brittany</forenames></author><author><keyname>Fay</keyname><forenames>Meg</forenames></author><author><keyname>Chawla</keyname><forenames>Aviral</forenames></author><author><keyname>Rowangould</keyname><forenames>Gregory</forenames></author></authors><title>Estimated Roadway Segment Traffic Data by Vehicle Class for the United   States: A Machine Learning Approach</title><categories>stat.AP</categories><comments>14 pages including references, 4 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Highway Performance Monitoring System, managed by the Federal Highway Administration, provides essential data on average annual daily traffic across U.S. roadways, but it has limited representation of medium- and heavy-duty vehicles on non-interstate roads. This gap limits research and policy analysis on the impacts of truck traffic, especially concerning air quality and public health. To address this, we use random forest regression to estimate medium- and heavy-duty vehicle traffic volumes in areas with sparse data. This results in a more comprehensive dataset, which enables the estimation of traffic density at the census block level as a proxy for traffic-related air pollution exposure. Our high-resolution spatial data products, rigorously validated, provide a more accurate representation of truck traffic and its environmental and health impacts. These datasets are valuable for transportation planning, public health research, and policy decisions aimed at mitigating the effects of truck traffic on vulnerable communities exposed to air pollution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05254</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05254</id><created>2025-02-07</created><authors><author><keyname>Swain</keyname><forenames>Arabind</forenames></author><author><keyname>Ridout</keyname><forenames>Sean Alexander</forenames></author><author><keyname>Nemenman</keyname><forenames>Ilya</forenames></author></authors><title>Distribution of singular values in large sample cross-covariance   matrices</title><categories>math.ST cond-mat.dis-nn physics.data-an stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  For two large matrices ${\mathbf X}$ and ${\mathbf Y}$ with Gaussian i.i.d.\ entries and dimensions $T\times N_X$ and $T\times N_Y$, respectively, we derive the probability distribution of the singular values of $\mathbf{X}^T \mathbf{Y}$ in different parameter regimes. This extends the Marchenko-Pastur result for the distribution of eigenvalues of empirical sample covariance matrices to singular values of empirical cross-covariances. Our results will help to establish statistical significance of cross-correlations in many data-science applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05295</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05295</id><created>2025-02-07</created><authors><author><keyname>Oprescu</keyname><forenames>Miruna</forenames></author><author><keyname>Park</keyname><forenames>David K.</forenames></author><author><keyname>Luo</keyname><forenames>Xihaier</forenames></author><author><keyname>Yoo</keyname><forenames>Shinjae</forenames></author><author><keyname>Kallus</keyname><forenames>Nathan</forenames></author></authors><title>GST-UNet: Spatiotemporal Causal Inference with Time-Varying Confounders</title><categories>cs.LG stat.ME</categories><comments>17 pages, 6 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Estimating causal effects from spatiotemporal data is a key challenge in fields such as public health, social policy, and environmental science, where controlled experiments are often infeasible. However, existing causal inference methods relying on observational data face significant limitations: they depend on strong structural assumptions to address spatiotemporal challenges $\unicode{x2013}$ such as interference, spatial confounding, and temporal carryover effects $\unicode{x2013}$ or fail to account for $\textit{time-varying confounders}$. These confounders, influenced by past treatments and outcomes, can themselves shape future treatments and outcomes, creating feedback loops that complicate traditional adjustment strategies. To address these challenges, we introduce the $\textbf{GST-UNet}$ ($\textbf{G}$-computation $\textbf{S}$patio-$\textbf{T}$emporal $\textbf{UNet}$), a novel end-to-end neural network framework designed to estimate treatment effects in complex spatial and temporal settings. The GST-UNet leverages regression-based iterative G-computation to explicitly adjust for time-varying confounders, providing valid estimates of potential outcomes and treatment effects. To the best of our knowledge, the GST-UNet is the first neural model to account for complex, non-linear dynamics and time-varying confounders in spatiotemporal interventions. We demonstrate the effectiveness of the GST-UNet through extensive simulation studies and showcase its practical utility with a real-world analysis of the impact of wildfire smoke on respiratory hospitalizations during the 2018 California Camp Fire. Our results highlight the potential of GST-UNet to advance spatiotemporal causal inference across a wide range of policy-driven and scientific applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05297</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05297</id><created>2025-02-07</created><authors><author><keyname>Rumsey</keyname><forenames>Susanna E.</forenames></author><author><keyname>Draper</keyname><forenames>Stark C.</forenames></author><author><keyname>Kschischang</keyname><forenames>Frank R.</forenames></author></authors><title>Quasicyclic Principal Component Analysis</title><categories>math.NA cs.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present quasicyclic principal component analysis (QPCA), a generalization of principal component analysis (PCA), that determines an optimized basis for a dataset in terms of families of shift-orthogonal principal vectors. This is of particular interest when analyzing cyclostationary data, whose cyclic structure is not exploited by the standard PCA algorithm. We first formulate QPCA as an optimization problem, which we show may be decomposed into a series of PCA problems in the frequency domain. We then formalize our solution as an explicit algorithm and analyze its computational complexity. Finally, we provide some examples of applications of QPCA to cyclostationary signal processing data, including an investigation of carrier pulse recovery, a presentation of methods for estimating an unknown oversampling rate, and a discussion of an appropriate approach for pre-processing data with a non-integer oversampling rate in order to better apply the QPCA algorithm. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05300</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05300</id><created>2025-02-07</created><authors><author><keyname>Ziyin</keyname><forenames>Liu</forenames></author><author><keyname>Xu</keyname><forenames>Yizhou</forenames></author><author><keyname>Poggio</keyname><forenames>Tomaso</forenames></author><author><keyname>Chuang</keyname><forenames>Isaac</forenames></author></authors><title>Parameter Symmetry Breaking and Restoration Determines the Hierarchical   Learning in AI Systems</title><categories>cs.LG cond-mat.dis-nn cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamics of learning in modern large AI systems is hierarchical, often characterized by abrupt, qualitative shifts akin to phase transitions observed in physical systems. While these phenomena hold promise for uncovering the mechanisms behind neural networks and language models, existing theories remain fragmented, addressing specific cases. In this paper, we posit that parameter symmetry breaking and restoration serve as a unifying mechanism underlying these behaviors. We synthesize prior observations and show how this mechanism explains three distinct hierarchies in neural networks: learning dynamics, model complexity, and representation formation. By connecting these hierarchies, we highlight symmetry -- a cornerstone of theoretical physics -- as a potential fundamental principle in modern AI. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05301</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05301</id><created>2025-02-07</created><authors><author><keyname>Llorente</keyname><forenames>Fernando</forenames></author><author><keyname>Waxman</keyname><forenames>Daniel</forenames></author><author><keyname>Djurić</keyname><forenames>Petar M.</forenames></author></authors><title>Decentralized Online Ensembles of Gaussian Processes for Multi-Agent   Systems</title><categories>cs.LG cs.MA eess.SP stat.ML</categories><comments>5 pages, 2 figures. Accepted to ICASSP 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flexible and scalable decentralized learning solutions are fundamentally important in the application of multi-agent systems. While several recent approaches introduce (ensembles of) kernel machines in the distributed setting, Bayesian solutions are much more limited. We introduce a fully decentralized, asymptotically exact solution to computing the random feature approximation of Gaussian processes. We further address the choice of hyperparameters by introducing an ensembling scheme for Bayesian multiple kernel learning based on online Bayesian model averaging. The resulting algorithm is tested against Bayesian and frequentist methods on simulated and real-world datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05305</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05305</id><created>2025-02-07</created><authors><author><keyname>Jiang</keyname><forenames>Liwei</forenames></author><author><keyname>Roy</keyname><forenames>Abhishek</forenames></author><author><keyname>Balasubramanian</keyname><forenames>Krishna</forenames></author><author><keyname>Davis</keyname><forenames>Damek</forenames></author><author><keyname>Drusvyatskiy</keyname><forenames>Dmitriy</forenames></author><author><keyname>Na</keyname><forenames>Sen</forenames></author></authors><title>Online Covariance Estimation in Nonsmooth Stochastic Approximation</title><categories>stat.ML cs.LG math.OC</categories><comments>46 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider applying stochastic approximation (SA) methods to solve nonsmooth variational inclusion problems. Existing studies have shown that the averaged iterates of SA methods exhibit asymptotic normality, with an optimal limiting covariance matrix in the local minimax sense of H\'ajek and Le Cam. However, no methods have been proposed to estimate this covariance matrix in a nonsmooth and potentially non-monotone (nonconvex) setting. In this paper, we study an online batch-means covariance matrix estimator introduced in Zhu et al.(2023). The estimator groups the SA iterates appropriately and computes the sample covariance among batches as an estimate of the limiting covariance. Its construction does not require prior knowledge of the total sample size, and updates can be performed recursively as new data arrives. We establish that, as long as the batch size sequence is properly specified (depending on the stepsize sequence), the estimator achieves a convergence rate of order $O(\sqrt{d}n^{-1/8+\varepsilon})$ for any $\varepsilon&gt;0$, where $d$ and $n$ denote the problem dimensionality and the number of iterations (or samples) used. Although the problem is nonsmooth and potentially non-monotone (nonconvex), our convergence rate matches the best-known rate for covariance estimation methods using only first-order information in smooth and strongly-convex settings. The consistency of this covariance estimator enables asymptotically valid statistical inference, including constructing confidence intervals and performing hypothesis testing. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05319</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05319</id><created>2025-02-07</created><authors><author><keyname>Jiang</keyname><forenames>Yicong</forenames></author><author><keyname>Janson</keyname><forenames>Lucas</forenames></author></authors><title>Semiparametric Inference for Partially Identifiable Data Fusion   Estimands via Double Machine Learning</title><categories>stat.ME</categories><comments>45 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many statistical estimands of interest (e.g., in regression or causality) are functions of the joint distribution of multiple random variables. But in some applications, data is not available that measures all random variables on each subject, and instead the only possible approach is one of data fusion, where multiple independent data sets, each measuring a subset of the random variables of interest, are combined for inference. In general, since all random variables are never observed jointly, their joint distribution, and hence also the estimand which is a function of it, is only partially identifiable. Unfortunately, the endpoints of the partially identifiable region depend in general on entire conditional distributions, rendering them hard both operationally and statistically to estimate. To address this, we present a novel outer-bound on the region of partial identifiability (and establish conditions under which it is tight) that depends only on certain conditional first and second moments. This allows us to derive semiparametrically efficient estimators of our endpoint outer-bounds that only require the standard machine learning toolbox which learns conditional means. We prove asymptotic normality and semiparametric efficiency of our estimators and provide consistent estimators of their variances, enabling asymptotically valid confidence interval construction for our original partially identifiable estimand. We demonstrate the utility of our method in simulations and a data fusion problem from economics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05322</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05322</id><created>2025-02-07</created><authors><author><keyname>Lin</keyname><forenames>Bo</forenames></author><author><keyname>Ferry</keyname><forenames>Kamillo</forenames></author><author><keyname>Améndola</keyname><forenames>Carlos</forenames></author><author><keyname>Monod</keyname><forenames>Anthea</forenames></author><author><keyname>Yoshida</keyname><forenames>Ruriko</forenames></author></authors><title>Tropical Fr\'echet Means</title><categories>math.OC math.CO math.MG math.ST stat.TH</categories><comments>18 pages. 5 figures</comments><msc-class>14T90, 62R01, 62R20, 90C24</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Fr\'echet mean is a key measure of central tendency as a barycenter for a given set of points in a general metric space. It is computed by solving an optimization problem and is a fundamental quantity in statistics. In this paper, we study Fr\'echet means in tropical geometry -- a piecewise linear, combinatorial, and polyhedral variant of algebraic geometry that has gained prominence in applications. A key property of Fr\'echet means is that uniqueness is generally not guaranteed, which is true in tropical settings. In solving the tropical Fr\'echet mean optimization problem, we obtain a geometric characterization of the collection of all Fr\'echet means in a general tropical space as a tropically and classically convex polytope. Furthermore, we prove that a certificate of positivity for finitely many quadratic polynomials in $\mathbb{R}[x_1,\ldots,x_n]$ always exists, given that their quadratic homogeneous components are sums of squares. We propose an algorithm to symbolically compute the Fr\'echet mean polytope based on our exact quadratic optimization result and study its complexity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05336</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05336</id><created>2025-02-07</created><authors><author><keyname>Danish</keyname><forenames>Muhammad Umair</forenames></author><author><keyname>Rehman</keyname><forenames>Umair</forenames></author><author><keyname>Grolinger</keyname><forenames>Katarina</forenames></author></authors><title>Leveraging Order-Theoretic Tournament Graphs for Assessing Internal   Consistency in Survey-Based Instruments Across Diverse Scenarios</title><categories>stat.OT</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper introduces Monotone Delta, an order-theoretic measure designed to enhance the reliability assessment of survey-based instruments in human-machine interactions. Traditional reliability measures, such as Cronbach's Alpha and McDonald's Omega, often yield misleading estimates due to their sensitivity to redundancy, multidimensional constructs, and assumptions of normality and uncorrelated errors. These limitations can compromise decision-making in human-centric evaluations, where survey instruments inform adaptive interfaces, cognitive workload assessments, and human-AI trust models. Monotone Delta addresses these issues by quantifying internal consistency through the minimization of ordinal contradictions and alignment with a unidimensional latent order using weighted tournaments. Unlike traditional approaches, it operates without parametric or model-based assumptions. We conducted theoretical analyses and experimental evaluations on four challenging scenarios: tau-equivalence, redundancy, multidimensionality, and non-normal distributions, and proved that Monotone Delta provides more stable reliability assessments compared to existing methods. The Monotone Delta is a valuable alternative for evaluating questionnaire-based assessments in psychology, human factors, healthcare, and interactive system design, enabling organizations to optimize survey instruments, reduce costly redundancies, and enhance confidence in human-system interactions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05351</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05351</id><created>2025-02-07</created><authors><author><keyname>Chatterjee</keyname><forenames>Subhamoy</forenames></author><author><keyname>Munoz-Jaramillo</keyname><forenames>Andres</forenames></author><author><keyname>Malanushenko</keyname><forenames>Anna</forenames></author></authors><title>Deep Generative model that uses physical quantities to generate and   retrieve solar magnetic active regions</title><categories>astro-ph.SR cs.LG stat.ML</categories><comments>9 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep generative models have shown immense potential in generating unseen data that has properties of real data. These models learn complex data-generating distributions starting from a smaller set of latent dimensions. However, generative models have encountered great skepticism in scientific domains due to the disconnection between generative latent vectors and scientifically relevant quantities. In this study, we integrate three types of machine learning models to generate solar magnetic patches in a physically interpretable manner and use those as a query to find matching patches in real observations. We use the magnetic field measurements from Space-weather HMI Active Region Patches (SHARPs) to train a Generative Adversarial Network (GAN). We connect the physical properties of GAN-generated images with their latent vectors to train Support Vector Machines (SVMs) that do mapping between physical and latent spaces. These produce directions in the GAN latent space along which known physical parameters of the SHARPs change. We train a self-supervised learner (SSL) to make queries with generated images and find matches from real data. We find that the GAN-SVM combination enables users to produce high-quality patches that change smoothly only with a prescribed physical quantity, making generative models physically interpretable. We also show that GAN outputs can be used to retrieve real data that shares the same physical properties as the generated query. This elevates Generative Artificial Intelligence (AI) from a means-to-produce artificial data to a novel tool for scientific data interrogation, supporting its applicability beyond the domain of heliophysics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05360</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05360</id><created>2025-02-07</created><authors><author><keyname>Na</keyname><forenames>Sanghoon</forenames></author><author><keyname>Yang</keyname><forenames>Haizhao</forenames></author></authors><title>Curse of Dimensionality in Neural Network Optimization</title><categories>cs.LG math.OC stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The curse of dimensionality in neural network optimization under the mean-field regime is studied. It is demonstrated that when a shallow neural network with a Lipschitz continuous activation function is trained using either empirical or population risk to approximate a target function that is $r$ times continuously differentiable on $[0,1]^d$, the population risk may not decay at a rate faster than $t^{-\frac{4r}{d-2r}}$, where $t$ is an analog of the total number of optimization iterations. This result highlights the presence of the curse of dimensionality in the optimization computation required to achieve a desired accuracy. Instead of analyzing parameter evolution directly, the training dynamics are examined through the evolution of the parameter distribution under the 2-Wasserstein gradient flow. Furthermore, it is established that the curse of dimensionality persists when a locally Lipschitz continuous activation function is employed, where the Lipschitz constant in $[-x,x]$ is bounded by $O(x^\delta)$ for any $x \in \mathbb{R}$. In this scenario, the population risk is shown to decay at a rate no faster than $t^{-\frac{(4+2\delta)r}{d-2r}}$. To the best of our knowledge, this work is the first to analyze the impact of function smoothness on the curse of dimensionality in neural network optimization theory. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05363</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05363</id><created>2025-02-07</created><authors><author><keyname>Renson</keyname><forenames>Audrey</forenames></author><author><keyname>Montoya</keyname><forenames>Lina</forenames></author><author><keyname>Goin</keyname><forenames>Dana E.</forenames></author><author><keyname>Díaz</keyname><forenames>Iván</forenames></author><author><keyname>Ross</keyname><forenames>Rachael K.</forenames></author></authors><title>Pulling back the curtain: the road from statistical estimand to   machine-learning based estimator for epidemiologists (no wizard required)</title><categories>stat.ME stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Epidemiologists increasingly use causal inference methods that rely on machine learning, as these approaches can relax unnecessary model specification assumptions. While deriving and studying asymptotic properties of such estimators is a task usually associated with statisticians, it is useful for epidemiologists to understand the steps involved, as epidemiologists are often at the forefront of defining important new research questions and translating them into new parameters to be estimated. In this paper, our goal was to provide a relatively accessible guide through the process of (i) deriving an estimator based on the so-called efficient influence function (which we define and explain), and (ii) showing such an estimator's ability to validly incorporate machine learning, by demonstrating the so-called rate double robustness property. The derivations in this paper rely mainly on algebra and some foundational results from statistical inference, which are explained. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05366</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05366</id><created>2025-02-07</created><authors><author><keyname>Somé</keyname><forenames>Sobom M.</forenames></author><author><keyname>Kokonendji</keyname><forenames>Célestin C.</forenames></author><author><keyname>Dobélé-Kpoka</keyname><forenames>Francial G. B. Libengué</forenames></author></authors><title>An effective estimation of multivariate density functions using   extended-beta kernels with Bayesian adaptive bandwidths</title><categories>math.ST stat.TH</categories><comments>39 pages, 20 figures</comments><msc-class>62G05, 62G07, 62H12, 62G20, 65D10, 65D30</msc-class><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Multivariate kernel density estimations have received much spate of interest. In addition to conventional methods of (non-)classical associated-kernels for (un)bounded densities and bandwidth selections, the multiple extended-beta kernel (MEBK) estimators with Bayesian adaptive bandwidths are invested to gain a deeper and better insight into the estimation of multivariate density functions. Being unimodal, the univariate extended-beta smoother has an adaptable compact support which is suitable for each dataset, always limited. The support of the density MBEK estimator can be known or estimated by extreme values. Thus, asymptotical properties for the (non-)normalized estimators are established. Explicit and general choices of bandwidths using the flexible Bayesian adaptive method are provided. Behavioural analyses, specifically undertaken on the sensitive edges of the estimator support, are studied and compared to Gaussian and gamma kernel estimators. Finally, simulation studies and three applications on original and usual real-data sets of the proposed method yielded very interesting advantages with respect to its flexibility as well as its universality. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05379</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05379</id><created>2025-02-07</created><authors><author><keyname>Matin</keyname><forenames>Sakib</forenames></author><author><keyname>Allen</keyname><forenames>Alice</forenames></author><author><keyname>Shinkle</keyname><forenames>Emily</forenames></author><author><keyname>Pachalieva</keyname><forenames>Aleksandra</forenames></author><author><keyname>Craven</keyname><forenames>Galen T.</forenames></author><author><keyname>Nebgen</keyname><forenames>Benjamin</forenames></author><author><keyname>Smith</keyname><forenames>Justin</forenames></author><author><keyname>Messerly</keyname><forenames>Richard</forenames></author><author><keyname>Li</keyname><forenames>Ying Wai</forenames></author><author><keyname>Tretiak</keyname><forenames>Sergei</forenames></author><author><keyname>Barros</keyname><forenames>Kipton</forenames></author><author><keyname>Lubbers</keyname><forenames>Nicholas</forenames></author></authors><title>Teacher-student training improves accuracy and efficiency of machine   learning inter-atomic potentials</title><categories>physics.chem-ph stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Machine learning inter-atomic potentials (MLIPs) are revolutionizing the field of molecular dynamics (MD) simulations. Recent MLIPs have tended towards more complex architectures trained on larger datasets. The resulting increase in computational and memory costs may prohibit the application of these MLIPs to perform large-scale MD simulations. Here, we present a teacher-student training framework in which the latent knowledge from the teacher (atomic energies) is used to augment the students' training. We show that the light-weight student MLIPs have faster MD speeds at a fraction of the memory footprint compared to the teacher models. Remarkably, the student models can even surpass the accuracy of the teachers, even though both are trained on the same quantum chemistry dataset. Our work highlights a practical method for MLIPs to reduce the resources required for large-scale MD simulations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05380</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05380</id><created>2025-02-07</created><authors><author><keyname>Lotspeich</keyname><forenames>Sarah C.</forenames></author><author><keyname>Kedar</keyname><forenames>Sheetal</forenames></author><author><keyname>Tahir</keyname><forenames>Rabeya</forenames></author><author><keyname>Keleghan</keyname><forenames>Aidan D.</forenames></author><author><keyname>Miranda</keyname><forenames>Amelia</forenames></author><author><keyname>Duda</keyname><forenames>Stephany N.</forenames></author><author><keyname>Bancks</keyname><forenames>Mike</forenames></author><author><keyname>Wells</keyname><forenames>Brian J.</forenames></author><author><keyname>Khanna</keyname><forenames>Ashish K.</forenames></author><author><keyname>Rigdon</keyname><forenames>Joseph</forenames></author></authors><title>Overcoming data challenges to measure whole-person health in electronic   health records</title><categories>stat.ME stat.AP</categories><comments>16 pages, 2 tables, 6 figures, supplementary materials and code on   GitHub</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The allostatic load index (ALI) is a composite measure of whole-person health. Data from electronic health records (EHR) present a huge opportunity to operationalize the ALI in the learning health system, except they are prone to missingness and errors. Validation of EHR data (e.g., through chart reviews) can provide better-quality data, but realistically, only a subset of patients' data can be validated, and most protocols do not recover missing data. Using a representative sample of 1000 patients from the EHR at an extensive learning health system (100 of whom could be validated), we propose methods to design, conduct, and analyze statistically efficient and robust studies of the ALI and healthcare utilization. With semiparametric maximum likelihood estimation, we robustly incorporate all available data into statistical models. Using targeted design strategies, we examine ways to select the most informative patients for validation. Incorporating clinical expertise, we devise a novel validation protocol to promote the quality and completeness of EHR data. Validating the EHR data uncovered relatively low error rates and recovered some missing data. Through simulation studies based on preliminary data, residual sampling was identified as the most informative strategy for completing our validation study. Statistical models of partially validated data indicated higher odds of engaging in the healthcare system were associated with worse whole-person health (i.e., higher ALI), adjusting for age. Targeted validation with an enriched protocol allowed us to ensure the quality and promote the completeness of the EHR. Findings from our validation study were incorporated into analyses as we operationalize the ALI as a whole-person health measure intended to predict healthcare utilization in the academic learning health system. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05394</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05394</id><created>2025-02-07</created><authors><author><keyname>Zeng</keyname><forenames>Sirui</forenames></author><author><keyname>Eden</keyname><forenames>Uri T.</forenames></author></authors><title>A state-space framework for causal detection of hippocampal   ripple-replay events</title><categories>q-bio.NC stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Hippocampal ripple-replay events are typically identified using a two-step process that at each time point uses past and future data to determine whether an event is occurring. This prevents researchers from identifying these events in real time for closed-loop experiments. It also prevents the identification of periods of nonlocal representation that are not accompanied by large changes in the spectral content of the local field potentials (LFPs). In this work, we present a new state-space model framework that is able to detect concurrent changes in the rhythmic structure of LFPs with nonlocal activity in place cells to identify ripple-replay events in a causal manner. The model combines latent factors related to neural oscillations, represented space, and switches between coding properties to explain simultaneously the spiking activity from multiple units and the rhythmic content of LFPs recorded from multiple sources. The model is temporally causal, meaning that estimates of the switching state can be made at each instant using only past information from the spike and LFP signals, or can be combined with future data to refine those estimates. We applied this model framework to simulated and real hippocampal data to demonstrate its performance in identifying ripple-replay events. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05407</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05407</id><created>2025-02-07</created><authors><author><keyname>Kumar</keyname><forenames>Akash</forenames></author></authors><title>The Complexity of Learning Sparse Superposed Features with Feedback</title><categories>cs.LG cs.AI stat.ML</categories><comments>40 pages, 20 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The success of deep networks is crucially attributed to their ability to capture latent features within a representation space. In this work, we investigate whether the underlying learned features of a model can be efficiently retrieved through feedback from an agent, such as a large language model (LLM), in the form of relative \textit{triplet comparisons}. These features may represent various constructs, including dictionaries in LLMs or components of a covariance matrix of Mahalanobis distances. We analyze the feedback complexity associated with learning a feature matrix in sparse settings. Our results establish tight bounds when the agent is permitted to construct activations and demonstrate strong upper bounds in sparse scenarios when the agent's feedback is limited to distributional information. We validate our theoretical findings through experiments on two distinct applications: feature recovery from Recursive Feature Machine-trained models and dictionary extraction from sparse autoencoders trained on Large Language Models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05411</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05411</id><created>2025-02-07</created><authors><author><keyname>Manzhos</keyname><forenames>Sergei</forenames></author><author><keyname>Luder</keyname><forenames>Johann</forenames></author><author><keyname>Ihara</keyname><forenames>Manabu</forenames></author></authors><title>Machine learning-guided construction of an analytic kinetic energy   functional for orbital free density functional theory</title><categories>cond-mat.mtrl-sci stat.ML</categories><comments>16 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Machine learning (ML) of kinetic energy functionals (KEF) for orbital-free density functional theory (OF-DFT) holds the promise of addressing an important bottleneck in large-scale ab initio materials modeling where sufficiently accurate analytic KEFs are lacking. However, ML models are not as easily handled as analytic expressions; they need to be provided in the form of algorithms and associated data. Here, we bridge the two approaches and construct an analytic expression for a KEF guided by interpretative machine learning of crystal cell-averaged kinetic energy densities ({\tau}) of several hundred materials. A previously published dataset including multiple phases of 433 unary, binary, and ternary compounds containing Li, Al, Mg, Si, As, Ga, Sb, Na, Sn, P, and In was used for training, including data at the equilibrium geometry as well as strained structures. A hybrid Gaussian process regression - neural network (GPR-NN) method was used to understand the type of functional dependence of {\tau} on the features which contained cell-averaged terms of the 4th order gradient expansion and the product of the electron density and Kohn-Sham effective potential. Based on this analysis, an analytic model is constructed that can reproduce Kohn-Sham DFT energy-volume curves with sufficient accuracy (pronounced minima that are sufficiently close to the minima of the Kohn-Sham DFT-based curves and with sufficiently close curvatures) to enable structure optimizations and elastic response calculations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05458</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05458</id><created>2025-02-08</created><authors><author><keyname>Njifon</keyname><forenames>Marianne Abémgnigni</forenames></author><author><keyname>Weber</keyname><forenames>Tobias</forenames></author><author><keyname>Bezborodov</keyname><forenames>Viktor</forenames></author><author><keyname>Krueger</keyname><forenames>Tyll</forenames></author><author><keyname>Schuhmacher</keyname><forenames>Dominic</forenames></author></authors><title>Block Graph Neural Networks for tumor heterogeneity prediction</title><categories>cs.CV cs.LG stat.ML</categories><comments>27 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate tumor classification is essential for selecting effective treatments, but current methods have limitations. Standard tumor grading, which categorizes tumors based on cell differentiation, is not recommended as a stand-alone procedure, as some well-differentiated tumors can be malignant. Tumor heterogeneity assessment via single-cell sequencing offers profound insights but can be costly and may still require significant manual intervention. Many existing statistical machine learning methods for tumor data still require complex pre-processing of MRI and histopathological data.   In this paper, we propose to build on a mathematical model that simulates tumor evolution (O\.{z}a\'{n}ski (2017)) and generate artificial datasets for tumor classification. Tumor heterogeneity is estimated using normalized entropy, with a threshold to classify tumors as having high or low heterogeneity. Our contributions are threefold: (1) the cut and graph generation processes from the artificial data, (2) the design of tumor features, and (3) the construction of Block Graph Neural Networks (BGNN), a Graph Neural Network-based approach to predict tumor heterogeneity. The experimental results reveal that the combination of the proposed features and models yields excellent results on artificially generated data ($89.67\%$ accuracy on the test data). In particular, in alignment with the emerging trends in AI-assisted grading and spatial transcriptomics, our results suggest that enriching traditional grading methods with birth (e.g., Ki-67 proliferation index) and death markers can improve heterogeneity prediction and enhance tumor classification. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05459</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05459</id><created>2025-02-08</created><authors><author><keyname>Dhibar</keyname><forenames>Sibasish</forenames></author></authors><title>DCENWCNet: A Deep CNN Ensemble Network for White Blood Cell   Classification with LIME-Based Explainability</title><categories>cs.CV cs.AI q-bio.CB stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  White blood cells (WBC) are important parts of our immune system, and they protect our body against infections by eliminating viruses, bacteria, parasites and fungi. The number of WBC types and the total number of WBCs provide important information about our health status. A traditional method, convolutional neural networks (CNN), a deep learning architecture, can classify the blood cell from a part of an object and perform object recognition. Various CNN models exhibit potential; however, their development often involves ad-hoc processes that neglect unnecessary layers, leading to issues with unbalanced datasets and insufficient data augmentation. To address these challenges, we propose a novel ensemble approach that integrates three CNN architectures, each uniquely configured with different dropout and max-pooling layer settings to enhance feature learning. This ensemble model, named DCENWCNet, effectively balances the bias-variance trade-off. When evaluated on the widely recognized Rabbin-WBC dataset, our model outperforms existing state-of-the-art networks, achieving highest mean accuracy. Additionally, it demonstrates superior performance in precision, recall, F1-score, and Area Under the ROC Curve (AUC) across all categories. To delve deeper into the interpretability of classifiers, we employ reliable post-hoc explanation techniques, including Local Interpretable Model-Agnostic Explanations (LIME). These methods approximate the behavior of a black-box model by elucidating the relationships between feature values and predictions. Interpretable results enable users to comprehend and validate the model's predictions, thereby increasing their confidence in the automated diagnosis. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05460</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05460</id><created>2025-02-08</created><authors><author><keyname>Liang</keyname><forenames>Qiaoyu</forenames></author><author><keyname>Zhu</keyname><forenames>Zihan</forenames></author><author><keyname>Fu</keyname><forenames>Ziang</forenames></author><author><keyname>Evans</keyname><forenames>Michael</forenames></author><author><keyname>Zhao</keyname><forenames>Jinman</forenames></author></authors><title>False Discovery Rate Control via Frequentist-assisted Horseshoe</title><categories>stat.ME math.ST stat.ML stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The horseshoe prior, a widely used handy alternative to the spike-and-slab prior, has proven to be an exceptional default global-local shrinkage prior in Bayesian inference and machine learning. However, designing tests with frequentist false discovery rate (FDR) control using the horseshoe prior or the general class of global-local shrinkage priors remains an open problem. In this paper, we propose a frequentist-assisted horseshoe procedure that not only resolves this long-standing FDR control issue for the high dimensional normal means testing problem but also exhibits satisfactory finite-sample FDR control under any desired nominal level for both large-scale multiple independent and correlated tests. We carry out the frequentist-assisted horseshoe procedure in an easy and intuitive way by using the minimax estimator of the global parameter of the horseshoe prior while maintaining the remaining full Bayes vanilla horseshoe structure. The results of both intensive simulations under different sparsity levels, and real-world data demonstrate that the frequentist-assisted horseshoe procedure consistently achieves robust finite-sample FDR control. Existing frequentist or Bayesian FDR control procedures can lose finite-sample FDR control in a variety of common sparse cases. Based on the intimate relationship between the minimax estimation and the level of FDR control discovered in this work, we point out potential generalizations to achieve FDR control for both more complicated models and the general global-local shrinkage prior family. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05470</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05470</id><created>2025-02-08</created><authors><author><keyname>Muia</keyname><forenames>Mathias N.</forenames></author><author><keyname>Atutey</keyname><forenames>Olivia</forenames></author><author><keyname>Hasan</keyname><forenames>Mahmud</forenames></author></authors><title>Kernel Smoothing for Bounded Copula Densities</title><categories>stat.ME math.ST stat.TH</categories><comments>23 pages, 12 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Nonparametric estimation of copula density functions using kernel estimators presents significant challenges. One issue is the potential unboundedness of certain copula density functions at the corners of the unit square. Another is the boundary bias inherent in kernel density estimation. This paper presents a kernel-based method for estimating bounded copula density functions, addressing boundary bias through the mirror-reflection technique. Optimal smoothing parameters are derived via Asymptotic Mean Integrated Squared Error (AMISE) minimization and cross-validation, with theoretical guarantees of consistency and asymptotic normality. Two kernel smoothing strategies are proposed: the rule-of-thumb approach and least squares cross-validation (LSCV). Simulation studies highlight the efficacy of the rule-of-thumb method in bandwidth selection for copulas with unbounded marginal supports. The methodology is further validated through an application to the Wisconsin Breast Cancer Diagnostic Dataset (WBCDD), where LSCV is used for bandwidth selection. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05494</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05494</id><created>2025-02-08</created><authors><author><keyname>Zhou</keyname><forenames>Ya</forenames></author><author><keyname>Yang</keyname><forenames>Yujie</forenames></author><author><keyname>Gan</keyname><forenames>Jianhuang</forenames></author><author><keyname>Li</keyname><forenames>Xiangjie</forenames></author><author><keyname>Yuan</keyname><forenames>Jing</forenames></author><author><keyname>Zhao</keyname><forenames>Wei</forenames></author></authors><title>Multi-scale Masked Autoencoder for Electrocardiogram Anomaly Detection</title><categories>cs.LG cs.AI stat.AP</categories><comments>Under review in a journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electrocardiogram (ECG) analysis is a fundamental tool for diagnosing cardiovascular conditions, yet anomaly detection in ECG signals remains challenging due to their inherent complexity and variability. We propose Multi-scale Masked Autoencoder for ECG anomaly detection (MMAE-ECG), a novel end-to-end framework that effectively captures both global and local dependencies in ECG data. Unlike state-of-the-art methods that rely on heartbeat segmentation or R-peak detection, MMAE-ECG eliminates the need for such pre-processing steps, enhancing its suitability for clinical deployment. MMAE-ECG partitions ECG signals into non-overlapping segments, with each segment assigned learnable positional embeddings. A novel multi-scale masking strategy and multi-scale attention mechanism, along with distinct positional embeddings, enable a lightweight Transformer encoder to effectively capture both local and global dependencies. The masked segments are then reconstructed using a single-layer Transformer block, with an aggregation strategy employed during inference to refine the outputs. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art approaches while significantly reducing computational complexity-approximately 1/78 of the floating-point operations (FLOPs) required for inference. Ablation studies further validate the effectiveness of each component, highlighting the potential of multi-scale masked autoencoders for anomaly detection. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05505</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05505</id><created>2025-02-08</created><authors><author><keyname>Lin</keyname><forenames>Zinan</forenames></author><author><keyname>Baltrusaitis</keyname><forenames>Tadas</forenames></author><author><keyname>Yekhanin</keyname><forenames>Sergey</forenames></author></authors><title>Differentially Private Synthetic Data via APIs 3: Using Simulators   Instead of Foundation Model</title><categories>cs.LG cs.CR cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differentially private (DP) synthetic data, which closely resembles the original private data while maintaining strong privacy guarantees, has become a key tool for unlocking the value of private data without compromising privacy. Recently, Private Evolution (PE) has emerged as a promising method for generating DP synthetic data. Unlike other training-based approaches, PE only requires access to inference APIs from foundation models, enabling it to harness the power of state-of-the-art models. However, a suitable foundation model for a specific private data domain is not always available. In this paper, we discover that the PE framework is sufficiently general to allow inference APIs beyond foundation models. Specifically, we show that simulators -- such as computer graphics-based image synthesis tools -- can also serve as effective APIs within the PE framework. This insight greatly expands the applicability of PE, enabling the use of a wide variety of domain-specific simulators for DP data synthesis. We explore the potential of this approach, named Sim-PE, in the context of image synthesis. Across three diverse simulators, Sim-PE performs well, improving the downstream classification accuracy of PE by up to 3x and reducing the FID score by up to 80%. We also show that simulators and foundation models can be easily leveraged together within the PE framework to achieve further improvements. The code is open-sourced in the Private Evolution Python library: https://github.com/microsoft/DPSDA. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05548</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05548</id><created>2025-02-08</created><authors><author><keyname>Teimouri</keyname><forenames>Mahdi</forenames></author></authors><title>Soccer Goalkeeper Performance Evaluation: Clustering Approach</title><categories>stat.AP</categories><comments>6 Tables, 3 Figures,</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The objective of this work was to help the soccer field managers to evaluate the performance of a goalkeeper in saving the penalty kicks. To this end, based on the concept of clustering, four measures were proposed for evaluating the goalkeeper's performance in terms of both the saved kicks and detecting the direction of kicked ball. The well-known measures ignore the goalkeeper's ability in detecting the directional jump, while the forth proposed measure in this work was regarded this fact. The effectiveness of the proposed measures were analyzed and demonstrated by evaluating the performance of goalkeepers participated in four important soccer matches. The results were consistent with the proposed measures. In summary, as well as the known point statistics for evaluating the performance of goalkeeper, the measures were proposed in this work that regard as well the ability of goalkeeper in detecting the direction of kicks are also suggested. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05565</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05565</id><created>2025-02-08</created><authors><author><keyname>Baheri</keyname><forenames>Ali</forenames></author><author><keyname>Shahbazi</keyname><forenames>Marzieh Amiri</forenames></author></authors><title>Multi-Scale Conformal Prediction: A Theoretical Framework with Coverage   Guarantees</title><categories>math.ST cs.SY eess.SY stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a multi-scale extension of conformal prediction, an approach that constructs prediction sets with finite-sample coverage guarantees under minimal statistical assumptions. Classic conformal prediction relies on a single notion of conformity, overlooking the multi-level structures that arise in applications such as image analysis, hierarchical data exploration, and multi-resolution time series modeling. In contrast, the proposed framework defines a distinct conformity function at each relevant scale or resolution, producing multiple conformal predictors whose prediction sets are then intersected to form the final multi-scale output. We establish theoretical results confirming that the multi-scale prediction set retains the marginal coverage guarantees of the original conformal framework and can, in fact, yield smaller or more precise sets in practice. By distributing the total miscoverage probability across scales in proportion to their informative power, the method further refines the set sizes. We also show that dependence between scales can lead to conservative coverage, ensuring that the actual coverage exceeds the nominal level. Numerical experiments in a synthetic classification setting demonstrate that multi-scale conformal prediction achieves or surpasses the nominal coverage level while generating smaller prediction sets compared to single-scale conformal methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05620</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05620</id><created>2025-02-08</created><authors><author><keyname>Benavoli</keyname><forenames>Alessio</forenames></author><author><keyname>Piga</keyname><forenames>Dario</forenames></author><author><keyname>Forgione</keyname><forenames>Marco</forenames></author><author><keyname>Zaffalon</keyname><forenames>Marco</forenames></author></authors><title>dynoGP: Deep Gaussian Processes for dynamic system identification</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present a novel approach to system identification for dynamical systems, based on a specific class of Deep Gaussian Processes (Deep GPs). These models are constructed by interconnecting linear dynamic GPs (equivalent to stochastic linear time-invariant dynamical systems) and static GPs (to model static nonlinearities). Our approach combines the strengths of data-driven methods, such as those based on neural network architectures, with the ability to output a probability distribution. This offers a more comprehensive framework for system identification that includes uncertainty quantification. Using both simulated and real-world data, we demonstrate the effectiveness of the proposed approach. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05623</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05623</id><created>2025-02-08</created><authors><author><keyname>Wibisono</keyname><forenames>Andre</forenames></author></authors><title>Mixing Time of the Proximal Sampler in Relative Fisher Information via   Strong Data Processing Inequality</title><categories>cs.IT cs.LG math.IT math.OC math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the mixing time guarantee for sampling in relative Fisher information via the Proximal Sampler algorithm, which is an approximate proximal discretization of the Langevin dynamics. We show that when the target probability distribution is strongly log-concave, the relative Fisher information converges exponentially fast along the Proximal Sampler; this matches the exponential convergence rate of the relative Fisher information along the continuous-time Langevin dynamics for strongly log-concave target. When combined with a standard implementation of the Proximal Sampler via rejection sampling, this exponential convergence rate provides a high-accuracy iteration complexity guarantee for the Proximal Sampler in relative Fisher information when the target distribution is strongly log-concave and log-smooth. Our proof proceeds by establishing a strong data processing inequality for relative Fisher information along the Gaussian channel under strong log-concavity, and a data processing inequality along the reverse Gaussian channel for a special distribution. The forward and reverse Gaussian channels compose to form the Proximal Sampler, and these data processing inequalities imply the exponential convergence rate of the relative Fisher information along the Proximal Sampler. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05659</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05659</id><created>2025-02-08</created><authors><author><keyname>Akhter</keyname><forenames>Zuber</forenames></author><author><keyname>MirMostafaee</keyname><forenames>S. M. T. K.</forenames></author><author><keyname>Bakar</keyname><forenames>Abu</forenames></author><author><keyname>Ormoz</keyname><forenames>Ehsan</forenames></author></authors><title>On the order statistics from the XLindley distribution and associated   inference with an application to fatigue data</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the order statistics from a newly-introduced lifetime distribution called the XLindley distribution. We have derived explicit closed form expressions for the single moments and product moments of order statistics from the XLindley distribution. Utilizing these expressions, we calculated the means, variances, and covariances of order statistics for sample sizes ranging from n = 1 to n = 10 and arbitrarily selected parameter values. Additionally, these moments allow us to identify the best linear unbiased estimators and best linear invariant estimators for the location and scale parameters based on both complete samples and Type-II right censored samples. We also address the linear prediction of unobserved order statistics based on Type-II right-censored samples. We also explore the formulation of confidence intervals for location and scale parameters, along with prediction intervals for unobserved order statistics. To provide comparison and illustration, we conduct a simulation study and analyze a real data example. Finally, we conclude with several remarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05668</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05668</id><created>2025-02-08</created><authors><author><keyname>Schechtman</keyname><forenames>Sholom</forenames></author><author><keyname>Schreuder</keyname><forenames>Nicolas</forenames></author></authors><title>The late-stage training dynamics of (stochastic) subgradient descent on   homogeneous neural networks</title><categories>cs.LG cs.NE math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the implicit bias of constant step stochastic subgradient descent (SGD). We consider the setting of binary classification with homogeneous neural networks - a large class of deep neural networks with ReLU-type activation functions such as MLPs and CNNs without biases. We interpret the dynamics of normalized SGD iterates as an Euler-like discretization of a conservative field flow that is naturally associated to the normalized classification margin. Owing to this interpretation, we show that normalized SGD iterates converge to the set of critical points of the normalized margin at late-stage training (i.e., assuming that the data is correctly classified with positive normalized margin). Up to our knowledge, this is the first extension of the analysis of Lyu and Li (2020) on the discrete dynamics of gradient descent to the nonsmooth and stochastic setting. Our main result applies to binary classification with exponential or logistic losses. We additionally discuss extensions to more general settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05672</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05672</id><created>2025-02-08</created><authors><author><keyname>Štrupl</keyname><forenames>Miroslav</forenames></author><author><keyname>Szehr</keyname><forenames>Oleg</forenames></author><author><keyname>Faccio</keyname><forenames>Francesco</forenames></author><author><keyname>Ashley</keyname><forenames>Dylan R.</forenames></author><author><keyname>Srivastava</keyname><forenames>Rupesh Kumar</forenames></author><author><keyname>Schmidhuber</keyname><forenames>Jürgen</forenames></author></authors><title>On the Convergence and Stability of Upside-Down Reinforcement Learning,   Goal-Conditioned Supervised Learning, and Online Decision Transformers</title><categories>stat.ML cs.AI cs.LG cs.NE cs.SY eess.SY</categories><comments>85 pages in main text + 4 pages of references + 26 pages of   appendices, 12 figures in main text + 2 figures in appendices; source code   available at https://github.com/struplm/eUDRL-GCSL-ODT-Convergence-public</comments><msc-class>68T07</msc-class><acm-class>I.2.6; I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article provides a rigorous analysis of convergence and stability of Episodic Upside-Down Reinforcement Learning, Goal-Conditioned Supervised Learning and Online Decision Transformers. These algorithms performed competitively across various benchmarks, from games to robotic tasks, but their theoretical understanding is limited to specific environmental conditions. This work initiates a theoretical foundation for algorithms that build on the broad paradigm of approaching reinforcement learning through supervised learning or sequence modeling. At the core of this investigation lies the analysis of conditions on the underlying environment, under which the algorithms can identify optimal solutions. We also assess whether emerging solutions remain stable in situations where the environment is subject to tiny levels of noise. Specifically, we study the continuity and asymptotic convergence of command-conditioned policies, values and the goal-reaching objective depending on the transition kernel of the underlying Markov Decision Process. We demonstrate that near-optimal behavior is achieved if the transition kernel is located in a sufficiently small neighborhood of a deterministic kernel. The mentioned quantities are continuous (with respect to a specific topology) at deterministic kernels, both asymptotically and after a finite number of learning cycles. The developed methods allow us to present the first explicit estimates on the convergence and stability of policies and values in terms of the underlying transition kernels. On the theoretical side we introduce a number of new concepts to reinforcement learning, like working in segment spaces, studying continuity in quotient topologies and the application of the fixed-point theory of dynamical systems. The theoretical study is accompanied by a detailed investigation of example environments and numerical experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05676</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05676</id><created>2025-02-08</created><authors><author><keyname>van der Laan</keyname><forenames>Lars</forenames></author><author><keyname>Alaa</keyname><forenames>Ahmed</forenames></author></authors><title>Generalized Venn and Venn-Abers Calibration with Applications in   Conformal Prediction</title><categories>stat.ML cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Ensuring model calibration is critical for reliable predictions, yet popular distribution-free methods, such as histogram binning and isotonic regression, provide only asymptotic guarantees. We introduce a unified framework for Venn and Venn-Abers calibration, generalizing Vovk's binary classification approach to arbitrary prediction tasks and loss functions. Venn calibration leverages binning calibrators to construct prediction sets that contain at least one marginally perfectly calibrated point prediction in finite samples, capturing epistemic uncertainty in the calibration process. The width of these sets shrinks asymptotically to zero, converging to a conditionally calibrated point prediction. Furthermore, we propose Venn multicalibration, a novel methodology for finite-sample calibration across subpopulations. For quantile loss, group-conditional and multicalibrated conformal prediction arise as special cases of Venn multicalibration, and Venn calibration produces novel conformal prediction intervals that achieve quantile-conditional coverage. As a separate contribution, we extend distribution-free conditional calibration guarantees of histogram binning and isotonic calibration to general losses. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05684</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05684</id><created>2025-02-08</created><authors><author><keyname>Xu</keyname><forenames>Shizhou</forenames></author><author><keyname>Strohmer</keyname><forenames>Thomas</forenames></author></authors><title>Machine Unlearning via Information Theoretic Regularization</title><categories>cs.LG cs.AI cs.IT math.IT stat.ML</categories><comments>31 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How can we effectively remove or "unlearn" undesirable information, such as specific features or individual data points, from a learning outcome while minimizing utility loss and ensuring rigorous guarantees? We introduce a mathematical framework based on information-theoretic regularization to address both feature and data point unlearning. For feature unlearning, we derive a unified solution that simultaneously optimizes diverse learning objectives, including entropy, conditional entropy, KL-divergence, and the energy of conditional probability. For data point unlearning, we first propose a novel definition that serves as a practical condition for unlearning via retraining, is easy to verify, and aligns with the principles of differential privacy from an inference perspective. Then, we provide provable guarantees for our framework on data point unlearning. By combining flexibility in learning objectives with simplicity in regularization design, our approach is highly adaptable and practical for a wide range of machine learning and AI applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05706</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05706</id><created>2025-02-08</created><authors><author><keyname>Sridhar</keyname><forenames>Anupama</forenames></author><author><keyname>Johansen</keyname><forenames>Alexander</forenames></author></authors><title>TD(0) Learning converges for Polynomial mixing and non-linear functions</title><categories>stat.ML cs.LG</categories><comments>12 pages main text</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Theoretical work on Temporal Difference (TD) learning has provided finite-sample and high-probability guarantees for data generated from Markov chains. However, these bounds typically require linear function approximation, instance-dependent step sizes, algorithmic modifications, and restrictive mixing rates. We present theoretical findings for TD learning under more applicable assumptions, including instance-independent step sizes, full data utilization, and polynomial ergodicity, applicable to both linear and non-linear functions. \textbf{To our knowledge, this is the first proof of TD(0) convergence on Markov data under universal and instance-independent step sizes.} While each contribution is significant on its own, their combination allows these bounds to be effectively utilized in practical application settings. Our results include bounds for linear models and non-linear under generalized gradients and H\"older continuity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05709</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05709</id><created>2025-02-08</created><authors><author><keyname>Lee</keyname><forenames>Junghwan</forenames></author><author><keyname>Xu</keyname><forenames>Chen</forenames></author><author><keyname>Xie</keyname><forenames>Yao</forenames></author></authors><title>Flow-based Conformal Prediction for Multi-dimensional Time Series</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Conformal prediction for time series presents two key challenges: (1) leveraging sequential correlations in features and non-conformity scores and (2) handling multi-dimensional outcomes. We propose a novel conformal prediction method to address these two key challenges by integrating Transformer and Normalizing Flow. Specifically, the Transformer encodes the historical context of time series, and normalizing flow learns the transformation from the base distribution to the distribution of non-conformity scores conditioned on the encoded historical context. This enables the construction of prediction regions by transforming samples from the base distribution using the learned conditional flow. We ensure the marginal coverage by defining the prediction regions as sets in the transformed space that correspond to a predefined probability mass in the base distribution. The model is trained end-to-end by Flow Matching, avoiding the need for computationally intensive numerical solutions of ordinary differential equations. We demonstrate that our proposed method achieves smaller prediction regions compared to the baselines while satisfying the desired coverage through comprehensive experiments using simulated and real-world time series datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05715</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05715</id><created>2025-02-08</created><authors><author><keyname>Xu</keyname><forenames>Ziyu</forenames></author><author><keyname>Wang</keyname><forenames>Catherine</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author><author><keyname>Roeder</keyname><forenames>Kathryn</forenames></author><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author></authors><title>Active multiple testing with proxy p-values and e-values</title><categories>stat.ME</categories><comments>42 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Researchers often lack the resources to test every hypothesis of interest directly or compute test statistics comprehensively, but often possess auxiliary data from which we can compute an estimate of the experimental outcome. We introduce a novel approach for selecting which hypotheses to query a statistic (i.e., run an experiment, perform expensive computation, etc.) in a hypothesis testing setup by leveraging estimates (e.g., from experts, machine learning models, previous experiments, etc.) to compute proxy statistics. Our framework allows a scientist to propose a proxy statistic, and then query the true statistic with some probability based on the value of the proxy. We make no assumptions about how the proxy is derived and it can be arbitrarily dependent with the true statistic. If the true statistic is not queried, the proxy is used in its place. We characterize "active" methods that produce valid p-values and e-values in this setting and utilize this framework in the multiple testing setting to create procedures with false discovery rate (FDR) control. Through simulations and real data analysis of causal effects in scCRISPR screen experiments, we empirically demonstrate that our proxy framework has both high power and low resource usage when our proxies are accurate estimates of the respective true statistics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05717</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05717</id><created>2025-02-08</created><authors><author><keyname>Hainmueller</keyname><forenames>Jens</forenames></author><author><keyname>Liu</keyname><forenames>Jiehan</forenames></author><author><keyname>Liu</keyname><forenames>Ziyi</forenames></author><author><keyname>Mummolo</keyname><forenames>Jonathan</forenames></author><author><keyname>Xu</keyname><forenames>Yiqing</forenames></author></authors><title>A Response to Recent Critiques of Hainmueller, Mummolo and Xu (2019) on   Estimating Conditional Relationships</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Simonsohn (2024a) and Simonsohn (2024b) critique Hainmueller, Mummolo and Xu (2019, HMX), arguing that failing to model nonlinear relationships between the treatment and moderator leads to biased marginal effect estimates and uncontrolled Type-I error rates. While these critiques highlight the issue of under-modeling nonlinearity in applied research, they are fundamentally flawed in several key ways. First, the causal estimand for interaction effects and the necessary identifying assumptions are not clearly defined in these critiques. Once properly stated, the critiques no longer hold. Second, the kernel estimator HMX proposes recovers the true causal effects in the scenarios presented in these recent critiques, which compared effects to the wrong benchmark, producing misleading conclusions. Third, while Generalized Additive Models (GAM) can be a useful exploratory tool (as acknowledged in HMX), they are not designed to estimate marginal effects, and better alternatives exist, particularly in the presence of additional covariates. Our response aims to clarify these misconceptions and provide updated recommendations for researchers studying interaction effects through the estimation of conditional marginal effects. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05719</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05719</id><created>2025-02-08</created><authors><author><keyname>Islam</keyname><forenames>Tanvir</forenames></author></authors><title>Extended Histogram-based Outlier Score (EHBOS)</title><categories>cs.LG cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Histogram-Based Outlier Score (HBOS) is a widely used outlier or anomaly detection method known for its computational efficiency and simplicity. However, its assumption of feature independence limits its ability to detect anomalies in datasets where interactions between features are critical. In this paper, we propose the Extended Histogram-Based Outlier Score (EHBOS), which enhances HBOS by incorporating two-dimensional histograms to capture dependencies between feature pairs. This extension allows EHBOS to identify contextual and dependency-driven anomalies that HBOS fails to detect. We evaluate EHBOS on 17 benchmark datasets, demonstrating its effectiveness and robustness across diverse anomaly detection scenarios. EHBOS outperforms HBOS on several datasets, particularly those where feature interactions are critical in defining the anomaly structure, achieving notable improvements in ROC AUC. These results highlight that EHBOS can be a valuable extension to HBOS, with the ability to model complex feature dependencies. EHBOS offers a powerful new tool for anomaly detection, particularly in datasets where contextual or relational anomalies play a significant role. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05722</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05722</id><created>2025-02-08</created><authors><author><keyname>Saito</keyname><forenames>Naoki</forenames></author><author><keyname>Weber</keyname><forenames>David</forenames></author></authors><title>Explainable and Class-Revealing Signal Feature Extraction via Scattering   Transform and Constrained Zeroth-Order Optimization</title><categories>cs.LG eess.SP math.OC stat.ML</categories><comments>5 pages; 6 figures; submitted to 2025 IEEE Statistical Signal   Processing Workshop</comments><msc-class>42C40, 68T10, 62H30, 65K10, 90C56</msc-class><acm-class>I.5.1; I.2.6; G.1.6; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method to extract discriminant and explainable features from a particular machine learning model, i.e., a combination of the scattering transform and the multiclass logistic regression. Although this model is well-known for its ability to learn various signal classes with high classification rate, it remains elusive to understand why it can generate such successful classification, mainly due to the nonlinearity of the scattering transform. In order to uncover the meaning of the scattering transform coefficients selected by the multiclass logistic regression (with the Lasso penalty), we adopt zeroth-order optimization algorithms to search an input pattern that maximizes the class probability of a class of interest given the learned model. In order to do so, it turns out that imposing sparsity and smoothness of input patterns is important. We demonstrate the effectiveness of our proposed method using a couple of synthetic time-series classification problems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05725</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05725</id><created>2025-02-08</created><authors><author><keyname>Flores</keyname><forenames>Bernardo</forenames></author></authors><title>Predictive Coresets</title><categories>stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Modern data analysis often involves massive datasets with hundreds of thousands of observations, making traditional inference algorithms computationally prohibitive. Coresets are selection methods designed to choose a smaller subset of observations while maintaining similar learning performance. Conventional coreset approaches determine these weights by minimizing the Kullback-Leibler (KL) divergence between the likelihood functions of the full and weighted datasets; as a result, this makes them ill-posed for nonparametric models, where the likelihood is often intractable. We propose an alternative variational method which employs randomized posteriors and finds weights to match the unknown posterior predictive distributions conditioned on the full and reduced datasets. Our approach provides a general algorithm based on predictive recursions suitable for nonparametric priors. We evaluate the performance of the proposed coreset construction on diverse problems, including random partitions and density estimation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05726</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05726</id><created>2025-02-08</created><authors><author><keyname>Teoh</keyname><forenames>Jayden</forenames></author><author><keyname>Li</keyname><forenames>Wenjun</forenames></author><author><keyname>Varakantham</keyname><forenames>Pradeep</forenames></author></authors><title>Improving Environment Novelty Quantification for Effective Unsupervised   Environment Design</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Unsupervised Environment Design (UED) formalizes the problem of autocurricula through interactive training between a teacher agent and a student agent. The teacher generates new training environments with high learning potential, curating an adaptive curriculum that strengthens the student's ability to handle unseen scenarios. Existing UED methods mainly rely on regret, a metric that measures the difference between the agent's optimal and actual performance, to guide curriculum design. Regret-driven methods generate curricula that progressively increase environment complexity for the student but overlook environment novelty -- a critical element for enhancing an agent's generalizability. Measuring environment novelty is especially challenging due to the underspecified nature of environment parameters in UED, and existing approaches face significant limitations. To address this, this paper introduces the Coverage-based Evaluation of Novelty In Environment (CENIE) framework. CENIE proposes a scalable, domain-agnostic, and curriculum-aware approach to quantifying environment novelty by leveraging the student's state-action space coverage from previous curriculum experiences. We then propose an implementation of CENIE that models this coverage and measures environment novelty using Gaussian Mixture Models. By integrating both regret and novelty as complementary objectives for curriculum design, CENIE facilitates effective exploration across the state-action space while progressively increasing curriculum complexity. Empirical evaluations demonstrate that augmenting existing regret-based UED algorithms with CENIE achieves state-of-the-art performance across multiple benchmarks, underscoring the effectiveness of novelty-driven autocurricula for robust generalization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05730</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05730</id><created>2025-02-08</created><authors><author><keyname>Compton</keyname><forenames>Spencer</forenames></author><author><keyname>Valiant</keyname><forenames>Gregory</forenames></author></authors><title>Attainability of Two-Point Testing Rates for Finite-Sample Location   Estimation</title><categories>math.ST cs.DS stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LeCam's two-point testing method yields perhaps the simplest lower bound for estimating the mean of a distribution: roughly, if it is impossible to well-distinguish a distribution centered at $\mu$ from the same distribution centered at $\mu+\Delta$, then it is impossible to estimate the mean by better than $\Delta/2$. It is setting-dependent whether or not a nearly matching upper bound is attainable. We study the conditions under which the two-point testing lower bound can be attained for univariate mean estimation; both in the setting of location estimation (where the distribution is known up to translation) and adaptive location estimation (unknown distribution). Roughly, we will say an estimate nearly attains the two-point testing lower bound if it incurs error that is at most polylogarithmically larger than the Hellinger modulus of continuity for $\tilde{\Omega}(n)$ samples.   Adaptive location estimation is particularly interesting as some distributions admit much better guarantees than sub-Gaussian rates (e.g. $\operatorname{Unif}(\mu-1,\mu+1)$ permits error $\Theta(\frac{1}{n})$, while the sub-Gaussian rate is $\Theta(\frac{1}{\sqrt{n}})$), yet it is not obvious whether these rates may be adaptively attained by one unified approach. Our main result designs an algorithm that nearly attains the two-point testing rate for mixtures of symmetric, log-concave distributions with a common mean. Moreover, this algorithm runs in near-linear time and is parameter-free. In contrast, we show the two-point testing rate is not nearly attainable even for symmetric, unimodal distributions.   We complement this with results for location estimation, showing the two-point testing rate is nearly attainable for unimodal distributions, but unattainable for symmetric distributions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05735</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05735</id><created>2025-02-08</created><authors><author><keyname>Khatamsaz</keyname><forenames>Danial</forenames></author><author><keyname>Wagner</keyname><forenames>Joseph</forenames></author><author><keyname>Vela</keyname><forenames>Brent</forenames></author><author><keyname>Arroyave</keyname><forenames>Raymundo</forenames></author><author><keyname>Allaire</keyname><forenames>Douglas L.</forenames></author></authors><title>Towards Autonomous Experimentation: Bayesian Optimization over Problem   Formulation Space for Accelerated Alloy Development</title><categories>eess.SY cs.CE cs.LG cs.SY math.OC stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Accelerated discovery in materials science demands autonomous systems capable of dynamically formulating and solving design problems. In this work, we introduce a novel framework that leverages Bayesian optimization over a problem formulation space to identify optimal design formulations in line with decision-maker preferences. By mapping various design scenarios to a multi attribute utility function, our approach enables the system to balance conflicting objectives such as ductility, yield strength, density, and solidification range without requiring an exact problem definition at the outset. We demonstrate the efficacy of our method through an in silico case study on a Mo-Nb-Ti-V-W alloy system targeted for gas turbine engine blade applications. The framework converges on a sweet spot that satisfies critical performance thresholds, illustrating that integrating problem formulation discovery into the autonomous design loop can significantly streamline the experimental process. Future work will incorporate human feedback to further enhance the adaptability of the system in real-world experimental settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05773</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05773</id><created>2025-02-08</created><authors><author><keyname>Li</keyname><forenames>Junbo</forenames></author><author><keyname>Wang</keyname><forenames>Zhangyang</forenames></author><author><keyname>Liu</keyname><forenames>Qiang</forenames></author></authors><title>PIPA: Preference Alignment as Prior-Informed Statistical Estimation</title><categories>cs.LG cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Offline preference alignment for language models such as Direct Preference Optimization (DPO) is favored for its effectiveness and simplicity, eliminating the need for costly reinforcement learning. Various offline algorithms have been developed for different data settings, yet they lack a unified understanding.   In this study, we introduce Pior-Informed Preference Alignment (PIPA), a unified, RL-free probabilistic framework that formulates language model preference alignment as a Maximum Likelihood Estimation (MLE) problem with prior constraints. This method effectively accommodates both paired and unpaired data, as well as answer and step-level annotations. We illustrate that DPO and KTO are special cases with different prior constraints within our framework. By integrating different types of prior information, we developed two variations of PIPA: PIPA-M and PIPA-N. Both algorithms demonstrate a $3\sim10\%$ performance enhancement on the GSM8K and MATH benchmarks across all configurations, achieving these gains without additional training or computational costs compared to existing algorithms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05776</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05776</id><created>2025-02-08</created><authors><author><keyname>Bracale</keyname><forenames>Daniele</forenames></author><author><keyname>Banerjee</keyname><forenames>Moulinath</forenames></author><author><keyname>Sun</keyname><forenames>Yuekai</forenames></author><author><keyname>Stoll</keyname><forenames>Kevin</forenames></author><author><keyname>Turki</keyname><forenames>Salam</forenames></author></authors><title>Dynamic Pricing in the Linear Valuation Model using Shape Constraints</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a shape-constrained approach to dynamic pricing for censored data in the linear valuation model that eliminates the need for tuning parameters commonly required in existing methods. Previous works have addressed the challenge of unknown market noise distribution F using strategies ranging from kernel methods to reinforcement learning algorithms, such as bandit techniques and upper confidence bounds (UCB), under the Lipschitz (and stronger) assumption(s) on $F_0$. In contrast, our method relies on isotonic regression under the weaker assumption that $F_0$ is $\alpha$-Holder continuous for some $\alpha \in (0,1]$. We obtain an upper bound on the asymptotic expected regret that matches existing bounds in the literature for $\alpha = 1$ (the Lipschitz case). Simulations and experiments with real-world data obtained by Welltower Inc (a major healthcare Real Estate Investment Trust) consistently demonstrate that our method attains better empirical regret in comparison to several existing methods in the literature while offering the advantage of being completely tuning-parameter free. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05784</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05784</id><created>2025-02-09</created><authors><author><keyname>Nitanda</keyname><forenames>Atsushi</forenames></author><author><keyname>Lee</keyname><forenames>Anzelle</forenames></author><author><keyname>Kai</keyname><forenames>Damian Tan Xing</forenames></author><author><keyname>Sakaguchi</keyname><forenames>Mizuki</forenames></author><author><keyname>Suzuki</keyname><forenames>Taiji</forenames></author></authors><title>Propagation of Chaos for Mean-Field Langevin Dynamics and its   Application to Model Ensemble</title><categories>stat.ML cs.LG</categories><comments>23 pages,</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mean-field Langevin dynamics (MFLD) is an optimization method derived by taking the mean-field limit of noisy gradient descent for two-layer neural networks in the mean-field regime. Recently, the propagation of chaos (PoC) for MFLD has gained attention as it provides a quantitative characterization of the optimization complexity in terms of the number of particles and iterations. A remarkable progress by Chen et al. (2022) showed that the approximation error due to finite particles remains uniform in time and diminishes as the number of particles increases. In this paper, by refining the defective log-Sobolev inequality -- a key result from that earlier work -- under the neural network training setting, we establish an improved PoC result for MFLD, which removes the exponential dependence on the regularization coefficient from the particle approximation term of the optimization complexity. As an application, we propose a PoC-based model ensemble strategy with theoretical guarantees. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05880</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05880</id><created>2025-02-09</created><authors><author><keyname>Baghfalaki</keyname><forenames>Taban</forenames></author><author><keyname>Ganjali</keyname><forenames>Mojtaba</forenames></author><author><keyname>Martins</keyname><forenames>Rui</forenames></author></authors><title>Approximate Bayesian inference for joint partially linear modeling of   longitudinal measurements and spatial time-to-event data</title><categories>stat.ME stat.CO</categories><comments>23</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The integration of longitudinal measurements and survival time in statistical modeling offers a powerful framework for capturing the interplay between these two essential outcomes, particularly when they exhibit associations. However, in scenarios where spatial dependencies among entities are present due to geographic regions, traditional approaches may fall short. In response, this paper introduces a novel approximate Bayesian hierarchical model tailored for jointly analyzing longitudinal and spatial survival outcomes. The model leverages a conditional autoregressive structure to incorporate spatial effects, while simultaneously employing a joint partially linear model to capture the nonlinear influence of time on longitudinal responses. Through extensive simulation studies, the efficacy of the proposed method is rigorously evaluated. Furthermore, its practical utility is demonstrated through an application to real-world HIV/AIDS data sourced from various Brazilian states, showcasing its adaptability and relevance in epidemiological research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05883</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05883</id><created>2025-02-09</created><authors><author><keyname>Khamis</keyname><forenames>Abdelwahed</forenames></author><author><keyname>Khalifa</keyname><forenames>Sara</forenames></author></authors><title>NeuralPrefix: A Zero-shot Sensory Data Imputation Plugin</title><categories>cs.LG cs.AI stat.ML</categories><comments>Accepted in PerCom 25</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Real-world sensing challenges such as sensor failures, communication issues, and power constraints lead to data intermittency. An issue that is known to undermine the traditional classification task that assumes a continuous data stream. Previous works addressed this issue by designing bespoke solutions (i.e. task-specific and/or modality-specific imputation). These approaches, while effective for their intended purposes, had limitations in their applicability across different tasks and sensor modalities. This raises an important question: Can we build a task-agnostic imputation pipeline that is transferable to new sensors without requiring additional training? In this work, we formalise the concept of zero-shot imputation and propose a novel approach that enables the adaptation of pre-trained models to handle data intermittency. This framework, named NeuralPrefix, is a generative neural component that precedes a task model during inference, filling in gaps caused by data intermittency. NeuralPrefix is built as a continuous dynamical system, where its internal state can be estimated at any point in time by solving an Ordinary Differential Equation (ODE). This approach allows for a more versatile and adaptable imputation method, overcoming the limitations of task-specific and modality-specific solutions. We conduct a comprehensive evaluation of NeuralPrefix on multiple sensory datasets, demonstrating its effectiveness across various domains. When tested on intermittent data with a high 50% missing data rate, NeuralPreifx accurately recovers all the missing samples, achieving SSIM score between 0.93-0.96. Zero-shot evaluations show that NeuralPrefix generalises well to unseen datasets, even when the measurements come from a different modality. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05950</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05950</id><created>2025-02-09</created><authors><author><keyname>Kirpichenko</keyname><forenames>Stanislav R.</forenames></author><author><keyname>Utkin</keyname><forenames>Lev V.</forenames></author><author><keyname>Konstantinov</keyname><forenames>Andrei V.</forenames></author><author><keyname>Verbova</keyname><forenames>Natalya M.</forenames></author></authors><title>Survival Concept-Based Learning Models</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concept-based learning enhances prediction accuracy and interpretability by leveraging high-level, human-understandable concepts. However, existing CBL frameworks do not address survival analysis tasks, which involve predicting event times in the presence of censored data -- a common scenario in fields like medicine and reliability analysis. To bridge this gap, we propose two novel models: SurvCBM (Survival Concept-based Bottleneck Model) and SurvRCM (Survival Regularized Concept-based Model), which integrate concept-based learning with survival analysis to handle censored event time data. The models employ the Cox proportional hazards model and the Beran estimator. SurvCBM is based on the architecture of the well-known concept bottleneck model, offering interpretable predictions through concept-based explanations. SurvRCM uses concepts as regularization to enhance accuracy. Both models are trained end-to-end and provide interpretable predictions in terms of concepts. Two interpretability approaches are proposed: one leveraging the linear relationship in the Cox model and another using an instance-based explanation framework with the Beran estimator. Numerical experiments demonstrate that SurvCBM outperforms SurvRCM and traditional survival models, underscoring the importance and advantages of incorporating concept information. The code for the proposed algorithms is publicly available. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05969</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05969</id><created>2025-02-09</created><authors><author><keyname>Fan</keyname><forenames>Yingying</forenames></author><author><keyname>Gao</keyname><forenames>Lan</forenames></author><author><keyname>Lv</keyname><forenames>Jinchi</forenames></author><author><keyname>Xu</keyname><forenames>Xiaocong</forenames></author></authors><title>Asymptotic FDR Control with Model-X Knockoffs: Is Moments Matching   Sufficient?</title><categories>stat.ML cs.LG math.ST stat.TH</categories><comments>90 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a unified theoretical framework for studying the robustness of the model-X knockoffs framework by investigating the asymptotic false discovery rate (FDR) control of the practically implemented approximate knockoffs procedure. This procedure deviates from the model-X knockoffs framework by substituting the true covariate distribution with a user-specified distribution that can be learned using in-sample observations. By replacing the distributional exchangeability condition of the model-X knockoff variables with three conditions on the approximate knockoff statistics, we establish that the approximate knockoffs procedure achieves the asymptotic FDR control. Using our unified framework, we further prove that an arguably most popularly used knockoff variable generation method--the Gaussian knockoffs generator based on the first two moments matching--achieves the asymptotic FDR control when the two-moment-based knockoff statistics are employed in the knockoffs inference procedure. For the first time in the literature, our theoretical results justify formally the effectiveness and robustness of the Gaussian knockoffs generator. Simulation and real data examples are conducted to validate the theoretical findings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05974</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05974</id><created>2025-02-09</created><authors><author><keyname>Liu</keyname><forenames>Haolin</forenames></author><author><keyname>Wei</keyname><forenames>Chen-Yu</forenames></author><author><keyname>Zimmert</keyname><forenames>Julian</forenames></author></authors><title>Decision Making in Hybrid Environments: A Model Aggregation Approach</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work by Foster et al. (2021, 2022, 2023) and Xu and Zeevi (2023) developed the framework of decision estimation coefficient (DEC) that characterizes the complexity of general online decision making problems and provides a general algorithm design principle. These works, however, either focus on the pure stochastic regime where the world remains fixed over time, or the pure adversarial regime where the world arbitrarily changes over time. For the hybrid regime where the dynamics of the world is fixed while the reward arbitrarily changes, they only give pessimistic bounds on the decision complexity. In this work, we propose a general extension of DEC that more precisely characterizes this case. Besides applications in special cases, our framework leads to a flexible algorithm design where the learner learns over subsets of the hypothesis set, trading estimation complexity with decision complexity, which could be of independent interest. Our work covers model-based learning and model-free learning in the hybrid regime, with a newly proposed extension of the bilinear classes (Du et al., 2021) to the adversarial-reward case. We also recover some existing model-free learning results in the pure stochastic regime. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05994</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05994</id><created>2025-02-09</created><authors><author><keyname>Micheli</keyname><forenames>Alessandro</forenames></author><author><keyname>Monod</keyname><forenames>Mélodie</forenames></author><author><keyname>Bhatt</keyname><forenames>Samir</forenames></author></authors><title>Diffusion Models for Inverse Problems in the Exponential Family</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diffusion models have emerged as powerful tools for solving inverse problems, yet prior work has primarily focused on observations with Gaussian measurement noise, restricting their use in real-world scenarios. This limitation persists due to the intractability of the likelihood score, which until now has only been approximated in the simpler case of Gaussian likelihoods. In this work, we extend diffusion models to handle inverse problems where the observations follow a distribution from the exponential family, such as a Poisson or a Binomial distribution. By leveraging the conjugacy properties of exponential family distributions, we introduce the evidence trick, a method that provides a tractable approximation to the likelihood score. In our experiments, we demonstrate that our methodology effectively performs Bayesian inference on spatially inhomogeneous Poisson processes with intensities as intricate as ImageNet images. Furthermore, we demonstrate the real-world impact of our methodology by showing that it performs competitively with the current state-of-the-art in predicting malaria prevalence estimates in Sub-Saharan Africa. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06002</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06002</id><created>2025-02-09</created><authors><author><keyname>Dillon</keyname><forenames>Travis</forenames></author></authors><title>Fixed-strength spherical designs</title><categories>math.ST math.MG stat.TH</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A spherical $t$-design is a finite subset $X$ of the unit sphere such that every polynomial of degree at most $t$ has the same average over $X$ as it does over the entire sphere. Determining the minimum possible size of spherical designs, especially in a fixed dimension as $t \to \infty$, has been an important research topic for several decades. This paper presents results on the complementary asymptotic regime, where $t$ is fixed and the dimension tends to infinity. We combine techniques from algebra, geometry, and probability to prove upper bounds on the size of designs, including an optimal bound for signed spherical designs. We also prove lower and upper bounds for approximate designs and establish an explicit connection between spherical and Gaussian designs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06007</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06007</id><created>2025-02-09</created><authors><author><keyname>He</keyname><forenames>Yihan</forenames></author><author><keyname>Chen</keyname><forenames>Hong-Yu</forenames></author><author><keyname>Cao</keyname><forenames>Yuan</forenames></author><author><keyname>Fan</keyname><forenames>Jianqing</forenames></author><author><keyname>Liu</keyname><forenames>Han</forenames></author></authors><title>Transformers versus the EM Algorithm in Multi-class Clustering</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  LLMs demonstrate significant inference capacities in complicated machine learning tasks, using the Transformer model as its backbone. Motivated by the limited understanding of such models on the unsupervised learning problems, we study the learning guarantees of Transformers in performing multi-class clustering of the Gaussian Mixture Models. We develop a theory drawing strong connections between the Softmax Attention layers and the workflow of the EM algorithm on clustering the mixture of Gaussians. Our theory provides approximation bounds for the Expectation and Maximization steps by proving the universal approximation abilities of multivariate mappings by Softmax functions. In addition to the approximation guarantees, we also show that with a sufficient number of pre-training samples and an initialization, Transformers can achieve the minimax optimal rate for the problem considered. Our extensive simulations empirically verified our theory by revealing the strong learning capacities of Transformers even beyond the assumptions in the theory, shedding light on the powerful inference capacities of LLMs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06008</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06008</id><created>2025-02-09</created><authors><author><keyname>Fan</keyname><forenames>Xinyuan</forenames></author><author><keyname>Leng</keyname><forenames>Chenlei</forenames></author><author><keyname>Wu</keyname><forenames>Weichi</forenames></author></authors><title>Causal Inference under Interference: Regression Adjustment and   Optimality</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In randomized controlled trials without interference, regression adjustment is widely used to enhance the efficiency of treatment effect estimation. This paper extends this efficiency principle to settings with network interference, where a unit's response may depend on the treatments assigned to its neighbors in a network. We make three key contributions: (1) we establish a central limit theorem for a linear regression-adjusted estimator and prove its optimality in achieving the smallest asymptotic variance within a class of linear adjustments; (2) we develop a novel, consistent estimator for the asymptotic variance of this linear estimator; and (3) we propose a nonparametric estimator that integrates kernel smoothing and trimming techniques, demonstrating its asymptotic normality and its optimality in minimizing asymptotic variance within a broader class of nonlinear adjustments. Extensive simulations validate the superior performance of our estimators, and a real-world data application illustrates their practical utility. Our findings underscore the power of regression-based methods and reveal the potential of kernel-and-trimming-based approaches for further enhancing efficiency under network interference. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06011</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06011</id><created>2025-02-09</created><authors><author><keyname>Taufiq</keyname><forenames>Muhammad Faaiz</forenames></author></authors><title>Uncertainty Quantification and Causal Considerations for Off-Policy   Decision Making</title><categories>stat.ML cs.LG</categories><comments>PhD thesis</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Off-policy evaluation (OPE) is a critical challenge in robust decision-making that seeks to assess the performance of a new policy using data collected under a different policy. However, the existing OPE methodologies suffer from several limitations arising from statistical uncertainty as well as causal considerations. In this thesis, we address these limitations by presenting three different works. Firstly, we consider the problem of high variance in the importance-sampling-based OPE estimators. We introduce the Marginal Ratio (MR) estimator, a novel OPE method that reduces variance by focusing on the marginal distribution of outcomes rather than direct policy shifts, improving robustness in contextual bandits. Next, we propose Conformal Off-Policy Prediction (COPP), a principled approach for uncertainty quantification in OPE that provides finite-sample predictive intervals, ensuring robust decision-making in risk-sensitive applications. Finally, we address causal unidentifiability in off-policy decision-making by developing novel bounds for sequential decision settings, which remain valid under arbitrary unmeasured confounding. We apply these bounds to assess the reliability of digital twin models, introducing a falsification framework to identify scenarios where model predictions diverge from real-world behaviour. Our contributions provide new insights into robust decision-making under uncertainty and establish principled methods for evaluating policies in both static and dynamic settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06022</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06022</id><created>2025-02-09</created><authors><author><keyname>Szwagier</keyname><forenames>Tom</forenames></author><author><keyname>Pennec</keyname><forenames>Xavier</forenames></author></authors><title>Nested subspace learning with flags</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Many machine learning methods look for low-dimensional representations of the data. The underlying subspace can be estimated by first choosing a dimension $q$ and then optimizing a certain objective function over the space of $q$-dimensional subspaces (the Grassmannian). Trying different $q$ yields in general non-nested subspaces, which raises an important issue of consistency between the data representations. In this paper, we propose a simple trick to enforce nestedness in subspace learning methods. It consists in lifting Grassmannian optimization problems to flag manifolds (the space of nested subspaces of increasing dimension) via nested projectors. We apply the flag trick to several classical machine learning methods and show that it successfully addresses the nestedness issue. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06044</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06044</id><created>2025-02-09</created><authors><author><keyname>Sopa</keyname><forenames>Getoar</forenames></author><author><keyname>Marusic</keyname><forenames>Juraj</forenames></author><author><keyname>Avella-Medina</keyname><forenames>Marco</forenames></author><author><keyname>Cunningham</keyname><forenames>John P.</forenames></author></authors><title>Scalable Differentially Private Bayesian Optimization</title><categories>stat.ML cs.LG</categories><comments>18 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In recent years, there has been much work on scaling Bayesian Optimization to high-dimensional problems, for example hyperparameter tuning in large neural network models. These scalable methods have been successful, finding high objective values much more quickly than traditional global Bayesian Optimization or random search-based methods. At the same time, these large neural network models often use sensitive data, but preservation of Differential Privacy has not scaled alongside these modern Bayesian Optimization procedures. Here we develop a method to privately estimate potentially high-dimensional parameter spaces using Gradient Informative Bayesian Optimization. Our theoretical results prove that under suitable conditions, our method converges exponentially fast to a ball around the optimal parameter configuration. Moreover, regardless of whether the assumptions are satisfied, we show that our algorithm maintains privacy and empirically demonstrates superior performance to existing methods in the high-dimensional hyperparameter setting. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06046</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06046</id><created>2025-02-09</created><authors><author><keyname>Maity</keyname><forenames>Subha</forenames></author></authors><title>Estimation with missing not at random binary outcomes via exponential   tilts</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study the problem of missing not at random (MNAR) datasets with binary outcomes. We propose an exponential tilt based approach that bypasses any knowledge on 'nonresponse instruments' or 'shadow variables' that are usually required for statistical estimation. We establish a sufficient condition for identifiability of tilt parameters and propose an algorithm to estimate them. Based on these tilt parameter estimates, we propose importance weighted and doubly robust estimators for any mean functions of interest, and validate their performances in a synthetic dataset. In an experiment with the Waterbirds dataset, we utilize our tilt framework to perform unsupervised transfer learning, when the responses are missing from a target domain of interest, and achieve a prediction performance that is comparable to a gold standard. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06051</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06051</id><created>2025-02-09</created><authors><author><keyname>Zhao</keyname><forenames>Qingyue</forenames></author><author><keyname>Ji</keyname><forenames>Kaixuan</forenames></author><author><keyname>Zhao</keyname><forenames>Heyang</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author><author><keyname>Gu</keyname><forenames>Quanquan</forenames></author></authors><title>Nearly Optimal Sample Complexity of Offline KL-Regularized Contextual   Bandits under Single-Policy Concentrability</title><categories>cs.LG cs.AI math.ST stat.ML stat.TH</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  KL-regularized policy optimization has become a workhorse in learning-based decision making, while its theoretical understanding is still very limited. Although recent progress has been made towards settling the sample complexity of KL-regularized contextual bandits, existing sample complexity bounds are either $\tilde{O}(\epsilon^{-2})$ under single-policy concentrability or $\tilde{O}(\epsilon^{-1})$ under all-policy concentrability. In this paper, we propose the \emph{first} algorithm with $\tilde{O}(\epsilon^{-1})$ sample complexity under single-policy concentrability for offline contextual bandits. Our algorithm is designed for general function approximation and based on the principle of \emph{pessimism in the face of uncertainty}. The core of our proof leverages the strong convexity of the KL regularization, and the conditional non-negativity of the gap between the true reward and its pessimistic estimator to refine a mean-value-type risk upper bound to its extreme. This in turn leads to a novel covariance-based analysis, effectively bypassing the need for uniform control over the discrepancy between any two functions in the function class. The near-optimality of our algorithm is demonstrated by an $\tilde{\Omega}(\epsilon^{-1})$ lower bound. Furthermore, we extend our algorithm to contextual dueling bandits and achieve a similar nearly optimal sample complexity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06061</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06061</id><created>2025-02-09</created><authors><author><keyname>Fan</keyname><forenames>Jiajun</forenames></author><author><keyname>Shen</keyname><forenames>Shuaike</forenames></author><author><keyname>Cheng</keyname><forenames>Chaoran</forenames></author><author><keyname>Chen</keyname><forenames>Yuxin</forenames></author><author><keyname>Liang</keyname><forenames>Chumeng</forenames></author><author><keyname>Liu</keyname><forenames>Ge</forenames></author></authors><title>Online Reward-Weighted Fine-Tuning of Flow Matching with Wasserstein   Regularization</title><categories>cs.LG cs.AI cs.CV stat.ML</categories><comments>61 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advancements in reinforcement learning (RL) have achieved great success in fine-tuning diffusion-based generative models. However, fine-tuning continuous flow-based generative models to align with arbitrary user-defined reward functions remains challenging, particularly due to issues such as policy collapse from overoptimization and the prohibitively high computational cost of likelihoods in continuous-time flows. In this paper, we propose an easy-to-use and theoretically sound RL fine-tuning method, which we term Online Reward-Weighted Conditional Flow Matching with Wasserstein-2 Regularization (ORW-CFM-W2). Our method integrates RL into the flow matching framework to fine-tune generative models with arbitrary reward functions, without relying on gradients of rewards or filtered datasets. By introducing an online reward-weighting mechanism, our approach guides the model to prioritize high-reward regions in the data manifold. To prevent policy collapse and maintain diversity, we incorporate Wasserstein-2 (W2) distance regularization into our method and derive a tractable upper bound for it in flow matching, effectively balancing exploration and exploitation of policy optimization. We provide theoretical analyses to demonstrate the convergence properties and induced data distributions of our method, establishing connections with traditional RL algorithms featuring Kullback-Leibler (KL) regularization and offering a more comprehensive understanding of the underlying mechanisms and learning behavior of our approach. Extensive experiments on tasks including target image generation, image compression, and text-image alignment demonstrate the effectiveness of our method, where our method achieves optimal policy convergence while allowing controllable trade-offs between reward maximization and diversity preservation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06067</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06067</id><created>2025-02-09</created><authors><author><keyname>Burt</keyname><forenames>David R.</forenames></author><author><keyname>Berlinghieri</keyname><forenames>Renato</forenames></author><author><keyname>Bates</keyname><forenames>Stephen</forenames></author><author><keyname>Broderick</keyname><forenames>Tamara</forenames></author></authors><title>Lipschitz-Driven Inference: Bias-corrected Confidence Intervals for   Spatial Linear Models</title><categories>stat.ML cs.LG stat.ME</categories><comments>34 pages; 15 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Linear models remain ubiquitous in modern spatial applications - including climate science, public health, and economics - due to their interpretability, speed, and reproducibility. While practitioners generally report a form of uncertainty, popular spatial uncertainty quantification methods do not jointly handle model misspecification and distribution shift - despite both being essentially always present in spatial problems. In the present paper, we show that existing methods for constructing confidence (or credible) intervals in spatial linear models fail to provide correct coverage due to unaccounted-for bias. In contrast to classical methods that rely on an i.i.d. assumption that is inappropriate in spatial problems, in the present work we instead make a spatial smoothness (Lipschitz) assumption. We are then able to propose a new confidence-interval construction that accounts for bias in the estimation procedure. We demonstrate that our new method achieves nominal coverage via both theory and experiments. Code to reproduce experiments is available at https://github.com/DavidRBurt/Lipschitz-Driven-Inference. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06089</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06089</id><created>2025-02-09</created><authors><author><keyname>Gourdeau</keyname><forenames>Pascale</forenames></author><author><keyname>Lechner</keyname><forenames>Tosca</forenames></author><author><keyname>Urner</keyname><forenames>Ruth</forenames></author></authors><title>On the Computability of Multiclass PAC Learning</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study the problem of computable multiclass learnability within the Probably Approximately Correct (PAC) learning framework of Valiant (1984). In the recently introduced computable PAC (CPAC) learning framework of Agarwal et al. (2020), both learners and the functions they output are required to be computable. We focus on the case of finite label space and start by proposing a computable version of the Natarajan dimension and showing that it characterizes CPAC learnability in this setting. We further generalize this result by establishing a meta-characterization of CPAC learnability for a certain family of dimensions: computable distinguishers. Distinguishers were defined by Ben-David et al. (1992) as a certain family of embeddings of the label space, with each embedding giving rise to a dimension. It was shown that the finiteness of each such dimension characterizes multiclass PAC learnability for finite label space in the non-computable setting. We show that the corresponding computable dimensions for distinguishers characterize CPAC learning. We conclude our analysis by proving that the DS dimension, which characterizes PAC learnability for infinite label space, cannot be expressed as a distinguisher (even in the case of finite label space). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06096</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06096</id><created>2025-02-09</created><authors><author><keyname>Saha</keyname><forenames>Aytijhya</forenames></author><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author></authors><title>Post-detection inference for sequential changepoint localization</title><categories>stat.ML cs.AI cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper addresses a fundamental but largely unexplored challenge in sequential changepoint analysis: conducting inference following a detected change. We study the problem of localizing the changepoint using only the data observed up to a data-dependent stopping time at which a sequential detection algorithm $\mathcal A$ declares a change. We first construct confidence sets for the unknown changepoint when pre- and post-change distributions are assumed to be known. We then extend our framework to composite pre- and post-change scenarios. We impose no conditions on the observation space or on $\mathcal A$ -- we only need to be able to run $\mathcal A$ on simulated data sequences. In summary, this work offers both theoretically sound and practically effective tools for sequential changepoint localization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06117</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06117</id><created>2025-02-09</created><authors><author><keyname>Li</keyname><forenames>Dongyuan</forenames></author><author><keyname>Kosugi</keyname><forenames>Satoshi</forenames></author><author><keyname>Zhang</keyname><forenames>Ying</forenames></author><author><keyname>Okumura</keyname><forenames>Manabu</forenames></author><author><keyname>Xia</keyname><forenames>Feng</forenames></author><author><keyname>Jiang</keyname><forenames>Renhe</forenames></author></authors><title>Revisiting Dynamic Graph Clustering via Matrix Factorization</title><categories>cs.LG cs.AI stat.ML</categories><comments>Accepted by TheWebConf 2025 (Oral)</comments><doi>10.1145/3696410.3714646</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic graph clustering aims to detect and track time-varying clusters in dynamic graphs, revealing the evolutionary mechanisms of complex real-world dynamic systems. Matrix factorization-based methods are promising approaches for this task; however, these methods often struggle with scalability and can be time-consuming when applied to large-scale dynamic graphs. Moreover, they tend to lack robustness and are vulnerable to real-world noisy data. To address these issues, we make three key contributions. First, to improve scalability, we propose temporal separated matrix factorization, where a single matrix is divided into multiple smaller matrices for independent factorization, resulting in faster computation. Second, to improve robustness, we introduce bi-clustering regularization, which jointly optimizes graph embedding and clustering, thereby filtering out noisy features from the graph embeddings. Third, to further enhance effectiveness and efficiency, we propose selective embedding updating, where we update only the embeddings of dynamic nodes while the embeddings of static nodes are fixed among different timestamps. Experimental results on six synthetic and five real-world benchmarks demonstrate the scalability, robustness and effectiveness of our proposed method. Source code is available at https://github.com/Clearloveyuan/DyG-MF. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06142</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06142</id><created>2025-02-09</created><authors><author><keyname>Kim</keyname><forenames>Wonyoung</forenames></author><author><keyname>Park</keyname><forenames>Sungwoo</forenames></author><author><keyname>Iyengar</keyname><forenames>Garud</forenames></author><author><keyname>Zeevi</keyname><forenames>Assaf</forenames></author><author><keyname>Oh</keyname><forenames>Min-hwan</forenames></author></authors><title>Linear Bandits with Partially Observable Features</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We introduce a novel linear bandit problem with partially observable features, resulting in partial reward information and spurious estimates. Without proper address for latent part, regret possibly grows linearly in decision horizon $T$, as their influence on rewards are unknown. To tackle this, we propose a novel analysis to handle the latent features and an algorithm that achieves sublinear regret. The core of our algorithm involves (i) augmenting basis vectors orthogonal to the observed feature space, and (ii) introducing an efficient doubly robust estimator. Our approach achieves a regret bound of $\tilde{O}(\sqrt{(d + d_h)T})$, where $d$ is the dimension of observed features, and $d_h$ is the unknown dimension of the subspace of the unobserved features. Notably, our algorithm requires no prior knowledge of the unobserved feature space, which may expand as more features become hidden. Numerical experiments confirm that our algorithm outperforms both non-contextual multi-armed bandits and linear bandit algorithms depending solely on observed features. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06151</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06151</id><created>2025-02-09</created><authors><author><keyname>Hegazy</keyname><forenames>Kareem</forenames></author><author><keyname>Mahoney</keyname><forenames>Michael W.</forenames></author><author><keyname>Erichson</keyname><forenames>N. Benjamin</forenames></author></authors><title>Powerformer: A Transformer with Weighted Causal Attention for   Time-series Forecasting</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transformers have recently shown strong performance in time-series forecasting, but their all-to-all attention mechanism overlooks the (temporal) causal and often (temporally) local nature of data. We introduce Powerformer, a novel Transformer variant that replaces noncausal attention weights with causal weights that are reweighted according to a smooth heavy-tailed decay. This simple yet effective modification endows the model with an inductive bias favoring temporally local dependencies, while still allowing sufficient flexibility to learn the unique correlation structure of each dataset. Our empirical results demonstrate that Powerformer not only achieves state-of-the-art accuracy on public time-series benchmarks, but also that it offers improved interpretability of attention patterns. Our analyses show that the model's locality bias is amplified during training, demonstrating an interplay between time-series data and power-law-based attention. These findings highlight the importance of domain-specific modifications to the Transformer architecture for time-series forecasting, and they establish Powerformer as a strong, efficient, and principled baseline for future research and real-world applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06163</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06163</id><created>2025-02-10</created><authors><author><keyname>Spalding-Jamieson</keyname><forenames>Jack</forenames></author><author><keyname>Robson</keyname><forenames>Eliot Wong</forenames></author><author><keyname>Zheng</keyname><forenames>Da Wei</forenames></author></authors><title>Scalable k-Means Clustering for Large k via Seeded Approximate   Nearest-Neighbor Search</title><categories>cs.LG cs.CG stat.ML</categories><comments>29 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For very large values of $k$, we consider methods for fast $k$-means clustering of massive datasets with $10^7\sim10^9$ points in high-dimensions ($d\geq100$). All current practical methods for this problem have runtimes at least $\Omega(k^2)$. We find that initialization routines are not a bottleneck for this case. Instead, it is critical to improve the speed of Lloyd's local-search algorithm, particularly the step that reassigns points to their closest center. Attempting to improve this step naturally leads us to leverage approximate nearest-neighbor search methods, although this alone is not enough to be practical. Instead, we propose a family of problems we call "Seeded Approximate Nearest-Neighbor Search", for which we propose "Seeded Search-Graph" methods as a solution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06164</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06164</id><created>2025-02-10</created><authors><author><keyname>Chen</keyname><forenames>Panqi</forenames></author><author><keyname>Cheng</keyname><forenames>Lei</forenames></author><author><keyname>Li</keyname><forenames>Jianlong</forenames></author><author><keyname>Li</keyname><forenames>Weichang</forenames></author><author><keyname>Liu</keyname><forenames>Weiqing</forenames></author><author><keyname>Bian</keyname><forenames>Jiang</forenames></author><author><keyname>Fang</keyname><forenames>Shikai</forenames></author></authors><title>Generalized Temporal Tensor Decomposition with Rank-revealing Latent-ODE</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tensor decomposition is a fundamental tool for analyzing multi-dimensional data by learning low-rank factors to represent high-order interactions. While recent works on temporal tensor decomposition have made significant progress by incorporating continuous timestamps in latent factors, they still struggle with general tensor data with continuous indexes not only in the temporal mode but also in other modes, such as spatial coordinates in climate data. Additionally, the problem of determining the tensor rank remains largely unexplored in temporal tensor models. To address these limitations, we propose \underline{G}eneralized temporal tensor decomposition with \underline{R}ank-r\underline{E}vealing laten\underline{T}-ODE (GRET).   Our approach encodes continuous spatial indexes as learnable Fourier features and employs neural ODEs in latent space to learn the temporal trajectories of factors. To automatically reveal the rank of temporal tensors, we introduce a rank-revealing Gaussian-Gamma prior over the factor trajectories. We develop an efficient variational inference scheme with an analytical evidence lower bound, enabling sampling-free optimization. Through extensive experiments on both synthetic and real-world datasets, we demonstrate that GRET not only reveals the underlying ranks of temporal tensors but also significantly outperforms existing methods in prediction performance and robustness against noise. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06165</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06165</id><created>2025-02-10</created><authors><author><keyname>Gong</keyname><forenames>Chengxin</forenames></author><author><keyname>Lin</keyname><forenames>Wei</forenames></author><author><keyname>Zhang</keyname><forenames>Cheng</forenames></author></authors><title>Adversarial Transform Particle Filters</title><categories>stat.ME stat.ML</categories><comments>38 pages, 10 figures, 4 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The particle filter (PF) and the ensemble Kalman filter (EnKF) are widely used for approximate inference in state-space models. From a Bayesian perspective, these algorithms represent the prior by an ensemble of particles and update it to the posterior with new observations over time. However, the PF often suffers from weight degeneracy in high-dimensional settings, whereas the EnKF relies on linear Gaussian assumptions that can introduce significant approximation errors. In this paper, we propose the Adversarial Transform Particle Filter (ATPF), a novel filtering framework that combines the strengths of the PF and the EnKF through adversarial learning. Specifically, importance sampling is used to ensure statistical consistency as in the PF, while adversarially learned transformations, such as neural networks, allow accurate posterior matching for nonlinear and non-Gaussian systems. In addition, we incorporate kernel methods to ease optimization and leverage regularization techniques based on optimal transport for better statistical properties and numerical stability. We provide theoretical guarantees, including generalization bounds for both the analysis and forecast steps of ATPF. Extensive experiments across various nonlinear and non-Gaussian scenarios demonstrate the effectiveness and practical advantages of our method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06168</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06168</id><created>2025-02-10</created><authors><author><keyname>Xu</keyname><forenames>Jianyu</forenames></author><author><keyname>Wang</keyname><forenames>Yining</forenames></author><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Wang</keyname><forenames>Yu-Xiang</forenames></author></authors><title>Dynamic Pricing with Adversarially-Censored Demands</title><categories>stat.ML cs.LG econ.EM math.OC</categories><comments>33 pages, 1 figure</comments><msc-class>91B06, 91B24, 62P20, 62C20, 90B50</msc-class><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an online dynamic pricing problem where the potential demand at each time period $t=1,2,\ldots, T$ is stochastic and dependent on the price. However, a perishable inventory is imposed at the beginning of each time $t$, censoring the potential demand if it exceeds the inventory level. To address this problem, we introduce a pricing algorithm based on the optimistic estimates of derivatives. We show that our algorithm achieves $\tilde{O}(\sqrt{T})$ optimal regret even with adversarial inventory series. Our findings advance the state-of-the-art in online decision-making problems with censored feedback, offering a theoretically optimal solution against adversarial observations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06173</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06173</id><created>2025-02-10</created><authors><author><keyname>Jantre</keyname><forenames>Sanket</forenames></author><author><keyname>Wang</keyname><forenames>Tianle</forenames></author><author><keyname>Park</keyname><forenames>Gilchan</forenames></author><author><keyname>Chopra</keyname><forenames>Kriti</forenames></author><author><keyname>Jeon</keyname><forenames>Nicholas</forenames></author><author><keyname>Qian</keyname><forenames>Xiaoning</forenames></author><author><keyname>Urban</keyname><forenames>Nathan M.</forenames></author><author><keyname>Yoon</keyname><forenames>Byung-Jun</forenames></author></authors><title>Uncertainty-Aware Adaptation of Large Language Models for   Protein-Protein Interaction Analysis</title><categories>cs.LG cs.AI cs.CL stat.AP stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Identification of protein-protein interactions (PPIs) helps derive cellular mechanistic understanding, particularly in the context of complex conditions such as neurodegenerative disorders, metabolic syndromes, and cancer. Large Language Models (LLMs) have demonstrated remarkable potential in predicting protein structures and interactions via automated mining of vast biomedical literature; yet their inherent uncertainty remains a key challenge for deriving reproducible findings, critical for biomedical applications. In this study, we present an uncertainty-aware adaptation of LLMs for PPI analysis, leveraging fine-tuned LLaMA-3 and BioMedGPT models. To enhance prediction reliability, we integrate LoRA ensembles and Bayesian LoRA models for uncertainty quantification (UQ), ensuring confidence-calibrated insights into protein behavior. Our approach achieves competitive performance in PPI identification across diverse disease contexts while addressing model uncertainty, thereby enhancing trustworthiness and reproducibility in computational biology. These findings underscore the potential of uncertainty-aware LLM adaptation for advancing precision medicine and biomedical research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06178</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06178</id><created>2025-02-10</created><authors><author><keyname>Zhu</keyname><forenames>Tansheng</forenames></author><author><keyname>Zhou</keyname><forenames>Hongyu</forenames></author><author><keyname>Jin</keyname><forenames>Ke</forenames></author><author><keyname>Xu</keyname><forenames>Xusheng</forenames></author><author><keyname>Yuan</keyname><forenames>Qiufan</forenames></author><author><keyname>Ji</keyname><forenames>Lijie</forenames></author></authors><title>Bayesian Optimization by Kernel Regression and Density-based Exploration</title><categories>math.OC cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian optimization is highly effective for optimizing expensive-to-evaluate black-box functions, but it faces significant computational challenges due to the high computational complexity of Gaussian processes, which results in a total time complexity that is quartic with respect to the number of iterations. To address this limitation, we propose the Bayesian Optimization by Kernel regression and density-based Exploration (BOKE) algorithm. BOKE uses kernel regression for efficient function approximation, kernel density for exploration, and the improved kernel regression upper confidence bound criteria to guide the optimization process, thus reducing computational costs to quadratic. Our theoretical analysis rigorously establishes the global convergence of BOKE and ensures its robustness. Through extensive numerical experiments on both synthetic and real-world optimization tasks, we demonstrate that BOKE not only performs competitively compared to Gaussian process-based methods but also exhibits superior computational efficiency. These results highlight BOKE's effectiveness in resource-constrained environments, providing a practical approach for optimization problems in engineering applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06188</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06188</id><created>2025-02-10</created><authors><author><keyname>Waudby-Smith</keyname><forenames>Ian</forenames></author><author><keyname>Larsson</keyname><forenames>Martin</forenames></author><author><keyname>Ramdas</keyname><forenames>Aaditya</forenames></author></authors><title>Nonasymptotic and distribution-uniform Koml\'os-Major-Tusn\'ady   approximation</title><categories>math.PR math.ST stat.TH</categories><comments>27 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present nonasymptotic concentration inequalities for sums of independent and identically distributed random variables that yield asymptotic strong Gaussian approximations of Koml\'os, Major, and Tusn\'ady (KMT) [1975,1976]. The constants appearing in our inequalities are either universal or explicit, and thus as corollaries, they imply distribution-uniform generalizations of the aforementioned KMT approximations. In particular, it is shown that uniform integrability of a random variable's $q^{\text{th}}$ moment is both necessary and sufficient for the KMT approximations to hold uniformly at the rate of $o(n^{1/q})$ for $q &gt; 2$ and that having a uniformly lower bounded Sakhanenko parameter -- equivalently, a uniformly upper-bounded Bernstein parameter -- is both necessary and sufficient for the KMT approximations to hold uniformly at the rate of $O(\log n)$. Instantiating these uniform results for a single probability space yields the analogous results of KMT exactly. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06200</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06200</id><created>2025-02-10</created><authors><author><keyname>He</keyname><forenames>Yuchen</forenames></author><author><keyname>Zhang</keyname><forenames>Chihao</forenames></author></authors><title>On the query complexity of sampling from non-log-concave distributions</title><categories>cs.DS cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study the problem of sampling from a $d$-dimensional distribution with density $p(x)\propto e^{-f(x)}$, which does not necessarily satisfy good isoperimetric conditions.   Specifically, we show that for any $L,M$ satisfying $LM\ge d\ge 5$, $\epsilon\in \left\{0,\frac{1}{32}\right\}$, and any algorithm with query accesses to the value of $f(x)$ and $\nabla f(x)$, there exists an $L$-log-smooth distribution with second moment at most $M$ such that the algorithm requires $\left\{\frac{LM}{d\epsilon}\right\}^{\Omega(d)}$ queries to compute a sample whose distribution is within $\epsilon$ in total variation distance to the target distribution. We complement the lower bound with an algorithm requiring $\left\{\frac{LM}{d\epsilon}\right\}^{\mathcal O(d)}$ queries, thereby characterizing the tight (up to the constant in the exponent) query complexity for sampling from the family of non-log-concave distributions.   Our results are in sharp contrast with the recent work of Huang et al. (COLT'24), where an algorithm with quasi-polynomial query complexity was proposed for sampling from a non-log-concave distribution when $M=\mathtt{poly}(d)$. Their algorithm works under the stronger condition that all distributions along the trajectory of the Ornstein-Uhlenbeck process, starting from the target distribution, are $\mathcal O(1)$-log-smooth. We investigate this condition and prove that it is strictly stronger than requiring the target distribution to be $\mathcal O(1)$-log-smooth. Additionally, we study this condition in the context of mixtures of Gaussians.   Finally, we place our results within the broader theme of ``sampling versus optimization'', as studied in Ma et al. (PNAS'19). We show that for a wide range of parameters, sampling is strictly easier than optimization by a super-exponential factor in the dimension $d$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06213</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06213</id><created>2025-02-10</created><authors><author><keyname>Banin</keyname><forenames>Mattia</forenames></author><author><keyname>Barigozzi</keyname><forenames>Matteo</forenames></author><author><keyname>Trapin</keyname><forenames>Luca</forenames></author></authors><title>Predicting Energy Demand with Tensor Factor Models</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Hourly consumption from multiple providers displays pronounced intra-day, intra-week, and annual seasonalities, as well as strong cross-sectional correlations. We introduce a novel approach for forecasting high-dimensional U.S. electricity demand data by accounting for multiple seasonal patterns via tensor factor models. To this end, we restructure the hourly electricity demand data into a sequence of weekly tensors. Each weekly tensor is a three-mode array whose dimensions correspond to the hours of the day, the days of the week, and the number of providers. This multi-dimensional representation enables a factor decomposition that distinguishes among the various seasonal patterns along each mode: factor loadings over the hour dimension highlight intra-day cycles, factor loadings over the day dimension capture differences across weekdays and weekends, and factor loadings over the provider dimension reveal commonalities and shared dynamics among the different entities. We rigorously compare the predictive performance of our tensor factor model against several benchmarks, including traditional vector factor models and cutting-edge functional time series methods. The results consistently demonstrate that the tensor-based approach delivers superior forecasting accuracy at different horizons and provides interpretable factors that align with domain knowledge. Beyond its empirical advantages, our framework offers a systematic way to gain insight into the underlying processes that shape electricity demand patterns. In doing so, it paves the way for more nuanced, data-driven decision-making and can be adapted to address similar challenges in other high-dimensional time series applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06231</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06231</id><created>2025-02-10</created><authors><author><keyname>Karlsson</keyname><forenames>Rickard K. A.</forenames></author><author><keyname>Krijthe</keyname><forenames>Jesse H.</forenames></author></authors><title>Falsification of Unconfoundedness by Testing Independence of Causal   Mechanisms</title><categories>stat.ME cs.LG stat.ML</categories><comments>20 pages, including 5 figures, 2 tables, and appendices</comments><msc-class>62F03 (Primary) 68T01 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A major challenge in estimating treatment effects in observational studies is the reliance on untestable conditions such as the assumption of no unmeasured confounding. In this work, we propose an algorithm that can falsify the assumption of no unmeasured confounding in a setting with observational data from multiple heterogeneous sources, which we refer to as environments. Our proposed falsification strategy leverages a key observation that unmeasured confounding can cause observed causal mechanisms to appear dependent. Building on this observation, we develop a novel two-stage procedure that detects these dependencies with high statistical power while controlling false positives. The algorithm does not require access to randomized data and, in contrast to other falsification approaches, functions even under transportability violations when the environment has a direct effect on the outcome of interest. To showcase the practical relevance of our approach, we show that our method is able to efficiently detect confounding on both simulated and real-world data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06232</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06232</id><created>2025-02-10</created><authors><author><keyname>Larsen</keyname><forenames>Tobias Ellegaard</forenames></author><author><keyname>Ekstrøm</keyname><forenames>Claus Thorn</forenames></author><author><keyname>Petersen</keyname><forenames>Anne Helby</forenames></author></authors><title>Score-Based Causal Discovery with Temporal Background Information</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Temporal background information can improve causal discovery algorithms by orienting edges and identifying relevant adjustment sets. We develop the Temporal Greedy Equivalence Search (TGES) algorithm and terminology essential for score-based causal discovery with tiered background knowledge. TGES learns a restricted Markov equivalence class of directed acyclic graphs (DAGs) using observational data and tiered background knowledge. To construct TGES we formulate a scoring criterion that accounts for tiered background knowledge. We establish theoretical results for TGES, stating that the algorithm always returns a tiered maximally oriented partially directed acyclic graph (tiered MPDAG) and that this tiered MPDAG contains the true DAG in the large sample limit. We present a simulation study indicating a gain from using tiered background knowledge and an improved precision-recall trade-off compared to the temporal PC algorithm. We provide a real-world example on life-course health data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06268</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06268</id><created>2025-02-10</created><authors><author><keyname>Lin</keyname><forenames>Wu</forenames></author><author><keyname>Dangel</keyname><forenames>Felix</forenames></author><author><keyname>Eschenhagen</keyname><forenames>Runa</forenames></author><author><keyname>Bae</keyname><forenames>Juhan</forenames></author><author><keyname>Turner</keyname><forenames>Richard E.</forenames></author><author><keyname>Grosse</keyname><forenames>Roger B.</forenames></author></authors><title>Spectral-factorized Positive-definite Curvature Learning for NN Training</title><categories>stat.ML cs.LG</categories><comments>technical report</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Many training methods, such as Adam(W) and Shampoo, learn a positive-definite curvature matrix and apply an inverse root before preconditioning. Recently, non-diagonal training methods, such as Shampoo, have gained significant attention; however, they remain computationally inefficient and are limited to specific types of curvature information due to the costly matrix root computation via matrix decomposition. To address this, we propose a Riemannian optimization approach that dynamically adapts spectral-factorized positive-definite curvature estimates, enabling the efficient application of arbitrary matrix roots and generic curvature learning. We demonstrate the efficacy and versatility of our approach in positive-definite matrix optimization and covariance adaptation for gradient-free optimization, as well as its efficiency in curvature learning for neural net training. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06321</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06321</id><created>2025-02-10</created><authors><author><keyname>Hakimi</keyname><forenames>Faouzi</forenames><affiliation>UT3</affiliation></author></authors><title>Robust estimation with latin hypercube sampling: a central limit theorem   for Z-estimators</title><categories>math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Latin hypercube sampling (LHS) is a stratified samplingmethod widely used in computer experiments. In thiswork, we extend convergence results on the sample meanwith Latin hypercube sampling to the class of Z -estimators,gathering all estimators that can be written as zeros of asample mean function. In particular, the asymptotic vari-ance of this estimate is obtained. This asymptotic vari-ance is shown to be lower using LHS than using classicindependent and identically distributed sampling. A Cen-tral Limit theorem for Z -estimators under LHS is alsogiven. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06331</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06331</id><created>2025-02-10</created><authors><author><keyname>Caprio</keyname><forenames>Michele</forenames></author><author><keyname>Sale</keyname><forenames>Yusuf</forenames></author><author><keyname>Hüllermeier</keyname><forenames>Eyke</forenames></author></authors><title>Conformal Prediction Regions are Imprecise Highest Density Regions</title><categories>stat.ML cs.LG math.PR</categories><msc-class>Primary: 68T37, Secondary: 62M20, 60G25, 20M32, 15A80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Cella and Martin proved how, under an assumption called consonance, a credal set (i.e. a closed and convex set of probabilities) can be derived from the conformal transducer associated with transductive conformal prediction. We show that the Imprecise Highest Density Region (IHDR) associated with such a credal set corresponds to the classical Conformal Prediction Region. In proving this result, we relate the set of probability density/mass functions (pdf/pmf's) associated with the elements of the credal set to the imprecise probabilistic concept of a cloud. As a result, we establish new relationships between Conformal Prediction and Imprecise Probability (IP) theories. A byproduct of our presentation is the discovery that consonant plausibility functions are monoid homomorphisms, a new algebraic property of an IP tool. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06334</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06334</id><created>2025-02-10</created><authors><author><keyname>F.</keyname><forenames>Luis Torres-Torres</forenames><affiliation>UTP</affiliation></author><author><keyname>Arias-García</keyname><forenames>Jonatan</forenames><affiliation>UTP</affiliation></author><author><keyname>García</keyname><forenames>Hernán F.</forenames><affiliation>UPHF, CERAMATHS</affiliation></author><author><keyname>López-Lopera</keyname><forenames>Andrés F.</forenames><affiliation>UPHF, CERAMATHS</affiliation></author><author><keyname>Vargas-Bonilla</keyname><forenames>Jesús F.</forenames></author></authors><title>Gaussian Process-driven Hidden Markov Models for Early Diagnosis of   Infant Gait Anomalies</title><categories>stat.AP</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gait analysis is critical in the early detection and intervention of motor neurological disorders in infants. Despite its importance, traditional methods often struggle to model the high variability and rapid developmental changes inherent to infant gait. To address these challenges, we propose a probabilistic Gaussian Process (GP)-driven Hidden Markov Model (HMM) to capture the complex temporal dynamics of infant gait cycles and enable automatic recognition of gait anomalies. We use a Multi-Output GP (MoGP) framework to model interdependencies between multiple gait signals, with a composite kernel designed to account for smooth, non-smooth, and periodic behaviors exhibited in gait cycles. The HMM segments gait phases into normal and abnormal states, facilitating the precise identification of pathological movement patterns in stance and swing phases. The proposed model is trained and assessed using a dataset of infants with and without motor neurological disorders via leave-one-subject-out cross-validation. Results demonstrate that the MoGP outperforms Long Short-Term Memory (LSTM) based neural networks in modeling gait dynamics, offering improved accuracy, variance explanation, and temporal alignment. Further, the predictive performance of MoGP provides a principled framework for uncertainty quantification, allowing confidence estimation in gait trajectory predictions. Additionally, the HMM enhances interpretability by explicitly modeling gait phase transitions, improving the detection of subtle anomalies across multiple gait cycles. These findings highlight the MoGP-HMM framework as a robust automatic gait analysis tool, allowing early diagnosis and intervention strategies for infants with neurological motor disorders. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06343</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06343</id><created>2025-02-10</created><authors><author><keyname>Cadei</keyname><forenames>Riccardo</forenames></author><author><keyname>Demirel</keyname><forenames>Ilker</forenames></author><author><keyname>De Bartolomeis</keyname><forenames>Piersilvio</forenames></author><author><keyname>Lindorfer</keyname><forenames>Lukas</forenames></author><author><keyname>Cremer</keyname><forenames>Sylvia</forenames></author><author><keyname>Schmid</keyname><forenames>Cordelia</forenames></author><author><keyname>Locatello</keyname><forenames>Francesco</forenames></author></authors><title>Causal Lifting of Neural Representations: Zero-Shot Generalization for   Causal Inferences</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  A plethora of real-world scientific investigations is waiting to scale with the support of trustworthy predictive models that can reduce the need for costly data annotations. We focus on causal inferences on a target experiment with unlabeled factual outcomes, retrieved by a predictive model fine-tuned on a labeled similar experiment. First, we show that factual outcome estimation via Empirical Risk Minimization (ERM) may fail to yield valid causal inferences on the target population, even in a randomized controlled experiment and infinite training samples. Then, we propose to leverage the observed experimental settings during training to empower generalization to downstream interventional investigations, ``Causal Lifting'' the predictive model. We propose Deconfounded Empirical Risk Minimization (DERM), a new simple learning procedure minimizing the risk over a fictitious target population, preventing potential confounding effects. We validate our method on both synthetic and real-world scientific data. Notably, for the first time, we zero-shot generalize causal inferences on ISTAnt dataset (without annotation) by causal lifting a predictive model on our experiment variant. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06347</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06347</id><created>2025-02-10</created><authors><author><keyname>Kawashima</keyname><forenames>Takayuki</forenames></author><author><keyname>Yoneoka</keyname><forenames>Daisuke</forenames></author><author><keyname>Tanoue</keyname><forenames>Yuta</forenames></author><author><keyname>Eguchi</keyname><forenames>Akifumi</forenames></author><author><keyname>Nomura</keyname><forenames>Shuhei</forenames></author></authors><title>New Insight of Spatial Scan Statistics via Regression Model</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spatial scan statistic is widely used to detect disease clusters in epidemiological surveillance. Since the seminal work by~\cite{kulldorff1997}, numerous extensions have emerged, including methods for defining scan regions, detecting multiple clusters, and expanding statistical models. Notably,~\cite{jung2009} and~\cite{ZHANG20092851} introduced a regression-based approach accounting for covariates, encompassing classical methods such as those of~\cite{kulldorff1997}. Another key extension is the expectation-based approach~\citep{neill2005anomalous,neillphdthesis}, which differs from the population-based approach represented by~\cite{kulldorff1997} in terms of hypothesis testing. In this paper, we bridge the regression-based approach with both expectation-based and population-based approaches. We reveal that the two approaches are separated by a simple difference: the presence or absence of an intercept term in the regression model. Exploiting the above simple difference, we propose new spatial scan statistics under the Gaussian and Bernoulli models. We further extend the regression-based approach by incorporating the well-known sparse L0 penalty and show that the derivation of spatial scan statistics can be expressed as an equivalent optimization problem. Our extended framework accommodates extensions such as space-time scan statistics and detecting multiple clusters while naturally connecting with existing spatial regression-based cluster detection. Considering the relation to case-specific models~\citep{she2011,10.1214/11-STS377}, clusters detected by spatial scan statistics can be viewed as outliers in terms of robust statistics. Numerical experiments with real data illustrate the behavior of our proposed statistics under specified settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06363</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06363</id><created>2025-02-10</created><authors><author><keyname>Iwazaki</keyname><forenames>Shogo</forenames></author><author><keyname>Takeno</keyname><forenames>Shion</forenames></author></authors><title>Improved Regret Analysis in Gaussian Process Bandits: Optimality for   Noiseless Reward, RKHS norm, and Non-Stationary Variance</title><categories>cs.LG stat.ML</categories><comments>35 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Gaussian process (GP) bandit problem, whose goal is to minimize regret under an unknown reward function lying in some reproducing kernel Hilbert space (RKHS). The maximum posterior variance analysis is vital in analyzing near-optimal GP bandit algorithms such as maximum variance reduction (MVR) and phased elimination (PE). Therefore, we first show the new upper bound of the maximum posterior variance, which improves the dependence of the noise variance parameters of the GP. By leveraging this result, we refine the MVR and PE to obtain (i) a nearly optimal regret upper bound in the noiseless setting and (ii) regret upper bounds that are optimal with respect to the RKHS norm of the reward function. Furthermore, as another application of our proposed bound, we analyze the GP bandit under the time-varying noise variance setting, which is the kernelized extension of the linear bandit with heteroscedastic noise. For this problem, we show that MVR and PE-based algorithms achieve noise variance-dependent regret upper bounds, which matches our regret lower bound. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06379</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06379</id><created>2025-02-10</created><authors><author><keyname>Kelvinius</keyname><forenames>Filip Ekström</forenames></author><author><keyname>Zhao</keyname><forenames>Zheng</forenames></author><author><keyname>Lindsten</keyname><forenames>Fredrik</forenames></author></authors><title>Solving Linear-Gaussian Bayesian Inverse Problems with Decoupled   Diffusion Sequential Monte Carlo</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recent line of research has exploited pre-trained generative diffusion models as priors for solving Bayesian inverse problems. We contribute to this research direction by designing a sequential Monte Carlo method for linear-Gaussian inverse problems which builds on ``decoupled diffusion", where the generative process is designed such that larger updates to the sample are possible. The method is asymptotically exact and we demonstrate the effectiveness of our Decoupled Diffusion Sequential Monte Carlo (DDSMC) algorithm on both synthetic data and image reconstruction tasks. Further, we demonstrate how the approach can be extended to discrete data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06381</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06381</id><created>2025-02-10</created><authors><author><keyname>Pin</keyname><forenames>Lukas</forenames></author><author><keyname>Villar</keyname><forenames>Sofía S.</forenames></author><author><keyname>Rosenberger</keyname><forenames>William F.</forenames></author></authors><title>Revisiting Optimal Proportions for Binary Responses: Insights from   Incorporating the Absent Perspective of Type-I Error Rate Control</title><categories>stat.ME math.ST stat.AP stat.TH</categories><comments>14 pages, 3 figures, 5 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This work revisits optimal response-adaptive designs from a type-I error rate perspective, highlighting when and how much these allocations exacerbate type-I error rate inflation - an issue previously undocumented. We explore a range of approaches from the literature that can be applied to reduce type-I error rate inflation. However, we found that all of these approaches fail to give a robust solution to the problem. To address this, we derive two optimal proportions, incorporating the more robust score test (instead of the Wald test) with finite sample estimators (instead of the unknown true values) in the formulation of the optimization problem. One proportion optimizes statistical power and the other minimizes the total number failures in a trail while maintaining a predefined power level. Through simulations based on an early-phase and a confirmatory trial we provide crucial practical insight into how these new optimal proportion designs can offer substantial patient outcomes advantages while controlling type-I error rate. While we focused on binary outcomes, the framework offers valuable insights that naturally extend to other outcome types, multi-armed trials and alternative measures of interest. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06397</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06397</id><created>2025-02-10</created><authors><author><keyname>He</keyname><forenames>Yong</forenames></author><author><keyname>Ma</keyname><forenames>Xiaoyang</forenames></author><author><keyname>Wang</keyname><forenames>Xingheng</forenames></author><author><keyname>Wang</keyname><forenames>Yalin</forenames></author></authors><title>Factor Modelling for Biclustering Large-dimensional Matrix-valued Time   Series</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel unsupervised learning method is proposed in this paper for biclustering large-dimensional matrix-valued time series based on an entirely new latent two-way factor structure. Each block cluster is characterized by its own row and column cluster-specific factors in addition to some common matrix factors which impact on all the matrix time series. We first estimate the global loading spaces by projecting the observation matrices onto the row or column loading space corresponding to common factors. The loading spaces for cluster-specific factors are then further recovered by projecting the observation matrices onto the orthogonal complement space of the estimated global loading spaces. To identify the latent row/column clusters simultaneously for matrix-valued time series, we provide a $K$-means algorithm based on the estimated row/column factor loadings of the cluster-specific weak factors. Theoretically, we derive faster convergence rates for global loading matrices than those of the state-of-the-art methods available in the literature under mild conditions. We also propose an one-pass eigenvalue-ratio method to estimate the numbers of global and cluster-specific factors. The consistency with explicit convergence rates is also established for the estimators of the local loading matrices, the factor numbers and the latent cluster memberships. Numerical experiments with both simulated data as well as a real data example are also reported to illustrate the usefulness of our proposed method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06398</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06398</id><created>2025-02-10</created><authors><author><keyname>Wu</keyname><forenames>Peng</forenames></author><author><keyname>Li</keyname><forenames>Haoxuan</forenames></author><author><keyname>Zheng</keyname><forenames>Chunyuan</forenames></author><author><keyname>Zeng</keyname><forenames>Yan</forenames></author><author><keyname>Chen</keyname><forenames>Jiawei</forenames></author><author><keyname>Liu</keyname><forenames>Yang</forenames></author><author><keyname>Guo</keyname><forenames>Ruocheng</forenames></author><author><keyname>Zhang</keyname><forenames>Kun</forenames></author></authors><title>Learning Counterfactual Outcomes Under Rank Preservation</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Counterfactual inference aims to estimate the counterfactual outcome at the individual level given knowledge of an observed treatment and the factual outcome, with broad applications in fields such as epidemiology, econometrics, and management science. Previous methods rely on a known structural causal model (SCM) or assume the homogeneity of the exogenous variable and strict monotonicity between the outcome and exogenous variable. In this paper, we propose a principled approach for identifying and estimating the counterfactual outcome. We first introduce a simple and intuitive rank preservation assumption to identify the counterfactual outcome without relying on a known structural causal model. Building on this, we propose a novel ideal loss for theoretically unbiased learning of the counterfactual outcome and further develop a kernel-based estimator for its empirical estimation. Our theoretical analysis shows that the rank preservation assumption is not stronger than the homogeneity and strict monotonicity assumptions, and shows that the proposed ideal loss is convex, and the proposed estimator is unbiased. Extensive semi-synthetic and real-world experiments are conducted to demonstrate the effectiveness of the proposed method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06443</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06443</id><created>2025-02-10</created><authors><author><keyname>Cornacchia</keyname><forenames>Elisabetta</forenames></author><author><keyname>Mikulincer</keyname><forenames>Dan</forenames></author><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author></authors><title>Low-dimensional Functions are Efficiently Learnable under Randomly   Biased Distributions</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The problem of learning single index and multi index models has gained significant interest as a fundamental task in high-dimensional statistics. Many recent works have analysed gradient-based methods, particularly in the setting of isotropic data distributions, often in the context of neural network training. Such studies have uncovered precise characterisations of algorithmic sample complexity in terms of certain analytic properties of the target function, such as the leap, information, and generative exponents. These properties establish a quantitative separation between low and high complexity learning tasks. In this work, we show that high complexity cases are rare. Specifically, we prove that introducing a small random perturbation to the data distribution--via a random shift in the first moment--renders any Gaussian single index model as easy to learn as a linear function. We further extend this result to a class of multi index models, namely sparse Boolean functions, also known as Juntas. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06462</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06462</id><created>2025-02-10</created><authors><author><keyname>Franchi</keyname><forenames>Massimo</forenames></author><author><keyname>Paruolo</keyname><forenames>Paolo</forenames></author></authors><title>Inference on the cointegration and the attractor spaces via functional   approximation</title><categories>stat.ME math.ST stat.TH</categories><comments>28 pages. arXiv admin note: text overlap with arXiv:2411.19572</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper discusses semiparametric inference on hypotheses on the cointegration and the attractor spaces for I(1) linear processes, using canonical correlation analysis and functional approximation of Brownian Motions. It proposes inference criteria based on the estimation of the number of common trends in various subsets of variables, and compares them to sequences of tests of hypotheses. The exact limit distribution for one of the test statistics is derived in the univariate case. Properties of the inferential tools are discussed theoretically and illustrated via a Monte Carlo study. An empirical analysis of exchange rates is also included. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06480</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06480</id><created>2025-02-10</created><authors><author><keyname>Boone</keyname><forenames>Victor</forenames></author><author><keyname>Gaujal</keyname><forenames>Bruno</forenames></author></authors><title>Logarithmic Regret of Exploration in Average Reward Markov Decision   Processes</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In average reward Markov decision processes, state-of-the-art algorithms for regret minimization follow a well-established framework: They are model-based, optimistic and episodic. First, they maintain a confidence region from which optimistic policies are computed using a well-known subroutine called Extended Value Iteration (EVI). Second, these policies are used over time windows called episodes, each ended by the Doubling Trick (DT) rule or a variant thereof. In this work, without modifying EVI, we show that there is a significant advantage in replacing (DT) by another simple rule, that we call the Vanishing Multiplicative (VM) rule. When managing episodes with (VM), the algorithm's regret is, both in theory and in practice, as good if not better than with (DT), while the one-shot behavior is greatly improved. More specifically, the management of bad episodes (when sub-optimal policies are being used) is much better under (VM) than (DT) by making the regret of exploration logarithmic rather than linear. These results are made possible by a new in-depth understanding of the contrasting behaviors of confidence regions during good and bad episodes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06492</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06492</id><created>2025-02-10</created><authors><author><keyname>Gorfine</keyname><forenames>Malka</forenames></author><author><keyname>Cook</keyname><forenames>Richard J.</forenames></author><author><keyname>Andersen</keyname><forenames>Per Kragh</forenames></author><author><keyname>Therneau</keyname><forenames>Terry M.</forenames></author><author><keyname>Joly</keyname><forenames>Pierre</forenames></author><author><keyname>Putter</keyname><forenames>Hein</forenames></author><author><keyname>Perme</keyname><forenames>Maja Pohar</forenames></author><author><keyname>Abrahamowicz</keyname><forenames>Michal</forenames></author><author><keyname>Initiative</keyname><forenames>On Behalf of Topic Group 8 "Survival Analysis" of the STRATOS</forenames></author></authors><title>An Overview and Recent Developments in the Analysis of Multistate   Processes</title><categories>stat.ME</categories><comments>62 pages, 3 figures, 3 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multistate models offer a powerful framework for studying disease processes and can be used to formulate intensity-based and more descriptive marginal regression models. They also represent a natural foundation for the construction of joint models for disease processes and dynamic marker processes, as well as joint models incorporating random censoring and intermittent observation times. This article reviews the ways multistate models can be formed and fitted to life history data. Recent work on pseudo-values and the incorporation of random effects to model dependence on the process history and between-process heterogeneity are also discussed. The software available to facilitate such analyses is listed. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06514</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06514</id><created>2025-02-10</created><authors><author><keyname>Amorino</keyname><forenames>Chiara</forenames></author><author><keyname>Nourdin</keyname><forenames>Ivan</forenames></author><author><keyname>Shevchenko</keyname><forenames>Radomyra</forenames></author></authors><title>Fractional interacting particle system: drift parameter estimation via   Malliavin calculus</title><categories>math.ST math.PR stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We address the problem of estimating the drift parameter in a system of $N$ interacting particles driven by additive fractional Brownian motion of Hurst index \( H \geq 1/2 \). Considering continuous observation of the interacting particles over a fixed interval \([0, T]\), we examine the asymptotic regime as \( N \to \infty \). Our main tool is a random variable reminiscent of the least squares estimator but unobservable due to its reliance on the Skorohod integral. We demonstrate that this object is consistent and asymptotically normal by establishing a quantitative propagation of chaos for Malliavin derivatives, which holds for any \( H \in (0,1) \). Leveraging a connection between the divergence integral and the Young integral, we construct computable estimators of the drift parameter. These estimators are shown to be consistent and asymptotically Gaussian. Finally, a numerical study highlights the strong performance of the proposed estimators. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06516</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06516</id><created>2025-02-10</created><authors><author><keyname>Um</keyname><forenames>Soobin</forenames></author><author><keyname>Kim</keyname><forenames>Beomsu</forenames></author><author><keyname>Ye</keyname><forenames>Jong Chul</forenames></author></authors><title>Boost-and-Skip: A Simple Guidance-Free Diffusion for Minority Generation</title><categories>cs.LG cs.AI cs.CV stat.ML</categories><comments>29 pages, 11 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Minority samples are underrepresented instances located in low-density regions of a data manifold, and are valuable in many generative AI applications, such as data augmentation, creative content generation, etc. Unfortunately, existing diffusion-based minority generators often rely on computationally expensive guidance dedicated for minority generation. To address this, here we present a simple yet powerful guidance-free approach called Boost-and-Skip for generating minority samples using diffusion models. The key advantage of our framework requires only two minimal changes to standard generative processes: (i) variance-boosted initialization and (ii) timestep skipping. We highlight that these seemingly-trivial modifications are supported by solid theoretical and empirical evidence, thereby effectively promoting emergence of underrepresented minority features. Our comprehensive experiments demonstrate that Boost-and-Skip greatly enhances the capability of generating minority samples, even rivaling guidance-based state-of-the-art approaches while requiring significantly fewer computations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06525</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06525</id><created>2025-02-10</created><authors><author><keyname>Vauthier</keyname><forenames>Christophe</forenames></author><author><keyname>Mérigot</keyname><forenames>Quentin</forenames></author><author><keyname>Korba</keyname><forenames>Anna</forenames></author></authors><title>Properties of Wasserstein Gradient Flows for the Sliced-Wasserstein   Distance</title><categories>stat.ML cs.LG</categories><comments>32p</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the properties of the Sliced Wasserstein Distance (SW) when employed as an objective functional. The SW metric has gained significant interest in the optimal transport and machine learning literature, due to its ability to capture intricate geometric properties of probability distributions while remaining computationally tractable, making it a valuable tool for various applications, including generative modeling and domain adaptation. Our study aims to provide a rigorous analysis of the critical points arising from the optimization of the SW objective. By computing explicit perturbations, we establish that stable critical points of SW cannot concentrate on segments. This stability analysis is crucial for understanding the behaviour of optimization algorithms for models trained using the SW objective. Furthermore, we investigate the properties of the SW objective, shedding light on the existence and convergence behavior of critical points. We illustrate our theoretical results through numerical experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06536</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06536</id><created>2025-02-10</created><authors><author><keyname>Fokkema</keyname><forenames>Hidde</forenames></author><author><keyname>van Erven</keyname><forenames>Tim</forenames></author><author><keyname>Magliacane</keyname><forenames>Sara</forenames></author></authors><title>Sample-efficient Learning of Concepts with Theoretical Guarantees: from   Data to Concepts without Interventions</title><categories>stat.ML cs.LG</categories><comments>47 pages, 16 figures, 9 Tables, Preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning is a vital part of many real-world systems, but several concerns remain about the lack of interpretability, explainability and robustness of black-box AI systems. Concept-based models (CBM) address some of these challenges by learning interpretable concepts from high-dimensional data, e.g. images, which are used to predict labels. An important issue in CBMs is concept leakage, i.e., spurious information in the learned concepts, which effectively leads to learning "wrong" concepts. Current mitigating strategies are heuristic, have strong assumptions, e.g., they assume that the concepts are statistically independent of each other, or require substantial human interaction in terms of both interventions and labels provided by annotators. In this paper, we describe a framework that provides theoretical guarantees on the correctness of the learned concepts and on the number of required labels, without requiring any interventions. Our framework leverages causal representation learning (CRL) to learn high-level causal variables from low-level data, and learns to align these variables with interpretable concepts. We propose a linear and a non-parametric estimator for this mapping, providing a finite-sample high probability result in the linear case and an asymptotic consistency result for the non-parametric estimator. We implement our framework with state-of-the-art CRL methods, and show its efficacy in learning the correct concepts in synthetic and image benchmarks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06545</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06545</id><created>2025-02-10</created><authors><author><keyname>Marsden</keyname><forenames>Annie</forenames></author><author><keyname>Hazan</keyname><forenames>Elad</forenames></author></authors><title>Dimension-free Regret for Learning Asymmetric Linear Dynamical Systems</title><categories>cs.LG stat.ML</categories><comments>19 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Previously, methods for learning marginally stable linear dynamical systems either required the transition matrix to be symmetric or incurred regret bounds that scale polynomially with the system's hidden dimension. In this work, we introduce a novel method that overcomes this trade-off, achieving dimension-free regret despite the presence of asymmetric matrices and marginal stability. Our method combines spectral filtering with linear predictors and employs Chebyshev polynomials in the complex plane to construct a novel spectral filtering basis. This construction guarantees sublinear regret in an online learning framework, without relying on any statistical or generative assumptions. Specifically, we prove that as long as the transition matrix has eigenvalues with complex component bounded by $1/\mathrm{poly} \log T$, then our method achieves regret $\tilde{O}(T^{9/10})$ when compared to the best linear dynamical predictor in hindsight. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06547</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06547</id><created>2025-02-10</created><authors><author><keyname>Nordenfors</keyname><forenames>Oskar</forenames></author><author><keyname>Flinth</keyname><forenames>Axel</forenames></author></authors><title>Data Augmentation and Regularization for Learning Group Equivariance</title><categories>stat.ML cs.LG math.OC</categories><msc-class>68T07, 20C35, 37N40</msc-class><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In many machine learning tasks, known symmetries can be used as an inductive bias to improve model performance. In this paper, we consider learning group equivariance through training with data augmentation. We summarize results from a previous paper of our own, and extend the results to show that equivariance of the trained model can be achieved through training on augmented data in tandem with regularization. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06564</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06564</id><created>2025-02-10</created><authors><author><keyname>Novikov</keyname><forenames>Gleb</forenames></author></authors><title>Robust Scatter Matrix Estimation for Elliptical Distributions in   Polynomial Time</title><categories>cs.DS cs.LG math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of computationally efficient robust estimation of scatter matrices of elliptical distributions under the strong contamination model. We design polynomial time algorithms that achieve dimension-independent error in Frobenius norm.   Our first result is a sequence of efficient algorithms that approaches nearly optimal error. Specifically, under a mild assumption on the eigenvalues of the scatter matrix $\Sigma$, for every $t \in \mathbb{N}$, we design an estimator that, given $n = d^{O(t)}$ samples, in time $n^{O(t)}$ finds $\hat{\Sigma}$ such that $ \Vert{\Sigma^{-1/2}\, ({\hat{\Sigma} - \Sigma})\, \Sigma^{-1/2}}\Vert_{\text{F}} \le O(t \cdot \varepsilon^{1-\frac{1}{t}})$, where $\varepsilon$ is the fraction of corruption. We do not require any assumptions on the moments of the distribution, while all previously known computationally efficient algorithms for robust covariance/scatter estimation with dimension-independent error rely on strong assumptions on the moments, such as sub-Gaussianity or (certifiable) hypercontractivity.   Furthermore, under a stronger assumption on the eigenvalues of $\Sigma$ (that, in particular, is satisfied by all matrices with constant condition number),   we provide a fast (sub-quadratic in the input size) algorithm that, given nearly optimal number of samples $n = \tilde{O}(d^2/\varepsilon)$, in time $\tilde{O}({nd^2 poly(1/\varepsilon)})$ finds $\hat{\Sigma}$ such that $\Vert\hat{\Sigma} - \Sigma\Vert_{\text{F}} \le O(\Vert{\Sigma}\Vert \cdot \sqrt{\varepsilon})$.   Our approach is based on robust covariance estimation of the spatial sign (the projection onto the sphere of radius $\sqrt{d}$) of elliptical distributions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06567</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06567</id><created>2025-02-10</created><authors><author><keyname>Aubinais</keyname><forenames>Eric</forenames></author><author><keyname>Formont</keyname><forenames>Philippe</forenames></author><author><keyname>Piantanida</keyname><forenames>Pablo</forenames></author><author><keyname>Gassiat</keyname><forenames>Elisabeth</forenames></author></authors><title>Membership Inference Risks in Quantized Models: A Theoretical and   Empirical Study</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Quantizing machine learning models has demonstrated its effectiveness in lowering memory and inference costs while maintaining performance levels comparable to the original models. In this work, we investigate the impact of quantization procedures on the privacy of data-driven models, specifically focusing on their vulnerability to membership inference attacks. We derive an asymptotic theoretical analysis of Membership Inference Security (MIS), characterizing the privacy implications of quantized algorithm weights against the most powerful (and possibly unknown) attacks. Building on these theoretical insights, we propose a novel methodology to empirically assess and rank the privacy levels of various quantization procedures. Using synthetic datasets, we demonstrate the effectiveness of our approach in assessing the MIS of different quantizers. Furthermore, we explore the trade-off between privacy and performance using real-world data and models in the context of molecular modeling. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06577</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06577</id><created>2025-02-10</created><authors><author><keyname>Simoes</keyname><forenames>Francisco N. F. Q.</forenames></author><author><keyname>Feigenbaum</keyname><forenames>Itai</forenames></author><author><keyname>Dastani</keyname><forenames>Mehdi</forenames></author><author><keyname>van Ommen</keyname><forenames>Thijs</forenames></author></authors><title>The Minimal Search Space for Conditional Causal Bandits</title><categories>cs.LG cs.AI stat.ML</categories><comments>Submitted to ICML2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Causal knowledge can be used to support decision-making problems. This has been recognized in the causal bandits literature, where a causal (multi-armed) bandit is characterized by a causal graphical model and a target variable. The arms are then interventions on the causal model, and rewards are samples of the target variable. Causal bandits were originally studied with a focus on hard interventions. We focus instead on cases where the arms are conditional interventions, which more accurately model many real-world decision-making problems by allowing the value of the intervened variable to be chosen based on the observed values of other variables. This paper presents a graphical characterization of the minimal set of nodes guaranteed to contain the optimal conditional intervention, which maximizes the expected reward. We then propose an efficient algorithm with a time complexity of $O(|V| + |E|)$ to identify this minimal set of nodes. We prove that the graphical characterization and the proposed algorithm are correct. Finally, we empirically demonstrate that our algorithm significantly prunes the search space and substantially accelerates convergence rates when integrated into standard multi-armed bandit algorithms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06597</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06597</id><created>2025-02-10</created><authors><author><keyname>Kalinin</keyname><forenames>Nikita P.</forenames></author><author><keyname>Upadhyay</keyname><forenames>Jalaj</forenames></author><author><keyname>Lampert</keyname><forenames>Christoph H.</forenames></author></authors><title>Continual Release Moment Estimation with Differential Privacy</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Joint Moment Estimation (JME), a method for continually and privately estimating both the first and second moments of data with reduced noise compared to naive approaches. JME uses the matrix mechanism and a joint sensitivity analysis to allow the second moment estimation with no additional privacy cost, thereby improving accuracy while maintaining privacy. We demonstrate JME's effectiveness in two applications: estimating the running mean and covariance matrix for Gaussian density estimation, and model training with DP-Adam on CIFAR-10. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06601</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06601</id><created>2025-02-10</created><authors><author><keyname>Mittal</keyname><forenames>Sarthak</forenames></author><author><keyname>Bracher</keyname><forenames>Niels Leif</forenames></author><author><keyname>Lajoie</keyname><forenames>Guillaume</forenames></author><author><keyname>Jaini</keyname><forenames>Priyank</forenames></author><author><keyname>Brubaker</keyname><forenames>Marcus</forenames></author></authors><title>Amortized In-Context Bayesian Posterior Estimation</title><categories>cs.LG cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bayesian inference provides a natural way of incorporating prior beliefs and assigning a probability measure to the space of hypotheses. Current solutions rely on iterative routines like Markov Chain Monte Carlo (MCMC) sampling and Variational Inference (VI), which need to be re-run whenever new observations are available. Amortization, through conditional estimation, is a viable strategy to alleviate such difficulties and has been the guiding principle behind simulation-based inference, neural processes and in-context methods using pre-trained models. In this work, we conduct a thorough comparative analysis of amortized in-context Bayesian posterior estimation methods from the lens of different optimization objectives and architectural choices. Such methods train an amortized estimator to perform posterior parameter inference by conditioning on a set of data examples passed as context to a sequence model such as a transformer. In contrast to language models, we leverage permutation invariant architectures as the true posterior is invariant to the ordering of context examples. Our empirical study includes generalization to out-of-distribution tasks, cases where the assumed underlying model is misspecified, and transfer from simulated to real problems. Subsequently, it highlights the superiority of the reverse KL estimator for predictive problems, especially when combined with the transformer architecture and normalizing flows. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06605</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06605</id><created>2025-02-10</created><authors><author><keyname>Wadsworth</keyname><forenames>Spencer</forenames></author><author><keyname>Niemi</keyname><forenames>Jarad</forenames></author></authors><title>Quantile Forecast Matching with a Bayesian Quantile Gaussian Process   Model</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A set of probabilities along with corresponding quantiles are often used to define predictive distributions or probabilistic forecasts. These quantile predictions offer easily interpreted uncertainty of an event, and quantiles are generally straightforward to estimate using standard statistical and machine learning methods. However, compared to a distribution defined by a probability density or cumulative distribution function, a set of quantiles has less distributional information. When given estimated quantiles, it may be desirable to estimate a fully defined continuous distribution function. Many researchers do so to make evaluation or ensemble modeling simpler. Most existing methods for fitting a distribution to quantiles lack accurate representation of the inherent uncertainty from quantile estimation or are limited in their applications. In this manuscript, we present a Gaussian process model, the quantile Gaussian process, which is based on established theory of quantile functions and sample quantiles, to construct a probability distribution given estimated quantiles. A Bayesian application of the quantile Gaussian process is evaluated for parameter inference and distribution approximation in simulation studies. The quantile Gaussian process is used to approximate the distributions of quantile forecasts from the 2023-24 US Centers for Disease Control collaborative flu forecasting initiative. The simulation studies and data analysis show that the quantile Gaussian process leads to accurate inference on model parameters, estimation of a continuous distribution, and uncertainty quantification of sample quantiles. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06628</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06628</id><created>2025-02-10</created><authors><author><keyname>Vos</keyname><forenames>Paul W.</forenames></author></authors><title>Random Variables aren't Random</title><categories>stat.OT</categories><comments>17 pages, no figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper examines the foundational concept of random variables in probability theory and statistical inference, demonstrating that their mathematical definition requires no reference to randomization or hypothetical repeated sampling. We show how measure-theoretic probability provides a framework for modeling populations through distributions, leading to three key contributions. First, we establish that random variables, properly understood as measurable functions, can be fully characterized without appealing to infinite hypothetical samples. Second, we demonstrate how this perspective enables statistical inference through logical rather than probabilistic reasoning, extending the reductio ad absurdum argument from deductive to inductive inference. Third, we show how this framework naturally leads to information-based assessment of statistical procedures, replacing traditional inference metrics that emphasize bias and variance with information-based approaches that better describe the families of distributions used in parametric inference. This reformulation addresses long-standing debates in statistical inference while providing a more coherent theoretical foundation. Our approach offers an alternative to traditional frequentist inference that maintains mathematical rigor while avoiding the philosophical complications inherent in repeated sampling interpretations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06645</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06645</id><created>2025-02-10</created><authors><author><keyname>Bevanda</keyname><forenames>Petar</forenames></author><author><keyname>Beier</keyname><forenames>Max</forenames></author><author><keyname>Lederer</keyname><forenames>Armin</forenames></author><author><keyname>Capone</keyname><forenames>Alexandre</forenames></author><author><keyname>Sosnowski</keyname><forenames>Stefan</forenames></author><author><keyname>Hirche</keyname><forenames>Sandra</forenames></author></authors><title>Koopman-Equivariant Gaussian Processes</title><categories>cs.LG cs.SY eess.SY stat.ML</categories><comments>Accepted to the 28th International Conference on Artificial   Intelligence and Statistics (AISTATS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Credible forecasting and representation learning of dynamical systems are of ever-increasing importance for reliable decision-making. To that end, we propose a family of Gaussian processes (GP) for dynamical systems with linear time-invariant responses, which are nonlinear only in initial conditions. This linearity allows us to tractably quantify forecasting and representational uncertainty, simultaneously alleviating the challenge of computing the distribution of trajectories from a GP-based dynamical system and enabling a new probabilistic treatment of learning Koopman operator representations. Using a trajectory-based equivariance -- which we refer to as \textit{Koopman equivariance} -- we obtain a GP model with enhanced generalization capabilities. To allow for large-scale regression, we equip our framework with variational inference based on suitable inducing points. Experiments demonstrate on-par and often better forecasting performance compared to kernel-based methods for learning dynamical systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06661</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06661</id><created>2025-02-10</created><authors><author><keyname>Little</keyname><forenames>Camille</forenames></author><author><keyname>Zheng</keyname><forenames>Lili</forenames></author><author><keyname>Allen</keyname><forenames>Genevera</forenames></author></authors><title>iLOCO: Distribution-Free Inference for Feature Interactions</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Feature importance measures are widely studied and are essential for understanding model behavior, guiding feature selection, and enhancing interpretability. However, many machine learning fitted models involve complex, higher-order interactions between features. Existing feature importance metrics fail to capture these higher-order effects while existing interaction metrics often suffer from limited applicability or excessive computation; no methods exist to conduct statistical inference for feature interactions. To bridge this gap, we first propose a new model-agnostic metric, interaction Leave-One-Covariate-Out iLOCO, for measuring the importance of higher-order feature interactions. Next, we leverage recent advances in LOCO inference to develop distribution-free and assumption-light confidence intervals for our iLOCO metric. To address computational challenges, we also introduce an ensemble learning method for calculating the iLOCO metric and confidence intervals that we show is both computationally and statistically efficient. We validate our iLOCO metric and our confidence intervals on both synthetic and real data sets, showing that our approach outperforms existing methods and provides the first inferential approach to detecting feature interactions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06671</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06671</id><created>2025-02-10</created><authors><author><keyname>Fan</keyname><forenames>Jianqing</forenames></author><author><keyname>Ge</keyname><forenames>Jiawei</forenames></author><author><keyname>Hou</keyname><forenames>Jikai</forenames></author></authors><title>Covariates-Adjusted Mixed-Membership Estimation: A Novel Network Model   with Optimal Guarantees</title><categories>math.ST stat.ML stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper addresses the problem of mixed-membership estimation in networks, where the goal is to efficiently estimate the latent mixed-membership structure from the observed network. Recognizing the widespread availability and valuable information carried by node covariates, we propose a novel network model that incorporates both community information, as represented by the Degree-Corrected Mixed Membership (DCMM) model, and node covariate similarities to determine connections.   We investigate the regularized maximum likelihood estimation (MLE) for this model and demonstrate that our approach achieves optimal estimation accuracy for both the similarity matrix and the mixed-membership, in terms of both the Frobenius norm and the entrywise loss. Since directly analyzing the original convex optimization problem is intractable, we employ nonconvex optimization to facilitate the analysis. A key contribution of our work is identifying a crucial assumption that bridges the gap between convex and nonconvex solutions, enabling the transfer of statistical guarantees from the nonconvex approach to its convex counterpart. Importantly, our analysis extends beyond the MLE loss and the mean squared error (MSE) used in matrix completion problems, generalizing to all the convex loss functions. Consequently, our analysis techniques extend to a broader set of applications, including ranking problems based on pairwise comparisons.   Finally, simulation experiments validate our theoretical findings, and real-world data analyses confirm the practical relevance of our model. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06678</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06678</id><created>2025-02-10</created><authors><author><keyname>Lau</keyname><forenames>Ivan</forenames></author><author><keyname>Scarlett</keyname><forenames>Jonathan</forenames></author></authors><title>Quantile Multi-Armed Bandits with 1-bit Feedback</title><categories>stat.ML cs.IT cs.LG math.IT</categories><comments>ALT 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a variant of best-arm identification involving elements of risk sensitivity and communication constraints. Specifically, the goal of the learner is to identify the arm with the highest quantile reward, while the communication from an agent (who observes rewards) and the learner (who chooses actions) is restricted to only one bit of feedback per arm pull. We propose an algorithm that utilizes noisy binary search as a subroutine, allowing the learner to estimate quantile rewards through 1-bit feedback. We derive an instance-dependent upper bound on the sample complexity of our algorithm and provide an algorithm-independent lower bound for specific instances, with the two matching to within logarithmic factors under mild conditions, or even to within constant factors in certain low error probability scaling regimes. The lower bound is applicable even in the absence of communication constraints, and thus we conclude that restricting to 1-bit feedback has a minimal impact on the scaling of the sample complexity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06685</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06685</id><created>2025-02-10</created><authors><author><keyname>He</keyname><forenames>Jiajun</forenames></author><author><keyname>Du</keyname><forenames>Yuanqi</forenames></author><author><keyname>Vargas</keyname><forenames>Francisco</forenames></author><author><keyname>Zhang</keyname><forenames>Dinghuai</forenames></author><author><keyname>Padhy</keyname><forenames>Shreyas</forenames></author><author><keyname>OuYang</keyname><forenames>RuiKang</forenames></author><author><keyname>Gomes</keyname><forenames>Carla</forenames></author><author><keyname>Hernández-Lobato</keyname><forenames>José Miguel</forenames></author></authors><title>No Trick, No Treat: Pursuits and Challenges Towards Simulation-free   Training of Neural Samplers</title><categories>cs.LG stat.ML</categories><comments>21 pages, 5 figures, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the sampling problem, where the aim is to draw samples from a distribution whose density is known only up to a normalization constant. Recent breakthroughs in generative modeling to approximate a high-dimensional data distribution have sparked significant interest in developing neural network-based methods for this challenging problem. However, neural samplers typically incur heavy computational overhead due to simulating trajectories during training. This motivates the pursuit of simulation-free training procedures of neural samplers. In this work, we propose an elegant modification to previous methods, which allows simulation-free training with the help of a time-dependent normalizing flow. However, it ultimately suffers from severe mode collapse. On closer inspection, we find that nearly all successful neural samplers rely on Langevin preconditioning to avoid mode collapsing. We systematically analyze several popular methods with various objective functions and demonstrate that, in the absence of Langevin preconditioning, most of them fail to adequately cover even a simple target. Finally, we draw attention to a strong baseline by combining the state-of-the-art MCMC method, Parallel Tempering (PT), with an additional generative model to shed light on future explorations of neural samplers. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06689</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06689</id><created>2025-02-10</created><authors><author><keyname>Sule</keyname><forenames>Shashank</forenames></author><author><keyname>Czaja</keyname><forenames>Wojciech</forenames></author></authors><title>Neumann eigenmaps for landmark embedding</title><categories>math.ST cs.LG cs.NA math.NA stat.ML stat.TH</categories><msc-class>65C50, 68T10, 62M15</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We present Neumann eigenmaps (NeuMaps), a novel approach for enhancing the standard diffusion map embedding using landmarks, i.e distinguished samples within the dataset. By interpreting these landmarks as a subgraph of the larger data graph, NeuMaps are obtained via the eigendecomposition of a renormalized Neumann Laplacian. We show that NeuMaps offer two key advantages: (1) they provide a computationally efficient embedding that accurately recovers the diffusion distance associated with the reflecting random walk on the subgraph, and (2) they naturally incorporate the Nystr\"om extension within the diffusion map framework through the discrete Neumann boundary condition. Through examples in digit classification and molecular dynamics, we demonstrate that NeuMaps not only improve upon existing landmark-based embedding methods but also enhance the stability of diffusion map embeddings to the removal of highly significant points. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06719</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06719</id><created>2025-02-10</created><authors><author><keyname>Sheshukova</keyname><forenames>Marina</forenames></author><author><keyname>Samsonov</keyname><forenames>Sergey</forenames></author><author><keyname>Belomestny</keyname><forenames>Denis</forenames></author><author><keyname>Moulines</keyname><forenames>Eric</forenames></author><author><keyname>Shao</keyname><forenames>Qi-Man</forenames></author><author><keyname>Zhang</keyname><forenames>Zhuo-Song</forenames></author><author><keyname>Naumov</keyname><forenames>Alexey</forenames></author></authors><title>Gaussian Approximation and Multiplier Bootstrap for Stochastic Gradient   Descent</title><categories>stat.ML cs.LG math.OC math.PR math.ST stat.TH</categories><msc-class>60F05, 62L20, 93E35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we establish non-asymptotic convergence rates in the central limit theorem for Polyak-Ruppert-averaged iterates of stochastic gradient descent (SGD). Our analysis builds on the result of the Gaussian approximation for nonlinear statistics of independent random variables of Shao and Zhang (2022). Using this result, we prove the non-asymptotic validity of the multiplier bootstrap for constructing the confidence sets for the optimal solution of an optimization problem. In particular, our approach avoids the need to approximate the limiting covariance of Polyak-Ruppert SGD iterates, which allows us to derive approximation rates in convex distance of order up to $1/\sqrt{n}$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06751</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06751</id><created>2025-02-10</created><authors><author><keyname>Vitvitskyi</keyname><forenames>Alex</forenames></author><author><keyname>Araújo</keyname><forenames>João G. M.</forenames></author><author><keyname>Lackenby</keyname><forenames>Marc</forenames></author><author><keyname>Veličković</keyname><forenames>Petar</forenames></author></authors><title>What makes a good feedforward computational graph?</title><categories>cs.LG cs.AI cs.SI stat.ML</categories><comments>Work in progress -- comments welcome. 16 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  As implied by the plethora of literature on graph rewiring, the choice of computational graph employed by a neural network can make a significant impact on its downstream performance. Certain effects related to the computational graph, such as under-reaching and over-squashing, may even render the model incapable of learning certain functions. Most of these effects have only been thoroughly studied in the domain of undirected graphs; however, recent years have seen a significant rise in interest in feedforward computational graphs: directed graphs without any back edges. In this paper, we study the desirable properties of a feedforward computational graph, discovering two important complementary measures: fidelity and mixing time, and evaluating a few popular choices of graphs through the lens of these measures. Our study is backed by both theoretical analyses of the metrics' asymptotic behaviour for various graphs, as well as correlating these metrics to the performance of trained neural network models using the corresponding graphs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06753</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06753</id><created>2025-02-10</created><authors><author><keyname>Semenova</keyname><forenames>Elizaveta</forenames></author></authors><title>Case for a unified surrogate modelling framework in the age of AI</title><categories>stat.CO cs.LG</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Surrogate models are widely used in natural sciences, engineering, and machine learning to approximate complex systems and reduce computational costs. However, the current landscape lacks standardisation across key stages of the pipeline, including data collection, sampling design, model class selection, evaluation metrics, and downstream task performance analysis. This fragmentation limits reproducibility, reliability, and cross-domain applicability. The issue has only been exacerbated by the AI revolution and a new suite of surrogate model classes that it offers. In this position paper, we argue for the urgent need for a unified framework to guide the development and evaluation of surrogate models. We outline essential steps for constructing a comprehensive pipeline and discuss alternative perspectives, such as the benefits of domain-specific frameworks. By advocating for a standardised approach, this paper seeks to improve the reliability of surrogate modelling, foster cross-disciplinary knowledge transfer, and, as a result, accelerate scientific progress. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06758</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06758</id><created>2025-02-10</created><authors><author><keyname>Imai</keyname><forenames>Kosuke</forenames></author><author><keyname>Li</keyname><forenames>Michael Lingzhi</forenames></author></authors><title>Comment on "Generic machine learning inference on heterogeneous   treatment effects in randomized experiments."</title><categories>stat.ME econ.EM</categories><comments>Comment on arXiv:1712.04802</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the split-sample robust inference (SSRI) methodology proposed by Chernozhukov, Demirer, Duflo, and Fernandez-Val (CDDF) for quantifying uncertainty in heterogeneous treatment effect estimation. While SSRI effectively accounts for randomness in data splitting, its computational cost can be prohibitive when combined with complex machine learning (ML) models. We present an alternative randomization inference (RI) approach that maintains SSRI's generality without requiring repeated data splitting. By leveraging cross-fitting and design-based inference, RI achieves valid confidence intervals while significantly reducing computational burden. We compare the two methods through simulation, demonstrating that RI retains statistical efficiency while being more practical for large-scale applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06765</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06765</id><created>2025-02-10</created><authors><author><keyname>Müller</keyname><forenames>Manuel M.</forenames></author><author><keyname>Luo</keyname><forenames>Yuetian</forenames></author><author><keyname>Barber</keyname><forenames>Rina Foygel</forenames></author></authors><title>Are all models wrong? Fundamental limits in distribution-free empirical   model falsification</title><categories>math.ST cs.LG stat.ML stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In statistics and machine learning, when we train a fitted model on available data, we typically want to ensure that we are searching within a model class that contains at least one accurate model -- that is, we would like to ensure an upper bound on the model class risk (the lowest possible risk that can be attained by any model in the class). However, it is also of interest to establish lower bounds on the model class risk, for instance so that we can determine whether our fitted model is at least approximately optimal within the class, or, so that we can decide whether the model class is unsuitable for the particular task at hand. Particularly in the setting of interpolation learning where machine learning models are trained to reach zero error on the training data, we might ask if, at the very least, a positive lower bound on the model class risk is possible -- or are we unable to detect that "all models are wrong"? In this work, we answer these questions in a distribution-free setting by establishing a model-agnostic, fundamental hardness result for the problem of constructing a lower bound on the best test error achievable over a model class, and examine its implications on specific model classes such as tree-based methods and linear regression. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.06777</identifier><datestamp>2025-02-11</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.06777</id><created>2025-02-10</created><authors><author><keyname>Han</keyname><forenames>Yuxuan</forenames></author><author><keyname>Zhong</keyname><forenames>Han</forenames></author><author><keyname>Lu</keyname><forenames>Miao</forenames></author><author><keyname>Blanchet</keyname><forenames>Jose</forenames></author><author><keyname>Zhou</keyname><forenames>Zhengyuan</forenames></author></authors><title>Learning an Optimal Assortment Policy under Observational Data</title><categories>stat.ML cs.LG math.OC math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the fundamental problem of offline assortment optimization under the Multinomial Logit (MNL) model, where sellers must determine the optimal subset of the products to offer based solely on historical customer choice data. While most existing approaches to learning-based assortment optimization focus on the online learning of the optimal assortment through repeated interactions with customers, such exploration can be costly or even impractical in many real-world settings. In this paper, we consider the offline learning paradigm and investigate the minimal data requirements for efficient offline assortment optimization. To this end, we introduce Pessimistic Rank-Breaking (PRB), an algorithm that combines rank-breaking with pessimistic estimation. We prove that PRB is nearly minimax optimal by establishing the tight suboptimality upper bound and a nearly matching lower bound. This further shows that "optimal item coverage" - where each item in the optimal assortment appears sufficiently often in the historical data - is both sufficient and necessary for efficient offline learning. This significantly relaxes the previous requirement of observing the complete optimal assortment in the data. Our results provide fundamental insights into the data requirements for offline assortment optimization under the MNL model. </abstract></arXiv></metadata></record>
