<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:1802.07051</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>1802.07051</id><created>2018-02-20</created><updated>2020-02-07</updated><authors><author><keyname>Lin</keyname><forenames>Hanti</forenames></author><author><keyname>Zhang</keyname><forenames>Jiji</forenames></author></authors><title>On Learning Causal Structures from Non-Experimental Data without Any   Faithfulness Assumption</title><categories>stat.ML cs.LG stat.ME</categories><comments>To be published in Proceedings of Machine Learning Research, volume   117</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the problem of learning, from non-experimental data, the causal (Markov equivalence) structure of the true, unknown causal Bayesian network (CBN) on a given, fixed set of (categorical) variables. This learning problem is known to be so hard that there is no learning algorithm that converges to the truth for all possible CBNs (on the given set of variables). So the convergence property has to be sacrificed for some CBNs---but for which? In response, the standard practice has been to design and employ learning algorithms that secure the convergence property for at least all the CBNs that satisfy the famous faithfulness condition, which implies sacrificing the convergence property for some CBNs that violate the faithfulness condition (Spirtes et al. 2000). This standard design practice can be justified by assuming---that is, accepting on faith---that the true, unknown CBN satisfies the faithfulness condition. But the real question is this: Is it possible to explain, without assuming the faithfulness condition or any of its weaker variants, why it is mandatory rather than optional to follow the standard design practice? This paper aims to answer the above question in the affirmative. We first define an array of modes of convergence to the truth as desiderata that might or might not be achieved by a causal learning algorithm. Those modes of convergence concern (i) how pervasive the domain of convergence is on the space of all possible CBNs and (ii) how uniformly the convergence happens. Then we prove a result to the following effect: for any learning algorithm that tackles the causal learning problem in question, if it achieves the best achievable mode of convergence (considered in this paper), then it must follow the standard design practice of converging to the truth for at least all CBNs that satisfy the faithfulness condition---it is a requirement, not an option. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2010.01510</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2010.01510</id><created>2020-10-04</created><updated>2021-04-30</updated><authors><author><keyname>Vono</keyname><forenames>Maxime</forenames></author><author><keyname>Dobigeon</keyname><forenames>Nicolas</forenames></author><author><keyname>Chainais</keyname><forenames>Pierre</forenames></author></authors><title>High-dimensional Gaussian sampling: a review and a unifying approach   based on a stochastic proximal point algorithm</title><categories>stat.CO</categories><comments>53 pages, 11 figures</comments><doi>10.1137/20M1371026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient sampling from a high-dimensional Gaussian distribution is an old but high-stake issue. Vanilla Cholesky samplers imply a computational cost and memory requirements which can rapidly become prohibitive in high dimension. To tackle these issues, multiple methods have been proposed from different communities ranging from iterative numerical linear algebra to Markov chain Monte Carlo (MCMC) approaches. Surprisingly, no complete review and comparison of these methods have been conducted. This paper aims at reviewing all these approaches by pointing out their differences, close relations, benefits and limitations. In addition to this state of the art, this paper proposes a unifying Gaussian simulation framework by deriving a stochastic counterpart of the celebrated proximal point algorithm in optimization. This framework offers a novel and unifying revisit of most of the existing MCMC approaches while extending them. Guidelines to choose the appropriate Gaussian simulation method for a given sampling problem in high dimension are proposed and illustrated with numerical examples. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2010.12696</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2010.12696</id><created>2020-10-23</created><updated>2021-06-16</updated><authors><author><keyname>Zheng</keyname><forenames>Xiaotian</forenames></author><author><keyname>Kottas</keyname><forenames>Athanasios</forenames></author><author><keyname>Sans√≥</keyname><forenames>Bruno</forenames></author></authors><title>On Construction and Estimation of Stationary Mixture Transition   Distribution Models</title><categories>stat.ME</categories><journal-ref>Journal of Computational and Graphical Statistics, 31, 283-293   (2022)</journal-ref><doi>10.1080/10618600.2021.1981342</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixture transition distribution time series models build high-order dependence through a weighted combination of first-order transition densities for each one of a specified number of lags. We present a framework to construct stationary transition mixture distribution models that extend beyond linear, Gaussian dynamics. We study conditions for first-order strict stationarity which allow for different constructions with either continuous or discrete families for the first-order transition densities given a pre-specified family for the marginal density, and with general forms for the resulting conditional expectations. Inference and prediction are developed under the Bayesian framework with particular emphasis on flexible, structured priors for the mixture weights. Model properties are investigated both analytically and through synthetic data examples. Finally, Poisson and Lomax examples are illustrated through real data applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2104.13132</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2104.13132</id><created>2021-04-27</created><authors><author><keyname>Klotz</keyname><forenames>Lutz</forenames></author><author><keyname>Frank</keyname><forenames>Michael</forenames></author></authors><title>Stability of trigonometric approximation in $L^p$ and applications to   prediction theory</title><categories>math.ST math.PR stat.TH</categories><comments>34 pages</comments><msc-class>60G25 (Primary) 62M10, 91B84 (Secondary)</msc-class><journal-ref>Anal. Funct. Anal. 13 (2022), article no. 52</journal-ref><doi>10.1007/s43034-022-00197-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\Gamma$ be an LCA group and $(\mu_n)$ be a sequence of bounded regular Borel measures on $\Gamma$ tending to a measure $\mu_0$. Let $G$ be the dual group of $\Gamma$, $S$ be a non-empty subset of $G \setminus \{ 0 \}$, and $[{\mathcal T}(S)]_{\mu_n,p}$ the subspace of $L^p(\mu_n)$, $p \in (0,\infty)$, spanned by the characters of $\Gamma$ which are generated by the elements of $S$. The limit behaviour of the sequence of metric projections of the function $1$ onto $[{\mathcal T}(S)]_{\mu_n,p}$ as well as of the sequence of the corresponding approximation errors are studied. The results are applied to obtain stability theorems for prediction of weakly stationary or harmonizable symmetric $p$-stable stochastic processes. Along with the general problem the particular cases of linear interpolation or extrapolation as well as of a finite or periodic observation set are studied in detail and compared to each other. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2105.06523</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2105.06523</id><created>2021-05-13</created><updated>2023-10-02</updated><authors><author><keyname>Zhan</keyname><forenames>Tianyu</forenames></author><author><keyname>Fu</keyname><forenames>Haoda</forenames></author><author><keyname>Kang</keyname><forenames>Jian</forenames></author></authors><title>Deep Neural Networks Guided Ensemble Learning for Point Estimation</title><categories>stat.ME</categories><doi>10.1080/19466315.2023.2260776</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In modern statistics, interests shift from pursuing the uniformly minimum variance unbiased estimator to reducing mean squared error (MSE) or residual squared error. Shrinkage based estimation and regression methods offer better prediction accuracy and improved interpretation. However, the characterization of such optimal statistics in terms of minimizing MSE remains open and challenging in many problems, for example estimating treatment effect in adaptive clinical trials with pre-planned modifications to design aspects based on accumulated data. From an alternative perspective, we propose a deep neural network based automatic method to construct an improved estimator from existing ones. Theoretical properties are studied to provide guidance on applicability of our estimator to seek potential improvement. Simulation studies demonstrate that the proposed method has considerable finite-sample efficiency gain as compared with several common estimators. In the Adaptive COVID-19 Treatment Trial (ACTT) as an important application, our ensemble estimator essentially contributes to a more ethical and efficient adaptive clinical trial with fewer patients enrolled. The proposed framework can be generally applied to various statistical problems, and can be served as a reference measure to guide statistical research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2110.15824</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2110.15824</id><created>2021-10-27</created><updated>2023-07-03</updated><authors><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author><author><keyname>Zhong</keyname><forenames>Yiqiao</forenames></author><author><keyname>Zhou</keyname><forenames>Kangjie</forenames></author></authors><title>Tractability from overparametrization: The example of the negative   perceptron</title><categories>cs.LG math.PR math.ST stat.TH</categories><comments>107 pages; 7 pdf figures</comments><doi>10.1007/s00440-023-01248-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the negative perceptron problem we are given $n$ data points $({\boldsymbol x}_i,y_i)$, where ${\boldsymbol x}_i$ is a $d$-dimensional vector and $y_i\in\{+1,-1\}$ is a binary label. The data are not linearly separable and hence we content ourselves to find a linear classifier with the largest possible \emph{negative} margin. In other words, we want to find a unit norm vector ${\boldsymbol \theta}$ that maximizes $\min_{i\le n}y_i\langle {\boldsymbol \theta},{\boldsymbol x}_i\rangle$. This is a non-convex optimization problem (it is equivalent to finding a maximum norm vector in a polytope), and we study its typical properties under two random models for the data.   We consider the proportional asymptotics in which $n,d\to \infty$ with $n/d\to\delta$, and prove upper and lower bounds on the maximum margin $\kappa_{\text{s}}(\delta)$ or -- equivalently -- on its inverse function $\delta_{\text{s}}(\kappa)$. In other words, $\delta_{\text{s}}(\kappa)$ is the overparametrization threshold: for $n/d\le \delta_{\text{s}}(\kappa)-\varepsilon$ a classifier achieving vanishing training error exists with high probability, while for $n/d\ge \delta_{\text{s}}(\kappa)+\varepsilon$ it does not. Our bounds on $\delta_{\text{s}}(\kappa)$ match to the leading order as $\kappa\to -\infty$. We then analyze a linear programming algorithm to find a solution, and characterize the corresponding threshold $\delta_{\text{lin}}(\kappa)$. We observe a gap between the interpolation threshold $\delta_{\text{s}}(\kappa)$ and the linear programming threshold $\delta_{\text{lin}}(\kappa)$, raising the question of the behavior of other algorithms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2111.12921</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2111.12921</id><created>2021-11-25</created><updated>2025-02-25</updated><authors><author><keyname>Cai</keyname><forenames>Junhui</forenames></author><author><keyname>Yang</keyname><forenames>Dan</forenames></author><author><keyname>Chen</keyname><forenames>Ran</forenames></author><author><keyname>Zhu</keyname><forenames>Wu</forenames></author><author><keyname>Shen</keyname><forenames>Haipeng</forenames></author><author><keyname>Zhao</keyname><forenames>Linda</forenames></author></authors><title>Network regression and supervised centrality estimation</title><categories>econ.EM cs.SI stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The centrality in a network is often used to measure nodes' importance and model network effects on a certain outcome. Empirical studies widely adopt a two-stage procedure, which first estimates the centrality from the observed noisy network and then infers the network effect from the estimated centrality, even though it lacks theoretical understanding. We propose a unified modeling framework to study the properties of centrality estimation and inference and the subsequent network regression analysis with noisy network observations. Furthermore, we propose a supervised centrality estimation methodology, which aims to simultaneously estimate both centrality and network effect. We showcase the advantages of our method compared with the two-stage method both theoretically and numerically via extensive simulations and a case study in predicting currency risk premiums from the global trade network. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2201.02532</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2201.02532</id><created>2022-01-07</created><updated>2025-02-25</updated><authors><author><keyname>Otto</keyname><forenames>Sven</forenames></author><author><keyname>Salish</keyname><forenames>Nazarii</forenames></author></authors><title>Approximate Factor Models for Functional Time Series</title><categories>econ.EM stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a novel approximate factor model tailored for analyzing time-dependent curve data. Our model decomposes such data into two distinct components: a low-dimensional predictable factor component and an unpredictable error term. These components are identified through the autocovariance structure of the underlying functional time series. The model parameters are consistently estimated using the eigencomponents of a cumulative autocovariance operator and an information criterion is proposed to determine the appropriate number of factors. Applications to mortality and yield curve modeling illustrate key advantages of our approach over the widely used functional principal component analysis, as it offers parsimonious structural representations of the underlying dynamics along with gains in out-of-sample forecast performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2201.11482</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2201.11482</id><created>2022-01-27</created><updated>2025-02-25</updated><authors><author><keyname>Keilbar</keyname><forenames>Georg</forenames></author><author><keyname>Rodriguez-Poo</keyname><forenames>Juan M.</forenames></author><author><keyname>Soberon</keyname><forenames>Alexandra</forenames></author><author><keyname>Wang</keyname><forenames>Weining</forenames></author></authors><title>A projection based approach for interactive fixed effects panel data   models</title><categories>econ.EM stat.ME</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper introduces a straightforward sieve-based approach for estimating and conducting inference on regression parameters in panel data models with interactive fixed effects. The method's key assumption is that factor loadings can be decomposed into an unknown smooth function of individual characteristics plus an idiosyncratic error term. Our estimator offers advantages over existing approaches by taking a simple partial least squares form, eliminating the need for iterative procedures or preliminary factor estimation. In deriving the asymptotic properties, we discover that the limiting distribution exhibits a discontinuity that depends on how well our basis functions explain the factor loadings, as measured by the variance of the error factor loadings. This finding reveals that conventional ``plug-in'' methods using the estimated asymptotic covariance can produce excessively conservative coverage probabilities. We demonstrate that uniformly valid non-conservative inference can be achieved through the cross-sectional bootstrap method. Monte Carlo simulations confirm the estimator's strong performance in terms of mean squared error and good coverage results for the bootstrap procedure. We demonstrate the practical relevance of our methodology by analyzing growth rate determinants across OECD countries. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2202.01085</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2202.01085</id><created>2022-02-02</created><updated>2025-02-23</updated><authors><author><keyname>Hu</keyname><forenames>Robert</forenames></author><author><keyname>Chau</keyname><forenames>Siu Lun</forenames></author><author><keyname>Sejdinovic</keyname><forenames>Dino</forenames></author><author><keyname>Glaun√®s</keyname><forenames>Joan Alexis</forenames></author></authors><title>Giga-scale Kernel Matrix Vector Multiplication on GPU</title><categories>math.NA cs.LG cs.MS cs.NA stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Kernel matrix-vector multiplication (KMVM) is a foundational operation in machine learning and scientific computing. However, as KMVM tends to scale quadratically in both memory and time, applications are often limited by these computational constraints. In this paper, we propose a novel approximation procedure coined \textit{Faster-Fast and Free Memory Method} ($\fthreem$) to address these scaling issues of KMVM for tall~($10^8\sim 10^9$) and skinny~($D\leq7$) data. Extensive experiments demonstrate that $\fthreem$ has empirical \emph{linear time and memory} complexity with a relative error of order $10^{-3}$ and can compute a full KMVM for a billion points \emph{in under a minute} on a high-end GPU, leading to a significant speed-up in comparison to existing CPU methods. We demonstrate the utility of our procedure by applying it as a drop-in for the state-of-the-art GPU-based linear solver FALKON, \emph{improving speed 1.5-5.5 times} at the cost of $&lt;1\%$ drop in accuracy. We further demonstrate competitive results on \emph{Gaussian Process regression} coupled with significant speedups on a variety of real-world datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2202.03638</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2202.03638</id><created>2022-02-07</created><updated>2025-02-24</updated><authors><author><keyname>Linton</keyname><forenames>Oliver B.</forenames></author><author><keyname>Tang</keyname><forenames>Haihan</forenames></author><author><keyname>Wu</keyname><forenames>Jianbin</forenames></author></authors><title>A Large Confirmatory Dynamic Factor Model for Stock Market Returns in   Different Time Zones</title><categories>math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We propose a confirmatory dynamic factor model for a large number of stocks whose returns are observed daily across multiple time zones. The model has a global factor and a continental factor that both drive the individual stock return series. We propose two estimators of the model: a quasi-maximum likelihood estimator (QML-just-identified), and an improved estimator based on an Expectation Maximization (EM) algorithm (QML-all-res). Our estimators are consistent and asymptotically normal under the large approximate factor model setting. In particular, the asymptotic distributions of QML-all-res are the same as those of the infeasible OLS estimators that treat factors as known and utilize all the restrictions on the parameters of the model. We apply the model to MSCI equity indices of 42 developed and emerging markets, and find that most markets are more integrated when the CBOE Volatility Index (VIX) is high. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2207.02654</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2207.02654</id><created>2022-07-06</created><updated>2025-02-24</updated><authors><author><keyname>Blier-Wong</keyname><forenames>Christopher</forenames></author><author><keyname>Cossette</keyname><forenames>H√©l√®ne</forenames></author><author><keyname>Marceau</keyname><forenames>Etienne</forenames></author></authors><title>Efficient evaluation of risk allocations</title><categories>stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Expectations of marginals conditional on the total risk of a portfolio are crucial in risk-sharing and allocation. However, computing these conditional expectations may be challenging, especially in critical cases where the marginal risks have compound distributions or when the risks are dependent. We introduce a generating function method to compute these conditional expectations. We provide efficient algorithms to compute the conditional expectations of marginals given the total risk for a portfolio of risks with lattice-type support. We show that the ordinary generating function of unconditional expected allocations is a function of the multivariate probability generating function of the portfolio. The generating function method allows us to develop recursive and transform-based techniques to compute the unconditional expected allocations. We illustrate our method to large-scale risk-sharing and risk allocation problems, including cases where the marginal risks have compound distributions, where the portfolio is composed of dependent risks, and where the risks have heavy tails, leading in some cases to computational gains of several orders of magnitude. Our approach is useful for risk-sharing in peer-to-peer insurance and risk allocation based on Euler's rule. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2208.07900</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2208.07900</id><created>2022-08-16</created><updated>2025-02-21</updated><authors><author><keyname>Godoy</keyname><forenames>Lucas da Cunha</forenames></author><author><keyname>Prates</keyname><forenames>Marcos Oliveira</forenames></author><author><keyname>Yan</keyname><forenames>Jun</forenames></author></authors><title>Statistical Inferences and Predictions for Areal Data and Spatial Data   Fusion with Hausdorff--Gaussian Processes</title><categories>stat.ME</categories><msc-class>62</msc-class><acm-class>G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate modeling of spatial dependence is pivotal in analyzing spatial data, influencing parameter estimation and predictions. The spatial structure of the data significantly impacts valid statistical inference. Existing models for areal data often rely on adjacency matrices, struggling to differentiate between polygons of varying sizes and shapes. Conversely, data fusion models rely on computationally intensive numerical integrals, presenting challenges for moderately large datasets. In response to these issues, we propose the Hausdorff-Gaussian process (HGP), a versatile model utilizing the Hausdorff distance to capture spatial dependence in both point and areal data. Integration into generalized linear mixed-effects models enhances its applicability, particularly in addressing data fusion challenges. We validate our approach through a comprehensive simulation study and application to two real-world scenarios: one involving areal data and another demonstrating its effectiveness in data fusion. The results suggest that the HGP is competitive with specialized models regarding goodness-of-fit and prediction performances. In summary, the HGP offers a flexible and robust solution for modeling spatial data of various types and shapes, with potential applications spanning fields such as public health and climate science. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2210.08393</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2210.08393</id><created>2022-10-15</created><updated>2024-08-15</updated><authors><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Jing</keyname><forenames>Wenbo</forenames></author><author><keyname>Liu</keyname><forenames>Weidong</forenames></author><author><keyname>Zhang</keyname><forenames>Yichen</forenames></author></authors><title>Distributed Estimation and Inference for Semi-parametric Binary Response   Models</title><categories>math.ST stat.ML stat.TH</categories><journal-ref>Annals of Statistics 2024, 52(3): 922-947</journal-ref><doi>10.1214/24-AOS2376</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of modern technology has enabled data collection of unprecedented size, which poses new challenges to many statistical estimation and inference problems. This paper studies the maximum score estimator of a semi-parametric binary choice model under a distributed computing environment without pre-specifying the noise distribution. An intuitive divide-and-conquer estimator is computationally expensive and restricted by a non-regular constraint on the number of machines, due to the highly non-smooth nature of the objective function. We propose (1) a one-shot divide-and-conquer estimator after smoothing the objective to relax the constraint, and (2) a multi-round estimator to completely remove the constraint via iterative smoothing. We specify an adaptive choice of kernel smoother with a sequentially shrinking bandwidth to achieve the superlinear improvement of the optimization error over the multiple iterations. The improved statistical accuracy per iteration is derived, and a quadratic convergence up to the optimal statistical error rate is established. We further provide two generalizations to handle the heterogeneity of datasets and high-dimensional problems where the parameter of interest is sparse. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2211.16298</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2211.16298</id><created>2022-11-29</created><updated>2025-02-25</updated><authors><author><keyname>Breunig</keyname><forenames>Christoph</forenames></author><author><keyname>Liu</keyname><forenames>Ruixuan</forenames></author><author><keyname>Yu</keyname><forenames>Zhengfei</forenames></author></authors><title>Double Robust Bayesian Inference on Average Treatment Effects</title><categories>econ.EM stat.ME stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a double robust Bayesian inference procedure on the average treatment effect (ATE) under unconfoundedness. For our new Bayesian approach, we first adjust the prior distributions of the conditional mean functions, and then correct the posterior distribution of the resulting ATE. Both adjustments make use of pilot estimators motivated by the semiparametric influence function for ATE estimation. We prove asymptotic equivalence of our Bayesian procedure and efficient frequentist ATE estimators by establishing a new semiparametric Bernstein-von Mises theorem under double robustness; i.e., the lack of smoothness of conditional mean functions can be compensated by high regularity of the propensity score and vice versa. Consequently, the resulting Bayesian credible sets form confidence intervals with asymptotically exact coverage probability. In simulations, our method provides precise point estimates of the ATE through the posterior mean and credible intervals that closely align with the nominal coverage probability. Furthermore, our approach achieves a shorter interval length in comparison to existing methods. We illustrate our method in an application to the National Supported Work Demonstration following LaLonde [1986] and Dehejia and Wahba [1999]. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2212.03362</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2212.03362</id><created>2022-12-06</created><updated>2025-02-21</updated><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Sly</keyname><forenames>Allan</forenames></author><author><keyname>Sohn</keyname><forenames>Youngtak</forenames></author></authors><title>Exact Phase Transitions for Stochastic Block Models and Reconstruction   on Trees</title><categories>math.PR cs.SI math.CO math.ST stat.TH</categories><comments>54 pages. To appear in Annals of Probability</comments><msc-class>05C80, 60J85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we continue to rigorously establish the predictions in ground breaking work in statistical physics by Decelle, Krzakala, Moore, Zdeborov\'a (2011) regarding the block model, in particular in the case of $q=3$ and $q=4$ communities.   We prove that for $q=3$ and $q=4$ there is no computational-statistical gap if the average degree is above some constant by showing it is information theoretically impossible to detect below the Kesten-Stigum bound. The proof is based on showing that for the broadcast process on Galton-Watson trees, reconstruction is impossible for $q=3$ and $q=4$ if the average degree is sufficiently large. This improves on the result of Sly (2009), who proved similar results for regular trees for $q=3$. Our analysis of the critical case $q=4$ provides a detailed picture showing that the tightness of the Kesten-Stigum bound in the antiferromagnetic case depends on the average degree of the tree. We also prove that for $q\geq 5$, the Kestin-Stigum bound is not sharp.   Our results prove conjectures of Decelle, Krzakala, Moore, Zdeborov\'a (2011), Moore (2017), Abbe and Sandon (2018) and Ricci-Tersenghi, Semerjian, and Zdeborov\'a (2019). Our proofs are based on a new general coupling of the tree and graph processes and on a refined analysis of the broadcast process on the tree. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2301.05341</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2301.05341</id><created>2023-01-12</created><updated>2023-12-28</updated><authors><author><keyname>Marie</keyname><forenames>Nicolas</forenames></author></authors><title>On a Computable Skorokhod's Integral Based Estimator of the Drift   Parameter in Fractional SDE</title><categories>math.ST math.PR stat.TH</categories><comments>34 pages, 6 figures</comments><journal-ref>Scandinavian Journal of Statistics 52, 1, 1-37, 2025</journal-ref><doi>10.1111/sjos.12711</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with a Skorokhod's integral based least squares type estimator $\widehat\theta_N$ of the drift parameter $\theta_0$ computed from $N\in\mathbb N^*$ (possibly dependent) copies $X^1,\dots,X^N$ of the solution $X$ of $dX_t =\theta_0b(X_t)dt +\sigma dB_t$, where $B$ is a fractional Brownian motion of Hurst index $H\in (1/3,1)$. On the one hand, some convergence results are established on $\widehat\theta_N$ when $H = 1/2$. On the other hand, when $H\neq 1/2$, Skorokhod's integral based estimators as $\widehat\theta_N$ cannot be computed from data, but in this paper some convergence results are established on a computable approximation of $\widehat\theta_N$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2301.09138</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2301.09138</id><created>2023-01-22</created><updated>2025-02-24</updated><authors><author><keyname>Heese</keyname><forenames>Raoul</forenames></author><author><keyname>Gerlach</keyname><forenames>Thore</forenames></author><author><keyname>M√ºcke</keyname><forenames>Sascha</forenames></author><author><keyname>M√ºller</keyname><forenames>Sabine</forenames></author><author><keyname>Jakobs</keyname><forenames>Matthias</forenames></author><author><keyname>Piatkowski</keyname><forenames>Nico</forenames></author></authors><title>Explaining Quantum Circuits with Shapley Values: Towards Explainable   Quantum Machine Learning</title><categories>quant-ph cs.LG stat.ML</categories><comments>41 pages, 27 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Methods of artificial intelligence (AI) and especially machine learning (ML) have been growing ever more complex, and at the same time have more and more impact on people's lives. This leads to explainable AI (XAI) manifesting itself as an important research field that helps humans to better comprehend ML systems. In parallel, quantum machine learning (QML) is emerging with the ongoing improvement of quantum computing hardware combined with its increasing availability via cloud services. QML enables quantum-enhanced ML in which quantum mechanics is exploited to facilitate ML tasks, typically in the form of quantum-classical hybrid algorithms that combine quantum and classical resources. Quantum gates constitute the building blocks of gate-based quantum hardware and form circuits that can be used for quantum computations. For QML applications, quantum circuits are typically parameterized and their parameters are optimized classically such that a suitably defined objective function is minimized. Inspired by XAI, we raise the question of the explainability of such circuits by quantifying the importance of (groups of) gates for specific goals. To this end, we apply the well-established concept of Shapley values. The resulting attributions can be interpreted as explanations for why a specific circuit works well for a given task, improving the understanding of how to construct parameterized (or variational) quantum circuits, and fostering their human interpretability in general. An experimental evaluation on simulators and two superconducting quantum hardware devices demonstrates the benefits of the proposed framework for classification, generative modeling, transpilation, and optimization. Furthermore, our results shed some light on the role of specific gates in popular QML approaches. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2303.00055</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2303.00055</id><created>2023-02-28</created><updated>2024-04-17</updated><authors><author><keyname>Berthier</keyname><forenames>Rapha√´l</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author><author><keyname>Zhou</keyname><forenames>Kangjie</forenames></author></authors><title>Learning time-scales in two-layers neural networks</title><categories>cs.LG math.OC stat.ML</categories><comments>64 pages, 15 figures</comments><msc-class>34E15, 37N40, 68T07</msc-class><doi>10.1007/s10208-024-09664-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gradient-based learning in multi-layer neural networks displays a number of striking features. In particular, the decrease rate of empirical risk is non-monotone even after averaging over large batches. Long plateaus in which one observes barely any progress alternate with intervals of rapid decrease. These successive phases of learning often take place on very different time scales. Finally, models learnt in an early phase are typically `simpler' or `easier to learn' although in a way that is difficult to formalize.   Although theoretical explanations of these phenomena have been put forward, each of them captures at best certain specific regimes. In this paper, we study the gradient flow dynamics of a wide two-layer neural network in high-dimension, when data are distributed according to a single-index model (i.e., the target function depends on a one-dimensional projection of the covariates). Based on a mixture of new rigorous results, non-rigorous mathematical derivations, and numerical simulations, we propose a scenario for the learning dynamics in this setting. In particular, the proposed evolution exhibits separation of timescales and intermittency. These behaviors arise naturally because the population gradient flow can be recast as a singularly perturbed dynamical system. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2304.09310</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2304.09310</id><created>2023-04-18</created><updated>2025-02-25</updated><authors><author><keyname>Mozafari-Majd</keyname><forenames>Emadaldin</forenames></author><author><keyname>Koivunen</keyname><forenames>Visa</forenames></author></authors><title>The Adaptive $\tau$-Lasso: Robustness and Oracle Properties</title><categories>stat.ML cs.LG eess.SP</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  This paper introduces a new regularized version of the robust $\tau$-regression estimator for analyzing high-dimensional datasets subject to gross contamination in the response variables and covariates. The resulting estimator, termed adaptive $\tau$-Lasso, is robust to outliers and high-leverage points. It also incorporates an adaptive $\ell_1$-norm penalty term, which enables the selection of relevant variables and reduces the bias associated with large true regression coefficients. More specifically, this adaptive $\ell_1$-norm penalty term assigns a weight to each regression coefficient. For a fixed number of predictors $p$, we show that the adaptive $\tau$-Lasso has the oracle property, ensuring both variable-selection consistency and asymptotic normality. Asymptotic normality applies only to the entries of the regression vector corresponding to the true support, assuming knowledge of the true regression vector support. We characterize its robustness by establishing the finite-sample breakdown point and the influence function. We carry out extensive simulations and observe that the class of $\tau$-Lasso estimators exhibits robustness and reliable performance in both contaminated and uncontaminated data settings. We also validate our theoretical findings on robustness properties through simulations. In the face of outliers and high-leverage points, the adaptive $\tau$-Lasso and $\tau$-Lasso estimators achieve the best performance or match the best performances of competing regularized estimators, with minimal or no loss in terms of prediction and variable selection accuracy for almost all scenarios considered in this study. Therefore, the adaptive $\tau$-Lasso and $\tau$-Lasso estimators provide attractive tools for a variety of sparse linear regression problems, particularly in high-dimensional settings and when the data is contaminated by outliers and high-leverage points. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2305.06127</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2305.06127</id><created>2023-05-10</created><updated>2025-02-21</updated><authors><author><keyname>Semnani</keyname><forenames>Pardis</forenames></author><author><keyname>Robeva</keyname><forenames>Elina</forenames></author></authors><title>Causal Structure Learning in Directed, Possibly Cyclic, Graphical Models</title><categories>math.ST math.CO stat.TH</categories><comments>final version; some remarks and examples added; link to the GitHub   repository added; 42 pages; 19 figures</comments><msc-class>62D20 (Primary), 62H22, 05C20 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We consider the problem of learning a directed graph $G^\star$ from observational data. We assume that the distribution which gives rise to the samples is Markov and faithful to the graph $G^\star$ and that there are no unobserved variables. We do not rely on any further assumptions regarding the graph or the distribution of the variables. Particularly, we allow for directed cycles in $G^\star$ and work in the fully non-parametric setting. Given the set of conditional independence statements satisfied by the distribution, we aim to find a directed graph which satisfies the same $d$-separation statements as $G^\star$. We propose a hybrid approach consisting of two steps. We first find a partially ordered partition of the vertices of $G^\star$ by optimizing a certain score in a greedy fashion. We prove that any optimal partition uniquely characterizes the Markov equivalence class of $G^\star$. Given an optimal partition, we propose an algorithm for constructing a graph in the Markov equivalence class of $G^\star$ whose strongly connected components correspond to the elements of the partition, and which are partially ordered according to the partial order of the partition. Our algorithm comes in two versions -- one which is provably correct and another one which performs fast in practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2305.16735</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2305.16735</id><created>2023-05-26</created><updated>2025-02-23</updated><authors><author><keyname>Taylor</keyname><forenames>James W.</forenames></author><author><keyname>Meng</keyname><forenames>Xiaochun</forenames></author></authors><title>Angular Combining of Forecasts of Probability Distributions</title><categories>stat.ME stat.AP</categories><msc-class>90B50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When multiple forecasts are available for a probability distribution, forecast combining enables a pragmatic synthesis of the information to extract the wisdom of the crowd. The linear opinion pool has been widely used, whereby the combining is applied to the probabilities of the distributional forecasts. However, it has been argued that this will tend to deliver overdispersed distributions, prompting the combination to be applied, instead, to the quantiles of the distributional forecasts. Results from different applications are mixed, leaving it as an empirical question whether to combine probabilities or quantiles. In this paper, we present an alternative approach. Looking at the distributional forecasts, combining the probabilities can be viewed as vertical combining, with quantile combining seen as horizontal combining. Our proposal is to allow combining to take place on an angle between the extreme cases of vertical and horizontal combining. We term this angular combining. The angle is a parameter that can be optimized using a proper scoring rule. For implementation, we provide a pragmatic numerical approach and a simulation algorithm. Among our theoretical results, we show that, as with vertical and horizontal averaging, angular averaging results in a distribution with mean equal to the average of the means of the distributions that are being combined. We also show that angular averaging produces a distribution with lower variance than vertical averaging, and, under certain assumptions, greater variance than horizontal averaging. We provide empirical results for distributional forecasts of Covid mortality, macroeconomic survey data, and electricity prices. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2306.00453</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2306.00453</id><created>2023-06-01</created><updated>2025-01-20</updated><authors><author><keyname>Schrunner</keyname><forenames>Stefan</forenames></author><author><keyname>Pishrobat</keyname><forenames>Parham</forenames></author><author><keyname>Janssen</keyname><forenames>Joseph</forenames></author><author><keyname>Jenul</keyname><forenames>Anna</forenames></author><author><keyname>Cao</keyname><forenames>Jiguo</forenames></author><author><keyname>Ameli</keyname><forenames>Ali A.</forenames></author><author><keyname>Welch</keyname><forenames>William J.</forenames></author></authors><title>A Gaussian Sliding Windows Regression Model for Hydrological Inference</title><categories>stat.ME</categories><doi>10.1093/jrsssc/qlaf009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical models are an essential tool to model, forecast and understand the hydrological processes in watersheds. In particular, the understanding of time lags associated with the delay between rainfall occurrence and subsequent changes in streamflow is of high practical importance. Since water can take a variety of flow paths to generate streamflow, a series of distinct runoff pulses may combine to create the observed streamflow time series. Current state-of-the-art models are not able to sufficiently confront the problem complexity with interpretable parametrization, thus preventing novel insights about the dynamics of distinct flow paths from being formed. The proposed Gaussian Sliding Windows Regression Model targets this problem by combining the concept of multiple windows sliding along the time axis with multiple linear regression. The window kernels, which indicate the weights applied to different time lags, are implemented via Gaussian-shaped kernels. As a result, straightforward process inference can be achieved since each window can represent one flow path. Experiments on simulated and real-world scenarios underline that the proposed model achieves accurate parameter estimates and competitive predictive performance, while fostering explainable and interpretable hydrological modeling. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2306.02071</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2306.02071</id><created>2023-06-03</created><updated>2024-11-04</updated><authors><author><keyname>Garrido-Lucero</keyname><forenames>Felipe</forenames></author><author><keyname>Heymann</keyname><forenames>Benjamin</forenames></author><author><keyname>Vono</keyname><forenames>Maxime</forenames></author><author><keyname>Loiseau</keyname><forenames>Patrick</forenames></author><author><keyname>Perchet</keyname><forenames>Vianney</forenames></author></authors><title>DU-Shapley: A Shapley Value Proxy for Efficient Dataset Valuation</title><categories>cs.AI cs.GT stat.CO stat.ML</categories><comments>22 pages</comments><journal-ref>https://proceedings.neurips.cc/paper_files/paper/2024/hash/03cd3cf3f74d4f9ce5958de269960884-Abstract-Conference.html</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the dataset valuation problem, that is, the problem of quantifying the incremental gain, to some relevant pre-defined utility of a machine learning task, of aggregating an individual dataset to others. The Shapley value is a natural tool to perform dataset valuation due to its formal axiomatic justification, which can be combined with Monte Carlo integration to overcome the computational tractability challenges. Such generic approximation methods, however, remain expensive in some cases. In this paper, we exploit the knowledge about the structure of the dataset valuation problem to devise more efficient Shapley value estimators. We propose a novel approximation, referred to as discrete uniform Shapley, which is expressed as an expectation under a discrete uniform distribution with support of reasonable size. We justify the relevancy of the proposed framework via asymptotic and non-asymptotic theoretical guarantees and illustrate its benefits via an extensive set of numerical experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2306.07566</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2306.07566</id><created>2023-06-13</created><updated>2025-02-24</updated><authors><author><keyname>Chen</keyname><forenames>Jian</forenames></author><author><keyname>Li</keyname><forenames>Zhehao</forenames></author><author><keyname>Mao</keyname><forenames>Xiaojie</forenames></author></authors><title>Learning with Selectively Labeled Data from Multiple Decision-makers</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study the problem of classification with selectively labeled data, whose distribution may differ from the full population due to historical decision-making. We exploit the fact that in many applications historical decisions were made by multiple decision-makers, each with different decision rules. We analyze this setup under a principled instrumental variable (IV) framework and rigorously study the identification of classification risk. We establish conditions for the exact identification of classification risk and derive tight partial identification bounds when exact identification fails. We further propose a unified cost-sensitive learning (UCL) approach to learn classifiers robust to selection bias in both identification settings. We further theoretically and numerically validate the efficacy of our proposed method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2307.04949</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2307.04949</id><created>2023-07-10</created><updated>2024-01-15</updated><authors><author><keyname>Marie</keyname><forenames>Nicolas</forenames></author></authors><title>On a Calculable Skorokhod's Integral Based Projection Estimator of the   Drift Function in Fractional SDE</title><categories>math.ST math.PR stat.TH</categories><comments>15 pages</comments><journal-ref>Statistical Inference for Stochastic Processes 27, 391-405, 2024</journal-ref><doi>10.1007/s11203-024-09306-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with a Skorokhod's integral based projection type estimator $\widehat b_m$ of the drift function $b_0$ computed from $N\in\mathbb N^*$ independent copies $X^1,\dots,X^N$ of the solution $X$ of $dX_t = b_0(X_t)dt +\sigma dB_t$, where $B$ is a fractional Brownian motion of Hurst index $H\in (1/2,1)$. Skorokhod's integral based estimators cannot be calculated directly from $X^1,\dots,X^N$, but in this paper an $\mathbb L^2$-error bound is established on a calculable approximation of $\widehat b_m$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2307.15004</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2307.15004</id><created>2023-07-27</created><updated>2025-02-25</updated><authors><author><keyname>Wan</keyname><forenames>Phyllis</forenames></author><author><keyname>Zhou</keyname><forenames>Chen</forenames></author></authors><title>Graphical lasso for extremes</title><categories>stat.ME math.ST stat.TH</categories><msc-class>62G32, 62H12, 62F12</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we estimate the sparse dependence structure in the tail region of a multivariate random vector, potentially of high dimension. The tail dependence is modeled via a graphical model for extremes embedded in the H\"usler-Reiss distribution. We propose the extreme graphical lasso procedure to estimate the sparsity in the tail dependence, similar to the Gaussian graphical lasso in high dimensional statistics. We prove its consistency in identifying the graph structure and estimating model parameters. The efficiency and accuracy of the proposed method are illustrated by simulations and real data examples. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2308.15642</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2308.15642</id><created>2023-08-29</created><updated>2024-06-18</updated><authors><author><keyname>Zurek</keyname><forenames>Matthew</forenames></author><author><keyname>Chen</keyname><forenames>Yudong</forenames></author></authors><title>Gap-Free Clustering: Sensitivity and Robustness of SDP</title><categories>cs.LG cs.DS cs.IT math.IT math.OC stat.ML</categories><journal-ref>The 37th Annual Conference on Learning Theory (COLT 2024)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study graph clustering in the Stochastic Block Model (SBM) in the presence of both large clusters and small, unrecoverable clusters. Previous convex relaxation approaches achieving exact recovery do not allow any small clusters of size $o(\sqrt{n})$, or require a size gap between the smallest recovered cluster and the largest non-recovered cluster. We provide an algorithm based on semidefinite programming (SDP) which removes these requirements and provably recovers large clusters regardless of the remaining cluster sizes. Mid-sized clusters pose unique challenges to the analysis, since their proximity to the recovery threshold makes them highly sensitive to small noise perturbations and precludes a closed-form candidate solution. We develop novel techniques, including a leave-one-out-style argument which controls the correlation between SDP solutions and noise vectors even when the removal of one row of noise can drastically change the SDP solution. We also develop improved eigenvalue perturbation bounds of potential independent interest. Our results are robust to certain semirandom settings that are challenging for alternative algorithms. Using our gap-free clustering procedure, we obtain efficient algorithms for the problem of clustering with a faulty oracle with superior query complexities, notably achieving $o(n^2)$ sample complexity even in the presence of a large number of small clusters. Our gap-free clustering procedure also leads to improved algorithms for recursive clustering. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2308.16245</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2308.16245</id><created>2023-08-30</created><updated>2024-05-25</updated><authors><author><keyname>L√∂fstr√∂m</keyname><forenames>Tuwe</forenames></author><author><keyname>L√∂fstr√∂m</keyname><forenames>Helena</forenames></author><author><keyname>Johansson</keyname><forenames>Ulf</forenames></author><author><keyname>S√∂nstr√∂d</keyname><forenames>Cecilia</forenames></author><author><keyname>Matela</keyname><forenames>Rudy</forenames></author></authors><title>Calibrated Explanations for Regression</title><categories>cs.LG cs.AI stat.ML</categories><comments>34 pages, 13 figures (resubmitted draft after minor revision)</comments><msc-class>68-04, 68W99</msc-class><journal-ref>Mach Learn 114, 100 (2025)</journal-ref><doi>10.1007/s10994-024-06642-8</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Artificial Intelligence (AI) is often an integral part of modern decision support systems. The best-performing predictive models used in AI-based decision support systems lack transparency. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance. However, a critical drawback of existing local explanation methods is their inability to quantify the uncertainty associated with a feature's importance. This paper introduces an extension of a feature importance explanation method, Calibrated Explanations, previously only supporting classification, with support for standard regression and probabilistic regression, i.e., the probability that the target is above an arbitrary threshold. The extension for regression keeps all the benefits of Calibrated Explanations, such as calibration of the prediction from the underlying model with confidence intervals, uncertainty quantification of feature importance, and allows both factual and counterfactual explanations. Calibrated Explanations for standard regression provides fast, reliable, stable, and robust explanations. Calibrated Explanations for probabilistic regression provides an entirely new way of creating probabilistic explanations from any ordinary regression model, allowing dynamic selection of thresholds. The method is model agnostic with easily understood conditional rules. An implementation in Python is freely available on GitHub and for installation using both pip and conda, making the results in this paper easily replicable. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.06746</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.06746</id><created>2023-10-10</created><updated>2025-02-23</updated><authors><author><keyname>Wu</keyname><forenames>Ying</forenames></author><author><keyname>Liu</keyname><forenames>Hanzhong</forenames></author><author><keyname>Ren</keyname><forenames>Kai</forenames></author><author><keyname>Ma</keyname><forenames>Shujie</forenames></author><author><keyname>Chang</keyname><forenames>Xiangyu</forenames></author></authors><title>A New Causal Rule Learning Approach to Interpretable Estimation of   Heterogeneous Treatment Effect</title><categories>cs.LG stat.ME stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interpretability plays a critical role in the application of statistical learning for estimating heterogeneous treatment effects (HTE) for complex diseases. In this study, we leverage a rule-based workflow, namely causal rule learning (CRL) to estimate and enhance our understanding of HTE for atrial septal defect, addressing an overlooked question in previous literature: what if an individual simultaneously belongs to multiple groups with different average treatment effects? The CRL process consists of three steps: rule discovery, which generates a set of causal rules with corresponding subgroup average treatment effects; rule selection, which identifies a subset of these rules to deconstruct individual-level treatment effects as a linear combination of subgroup-level effects; and rule analysis, which outlines a detailed procedure for further analyzing each selected rule from multiple perspectives to identify the most promising rules for validation. Extensive simulation studies and real-world data analysis demonstrate that CRL outperforms other methods in providing interpretable estimates of HTE, especially when dealing with complex ground truth and sufficient sample sizes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2310.16989</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2310.16989</id><created>2023-10-25</created><updated>2024-09-19</updated><authors><author><keyname>Liang</keyname><forenames>Tengyuan</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author></authors><title>Randomization Inference When N Equals One</title><categories>stat.ME math.OC math.ST stat.TH</categories><comments>30 pages, 3 figures</comments><journal-ref>Biometrika 2025</journal-ref><doi>10.1093/biomet/asaf013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  N-of-1 experiments, where a unit serves as its own control and treatment in different time windows, have been used in certain medical contexts for decades. However, due to effects that accumulate over long time windows and interventions that have complex evolution, a lack of robust inference tools has limited the widespread applicability of such N-of-1 designs. This work combines techniques from experiment design in causal inference and system identification from control theory to provide such an inference framework. We derive a model of the dynamic interference effect that arises in linear time-invariant dynamical systems. We show that a family of causal estimands analogous to those studied in potential outcomes are estimable via a standard estimator derived from the method of moments. We derive formulae for higher moments of this estimator and describe conditions under which N-of-1 designs may provide faster ways to estimate the effects of interventions in dynamical systems. We also provide conditions under which our estimator is asymptotically normal and derive valid confidence intervals for this setting. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.07697</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.07697</id><created>2023-12-12</created><authors><author><keyname>Zhan</keyname><forenames>Tianyu</forenames></author></authors><title>A Class of Computational Methods to Reduce Selection Bias when Designing   Phase 3 Clinical Trials</title><categories>stat.ME</categories><doi>10.1002/sim.10041</doi><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  When designing confirmatory Phase 3 studies, one usually evaluates one or more efficacious and safe treatment option(s) based on data from previous studies. However, several retrospective research articles reported the phenomenon of ``diminished treatment effect in Phase 3'' based on many case studies. Even under basic assumptions, it was shown that the commonly used estimator could substantially overestimate the efficacy of selected group(s). As alternatives, we propose a class of computational methods to reduce estimation bias and mean squared error (MSE) with a broader scope of multiple treatment groups and flexibility to accommodate summary results by group as input. Based on simulation studies and a real data example, we provide practical implementation guidance for this class of methods under different scenarios. For more complicated problems, our framework can serve as a starting point with additional layers built in. Proposed methods can also be widely applied to other selection problems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2312.10431</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2312.10431</id><created>2023-12-16</created><updated>2025-02-24</updated><authors><author><keyname>Mueller</keyname><forenames>Markus</forenames></author><author><keyname>Gruber</keyname><forenames>Kathrin</forenames></author><author><keyname>Fok</keyname><forenames>Dennis</forenames></author></authors><title>Continuous Diffusion for Mixed-Type Tabular Data</title><categories>cs.LG stat.ML</categories><comments>published in ICLR 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Score-based generative models, commonly referred to as diffusion models, have proven to be successful at generating text and image data. However, their adaptation to mixed-type tabular data remains underexplored. In this work, we propose CDTD, a Continuous Diffusion model for mixed-type Tabular Data. CDTD is based on a novel combination of score matching and score interpolation to enforce a unified continuous noise distribution for both continuous and categorical features. We explicitly acknowledge the necessity of homogenizing distinct data types by relying on model-specific loss calibration and initialization schemes.To further address the high heterogeneity in mixed-type tabular data, we introduce adaptive feature- or type-specific noise schedules. These ensure balanced generative performance across features and optimize the allocation of model capacity across features and diffusion time. Our experimental results show that CDTD consistently outperforms state-of-the-art benchmark models, captures feature correlations exceptionally well, and that heterogeneity in the noise schedule design boosts sample quality. Replication code is available at https://github.com/muellermarkus/cdtd. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.03622</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.03622</id><created>2024-01-07</created><updated>2025-02-25</updated><authors><author><keyname>Wang</keyname><forenames>Rui</forenames></author><author><keyname>Jiang</keyname><forenames>Dandan</forenames></author></authors><title>Inference on testing the number of spikes in a high-dimensional   generalized spiked Fisher matrix</title><categories>math.ST stat.TH</categories><msc-class>62H15, 62H10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The spiked Fisher matrix is a significant topic for two-sample problems in multivariate statistical inference. This paper is dedicated to testing the number of spikes in a high-dimensional generalized spiked Fisher matrix that relaxes the Gaussian population assumption and the diagonal constraints on the population covariance matrices. First, we propose a general test statistic predicated on partial linear spectral statistics to test the number of spikes, then establish the central limit theorem (CLT) for this statistic under the null hypothesis. Second, we apply the CLT to address two statistical problems: variable selection in high-dimensional linear regression and change point detection. For each test problem, we construct new statistics and derive their asymptotic distributions under the null hypothesis. Finally, simulations and empirical analysis are conducted to demonstrate the remarkable effectiveness and generality of our proposed methods across various scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.04418</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.04418</id><created>2024-01-09</created><updated>2025-02-24</updated><authors><author><keyname>Saha</keyname><forenames>Shital</forenames></author><author><keyname>Kayal</keyname><forenames>Suchandan</forenames></author><author><keyname>Balakrishnan</keyname><forenames>N.</forenames></author></authors><title>Some Generalized Information and Divergence Generating Functions:   Properties, Estimation, Validation and Applications</title><categories>math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose R\'enyi information generating function and discuss its properties. A connection between the R\'enyi information generating function and the diversity index is proposed for discrete type random variables. The relation between the R\'enyi information generating function and Shannon entropy of order $q&gt;0$ is established and several bounds are obtained. The R\'enyi information generating function of escort distribution is derived. Furthermore, we introduce R\'enyi divergence information generating function and discuss its effect under monotone transformations. We present non-parametric and parametric estimators of the R\'enyi information generating function. A simulation study is carried out and a real data relating to the failure times of electronic components is analyzed. A comparison study between the non-parametric and parametric estimators is made in terms of the standard deviation, absolute bias, and mean square error. We have observed superior performance for the newly proposed estimators. Some applications of the proposed R\'enyi information generating function and R\'enyi divergence information generating function are provided. For three coherent systems, we calculate the values of the R\'enyi information generating function and other well-established uncertainty measures and similar behaviour of the R\'enyi information generating function is observed. Further, a study regarding the usefulness of the R\'enyi divergence information generating function and R\'enyi information generating function as model selection criteria is conducted. Finally, three chaotic maps are considered and then used to establish a validation of the proposed information generating function. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.11562</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.11562</id><created>2024-01-21</created><updated>2025-02-25</updated><authors><author><keyname>Worah</keyname><forenames>Pratik</forenames></author></authors><title>Enhancing selectivity using Wasserstein distance based reweighing</title><categories>stat.ML cs.LG q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two labeled data-sets $\mathcal{S}$ and $\mathcal{T}$, we design a simple and efficient greedy algorithm to reweigh the loss function such that the limiting distribution of the neural network weights that result from training on $\mathcal{S}$ approaches the limiting distribution that would have resulted by training on $\mathcal{T}$.   On the theoretical side, we prove that when the metric entropy of the input datasets is bounded, our greedy algorithm outputs a close to optimal reweighing, i.e., the two invariant distributions of network weights will be provably close in total variation distance. Moreover, the algorithm is simple and scalable, and we prove bounds on the efficiency of the algorithm as well.   As a motivating application, we train a neural net to recognize small molecule binders to MNK2 (a MAP Kinase, responsible for cell signaling) which are non-binders to MNK1 (a highly similar protein). In our example dataset, of the 43 distinct small molecules predicted to be most selective from the enamine catalog, 2 small molecules were experimentally verified to be selective, i.e., they reduced the enzyme activity of MNK2 below 50\% but not MNK1, at 10$\mu$M -- a 5\% success rate. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.12482</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.12482</id><created>2024-01-22</created><updated>2025-02-25</updated><authors><author><keyname>Yara</keyname><forenames>Atsutomo</forenames></author><author><keyname>Terada</keyname><forenames>Yoshikazu</forenames></author></authors><title>Nonparametric logistic regression with deep learning</title><categories>math.ST stat.ML stat.TH</categories><comments>28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the nonparametric logistic regression problem. In the logistic regression, we usually consider the maximum likelihood estimator, and the excess risk is the expectation of the Kullback-Leibler (KL) divergence between the true and estimated conditional class probabilities. However, in the nonparametric logistic regression, the KL divergence could diverge easily, and thus, the convergence of the excess risk is difficult to prove or does not hold. Several existing studies show the convergence of the KL divergence under strong assumptions. In most cases, our goal is to estimate the true conditional class probabilities. Thus, instead of analyzing the excess risk itself, it suffices to show the consistency of the maximum likelihood estimator in some suitable metric. In this paper, using a simple unified approach for analyzing the nonparametric maximum likelihood estimator (NPMLE), we directly derive convergence rates of the NPMLE in the Hellinger distance under mild assumptions. Although our results are similar to the results in some existing studies, we provide simple and more direct proofs for these results. As an important application, we derive convergence rates of the NPMLE with fully connected deep neural networks and show that the derived rate nearly achieves the minimax optimal rate. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2401.16990</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2401.16990</id><created>2024-01-30</created><updated>2025-02-25</updated><authors><author><keyname>de Aguas</keyname><forenames>Johan</forenames></author><author><keyname>Pensar</keyname><forenames>Johan</forenames></author><author><keyname>P√©rez</keyname><forenames>Tom√°s Varnet</forenames></author><author><keyname>Biele</keyname><forenames>Guido</forenames></author></authors><title>Recovery and inference of causal effects with sequential adjustment for   confounding and attrition</title><categories>stat.ME</categories><msc-class>62A09, 62D20</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Confounding bias and selection bias bring two significant challenges to the validity of conclusions drawn from applied causal inference. The latter can stem from informative missingness, such as in cases of attrition. We introduce the Sequential Adjustment Criteria (SAC), which extend available graphical conditions for recovering causal effects from confounding and attrition using sequential regressions, allowing for the inclusion of post-exposure and forbidden variables in the adjustment sets. We propose an estimator for the recovered Average Treatment Effect (ATE) based on Targeted Minimum-Loss Estimation (TMLE), which exhibits multiple robustness under certain technical conditions. This approach ensures consistency even in scenarios where the Double Inverse Probability Weighting (DIPW) and the naive plug-in sequential regressions approaches fall short. Through a simulation study, we assess the performance of the proposed estimator against alternative methods across different graph setups and model specification scenarios. As a motivating application, we examine the effect of pharmacological treatment for Attention-Deficit/Hyperactivity Disorder (ADHD) upon the scores obtained by diagnosed Norwegian schoolchildren in national tests using observational data ($n=9\,352$). Our findings align with the accumulated clinical evidence, affirming a positive but small impact of medication on academic achievement. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.00849</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.00849</id><created>2024-02-01</created><updated>2025-02-21</updated><authors><author><keyname>Varƒ±cƒ±</keyname><forenames>Burak</forenames></author><author><keyname>Acart√ºrk</keyname><forenames>Emre</forenames></author><author><keyname>Shanmugam</keyname><forenames>Karthikeyan</forenames></author><author><keyname>Kumar</keyname><forenames>Abhishek</forenames></author><author><keyname>Tajer</keyname><forenames>Ali</forenames></author></authors><title>Score-based Causal Representation Learning: Linear and General   Transformations</title><categories>cs.LG stat.ML</categories><comments>Main changes: additional identifiability results from single-node   interventions, simplified linear algorithm, and additional experiments.   General transform results also appear in our paper General Identifiability   and Achievability for Causal Representation Learning (arXiv:2310.15450)   appeared at AISTATS 2024 (oral)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables. Linear and general transformations are investigated. The paper addresses both the identifiability and achievability aspects. Identifiability refers to determining algorithm-agnostic conditions that ensure recovering the true latent causal variables and the latent causal graph underlying them. Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees. By drawing novel connections between score functions (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a score-based class of algorithms that ensures both identifiability and achievability. First, the paper focuses on linear transformations and shows that one stochastic hard intervention per node suffices to guarantee identifiability. It also provides partial identifiability guarantees for soft interventions, including identifiability up to mixing with parents for general causal models and perfect latent graph recovery for sufficiently non-linear causal models. Secondly, it focuses on general transformations and shows that two stochastic hard interventions per node suffice for identifiability. This is achieved by defining a differentiable loss function whose global optima ensures identifiability for general CRL. Notably, one does not need to know which pair of interventional environments have the same node intervened. Finally, the theoretical results are empirically validated via experiments on structured synthetic data and image data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.01543</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.01543</id><created>2024-02-02</created><updated>2025-02-24</updated><authors><author><keyname>Bertsimas</keyname><forenames>Dimitris</forenames></author><author><keyname>Delarue</keyname><forenames>Arthur</forenames></author><author><keyname>Pauphilet</keyname><forenames>Jean</forenames></author></authors><title>Adaptive Optimization for Prediction with Missing Data</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  When training predictive models on data with missing entries, the most widely used and versatile approach is a pipeline technique where we first impute missing entries and then compute predictions. In this paper, we view prediction with missing data as a two-stage adaptive optimization problem and propose a new class of models, adaptive linear regression models, where the regression coefficients adapt to the set of observed features. We show that some adaptive linear regression models are equivalent to learning an imputation rule and a downstream linear regression model simultaneously instead of sequentially. We leverage this joint-impute-then-regress interpretation to generalize our framework to non-linear models. In settings where data is strongly not missing at random, our methods achieve a 2-10% improvement in out-of-sample accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.01632</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.01632</id><created>2024-02-02</created><updated>2025-02-22</updated><authors><author><keyname>Ziomek</keyname><forenames>Juliusz</forenames></author><author><keyname>Adachi</keyname><forenames>Masaki</forenames></author><author><keyname>Osborne</keyname><forenames>Michael A.</forenames></author></authors><title>Time-Varying Gaussian Process Bandits with Unknown Prior</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bayesian optimisation requires fitting a Gaussian process model, which in turn requires specifying prior on the unknown black-box function -- most of the theoretical literature assumes this prior is known. However, it is common to have more than one possible prior for a given black-box function, for example suggested by domain experts with differing opinions. In some cases, the type-II maximum likelihood estimator for selecting prior enjoys the consistency guarantee, but it does not universally apply to all types of priors. If the problem is stationary, one could rely on the Regret Balancing scheme to conduct the optimisation, but in the case of time-varying problems, such a scheme cannot be used. To address this gap in existing research, we propose a novel algorithm, PE-GP-UCB, which is capable of solving time-varying Bayesian optimisation problems even without the exact knowledge of the function's prior. The algorithm relies on the fact that either the observed function values are consistent with some of the priors, in which case it is easy to reject the wrong priors, or the observations are consistent with all candidate priors, in which case it does not matter which prior our model relies on. We provide a regret bound on the proposed algorithm. Finally, we empirically evaluate our algorithm on toy and real-world time-varying problems and show that it outperforms the maximum likelihood estimator, fully Bayesian treatment of unknown prior and Regret Balancing. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.02306</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.02306</id><created>2024-02-03</created><updated>2025-02-22</updated><authors><author><keyname>Chen</keyname><forenames>Xinyuan</forenames></author><author><keyname>Hu</keyname><forenames>Liangyuan</forenames></author><author><keyname>Li</keyname><forenames>Fan</forenames></author></authors><title>A flexible Bayesian g-formula for causal survival analyses with   time-dependent confounding</title><categories>stat.ME stat.CO stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In longitudinal observational studies with time-to-event outcomes, a common objective in causal analysis is to estimate the causal survival curve under hypothetical intervention scenarios. The g-formula is a useful tool for this analysis. To enhance the traditional parametric g-formula, we developed an alternative g-formula estimator, which incorporates the Bayesian Additive Regression Trees (BART) into the modeling of the time-evolving generative components, aiming to mitigate the bias due to model misspecification. We focus on binary time-varying treatments and introduce a general class of g-formulas for discrete survival data that can incorporate the longitudinal balancing scores. The minimum sufficient formulation of these longitudinal balancing scores is linked to the nature of treatment strategies, i.e., static or dynamic. For each type of treatment strategy, we provide posterior sampling algorithms. We conducted simulations to illustrate the empirical performance of the proposed method and demonstrate its practical utility using data from the Yale New Haven Health System's electronic health records. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.03982</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.03982</id><created>2024-02-06</created><updated>2025-02-21</updated><authors><author><keyname>Hong</keyname><forenames>Yusu</forenames></author><author><keyname>Lin</keyname><forenames>Junhong</forenames></author></authors><title>On Convergence of Adam for Stochastic Optimization under Relaxed   Assumptions</title><categories>math.OC cs.LG stat.ML</categories><comments>NeurIPS 2024</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Adaptive Momentum Estimation (Adam) algorithm is highly effective in training various deep learning tasks. Despite this, there's limited theoretical understanding for Adam, especially when focusing on its vanilla form in non-convex smooth scenarios with potential unbounded gradients and affine variance noise. In this paper, we study vanilla Adam under these challenging conditions. We introduce a comprehensive noise model which governs affine variance noise, bounded noise and sub-Gaussian noise. We show that Adam can find a stationary point with a $\mathcal{O}(\text{poly}(\log T)/\sqrt{T})$ rate in high probability under this general noise model where $T$ denotes total number iterations, matching the lower rate of stochastic first-order algorithms up to logarithm factors. More importantly, we reveal that Adam is free of tuning step-sizes with any problem-parameters, yielding a better adaptation property than the Stochastic Gradient Descent under the same conditions. We also provide a probabilistic convergence result for Adam under a generalized smooth condition which allows unbounded smoothness parameters and has been illustrated empirically to more accurately capture the smooth property of many practical objective functions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2402.06915</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2402.06915</id><created>2024-02-10</created><updated>2025-02-23</updated><authors><author><keyname>Cho</keyname><forenames>Haeran</forenames></author><author><keyname>Kley</keyname><forenames>Tobias</forenames></author><author><keyname>Li</keyname><forenames>Housen</forenames></author></authors><title>Detection and inference of changes in high-dimensional linear regression   with non-sparse structures</title><categories>stat.ME math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For data segmentation in high-dimensional linear regression settings, the regression parameters are often assumed to be sparse segment-wise, which enables many existing methods to estimate the parameters locally via $\ell_1$-regularised maximum likelihood-type estimation and then contrast them for change point detection. Contrary to this common practice, we show that the exact sparsity of neither regression parameters nor their differences, a.k.a.\ differential parameters, is necessary for consistency in multiple change point detection. In fact, both statistically and computationally, better efficiency is attained by a simple strategy that scans for large discrepancies in local covariance between the regressors and the response. We go a step further and propose a suite of tools for directly inferring about the differential parameters post-segmentation, which are applicable even when the regression parameters themselves are non-sparse. Theoretical investigations are conducted under general conditions permitting non-Gaussianity, temporal dependence and ultra-high dimensionality. Numerical results from simulated and macroeconomic datasets demonstrate the competitiveness and efficacy of the proposed methods. Implementation of all methods is provided in the R package \texttt{inferchange} on GitHub. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.04405</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.04405</id><created>2024-03-07</created><updated>2025-02-25</updated><authors><author><keyname>Campi</keyname><forenames>Marta</forenames></author><author><keyname>Staerman</keyname><forenames>Guillaume</forenames></author><author><keyname>Peters</keyname><forenames>Gareth W.</forenames></author><author><keyname>Matsui</keyname><forenames>Tomoko</forenames></author></authors><title>Signature Isolation Forest</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Functional Isolation Forest (FIF) is a recent state-of-the-art Anomaly Detection (AD) algorithm designed for functional data. It relies on a tree partition procedure where an abnormality score is computed by projecting each curve observation on a drawn dictionary through a linear inner product. Such linear inner product and the dictionary are a priori choices that highly influence the algorithm's performances and might lead to unreliable results, particularly with complex datasets. This work addresses these challenges by introducing \textit{Signature Isolation Forest}, a novel AD algorithm class leveraging the rough path theory's signature transform. Our objective is to remove the constraints imposed by FIF through the proposition of two algorithms which specifically target the linearity of the FIF inner product and the choice of the dictionary. We provide several numerical experiments, including a real-world applications benchmark showing the relevance of our methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.11477</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.11477</id><created>2024-03-18</created><updated>2024-06-04</updated><authors><author><keyname>Zurek</keyname><forenames>Matthew</forenames></author><author><keyname>Chen</keyname><forenames>Yudong</forenames></author></authors><title>Span-Based Optimal Sample Complexity for Weakly Communicating and   General Average Reward MDPs</title><categories>cs.LG cs.IT math.IT math.OC stat.ML</categories><comments>Revision adds Theorem 3 on the difficulty of estimating the span of   the optimal bias. arXiv admin note: text overlap with arXiv:2311.13469</comments><journal-ref>Conference on Neural Information Processing Systems (NeurIPS),   2024</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the sample complexity of learning an $\varepsilon$-optimal policy in an average-reward Markov decision process (MDP) under a generative model. For weakly communicating MDPs, we establish the complexity bound $\widetilde{O}(SA\frac{H}{\varepsilon^2} )$, where $H$ is the span of the bias function of the optimal policy and $SA$ is the cardinality of the state-action space. Our result is the first that is minimax optimal (up to log factors) in all parameters $S,A,H$, and $\varepsilon$, improving on existing work that either assumes uniformly bounded mixing times for all policies or has suboptimal dependence on the parameters. We also initiate the study of sample complexity in general (multichain) average-reward MDPs. We argue a new transient time parameter $B$ is necessary, establish an $\widetilde{O}(SA\frac{B + H}{\varepsilon^2})$ complexity bound, and prove a matching (up to log factors) minimax lower bound. Both results are based on reducing the average-reward MDP to a discounted MDP, which requires new ideas in the general setting. To optimally analyze this reduction, we develop improved bounds for $\gamma$-discounted MDPs, showing that $\widetilde{O}(SA\frac{H}{(1-\gamma)^2\varepsilon^2} )$ and $\widetilde{O}(SA\frac{B + H}{(1-\gamma)^2\varepsilon^2} )$ samples suffice to learn $\varepsilon$-optimal policies in weakly communicating and in general MDPs, respectively. Both these results circumvent the well-known minimax lower bound of $\widetilde{\Omega}(SA\frac{1}{(1-\gamma)^3\varepsilon^2} )$ for $\gamma$-discounted MDPs, and establish a quadratic rather than cubic horizon dependence for a fixed MDP instance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2403.13260</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2403.13260</id><created>2024-03-19</created><updated>2025-02-23</updated><authors><author><keyname>Pan</keyname><forenames>Tianyu</forenames></author><author><keyname>Mei</keyname><forenames>Xiaoling</forenames></author><author><keyname>Zhang</keyname><forenames>Xiang</forenames></author><author><keyname>Shen</keyname><forenames>Weining</forenames></author><author><keyname>Ye</keyname><forenames>Ting</forenames></author></authors><title>A Bayesian Approach for Selecting Relevant External Data (BASE):   Application to a study of Long-Term Outcomes in a Hemophilia Gene Therapy   Trial</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Gene therapies aim to address the root causes of diseases, particularly those stemming from rare genetic defects that can be life-threatening or severely debilitating. Although an increasing number of gene therapies have received regulatory approvals in recent years, understanding their long-term efficacy in trials with limited follow-up time remains challenging. To address this critical question, we propose a novel Bayesian framework designed to selectively integrate relevant external data with internal trial data to improve the inference of the durability of long-term efficacy. We proved that the proposed method has desired theoretical properties, such as identifying and favoring external subsets deemed relevant, where the relevance is defined as the similarity, induced by the marginal likelihood, between the generating mechanisms of the internal data and the selected external data. We also conducted comprehensive simulations to evaluate its performance under various scenarios. Furthermore, we apply this method to predict and infer the endogenous factor IX (FIX) levels of patients who receive Etranacogene dezaparvovec over the long-term. Our estimated long-term FIX levels, validated by recent trial data, indicate that Etranacogene dezaparvovec induces sustained FIX production. Together, the theoretical findings, simulation results, and successful application of this framework underscore its potential to address similar long-term effectiveness estimation and inference questions in real world applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.00221</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.00221</id><created>2024-03-29</created><updated>2025-02-24</updated><authors><author><keyname>Sakaguchi</keyname><forenames>Shosei</forenames></author></authors><title>Robust Learning for Optimal Dynamic Treatment Regimes with Observational   Data</title><categories>stat.ME econ.EM math.ST stat.ML stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Public policies and medical interventions often involve dynamics in their treatment assignments, where individuals receive a series of interventions over multiple stages. We study the statistical learning of optimal dynamic treatment regimes (DTRs) that determine the optimal treatment assignment for each individual at each stage based on their evolving history. We propose a doubly robust, classification-based approach to learning the optimal DTR using observational data under the assumption of sequential ignorability. The approach learns the optimal DTR through backward induction. At each step, it constructs an augmented inverse probability weighting (AIPW) estimator of the policy value function and maximizes it to learn the optimal policy for the corresponding stage. We show that the resulting DTR achieves an optimal convergence rate of $n^{-1/2}$ for welfare regret under mild convergence conditions on estimators of the nuisance components. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.03453</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.03453</id><created>2024-04-04</created><updated>2025-02-24</updated><authors><author><keyname>Steinwart</keyname><forenames>Ingo</forenames></author></authors><title>Conditioning of Banach Space Valued Gaussian Random Variables: An   Approximation Approach Based on Martingales</title><categories>math.PR cs.LG math.ST stat.TH</categories><comments>46 pages plus 30 pages of supplemental material plus 4 pages of   references</comments><msc-class>Primary 60G15, Secondary 60B10, 60B11, 62F15, 62G05, 62G20, 28C20,   68T05</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We investigate the conditional distributions of two Banach space valued, jointly Gaussian random variables. In particular, we show that these conditional distributions are again Gaussian and that their means and covariances can be determined by a general finite dimensional approximation scheme. Here, it turns out that the covariance operators occurring in this scheme converge with respect to the nuclear norm and that the conditional probabilities converge weakly. Furthermore, we discuss how our approximation scheme can be implemented in several classes of important Banach spaces such as (reproducing kernel) Hilbert spaces, spaces of continuous functions, and other spaces consisting of functions. As an example, we then apply our general results to the case of continuous Gaussian processes that are conditioned to partial but infinite observations of their paths. Here we show that conditioning on sufficiently rich, increasing sets of finitely many observations leads to consistent approximations, that is, both the mean and covariance functions converge uniformly and the conditional probabilities converge weakly. Moreover, we discuss how these results improve our understanding of the popular Gaussian processes for machine learning. From a technical perspective our results are based upon a Banach space valued martingale approach for regular conditional probabilities. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2404.12589</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2404.12589</id><created>2024-04-18</created><updated>2025-02-24</updated><authors><author><keyname>Choi</keyname><forenames>Michael C. H.</forenames></author><author><keyname>Wang</keyname><forenames>Youjia</forenames></author><author><keyname>Wolfer</keyname><forenames>Geoffrey</forenames></author></authors><title>Geometry and factorization of multivariate Markov chains with   applications to the swapping algorithm</title><categories>math.PR cs.IT math.IT math.OC stat.CO</categories><comments>31 pages, 2 figures</comments><msc-class>60F10, 60J10, 60J22, 94A15, 94A17</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper analyzes the factorizability and geometry of transition matrices of multivariate Markov chains. Specifically, we demonstrate that the induced chains on factors of a product space can be regarded as information projections with respect to the Kullback-Leibler divergence. This perspective yields Han-Shearer type inequalities and submodularity of the entropy rate of Markov chains, as well as applications in the context of large deviations and mixing time comparison. As a concrete algorithmic application, we introduce a projection sampler based on the swapping algorithm, which resamples the highest-temperature coordinate at stationarity at each step. We prove that such practice accelerates the mixing time by multiplicative factors related to the number of temperatures and the dimension of the underlying state space when compared with the original swapping algorithm. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.05389</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.05389</id><created>2024-05-08</created><updated>2025-02-23</updated><authors><author><keyname>Cheng</keyname><forenames>Bing</forenames></author><author><keyname>Tong</keyname><forenames>Howell</forenames></author></authors><title>On foundation of generative statistics with G-entropy: a gradient-based   approach</title><categories>stat.ME</categories><comments>29 pages</comments><msc-class>60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the interplay between statistics and generative artificial intelligence. Generative statistics, an integral part of the latter, aims to construct models that can generate efficiently and meaningfully new data across the whole of the (usually high dimensional) sample space, e.g. a new photo. Within it, the gradient-based approach is a current favourite that exploits effectively, for the above purpose, the information contained in the observed sample, e.g. an old photo. However, often there are missing data in the observed sample, e.g., missing bits in the old photo. To handle this situation, we have proposed a gradient-based algorithm for generative modelling. More importantly, our paper underpins rigorously this powerful approach by introducing a new G-entropy that is related to the Fisher divergence. (The G-entropy is also of independent interest.) The underpinning has enabled the gradient-based approach to expand its scope. For example, it can now provide a tool for generative model selection. Possible future projects include discrete data and Bayesian variational inference. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.10221</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.10221</id><created>2024-05-16</created><updated>2025-02-22</updated><authors><author><keyname>Tu</keyname><forenames>Ben</forenames></author><author><keyname>Kantas</keyname><forenames>Nikolas</forenames></author><author><keyname>Lee</keyname><forenames>Robert M.</forenames></author><author><keyname>Shafei</keyname><forenames>Behrang</forenames></author></authors><title>Scalarisation-based risk concepts for robust multi-objective   optimisation</title><categories>math.OC cs.LG stat.ML</categories><comments>The code is available at: https://github.com/benmltu/scalarize</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust optimisation is a well-established framework for optimising functions in the presence of uncertainty. The inherent goal of this problem is to identify a collection of inputs whose outputs are both desirable for the decision maker, whilst also being robust to the underlying uncertainties in the problem. In this work, we study the multi-objective case of this problem. We identify that the majority of all robust multi-objective algorithms rely on two key operations: robustification and scalarisation. Robustification refers to the strategy that is used to account for the uncertainty in the problem. Scalarisation refers to the procedure that is used to encode the relative importance of each objective to a scalar-valued reward. As these operations are not necessarily commutative, the order that they are performed in has an impact on the resulting solutions that are identified and the final decisions that are made. The purpose of this work is to give a thorough exposition on the effects of these different orderings and in particular highlight when one should opt for one ordering over the other. As part of our analysis, we showcase how many existing risk concepts can be integrated into the specification and solution of a robust multi-objective optimisation problem. Besides this, we also demonstrate how one can principally define the notion of a robust Pareto front and a robust performance metric based on our ``robustify and scalarise'' methodology. To illustrate the efficacy of these new ideas, we present two insightful case studies which are based on real-world data sets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.13922</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.13922</id><created>2024-05-22</created><updated>2025-02-25</updated><authors><author><keyname>Emde</keyname><forenames>Cornelius</forenames></author><author><keyname>Pinto</keyname><forenames>Francesco</forenames></author><author><keyname>Lukasiewicz</keyname><forenames>Thomas</forenames></author><author><keyname>Torr</keyname><forenames>Philip H. S.</forenames></author><author><keyname>Bibi</keyname><forenames>Adel</forenames></author></authors><title>Towards Certification of Uncertainty Calibration under Adversarial   Attacks</title><categories>cs.LG stat.ML</categories><comments>10 pages main paper, appendix included Published at: International   Conference on Learning Representations (ICLR) 2025</comments><journal-ref>International Conference on Learning Representations (ICLR) 2025</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since neural classifiers are known to be sensitive to adversarial perturbations that alter their accuracy, \textit{certification methods} have been developed to provide provable guarantees on the insensitivity of their predictions to such perturbations. Furthermore, in safety-critical applications, the frequentist interpretation of the confidence of a classifier (also known as model calibration) can be of utmost importance. This property can be measured via the Brier score or the expected calibration error. We show that attacks can significantly harm calibration, and thus propose certified calibration as worst-case bounds on calibration under adversarial perturbations. Specifically, we produce analytic bounds for the Brier score and approximate bounds via the solution of a mixed-integer program on the expected calibration error. Finally, we propose novel calibration attacks and demonstrate how they can improve model calibration through \textit{adversarial calibration training}. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.15885</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.15885</id><created>2024-05-24</created><updated>2025-02-22</updated><authors><author><keyname>Zheng</keyname><forenames>Kaiwen</forenames></author><author><keyname>He</keyname><forenames>Guande</forenames></author><author><keyname>Chen</keyname><forenames>Jianfei</forenames></author><author><keyname>Bao</keyname><forenames>Fan</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author></authors><title>Diffusion Bridge Implicit Models</title><categories>cs.LG stat.ML</categories><comments>Accepted at ICLR 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Denoising diffusion bridge models (DDBMs) are a powerful variant of diffusion models for interpolating between two arbitrary paired distributions given as endpoints. Despite their promising performance in tasks like image translation, DDBMs require a computationally intensive sampling process that involves the simulation of a (stochastic) differential equation through hundreds of network evaluations. In this work, we take the first step in fast sampling of DDBMs without extra training, motivated by the well-established recipes in diffusion models. We generalize DDBMs via a class of non-Markovian diffusion bridges defined on the discretized timesteps concerning sampling, which share the same marginal distributions and training objectives, give rise to generative processes ranging from stochastic to deterministic, and result in diffusion bridge implicit models (DBIMs). DBIMs are not only up to 25$\times$ faster than the vanilla sampler of DDBMs but also induce a novel, simple, and insightful form of ordinary differential equation (ODE) which inspires high-order numerical solvers. Moreover, DBIMs maintain the generation diversity in a distinguished way, by using a booting noise in the initial sampling step, which enables faithful encoding, reconstruction, and semantic interpolation in image translation tasks. Code is available at https://github.com/thu-ml/DiffusionBridge. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.19912</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.19912</id><created>2024-05-30</created><updated>2025-02-23</updated><authors><author><keyname>Schrab</keyname><forenames>Antonin</forenames></author><author><keyname>Kim</keyname><forenames>Ilmun</forenames></author></authors><title>Robust Kernel Hypothesis Testing under Data Corruption</title><categories>stat.ML cs.LG</categories><comments>22 pages, 2 figures, 2 algorithms</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We propose a general method for constructing robust permutation tests under data corruption. The proposed tests effectively control the non-asymptotic type I error under data corruption, and we prove their consistency in power under minimal conditions. This contributes to the practical deployment of hypothesis tests for real-world applications with potential adversarial attacks. For the two-sample and independence settings, we show that our kernel robust tests are minimax optimal, in the sense that they are guaranteed to be non-asymptotically powerful against alternatives uniformly separated from the null in the kernel MMD and HSIC metrics at some optimal rate (tight with matching lower bound). We point out that existing differentially private tests can be adapted to be robust to data corruption, and we demonstrate in experiments that our proposed tests achieve much higher power than these private tests. Finally, we provide publicly available implementations and empirically illustrate the practicality of our robust tests. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2405.20318</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2405.20318</id><created>2024-05-30</created><updated>2025-02-24</updated><authors><author><keyname>Ceraolo</keyname><forenames>Roberto</forenames></author><author><keyname>Kharlapenko</keyname><forenames>Dmitrii</forenames></author><author><keyname>Khan</keyname><forenames>Ahmad</forenames></author><author><keyname>Reymond</keyname><forenames>Am√©lie</forenames></author><author><keyname>Mihalcea</keyname><forenames>Rada</forenames></author><author><keyname>Sch√∂lkopf</keyname><forenames>Bernhard</forenames></author><author><keyname>Sachan</keyname><forenames>Mrinmaya</forenames></author><author><keyname>Jin</keyname><forenames>Zhijing</forenames></author></authors><title>Quriosity: Analyzing Human Questioning Behavior and Causal Inquiry   through Curiosity-Driven Queries</title><categories>cs.CL cs.AI cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Recent progress in Large Language Model (LLM) technology has changed our role in interacting with these models. Instead of primarily testing these models with questions we already know answers to, we are now using them for queries where the answers are unknown to us, driven by human curiosity. This shift highlights the growing need to understand curiosity-driven human questions - those that are more complex, open-ended, and reflective of real-world needs. To this end, we present Quriosity, a collection of 13.5K naturally occurring questions from three diverse sources: human-to-search-engine queries, human-to-human interactions, and human-to-LLM conversations. Our comprehensive collection enables a rich understanding of human curiosity across various domains and contexts. Our analysis reveals a significant presence of causal questions (up to 42%) in the dataset, for which we develop an iterative prompt improvement framework to identify all causal queries and examine their unique linguistic properties, cognitive complexity and source distribution. Our paper paves the way for future work on causal question identification and open-ended chatbot interactions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.03321</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.03321</id><created>2024-06-05</created><updated>2025-02-25</updated><authors><author><keyname>Chernis</keyname><forenames>Tony</forenames></author><author><keyname>Koop</keyname><forenames>Gary</forenames></author><author><keyname>Tallman</keyname><forenames>Emily</forenames></author><author><keyname>West</keyname><forenames>Mike</forenames></author></authors><title>Decision synthesis in monetary policy</title><categories>stat.ME econ.EM</categories><comments>25 pages including references and appendix, 8 figures</comments><msc-class>62F15, 62C05, 62M10</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The macroeconomy is a sophisticated dynamic system involving significant uncertainties that complicate modelling. In response, decision-makers consider multiple models that provide different predictions and policy recommendations which are then synthesized into a policy decision. In this setting, we develop Bayesian predictive decision synthesis (BPDS) to formalize monetary policy decision processes. BPDS draws on recent developments in model combination and statistical decision theory that yield new opportunities in combining multiple models, emphasizing the integration of decision goals, expectations and outcomes into the model synthesis process. Our case study concerns central bank policy decisions about target interest rates with a focus on implications for multi-step macroeconomic forecasting. This application also motivates new methodological developments in conditional forecasting and BPDS, presented and developed here. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.03620</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.03620</id><created>2024-06-05</created><updated>2025-02-21</updated><authors><author><keyname>Asi</keyname><forenames>Hilal</forenames></author><author><keyname>Koren</keyname><forenames>Tomer</forenames></author><author><keyname>Liu</keyname><forenames>Daogao</forenames></author><author><keyname>Talwar</keyname><forenames>Kunal</forenames></author></authors><title>Private Online Learning via Lazy Algorithms</title><categories>cs.LG cs.CR cs.DS math.OC stat.ML</categories><comments>Fix some small typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of private online learning, specifically, online prediction from experts (OPE) and online convex optimization (OCO). We propose a new transformation that transforms lazy online learning algorithms into private algorithms. We apply our transformation for differentially private OPE and OCO using existing lazy algorithms for these problems. Our final algorithms obtain regret, which significantly improves the regret in the high privacy regime $\varepsilon \ll 1$, obtaining $\sqrt{T \log d} + T^{1/3} \log(d)/\varepsilon^{2/3}$ for DP-OPE and $\sqrt{T} + T^{1/3} \sqrt{d}/\varepsilon^{2/3}$ for DP-OCO. We also complement our results with a lower bound for DP-OPE, showing that these rates are optimal for a natural family of low-switching private algorithms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.04068</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.04068</id><created>2024-06-06</created><updated>2025-02-23</updated><authors><author><keyname>Chidambaram</keyname><forenames>Muthu</forenames></author><author><keyname>Ge</keyname><forenames>Rong</forenames></author></authors><title>Reassessing How to Compare and Improve the Calibration of Machine   Learning Models</title><categories>cs.LG math.ST stat.ML stat.TH</categories><comments>ICLR 2025, 29 pages, 14 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A machine learning model is calibrated if its predicted probability for an outcome matches the observed frequency for that outcome conditional on the model prediction. This property has become increasingly important as the impact of machine learning models has continued to spread to various domains. As a result, there are now a dizzying number of recent papers on measuring and improving the calibration of (specifically deep learning) models. In this work, we reassess the reporting of calibration metrics in the recent literature. We show that there exist trivial recalibration approaches that can appear seemingly state-of-the-art unless calibration and prediction metrics (i.e. test accuracy) are accompanied by additional generalization metrics such as negative log-likelihood. We then use a calibration-based decomposition of Bregman divergences to develop a new extension to reliability diagrams that jointly visualizes calibration and generalization error, and show how our visualization can be used to detect trade-offs between calibration and generalization. Along the way, we prove novel results regarding the relationship between full calibration error and confidence calibration error for Bregman divergences. We also establish the consistency of the kernel regression estimator for calibration error used in our visualization approach, which generalizes existing consistency results in the literature. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.04874</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.04874</id><created>2024-06-07</created><updated>2025-02-22</updated><authors><author><keyname>Baragatti</keyname><forenames>Meili</forenames></author><author><keyname>C√©line</keyname><forenames>Casenave</forenames></author><author><keyname>Cloez</keyname><forenames>Bertrand</forenames></author><author><keyname>M√©tivier</keyname><forenames>David</forenames></author><author><keyname>Sanchez</keyname><forenames>Isabelle</forenames></author></authors><title>Approximate Bayesian Computation with Deep Learning and Conformal   prediction</title><categories>stat.ME</categories><comments>57 pages, 11 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Approximate Bayesian Computation (ABC) methods are commonly used to approximate posterior distributions in models with unknown or computationally intractable likelihoods. Classical ABC methods are based on nearest neighbor type algorithms and rely on the choice of so-called summary statistics, distances between datasets and a tolerance threshold. Recently, methods combining ABC with more complex machine learning algorithms have been proposed to mitigate the impact of these ``user-choices''. In this paper, we propose the first, to our knowledge, ABC method completely free of summary statistics, distance, and tolerance threshold.   Moreover, in contrast with usual generalizations of the ABC method, it associates a confidence interval (having a proper frequentist marginal coverage) with the posterior mean estimation (or other moment-type estimates).   Our method, named ABCD-Conformal, uses a neural network with Monte Carlo Dropout to provide an estimation of the posterior mean (or other moment type functionals), and conformal theory to obtain associated confidence sets. Efficient for estimating multidimensional parameters and amortized, we test this new method on four different applications and compare it with other ABC methods in the literature. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.05428</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.05428</id><created>2024-06-08</created><updated>2025-02-23</updated><authors><author><keyname>Huang</keyname><forenames>Dong</forenames></author><author><keyname>Song</keyname><forenames>Xianwen</forenames></author><author><keyname>Yang</keyname><forenames>Pengkun</forenames></author></authors><title>Information-Theoretic Thresholds for the Alignments of Partially   Correlated Graphs</title><categories>cs.IT math.IT math.ST stat.ML stat.TH</categories><comments>41 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of recovering the hidden vertex correspondence between two correlated random graphs. We propose the partially correlated Erd\H{o}s-R\'enyi graphs model, wherein a pair of induced subgraphs with a certain number are correlated. We investigate the information-theoretic thresholds for recovering the latent correlated subgraphs and the hidden vertex correspondence. We prove that there exists an optimal rate for partial recovery for the number of correlated nodes, above which one can correctly match a fraction of vertices and below which correctly matching any positive fraction is impossible, and we also derive an optimal rate for exact recovery. In the proof of possibility results, we propose correlated functional digraphs, which partition the edges of the intersection graph into two types of components, and bound the error probability by lower-order cumulant generating functions. The proof of impossibility results build upon the generalized Fano's inequality and the recovery thresholds settled in correlated Erd\H{o}s-R\'enyi graphs model. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.09657</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.09657</id><created>2024-06-13</created><updated>2025-02-21</updated><authors><author><keyname>Ronen</keyname><forenames>Omer</forenames></author><author><keyname>Humayun</keyname><forenames>Ahmed Imtiaz</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard</forenames></author><author><keyname>Balestriero</keyname><forenames>Randall</forenames></author><author><keyname>Yu</keyname><forenames>Bin</forenames></author></authors><title>Mitigating over-exploration in latent space optimization using LES</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We develop Latent Exploration Score (LES) to mitigate over-exploration in Latent Space Optimization (LSO), a popular method for solving black-box discrete optimization problems. LSO utilizes continuous optimization within the latent space of a Variational Autoencoder (VAE) and is known to be susceptible to over-exploration, which manifests in unrealistic solutions that reduce its practicality. LES leverages the trained decoder's approximation of the data distribution, and can be employed with any VAE decoder - including pretrained ones - without additional training, architectural changes or access to the training data. Our evaluation across five LSO benchmark tasks and twenty-two VAE models demonstrates that LES always enhances the quality of the solutions while maintaining high objective values, leading to improvements over existing solutions in most cases. We believe that new avenues to LSO will be opened by LES' ability to identify out of distribution areas, differentiability, and computational tractability. Open source code for LES is available at https://github.com/OmerRonen/les. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.11667</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.11667</id><created>2024-06-17</created><updated>2025-02-23</updated><authors><author><keyname>Daskalakis</keyname><forenames>Constantinos</forenames></author><author><keyname>Golowich</keyname><forenames>Noah</forenames></author></authors><title>Is Efficient PAC Learning Possible with an Oracle That Responds 'Yes' or   'No'?</title><categories>cs.LG cs.AI stat.ML</categories><comments>COLT 2024</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The empirical risk minimization (ERM) principle has been highly impactful in machine learning, leading both to near-optimal theoretical guarantees for ERM-based learning algorithms as well as driving many of the recent empirical successes in deep learning. In this paper, we investigate the question of whether the ability to perform ERM, which computes a hypothesis minimizing empirical risk on a given dataset, is necessary for efficient learning: in particular, is there a weaker oracle than ERM which can nevertheless enable learnability? We answer this question affirmatively, showing that in the realizable setting of PAC learning for binary classification, a concept class can be learned using an oracle which only returns a single bit indicating whether a given dataset is realizable by some concept in the class. The sample complexity and oracle complexity of our algorithm depend polynomially on the VC dimension of the hypothesis class, thus showing that there is only a polynomial price to pay for use of our weaker oracle. Our results extend to the agnostic learning setting with a slight strengthening of the oracle, as well as to the partial concept, multiclass and real-valued learning settings. In the setting of partial concept classes, prior to our work no oracle-efficient algorithms were known, even with a standard ERM oracle. Thus, our results address a question of Alon et al. (2021) who asked whether there are algorithmic principles which enable efficient learnability in this setting. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.12120</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.12120</id><created>2024-06-17</created><updated>2025-02-23</updated><authors><author><keyname>Zhao</keyname><forenames>Yulai</forenames></author><author><keyname>Uehara</keyname><forenames>Masatoshi</forenames></author><author><keyname>Scalia</keyname><forenames>Gabriele</forenames></author><author><keyname>Kung</keyname><forenames>Sunyuan</forenames></author><author><keyname>Biancalani</keyname><forenames>Tommaso</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author><author><keyname>Hajiramezanali</keyname><forenames>Ehsan</forenames></author></authors><title>Adding Conditional Control to Diffusion Models with Reinforcement   Learning</title><categories>cs.LG cs.AI stat.ML</categories><comments>ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diffusion models are powerful generative models that allow for precise control over the characteristics of the generated samples. While these diffusion models trained on large datasets have achieved success, there is often a need to introduce additional controls in downstream fine-tuning processes, treating these powerful models as pre-trained diffusion models. This work presents a novel method based on reinforcement learning (RL) to add such controls using an offline dataset comprising inputs and labels. We formulate this task as an RL problem, with the classifier learned from the offline dataset and the KL divergence against pre-trained models serving as the reward functions. Our method, $\textbf{CTRL}$ ($\textbf{C}$onditioning pre-$\textbf{T}$rained diffusion models with $\textbf{R}$einforcement $\textbf{L}$earning), produces soft-optimal policies that maximize the abovementioned reward functions. We formally demonstrate that our method enables sampling from the conditional distribution with additional controls during inference. Our RL-based approach offers several advantages over existing methods. Compared to classifier-free guidance, it improves sample efficiency and can greatly simplify dataset construction by leveraging conditional independence between the inputs and additional controls. Additionally, unlike classifier guidance, it eliminates the need to train classifiers from intermediate states to additional controls. The code is available at https://github.com/zhaoyl18/CTRL. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.15957</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.15957</id><created>2024-06-22</created><updated>2025-02-21</updated><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Sly</keyname><forenames>Allan</forenames></author><author><keyname>Sohn</keyname><forenames>Youngtak</forenames></author></authors><title>Weak recovery, hypothesis testing, and mutual information in stochastic   block models and planted factor graphs</title><categories>math.PR cs.IT cs.SI math.CO math.IT math.ST stat.TH</categories><comments>80 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The stochastic block model is a canonical model of communities in random graphs. It was introduced in the social sciences and statistics as a model of communities, and in theoretical computer science as an average case model for graph partitioning problems under the name of the ``planted partition model.'' Given a sparse stochastic block model, the two standard inference tasks are: (i) Weak recovery: can we estimate the communities with non trivial overlap with the true communities? (ii) Detection/Hypothesis testing: can we distinguish if the sample was drawn from the block model or from a random graph with no community structure with probability tending to $1$ as the graph size tends to infinity?   In this work, we show that for sparse stochastic block models, the two inference tasks are equivalent except at a critical point. That is, weak recovery is information theoretically possible if and only if detection is possible. We thus find a strong connection between these two notions of inference for the model. We further prove that when detection is impossible, an explicit hypothesis test based on low degree polynomials in the adjacency matrix of the observed graph achieves the optimal statistical power. This low degree test is efficient as opposed to the likelihood ratio test, which is not known to be efficient. Moreover, we prove that the asymptotic mutual information between the observed network and the community structure exhibits a phase transition at the weak recovery threshold.   Our results are proven in much broader settings including the hypergraph stochastic block models and general planted factor graphs. In these settings we prove that the impossibility of weak recovery implies contiguity and provide a condition which guarantees the equivalence of weak recovery and detection. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2406.18651</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2406.18651</id><created>2024-06-26</created><updated>2025-01-31</updated><authors><author><keyname>Nuradha</keyname><forenames>Theshani</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Contraction of Private Quantum Channels and Private Quantum Hypothesis   Testing</title><categories>quant-ph cs.CR cs.IT cs.LG math.IT stat.ML</categories><comments>36 pages; See independent work titled "Sample Complexity of Locally   Differentially Private Quantum Hypothesis Testing" by Hao-Chung Cheng,   Christoph Hirche, and Cambyse Rouz\'e</comments><journal-ref>IEEE Transactions on Information Theory, Volume 71, Issue 3, Pages   1851--1873, March 2025</journal-ref><doi>10.1109/TIT.2025.3527859</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A quantum generalized divergence by definition satisfies the data-processing inequality; as such, the relative decrease in such a divergence under the action of a quantum channel is at most one. This relative decrease is formally known as the contraction coefficient of the channel and the divergence. Interestingly, there exist combinations of channels and divergences for which the contraction coefficient is strictly less than one. Furthermore, understanding the contraction coefficient is fundamental for the study of statistical tasks under privacy constraints. To this end, here we establish upper bounds on contraction coefficients for the hockey-stick divergence under privacy constraints, where privacy is quantified with respect to the quantum local differential privacy (QLDP) framework, and we fully characterize the contraction coefficient for the trace distance under privacy constraints. With the machinery developed, we also determine an upper bound on the contraction of both the Bures distance and quantum relative entropy relative to the normalized trace distance, under QLDP constraints. Next, we apply our findings to establish bounds on the sample complexity of quantum hypothesis testing under privacy constraints. Furthermore, we study various scenarios in which the sample complexity bounds are tight, while providing order-optimal quantum channels that achieve those bounds. Lastly, we show how private quantum channels provide fairness and Holevo information stability in quantum learning settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.00584</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.00584</id><created>2024-06-30</created><updated>2025-02-23</updated><authors><author><keyname>Dunbar</keyname><forenames>Oliver R. A.</forenames></author><author><keyname>Nelsen</keyname><forenames>Nicholas H.</forenames></author><author><keyname>Mutic</keyname><forenames>Maya</forenames></author></authors><title>Hyperparameter Optimization for Randomized Algorithms: A Case Study on   Random Features</title><categories>cs.LG stat.CO stat.ML</categories><journal-ref>Statistics and Computing Vol. 35 No. 56 (2025)</journal-ref><doi>10.1007/s11222-025-10587-w</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Randomized algorithms exploit stochasticity to reduce computational complexity. One important example is random feature regression (RFR) that accelerates Gaussian process regression (GPR). RFR approximates an unknown function with a random neural network whose hidden weights and biases are sampled from a probability distribution. Only the final output layer is fit to data. In randomized algorithms like RFR, the hyperparameters that characterize the sampling distribution greatly impact performance, yet are not directly accessible from samples. This makes optimization of hyperparameters via standard (gradient-based) optimization tools inapplicable. Inspired by Bayesian ideas from GPR, this paper introduces a random objective function that is tailored for hyperparameter tuning of vector-valued random features. The objective is minimized with ensemble Kalman inversion (EKI). EKI is a gradient-free particle-based optimizer that is scalable to high-dimensions and robust to randomness in objective functions. A numerical study showcases the new black-box methodology to learn hyperparameter distributions in several problems that are sensitive to the hyperparameter selection: two global sensitivity analyses, integrating a chaotic dynamical system, and solving a Bayesian inverse problem from atmospheric dynamics. The success of the proposed EKI-based algorithm for RFR suggests its potential for automated optimization of hyperparameters arising in other randomized algorithms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.03312</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.03312</id><created>2024-07-03</created><updated>2025-02-25</updated><authors><author><keyname>Holthuijzen</keyname><forenames>Maike F.</forenames></author><author><keyname>Gramacy</keyname><forenames>Robert B.</forenames></author><author><keyname>Carey</keyname><forenames>Cayelan C.</forenames></author><author><keyname>Higdon</keyname><forenames>Dave M.</forenames></author><author><keyname>Thomas</keyname><forenames>R. Quinn</forenames></author></authors><title>Synthesizing data products, mathematical models, and observational   measurements for lake temperature forecasting</title><categories>stat.AP</categories><comments>20 pages, 9 figures</comments><acm-class>G.3; J.2</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We present a novel forecasting framework for lake water temperature, which is crucial for managing lake ecosystems and drinking water resources. The General Lake Model (GLM) has been previously used for this purpose, but, similar to many process-based simulation models, it: requires a large number of inputs, many of which are stochastic; presents challenges for uncertainty quantification (UQ); and can exhibit model bias. To address these issues, we propose a Gaussian process (GP) surrogate-based forecasting approach that efficiently handles large, high-dimensional data and accounts for input-dependent variability and systematic GLM bias. We validate the proposed approach and compare it with other forecasting methods, including a climatological model and raw GLM simulations. Our results demonstrate that our bias-corrected GP surrogate (GPBC) can outperform competing approaches in terms of forecast accuracy and UQ up to two weeks into the future. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.09390</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.09390</id><created>2024-07-12</created><updated>2025-02-25</updated><authors><author><keyname>Barigozzi</keyname><forenames>Matteo</forenames></author><author><keyname>Cho</keyname><forenames>Haeran</forenames></author><author><keyname>Maeng</keyname><forenames>Hyeyoung</forenames></author></authors><title>Tail-robust factor modelling of vector and tensor time series in high   dimensions</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  We study the problem of factor modelling vector- and tensor-valued time series in the presence of heavy tails in the data, which produce anomalous observations with non-negligible probability. For this, we propose to combine a two-step procedure for tensor data decomposition with data truncation, which is easy to implement and does not require an iterative search for a numerical solution. Departing away from the light-tail assumptions often adopted in the time series factor modelling literature, we derive the consistency and asymptotic normality of the proposed estimators while assuming the existence of the $(2 + 2\epsilon)$-th moment only for some $\epsilon \in (0, 1)$. Our rates explicitly depend on $\eps$ characterising the effect of heavy tails, and on the chosen level of truncation. We also propose a consistent criterion for determining the number of factors. Simulation studies and applications to two macroeconomic datasets demonstrate the good performance of the proposed estimators. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.09735</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.09735</id><created>2024-07-12</created><updated>2025-02-24</updated><authors><author><keyname>Liu</keyname><forenames>Siyan</forenames></author><author><keyname>Yeh</keyname><forenames>Chi-Kuang</forenames></author><author><keyname>Zhang</keyname><forenames>Xin</forenames></author><author><keyname>Tian</keyname><forenames>Qinglong</forenames></author><author><keyname>Li</keyname><forenames>Pengfei</forenames></author></authors><title>Positive and Unlabeled Data: Model, Estimation, Inference, and   Classification</title><categories>stat.ME stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study introduces a new approach to addressing positive and unlabeled (PU) data through the double exponential tilting model (DETM). Traditional methods often fall short because they only apply to selected completely at random (SCAR) PU data, where the labeled positive and unlabeled positive data are assumed to be from the same distribution. In contrast, our DETM's dual structure effectively accommodates the more complex and underexplored selected at random PU data, where the labeled and unlabeled positive data can be from different distributions. We rigorously establish the theoretical foundations of DETM, including identifiability, parameter estimation, and asymptotic properties. Additionally, we move forward to statistical inference by developing a goodness-of-fit test for the SCAR condition and constructing confidence intervals for the proportion of positive instances in the target domain. We leverage an approximated Bayes classifier for classification tasks, demonstrating DETM's robust performance in prediction. Through theoretical insights and practical applications, this study highlights DETM as a comprehensive framework for addressing the challenges of PU data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.10070</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.10070</id><created>2024-07-14</created><updated>2025-02-21</updated><authors><author><keyname>Rathore</keyname><forenames>Pratik</forenames></author><author><keyname>Frangella</keyname><forenames>Zachary</forenames></author><author><keyname>Yang</keyname><forenames>Jiaming</forenames></author><author><keyname>Derezi≈Ñski</keyname><forenames>Micha≈Ç</forenames></author><author><keyname>Udell</keyname><forenames>Madeleine</forenames></author></authors><title>Have ASkotch: A Neat Solution for Large-scale Kernel Ridge Regression</title><categories>cs.LG math.OC stat.ML</categories><comments>64 pages (including appendices), 16 figures, 5 tables</comments><msc-class>65F10, 68W20, 90C06</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Kernel ridge regression (KRR) is a fundamental computational tool, appearing in problems that range from computational chemistry to health analytics, with a particular interest due to its starring role in Gaussian process regression. However, full KRR solvers are challenging to scale to large datasets: both direct (i.e., Cholesky decomposition) and iterative methods (i.e., PCG) incur prohibitive computational and storage costs. The standard approach to scale KRR to large datasets chooses a set of inducing points and solves an approximate version of the problem, inducing points KRR. However, the resulting solution tends to have worse predictive performance than the full KRR solution. In this work, we introduce a new solver, ASkotch, for full KRR that provides better solutions faster than state-of-the-art solvers for full and inducing points KRR. ASkotch is a scalable, accelerated, iterative method for full KRR that provably obtains linear convergence. Under appropriate conditions, we show that ASkotch obtains condition-number-free linear convergence. This convergence analysis rests on the theory of ridge leverage scores and determinantal point processes. ASkotch outperforms state-of-the-art KRR solvers on a testbed of 23 large-scale KRR regression and classification tasks derived from a wide range of application domains, demonstrating the superiority of full KRR over inducing points KRR. Our work opens up the possibility of as-yet-unimagined applications of full KRR across a number of disciplines. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.12316</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.12316</id><created>2024-07-17</created><updated>2025-02-24</updated><authors><author><keyname>Kreiss</keyname><forenames>Jens-Peter</forenames></author><author><keyname>Leucht</keyname><forenames>Anne</forenames></author><author><keyname>Paparoditis</keyname><forenames>Efstathios</forenames></author></authors><title>Gaussian Approximation for Lag-Window Estimators and the Construction of   Confidence bands for the Spectral Density</title><categories>math.ST stat.TH</categories><comments>30</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we consider the construction of simultaneous confidence bands for the spectral density of a stationary time series using a Gaussian approximation for classical lag-window spectral density estimators evaluated at the set of all positive Fourier frequencies. The Gaussian approximation opens up the possibility to verify asymptotic validity of a multiplier bootstrap procedure and, even further, to derive the corresponding rate of convergence. A small simulation study sheds light on the finite sample properties of this bootstrap proposal. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.13389</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.13389</id><created>2024-07-18</created><updated>2025-02-23</updated><authors><author><keyname>Tretyakov</keyname><forenames>M. V.</forenames></author></authors><title>Sampling from mixture distributions based on regime-switching diffusions</title><categories>math.NA cs.NA math.PR math.ST stat.TH</categories><comments>accepted by SIAM J. Sci. Comp</comments><msc-class>65C30, 60H35, 60H10, 37H10</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  It is proposed to use stochastic differential equations with state-dependent switching rates (SDEwS) for sampling from finite mixture distributions. An Euler scheme with constant time step for SDEwS is considered. It is shown that the scheme converges with order one in weak sense and also in the ergodic limit. Numerical experiments illustrate the use of SDEwS for sampling from mixture distributions and confirm the theoretical results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2407.15340</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2407.15340</id><created>2024-07-21</created><authors><author><keyname>Romano</keyname><forenames>Elvira</forenames></author><author><keyname>Loffredo</keyname><forenames>Giuseppe</forenames></author><author><keyname>Maturo</keyname><forenames>Fabrizio</forenames></author></authors><title>Random Survival Forest for Censored Functional Data</title><categories>stat.ME stat.ML</categories><comments>18 pages</comments><msc-class>62R10, 62N02, 62P10</msc-class><acm-class>G.3</acm-class><journal-ref>Statistics in Medicine, 2025,44 (e10344), 1-22</journal-ref><doi>10.1002/sim.10344</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper introduces a Random Survival Forest (RSF) method for functional data. The focus is specifically on defining a new functional data structure, the Censored Functional Data (CFD), for dealing with temporal observations that are censored due to study limitations or incomplete data collection. This approach allows for precise modelling of functional survival trajectories, leading to improved interpretation and prediction of survival dynamics across different groups. A medical survival study on the benchmark SOFA data set is presented. Results show good performance of the proposed approach, particularly in ranking the importance of predicting variables, as captured through dynamic changes in SOFA scores and patient mortality rates. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.03463</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.03463</id><created>2024-08-06</created><updated>2025-02-23</updated><authors><author><keyname>Jeanselme</keyname><forenames>Vincent</forenames></author><author><keyname>Yoon</keyname><forenames>Chang Ho</forenames></author><author><keyname>Falck</keyname><forenames>Fabian</forenames></author><author><keyname>Tom</keyname><forenames>Brian</forenames></author><author><keyname>Barrett</keyname><forenames>Jessica</forenames></author></authors><title>Identifying treatment response subgroups in observational time-to-event   data</title><categories>stat.ME cs.AI</categories><comments>Preprint under review</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Identifying patient subgroups with different treatment responses is an important task to inform medical recommendations, guidelines, and the design of future clinical trials. Existing approaches for treatment effect estimation primarily rely on Randomised Controlled Trials (RCTs), which are often limited by insufficient power, multiple comparisons, and unbalanced covariates. In addition, RCTs tend to feature more homogeneous patient groups, making them less relevant for uncovering subgroups in the population encountered in real-world clinical practice. Subgroup analyses established for RCTs suffer from significant statistical biases when applied to observational studies, which benefit from larger and more representative populations. Our work introduces a novel, outcome-guided, subgroup analysis strategy for identifying subgroups of treatment response in both RCTs and observational studies alike. It hence positions itself in-between individualised and average treatment effect estimation to uncover patient subgroups with distinct treatment responses, critical for actionable insights that may influence treatment guidelines. In experiments, our approach significantly outperforms the current state-of-the-art method for subgroup analysis in both randomised and observational treatment regimes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.11074</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.11074</id><created>2024-08-18</created><updated>2024-08-31</updated><authors><author><keyname>Sep√∫lveda</keyname><forenames>Mauricio Vargas</forenames></author><author><keyname>Malamud</keyname><forenames>Jonathan Schneider</forenames></author></authors><title>cpp11armadillo: An R Package to Use the Armadillo C++ Library</title><categories>cs.MS cs.PL stat.CO</categories><comments>23 pages, 0 figures</comments><acm-class>D.1.5; D.3.3; F.2.1</acm-class><doi>10.1016/j.softx.2025.102087.</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This article introduces 'cpp11armadillo', a new R package that integrates the powerful Armadillo C++ library for linear algebra into the R programming environment. Targeted primarily at social scientists and other non-programmers, this article explains the computational benefits of moving code to C++ in terms of speed and syntax. We provide a comprehensive overview of Armadillo's capabilities, highlighting its user-friendly syntax akin to MATLAB and its efficiency for computationally intensive tasks. The 'cpp11armadillo' package simplifies a part of the process of using C++ within R by offering additional ease of integration for those who require high-performance linear algebra operations in their R workflows. This work aims to bridge the gap between computational efficiency and accessibility, making advanced linear algebra operations more approachable for R users without extensive programming backgrounds. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.13448</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.13448</id><created>2024-08-23</created><updated>2025-02-23</updated><authors><author><keyname>Duong</keyname><forenames>Bao</forenames></author><author><keyname>Le</keyname><forenames>Hung</forenames></author><author><keyname>Huang</keyname><forenames>Biwei</forenames></author><author><keyname>Nguyen</keyname><forenames>Thin</forenames></author></authors><title>Reinforcement Learning for Causal Discovery without Acyclicity   Constraints</title><categories>cs.LG stat.ME stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, reinforcement learning (RL) has proved a promising alternative for conventional local heuristics in score-based approaches to learning directed acyclic causal graphs (DAGs) from observational data. However, the intricate acyclicity constraint still challenges the efficient exploration of the vast space of DAGs in existing methods. In this study, we introduce ALIAS (reinforced dAg Learning wIthout Acyclicity conStraints), a novel approach to causal discovery powered by the RL machinery. Our method features an efficient policy for generating DAGs in just a single step with an optimal quadratic complexity, fueled by a novel parametrization of DAGs that directly translates a continuous space to the space of all DAGs, bypassing the need for explicitly enforcing acyclicity constraints. This approach enables us to navigate the search space more effectively by utilizing policy gradient methods and established scoring functions. In addition, we provide compelling empirical evidence for the strong performance of ALIAS in comparison with state-of-the-arts in causal discovery over increasingly difficult experiment conditions on both synthetic and real datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.15451</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.15451</id><created>2024-08-27</created><updated>2025-02-23</updated><authors><author><keyname>Qiao</keyname><forenames>Yiran</forenames></author><author><keyname>Yin</keyname><forenames>Yu</forenames></author><author><keyname>Chen</keyname><forenames>Chen</forenames></author><author><keyname>Ma</keyname><forenames>Jing</forenames></author></authors><title>Certified Causal Defense with Generalizable Robustness</title><categories>cs.LG cs.CR stat.ME</categories><comments>Submitted to AAAI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While machine learning models have proven effective across various scenarios, it is widely acknowledged that many models are vulnerable to adversarial attacks. Recently, there have emerged numerous efforts in adversarial defense. Among them, certified defense is well known for its theoretical guarantees against arbitrary adversarial perturbations on input within a certain range (e.g., $l_2$ ball). However, most existing works in this line struggle to generalize their certified robustness in other data domains with distribution shifts. This issue is rooted in the difficulty of eliminating the negative impact of spurious correlations on robustness in different domains. To address this problem, in this work, we propose a novel certified defense framework GLEAN, which incorporates a causal perspective into the generalization problem in certified defense. More specifically, our framework integrates a certifiable causal factor learning component to disentangle the causal relations and spurious correlations between input and label, and thereby exclude the negative effect of spurious correlations on defense. On top of that, we design a causally certified defense strategy to handle adversarial attacks on latent causal factors. In this way, our framework is not only robust against malicious noises on data in the training distribution but also can generalize its robustness across domains with distribution shifts. Extensive experiments on benchmark datasets validate the superiority of our framework in certified robustness generalization in different data domains. Code is available in the supplementary materials. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.16004</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.16004</id><created>2024-08-13</created><updated>2025-02-25</updated><authors><author><keyname>Risser</keyname><forenames>Mark D.</forenames></author><author><keyname>Ombadi</keyname><forenames>Mohammed</forenames></author><author><keyname>Wehner</keyname><forenames>Michael F.</forenames></author></authors><title>Granger causal inference for climate change attribution</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Climate change detection and attribution (D&amp;A) is concerned with determining the extent to which anthropogenic activities have influenced specific aspects of the global climate system. D&amp;A fits within the broader field of causal inference, the collection of statistical methods that identify cause and effect relationships. There are a wide variety of methods for making attribution statements, each of which require different types of input data and each of which are conditional to varying extents. Some methods are based on Pearl causality (experimental interference) while others leverage Granger (predictive) causality, and the causal framing provides important context for how the resulting attribution conclusion should be interpreted. However, while Granger-causal attribution analyses have become more common, there is no clear statement of their strengths and weaknesses and no clear consensus on where and when Granger-causal perspectives are appropriate. In this prospective paper, we provide a formal definition for Granger-based approaches to trend and event attribution and a clear comparison with more traditional methods for assessing the human influence on extreme weather and climate events. Broadly speaking, Granger-causal attribution statements can be constructed quickly from observations and do not require computationally-intesive dynamical experiments. These analyses also enable rapid attribution, which is useful in the aftermath of a severe weather event, and provide multiple lines of evidence for anthropogenic climate change when paired with Pearl-causal attribution. Confidence in attribution statements is increased when different methodologies arrive at similar conclusions. Moving forward, we encourage the D&amp;A community to embrace hybrid approaches to climate change attribution that leverage the strengths of both Granger and Pearl causality. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2408.16115</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2408.16115</id><created>2024-08-28</created><updated>2025-02-25</updated><authors><author><keyname>Bergna</keyname><forenames>Richard</forenames></author><author><keyname>Calvo-Ordo√±ez</keyname><forenames>Sergio</forenames></author><author><keyname>Opolka</keyname><forenames>Felix L.</forenames></author><author><keyname>Li√≤</keyname><forenames>Pietro</forenames></author><author><keyname>Hernandez-Lobato</keyname><forenames>Jose Miguel</forenames></author></authors><title>Uncertainty Modeling in Graph Neural Networks via Stochastic   Differential Equations</title><categories>cs.LG stat.ML</categories><comments>Accepted at ICLR 2025 as Spotlight. 17 pages including appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel Stochastic Differential Equation (SDE) framework to address the problem of learning uncertainty-aware representations for graph-structured data. While Graph Neural Ordinary Differential Equations (GNODEs) have shown promise in learning node representations, they lack the ability to quantify uncertainty. To address this, we introduce Latent Graph Neural Stochastic Differential Equations (LGNSDE), which enhance GNODE by embedding randomness through a Bayesian prior-posterior mechanism for epistemic uncertainty and Brownian motion for aleatoric uncertainty. By leveraging the existence and uniqueness of solutions to graph-based SDEs, we prove that the variance of the latent space bounds the variance of model outputs, thereby providing theoretically sensible guarantees for the uncertainty estimates. Furthermore, we show mathematically that LGNSDEs are robust to small perturbations in the input, maintaining stability over time. Empirical results across several benchmarks demonstrate that our framework is competitive in out-of-distribution detection, robustness to noise, and active learning, underscoring the ability of LGNSDEs to quantify uncertainty reliably. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.00568</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.00568</id><created>2024-08-31</created><updated>2024-09-07</updated><authors><author><keyname>Sepulveda</keyname><forenames>Mauricio Vargas</forenames></author></authors><title>Welding R and C++: A Tale of Two Programming Languages</title><categories>cs.MS cs.PL stat.CO</categories><comments>21 pages, 0 figures, 13 tables</comments><doi>10.1016/j.softx.2025.102087</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This article compares `cpp11armadillo` and `cpp11eigen`, new R packages that integrate the powerful Armadillo and Eigen C++ libraries for linear algebra into the R programming environment. This article provides a detailed comparison between Armadillo and Eigen speed and syntax. The goal of these packages is to simplify a part of the process of solving bottlenecks by using C++ within R, these offer additional ease of integration for users who require high-performance linear algebra operations in their R workflows. This document aims to discuss the tradeoff between computational efficiency and accessibility. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.02154</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.02154</id><created>2024-09-03</created><updated>2025-01-21</updated><authors><author><keyname>Bartlett</keyname><forenames>Deaglan J.</forenames></author><author><keyname>Chiarenza</keyname><forenames>Marco</forenames></author><author><keyname>Doeser</keyname><forenames>Ludvig</forenames></author><author><keyname>Leclercq</keyname><forenames>Florent</forenames></author></authors><title>COmoving Computer Acceleration (COCA): $N$-body simulations in an   emulated frame of reference</title><categories>astro-ph.IM astro-ph.CO cs.LG stat.ML</categories><comments>23 pages, 13 figures. Accepted for publication in A&amp;A</comments><journal-ref>A&amp;A, 694, A287 (2025)</journal-ref><doi>10.1051/0004-6361/202452217</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  $N$-body simulations are computationally expensive, so machine-learning (ML)-based emulation techniques have emerged as a way to increase their speed. Although fast, surrogate models have limited trustworthiness due to potentially substantial emulation errors that current approaches cannot correct for. To alleviate this problem, we introduce COmoving Computer Acceleration (COCA), a hybrid framework interfacing ML with an $N$-body simulator. The correct physical equations of motion are solved in an emulated frame of reference, so that any emulation error is corrected by design. This approach corresponds to solving for the perturbation of particle trajectories around the machine-learnt solution, which is computationally cheaper than obtaining the full solution, yet is guaranteed to converge to the truth as one increases the number of force evaluations. Although applicable to any ML algorithm and $N$-body simulator, this approach is assessed in the particular case of particle-mesh cosmological simulations in a frame of reference predicted by a convolutional neural network, where the time dependence is encoded as an additional input parameter to the network. COCA efficiently reduces emulation errors in particle trajectories, requiring far fewer force evaluations than running the corresponding simulation without ML. We obtain accurate final density and velocity fields for a reduced computational budget. We demonstrate that this method shows robustness when applied to examples outside the range of the training data. When compared to the direct emulation of the Lagrangian displacement field using the same training resources, COCA's ability to correct emulation errors results in more accurate predictions. COCA makes $N$-body simulations cheaper by skipping unnecessary force evaluations, while still solving the correct equations of motion and correcting for emulation errors made by ML. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.03749</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.03749</id><created>2024-09-05</created><updated>2025-02-24</updated><authors><author><keyname>Schmid</keyname><forenames>Christian</forenames></author><author><keyname>Murray</keyname><forenames>James M.</forenames></author></authors><title>Dynamics of Supervised and Reinforcement Learning in the Non-Linear   Perceptron</title><categories>cs.LG q-bio.NC stat.ML</categories><comments>NeurIPS 2024 camera ready version v3: Minor typo corrected</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The ability of a brain or a neural network to efficiently learn depends crucially on both the task structure and the learning rule. Previous works have analyzed the dynamical equations describing learning in the relatively simplified context of the perceptron under assumptions of a student-teacher framework or a linearized output. While these assumptions have facilitated theoretical understanding, they have precluded a detailed understanding of the roles of the nonlinearity and input-data distribution in determining the learning dynamics, limiting the applicability of the theories to real biological or artificial neural networks. Here, we use a stochastic-process approach to derive flow equations describing learning, applying this framework to the case of a nonlinear perceptron performing binary classification. We characterize the effects of the learning rule (supervised or reinforcement learning, SL/RL) and input-data distribution on the perceptron's learning curve and the forgetting curve as subsequent tasks are learned. In particular, we find that the input-data noise differently affects the learning speed under SL vs. RL, as well as determines how quickly learning of a task is overwritten by subsequent learning. Additionally, we verify our approach with real data using the MNIST dataset. This approach points a way toward analyzing learning dynamics for more-complex circuit architectures. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.06490</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.06490</id><created>2024-09-09</created><updated>2025-02-22</updated><authors><author><keyname>Chen</keyname><forenames>Yu-Hsi</forenames></author></authors><title>UAVDB: Trajectory-Guided Adaptable Bounding Boxes for UAV Detection</title><categories>cs.CV stat.AP</categories><comments>9 pages, 5 figures, 4 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The widespread deployment of Unmanned Aerial Vehicles (UAVs) in surveillance, security, and airspace management has created an urgent demand for precise, scalable, and efficient UAV detection. However, existing datasets often suffer from limited scale diversity and inaccurate annotations, hindering robust model development. This paper introduces UAVDB, a high-resolution UAV detection dataset constructed using Patch Intensity Convergence (PIC). This novel technique automatically generates high-fidelity bounding box annotations from UAV trajectory data~\cite{li2020reconstruction}, eliminating the need for manual labeling. UAVDB features single-class annotations with a fixed-camera setup and consists of RGB frames capturing UAVs across various scales, from large-scale UAVs to near-single-pixel representations, along with challenging backgrounds that pose difficulties for modern detectors. We first validate the accuracy and efficiency of PIC-generated bounding boxes by comparing Intersection over Union (IoU) performance and runtime against alternative annotation methods, demonstrating that PIC achieves higher annotation accuracy while being more efficient. Subsequently, we benchmark UAVDB using state-of-the-art (SOTA) YOLO-series detectors, establishing UAVDB as a valuable resource for advancing long-range and high-resolution UAV detection. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.08924</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.08924</id><created>2024-09-13</created><updated>2025-02-24</updated><authors><author><keyname>Li</keyname><forenames>Kendrick</forenames></author><author><keyname>Linderman</keyname><forenames>George C.</forenames></author><author><keyname>Shi</keyname><forenames>Xu</forenames></author><author><keyname>Tchetgen</keyname><forenames>Eric J. Tchetgen</forenames></author></authors><title>Regression-based proximal causal inference for right-censored   time-to-event data</title><categories>stat.ME</categories><comments>14 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Unmeasured confounding is one of the major concerns in causal inference from observational data. Proximal causal inference (PCI) is an emerging methodological framework to detect and potentially account for confounding bias by carefully leveraging a pair of negative control exposure (NCE) and outcome (NCO) variables, also known as treatment and outcome confounding proxies. Although regression-based PCI is well developed for binary and continuous outcomes, analogous PCI regression methods for right-censored time-to-event outcomes are currently lacking. In this paper, we propose a novel two-stage regression PCI approach for right-censored survival data under an additive hazard structural model. We provide theoretical justification for the proposed approach tailored to different types of NCOs, including continuous, count, and right-censored time-to-event variables. We illustrate the approach with an evaluation of the effectiveness of right heart catheterization among critically ill patients using data from the SUPPORT study. Our method is implemented in the open-access R package 'pci2s'. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.09973</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.09973</id><created>2024-09-16</created><updated>2025-02-24</updated><authors><author><keyname>Graham</keyname><forenames>Ellen</forenames><affiliation>University of Washington</affiliation></author><author><keyname>Carone</keyname><forenames>Marco</forenames><affiliation>University of Washington</affiliation></author><author><keyname>Rotnitzky</keyname><forenames>Andrea</forenames><affiliation>University of Washington</affiliation></author></authors><title>Towards a Unified Theory for Semiparametric Data Fusion with   Individual-Level Data</title><categories>math.ST stat.ME stat.ML stat.TH</categories><comments>122 pages. Updated to simplify notation and include a supplemental   section discussing the relationship between this work and arXiv:2111.14945</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the goal of conducting inference about a smooth finite-dimensional parameter by utilizing individual-level data from various independent sources. Recent advancements have led to the development of a comprehensive theory capable of handling scenarios where different data sources align with, possibly distinct subsets of, conditional distributions of a single factorization of the joint target distribution. While this theory proves effective in many significant contexts, it falls short in certain common data fusion problems, such as two-sample instrumental variable analysis, settings that integrate data from epidemiological studies with diverse designs (e.g., prospective cohorts and retrospective case-control studies), and studies with variables prone to measurement error that are supplemented by validation studies. In this paper, we extend the aforementioned comprehensive theory to allow for the fusion of individual-level data from sources aligned with conditional distributions that do not correspond to a single factorization of the target distribution. Assuming conditional and marginal distribution alignments, we provide universal results that characterize the class of all influence functions of regular asymptotically linear estimators and the efficient influence function of any pathwise differentiable parameter, irrespective of the number of data sources, the specific parameter of interest, or the statistical model for the target distribution. This theory paves the way for machine-learning debiased, semiparametric efficient estimation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2409.16409</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2409.16409</id><created>2024-09-24</created><updated>2025-02-24</updated><authors><author><keyname>Chen</keyname><forenames>Shijie</forenames></author><author><keyname>Lahiri</keyname><forenames>P.</forenames></author><author><keyname>Rao</keyname><forenames>J. N. K.</forenames></author></authors><title>Mean Squared Prediction Error Estimators of EBLUP of a Small Area Mean   Under a Semi-Parametric Fay-Herriot Model</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we derive a second-order unbiased (or nearly unbiased) mean squared prediction error (MSPE) estimator of the empirical best linear unbiased predictor (EBLUP) of a small area mean for a semi-parametric extension to the well-known Fay-Herriot model. Specifically, we derive our MSPE estimator essentially assuming certain moment conditions on both the sampling errors and random effects distributions. The normality-based Prasad-Rao MSPE estimator has a surprising robustness property in that it remains second-order unbiased under the non-normality of random effects when a simple Prasad-Rao method-of-moments estimator is used for the variance component and the sampling error distribution is normal. We show that the normality-based MSPE estimator is no longer second-order unbiased when the sampling error distribution has non-zero kurtosis or when the Fay-Herriot moment method is used to estimate the variance component, even when the sampling error distribution is normal. It is interesting to note that when the simple method-of moments estimator is used for the variance component, our proposed MSPE estimator does not require the estimation of kurtosis of the random effects. Results of a simulation study on the accuracy of the proposed MSPE estimator, under non-normality of both sampling and random effects distributions, are also presented. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.00116</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.00116</id><created>2024-09-30</created><updated>2025-02-24</updated><authors><author><keyname>Sire</keyname><forenames>Charlie</forenames></author><author><keyname>Garnier</keyname><forenames>Josselin</forenames></author><author><keyname>Durantin</keyname><forenames>C√©dric</forenames></author><author><keyname>Kerleguer</keyname><forenames>Baptiste</forenames></author><author><keyname>Defaux</keyname><forenames>Gilles</forenames></author><author><keyname>Perrin</keyname><forenames>Guillaume</forenames></author></authors><title>Bayesian Calibration for Prediction in a Multi-Output Transposition   Context</title><categories>stat.ME math.ST stat.TH</categories><comments>Submitted to International Journal for Uncertainty Quantification</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Numerical simulations are widely used to predict the behavior of physical systems, with Bayesian approaches being particularly well suited for this purpose. However, experimental observations are necessary to calibrate certain simulator parameters for the prediction. In this work, we use a multi-output simulator to predict all its outputs, including those that have never been experimentally observed. This situation is referred to as the transposition context. To accurately quantify the discrepancy between model outputs and real data in this context, conventional methods cannot be applied, and the Bayesian calibration must be augmented by incorporating a joint model error across all outputs. To achieve this, the proposed method is to consider additional numerical input parameters within a hierarchical Bayesian model, which includes hyperparameters for the prior distribution of the calibration variables. This approach is applied on a computer code with three outputs that models the Taylor cylinder impact test with a small number of observations. The outputs are considered as the observed variables one at a time, to work with three different transposition situations. The proposed method is compared with other approaches that embed model errors to demonstrate the significance of the hierarchical formulation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.01316</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.01316</id><created>2024-10-02</created><updated>2025-02-24</updated><authors><author><keyname>Hertrich</keyname><forenames>Johannes</forenames></author><author><keyname>Jahn</keyname><forenames>Tim</forenames></author><author><keyname>Quellmalz</keyname><forenames>Michael</forenames></author></authors><title>Fast Summation of Radial Kernels via QMC Slicing</title><categories>math.NA cs.LG cs.NA stat.CO stat.ML</categories><comments>Published as a conference paper at ICLR 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fast computation of large kernel sums is a challenging task, which arises as a subproblem in any kernel method. We approach the problem by slicing, which relies on random projections to one-dimensional subspaces and fast Fourier summation. We prove bounds for the slicing error and propose a quasi-Monte Carlo (QMC) approach for selecting the projections based on spherical quadrature rules. Numerical examples demonstrate that our QMC-slicing approach significantly outperforms existing methods like (QMC-)random Fourier features, orthogonal Fourier features or non-QMC slicing on standard test datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.02164</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.02164</id><created>2024-10-02</created><updated>2025-02-23</updated><authors><author><keyname>Ghane</keyname><forenames>Reza</forenames></author><author><keyname>Akhtiamov</keyname><forenames>Danil</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Universality in Transfer Learning for Linear Models</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study the problem of transfer learning and fine-tuning in linear models for both regression and binary classification. In particular, we consider the use of stochastic gradient descent (SGD) on a linear model initialized with pretrained weights and using a small training data set from the target distribution. In the asymptotic regime of large models, we provide an exact and rigorous analysis and relate the generalization errors (in regression) and classification errors (in binary classification) for the pretrained and fine-tuned models. In particular, we give conditions under which the fine-tuned model outperforms the pretrained one. An important aspect of our work is that all the results are "universal", in the sense that they depend only on the first and second order statistics of the target distribution. They thus extend well beyond the standard Gaussian assumptions commonly made in the literature. Furthermore, our universality results extend beyond standard SGD training to the test error of a classification task trained using a ridge regression. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.05602</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.05602</id><created>2024-10-07</created><updated>2025-02-24</updated><authors><author><keyname>Park</keyname><forenames>Byoungwoo</forenames></author><author><keyname>Lee</keyname><forenames>Hyungi</forenames></author><author><keyname>Lee</keyname><forenames>Juho</forenames></author></authors><title>Amortized Control of Continuous State Space Feynman-Kac Model for   Irregular Time Series</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Many real-world datasets, such as healthcare, climate, and economics, are often collected as irregular time series, which poses challenges for accurate modeling. In this paper, we propose the Amortized Control of continuous State Space Model (ACSSM) for continuous dynamical modeling of time series for irregular and discrete observations. We first present a multi-marginal Doob's $h$-transform to construct a continuous dynamical system conditioned on these irregular observations. Following this, we introduce a variational inference algorithm with a tight evidence lower bound (ELBO), leveraging stochastic optimal control (SOC) theory to approximate the intractable Doob's $h$-transform and simulate the conditioned dynamics. To improve efficiency and scalability during both training and inference, ACSSM employs amortized inference to decouple representation learning from the latent dynamics. Additionally, it incorporates a simulation-free latent dynamics framework and a transformer-based data assimilation scheme, facilitating parallel inference of the latent states and ELBO computation. Through empirical evaluations across a variety of real-world datasets, ACSSM demonstrates superior performance in tasks such as classification, regression, interpolation, and extrapolation, while maintaining computational efficiency. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.05898</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.05898</id><created>2024-10-08</created><updated>2025-02-25</updated><authors><author><keyname>Ventura</keyname><forenames>Enrico</forenames></author><author><keyname>Achilli</keyname><forenames>Beatrice</forenames></author><author><keyname>Silvestri</keyname><forenames>Gianluigi</forenames></author><author><keyname>Lucibello</keyname><forenames>Carlo</forenames></author><author><keyname>Ambrogioni</keyname><forenames>Luca</forenames></author></authors><title>Manifolds, Random Matrices and Spectral Gaps: The geometric phases of   generative diffusion</title><categories>stat.ML cs.LG</categories><comments>22 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the latent geometry of generative diffusion models under the manifold hypothesis. For this purpose, we analyze the spectrum of eigenvalues (and singular values) of the Jacobian of the score function, whose discontinuities (gaps) reveal the presence and dimensionality of distinct sub-manifolds. Using a statistical physics approach, we derive the spectral distributions and formulas for the spectral gaps under several distributional assumptions, and we compare these theoretical predictions with the spectra estimated from trained networks. Our analysis reveals the existence of three distinct qualitative phases during the generative process: a trivial phase; a manifold coverage phase where the diffusion process fits the distribution internal to the manifold; a consolidation phase where the score becomes orthogonal to the manifold and all particles are projected on the support of the data. This `division of labor' between different timescales provides an elegant explanation of why generative diffusion models are not affected by the manifold overfitting phenomenon that plagues likelihood-based models, since the internal distribution and the manifold geometry are produced at different time points during generation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.11399</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.11399</id><created>2024-10-15</created><updated>2025-02-24</updated><authors><author><keyname>Lin</keyname><forenames>Hanti</forenames></author></authors><title>Convergence to the Truth</title><categories>stat.OT cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This article reviews and develops an epistemological tradition in the philosophy of science, known as convergentism, which holds that inference methods should be assessed based on their ability to converge to the truth across a range of possible scenarios. Emphasis is placed on its historical origins in the work of C. S. Peirce and its recent developments in formal epistemology and data science (including statistics and machine learning). Comparisons are made with three other traditions: (1) explanationism, which holds that theory choice should be guided by a theory's overall balance of explanatory virtues, such as simplicity and fit with data; (2) instrumentalism, which maintains that scientific inference should be driven by the goal of obtaining useful models rather than true theories; and (3) Bayesianism, which shifts the focus from all-or-nothing beliefs to degrees of belief. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.14759</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.14759</id><created>2024-10-18</created><updated>2025-02-24</updated><authors><author><keyname>Neufeld</keyname><forenames>Ariel</forenames></author><author><keyname>Schmocker</keyname><forenames>Philipp</forenames></author></authors><title>Universal approximation results for neural networks with non-polynomial   activation function over non-compact domains</title><categories>stat.ML cs.LG cs.NE math.CA</categories><comments>arXiv admin note: text overlap with arXiv:2312.08410</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper extends the universal approximation property of single-hidden-layer feedforward neural networks beyond compact domains, which is of particular interest for the approximation within weighted $C^k$-spaces and weighted Sobolev spaces over unbounded domains. More precisely, by assuming that the activation function is non-polynomial, we establish universal approximation results within function spaces defined over non-compact subsets of a Euclidean space, including $L^p$-spaces, weighted $C^k$-spaces, and weighted Sobolev spaces, where the latter two include the approximation of the (weak) derivatives. Moreover, we provide some dimension-independent rates for approximating a function with sufficiently regular and integrable Fourier transform by neural networks with non-polynomial activation function. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.14949</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.14949</id><created>2024-10-18</created><updated>2025-02-22</updated><authors><author><keyname>Roy</keyname><forenames>Saptarshi</forenames></author><author><keyname>Bansal</keyname><forenames>Vansh</forenames></author><author><keyname>Sarkar</keyname><forenames>Purnamrita</forenames></author><author><keyname>Rinaldo</keyname><forenames>Alessandro</forenames></author></authors><title>2-Rectifications are Enough for Straight Flows: A Theoretical Insight   into Wasserstein Convergence</title><categories>cs.LG stat.ML</categories><comments>29 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diffusion models have emerged as a powerful tool for image generation and denoising. Typically, generative models learn a trajectory between the starting noise distribution and the target data distribution. Recently Liu et al. (2023b) designed a novel alternative generative model Rectified Flow (RF), which aims to learn straight flow trajectories from noise to data using a sequence of convex optimization problems with close ties to optimal transport. If the trajectory is curved, one must use many Euler discretization steps or novel strategies, such as exponential integrators, to achieve a satisfactory generation quality. In contrast, RF has been shown to theoretically straighten the trajectory through successive rectifications, reducing the number of function evaluations (NFEs) while sampling. It has also been shown empirically that RF may improve the straightness in two rectifications if one can solve the underlying optimization problem within a sufficiently small error. In this paper, we make two key theoretical contributions: 1) we provide the first theoretical analysis of the Wasserstein distance between the sampling distribution of RF and the target distribution. Our error rate is characterized by the number of discretization steps and a \textit{new formulation of straightness} stronger than that in the original work. 2) under a mild regularity assumption, we show that for a rectified flow from a Gaussian to any general target distribution with finite first moment (e.g. mixture of Gaussians), two rectifications are sufficient to achieve a straight flow, which is in line with the previous empirical findings. Additionally, we also present empirical results on both simulated and real datasets to validate our theoretical findings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.15824</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.15824</id><created>2024-10-21</created><updated>2025-02-22</updated><authors><author><keyname>Majumder</keyname><forenames>Abhishek Pal</forenames></author></authors><title>Long time behavior of semi-Markov modulated perpetuity and some related   processes</title><categories>math.PR math.DS q-fin.MF stat.CO</categories><comments>Any comments are welcome at palmabhishek@gmail.com</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Examples of stochastic processes whose state space representations involve functions of an integral type structure $$I_{t}^{(a,b)}:=\int_{0}^{t}b(Y_{s})e^{-\int_{s}^{t}a(Y_{r})dr}ds, \quad t\ge 0$$ are studied under an ergodic semi-Markovian environment described by an $S$ valued jump type process $Y:=(Y_{s}:s\in\mathbb{R}^{+})$ that is ergodic with a limiting distribution $\pi\in\mathcal{P}(S)$. Under different assumptions on signs of $E_{\pi}a(\cdot):=\sum_{j\in S}\pi_{j}a(j)$ and tail properties of the sojourn times of $Y$ we obtain different long time limit results for $I^{(a,b)}_{}:=(I^{(a,b)}_{t}:t\ge 0).$ In all cases mixture type of laws emerge which are naturally represented through an affine stochastic recurrence equation (SRE) $X\stackrel{d}{=}AX+B,\,\, X\perp\!\!\!\perp (A, B)$. Examples include explicit long-time representations of pitchfork bifurcation, and regime-switching diffusions under semi-Markov modulated environments, etc. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.18076</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.18076</id><created>2024-10-23</created><updated>2025-02-23</updated><authors><author><keyname>Wilcoxson</keyname><forenames>Max</forenames></author><author><keyname>Li</keyname><forenames>Qiyang</forenames></author><author><keyname>Frans</keyname><forenames>Kevin</forenames></author><author><keyname>Levine</keyname><forenames>Sergey</forenames></author></authors><title>Leveraging Skills from Unlabeled Prior Data for Efficient Online   Exploration</title><categories>cs.LG cs.AI stat.ML</categories><comments>27 pages, 19 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsupervised pretraining has been transformative in many supervised domains. However, applying such ideas to reinforcement learning (RL) presents a unique challenge in that fine-tuning does not involve mimicking task-specific data, but rather exploring and locating the solution through iterative self-improvement. In this work, we study how unlabeled offline trajectory data can be leveraged to learn efficient exploration strategies. While prior data can be used to pretrain a set of low-level skills, or as additional off-policy data for online RL, it has been unclear how to combine these ideas effectively for online exploration. Our method SUPE (Skills from Unlabeled Prior data for Exploration) demonstrates that a careful combination of these ideas compounds their benefits. Our method first extracts low-level skills using a variational autoencoder (VAE), and then pseudo-labels unlabeled trajectories with optimistic rewards and high-level action labels, transforming prior data into high-level, task-relevant examples that encourage novelty-seeking behavior. Finally, SUPE uses these transformed examples as additional off-policy data for online RL to learn a high-level policy that composes pretrained low-level skills to explore efficiently. In our experiments, SUPE consistently outperforms prior strategies across a suite of 42 long-horizon, sparse-reward tasks. Code: https://github.com/rail-berkeley/supe. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.18148</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.18148</id><created>2024-10-22</created><updated>2025-02-21</updated><authors><author><keyname>Somasekharan</keyname><forenames>Nithin</forenames></author><author><keyname>Pan</keyname><forenames>Shaowu</forenames></author></authors><title>Deep Autoencoder with SVD-Like Convergence and Flat Minima</title><categories>cs.LG cs.AI physics.comp-ph stat.ML</categories><comments>21 pages</comments><msc-class>68T07, 76F99</msc-class><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Representation learning for high-dimensional, complex physical systems aims to identify a low-dimensional intrinsic latent space, which is crucial for reduced-order modeling and modal analysis. To overcome the well-known Kolmogorov barrier, deep autoencoders (AEs) have been introduced in recent years, but they often suffer from poor convergence behavior as the rank of the latent space increases. To address this issue, we propose the learnable weighted hybrid autoencoder, a hybrid approach that combines the strengths of singular value decomposition (SVD) with deep autoencoders through a learnable weighted framework. We find that the introduction of learnable weighting parameters is essential -- without them, the resulting model would either collapse into a standard POD or fail to exhibit the desired convergence behavior. Interestingly, we empirically find that our trained model has a sharpness thousands of times smaller compared to other models. Our experiments on classical chaotic PDE systems, including the 1D Kuramoto-Sivashinsky and forced isotropic turbulence datasets, demonstrate that our approach significantly improves generalization performance compared to several competing methods. Additionally, when combining with time series modeling techniques (e.g., Koopman operator, LSTM), the proposed technique offers significant improvements for surrogate modeling of high-dimensional multi-scale PDE systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.19449</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.19449</id><created>2024-10-25</created><updated>2025-02-25</updated><authors><author><keyname>Noble</keyname><forenames>Maxence</forenames></author><author><keyname>Grenioux</keyname><forenames>Louis</forenames></author><author><keyname>Gabri√©</keyname><forenames>Marylou</forenames></author><author><keyname>Durmus</keyname><forenames>Alain Oliviero</forenames></author></authors><title>Learned Reference-based Diffusion Sampling for multi-modal distributions</title><categories>stat.ML cs.LG stat.CO</categories><comments>Accepted at ICLR 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Over the past few years, several approaches utilizing score-based diffusion have been proposed to sample from probability distributions, that is without having access to exact samples and relying solely on evaluations of unnormalized densities. The resulting samplers approximate the time-reversal of a noising diffusion process, bridging the target distribution to an easy-to-sample base distribution. In practice, the performance of these methods heavily depends on key hyperparameters that require ground truth samples to be accurately tuned. Our work aims to highlight and address this fundamental issue, focusing in particular on multi-modal distributions, which pose significant challenges for existing sampling methods. Building on existing approaches, we introduce Learned Reference-based Diffusion Sampler (LRDS), a methodology specifically designed to leverage prior knowledge on the location of the target modes in order to bypass the obstacle of hyperparameter tuning. LRDS proceeds in two steps by (i) learning a reference diffusion model on samples located in high-density space regions and tailored for multimodality, and (ii) using this reference model to foster the training of a diffusion-based sampler. We experimentally demonstrate that LRDS best exploits prior knowledge on the target distribution compared to competing algorithms on a variety of challenging distributions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.21107</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.21107</id><created>2024-10-28</created><updated>2025-02-24</updated><authors><author><keyname>Lin</keyname><forenames>Ya-Wei Eileen</forenames></author><author><keyname>Coifman</keyname><forenames>Ronald R.</forenames></author><author><keyname>Mishne</keyname><forenames>Gal</forenames></author><author><keyname>Talmon</keyname><forenames>Ronen</forenames></author></authors><title>Tree-Wasserstein Distance for High Dimensional Data with a Latent   Feature Hierarchy</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding meaningful distances between high-dimensional data samples is an important scientific task. To this end, we propose a new tree-Wasserstein distance (TWD) for high-dimensional data with two key aspects. First, our TWD is specifically designed for data with a latent feature hierarchy, i.e., the features lie in a hierarchical space, in contrast to the usual focus on embedding samples in hyperbolic space. Second, while the conventional use of TWD is to speed up the computation of the Wasserstein distance, we use its inherent tree as a means to learn the latent feature hierarchy. The key idea of our method is to embed the features into a multi-scale hyperbolic space using diffusion geometry and then present a new tree decoding method by establishing analogies between the hyperbolic embedding and trees. We show that our TWD computed based on data observations provably recovers the TWD defined with the latent feature hierarchy and that its computation is efficient and scalable. We showcase the usefulness of the proposed TWD in applications to word-document and single-cell RNA-sequencing datasets, demonstrating its advantages over existing TWDs and methods based on pre-trained models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.21869</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.21869</id><created>2024-10-29</created><updated>2025-02-25</updated><authors><author><keyname>Reizinger</keyname><forenames>Patrik</forenames></author><author><keyname>Bizeul</keyname><forenames>Alice</forenames></author><author><keyname>Juhos</keyname><forenames>Attila</forenames></author><author><keyname>Vogt</keyname><forenames>Julia E.</forenames></author><author><keyname>Balestriero</keyname><forenames>Randall</forenames></author><author><keyname>Brendel</keyname><forenames>Wieland</forenames></author><author><keyname>Klindt</keyname><forenames>David</forenames></author></authors><title>Cross-Entropy Is All You Need To Invert the Data Generating Process</title><categories>cs.LG cs.AI stat.ML</categories><comments>ICLR 2025 (oral) camera ready</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Supervised learning has become a cornerstone of modern machine learning, yet a comprehensive theory explaining its effectiveness remains elusive. Empirical phenomena, such as neural analogy-making and the linear representation hypothesis, suggest that supervised models can learn interpretable factors of variation in a linear fashion. Recent advances in self-supervised learning, particularly nonlinear Independent Component Analysis, have shown that these methods can recover latent structures by inverting the data generating process. We extend these identifiability results to parametric instance discrimination, then show how insights transfer to the ubiquitous setting of supervised learning with cross-entropy minimization. We prove that even in standard classification tasks, models learn representations of ground-truth factors of variation up to a linear transformation. We corroborate our theoretical contribution with a series of empirical studies. First, using simulated data matching our theoretical assumptions, we demonstrate successful disentanglement of latent factors. Second, we show that on DisLib, a widely-used disentanglement benchmark, simple classification tasks recover latent structures up to linear transformations. Finally, we reveal that models trained on ImageNet encode representations that permit linear decoding of proxy factors of variation. Together, our theoretical findings and experiments offer a compelling explanation for recent observations of linear representations, such as superposition in neural networks. This work takes a significant step toward a cohesive theory that accounts for the unreasonable effectiveness of supervised deep learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2410.22300</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2410.22300</id><created>2024-10-29</created><updated>2025-02-23</updated><authors><author><keyname>Wallin</keyname><forenames>Gabriel</forenames></author><author><keyname>Chen</keyname><forenames>Yunxiao</forenames></author><author><keyname>Lee</keyname><forenames>Yi-Hsuan</forenames></author><author><keyname>Li</keyname><forenames>Xiaoou</forenames></author></authors><title>A Latent Variable Model with Change-Points and Its Application to Time   Pressure Effects in Educational Assessment</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Educational assessments are valuable tools for measuring student knowledge and skills, but their validity can be compromised when test takers exhibit changes in response behavior due to factors such as time pressure. To address this issue, we introduce a novel latent factor model with change-points for item response data, designed to detect and account for individual-level shifts in response patterns during testing. This model extends traditional Item Response Theory (IRT) by incorporating person-specific change-points, which enables simultaneous estimation of item parameters, person latent traits, and the location of behavioral changes. We evaluate the proposed model through extensive simulation studies, which demonstrate its ability to accurately recover item parameters, change-point locations, and individual ability estimates under various conditions. Our findings show that accounting for change-points significantly reduces bias in ability estimates, particularly for respondents affected by time pressure. Application of the model to two real-world educational testing datasets reveals distinct patterns of change-point occurrence between high-stakes and lower-stakes tests, providing insights into how test-taking behavior evolves during the tests. This approach offers a more nuanced understanding of test-taking dynamics, with important implications for test design, scoring, and interpretation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.05391</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.05391</id><created>2024-11-08</created><updated>2025-02-24</updated><authors><author><keyname>Hoessly</keyname><forenames>Linard</forenames></author></authors><title>Impossibility results for equating the Youden Index with average scoring   rules and Tjur $R^2$-like metrics</title><categories>stat.OT math.PR math.ST stat.TH</categories><msc-class>62C99, 62H17, 62P10, , 62J12</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the Youden index fas well as measures evaluating predicted probabilities for the maximum-likelihood estimate of a logistic regression model with predictor the classifier. We give impossibility results showing that the Youden index can not equal any average of a real scoring rule nor any metric averaging over binary outcomes (0s and 1s) for any continuous real-valued scoring rule. This shows the obstructions of such potential equivalences and highlights the distinct roles these metrics play in diagnostic assessment. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.05869</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.05869</id><created>2024-11-07</created><updated>2025-02-24</updated><authors><author><keyname>Risser</keyname><forenames>Mark D.</forenames></author><author><keyname>Noack</keyname><forenames>Marcus M.</forenames></author><author><keyname>Luo</keyname><forenames>Hengrui</forenames></author><author><keyname>Pandolfi</keyname><forenames>Ronald</forenames></author></authors><title>Compactly-supported nonstationary kernels for computing exact Gaussian   processes on big data</title><categories>stat.ML cs.LG stat.AP stat.CO stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Gaussian process (GP) is a widely used probabilistic machine learning method with implicit uncertainty characterization for stochastic function approximation, stochastic modeling, and analyzing real-world measurements of nonlinear processes. Traditional implementations of GPs involve stationary kernels (also termed covariance functions) that limit their flexibility, and exact methods for inference that prevent application to data sets with more than about ten thousand points. Modern approaches to address stationarity assumptions generally fail to accommodate large data sets, while all attempts to address scalability focus on approximating the Gaussian likelihood, which can involve subjectivity and lead to inaccuracies. In this work, we explicitly derive an alternative kernel that can discover and encode both sparsity and nonstationarity. We embed the kernel within a fully Bayesian GP model and leverage high-performance computing resources to enable the analysis of massive data sets. We demonstrate the favorable performance of our novel kernel relative to existing exact and approximate GP methods across a variety of synthetic data examples. Furthermore, we conduct space-time prediction based on more than one million measurements of daily maximum temperature and verify that our results outperform state-of-the-art methods in the Earth sciences. More broadly, having access to exact GPs that use ultra-scalable, sparsity-discovering, nonstationary kernels allows GP methods to truly compete with a wide variety of machine learning methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.06688</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.06688</id><created>2024-11-10</created><updated>2025-02-24</updated><authors><author><keyname>Katsman</keyname><forenames>Isay</forenames></author><author><keyname>Gilbert</keyname><forenames>Anna</forenames></author></authors><title>Shedding Light on Problems with Hyperbolic Graph Learning</title><categories>cs.LG stat.ML</categories><comments>Published in TMLR</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent papers in the graph machine learning literature have introduced a number of approaches for hyperbolic representation learning. The asserted benefits are improved performance on a variety of graph tasks, node classification and link prediction included. Claims have also been made about the geometric suitability of particular hierarchical graph datasets to representation in hyperbolic space. Despite these claims, our work makes a surprising discovery: when simple Euclidean models with comparable numbers of parameters are properly trained in the same environment, in most cases, they perform as well, if not better, than all introduced hyperbolic graph representation learning models, even on graph datasets previously claimed to be the most hyperbolic as measured by Gromov $\delta$-hyperbolicity (i.e., perfect trees). This observation gives rise to a simple question: how can this be? We answer this question by taking a careful look at the field of hyperbolic graph representation learning as it stands today, and find that a number of results do not diligently present baselines, make faulty modelling assumptions when constructing algorithms, and use misleading metrics to quantify geometry of graph datasets. We take a closer look at each of these three problems, elucidate the issues, perform an analysis of methods, and introduce a parametric family of benchmark datasets to ascertain the applicability of (hyperbolic) graph neural networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.08547</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.08547</id><created>2024-11-13</created><updated>2024-12-27</updated><authors><author><keyname>Lin</keyname><forenames>Hanti</forenames></author></authors><title>Frequentist Statistics as Internalist Reliabilism</title><categories>stat.OT stat.ME</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  There has long been an impression that reliabilism implies externalism and that frequentist statistics, due to its reliabilist nature, is inherently externalist. I argue, however, that frequentist statistics can plausibly be understood as a form of internalist reliabilism -- internalist in the conventional sense, yet reliabilist in certain unconventional and intriguing ways. Crucially, in developing the thesis that reliabilism does not imply externalism, my aim is not to stretch the meaning of `reliabilism' merely to sever the implication. Instead, it is to gain a deeper understanding of frequentist statistics, which stands as one of the most sustained attempts by scientists to develop an epistemology for their own use. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.12127</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.12127</id><created>2024-11-18</created><updated>2025-02-24</updated><authors><author><keyname>Friedbaum</keyname><forenames>Jesse</forenames></author><author><keyname>Adiga</keyname><forenames>Sudarshan</forenames></author><author><keyname>Tandon</keyname><forenames>Ravi</forenames></author></authors><title>Fine-Grained Uncertainty Quantification via Collisions</title><categories>cs.LG cs.IT math.IT math.ST stat.ML stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a new and intuitive metric for aleatoric uncertainty quantification (UQ), the prevalence of class collisions defined as the same input being observed in different classes. We use the rate of class collisions to define the collision matrix, a novel and uniquely fine-grained measure of uncertainty. For a classification problem involving $K$ classes, the $K\times K$ collision matrix $S$ measures the inherent difficulty in distinguishing between each pair of classes. We discuss several applications of the collision matrix, establish its fundamental mathematical properties, as well as show its relationship with existing UQ methods, including the Bayes error rate (BER). We also address the new problem of estimating the collision matrix using one-hot labeled data by proposing a series of innovative techniques to estimate $S$. First, we learn a pair-wise contrastive model which accepts two inputs and determines if they belong to the same class. We then show that this contrastive model (which is PAC learnable) can be used to estimate the Gramian matrix of $S$, defined as $G=S^TS$. Finally, we show that under reasonable assumptions, $G$ can be used to uniquely recover $S$, a new result on non-negative matrices which could be of independent interest. With a method to estimate $S$ established, we demonstrate how this estimate of $S$, in conjunction with the contrastive model, can be used to estimate the posterior class portability distribution of any point. Experimental results are also presented to validate our methods of estimating the collision matrix and class posterior distributions on several datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.13443</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.13443</id><created>2024-11-20</created><updated>2025-02-23</updated><authors><author><keyname>Ding</keyname><forenames>Zhao</forenames></author><author><keyname>Duan</keyname><forenames>Chenguang</forenames></author><author><keyname>Jiao</keyname><forenames>Yuling</forenames></author><author><keyname>Yang</keyname><forenames>Jerry Zhijian</forenames></author><author><keyname>Yuan</keyname><forenames>Cheng</forenames></author><author><keyname>Zhang</keyname><forenames>Pingwen</forenames></author></authors><title>Nonlinear Assimilation via Score-based Sequential Langevin Sampling</title><categories>math.NA cs.NA math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents score-based sequential Langevin sampling (SSLS), a novel approach to nonlinear data assimilation within a recursive Bayesian framework. The proposed method decomposes the assimilation process into alternating prediction and update steps, leveraging dynamic models for state prediction while incorporating observational data through score-based Langevin Monte Carlo during updates. To address challenges in posterior sampling, we introduce an annealing strategy within the update mechanism. We provide theoretical guarantees for SSLS convergence in total variation (TV) distance under certain conditions, providing insights into error behavior with respect to key hyper-parameters. Our numerical experiments across challenging scenarios -- including high-dimensional systems, strong nonlinearity, and sparse observations -- demonstrate the robust performance of the proposed method. Furthermore, SSLS effectively quantifies the uncertainty associated with the estimated states, making it particularly valuable for the error calibration. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2411.15819</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2411.15819</id><created>2024-11-24</created><updated>2025-02-22</updated><authors><author><keyname>Hu</keyname><forenames>Yifan</forenames></author><author><keyname>Hou</keyname><forenames>Yanxi</forenames></author></authors><title>Bootstrap-based Inference for Bivariate Heteroscedastic Extremes with a   Changing Tail Copula</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a copula-based model for independent but non-identically distributed data with heteroscedastic extremes marginal and changing tail dependence structures. We establish a unified framework for inference by proving the weak convergence of the bivariate sequential tail empirical process and its empirical bootstrap counterpart. We derive the asymptotic properties of several estimators on the tail, including the quasi-tail copula, integrated scedasis function, and Hill estimator, treating them as functionals of the bivariate sequential tail empirical process. This process-centric approach enables the development of bootstrap-based methods and ensures the theoretical validity of the derived statistics. As an application of our inference method, we propose bootstrap-based tests for the equivalence of extreme value indices, the equivalence of scedasis functions, and non-changing tail dependence when marginal scedasis functions are identical. Our simulations validate the robustness and efficiency of the bootstrap-based tests. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.01344</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.01344</id><created>2024-12-02</created><updated>2025-02-24</updated><authors><author><keyname>Chen</keyname><forenames>Qianyi</forenames></author><author><keyname>Chen</keyname><forenames>Ying</forenames></author><author><keyname>Li</keyname><forenames>Bo</forenames></author></authors><title>Practical Performative Policy Learning with Strategic Agents</title><categories>cs.LG cs.GT stat.ME stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the performative policy learning problem, where agents adjust their features in response to a released policy to improve their potential outcomes, inducing an endogenous distribution shift. There has been growing interest in training machine learning models in strategic environments, including strategic classification and performative prediction. However, existing approaches often rely on restrictive parametric assumptions: micro-level utility models in strategic classification and macro-level data distribution maps in performative prediction, severely limiting scalability and generalizability. We approach this problem as a complex causal inference task, relaxing parametric assumptions on both micro-level agent behavior and macro-level data distribution. Leveraging bounded rationality, we uncover a practical low-dimensional structure in distribution shifts and construct an effective mediator in the causal path from the deployed model to the shifted data. We then propose a gradient-based policy optimization algorithm with a differentiable classifier as a substitute for the high-dimensional distribution map. Our algorithm efficiently utilizes batch feedback and limited manipulation patterns. Our approach achieves high sample efficiency compared to methods reliant on bandit feedback or zero-order optimization. We also provide theoretical guarantees for algorithmic convergence. Extensive and challenging experiments on high-dimensional settings demonstrate our method's practical efficacy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.02969</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.02969</id><created>2024-12-03</created><authors><author><keyname>Lin</keyname><forenames>Hanti</forenames></author></authors><title>Unified Inductive Logic: From Formal Learning to Statistical Inference   to Supervised Learning</title><categories>stat.OT cs.LG stat.ME</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  While the traditional conception of inductive logic is Carnapian, I develop a Peircean alternative and use it to unify formal learning theory, statistics, and a significant part of machine learning: supervised learning. Some crucial standards for evaluating non-deductive inferences have been assumed separately in those areas, but can actually be justified by a unifying principle. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.05481</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.05481</id><created>2024-12-06</created><updated>2025-02-24</updated><authors><author><keyname>Wang</keyname><forenames>Benjie</forenames></author><author><keyname>Mau√°</keyname><forenames>Denis Deratani</forenames></author><author><keyname>Broeck</keyname><forenames>Guy Van den</forenames></author><author><keyname>Choi</keyname><forenames>YooJung</forenames></author></authors><title>A Compositional Atlas for Algebraic Circuits</title><categories>cs.AI cs.LG cs.LO stat.ML</categories><comments>NeurIPS 2024</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Circuits based on sum-product structure have become a ubiquitous representation to compactly encode knowledge, from Boolean functions to probability distributions. By imposing constraints on the structure of such circuits, certain inference queries become tractable, such as model counting and most probable configuration. Recent works have explored analyzing probabilistic and causal inference queries as compositions of basic operators to derive tractability conditions. In this paper, we take an algebraic perspective for compositional inference, and show that a large class of queries - including marginal MAP, probabilistic answer set programming inference, and causal backdoor adjustment - correspond to a combination of basic operators over semirings: aggregation, product, and elementwise mapping. Using this framework, we uncover simple and general sufficient conditions for tractable composition of these operators, in terms of circuit properties (e.g., marginal determinism, compatibility) and conditions on the elementwise mappings. Applying our analysis, we derive novel tractability conditions for many such compositional queries. Our results unify tractability conditions for existing problems on circuits, while providing a blueprint for analysing novel compositional inference queries. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.05673</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.05673</id><created>2024-12-07</created><updated>2025-02-21</updated><authors><author><keyname>Chakraborty</keyname><forenames>Saptarshi</forenames></author><author><keyname>Khare</keyname><forenames>Kshitij</forenames></author><author><keyname>Michailidis</keyname><forenames>George</forenames></author></authors><title>A generalized Bayesian approach for high-dimensional robust regression   with serially correlated errors and predictors</title><categories>stat.ME math.ST stat.CO stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper introduces a loss-based generalized Bayesian methodology for high-dimensional robust regression with serially correlated errors and predictors. The proposed framework employs a novel scaled pseudo-Huber (SPH) loss function, which smooths the well-known Huber loss, effectively balancing quadratic ($\ell_2$) and absolute linear ($\ell_1$) loss behaviors. This flexibility enables the framework to accommodate both thin-tailed and heavy-tailed data efficiently. The generalized Bayesian approach constructs a working likelihood based on the SPH loss, facilitating efficient and stable estimation while providing rigorous uncertainty quantification for all model parameters. Notably, this approach allows formal statistical inference without requiring ad hoc tuning parameter selection while adaptively addressing a wide range of tail behavior in the errors. By specifying appropriate prior distributions for the regression coefficients--such as ridge priors for small or moderate-dimensional settings and spike-and-slab priors for high-dimensional settings--the framework ensures principled inference. We establish rigorous theoretical guarantees for accurate parameter estimation and correct predictor selection under sparsity assumptions for a wide range of data generating setups. Extensive simulation studies demonstrate the superior performance of our approach compared to traditional Bayesian regression methods based on $\ell_2$ and $\ell_1$-loss functions. The results highlight its flexibility and robustness, particularly in challenging high-dimensional settings characterized by data contamination. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.06381</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.06381</id><created>2024-12-09</created><updated>2025-02-24</updated><authors><author><keyname>Than</keyname><forenames>Khoat</forenames></author><author><keyname>Phan</keyname><forenames>Dat</forenames></author><author><keyname>Vu</keyname><forenames>Giang</forenames></author></authors><title>Gentle Local Robustness implies Generalization</title><categories>cs.LG stat.ML</categories><comments>To appear in Machine Learning journal</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Robustness and generalization ability of machine learning models are of utmost importance in various application domains. There is a wide interest in efficient ways to analyze those properties. One important direction is to analyze connection between those two properties. Prior theories suggest that a robust learning algorithm can produce trained models with a high generalization ability. However, we show in this work that the existing error bounds are vacuous for the Bayes optimal classifier which is the best among all measurable classifiers for a classification problem with overlapping classes. Those bounds cannot converge to the true error of this ideal classifier. This is undesirable, surprizing, and never known before. We then present a class of novel bounds, which are model-dependent and provably tighter than the existing robustness-based ones. Unlike prior ones, our bounds are guaranteed to converge to the true error of the best classifier, as the number of samples increases. We further provide an extensive experiment and find that two of our bounds are often non-vacuous for a large class of deep neural networks, pretrained from ImageNet. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.08951</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.08951</id><created>2024-12-12</created><updated>2025-02-21</updated><authors><author><keyname>Lim</keyname><forenames>Kart-Leong</forenames></author><author><keyname>Jiang</keyname><forenames>Xudong</forenames></author></authors><title>Posterior Approximation using Stochastic Gradient Ascent with Adaptive   Stepsize</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scalable algorithms of posterior approximation allow Bayesian nonparametrics such as Dirichlet process mixture to scale up to larger dataset at fractional cost. Recent algorithms, notably the stochastic variational inference performs local learning from minibatch. The main problem with stochastic variational inference is that it relies on closed form solution. Stochastic gradient ascent is a modern approach to machine learning and is widely deployed in the training of deep neural networks. In this work, we explore using stochastic gradient ascent as a fast algorithm for the posterior approximation of Dirichlet process mixture. However, stochastic gradient ascent alone is not optimal for learning. In order to achieve both speed and performance, we turn our focus to stepsize optimization in stochastic gradient ascent. As as intermediate approach, we first optimize stepsize using the momentum method. Finally, we introduce Fisher information to allow adaptive stepsize in our posterior approximation. In the experiments, we justify that our approach using stochastic gradient ascent do not sacrifice performance for speed when compared to closed form coordinate ascent learning on these datasets. Lastly, our approach is also compatible with deep ConvNet features as well as scalable to large class datasets such as Caltech256 and SUN397. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.09166</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.09166</id><created>2024-12-12</created><updated>2025-02-25</updated><authors><author><keyname>Bailey</keyname><forenames>R. A.</forenames></author><author><keyname>Haines</keyname><forenames>L. M.</forenames></author></authors><title>The square array design</title><categories>stat.ME</categories><comments>To be submitted</comments><msc-class>62K10</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper is about the construction of augmented row-column designs for unreplicated trials. The method uses the representation of a $k \times t$ equireplicate incomplete-block design with $t$ treatments in $t$ blocks of size $k$, termed an auxiliary block design, as a $t \times t$ square array design with $k$ controls, where $k&lt;t$. This can be regarded as an extension of the representation of a Youden square as a partial latin square for unreplicated trials. Properties of the designs, in particular in relation to connectedness and randomization, are explored. Particular attention is given to square array designs which minimize the average variances of the estimates of paired comparisons between test lines and controls and between test-line and test-line effects. The use of equireplicate cyclic designs as auxiliary block designs is highlighted. These provide a flexible and workable family of augmented row-column square array designs. Designs whose auxiliary block designs are not cyclic are also covered. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.11692</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.11692</id><created>2024-12-16</created><updated>2025-02-25</updated><authors><author><keyname>Ma</keyname><forenames>Li</forenames></author><author><keyname>Bruni</keyname><forenames>Benedetta</forenames></author></authors><title>A partial likelihood approach to tree-based density modeling and its   application in Bayesian inference</title><categories>stat.ME math.ST stat.CO stat.ML stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Tree-based priors for probability distributions are usually specified using a predetermined, data-independent collection of candidate recursive partitions of the sample space. To characterize an unknown target density in detail over the entire sample space, candidate partitions must have the capacity to expand deeply into all areas of the sample space with potential non-zero sampling probability. Such an expansive system of partitions often incurs prohibitive computational costs and makes inference prone to overfitting, especially in regions with little probability mass. Thus, existing models typically make a compromise and rely on relatively shallow trees. This hampers one of the most desirable features of trees, their ability to characterize local features, and results in reduced statistical efficiency. Traditional wisdom suggests that this compromise is inevitable to ensure coherent likelihood-based reasoning in Bayesian inference, as a data-dependent partition system that allows deeper expansion only in regions with more observations would induce double dipping of the data. We propose a simple strategy to restore coherency while allowing the candidate partitions to be data-dependent, using Cox's partial likelihood. Our partial likelihood approach is broadly applicable to existing likelihood-based methods and, in particular, to Bayesian inference on tree-based models. We give examples in density estimation in which the partial likelihood is endowed with existing priors on tree-based models and compare with the standard, full-likelihood approach. The results show substantial gains in estimation accuracy and computational efficiency from adopting the partial likelihood. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.15076</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.15076</id><created>2024-12-19</created><updated>2025-02-22</updated><authors><author><keyname>Konigorski</keyname><forenames>Stefan</forenames></author><author><keyname>Ried-Larsen</keyname><forenames>Mathias</forenames></author><author><keyname>Schmid</keyname><forenames>Christopher H</forenames></author></authors><title>Digital N-of-1 Trials and their Application in Experimental Physiology</title><categories>stat.AP</categories><comments>Accepted in Experimental Physiology</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Traditionally, studies in experimental physiology have been conducted in small groups of human participants, animal models or cell lines. Identifying optimal study designs that achieve sufficient power for drawing proper statistical inferences to detect group level effects with small sample sizes has been challenging. Moreover, average effects derived from traditional group-level inference do not necessarily apply to individual participants. Here, we introduce N-of-1 trials as an innovative study design that can be used to draw valid statistical inference about the effects of interventions on individual participants and can be aggregated across multiple study participants to provide population-level inferences more efficiently than standard group randomized trials. In this manuscript, we introduce the key components and design features of N-of-1 trials, describe statistical analysis and interpretations of the results, and describe some available digital tools to facilitate their use using examples from experimental physiology. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.17471</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.17471</id><created>2024-12-23</created><updated>2025-02-21</updated><authors><author><keyname>Sun</keyname><forenames>Ao</forenames></author><author><keyname>Li</keyname><forenames>Yanting</forenames></author><author><keyname>Zhou</keyname><forenames>Xiao-Hua</forenames></author></authors><title>Biomarker combination based on the Youden index with and without gold   standard</title><categories>stat.ME stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In clinical practice, multiple biomarkers are often measured on the same subject for disease diagnosis, and combining them can improve diagnostic accuracy. Existing studies typically combine multiple biomarkers by maximizing the Area Under the ROC Curve (AUC), assuming a gold standard exists or that biomarkers follow a multivariate normal distribution. However, practical diagnostic settings require both optimal combination coefficients and an effective cutoff value, and the reference test may be imperfect. In this paper, we propose a two-stage method for identifying the optimal linear combination and cutoff value based on the Youden index. First, it maximizes an approximation of the empirical AUC to estimate the optimal linear coefficients for combining multiple biomarkers. Then, it maximizes the empirical Youden index to determine the optimal cutoff point for disease classification. Under the semiparametric single index model and regularity conditions, the estimators for the linear coefficients, cutoff point, and Youden index are consistent. This method is also applicable when the reference standard is imperfect. We demonstrate the performance of our method through simulations and apply it to construct a diagnostic scale for Chinese medicine. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2412.19529</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2412.19529</id><created>2024-12-27</created><updated>2025-02-21</updated><authors><author><keyname>Liu</keyname><forenames>Zijian</forenames></author><author><keyname>Zhou</keyname><forenames>Zhengyuan</forenames></author></authors><title>Nonconvex Stochastic Optimization under Heavy-Tailed Noises: Optimal   Convergence without Gradient Clipping</title><categories>math.OC cs.LG stat.ML</categories><comments>ICLR 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the study of heavy-tailed noises in first-order nonconvex stochastic optimization has gotten a lot of attention since it was recognized as a more realistic condition as suggested by many empirical observations. Specifically, the stochastic noise (the difference between the stochastic and true gradient) is considered to have only a finite $\mathfrak{p}$-th moment where $\mathfrak{p}\in\left(1,2\right]$ instead of assuming it always satisfies the classical finite variance assumption. To deal with this more challenging setting, people have proposed different algorithms and proved them to converge at an optimal $\mathcal{O}(T^{\frac{1-\mathfrak{p}}{3\mathfrak{p}-2}})$ rate for smooth objectives after $T$ iterations. Notably, all these new-designed algorithms are based on the same technique - gradient clipping. Naturally, one may want to know whether the clipping method is a necessary ingredient and the only way to guarantee convergence under heavy-tailed noises. In this work, by revisiting the existing Batched Normalized Stochastic Gradient Descent with Momentum (Batched NSGDM) algorithm, we provide the first convergence result under heavy-tailed noises but without gradient clipping. Concretely, we prove that Batched NSGDM can achieve the optimal $\mathcal{O}(T^{\frac{1-\mathfrak{p}}{3\mathfrak{p}-2}})$ rate even under the relaxed smooth condition. More interestingly, we also establish the first $\mathcal{O}(T^{\frac{1-\mathfrak{p}}{2\mathfrak{p}}})$ convergence rate in the case where the tail index $\mathfrak{p}$ is unknown in advance, which is arguably the common scenario in practice. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.03999</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.03999</id><created>2025-01-07</created><updated>2025-02-24</updated><authors><author><keyname>Song</keyname><forenames>Haochen</forenames></author><author><keyname>Musabirov</keyname><forenames>Ilya</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Ananya</forenames></author><author><keyname>Durand</keyname><forenames>Audrey</forenames></author><author><keyname>Franklin</keyname><forenames>Meredith</forenames></author><author><keyname>Rafferty</keyname><forenames>Anna</forenames></author><author><keyname>Williams</keyname><forenames>Joseph Jay</forenames></author></authors><title>Adaptive Experiments Under High-Dimensional and Data Sparse Settings:   Applications for Educational Platforms</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In online educational platforms, adaptive experiment designs play a critical role in personalizing learning pathways, instructional sequencing, and content recommendations. Traditional adaptive policies, such as Thompson Sampling, struggle with scalability in high-dimensional and sparse settings such as when there are large amount of treatments (arms) and limited resources such as funding and time to conduct to a classroom constraint student size. Furthermore, the issue of under-exploration in large-scale educational interventions can lead to suboptimal learning recommendations. To address these challenges, we build upon the concept of lenient regret, which tolerates limited suboptimal selections to enhance exploratory learning, and propose a framework for determining the feasible number of treatments given a sample size. We illustrate these ideas with a case study in online educational learnersourcing examples, where adaptive algorithms dynamically allocate peer-crafted interventions to other students under active recall exercise. Our proposed Weighted Allocation Probability Adjusted Thompson Sampling (WAPTS) algorithm enhances the efficiency of treatment allocation by adjusting sampling weights to balance exploration and exploitation in data-sparse environments. We present comparative evaluations of WAPTS across various sample sizes (N=50, 300, 1000) and treatment conditions, demonstrating its ability to mitigate under-exploration while optimizing learning outcomes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.04562</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.04562</id><created>2025-01-08</created><updated>2025-02-22</updated><authors><author><keyname>Bombelli</keyname><forenames>Ilaria</forenames></author><author><keyname>Iezzi</keyname><forenames>Domenica Fioredistella</forenames></author><author><keyname>Seri</keyname><forenames>Emiliano</forenames></author><author><keyname>Vichi</keyname><forenames>Maurizio</forenames></author></authors><title>Spherical Double K-Means: a co-clustering approach for text data   analysis</title><categories>stat.ME stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In text analysis, Spherical K-means (SKM) is a specialized k-means clustering algorithm widely utilized for grouping documents represented in high-dimensional, sparse term-document matrices, often normalized using techniques like TF-IDF. Researchers frequently seek to cluster not only documents but also the terms associated with them into coherent groups. To address this dual clustering requirement, we introduce Spherical Double K-Means (SDKM), a novel methodology that simultaneously clusters documents and terms. This approach offers several advantages: first, by integrating the clustering of documents and terms, SDKM provides deeper insights into the relationships between content and vocabulary, enabling more effective topic identification and keyword extraction. Additionally, the two-level clustering assists in understanding both overarching themes and specific terminologies within document clusters, enhancing interpretability. SDKM effectively handles the high dimensionality and sparsity inherent in text data by utilizing cosine similarity, leading to improved computational efficiency. Moreover, the method captures dynamic changes in thematic content over time, making it well-suited for applications in rapidly evolving fields. Ultimately, SDKM presents a comprehensive framework for advancing text mining efforts, facilitating the uncovering of nuanced patterns and structures that are critical for robust data analysis. We apply SDKM to the corpus of US presidential inaugural addresses, spanning from George Washington in 1789 to Joe Biden in 2021. Our analysis reveals distinct clusters of words and documents that correspond to significant historical themes and periods, showcasing the method's ability to facilitate a deeper understanding of the data. Our findings demonstrate the efficacy of SDKM in uncovering underlying patterns in textual data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.06435</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.06435</id><created>2025-01-10</created><updated>2025-02-23</updated><authors><author><keyname>Soltanifar</keyname><forenames>Mohsen</forenames></author><author><keyname>Lee</keyname><forenames>Chel Hee</forenames></author></authors><title>CMHSU: An R Statistical Software Package to Detect Mental Health Status,   Substance Use Status, and their Concurrent Status in the North American   Healthcare Administrative Databases</title><categories>stat.ME stat.CO</categories><comments>20 pages; 7 figures; version 3.0</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The concept of concurrent mental health and substance use (MHSU) and its detection in patients has garnered growing interest among psychiatrists and healthcare policymakers over the past four decades. Researchers have proposed various diagnostic methods, including the Data-Driven Diagnostic Method (DDDM), for the identification of MHSU. However, the absence of a standalone statistical software package to facilitate DDDM for large healthcare administrative databases has remained a significant gap. This paper introduces the R statistical software package CMHSU, available on the Comprehensive R Archive Network (CRAN), for the diagnosis of mental health (MH), substance use (SU), and their concurrent status (MHSU). The package implements DDDM using hospital and medical service physician visit counts along with maximum time span parameters for MH, SU, and MHSU diagnoses. A working example using a simulated real-world dataset is presented to examine various analytical aspects, including three key dimensions of MHSU detection based on the DDDM framework, as well as temporal analysis to demonstrate the package's application for healthcare policymakers. Additionally, the limitations of the CMHSU package and potential directions for its future extension are discussed. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.07741</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.07741</id><created>2025-01-13</created><updated>2025-02-24</updated><authors><author><keyname>Ghane</keyname><forenames>Reza</forenames></author><author><keyname>Bao</keyname><forenames>Anthony</forenames></author><author><keyname>Akhtiamov</keyname><forenames>Danil</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Concentration of Measure for Distributions Generated via Diffusion   Models</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We show via a combination of mathematical arguments and empirical evidence that data distributions sampled from diffusion models satisfy a Concentration of Measure Property saying that any Lipschitz $1$-dimensional projection of a random vector is not too far from its mean with high probability. This implies that such models are quite restrictive and gives an explanation for a fact previously observed in the literature that conventional diffusion models cannot capture "heavy-tailed" data (i.e. data $\mathbf{x}$ for which the norm $\|\mathbf{x}\|_2$ does not possess a sub-Gaussian tail) well. We then proceed to train a generalized linear model using stochastic gradient descent (SGD) on the diffusion-generated data for a multiclass classification task and observe empirically that a Gaussian universality result holds for the test error.   In other words, the test error depends only on the first and second order statistics of the diffusion-generated data in the linear setting. Results of such forms are desirable because they allow one to assume the data itself is Gaussian for analyzing performance of the trained classifier. Finally, we note that current approaches to proving universality do not apply to this case as the covariance matrices of the data tend to have vanishing minimum singular values for the diffusion-generated data, while the current proofs assume that this is not the case (see Subsection 3.4 for more details). This leaves extending previous mathematical universality results as an intriguing open question. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.09345</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.09345</id><created>2025-01-16</created><updated>2025-02-22</updated><authors><author><keyname>Zellinger</keyname><forenames>Michael J.</forenames></author><author><keyname>Thomson</keyname><forenames>Matt</forenames></author></authors><title>Rational Tuning of LLM Cascades via Probabilistic Modeling</title><categories>cs.LG cs.AI stat.ML</categories><comments>16 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding the reliability of large language models (LLMs) has recently garnered significant attention. Given LLMs' propensity to hallucinate, as well as their high sensitivity to prompt design, it is already challenging to predict the performance of an individual LLM. However, the problem becomes more complex for compound LLM systems such as cascades, where in addition to each model's standalone performance, we must understand how the error rates of different models interact. In this paper, we present a probabilistic model for the joint performance distribution of a sequence of LLMs, which enables a framework for rationally tuning the confidence thresholds of a LLM cascade using continuous optimization. Compared to selecting confidence thresholds using grid search, our parametric Markov-copula model significantly improves runtime scaling with respect to the length of the cascade and the desired resolution of the cost-error curve, turning them from intractable into low-order polynomial. In addition, the optimal thresholds computed using our continuous optimization-based algorithm increasingly outperform those found via grid search as cascade length grows, improving the area under the cost-error curve by 1.9% on average for cascades consisting of at least three models. Overall, our Markov-copula model provides a rational basis for tuning LLM cascade performance and points to the potential of probabilistic methods in analyzing LLM systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.10942</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.10942</id><created>2025-01-18</created><updated>2025-02-24</updated><authors><author><keyname>Li</keyname><forenames>Dong</forenames></author><author><keyname>Qiao</keyname><forenames>Xinghao</forenames></author><author><keyname>Yu</keyname><forenames>Cheng</forenames></author></authors><title>Large covariance matrix estimation with factor-assisted variable   clustering</title><categories>stat.ME</categories><comments>We have corrected some inaccurate descriptions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the covariance matrix estimation for high-dimensional time series within a new framework that combines low-rank factor and latent variable-specific cluster structures. The popular methods based on assuming the sparse error covariance matrix after taking out common factors may be invalid for many financial applications. Our formulation postulates a latent model-based error cluster structure after removing observable factors, which not only leads to more interpretable cluster patterns but also accounts for non-sparse cross-sectional correlations among the variable-specific residuals. Our method begins with using least-squares to estimate the factor loadings, followed by identifying the latent cluster structure by thresholding the scaled covariance difference measures of residuals. A novel ratio-based criterion is introduced to determine the threshold parameter when performing the developed clustering algorithm. We then establish the cluster recovery consistency of our method and derive the convergence rates of our proposed covariance matrix estimators under different norms. Finally, we demonstrate the superior finite sample performance of our proposal over the competing methods through both extensive simulations and a real data application on minimum variance portfolio. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.11261</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.11261</id><created>2025-01-19</created><authors><author><keyname>Wunderlich</keyname><forenames>Adam</forenames></author><author><keyname>Sanders</keyname><forenames>Aric</forenames></author></authors><title>The Expected Peak-to-Average Power Ratio of White Gaussian Noise in   Sampled I/Q Data</title><categories>eess.SP stat.AP</categories><comments>To appear in IEEE Transactions on Instrumentation and Measurement. 8   pages, 6 figures</comments><journal-ref>IEEE Transactions on Instrumentation and Measurement, 2025</journal-ref><doi>10.1109/TIM.2025.3544288</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the fundamental endeavors in radio frequency (RF) metrology is to measure the power of signals, where a common aim is to estimate the peak-to-average power ratio (PAPR), which quantifies the ratio of the maximum (peak) to the mean value. For a finite number of discrete-time samples of baseband in-phase and quadrature (I/Q) white Gaussian noise (WGN) that are independent and identically distributed with zero mean, we derive a closed-form, exact formula for mean PAPR that is well-approximated by the natural logarithm of the number of samples plus Euler's constant. Additionally, we give related theoretical results for the mean crest factor. After comparing our main result to previously published approximate formulas, we examine how violations of the WGN assumptions in sampled I/Q data result in deviations from the expected value of PAPR. Finally, utilizing a measured RF I/Q acquisition, we illustrate how our formula for mean PAPR can be applied to spectral analysis with spectrograms to verify when measured RF emissions are WGN in a given frequency band. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.11760</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.11760</id><created>2025-01-20</created><updated>2025-02-22</updated><authors><author><keyname>He</keyname><forenames>Wan</forenames></author><author><keyname>Bolnick</keyname><forenames>Daniel I.</forenames></author><author><keyname>Scarpino</keyname><forenames>Samuel V.</forenames></author><author><keyname>Eliassi-Rad</keyname><forenames>Tina</forenames></author></authors><title>Hypergraph Representations of scRNA-seq Data for Improved Clustering   with Random Walks</title><categories>q-bio.QM q-bio.GN stat.AP stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Analysis of single-cell RNA sequencing data is often conducted through network projections such as coexpression networks, primarily due to the abundant availability of network analysis tools for downstream tasks. However, this approach has several limitations: loss of higher-order information, inefficient data representation caused by converting a sparse dataset to a fully connected network, and overestimation of coexpression due to zero-inflation. To address these limitations, we propose conceptualizing scRNA-seq expression data as hypergraphs, which are generalized graphs in which the hyperedges can connect more than two vertices. In the context of scRNA-seq data, the hypergraph nodes represent cells and the edges represent genes. Each hyperedge connects all cells where its corresponding gene is actively expressed and records the expression of the gene across different cells. This hypergraph conceptualization enables us to explore multi-way relationships beyond the pairwise interactions in coexpression networks without loss of information. We propose two novel clustering methods: (1) the Dual-Importance Preference Hypergraph Walk (DIPHW) and (2) the Coexpression and Memory-Integrated Dual-Importance Preference Hypergraph Walk (CoMem-DIPHW). They outperform established methods on both simulated and real scRNA-seq datasets. The improvement brought by our proposed methods is especially significant when data modularity is weak. Furthermore, CoMem-DIPHW incorporates the gene coexpression network, cell coexpression network, and the cell-gene expression hypergraph from the single-cell abundance counts data altogether for embedding computation. This approach accounts for both the local level information from single-cell level gene expression and the global level information from the pairwise similarity in the two coexpression networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.14805</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.14805</id><created>2025-01-14</created><updated>2025-02-24</updated><authors><author><keyname>J√∏rgensen</keyname><forenames>Bastian Schmidt</forenames></author><author><keyname>M√∏ller</keyname><forenames>Jan Kloppenborg</forenames></author><author><keyname>Nystrup</keyname><forenames>Peter</forenames></author><author><keyname>Madsen</keyname><forenames>Henrik</forenames></author></authors><title>Sequential Methods for Error Correction of Probabilistic Wind Power   Forecasts</title><categories>stat.ME stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Reliable probabilistic production forecasts are required to better manage the uncertainty that the rapid build-out of wind power capacity adds to future energy systems. In this article, we consider sequential methods to correct errors in wind power production forecast ensembles derived from numerical weather predictions. We propose combining neural networks with time-adaptive quantile regression to enhance the accuracy of wind power forecasts. We refer to this approach as Neural Adaptive Basis for (time-adaptive) Quantile Regression or NABQR. First, we use NABQR to correct power production ensembles with neural networks. We find that Long Short-Term Memory networks are the most effective architecture for this purpose. Second, we apply time-adaptive quantile regression to the corrected ensembles to obtain optimal median predictions along with quantiles of the forecast distribution. With the suggested method, we beat state-of-the-art methods and achieve accuracy improvements up to 40% in mean absolute terms in an application to day-ahead forecasting of on- and offshore wind power production in Denmark. In addition, we explore the value of our method for applications in energy trading. We have implemented the NABQR method as an open-source Python package to support applications in renewable energy forecasting and future research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.15526</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.15526</id><created>2025-01-26</created><updated>2025-02-24</updated><authors><author><keyname>Zhan</keyname><forenames>Tianyu</forenames></author><author><keyname>Kang</keyname><forenames>Jian</forenames></author></authors><title>A general, flexible and harmonious framework to construct interpretable   functions in regression analysis</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  An interpretable model or method has several appealing features, such as reliability to adversarial examples, transparency of decision-making, and communication facilitator. However, interpretability is a subjective concept, and even its definition can be diverse. The same model may be deemed as interpretable by a study team, but regarded as a black-box algorithm by another squad. Simplicity, accuracy and generalizability are some additional important aspects of evaluating interpretability. In this work, we present a general, flexible and harmonious framework to construct interpretable functions in regression analysis with a focus on continuous outcomes. We formulate a functional skeleton in light of users' expectations of interpretability. A new measure based on Mallows's $C_p$-statistic is proposed for model selection to balance approximation, generalizability, and interpretability. We apply this approach to derive a sample size formula in adaptive clinical trial designs to demonstrate the general workflow, and to explain operating characteristics in a Bayesian Go/No-Go paradigm to show the potential advantages of using meaningful intermediate variables. Generalization to categorical outcomes is illustrated in an example of hypothesis testing based on Fisher's exact test. A real data analysis of NHANES (National Health and Nutrition Examination Survey) is conducted to investigate relationships between some important laboratory measurements. We also discuss some extensions of this method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.15926</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.15926</id><created>2025-01-27</created><updated>2025-02-24</updated><authors><author><keyname>Mintsa</keyname><forenames>Eddy Michel Ella</forenames></author></authors><title>Rates of convergence of a binary classification procedure for   time-homogeneous S.D.E paths</title><categories>math.ST stat.TH</categories><comments>41 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of binary classification of trajectories generated by time-homogeneous stochastic differential equations, we consider a mixture model of two diffusion processes characterized by a stochastic differential equation whose drift coefficient depends on the class or label, which is modeled as a discrete random variable taking two possible values and whose diffusion coefficient is independent of the class. We assume that the drift and diffusion coefficients are unknown as well as the law of the discrete random variable that models the class. In this paper, we study the minimax convergence rate of the resulting nonparametric plug-in classifier under different sets of assumptions on the mixture model considered. As the plug-in classifier is based on nonparametric estimators of the coefficients of the mixture model, we also study a minimax convergence rate of the risk of estimation of the drift coefficients on the real line. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.15933</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.15933</id><created>2025-01-27</created><updated>2025-02-24</updated><authors><author><keyname>Mintsa</keyname><forenames>Eddy Michel Ella</forenames></author></authors><title>Minimax rates of convergence for the nonparametric estimation of the   diffusion coefficient from time-homogeneous SDE paths</title><categories>math.ST stat.TH</categories><comments>32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a diffusion process X, solution of a time-homogeneous stochastic differential equation. We assume that the diffusion process X is observed at discrete times, at high frequency, which means that the time step tends toward zero. In addition, the drift and diffusion coefficients of the process X are assumed to be unknown. In this paper, we study the minimax rates of convergence of the nonparametric estimators of the square of the diffusion coefficient. Two observation schemes are considered depending on the estimation interval. The square of the diffusion coefficient is estimated on the real line from repeated observations of the process X, where the number of diffusion paths tends to infinity. For the case of a compact estimation interval, we study the nonparametric estimation of the square of the diffusion coefficient constructed from a single diffusion path on one side and from repeated observations on the other side, where the number of trajectories tends to infinity. In each of these cases, we establish minimax convergence rates of the risk of estimation of the diffusion coefficient over a space of Holder functions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.18195</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.18195</id><created>2025-01-30</created><updated>2025-02-24</updated><authors><author><keyname>Biscio</keyname><forenames>Christophe A. N.</forenames></author><author><keyname>Mazoyer</keyname><forenames>Adrien</forenames></author><author><keyname>Vejling</keyname><forenames>Martin V.</forenames></author></authors><title>Conformal novelty detection for replicate point patterns with FDR or   FWER control</title><categories>stat.ME</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monte Carlo tests are widely used for computing valid p-values without requiring known distributions of test statistics. When performing multiple Monte Carlo tests, it is essential to maintain control of the type I error. Some techniques for multiplicity control pose requirements on the joint distribution of the p-values, for instance independence, which can be computationally intensive to achieve using na\"ive multiple Monte Carlo testing. We highlight in this work that multiple Monte Carlo testing is an instance of conformal novelty detection. Leveraging this insight enables a more efficient multiple Monte Carlo testing procedure, avoiding excessive simulations while still ensuring exact control over the false discovery rate or the family-wise error rate. We call this approach conformal multiple Monte Carlo testing. The performance is investigated in the context of global envelope tests for point pattern data through a simulation study and an application to a sweat gland data set. Results reveal that with a fixed number of simulations under the null hypothesis, our proposed method yields substantial improvements in power of the testing procedure as compared to the na\"ive multiple Monte Carlo testing procedure. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2501.19254</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2501.19254</id><created>2025-01-31</created><updated>2025-02-24</updated><authors><author><keyname>Liu</keyname><forenames>Xinyu</forenames></author><author><keyname>Xie</keyname><forenames>Zixuan</forenames></author><author><keyname>Zhang</keyname><forenames>Shangtong</forenames></author></authors><title>Linear $Q$-Learning Does Not Diverge in $L^2$: Convergence Rates to a   Bounded Set</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $Q$-learning is one of the most fundamental reinforcement learning algorithms. It is widely believed that $Q$-learning with linear function approximation (i.e., linear $Q$-learning) suffers from possible divergence until the recent work Meyn (2024) which establishes the ultimate almost sure boundedness of the iterates of linear $Q$-learning. Building on this success, this paper further establishes the first $L^2$ convergence rate of linear $Q$-learning iterates (to a bounded set). Similar to Meyn (2024), we do not make any modification to the original linear $Q$-learning algorithm, do not make any Bellman completeness assumption, and do not make any near-optimality assumption on the behavior policy. All we need is an $\epsilon$-softmax behavior policy with an adaptive temperature. The key to our analysis is the general result of stochastic approximations under Markovian noise with fast-changing transition functions. As a side product, we also use this general result to establish the $L^2$ convergence rate of tabular $Q$-learning with an $\epsilon$-softmax behavior policy, for which we rely on a novel pseudo-contraction property of the weighted Bellman optimality operator. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.00214</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.00214</id><created>2025-01-31</created><updated>2025-02-21</updated><authors><author><keyname>Donohue</keyname><forenames>Michael C.</forenames></author><author><keyname>Insel</keyname><forenames>Philip S.</forenames></author><author><keyname>Langford</keyname><forenames>Oliver</forenames></author></authors><title>A critical evaluation of longitudinal proportional effect models</title><categories>stat.ME</categories><comments>9 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Nonlinear longitudinal models for repeated continuous measures with proportional treatment effects have been proposed to improve power and provide direct estimates of the proportional treatment effect in randomized clinical trials. These models make a strong assumption about a fixed proportional treatment effect over time, which can lead to bias and Type I error inflation when the assumption is violated. Even when the proportional effect assumption holds, we demonstrate that these models are biased and their inference is sensitive to the labeling of treatment groups. Typically, this bias favors the active group, inflates Type I error, and can result in one-sided testing. Conversely, the bias can make it more difficult to detect treatment harm, creating a safety concern. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.02483</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.02483</id><created>2025-02-04</created><updated>2025-02-25</updated><authors><author><keyname>De Bortoli</keyname><forenames>Valentin</forenames></author><author><keyname>Galashov</keyname><forenames>Alexandre</forenames></author><author><keyname>Guntupalli</keyname><forenames>J. Swaroop</forenames></author><author><keyname>Zhou</keyname><forenames>Guangyao</forenames></author><author><keyname>Murphy</keyname><forenames>Kevin</forenames></author><author><keyname>Gretton</keyname><forenames>Arthur</forenames></author><author><keyname>Doucet</keyname><forenames>Arnaud</forenames></author></authors><title>Distributional Diffusion Models with Scoring Rules</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Diffusion models generate high-quality synthetic data. They operate by defining a continuous-time forward process which gradually adds Gaussian noise to data until fully corrupted. The corresponding reverse process progressively "denoises" a Gaussian sample into a sample from the data distribution. However, generating high-quality outputs requires many discretization steps to obtain a faithful approximation of the reverse process. This is expensive and has motivated the development of many acceleration methods. We propose to accomplish sample generation by learning the posterior {\em distribution} of clean data samples given their noisy versions, instead of only the mean of this distribution. This allows us to sample from the probability transitions of the reverse process on a coarse time scale, significantly accelerating inference with minimal degradation of the quality of the output. This is accomplished by replacing the standard regression loss used to estimate conditional means with a scoring rule. We validate our method on image and robot trajectory generation, where we consistently outperform standard diffusion models at few discretization steps. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.03479</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.03479</id><created>2025-01-27</created><updated>2025-02-22</updated><authors><author><keyname>Cuicizion</keyname><forenames>Eliuvish</forenames></author><author><keyname>Ri</keyname><forenames>Itsugo</forenames></author><author><keyname>Holmes</keyname><forenames>Elaine</forenames></author><author><keyname>Chern</keyname><forenames>Jawad</forenames></author></authors><title>A Tutorial on Markov Renewal and Semi-Markov Proportional Hazards Model</title><categories>stat.AP stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Transition probability estimation plays a critical role in multi-state modeling, especially in clinical research. This paper investigates the application of semi-Markov and Markov renewal frameworks to the EBMT dataset, focusing on six clinical states encountered during hematopoietic stem cell transplantation. By comparing Aalen-Johansen (AJ) and Dabrowska-Sun-Horowitz (DSH) estimators, we demonstrate that semi-Markov models, which incorporate sojourn times, provide a more nuanced and temporally sensitive depiction of patient trajectories compared to memoryless Markov models. The DSH estimator consistently yields smoother probability curves, particularly for transitions involving prolonged states. We use empirical process theory and Burkholder-Davis-Gundy inequality to show weak convergence of the estimator. Future work includes extending the framework to accommodate advanced covariate structures and non-Markovian dynamics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.04793</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.04793</id><created>2025-02-07</created><updated>2025-02-24</updated><authors><author><keyname>Jeunen</keyname><forenames>Olivier</forenames></author></authors><title>$t$-Testing the Waters: Empirically Validating Assumptions for Reliable   A/B-Testing</title><categories>stat.ME cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A/B-tests are a cornerstone of experimental design on the web, with wide-ranging applications and use-cases. The statistical $t$-test comparing differences in means is the most commonly used method for assessing treatment effects, often justified through the Central Limit Theorem (CLT). The CLT ascertains that, as the sample size grows, the sampling distribution of the Average Treatment Effect converges to normality, making the $t$-test valid for sufficiently large sample sizes. When outcome measures are skewed or non-normal, quantifying what "sufficiently large" entails is not straightforward.   To ensure that confidence intervals maintain proper coverage and that $p$-values accurately reflect the false positive rate, it is critical to validate this normality assumption. We propose a practical method to test this, by analysing repeatedly resampled A/A-tests. When the normality assumption holds, the resulting $p$-value distribution should be uniform, and this property can be tested using the Kolmogorov-Smirnov test. This provides an efficient and effective way to empirically assess whether the $t$-test's assumptions are met, and the A/B-test is valid. We demonstrate our methodology and highlight how it helps to identify scenarios prone to inflated Type-I errors. Our approach provides a practical framework to ensure and improve the reliability and robustness of A/B-testing practices. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.05380</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.05380</id><created>2025-02-07</created><updated>2025-02-24</updated><authors><author><keyname>Lotspeich</keyname><forenames>Sarah C.</forenames></author><author><keyname>Kedar</keyname><forenames>Sheetal</forenames></author><author><keyname>Tahir</keyname><forenames>Rabeya</forenames></author><author><keyname>Keleghan</keyname><forenames>Aidan D.</forenames></author><author><keyname>Miranda</keyname><forenames>Amelia</forenames></author><author><keyname>Duda</keyname><forenames>Stephany N.</forenames></author><author><keyname>Bancks</keyname><forenames>Michael P.</forenames></author><author><keyname>Wells</keyname><forenames>Brian J.</forenames></author><author><keyname>Khanna</keyname><forenames>Ashish K.</forenames></author><author><keyname>Rigdon</keyname><forenames>Joseph</forenames></author></authors><title>Overcoming data challenges to measure whole-person health in electronic   health records</title><categories>stat.ME stat.AP</categories><comments>16 pages, 2 tables, 6 figures, supplementary materials and code on   GitHub</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The allostatic load index (ALI) is a composite measure of whole-person health. Data from electronic health records (EHR) present a huge opportunity to operationalize the ALI in the learning health system, except they are prone to missingness and errors. Validation of EHR data (e.g., through chart reviews) can provide better-quality data, but realistically, only a subset of patients' data can be validated, and most protocols do not recover missing data. Using a representative sample of 1000 patients from the EHR at an extensive learning health system (100 of whom could be validated), we propose methods to design, conduct, and analyze statistically efficient and robust studies of the ALI and healthcare utilization. With semiparametric maximum likelihood estimation, we robustly incorporate all available data into statistical models. Using targeted design strategies, we examine ways to select the most informative patients for validation. Incorporating clinical expertise, we devise a novel validation protocol to promote the quality and completeness of EHR data. Validating the EHR data uncovered relatively low error rates and recovered some missing data. Through simulation studies based on preliminary data, residual sampling was identified as the most informative strategy for completing our validation study. Statistical models of partially validated data indicated higher odds of engaging in the healthcare system were associated with worse whole-person health (i.e., higher ALI), adjusting for age. Targeted validation with an enriched protocol allowed us to ensure the quality and promote the completeness of the EHR. Findings from our validation study were incorporated into analyses as we operationalize the ALI as a whole-person health measure intended to predict healthcare utilization in the academic learning health system. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.07655</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.07655</id><created>2025-02-11</created><updated>2025-02-24</updated><authors><author><keyname>Du</keyname><forenames>Kasy</forenames></author></authors><title>A Short Note of Comparison between Convex and Non-convex Penalized   Likelihood</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper compares convex and non-convex penalized likelihood methods in high-dimensional statistical modeling, focusing on their strengths and limitations. Convex penalties, like LASSO, offer computational efficiency and strong theoretical guarantees but often introduce bias in parameter estimation. Non-convex penalties, such as SCAD and MCP, reduce bias and achieve oracle properties but pose optimization challenges due to non-convexity. The paper highlights key differences in bias-variance trade-offs, computational complexity, and robustness, offering practical guidance for method selection. It concludes that the choice depends on the problem context, balancing accuracy </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.10276</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.10276</id><created>2025-02-14</created><updated>2025-02-24</updated><authors><author><keyname>Scauda</keyname><forenames>Martina</forenames></author><author><keyname>Kuipers</keyname><forenames>Jack</forenames></author><author><keyname>Moffa</keyname><forenames>Giusi</forenames></author></authors><title>A Latent Causal Inference Framework for Ordinal Variables</title><categories>stat.ME stat.ML</categories><comments>22 pages, 4 figures (plus 16 pages of appendices)</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Ordinal variables, such as on the Likert scale, are common in applied research. Yet, existing methods for causal inference tend to target nominal or continuous data. When applied to ordinal data, this fails to account for the inherent ordering or imposes well-defined relative magnitudes. Hence, there is a need for specialised methods to compute interventional effects between ordinal variables while accounting for their ordinality. One potential framework is to presume a latent Gaussian Directed Acyclic Graph (DAG) model: that the ordinal variables originate from marginally discretising a set of Gaussian variables whose latent covariance matrix is constrained to satisfy the conditional independencies inherent in a DAG. Conditioned on a given latent covariance matrix and discretisation thresholds, we derive a closed-form function for ordinal causal effects in terms of interventional distributions in the latent space. Our causal estimation combines naturally with algorithms to learn the latent DAG and its parameters, like the Ordinal Structural EM algorithm. Simulations demonstrate the applicability of the proposed approach in estimating ordinal causal effects both for known and unknown structures of the latent graph. As an illustration of a real-world use case, the method is applied to survey data of 408 patients from a study on the functional relationships between symptoms of obsessive-compulsive disorder and depression. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.11071</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.11071</id><created>2025-02-16</created><updated>2025-02-24</updated><authors><author><keyname>Maurer</keyname><forenames>Andreas</forenames></author></authors><title>Generalization of the Gibbs algorithm with high probability at low   temperatures</title><categories>cs.LG stat.ML</categories><msc-class>68T05</msc-class><acm-class>G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper gives a bound on the generalization error of the Gibbs algorithm, which recovers known data-independent bounds for the high temperature range and extends to the low-temperature range, where generalization depends critically on the data-dependent loss-landscape. It is shown, that with high probability the generalization error of a single hypothesis drawn from the Gibbs posterior decreases with the total prior volume of all hypotheses with similar or smaller empirical error. This gives theoretical support to the belief in the benefit of flat minima. The zero temperature limit is discussed and the bound is extended to a class of similar stochastic algorithms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.11507</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.11507</id><created>2025-02-17</created><authors><author><keyname>Abba</keyname><forenames>Badamasi</forenames></author><author><keyname>Muhammad</keyname><forenames>Mustapha</forenames></author><author><keyname>Isa</keyname><forenames>Muhammad Salihu</forenames></author><author><keyname>Jingbiao</keyname><forenames>Wu</forenames></author></authors><title>A Bi-failure Mode Model for Competing Risk Modeling with HMC-Driven   Bayesian Framework</title><categories>stat.ME</categories><comments>24 pages, 10 figures, 8 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Bathtub failure rate (BFR) and roller-coaster failure rate (RCFR - a sequence of BFR and inverted BFR (IBFR)) shapes are among the non-monotone failure rate function (FRF) behaviors often observed in complex or competing risks (CR) datasets. Recent studies have introduced varied bathtub failure rate models for reliability modeling of such datasets. However, limited attention is paid to the reliability study of CR datasets characterized by RCFR. Motivated by this drawback, this paper proposes the so-called Bi-Failure Modes (BFM) model for robust reliability analysis of CR data exhibiting BFR, RCFR, and several other FRF shapes. The mean residual life function (MRLF) and cause-specific failure probabilities are studied in detail. The fundamental reciprocal relationships between the MRLF and FRF are established. We propose the Hamiltonian Monte Carlo (HMC)-based Bayesian framework for estimating the BFM parameters and its reliability attributes to offer greater computational efficiency and faster inference. Two CR datasets from electrode voltage endurance life and electrical appliance tests, respectively, characterized by BFR and RCFR behaviors, are employed to demonstrate the BFM adequacy. The recently introduced Bridge Criterion (BC) metric and other metrics are used to evaluate the BFM modeling performance against five recent methodologies under the maximum likelihood technique. The BFM compatibility with the two datasets is also examined. The findings portrayed the BFM's advantage over other competing candidates. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.13662</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.13662</id><created>2025-02-19</created><updated>2025-02-25</updated><authors><author><keyname>Yakovlev</keyname><forenames>Konstantin</forenames></author><author><keyname>Puchkin</keyname><forenames>Nikita</forenames></author></authors><title>Generalization error bound for denoising score matching under relaxed   manifold assumption</title><categories>cs.LG math.ST stat.ML stat.TH</categories><comments>70 pages; refined rates of convergence in the bounded case</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We examine theoretical properties of the denoising score matching estimate. We model the density of observations with a nonparametric Gaussian mixture. We significantly relax the standard manifold assumption allowing the samples step away from the manifold. At the same time, we are still able to leverage a nice distribution structure. We derive non-asymptotic bounds on the approximation and generalization errors of the denoising score matching estimate. The rates of convergence are determined by the intrinsic dimension. Furthermore, our bounds remain valid even if we allow the ambient dimension grow polynomially with the sample size. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.14069</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.14069</id><created>2025-02-19</created><updated>2025-02-22</updated><authors><author><keyname>Brunel</keyname><forenames>Victor-Emmanuel</forenames></author><author><keyname>Serres</keyname><forenames>Jordan</forenames></author></authors><title>Finite sample bounds for barycenter estimation in geodesic spaces</title><categories>math.ST math.PR stat.ML stat.TH</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  We study the problem of estimating the barycenter of a distribution given i.i.d. data in a geodesic space. Assuming an upper curvature bound in Alexandrov's sense and a support condition ensuring the strong geodesic convexity of the barycenter problem, we establish finite-sample error bounds in expectation and with high probability. Our results generalize Hoeffding- and Bernstein-type concentration inequalities from Euclidean to geodesic spaces. Building on these concentration inequalities, we derive statistical guarantees for two efficient algorithms for the computation of barycenters. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.14790</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.14790</id><created>2025-02-20</created><updated>2025-02-24</updated><authors><author><keyname>Terenin</keyname><forenames>Alexander</forenames></author><author><keyname>Negrea</keyname><forenames>Jeffrey</forenames></author></authors><title>An Adversarial Analysis of Thompson Sampling for Full-information Online   Learning: from Finite to Infinite Action Spaces</title><categories>cs.LG cs.GT math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop an analysis of Thompson sampling for online learning under full feedback - also known as prediction with expert advice - where the learner's prior is defined over the space of an adversary's future actions, rather than the space of experts. We show regret decomposes into regret the learner expected a priori, plus a prior-robustness-type term we call excess regret. In the classical finite-expert setting, this recovers optimal rates. As an initial step towards practical online learning in settings with a potentially-uncountably-infinite number of experts, we show that Thompson sampling with a certain Gaussian process prior widely-used in the Bayesian optimization literature has a $\mathcal{O}(\beta\sqrt{T\log(1+\lambda)})$ rate against a $\beta$-bounded $\lambda$-Lipschitz adversary. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.15104</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.15104</id><created>2025-02-20</created><updated>2025-02-24</updated><authors><author><keyname>Chun</keyname><forenames>Chanwoo</forenames></author><author><keyname>Canatar</keyname><forenames>Abdulkadir</forenames></author><author><keyname>Chung</keyname><forenames>SueYeon</forenames></author><author><keyname>Lee</keyname><forenames>Daniel D.</forenames></author></authors><title>Estimating Neural Representation Alignment from Sparsely Sampled Inputs   and Features</title><categories>q-bio.NC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In both artificial and biological systems, the centered kernel alignment (CKA) has become a widely used tool for quantifying neural representation similarity. While current CKA estimators typically correct for the effects of finite stimuli sampling, the effects of sampling a subset of neurons are overlooked, introducing notable bias in standard experimental scenarios. Here, we provide a theoretical analysis showing how this bias is affected by the representation geometry. We then introduce a novel estimator that corrects for both input and feature sampling. We use our method for evaluating both brain-to-brain and model-to-brain alignments and show that it delivers reliable comparisons even with very sparsely sampled neurons. We perform within-animal and across-animal comparisons on electrophysiological data from visual cortical areas V1, V4, and IT data, and use these as benchmarks to evaluate model-to-brain alignment. We also apply our method to reveal how object representations become progressively disentangled across layers in both biological and artificial systems. These findings underscore the importance of correcting feature-sampling biases in CKA and demonstrate that our bias-corrected estimator provides a more faithful measure of representation alignment. The improved estimates increase our understanding of how neural activity is structured across both biological and artificial systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.15215</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.15215</id><created>2025-02-21</created><updated>2025-02-23</updated><authors><author><keyname>Park</keyname><forenames>Seokhun</forenames></author><author><keyname>Kong</keyname><forenames>Insung</forenames></author><author><keyname>Choi</keyname><forenames>Yongchan</forenames></author><author><keyname>Park</keyname><forenames>Chanmoo</forenames></author><author><keyname>Kim</keyname><forenames>Yongdai</forenames></author></authors><title>Tensor Product Neural Networks for Functional ANOVA Model</title><categories>stat.ML cs.LG math.ST stat.TH</categories><comments>45 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Interpretability for machine learning models is becoming more and more important as machine learning models become more complex. The functional ANOVA model, which decomposes a high-dimensional function into a sum of lower dimensional functions so called components, is one of the most popular tools for interpretable AI, and recently, various neural network models have been developed for estimating each component in the functional ANOVA model. However, such neural networks are highly unstable when estimating components since the components themselves are not uniquely defined. That is, there are multiple functional ANOVA decompositions for a given function. In this paper, we propose a novel interpretable model which guarantees a unique functional ANOVA decomposition and thus is able to estimate each component stably. We call our proposed model ANOVA-NODE since it is a modification of Neural Oblivious Decision Ensembles (NODE) for the functional ANOVA model. Theoretically, we prove that ANOVA-NODE can approximate a smooth function well. Additionally, we experimentally show that ANOVA-NODE provides much more stable estimation of each component and thus much more stable interpretation when training data and initial values of the model parameters vary than existing neural network models do. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.15717</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.15717</id><created>2025-01-26</created><authors><author><keyname>Moklyachuk</keyname><forenames>Mikhail</forenames></author></authors><title>On interpolation problem for multidimensional harmonizable stable   sequences with noise observations</title><categories>math.ST stat.TH</categories><comments>arXiv admin note: substantial text overlap with arXiv:1607.00754,   arXiv:2406.17917</comments><msc-class>Primary: 60G10, 60G25, 60G35, Secondary: 62M20, 93E10, 93E11</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the problem of optimal linear estimation of the functional $$A_N \vec{\xi} =\sum_{j = 0}^{N} (\vec{a}(j))^{\top} \vec{\xi}(j)$$ that depends on the unknown values $\vec{\xi}(j),j=0,1,\dots,N,$ of a vector-valued harmonizable symmetric $\alpha$-stable random sequence $\vec{\xi}(j)=\left \{ \xi_ {k} (j) \right \}_{k = 1} ^ {T}$, from observations of the sequence $\vec{\xi}(j)+\vec{\eta}(j)$ at points $j\in\mathbb Z\setminus\{0,1,\dots,N\}$. We consider the problem for mutually independent vector-valued harmonizable symmetric $\alpha$-stable random sequences $\vec{\xi}(j)=\left \{ \xi_ {k} (j) \right \}_{k = 1} ^ {T}$ and $\vec{\eta}(j)=\left \{ \xi_ {k} (j) \right \}_{k = 1} ^ {T}$ which have absolutely continuous spectral measures and the spectral densities $f(\theta)$ and $g(\theta)$ satisfying the minimality condition. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.15726</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.15726</id><created>2025-01-29</created><authors><author><keyname>Tavares</keyname><forenames>Luiz</forenames></author><author><keyname>Mazzon</keyname><forenames>Jose</forenames></author><author><keyname>Paletta</keyname><forenames>Francisco</forenames></author><author><keyname>Barros</keyname><forenames>Fabio</forenames></author></authors><title>Bankruptcy analysis using images and convolutional neural networks (CNN)</title><categories>q-fin.RM cs.LG math.ST q-fin.ST stat.TH</categories><comments>23 pages, 7 tables, 1 Figure</comments><msc-class>68T07 (Primary) 91G80, 91B84, 68U10, 62P20 (Secondary)</msc-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The marketing departments of financial institutions strive to craft products and services that cater to the diverse needs of businesses of all sizes. However, it is evident upon analysis that larger corporations often receive a more substantial portion of available funds. This disparity arises from the relative ease of assessing the risk of default and bankruptcy in these more prominent companies. Historically, risk analysis studies have focused on data from publicly traded or stock exchange-listed companies, leaving a gap in knowledge about small and medium-sized enterprises (SMEs). Addressing this gap, this study introduces a method for evaluating SMEs by generating images for processing via a convolutional neural network (CNN). To this end, more than 10,000 images, one for each company in the sample, were created to identify scenarios in which the CNN can operate with higher assertiveness and reduced training error probability. The findings demonstrate a significant predictive capacity, achieving 97.8% accuracy, when a substantial number of images are utilized. Moreover, the image creation method paves the way for potential applications of this technique in various sectors and for different analytical purposes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.15752</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.15752</id><created>2025-02-10</created><authors><author><keyname>Mallory</keyname><forenames>Matthew Esmaili</forenames></author><author><keyname>Huang</keyname><forenames>Kevin Han</forenames></author><author><keyname>Austern</keyname><forenames>Morgane</forenames></author></authors><title>Universality of High-Dimensional Logistic Regression and a Novel CGMT   under Block Dependence with Applications to Data Augmentation</title><categories>math.ST stat.ML stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Over the last decade, a wave of research has characterized the exact asymptotic risk of many high-dimensional models in the proportional regime. Two foundational results have driven this progress: Gaussian universality, which shows that the asymptotic risk of estimators trained on non-Gaussian and Gaussian data is equivalent, and the convex Gaussian min-max theorem (CGMT), which characterizes the risk under Gaussian settings. However, these results rely on the assumption that the data consists of independent random vectors, an assumption that significantly limits its applicability to many practical setups. In this paper, we address this limitation by generalizing both results to the dependent setting. More precisely, we prove that Gaussian universality still holds for high-dimensional logistic regression under block dependence, and establish a novel CGMT framework that accommodates for correlation across both the covariates and observations. Using these results, we establish the impact of data augmentation, a widespread practice in deep learning, on the asymptotic risk. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.15772</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.15772</id><created>2025-02-16</created><authors><author><keyname>Yardimci</keyname><forenames>Yigitcan</forenames></author><author><keyname>Cavus</keyname><forenames>Mustafa</forenames></author></authors><title>Rashomon perspective for measuring uncertainty in the survival   predictive maintenance models</title><categories>stat.AP cs.LG</categories><comments>4 pages, 1 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The prediction of the Remaining Useful Life of aircraft engines is a critical area in high-reliability sectors such as aerospace and defense. Early failure predictions help ensure operational continuity, reduce maintenance costs, and prevent unexpected failures. Traditional regression models struggle with censored data, which can lead to biased predictions. Survival models, on the other hand, effectively handle censored data, improving predictive accuracy in maintenance processes. This paper introduces a novel approach based on the Rashomon perspective, which considers multiple models that achieve similar performance rather than relying on a single best model. This enables uncertainty quantification in survival probability predictions and enhances decision-making in predictive maintenance. The Rashomon survival curve was introduced to represent the range of survival probability estimates, providing insights into model agreement and uncertainty over time. The results on the CMAPSS dataset demonstrate that relying solely on a single model for RUL estimation may increase risk in some scenarios. The censoring levels significantly impact prediction uncertainty, with longer censoring times leading to greater variability in survival probabilities. These findings underscore the importance of incorporating model multiplicity in predictive maintenance frameworks to achieve more reliable and robust failure predictions. This paper contributes to uncertainty quantification in RUL prediction and highlights the Rashomon perspective as a powerful tool for predictive modeling. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.15789</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.15789</id><created>2025-02-18</created><authors><author><keyname>Gabashvili</keyname><forenames>Irene S.</forenames></author><author><keyname>Allsup</keyname><forenames>Christopher K.</forenames></author></authors><title>Resident Turnover and Community Satisfaction in Active Lifestyle   Communities</title><categories>stat.AP</categories><comments>27 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  An analysis of Tellico Village, a non-age-restricted active lifestyle community, reveals complex patterns in resident tenure and satisfaction. Longitudinal surveys (2018-2024) and property records show consistently high satisfaction levels (93%), yet a decline in median tenure from 13 years pre-COVID to 11 years post-COVID (p &lt; 0.001), reflecting a broader nationwide trend in homeownership duration.   Kaplan-Meier survival analysis identifies departure risk peaks at years 3, 5, 7, 11, 16, 22, and 26, corresponding to life transitions and market cycles. Satisfaction follows a U-shaped trajectory, lowest between years 6-9 (3-12). Key predictors include financial attitudes, recreational engagement, and openness to growth.   While aggregate willingness to pay higher POA fees strongly correlates with satisfaction, explaining 94% of the variance, this relationship weakens at the neighborhood (44%) and household (5%) levels. Machine learning models - including Support Vector Machines, Random Forests, and XGBoost - and hedonic price analysis provided limited predictive power, suggesting the influence of unmeasured variables.   The study advances understanding of housing tenure dynamics in lifestyle communities while highlighting the need for more sophisticated longitudinal tracking and instrumental variable approaches. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.15820</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.15820</id><created>2025-02-19</created><authors><author><keyname>Hayashi</keyname><forenames>Yusuke</forenames></author><author><keyname>Takahashi</keyname><forenames>Koichi</forenames></author></authors><title>Universal AI maximizes Variational Empowerment</title><categories>cs.AI cs.LG stat.ML</categories><comments>9 pages, no figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper presents a theoretical framework unifying AIXI -- a model of universal AI -- with variational empowerment as an intrinsic drive for exploration. We build on the existing framework of Self-AIXI -- a universal learning agent that predicts its own actions -- by showing how one of its established terms can be interpreted as a variational empowerment objective. We further demonstrate that universal AI's planning process can be cast as minimizing expected variational free energy (the core principle of active Inference), thereby revealing how universal AI agents inherently balance goal-directed behavior with uncertainty reduction curiosity). Moreover, we argue that power-seeking tendencies of universal AI agents can be explained not only as an instrumental strategy to secure future reward, but also as a direct consequence of empowerment maximization -- i.e.\ the agent's intrinsic drive to maintain or expand its own controllability in uncertain environments. Our main contribution is to show how these intrinsic motivations (empowerment, curiosity) systematically lead universal AI agents to seek and sustain high-optionality states. We prove that Self-AIXI asymptotically converges to the same performance as AIXI under suitable conditions, and highlight that its power-seeking behavior emerges naturally from both reward maximization and curiosity-driven exploration. Since AIXI can be view as a Bayes-optimal mathematical formulation for Artificial General Intelligence (AGI), our result can be useful for further discussion on AI safety and the controllability of AGI. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.15822</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.15822</id><created>2025-02-19</created><authors><author><keyname>Hu</keyname><forenames>Tianzuo</forenames></author></authors><title>Financial fraud detection system based on improved random forest and   gradient boosting machine (GBM)</title><categories>q-fin.ST cs.LG q-fin.GN stat.AP stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper proposes a financial fraud detection system based on improved Random Forest (RF) and Gradient Boosting Machine (GBM). Specifically, the system introduces a novel model architecture called GBM-SSRF (Gradient Boosting Machine with Simplified and Strengthened Random Forest), which cleverly combines the powerful optimization capabilities of the gradient boosting machine (GBM) with improved randomization. The computational efficiency and feature extraction capabilities of the Simplified and Strengthened Random Forest (SSRF) forest significantly improve the performance of financial fraud detection. Although the traditional random forest model has good classification capabilities, it has high computational complexity when faced with large-scale data and has certain limitations in feature selection. As a commonly used ensemble learning method, the GBM model has significant advantages in optimizing performance and handling nonlinear problems. However, GBM takes a long time to train and is prone to overfitting problems when data samples are unbalanced. In response to these limitations, this paper optimizes the random forest based on the structure, reducing the computational complexity and improving the feature selection ability through the structural simplification and enhancement of the random forest. In addition, the optimized random forest is embedded into the GBM framework, and the model can maintain efficiency and stability with the help of GBM's gradient optimization capability. Experiments show that the GBM-SSRF model not only has good performance, but also has good robustness and generalization capabilities, providing an efficient and reliable solution for financial fraud detection. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.15834</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.15834</id><created>2025-02-20</created><authors><author><keyname>Moskvoretskii</keyname><forenames>Viktor</forenames></author><author><keyname>Alvandian</keyname><forenames>Narek</forenames></author></authors><title>Challenges of Multi-Modal Coreset Selection for Depth Prediction</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Coreset selection methods are effective in accelerating training and reducing memory requirements but remain largely unexplored in applied multimodal settings. We adapt a state-of-the-art (SoTA) coreset selection technique for multimodal data, focusing on the depth prediction task. Our experiments with embedding aggregation and dimensionality reduction approaches reveal the challenges of extending unimodal algorithms to multimodal scenarios, highlighting the need for specialized methods to better capture inter-modal relationships. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.15848</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.15848</id><created>2025-02-20</created><authors><author><keyname>Hovd</keyname><forenames>Markus</forenames></author><author><keyname>Kryshchenko</keyname><forenames>Alona</forenames></author><author><keyname>Neely</keyname><forenames>Michael N.</forenames></author><author><keyname>Otalvaro</keyname><forenames>Julian</forenames></author><author><keyname>Schumitzky</keyname><forenames>Alan</forenames></author><author><keyname>Yamada</keyname><forenames>Walter M.</forenames></author></authors><title>A non-parametric optimal design algorithm for population   pharmacokinetics</title><categories>stat.ME cs.LG q-bio.QM stat.AP stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper introduces a non-parametric estimation algorithm designed to effectively estimate the joint distribution of model parameters with application to population pharmacokinetics. Our research group has previously developed the non-parametric adaptive grid (NPAG) algorithm, which while accurate, explores parameter space using an ad-hoc method to suggest new support points. In contrast, the non-parametric optimal design (NPOD) algorithm uses a gradient approach to suggest new support points, which reduces the amount of time spent evaluating non-relevant points and by this the overall number of cycles required to reach convergence. In this paper, we demonstrate that the NPOD algorithm achieves similar solutions to NPAG across two datasets, while being significantly more efficient in both the number of cycles required and overall runtime. Given the importance of developing robust and efficient algorithms for determining drug doses quickly in pharmacokinetics, the NPOD algorithm represents a valuable advancement in non-parametric modeling. Further analysis is needed to determine which algorithm performs better under specific conditions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.15891</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.15891</id><created>2025-02-21</created><authors><author><keyname>Oliveira</keyname><forenames>Deborah</forenames></author><author><keyname>Cerqueira</keyname><forenames>Andressa</forenames></author><author><keyname>Oliveira</keyname><forenames>Roberto</forenames></author></authors><title>Counting communities in weighted Stochastic Block Models via   semidefinite programming</title><categories>math.ST stat.TH</categories><comments>This is a first draft. Comments are welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating the number of communities in a weighted balanced Stochastic Block Model. We construct hypothesis tests based on semidefinite programming and with a statistic coming from a GOE matrix to distinguish between any two candidate numbers of communities. This is possible due to a universality result for a semidefinite programming-based function that we also prove. The tests are then used to form a sequential test to estimate the number of communities. Furthermore, we also construct estimators of the communities themselves. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.15900</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.15900</id><created>2025-02-21</created><authors><author><keyname>Chen</keyname><forenames>George H.</forenames></author><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author></authors><title>Explaining the Success of Nearest Neighbor Methods in Prediction</title><categories>cs.LG stat.ML</categories><comments>Originally published on May 31, 2018 in Foundations and Trends in   Machine Learning; this revised version fixes some proof details for k-NN and   fixed-radius NN regression and classification</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many modern methods for prediction leverage nearest neighbor search to find past training examples most similar to a test example, an idea that dates back in text to at least the 11th century and has stood the test of time. This monograph aims to explain the success of these methods, both in theory, for which we cover foundational nonasymptotic statistical guarantees on nearest-neighbor-based regression and classification, and in practice, for which we gather prominent methods for approximate nearest neighbor search that have been essential to scaling prediction systems reliant on nearest neighbor analysis to handle massive datasets. Furthermore, we discuss connections to learning distances for use with nearest neighbor methods, including how random decision trees and ensemble methods learn nearest neighbor structure, as well as recent developments in crowdsourcing and graphons.   In terms of theory, our focus is on nonasymptotic statistical guarantees, which we state in the form of how many training data and what algorithm parameters ensure that a nearest neighbor prediction method achieves a user-specified error tolerance. We begin with the most general of such results for nearest neighbor and related kernel regression and classification in general metric spaces. In such settings in which we assume very little structure, what enables successful prediction is smoothness in the function being estimated for regression, and a low probability of landing near the decision boundary for classification. In practice, these conditions could be difficult to verify for a real dataset. We then cover recent guarantees on nearest neighbor prediction in the three case studies of time series forecasting, recommending products to people over time, and delineating human organs in medical images by looking at image patches. In these case studies, clustering structure enables successful prediction. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.15905</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.15905</id><created>2025-02-21</created><authors><author><keyname>Chwila</keyname><forenames>Adam</forenames></author><author><keyname>Hada≈õ-Dyduch</keyname><forenames>Monika</forenames></author><author><keyname>Krzciuk</keyname><forenames>Ma≈Çgorzata</forenames></author><author><keyname>Stachurski</keyname><forenames>Tomasz</forenames></author><author><keyname>Wolny-Dominiak</keyname><forenames>Alicja</forenames></author><author><keyname>≈ªƒÖd≈Ço</keyname><forenames>Tomasz</forenames></author></authors><title>Improving ex ante accuracy assessment in predicting house price   dispersion: evidence from the USA</title><categories>stat.ME stat.AP</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  The study focuses on improving the ex ante prediction accuracy assessment in the case of forecasting various house price dispersion measures in the USA. It addresses a critical gap in real estate market forecasting by proposing a novel method for assessing ex ante prediction accuracy under unanticipated shocks. The proposal is based on a parametric bootstrap approach under a misspecified model, allowing for the simulation of future values and estimation of prediction errors in case of unexpected price changes. The study highlights the limitations of the traditional approach that fails to account for unforeseen market events and provides a more in-depth understanding of how prediction accuracy changes under unexpected scenarios. The proposed methods offers valuable insights for real estate market management by enabling more robust risk assessment and decision-making in the face of unexpected market fluctuations. Real data application is based on longitudinal U.S. data on real estate transactions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.15933</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.15933</id><created>2025-02-21</created><authors><author><keyname>Chen</keyname><forenames>Yuting</forenames></author><author><keyname>Lahiri</keyname><forenames>Partha</forenames></author><author><keyname>Salvati</keyname><forenames>Nicola</forenames></author></authors><title>Empirical Best Prediction of Poverty Indicators via Nested Error   Regression with High Dimensional Parameters</title><categories>stat.ME stat.AP</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper extends the Nested Error Regression Model with High-Dimensional Parameters (NERHDP) to address challenges in small area poverty estimation. Building on the NERHDP framework, we develop a robust and flexible approach to derive empirical best predictors (EBPs) of small area poverty indicators, while accommodating heterogeneity in regression coefficients and sampling variances across areas. To overcome computational limitations in the existing algorithm, we introduce an efficient method that significantly reduces computation time, enhancing scalability for large datasets. Additionally, we propose a novel method for generating area-specific poverty estimates for out-of-sample areas, improving the reliability of synthetic estimates. To quantify uncertainty, we introduce a parametric bootstrap method tailored to the extended model. Through design-based simulation studies, we demonstrate that the proposed method has better performance in terms of relative bias and relative root mean squared prediction error compared to existing approaches. Furthermore, the proposed method is applied to household survey data from the 2002 Albania Living Standards Measurement Survey to estimate poverty indicators for 374 municipalities using auxiliary information from the 2001 census. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.15945</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.15945</id><created>2025-02-21</created><authors><author><keyname>Chaya</keyname><forenames>Carolina</forenames></author><author><keyname>Castura</keyname><forenames>John</forenames></author><author><keyname>Greenacre</keyname><forenames>Michael</forenames></author></authors><title>One citation, one vote! A new approach for analysing   check-all-that-apply (CATA) data in sensometrics, using L1 norm methods</title><categories>stat.AP</categories><comments>29 pages, 8 figures, 3 tables, and supplementary material with 2   tables</comments><msc-class>62P20</msc-class><acm-class>G.3</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A unified framework is provided for analysing check-all-that-apply (CATA) product data following the ``one citation, one vote" principle. CATA data arise from studies where A consumers evaluate P products by describing samples by checking all of the T terms that apply. Giving every citation the same weight, regardless of the assessor, product, or term, leads to analyses based on the L1 norm where the median absolute deviation is the measure of dispersion. Five permutation tests are proposed to answer the following questions. Do any products differ? For which terms do products differ? Within each of the terms, which products differ? Which product pairs differ? On which terms does each product pair differ? Additionally, we show how products and terms can be clustered following the ``one citation, one vote" principle and how L1-norm principal component analysis (L1-norm PCA) can be applied to visualize CATA results in few dimensions. Together, the permutation tests, clustering methods, and L1-norm PCA provide a unified approach. The proposed methods are illustrated using a data set in which 100 consumers evaluated 11 products using 34 CATA terms.R code is provided to perform the analyses. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.15952</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.15952</id><created>2025-02-21</created><authors><author><keyname>Kumar</keyname><forenames>Akshay</forenames></author><author><keyname>Haupt</keyname><forenames>Jarvis</forenames></author></authors><title>Towards Understanding Gradient Flow Dynamics of Homogeneous Neural   Networks Beyond the Origin</title><categories>cs.LG math.OC stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent works exploring the training dynamics of homogeneous neural network weights under gradient flow with small initialization have established that in the early stages of training, the weights remain small and near the origin, but converge in direction. Building on this, the current paper studies the gradient flow dynamics of homogeneous neural networks with locally Lipschitz gradients, after they escape the origin. Insights gained from this analysis are used to characterize the first saddle point encountered by gradient flow after escaping the origin. Also, it is shown that for homogeneous feed-forward neural networks, under certain conditions, the sparsity structure emerging among the weights before the escape is preserved after escaping the origin and until reaching the next saddle point. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.15962</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.15962</id><created>2025-02-21</created><authors><author><keyname>Shen</keyname><forenames>Jie</forenames></author></authors><title>Towards Efficient Contrastive PAC Learning</title><categories>cs.LG cs.DS stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study contrastive learning under the PAC learning framework. While a series of recent works have shown statistical results for learning under contrastive loss, based either on the VC-dimension or Rademacher complexity, their algorithms are inherently inefficient or not implying PAC guarantees. In this paper, we consider contrastive learning of the fundamental concept of linear representations. Surprisingly, even under such basic setting, the existence of efficient PAC learners is largely open. We first show that the problem of contrastive PAC learning of linear representations is intractable to solve in general. We then show that it can be relaxed to a semi-definite program when the distance between contrastive samples is measured by the $\ell_2$-norm. We then establish generalization guarantees based on Rademacher complexity, and connect it to PAC guarantees under certain contrastive large-margin conditions. To the best of our knowledge, this is the first efficient PAC learning algorithm for contrastive learning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.15983</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.15983</id><created>2025-02-21</created><authors><author><keyname>Cristian</keyname><forenames>Rares</forenames></author><author><keyname>Harhsa</keyname><forenames>Pavithra</forenames></author><author><keyname>Perakis</keyname><forenames>Georgia</forenames></author><author><keyname>Quanz</keyname><forenames>Brian</forenames></author></authors><title>CoRe: Coherency Regularization for Hierarchical Time Series</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Hierarchical time series forecasting presents unique challenges, particularly when dealing with noisy data that may not perfectly adhere to aggregation constraints. This paper introduces a novel approach to soft coherency in hierarchical time series forecasting using neural networks. We present a network coherency regularization method, which we denote as CoRe (Coherency Regularization), a technique that trains neural networks to produce forecasts that are inherently coherent across hierarchies, without strictly enforcing aggregation constraints. Our method offers several key advantages. (1) It provides theoretical guarantees on the coherency of forecasts, even for out-of-sample data. (2) It is adaptable to scenarios where data may contain errors or missing values, making it more robust than strict coherency methods. (3) It can be easily integrated into existing neural network architectures for time series forecasting. We demonstrate the effectiveness of our approach on multiple benchmark datasets, comparing it against state-of-the-art methods in both coherent and noisy data scenarios. Additionally, our method can be used within existing generative probabilistic forecasting frameworks to generate coherent probabilistic forecasts. Our results show improved generalization and forecast accuracy, particularly in the presence of data inconsistencies. On a variety of datasets, including both strictly hierarchically coherent and noisy data, our training method has either equal or better accuracy at all levels of the hierarchy while being strictly more coherent out-of-sample than existing soft-coherency methods. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16005</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16005</id><created>2025-02-21</created><authors><author><keyname>Xiang</keyname><forenames>Daniel</forenames></author><author><keyname>Soloff</keyname><forenames>Jake A.</forenames></author><author><keyname>Fithian</keyname><forenames>William</forenames></author></authors><title>A frequentist local false discovery rate</title><categories>math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The local false discovery rate (lfdr) of Efron et al. (2001) enjoys major conceptual and decision-theoretic advantages over the false discovery rate (FDR) as an error criterion in multiple testing, but is only well-defined in Bayesian models where the truth status of each null hypothesis is random. We define a frequentist counterpart to the lfdr based on the relative frequency of nulls at each point in the sample space. The frequentist lfdr is defined without reference to any prior, but preserves several important properties of the Bayesian lfdr: For continuous test statistics, $\text{lfdr}(t)$ gives the probability, conditional on observing some statistic equal to $t$, that the corresponding null hypothesis is true. Evaluating the lfdr at an individual test statistic also yields a calibrated forecast of whether its null hypothesis is true. Finally, thresholding the lfdr at $\frac{1}{1+\lambda}$ gives the best separable rejection rule under the weighted classification loss where Type I errors are $\lambda$ times as costly as Type II errors. The lfdr can be estimated efficiently using parametric or non-parametric methods, and a closely related error criterion can be provably controlled in finite samples under independence assumptions. Whereas the FDR measures the average quality of all discoveries in a given rejection region, our lfdr measures how the quality of discoveries varies across the rejection region, allowing for a more fine-grained analysis without requiring the introduction of a prior. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16008</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16008</id><created>2025-02-21</created><authors><author><keyname>Mazumdar</keyname><forenames>Arya</forenames></author><author><keyname>Sangwan</keyname><forenames>Neha</forenames></author></authors><title>Exact Recovery of Sparse Binary Vectors from Generalized Linear   Measurements</title><categories>stat.ML cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of exact recovery of a $k$-sparse binary vector from generalized linear measurements (such as logistic regression). We analyze the linear estimation algorithm (Plan, Vershynin, Yudovina, 2017), and also show information theoretic lower bounds on the number of required measurements. As a consequence of our results, for noisy one bit quantized linear measurements ($\mathsf{1bCSbinary}$), we obtain a sample complexity of $O((k+\sigma^2)\log{n})$, where $\sigma^2$ is the noise variance. This is shown to be optimal due to the information theoretic lower bound. We also obtain tight sample complexity characterization for logistic regression.   Since $\mathsf{1bCSbinary}$ is a strictly harder problem than noisy linear measurements ($\mathsf{SparseLinearReg}$) because of added quantization, the same sample complexity is achievable for $\mathsf{SparseLinearReg}$. While this sample complexity can be obtained via the popular lasso algorithm, linear estimation is computationally more efficient. Our lower bound holds for any set of measurements for $\mathsf{SparseLinearReg}$, (similar bound was known for Gaussian measurement matrices) and is closely matched by the maximum-likelihood upper bound. For $\mathsf{SparseLinearReg}$, it was conjectured in Gamarnik and Zadik, 2017 that there is a statistical-computational gap and the number of measurements should be at least $(2k+\sigma^2)\log{n}$ for efficient algorithms to exist. It is worth noting that our results imply that there is no such statistical-computational gap for $\mathsf{1bCSbinary}$ and logistic regression. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16015</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16015</id><created>2025-02-21</created><authors><author><keyname>Navas-Palencia</keyname><forenames>Guillermo</forenames></author></authors><title>On the computation of the cumulative distribution function of the Normal   Inverse Gaussian distribution</title><categories>math.NA cs.MS cs.NA stat.CO</categories><msc-class>62-08, 33-04, 33F05</msc-class><acm-class>G.1; G.3; G.4</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we obtain various series and asymptotic expansions involving the modified Bessel function of the second kind for the normal inverse Gaussian cumulative distribution function. The new expansions accelerate computations, complementing the numerical integration methods implemented in statistical software packages. We also provide a detailed description of the algorithm and its corresponding implementation in C++. The performance and accuracy of the algorithm are extensively tested and benchmarked with open-source implementations, offering superior accuracy and speed-ups of a factor from 5 to 60. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16019</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16019</id><created>2025-02-21</created><authors><author><keyname>Koolen</keyname><forenames>Wouter M.</forenames></author><author><keyname>P√©rez-Ortiz</keyname><forenames>Muriel Felipe</forenames></author><author><keyname>Lardy</keyname><forenames>Tyron</forenames></author></authors><title>A Generalisation of Ville's Inequality to Monotonic Lower Bounds and   Thresholds</title><categories>math.ST math.PR stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Essentially all anytime-valid methods hinge on Ville's inequality to gain validity across time without incurring a union bound. Ville's inequality is a proper generalisation of Markov's inequality. It states that a non-negative supermartingale will only ever reach a multiple of its initial value with small probability. In the classic rendering both the lower bound (of zero) and the threshold are constant in time. We generalise both to monotonic curves. That is, we bound the probability that a supermartingale which remains above a given decreasing curve exceeds a given increasing threshold curve. We show our bound is tight by exhibiting a supermartingale for which the bound is an equality. Using our generalisation, we derive a clean finite-time version of the law of the iterated logarithm. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16042</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16042</id><created>2025-02-21</created><authors><author><keyname>Guo</keyname><forenames>Wenxuan</forenames></author><author><keyname>Liang</keyname><forenames>Tengyuan</forenames></author><author><keyname>Toulis</keyname><forenames>Panos</forenames></author></authors><title>Gaussianized Design Optimization for Covariate Balance in Randomized   Experiments</title><categories>stat.ME</categories><comments>65 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Achieving covariate balance in randomized experiments enhances the precision of treatment effect estimation. However, existing methods often require heuristic adjustments based on domain knowledge and are primarily developed for binary treatments. This paper presents Gaussianized Design Optimization, a novel framework for optimally balancing covariates in experimental design. The core idea is to Gaussianize the treatment assignments: we model treatments as transformations of random variables drawn from a multivariate Gaussian distribution, converting the design problem into a nonlinear continuous optimization over Gaussian covariance matrices. Compared to existing methods, our approach offers significant flexibility in optimizing covariate balance across a diverse range of designs and covariate types. Adapting the Burer-Monteiro approach for solving semidefinite programs, we introduce first-order local algorithms for optimizing covariate balance, improving upon several widely used designs. Furthermore, we develop inferential procedures for constructing design-based confidence intervals under Gaussianization and extend the framework to accommodate continuous treatments. Simulations demonstrate the effectiveness of Gaussianization in multiple practical scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16056</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16056</id><created>2025-02-21</created><authors><author><keyname>Olko</keyname><forenames>Mateusz</forenames></author><author><keyname>Gajewski</keyname><forenames>Mateusz</forenames></author><author><keyname>Wojciechowska</keyname><forenames>Joanna</forenames></author><author><keyname>Morzy</keyname><forenames>Miko≈Çaj</forenames></author><author><keyname>Sankowski</keyname><forenames>Piotr</forenames></author><author><keyname>Mi≈Ço≈õ</keyname><forenames>Piotr</forenames></author></authors><title>Since Faithfulness Fails: The Performance Limits of Neural Causal   Discovery</title><categories>cs.LG stat.ML</categories><comments>19 pages, 12 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Neural causal discovery methods have recently improved in terms of scalability and computational efficiency. However, our systematic evaluation highlights significant room for improvement in their accuracy when uncovering causal structures. We identify a fundamental limitation: neural networks cannot reliably distinguish between existing and non-existing causal relationships in the finite sample regime. Our experiments reveal that neural networks, as used in contemporary causal discovery approaches, lack the precision needed to recover ground-truth graphs, even for small graphs and relatively large sample sizes. Furthermore, we identify the faithfulness property as a critical bottleneck: (i) it is likely to be violated across any reasonable dataset size range, and (ii) its violation directly undermines the performance of neural discovery methods. These findings lead us to conclude that progress within the current paradigm is fundamentally constrained, necessitating a paradigm shift in this domain. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16075</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16075</id><created>2025-02-21</created><authors><author><keyname>Cai</keyname><forenames>Yuhang</forenames></author><author><keyname>Zhou</keyname><forenames>Kangjie</forenames></author><author><keyname>Wu</keyname><forenames>Jingfeng</forenames></author><author><keyname>Mei</keyname><forenames>Song</forenames></author><author><keyname>Lindsey</keyname><forenames>Michael</forenames></author><author><keyname>Bartlett</keyname><forenames>Peter L.</forenames></author></authors><title>Implicit Bias of Gradient Descent for Non-Homogeneous Deep Networks</title><categories>cs.LG math.OC stat.ML</categories><comments>96 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish the asymptotic implicit bias of gradient descent (GD) for generic non-homogeneous deep networks under exponential loss. Specifically, we characterize three key properties of GD iterates starting from a sufficiently small empirical risk, where the threshold is determined by a measure of the network's non-homogeneity. First, we show that a normalized margin induced by the GD iterates increases nearly monotonically. Second, we prove that while the norm of the GD iterates diverges to infinity, the iterates themselves converge in direction. Finally, we establish that this directional limit satisfies the Karush-Kuhn-Tucker (KKT) conditions of a margin maximization problem. Prior works on implicit bias have focused exclusively on homogeneous networks; in contrast, our results apply to a broad class of non-homogeneous networks satisfying a mild near-homogeneity condition. In particular, our results apply to networks with residual connections and non-homogeneous activation functions, thereby resolving an open problem posed by Ji and Telgarsky (2020). </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16081</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16081</id><created>2025-02-22</created><authors><author><keyname>Zheng</keyname><forenames>Bang Quan</forenames></author></authors><title>Breaking the Balance: Asymmetric Negative Voting in the 2020   Presidential Election</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While voters from opposing parties have traditionally exhibited symmetric levels of hostility toward out-party candidates, our analysis of the 2016 and 2020 Nationscape data reveals a notable departure from this pattern. In 2016, negative voting was relatively balanced, with similar levels of hostility directed at Hillary Clinton and Donald Trump. However, by 2020, asymmetric negative voting had emerged. As an incumbent seeking re-election amid a rapidly declining economy, the COVID-19 pandemic, and widespread uncertainty, Trump faced heightened negative perceptions fueled by dissatisfaction with his handling of the economy, race relations, the pandemic, and his leadership style. These factors galvanized younger, educated Democrats and Independents to vote against him in unprecedented numbers. In contrast, Republicans expressed less animosity toward Biden in 2020 than they had toward Clinton in 2016. This shift disrupted the balance in the typical pattern of symmetric negative voting. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16108</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16108</id><created>2025-02-22</created><authors><author><keyname>Saito</keyname><forenames>Asaki</forenames></author><author><keyname>Yamaguchi</keyname><forenames>Akihiro</forenames></author></authors><title>Accelerating true orbit pseudorandom number generation using Newton's   method</title><categories>stat.CO math.NT</categories><comments>16 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The binary expansions of irrational algebraic numbers can serve as high-quality pseudorandom binary sequences. This study presents an efficient method for computing the exact binary expansions of real quadratic algebraic integers using Newton's method. To this end, we clarify conditions under which the first $N$ bits of the binary expansion of an irrational number match those of its upper rational approximation. Furthermore, we establish that the worst-case time complexity of generating a sequence of length $N$ with the proposed method is equivalent to the complexity of multiplying two $N$-bit integers, showing its efficiency compared to a previously proposed true orbit generator. We report the results of numerical experiments on computation time and memory usage, highlighting in particular that the proposed method successfully accelerates true orbit pseudorandom number generation. We also confirm that a generated pseudorandom sequence successfully passes all the statistical tests included in RabbitFile of TestU01. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16115</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16115</id><created>2025-02-22</created><authors><author><keyname>Gao</keyname><forenames>Heng</forenames></author><author><keyname>He</keyname><forenames>Zhuolin</forenames></author><author><keyname>Pu</keyname><forenames>Jian</forenames></author></authors><title>Detecting OOD Samples via Optimal Transport Scoring Function</title><categories>cs.LG cs.CV stat.ML</categories><comments>IEEE International Conference on Acoustics, Speech and Signal   Processing (ICASSP), 2025</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  To deploy machine learning models in the real world, researchers have proposed many OOD detection algorithms to help models identify unknown samples during the inference phase and prevent them from making untrustworthy predictions. Unlike methods that rely on extra data for outlier exposure training, post hoc methods detect Out-of-Distribution (OOD) samples by developing scoring functions, which are model agnostic and do not require additional training. However, previous post hoc methods may fail to capture the geometric cues embedded in network representations. Thus, in this study, we propose a novel score function based on the optimal transport theory, named OTOD, for OOD detection. We utilize information from features, logits, and the softmax probability space to calculate the OOD score for each test sample. Our experiments show that combining this information can boost the performance of OTOD with a certain margin. Experiments on the CIFAR-10 and CIFAR-100 benchmarks demonstrate the superior performance of our method. Notably, OTOD outperforms the state-of-the-art method GEN by 7.19% in the mean FPR@95 on the CIFAR-10 benchmark using ResNet-18 as the backbone, and by 12.51% in the mean FPR@95 using WideResNet-28 as the backbone. In addition, we provide theoretical guarantees for OTOD. The code is available in https://github.com/HengGao12/OTOD. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16118</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16118</id><created>2025-02-22</created><authors><author><keyname>Xie</keyname><forenames>Dongyue</forenames></author></authors><title>Local False Sign Rate and the Role of Prior Covariance Rank in   Multivariate Empirical Bayes Multiple Testing</title><categories>stat.ME stat.AP</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We study the relationship between the rank of the prior covariance matrix and the local false sign rate in a multivariate empirical Bayes normal mean model. It has been observed that the false sign rate is inflated when the prior assigns weight to low-rank covariance matrices. We show that this issue arises due to the rank deficiency of prior covariance matrices and propose an adjustment to mitigate it. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16120</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16120</id><created>2025-02-22</created><authors><author><keyname>Li</keyname><forenames>Zhehao</forenames></author><author><keyname>Wu</keyname><forenames>Yanchen</forenames></author><author><keyname>Mao</keyname><forenames>Xiaojie</forenames></author></authors><title>A Fenchel-Young Loss Approach to Data-Driven Inverse Optimization</title><categories>math.OC stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Data-driven inverse optimization seeks to estimate unknown parameters in an optimization model from observations of optimization solutions. Many existing methods are ineffective in handling noisy and suboptimal solution observations and also suffer from computational challenges. In this paper, we build a connection between inverse optimization and the Fenchel-Young (FY) loss originally designed for structured prediction, proposing a FY loss approach to data-driven inverse optimization. This new approach is amenable to efficient gradient-based optimization, hence much more efficient than existing methods. We provide theoretical guarantees for the proposed method and use extensive simulation and real-data experiments to demonstrate its significant advantage in parameter estimation accuracy, decision error and computational speed. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16130</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16130</id><created>2025-02-22</created><authors><author><keyname>Dey</keyname><forenames>Asim K.</forenames></author></authors><title>A Bayesian mixed-effects model to evaluate the determinants of COVID-19   vaccine uptake in the US</title><categories>stat.AP q-bio.QM</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The COVID-19 pandemic has adversely affected US public health, resulting in over a hundred million cases and more than one million deaths. Vaccination is the key intervention against the COVID-19 pandemic. Multiple COVID-19 vaccines are now available for human use. However, a number of factors, including socio-demographic variables, impact the uptake of COVID-19 vaccines. In this study, we apply a Bayesian mixed-effects model to assess different socio-demographic and spatial factors that influence the acceptance of COVID-19 vaccines in the US. The fitted mixed-effects model provides the probabilistic inference about the vaccine acceptance determinants with uncertainty quantification. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16156</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16156</id><created>2025-02-22</created><authors><author><keyname>Ge</keyname><forenames>Lin</forenames></author><author><keyname>Cai</keyname><forenames>Hengrui</forenames></author><author><keyname>Wan</keyname><forenames>Runzhe</forenames></author><author><keyname>Xu</keyname><forenames>Yang</forenames></author><author><keyname>Song</keyname><forenames>Rui</forenames></author></authors><title>A Review of Causal Decision Making</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To make effective decisions, it is important to have a thorough understanding of the causal relationships among actions, environments, and outcomes. This review aims to surface three crucial aspects of decision-making through a causal lens: 1) the discovery of causal relationships through causal structure learning, 2) understanding the impacts of these relationships through causal effect learning, and 3) applying the knowledge gained from the first two aspects to support decision making via causal policy learning. Moreover, we identify challenges that hinder the broader utilization of causal decision-making and discuss recent advances in overcoming these challenges. Finally, we provide future research directions to address these challenges and to further enhance the implementation of causal decision-making in practice, with real-world applications illustrated based on the proposed causal decision-making. We aim to offer a comprehensive methodology and practical implementation framework by consolidating various methods in this area into a Python-based collection. URL: https://causaldm.github.io/Causal-Decision-Making. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16192</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16192</id><created>2025-02-22</created><authors><author><keyname>Dreassi</keyname><forenames>Emanuela</forenames></author><author><keyname>Pratelli</keyname><forenames>Luca</forenames></author><author><keyname>Rigo</keyname><forenames>Pietro</forenames></author></authors><title>Bayesian nonparametric inference on a Fr\'echet class</title><categories>stat.ME</categories><comments>16 pages</comments><msc-class>60G09, 60G57, 62F15, 62G99</msc-class><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Let $(\mathcal{X},\mathcal{F},\mu)$ and $(\mathcal{Y},\mathcal{G},\nu)$ be probability spaces and $(Z_n)$ a sequence of random variables with values in $(\mathcal{X}\times\mathcal{Y},\,\mathcal{F}\otimes\mathcal{G})$. Let $\Gamma(\mu,\nu)$ be the collection of all probability measures $p$ on $\mathcal{F}\otimes\mathcal{G}$ such that $$p\bigl(A\times\mathcal{Y}\bigr)=\mu(A)\quad\text{and}\quad p\bigl(\mathcal{X}\times B\bigr)=\nu(B)\quad\text{for all }A\in\mathcal{F}\text{ and }B\in\mathcal{G}.$$ In this paper, we build some probability measures $\Pi$ on $\Gamma(\mu,\nu)$. In addition, for each such $\Pi$, we assume that $(Z_n)$ is exchangeable with de Finetti's measure $\Pi$ and we evaluate the conditional distribution $\Pi(\cdot\mid Z_1,\ldots,Z_n)$. In Bayesian nonparametrics, if $(Z_1,\ldots, Z_n)$ are the available data, $\Pi$ and $\Pi(\cdot\mid Z_1,\ldots, Z_n)$ can be regarded as the prior and the posterior, respectively. To support this interpretation, it suffices to think of a problem where the unknown probability distribution of some bivariate phenomenon is constrained to have marginals $\mu$ and $\nu$. Finally, analogous results are obtained for the set $\Gamma(\mu)$ of those probability measures on $\mathcal{F}\otimes\mathcal{G}$ with marginal $\mu$ on $\mathcal{F}$ (but arbitrary marginal on $\mathcal{G}$). That is, we introduce some priors on $\Gamma(\mu)$ and we evaluate the corresponding posteriors. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16195</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16195</id><created>2025-02-22</created><authors><author><keyname>Shi</keyname><forenames>Chengchun</forenames></author></authors><title>Statistical Inference in Reinforcement Learning: A Selective Survey</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reinforcement learning (RL) is concerned with how intelligence agents take actions in a given environment to maximize the cumulative reward they receive. In healthcare, applying RL algorithms could assist patients in improving their health status. In ride-sharing platforms, applying RL algorithms could increase drivers' income and customer satisfaction. Over the past decade, RL has been arguably one of the most vibrant research frontiers in machine learning. Nevertheless, statistics as a field, as opposed to computer science, has only recently begun to engage with RL both in depth and in breadth. This paper present a selective review of statistical inferential tools for RL, covering both hypothesis testing and confidence interval construction. Our goal is to highlight the value of statistical inference in RL for both the statistics and machine learning communities, and to promote the broader application of classical statistical inference tools in this vibrant area of research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16232</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16232</id><created>2025-02-22</created><authors><author><keyname>Wang</keyname><forenames>Xintong</forenames></author><author><keyname>Guan</keyname><forenames>Xiaofei</forenames></author><author><keyname>Guo</keyname><forenames>Ling</forenames></author><author><keyname>Wu</keyname><forenames>Hao</forenames></author></authors><title>Flow-based linear embedding for Bayesian filtering of nonlinear   stochastic dynamical systems</title><categories>math.NA cs.LG cs.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian filtering for high-dimensional nonlinear stochastic dynamical systems is a fundamental yet challenging problem in many fields of science and engineering. Existing methods face significant obstacles: Gaussian-based filters struggle with non-Gaussian distributions, sequential Monte Carlo methods are computationally intensive and prone to particle degeneracy in high dimensions, and deep learning approaches often fail to balance accuracy and efficiency in complex filtering tasks. To address these challenges, we propose a flow-based Bayesian filter (FBF) that integrates normalizing flows to construct a latent linear state-space model with Gaussian filtering distributions. This framework enables efficient density estimation and sampling through invertible transformations provided by normalizing flows, which can be learned directly from data, thereby eliminating the need for prior knowledge of system dynamics or observation models. Numerical experiments demonstrate the advantages of FBF in terms of both accuracy and efficiency. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16250</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16250</id><created>2025-02-22</created><authors><author><keyname>Lim</keyname><forenames>Hoi-Jeong</forenames></author></authors><title>Meta-analysis in dental research</title><categories>stat.ME</categories><comments>6 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, importance of meta-analysis is increasing in the field of dentistry, since it is not easy to settle controversies arising from conflicting studies. Meta-analysis is the statistical method of combining results from two or more individual studies that have been done on the same topic. Merits of meta-analysis includes an increase in power, an improvement in precision, and the ability to address solution not provided by individual studies. However, it might mislead researchers when variation across studies and publication bias are not carefully taken into consideration. The purpose of this study is to help understand meta-analysis by making use of individual results in dental research paper. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16261</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16261</id><created>2025-02-22</created><authors><author><keyname>Lim</keyname><forenames>Hoi-Jeong</forenames></author><author><keyname>Park</keyname><forenames>Soo-Hyeon</forenames></author></authors><title>A step-by-step guide to generalized estimating equations using SPSS in   the field of dentistry</title><categories>stat.ME</categories><comments>20 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Generalized Estimating Equations (GEE) approach is a widely used statistical method for analyzing longitudinal data and clustered data in clinic studies. In dentistry, due to multiple outcomes obtained from one patient, the outcomes produced from individual patients are correlated with one another. This study focuses on the basic ideas of GEE and introduces the types of covariance matrix and working correlation matrix. The quasi-likelihood information criterion(QIC) and quasi-likelihood information criterion approximation(QICu) were used to select the best working matrix and the best fitting model for the correlated outcomes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16263</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16263</id><created>2025-02-22</created><authors><author><keyname>De-Diego</keyname><forenames>Elena M.</forenames></author><author><keyname>Perez-Suay</keyname><forenames>Adri√°n</forenames></author><author><keyname>Gordaliza</keyname><forenames>Paula</forenames></author><author><keyname>Loubes</keyname><forenames>Jean-Michel</forenames></author></authors><title>PLS-based approach for fair representation learning</title><categories>cs.LG cs.CY math.ST stat.ML stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We revisit the problem of fair representation learning by proposing Fair Partial Least Squares (PLS) components. PLS is widely used in statistics to efficiently reduce the dimension of the data by providing representation tailored for the prediction. We propose a novel method to incorporate fairness constraints in the construction of PLS components. This new algorithm provides a feasible way to construct such features both in the linear and the non linear case using kernel embeddings. The efficiency of our method is evaluated on different datasets, and we prove its superiority with respect to standard fair PCA method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16270</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16270</id><created>2025-02-22</created><authors><author><keyname>Mardia</keyname><forenames>Kanti V.</forenames></author><author><keyname>Eltzner</keyname><forenames>Benjamin</forenames></author><author><keyname>Huckemann</keyname><forenames>Stephan F.</forenames></author></authors><title>Constrained Shape Analysis with Applications to RNA Structure</title><categories>stat.ME cs.CE q-bio.BM</categories><comments>33 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications of shape analysis, lengths between some landmarks are constrained. For instance, biomolecules often have some bond lengths and some bond angles constrained, and variation occurs only along unconstrained bonds and constrained bonds' torsions where the latter are conveniently modelled by dihedral angles. Our work has been motivated by low resolution biomolecular chain RNA where only some prominent atomic bonds can be well identified. Here, we propose a new modelling strategy for such constrained shape analysis starting with a product of polar coordinates (polypolars), where, due to constraints, for example, some radial coordinates should be omitted, leaving products of spheres (polyspheres). We give insight into these coordinates for particular cases such as five landmarks which are motivated by a practical RNA application. We also discuss distributions for polypolar coordinates and give a specific methodology with illustration when the constrained size-and-shape variables are concentrated. There are applications of this in clustering and we give some insight into a modified version of the MINT-AGE algorithm. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16299</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16299</id><created>2025-02-22</created><authors><author><keyname>J√ºrgens</keyname><forenames>Mira</forenames></author><author><keyname>Mortier</keyname><forenames>Thomas</forenames></author><author><keyname>H√ºllermeier</keyname><forenames>Eyke</forenames></author><author><keyname>Bengs</keyname><forenames>Viktor</forenames></author><author><keyname>Waegeman</keyname><forenames>Willem</forenames></author></authors><title>A calibration test for evaluating set-based epistemic uncertainty   representations</title><categories>cs.LG cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The accurate representation of epistemic uncertainty is a challenging yet essential task in machine learning. A widely used representation corresponds to convex sets of probabilistic predictors, also known as credal sets. One popular way of constructing these credal sets is via ensembling or specialized supervised learning methods, where the epistemic uncertainty can be quantified through measures such as the set size or the disagreement among members. In principle, these sets should contain the true data-generating distribution. As a necessary condition for this validity, we adopt the strongest notion of calibration as a proxy. Concretely, we propose a novel statistical test to determine whether there is a convex combination of the set's predictions that is calibrated in distribution. In contrast to previous methods, our framework allows the convex combination to be instance dependent, recognizing that different ensemble members may be better calibrated in different regions of the input space. Moreover, we learn this combination via proper scoring rules, which inherently optimize for calibration. Building on differentiable, kernel-based estimators of calibration errors, we introduce a nonparametric testing procedure and demonstrate the benefits of capturing instance-level variability on of synthetic and real-world experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16313</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16313</id><created>2025-02-22</created><authors><author><keyname>Nguyen</keyname><forenames>Quang</forenames></author><author><keyname>Yurko</keyname><forenames>Ronald</forenames></author></authors><title>A multilevel model with heterogeneous variances for snap timing in the   National Football League</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Player tracking data have provided great opportunities to generate novel insights into understudied areas of American football, such as pre-snap motion. Using a Bayesian multilevel model with heterogeneous variances, we provide an assessment of NFL quarterbacks and their ability to synchronize the timing of the ball snap with pre-snap movement from their teammates. We focus on passing plays with receivers in motion at the snap and running a route, and define the snap timing as the time between the moment a receiver begins motioning and the ball snap event. We assume a Gamma distribution for the play-level snap timing and model the mean parameter with player and team random effects, along with relevant fixed effects such as the motion type identified via a Gaussian mixture model. Most importantly, we model the shape parameter with quarterback random effects, which enables us to estimate the differences in snap timing variability among NFL quarterbacks. We demonstrate that higher variability in snap timing is beneficial for the passing game, as it relates to facing less havoc created by the opposing defense. We also obtain a quarterback leaderboard based on our snap timing variability measure, and Patrick Mahomes stands out as the top player. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16331</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16331</id><created>2025-02-22</created><authors><author><keyname>Kumar</keyname><forenames>Akash</forenames></author><author><keyname>Parhi</keyname><forenames>Rahul</forenames></author><author><keyname>Belkin</keyname><forenames>Mikhail</forenames></author></authors><title>A Gap Between the Gaussian RKHS and Neural Networks: An Infinite-Center   Asymptotic Analysis</title><categories>cs.LG cs.AI stat.ML</categories><comments>22 pages, 1 figure</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent works have characterized the function-space inductive bias of infinite-width bounded-norm single-hidden-layer neural networks as a kind of bounded-variation-type space. This novel neural network Banach space encompasses many classical multivariate function spaces including certain Sobolev spaces and the spectral Barron spaces. Notably, this Banach space also includes functions that exhibit less classical regularity such as those that only vary in a few directions. On bounded domains, it is well-established that the Gaussian reproducing kernel Hilbert space (RKHS) strictly embeds into this Banach space, demonstrating a clear gap between the Gaussian RKHS and the neural network Banach space. It turns out that when investigating these spaces on unbounded domains, e.g., all of $\mathbb{R}^d$, the story is fundamentally different. We establish the following fundamental result: Certain functions that lie in the Gaussian RKHS have infinite norm in the neural network Banach space. This provides a nontrivial gap between kernel methods and neural networks by the exhibition of functions in which kernel methods can do strictly better than neural networks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16336</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16336</id><created>2025-02-22</created><authors><author><keyname>Plassier</keyname><forenames>Vincent</forenames></author><author><keyname>Fishkov</keyname><forenames>Alexander</forenames></author><author><keyname>Dheur</keyname><forenames>Victor</forenames></author><author><keyname>Guizani</keyname><forenames>Mohsen</forenames></author><author><keyname>Taieb</keyname><forenames>Souhaib Ben</forenames></author><author><keyname>Panov</keyname><forenames>Maxim</forenames></author><author><keyname>Moulines</keyname><forenames>Eric</forenames></author></authors><title>Rectifying Conformity Scores for Better Conditional Coverage</title><categories>stat.ML cs.LG math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new method for generating confidence sets within the split conformal prediction framework. Our method performs a trainable transformation of any given conformity score to improve conditional coverage while ensuring exact marginal coverage. The transformation is based on an estimate of the conditional quantile of conformity scores. The resulting method is particularly beneficial for constructing adaptive confidence sets in multi-output problems where standard conformal quantile regression approaches have limited applicability. We develop a theoretical bound that captures the influence of the accuracy of the quantile estimate on the approximate conditional validity, unlike classical bounds for conformal prediction methods that only offer marginal coverage. We experimentally show that our method is highly adaptive to the local data structure and outperforms existing methods in terms of conditional coverage, improving the reliability of statistical inference in various applications. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16355</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16355</id><created>2025-02-22</created><authors><author><keyname>Chakrabarty</keyname><forenames>Deeparnab</forenames></author><author><keyname>Chen</keyname><forenames>Xi</forenames></author><author><keyname>Ristic</keyname><forenames>Simeon</forenames></author><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author><author><keyname>Waingarten</keyname><forenames>Erik</forenames></author></authors><title>Monotonicity Testing of High-Dimensional Distributions with Subcube   Conditioning</title><categories>math.ST cs.CC cs.DM cs.DS cs.LG stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study monotonicity testing of high-dimensional distributions on $\{-1,1\}^n$ in the model of subcube conditioning, suggested and studied by Canonne, Ron, and Servedio~\cite{CRS15} and Bhattacharyya and Chakraborty~\cite{BC18}. Previous work shows that the \emph{sample complexity} of monotonicity testing must be exponential in $n$ (Rubinfeld, Vasilian~\cite{RV20}, and Aliakbarpour, Gouleakis, Peebles, Rubinfeld, Yodpinyanee~\cite{AGPRY19}). We show that the subcube \emph{query complexity} is $\tilde{\Theta}(n/\varepsilon^2)$, by proving nearly matching upper and lower bounds. Our work is the first to use directed isoperimetric inequalities (developed for function monotonicity testing) for analyzing a distribution testing algorithm. Along the way, we generalize an inequality of Khot, Minzer, and Safra~\cite{KMS18} to real-valued functions on $\{-1,1\}^n$.   We also study uniformity testing of distributions that are promised to be monotone, a problem introduced by Rubinfeld, Servedio~\cite{RS09} , using subcube conditioning. We show that the query complexity is $\tilde{\Theta}(\sqrt{n}/\varepsilon^2)$. Our work proves the lower bound, which matches (up to poly-logarithmic factors) the uniformity testing upper bound for general distributions (Canonne, Chen, Kamath, Levi, Waingarten~\cite{CCKLW21}). Hence, we show that monotonicity does not help, beyond logarithmic factors, in testing uniformity of distributions with subcube conditional queries. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16362</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16362</id><created>2025-02-22</created><authors><author><keyname>Philipps</keyname><forenames>Viviane</forenames><affiliation>on behalf of Measurement Error and Misclassification Topic Group</affiliation></author><author><keyname>Freedman</keyname><forenames>Laurence</forenames><affiliation>on behalf of Measurement Error and Misclassification Topic Group</affiliation></author><author><keyname>Deffner</keyname><forenames>Veronika</forenames><affiliation>on behalf of Measurement Error and Misclassification Topic Group</affiliation></author><author><keyname>Helmer</keyname><forenames>Catherine</forenames><affiliation>on behalf of Measurement Error and Misclassification Topic Group</affiliation></author><author><keyname>Boshuizen</keyname><forenames>Hendriek</forenames><affiliation>on behalf of Measurement Error and Misclassification Topic Group</affiliation></author><author><keyname>Thi√©baut</keyname><forenames>Anne C. M.</forenames><affiliation>on behalf of Measurement Error and Misclassification Topic Group</affiliation></author><author><keyname>Proust-Lima</keyname><forenames>C√©cile</forenames><affiliation>on behalf of Measurement Error and Misclassification Topic Group</affiliation></author></authors><title>Including an infrequently measured time-dependent error-prone covariate   in survival analyses: a simulation-based comparison of methods</title><categories>stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Epidemiologic studies often evaluate the association between an exposure and an event risk. When time-varying, exposure updates usually occur at discrete visits although changes are in continuous time and survival models require values to be constantly known. Moreover, exposures are likely measured with error, and their observation truncated at the event time. We aimed to quantify in a Cox regression the bias in the association resulting from intermittent measurements of an error-prone exposure. Using simulations under various scenarios, we compared five methods: last observation carried-forward (LOCF), classical two-stage regression-calibration using measurements up to the event (RC) or also after (PE-RC), multiple imputation (MI) and joint modeling of the exposure and the event (JM). The LOCF, and to a lesser extent the classical RC, showed substantial bias in almost all 43 scenarios. The RC bias was avoided when considering post-event information. The MI performed relatively well, as did the JM. Illustrations exploring the association of Body Mass Index and Executive Functioning with dementia risk showed consistent conclusions. Accounting for measurement error and discrete updates is critical when studying time-varying exposures. MI and JM techniques may be applied in this context, while classical RC should be avoided due to the informative truncation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16388</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16388</id><created>2025-02-22</created><authors><author><keyname>Xie</keyname><forenames>Weian</forenames></author></authors><title>Worst-case Error Bounds for Online Learning of Smooth Functions</title><categories>cs.LG cs.DM cs.DS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online learning is a model of machine learning where the learner is trained on sequential feedback. We investigate worst-case error for the online learning of real functions that have certain smoothness constraints. Suppose that $\mathcal{F}_q$ is the class of all absolutely continuous functions $f: [0, 1] \rightarrow \mathbb{R}$ such that $\|f'\|_q \le 1$, and $\operatorname{opt}_p(\mathcal{F}_q)$ is the best possible upper bound on the sum of the $p^{\text{th}}$ powers of absolute prediction errors for any number of trials guaranteed by any learner. We show that for any $\delta, \epsilon \in (0, 1)$, $\operatorname{opt}_{1+\delta} (\mathcal{F}_{1+\epsilon}) = O(\min(\delta, \epsilon)^{-1})$. Combined with the previous results of Kimber and Long (1995) and Geneson and Zhou (2023), we achieve a complete characterization of the values of $p, q \ge 1$ that result in $\operatorname{opt}_p(\mathcal{F}_q)$ being finite, a problem open for nearly 30 years.   We study the learning scenarios of smooth functions that also belong to certain special families of functions, such as polynomials. We prove a conjecture by Geneson and Zhou (2023) that it is not any easier to learn a polynomial in $\mathcal{F}_q$ than it is to learn any general function in $\mathcal{F}_q$. We also define a noisy model for the online learning of smooth functions, where the learner may receive incorrect feedback up to $\eta \ge 1$ times, denoting the worst-case error bound as $\operatorname{opt}^{\text{nf}}_{p, \eta} (\mathcal{F}_q)$. We prove that $\operatorname{opt}^{\text{nf}}_{p, \eta} (\mathcal{F}_q)$ is finite if and only if $\operatorname{opt}_p(\mathcal{F}_q)$ is. Moreover, we prove for all $p, q \ge 2$ and $\eta \ge 1$ that $\operatorname{opt}^{\text{nf}}_{p, \eta} (\mathcal{F}_q) = \Theta (\eta)$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16391</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16391</id><created>2025-02-22</created><authors><author><keyname>Han</keyname><forenames>Sangil</forenames></author><author><keyname>Kim</keyname><forenames>Kyoowon</forenames></author><author><keyname>Jung</keyname><forenames>Sungkyu</forenames></author></authors><title>Subspace Recovery in Winsorized PCA: Insights into Accuracy and   Robustness</title><categories>stat.ML cs.LG stat.AP stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper, we explore the theoretical properties of subspace recovery using Winsorized Principal Component Analysis (WPCA), utilizing a common data transformation technique that caps extreme values to mitigate the impact of outliers. Despite the widespread use of winsorization in various tasks of multivariate analysis, its theoretical properties, particularly for subspace recovery, have received limited attention. We provide a detailed analysis of the accuracy of WPCA, showing that increasing the number of samples while decreasing the proportion of outliers guarantees the consistency of the sample subspaces from WPCA with respect to the true population subspace. Furthermore, we establish perturbation bounds that ensure the WPCA subspace obtained from contaminated data remains close to the subspace recovered from pure data. Additionally, we extend the classical notion of breakdown points to subspace-valued statistics and derive lower bounds for the breakdown points of WPCA. Our analysis demonstrates that WPCA exhibits strong robustness to outliers while maintaining consistency under mild assumptions. A toy example is provided to numerically illustrate the behavior of the upper bounds for perturbation bounds and breakdown points, emphasizing winsorization's utility in subspace recovery. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16445</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16445</id><created>2025-02-23</created><authors><author><keyname>Haber</keyname><forenames>Eldad</forenames></author><author><keyname>Ahamed</keyname><forenames>Shadab</forenames></author><author><keyname>Siddiqui</keyname><forenames>Md. Shahriar Rahim</forenames></author><author><keyname>Zakariaei</keyname><forenames>Niloufar</forenames></author><author><keyname>Eliasof</keyname><forenames>Moshe</forenames></author></authors><title>Iterative Flow Matching -- Path Correction and Gradual Refinement for   Enhanced Generative Modeling</title><categories>cs.LG cs.AI cs.CV stat.ML</categories><comments>17 pages, 8 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Generative models for image generation are now commonly used for a wide variety of applications, ranging from guided image generation for entertainment to solving inverse problems. Nonetheless, training a generator is a non-trivial feat that requires fine-tuning and can lead to so-called hallucinations, that is, the generation of images that are unrealistic. In this work, we explore image generation using flow matching. We explain and demonstrate why flow matching can generate hallucinations, and propose an iterative process to improve the generation process. Our iterative process can be integrated into virtually $\textit{any}$ generative modeling technique, thereby enhancing the performance and robustness of image synthesis systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16462</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16462</id><created>2025-02-23</created><authors><author><keyname>H√∏gsgaard</keyname><forenames>Mikael M√∏ller</forenames></author><author><keyname>Larsen</keyname><forenames>Kasper Green</forenames></author></authors><title>Improved Margin Generalization Bounds for Voting Classifiers</title><categories>cs.LG cs.DS math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we establish a new margin-based generalization bound for voting classifiers, refining existing results and yielding tighter generalization guarantees for widely used boosting algorithms such as AdaBoost (Freund and Schapire, 1997). Furthermore, the new margin-based generalization bound enables the derivation of an optimal weak-to-strong learner: a Majority-of-3 large-margin classifiers with an expected error matching the theoretical lower bound. This result provides a more natural alternative to the Majority-of-5 algorithm by (H\o gsgaard et al. 2024) , and matches the Majority-of-3 result by (Aden-Ali et al. 2024) for the realizable prediction model. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16504</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16504</id><created>2025-02-23</created><authors><author><keyname>Wang</keyname><forenames>Lijia</forenames></author><author><keyname>Han</keyname><forenames>Xiao</forenames></author><author><keyname>Wu</keyname><forenames>Yanhui</forenames></author><author><keyname>Wang</keyname><forenames>Y. X. Rachel</forenames></author></authors><title>Local Information for Global Network Estimation in Latent Space Models</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In social networks, neighborhood is crucial for understanding individual behavior in response to environments, and thus it is essential to analyze an individual's local perspective within the global network. This paper studies how to utilize a partial information network centered around a given individual for global network estimation by fitting a general latent space model. Compared to the entire network, the partial information network contains a significant proportion of missing edges with its structure depending on a random, potentially sparse neighborhood, posing significant challenges for estimation. We address the challenges by proposing a projected gradient descent algorithm for maximizing the likelihood of the observed data and develop theoretical guarantees for its convergence under different neighborhood structures. Our convergence rates and estimation error bounds highlight the impact of bias in an individual's local view of the global network, and we further show that the bias can be quantified with an imbalance measure. Using simulated and real networks, we demonstrate the performance of our estimation method and how our approach enables researchers to gain additional insights into the structure of social networks, such as the tradeoff between degrees and imbalance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16513</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16513</id><created>2025-02-23</created><authors><author><keyname>Qin</keyname><forenames>Jing</forenames></author><author><keyname>Liu</keyname><forenames>Yukun</forenames></author><author><keyname>Li</keyname><forenames>Moming</forenames></author><author><keyname>Huang</keyname><forenames>Chiung-Yu</forenames></author></authors><title>Revamping Conformal Selection With Optimal Power: A Neyman--Pearson   Perspective</title><categories>stat.ME</categories><comments>34 pages, 4 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper introduces a novel conformal selection procedure, inspired by the Neyman--Pearson paradigm, to maximize the power of selecting qualified units while maintaining false discovery rate (FDR) control. Existing conformal selection methods may yield suboptimal power due to their reliance on conformal p-values, which are derived by substituting unobserved future outcomes with thresholds set by the null hypothesis. This substitution invalidates the exchangeability between imputed nonconformity scores for test data and those derived from calibration data, resulting in reduced power. In contrast, our approach circumvents the need for conformal p-values by constructing a likelihood-ratio-based decision rule that directly utilizes observed covariates from both calibration and test samples. The asymptotic optimality and FDR control of the proposed method are established under a correctly specified model, and modified selection procedures are introduced to improve power under model misspecification. The proposed methods are computationally efficient and can be readily extended to handle covariate shifts, making them well-suited for real-world applications. Simulation results show that these methods consistently achieve comparable or higher power than existing conformal p-value-based selection rules, particularly when the underlying distribution deviates from location-shift models, while effectively maintaining FDR control. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16542</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16542</id><created>2025-02-23</created><authors><author><keyname>Tyralis</keyname><forenames>Hristos</forenames></author><author><keyname>Papacharalampous</keyname><forenames>Georgia</forenames></author></authors><title>Transformations of predictions and realizations in consistent scoring   functions</title><categories>stat.ML cs.LG</categories><comments>28 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Scoring functions constructed by transforming the realization and prediction variables of (strictly) consistent scoring functions have been widely studied empirically, yet their theoretical foundations remain unexplored. To address this gap, we establish formal characterizations of (strict) consistency for these transformed scoring functions and their elicitable functionals. Our analysis focuses on two interrelated cases: (a) transformations applied exclusively to the realization variable, and (b) bijective transformations applied jointly to both realization and prediction variables. We formulate analogous characterizations for (strict) identification functions. The resulting theoretical framework is broadly applicable to statistical and machine learning methodologies. When applied to Bregman and expectile scoring functions, our framework shows how it enables two critical advances: (a) rigorous interpretation of prior empirical findings from models trained with transformed scoring functions, and (b) systematic construction of novel identifiable and elicitable functionals, specifically the g-transformed expectation and g-transformed expectile. By unifying theoretical insights with practical applications, this work advances principled methodologies for designing scoring functions in complex predictive tasks. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16556</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16556</id><created>2025-02-23</created><authors><author><keyname>Kuzmanko</keyname><forenames>Jonathan</forenames></author></authors><title>Beyond Words: How Large Language Models Perform in Quantitative   Management Problem-Solving</title><categories>cs.CL cs.AI cs.LG stat.AP</categories><comments>28 pages, 5 figures, 19 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study examines how Large Language Models (LLMs) perform when tackling quantitative management decision problems in a zero-shot setting. Drawing on 900 responses generated by five leading models across 20 diverse managerial scenarios, our analysis explores whether these base models can deliver accurate numerical decisions under varying presentation formats, scenario complexities, and repeated attempts. Contrary to prior findings, we observed no significant effects of text presentation format (direct, narrative, or tabular) or text length on accuracy. However, scenario complexity -- particularly in terms of constraints and irrelevant parameters -- strongly influenced performance, often degrading accuracy. Surprisingly, the models handled tasks requiring multiple solution steps more effectively than expected. Notably, only 28.8\% of responses were exactly correct, highlighting limitations in precision. We further found no significant ``learning effect'' across iterations: performance remained stable across repeated queries. Nonetheless, significant variations emerged among the five tested LLMs, with some showing superior binary accuracy. Overall, these findings underscore both the promise and the pitfalls of harnessing LLMs for complex quantitative decision-making, informing managers and researchers about optimal deployment strategies. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16574</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16574</id><created>2025-02-23</created><authors><author><keyname>Ndoye</keyname><forenames>Mouhamed</forenames></author><author><keyname>Diop</keyname><forenames>Aba</forenames></author></authors><title>Regularized zero-inflated Bernoulli regression model</title><categories>stat.ME math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Logistic regression model is widely used in many studies to investigate the relationship between a binary response variable Y and a set of potential predictors $X_1,\ldots, X_p$ (for example: $Y = 1$ if the outcome occurred and $Y = 0$ otherwise). One problem arising then is that, a proportion of the study subjects cannot experience the outcome of interest. This leads to an excessive presence of zeros in the study sample. This article is interested in estimating parameters of the zero-inflated Bernouilli regression model in a high-dimensional setting, i.e. with a large number of regressors. We use particulary Ridge regression and the Lasso which are typically achieved by constraining the weights of the model. and are useful when the number of predictors is much bigger than the number of observations. We establish the existency, consistency and asymptotic normality of the proposed regularized estimator. Then, we conduct a simulation study to investigate its finite-sample behavior, and application to real data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16591</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16591</id><created>2025-02-23</created><authors><author><keyname>Chen</keyname><forenames>Cong</forenames></author><author><keyname>Huang</keyname><forenames>Mo</forenames></author></authors><title>Adjustment for Inconsistency in Adaptive Phase 2/3 Designs with Dose   Optimization</title><categories>stat.AP</categories><comments>13 pages, 2 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Adaptive Phase 2/3 designs hold great promise in contemporary oncology drug development, especially when limited data from Phase 1 dose-finding is insufficient for identifying an optimal dose. However, there is a general concern about inconsistent results before and after the adaptation. The imperfection in dose selection further complicates the issue. In this paper, we explicitly incorporate the concerns about inconsistency into the statistical analysis under three hypothesis testing strategies. This investigation paves the way for further research in a less explored area. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16594</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16594</id><created>2025-02-23</created><authors><author><keyname>Pan</keyname><forenames>Sheng</forenames></author></authors><title>Robust transfer regression with corrupted labels</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In this paper, we introduce a robust transfer regression method designed to handle corrupted labels in target data, under the scenarios that the corruption affects a substantial portion of the labels and the locations of these corruptions are unknown. Theoretical analysis substantiates our approach, illustrating that the estimation error consists of three components: the first relates to the source data; the second encompasses the domain shift ; and the third captures the estimation error attributed to the corrupted vector. Our theoretical framework ensures that the proposed method surpasses estimations based solely on target data. We validate our method through numerical experiments aimed at reconstructing corrupted compressed signals. Additionally, we apply our method to analyze the association between O6-methylguanine-DNA methyltransferase (MGMT) methylation and gene expression in Glioblastoma (GBM) patients. Keywords: </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16617</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16617</id><created>2025-02-23</created><authors><author><keyname>Kang</keyname><forenames>Lulu</forenames></author><author><keyname>Xu</keyname><forenames>Minshen</forenames></author></authors><title>Optimal Kernel Learning for Gaussian Process Models with   High-Dimensional Input</title><categories>cs.LG stat.ME stat.ML</categories><comments>30 pages, 7 tables, 8 figures</comments><acm-class>G.3</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Gaussian process (GP) regression is a popular surrogate modeling tool for computer simulations in engineering and scientific domains. However, it often struggles with high computational costs and low prediction accuracy when the simulation involves too many input variables. For some simulation models, the outputs may only be significantly influenced by a small subset of the input variables, referred to as the ``active variables''. We propose an optimal kernel learning approach to identify these active variables, thereby overcoming GP model limitations and enhancing system understanding. Our method approximates the original GP model's covariance function through a convex combination of kernel functions, each utilizing low-dimensional subsets of input variables. Inspired by the Fedorov-Wynn algorithm from optimal design literature, we develop an optimal kernel learning algorithm to determine this approximation. We incorporate the effect heredity principle, a concept borrowed from the field of ``design and analysis of experiments'', to ensure sparsity in active variable selection. Through several examples, we demonstrate that the proposed method outperforms alternative approaches in correctly identifying active input variables and improving prediction accuracy. It is an effective solution for interpreting the surrogate GP regression and simplifying the complex underlying system. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16637</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16637</id><created>2025-02-23</created><authors><author><keyname>Cai</keyname><forenames>Ruichu</forenames></author><author><keyname>Huang</keyname><forenames>Junxian</forenames></author><author><keyname>Yang</keyname><forenames>Zhenhui</forenames></author><author><keyname>Li</keyname><forenames>Zijian</forenames></author><author><keyname>Eldele</keyname><forenames>Emadeldeen</forenames></author><author><keyname>Wu</keyname><forenames>Min</forenames></author><author><keyname>Sun</keyname><forenames>Fuchun</forenames></author></authors><title>Time Series Domain Adaptation via Latent Invariant Causal Mechanism</title><categories>cs.LG cs.AI stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time series domain adaptation aims to transfer the complex temporal dependence from the labeled source domain to the unlabeled target domain. Recent advances leverage the stable causal mechanism over observed variables to model the domain-invariant temporal dependence. However, modeling precise causal structures in high-dimensional data, such as videos, remains challenging. Additionally, direct causal edges may not exist among observed variables (e.g., pixels). These limitations hinder the applicability of existing approaches to real-world scenarios. To address these challenges, we find that the high-dimension time series data are generated from the low-dimension latent variables, which motivates us to model the causal mechanisms of the temporal latent process. Based on this intuition, we propose a latent causal mechanism identification framework that guarantees the uniqueness of the reconstructed latent causal structures. Specifically, we first identify latent variables by utilizing sufficient changes in historical information. Moreover, by enforcing the sparsity of the relationships of latent variables, we can achieve identifiable latent causal structures. Built on the theoretical results, we develop the Latent Causality Alignment (LCA) model that leverages variational inference, which incorporates an intra-domain latent sparsity constraint for latent structure reconstruction and an inter-domain latent sparsity constraint for domain-invariant structure reconstruction. Experiment results on eight benchmarks show a general improvement in the domain-adaptive time series classification and forecasting tasks, highlighting the effectiveness of our method in real-world scenarios. Codes are available at https://github.com/DMIRLAB-Group/LCA. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16658</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16658</id><created>2025-02-23</created><authors><author><keyname>Gao</keyname><forenames>Chao</forenames></author><author><keyname>Shan</keyname><forenames>Liren</forenames></author><author><keyname>Srinivas</keyname><forenames>Vaidehi</forenames></author><author><keyname>Vijayaraghavan</keyname><forenames>Aravindan</forenames></author></authors><title>Volume Optimality in Conformal Prediction with Structured Prediction   Sets</title><categories>cs.LG stat.ML</categories><comments>41 pages, 19 figures, 2 tables</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Conformal Prediction is a widely studied technique to construct prediction sets of future observations. Most conformal prediction methods focus on achieving the necessary coverage guarantees, but do not provide formal guarantees on the size (volume) of the prediction sets. We first prove an impossibility of volume optimality where any distribution-free method can only find a trivial solution. We then introduce a new notion of volume optimality by restricting the prediction sets to belong to a set family (of finite VC-dimension), specifically a union of $k$-intervals. Our main contribution is an efficient distribution-free algorithm based on dynamic programming (DP) to find a union of $k$-intervals that is guaranteed for any distribution to have near-optimal volume among all unions of $k$-intervals satisfying the desired coverage property. By adopting the framework of distributional conformal prediction (Chernozhukov et al., 2021), the new DP based conformity score can also be applied to achieve approximate conditional coverage and conditional restricted volume optimality, as long as a reasonable estimator of the conditional CDF is available. While the theoretical results already establish volume-optimality guarantees, they are complemented by experiments that demonstrate that our method can significantly outperform existing methods in many settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16659</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16659</id><created>2025-02-23</created><authors><author><keyname>Song</keyname><forenames>Eunhye</forenames></author><author><keyname>Kim</keyname><forenames>Taeho</forenames></author></authors><title>Optimizing Input Data Collection for Ranking and Selection</title><categories>stat.ME math.OC stat.ML</categories><comments>48 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study a ranking and selection (R&amp;S) problem when all solutions share common parametric Bayesian input models updated with the data collected from multiple independent data-generating sources. Our objective is to identify the best system by designing a sequential sampling algorithm that collects input and simulation data given a budget. We adopt the most probable best (MPB) as the estimator of the optimum and show that its posterior probability of optimality converges to one at an exponential rate as the sampling budget increases. Assuming that the input parameters belong to a finite set, we characterize the $\epsilon$-optimal static sampling ratios for input and simulation data that maximize the convergence rate. Using these ratios as guidance, we propose the optimal sampling algorithm for R&amp;S (OSAR) that achieves the $\epsilon$-optimal ratios almost surely in the limit. We further extend OSAR by adopting the kernel ridge regression to improve the simulation output mean prediction. This not only improves OSAR's finite-sample performance, but also lets us tackle the case where the input parameters lie in a continuous space with a strong consistency guarantee for finding the optimum. We numerically demonstrate that OSAR outperforms a state-of-the-art competitor. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16673</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16673</id><created>2025-02-23</created><authors><author><keyname>Smucler</keyname><forenames>Ezequiel</forenames></author><author><keyname>Robins</keyname><forenames>James M.</forenames></author><author><keyname>Rotnitzky</keyname><forenames>Andrea</forenames></author></authors><title>On the asymptotic validity of confidence sets for linear functionals of   solutions to integral equations</title><categories>math.ST stat.ME stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper examines the construction of confidence sets for parameters defined as a linear functional of the solution to an integral equation involving conditional expectations. We show that any confidence set uniformly valid over a broad class of probability laws, allowing the integral equation to be arbitrarily ill-posed must have, with high probability under some laws, a diameter at least as large as the diameter of the parameter's range over the model. Additionally, we establish that uniformly consistent estimators of the parameter do not exist. We show that, consistent with the weak instruments literature, Wald confidence intervals are not uniformly valid. Furthermore, we argue that inverting the score test, a successful approach in that literature, does not extend to the broader class of parameters considered here. We present a method for constructing uniformly valid confidence sets in the special case where all variables are binary and discuss its limitations. Finally, we emphasize that developing uniformly valid confidence sets for the general class of parameters considered in this paper remains an open problem. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16728</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16728</id><created>2025-02-23</created><authors><author><keyname>Jin</keyname><forenames>Jiashun</forenames></author><author><keyname>Wang</keyname><forenames>Jingming</forenames></author></authors><title>Fitting networks with a cancellation trick</title><categories>stat.ME stat.ML</categories><comments>33 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The degree-corrected block model (DCBM), latent space model (LSM), and $\beta$-model are all popular network models. We combine their modeling ideas and propose the logit-DCBM as a new model. Similar as the $\beta$-model and LSM, the logit-DCBM contains nonlinear factors, where fitting the parameters is a challenging open problem. We resolve this problem by introducing a cancellation trick. We also propose R-SCORE as a recursive community detection algorithm, where in each iteration, we first use the idea above to update our parameter estimation, and then use the results to remove the nonlinear factors in the logit-DCBM so the renormalized model approximately satisfies a low-rank model, just like the DCBM. Our numerical study suggests that R-SCORE significantly improves over existing spectral approaches in many cases. Also, theoretically, we show that the Hamming error rate of R-SCORE is faster than that of SCORE in a specific sparse region, and is at least as fast outside this region. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16758</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16758</id><created>2025-02-23</created><authors><author><keyname>Zhang</keyname><forenames>Zhenyuan</forenames></author><author><keyname>Luo</keyname><forenames>Hengrui</forenames></author></authors><title>Minimax Decision Trees via Martingale Approximations</title><categories>math.ST math.PR stat.TH</categories><comments>36 pages, 6 figures</comments><msc-class>62G08, 60G42</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a martingale-based approach to constructing decision trees that efficiently approximate a target variable through recursive conditioning. We introduce MinimaxSplit, a novel splitting criterion that minimizes the worst-case variance at each step, and analyze its cyclic variant, proving an exponential error decay rate under mild conditions. Our analysis builds upon partition-based martingale approximations, providing new insights into their convergence behavior. Unlike traditional variance-based methods, MinimaxSplit avoids end-cut preference and performs well in noisy settings. We derive empirical risk bounds and also explore its integration into random forests. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16773</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16773</id><created>2025-02-23</created><authors><author><keyname>Han</keyname><forenames>Fuqun</forenames></author><author><keyname>Osher</keyname><forenames>Stanley</forenames></author><author><keyname>Li</keyname><forenames>Wuchen</forenames></author></authors><title>Splitting Regularized Wasserstein Proximal Algorithms for Nonsmooth   Sampling Problems</title><categories>stat.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling from nonsmooth target probability distributions is essential in various applications, including the Bayesian Lasso. We propose a splitting-based sampling algorithm for the time-implicit discretization of the probability flow for the Fokker-Planck equation, where the score function defined as the gradient logarithm of the current probability density function, is approximated by the regularized Wasserstein proximal. When the prior distribution is the Laplace prior, our algorithm is explicitly formulated as a deterministic interacting particle system, incorporating softmax operators and shrinkage operations to efficiently compute the gradient drift vector field and the score function. The proposed formulation introduces a particular class of attention layers in transformer structures, which can sample sparse target distributions. We verify the convergence towards target distributions regarding R\'enyi divergences under suitable conditions. Numerical experiments in high-dimensional nonsmooth sampling problems, such as sampling from mixed Gaussian and Laplace distributions, logistic regressions, image restoration with L1-TV regularization, and Bayesian neural networks, demonstrate the efficiency and robust performance of the proposed method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16785</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16785</id><created>2025-02-23</created><authors><author><keyname>Nguyen</keyname><forenames>Michele</forenames></author><author><keyname>Rabonza</keyname><forenames>Maricar</forenames></author><author><keyname>Lallemant</keyname><forenames>David</forenames></author></authors><title>Weighted model calibration with spatial conditional information</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Cost functions such as mean square error are often used in environmental model calibration. These treat observations as independent and equally important even though model residuals exhibit spatial dependence and additional observations near existing points do not provide as much information on the system as those elsewhere. To address this issue, we develop a method to derive calibration weights based on spatial conditional information. Using simulation experiments with Gaussian processes and the Tephra2 volcanic tephra dispersion model, we show that the additional accuracy and precision from weighted inference increases with the degree of observation clustering and spatial dependence present. To demonstrate real-world relevance, the methods are applied to tephra load observations from the 2014 eruption of the Kelud volcano in Indonesia. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16808</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16808</id><created>2025-02-23</created><authors><author><keyname>Chada</keyname><forenames>Neil K.</forenames></author></authors><title>Multilevel Localized Ensemble Kalman Bucy Filters</title><categories>stat.CO</categories><comments>arXiv admin note: substantial text overlap with arXiv:2108.03935</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this article we propose and develop a new methodology which is inspired from Kalman filtering and multilevel Monte Carlo (MLMC), entitle the multilevel localized ensemble Kalman--Bucy Filter (MLLEnKBF). Based on the work of Chada et al. \cite{CJY20}, we provide an important extension on this which is to include the technique of covariance localization. Localization is important as it can induce stability and remove long spurious correlations, particularly with a small ensemble size. Our resulting algorithm is used for both state and parameter estimation, for the later we exploit our method for normalizing constant estimation. As of yet, MLMC has only been applied to localized data assimilation methods in a discrete-time setting, therefore this work acts as a first in the continuous-time setting. Numerical results indicate its performance, and benefit through a range of model problems, which include a linear Ornstein--Uhlenbeck process, of moderately high dimension, and the Lorenz 96 model, for parameter estimation. Our results demonstrate improved stability, and that with MLMC, one can reduce the computational complexity to attain an order is MSE $\mathcal{O}(\epsilon^2)$, for $\epsilon&gt;0$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16816</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16816</id><created>2025-02-23</created><authors><author><keyname>Xu</keyname><forenames>Yang</forenames></author><author><keyname>Mondal</keyname><forenames>Washim Uddin</forenames></author><author><keyname>Aggarwal</keyname><forenames>Vaneet</forenames></author></authors><title>Finite-Sample Analysis of Policy Evaluation for Robust Average Reward   Reinforcement Learning</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first finite-sample analysis for policy evaluation in robust average-reward Markov Decision Processes (MDPs). Prior works in this setting have established only asymptotic convergence guarantees, leaving open the question of sample complexity. In this work, we address this gap by establishing that the robust Bellman operator is a contraction under the span semi-norm, and developing a stochastic approximation framework with controlled bias. Our approach builds upon Multi-Level Monte Carlo (MLMC) techniques to estimate the robust Bellman operator efficiently. To overcome the infinite expected sample complexity inherent in standard MLMC, we introduce a truncation mechanism based on a geometric distribution, ensuring a finite constant sample complexity while maintaining a small bias that decays exponentially with the truncation level. Our method achieves the order-optimal sample complexity of $\tilde{\mathcal{O}}(\epsilon^{-2})$ for robust policy evaluation and robust average reward estimation, marking a significant advancement in robust reinforcement learning theory. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16824</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16824</id><created>2025-02-23</created><authors><author><keyname>Yun</keyname><forenames>Taeyoung</forenames></author><author><keyname>Om</keyname><forenames>Kiyoung</forenames></author><author><keyname>Lee</keyname><forenames>Jaewoo</forenames></author><author><keyname>Yun</keyname><forenames>Sujin</forenames></author><author><keyname>Park</keyname><forenames>Jinkyoo</forenames></author></authors><title>Posterior Inference with Diffusion Models for High-dimensional Black-box   Optimization</title><categories>cs.LG stat.ML</categories><comments>21 pages, 12 figures, 5 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Optimizing high-dimensional and complex black-box functions is crucial in numerous scientific applications. While Bayesian optimization (BO) is a powerful method for sample-efficient optimization, it struggles with the curse of dimensionality and scaling to thousands of evaluations. Recently, leveraging generative models to solve black-box optimization problems has emerged as a promising framework. However, those methods often underperform compared to BO methods due to limited expressivity and difficulty of uncertainty estimation in high-dimensional spaces. To overcome these issues, we introduce \textbf{DiBO}, a novel framework for solving high-dimensional black-box optimization problems. Our method iterates two stages. First, we train a diffusion model to capture the data distribution and an ensemble of proxies to predict function values with uncertainty quantification. Second, we cast the candidate selection as a posterior inference problem to balance exploration and exploitation in high-dimensional spaces. Concretely, we fine-tune diffusion models to amortize posterior inference. Extensive experiments demonstrate that our method outperforms state-of-the-art baselines across various synthetic and real-world black-box optimization tasks. Our code is publicly available \href{https://github.com/umkiyoung/DiBO}{here} </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16831</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16831</id><created>2025-02-23</created><authors><author><keyname>Eguchi</keyname><forenames>Shinto</forenames></author><author><keyname>Kato</keyname><forenames>Shogo</forenames></author></authors><title>Minimum Copula Divergence for Robust Estimation</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a robust estimation framework based solely on the copula function. We begin by introducing a family of divergence measures tailored for copulas, including the \(\alpha\)-, \(\beta\)-, and \(\gamma\)-copula divergences, which quantify the discrepancy between a parametric copula model and an empirical copula derived from data independently of marginal specifications. Using these divergence measures, we propose the minimum copula divergence estimator (MCDE), an estimation method that minimizes the divergence between the model and the empirical copula. The framework proves particularly effective in addressing model misspecifications and analyzing heavy-tailed data, where traditional methods such as the maximum likelihood estimator (MLE) may fail. Theoretical results show that common copula families, including Archimedean and elliptical copulas, satisfy conditions ensuring the boundedness of divergence-based estimators, thereby guaranteeing the robustness of MCDE, especially in the presence of extreme observations. Numerical examples further underscore MCDE's ability to adapt to varying dependence structures, ensuring its utility in real-world scenarios. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16849</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16849</id><created>2025-02-24</created><authors><author><keyname>Jones-McCormick</keyname><forenames>Taj</forenames></author><author><keyname>Jagannath</keyname><forenames>Aukosh</forenames></author><author><keyname>Sen</keyname><forenames>Subhabrata</forenames></author></authors><title>Provable Benefits of Unsupervised Pre-training and Transfer Learning via   Single-Index Models</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Unsupervised pre-training and transfer learning are commonly used techniques to initialize training algorithms for neural networks, particularly in settings with limited labeled data. In this paper, we study the effects of unsupervised pre-training and transfer learning on the sample complexity of high-dimensional supervised learning. Specifically, we consider the problem of training a single-layer neural network via online stochastic gradient descent. We establish that pre-training and transfer learning (under concept shift) reduce sample complexity by polynomial factors (in the dimension) under very general assumptions. We also uncover some surprising settings where pre-training grants exponential improvement over random initialization in terms of sample complexity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16870</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16870</id><created>2025-02-24</created><authors><author><keyname>Takeno</keyname><forenames>Shion</forenames></author><author><keyname>Okura</keyname><forenames>Yoshito</forenames></author><author><keyname>Inatsu</keyname><forenames>Yu</forenames></author><author><keyname>Tatsuya</keyname><forenames>Aoyama</forenames></author><author><keyname>Tanaka</keyname><forenames>Tomonari</forenames></author><author><keyname>Satoshi</keyname><forenames>Akahane</forenames></author><author><keyname>Hanada</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Hashimoto</keyname><forenames>Noriaki</forenames></author><author><keyname>Murayama</keyname><forenames>Taro</forenames></author><author><keyname>Lee</keyname><forenames>Hanju</forenames></author><author><keyname>Kojima</keyname><forenames>Shinya</forenames></author><author><keyname>Takeuchi</keyname><forenames>Ichiro</forenames></author></authors><title>Distributionally Robust Active Learning for Gaussian Process Regression</title><categories>cs.LG stat.ML</categories><comments>25 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Gaussian process regression (GPR) or kernel ridge regression is a widely used and powerful tool for nonlinear prediction. Therefore, active learning (AL) for GPR, which actively collects data labels to achieve an accurate prediction with fewer data labels, is an important problem. However, existing AL methods do not theoretically guarantee prediction accuracy for target distribution. Furthermore, as discussed in the distributionally robust learning literature, specifying the target distribution is often difficult. Thus, this paper proposes two AL methods that effectively reduce the worst-case expected error for GPR, which is the worst-case expectation in target distribution candidates. We show an upper bound of the worst-case expected squared error, which suggests that the error will be arbitrarily small by a finite number of data labels under mild conditions. Finally, we demonstrate the effectiveness of the proposed methods through synthetic and real-world datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16874</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16874</id><created>2025-02-24</created><authors><author><keyname>Zito</keyname><forenames>John</forenames></author><author><keyname>Kowal</keyname><forenames>Daniel R.</forenames></author></authors><title>A dynamic copula model for probabilistic forecasting of non-Gaussian   multivariate time series</title><categories>stat.ME</categories><comments>49 pages, 10 figures, 4 tables</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Multivariate time series (MTS) data often include a heterogeneous mix of non-Gaussian distributional features (asymmetry, multimodality, heavy tails) and data types (continuous and discrete variables). Traditional MTS methods based on convenient parametric distributions are typically ill-equipped to model this heterogeneity. Copula models provide an appealing alternative, but present significant obstacles for fully Bayesian inference and probabilistic forecasting. To overcome these challenges, we propose a novel and general strategy for posterior approximation in MTS copula models and apply it to a Gaussian copula built from a dynamic factor model. This framework provides scalable, fully Bayesian inference for cross-sectional and serial dependencies and nonparametrically learns heterogeneous marginal distributions. We validate this approach by establishing posterior consistency and confirm excellent finite-sample performance even under model misspecification using simulated data. We apply our method to crime count and macroeconomic MTS data and find superior probabilistic forecasting performance compared to popular MTS models. These results demonstrate that the proposed method is a versatile, general-purpose utility for probabilistic forecasting of MTS that works well across of range of applications with minimal user input. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16888</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16888</id><created>2025-02-24</created><authors><author><keyname>Cao</keyname><forenames>Jiahao</forenames></author><author><keyname>He</keyname><forenames>Shiyuan</forenames></author><author><keyname>Zhang</keyname><forenames>Bohai</forenames></author></authors><title>Functional Bayesian Additive Regression Trees with Shape Constraints</title><categories>stat.ME stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the great success of Bayesian additive regression trees (BART) on regression, we propose a nonparametric Bayesian approach for the function-on-scalar regression problem, termed as Functional BART (FBART). Utilizing spline-based function representation and tree-based domain partition model, FBART offers great flexibility in characterizing the complex and heterogeneous relationship between the response curve and scalar covariates. We devise a tailored Bayesian backfitting algorithm for estimating the parameters in the FBART model. Furthermore, we introduce an FBART model with shape constraints on the response curve, enhancing estimation and prediction performance when prior shape information of response curves is available. By incorporating a shape-constrained prior, we ensure that the posterior samples of the response curve satisfy the required shape constraints (e.g., monotonicity and/or convexity). Our proposed FBART model and its shape-constrained version are the new advances of BART models for functional data. Under certain regularity conditions, we derive the posterior convergence results for both FBART and its shape-constrained version. Finally, the superiority of the proposed methods over other competitive counterparts is validated through simulation experiments under various settings and analyses of two real datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16916</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16916</id><created>2025-02-24</created><authors><author><keyname>Al-Ghattas</keyname><forenames>Omar</forenames></author><author><keyname>Chen</keyname><forenames>Jiaheng</forenames></author><author><keyname>Sanz-Alonso</keyname><forenames>Daniel</forenames></author></authors><title>Sharp Concentration of Simple Random Tensors</title><categories>math.PR math.ST stat.TH</categories><comments>36 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper establishes sharp dimension-free concentration inequalities and expectation bounds for the deviation of the sum of simple random tensors from its expectation. As part of our analysis, we use generic chaining techniques to obtain a sharp high-probability upper bound on the suprema of multi-product empirical processes. In so doing, we generalize classical results for quadratic and product empirical processes to higher-order settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16933</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16933</id><created>2025-02-24</created><authors><author><keyname>Rathore</keyname><forenames>Vidhi</forenames></author><author><keyname>Manwani</keyname><forenames>Naresh</forenames></author></authors><title>Achieving Fair PCA Using Joint Eigenvalue Decomposition</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Principal Component Analysis (PCA) is a widely used method for dimensionality reduction, but it often overlooks fairness, especially when working with data that includes demographic characteristics. This can lead to biased representations that disproportionately affect certain groups. To address this issue, our approach incorporates Joint Eigenvalue Decomposition (JEVD), a technique that enables the simultaneous diagonalization of multiple matrices, ensuring fair and efficient representations. We formally show that the optimal solution of JEVD leads to a fair PCA solution. By integrating JEVD with PCA, we strike an optimal balance between preserving data structure and promoting fairness across diverse groups. We demonstrate that our method outperforms existing baseline approaches in fairness and representational quality on various datasets. It retains the core advantages of PCA while ensuring that sensitive demographic attributes do not create disparities in the reduced representation. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16936</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16936</id><created>2025-02-24</created><authors><author><keyname>Serr√†</keyname><forenames>Joan</forenames></author><author><keyname>Araz</keyname><forenames>R. Oguz</forenames></author><author><keyname>Bogdanov</keyname><forenames>Dmitry</forenames></author><author><keyname>Mitsufuji</keyname><forenames>Yuki</forenames></author></authors><title>Supervised contrastive learning from weakly-labeled audio segments for   musical version matching</title><categories>cs.SD cs.AI cs.LG eess.AS stat.ML</categories><comments>15 pages, 6 figures, 7 tables; includes Appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting musical versions (different renditions of the same piece) is a challenging task with important applications. Because of the ground truth nature, existing approaches match musical versions at the track level (e.g., whole song). However, most applications require to match them at the segment level (e.g., 20s chunks). In addition, existing approaches resort to classification and triplet losses, disregarding more recent losses that could bring meaningful improvements. In this paper, we propose a method to learn from weakly annotated segments, together with a contrastive loss variant that outperforms well-studied alternatives. The former is based on pairwise segment distance reductions, while the latter modifies an existing loss following decoupling, hyper-parameter, and geometric considerations. With these two elements, we do not only achieve state-of-the-art results in the standard track-level evaluation, but we also obtain a breakthrough performance in a segment-level evaluation. We believe that, due to the generality of the challenges addressed here, the proposed methods may find utility in domains beyond audio or musical version matching. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16948</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16948</id><created>2025-02-24</created><authors><author><keyname>Choi</keyname><forenames>Hansung</forenames></author><author><keyname>Seo</keyname><forenames>Daewon</forenames></author></authors><title>Deep Minimax Classifiers for Imbalanced Datasets with a Small Number of   Minority Samples</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of a minimax classifier is well-established in statistical decision theory, but its implementation via neural networks remains challenging, particularly in scenarios with imbalanced training data having a limited number of samples for minority classes. To address this issue, we propose a novel minimax learning algorithm designed to minimize the risk of worst-performing classes. Our algorithm iterates through two steps: a minimization step that trains the model based on a selected target prior, and a maximization step that updates the target prior towards the adversarial prior for the trained model. In the minimization, we introduce a targeted logit-adjustment loss function that efficiently identifies optimal decision boundaries under the target prior. Moreover, based on a new prior-dependent generalization bound that we obtained, we theoretically prove that our loss function has a better generalization capability than existing loss functions. During the maximization, we refine the target prior by shifting it towards the adversarial prior, depending on the worst-performing classes rather than on per-class risk estimates. Our maximization method is particularly robust in the regime of a small number of samples. Additionally, to adapt to overparameterized neural networks, we partition the entire training dataset into two subsets: one for model training during the minimization step and the other for updating the target prior during the maximization step. Our proposed algorithm has a provable convergence property, and empirical results indicate that our algorithm performs better than or is comparable to existing methods. All codes are publicly available at https://github.com/hansung-choi/TLA-linear-ascent. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16977</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16977</id><created>2025-02-24</created><authors><author><keyname>Dana</keyname><forenames>L√©o</forenames><affiliation>SIERRA</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>SIERRA</affiliation></author><author><keyname>Pillaud-Vivien</keyname><forenames>Loucas</forenames><affiliation>ENPC, CERMICS</affiliation></author></authors><title>Convergence of Shallow ReLU Networks on Weakly Interacting Data</title><categories>stat.ML cs.AI cs.LG math.ST stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyse the convergence of one-hidden-layer ReLU networks trained by gradient flow on $n$ data points. Our main contribution leverages the high dimensionality of the ambient space, which implies low correlation of the input samples, to demonstrate that a network with width of order $\log(n)$ neurons suffices for global convergence with high probability. Our analysis uses a Polyak-{\L}ojasiewicz viewpoint along the gradient-flow trajectory, which provides an exponential rate of convergence of $\frac{1}{n}$. When the data are exactly orthogonal, we give further refined characterizations of the convergence speed, proving its asymptotic behavior lies between the orders $\frac{1}{n}$ and $\frac{1}{\sqrt{n}}$, and exhibiting a phase-transition phenomenon in the convergence rate, during which it evolves from the lower bound to the upper, and in a relative time of order $\frac{1}{\log(n)}$. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.16988</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.16988</id><created>2025-02-24</created><authors><author><keyname>Wang</keyname><forenames>Chunyu</forenames></author><author><keyname>Tom</keyname><forenames>Brian DM</forenames></author></authors><title>A tutorial on optimal dynamic treatment regimes</title><categories>stat.OT stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A dynamic treatment regime is a sequence of treatment decision rules tailored to an individual's evolving status over time. In precision medicine, much focus has been placed on finding an optimal dynamic treatment regime which, if followed by everyone in the population, would yield the best outcome on average; and extensive investigation has been conducted from both methodological and applications standpoints. The aim of this tutorial is to provide readers who are interested in optimal dynamic treatment regimes with a systematic, detailed but accessible introduction, including the formal definition and formulation of this topic within the framework of causal inference, identification assumptions required to link the causal quantity of interest to the observed data, existing statistical models and estimation methods to learn the optimal regime from data, and application of these methods to both simulated and real data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17007</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17007</id><created>2025-02-24</created><authors><author><keyname>Sokol</keyname><forenames>Kacper</forenames></author><author><keyname>H√ºllermeier</keyname><forenames>Eyke</forenames></author></authors><title>All You Need for Counterfactual Explainability Is Principled and   Reliable Estimate of Aleatoric and Epistemic Uncertainty</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This position paper argues that, to its detriment, transparency research overlooks many foundational concepts of artificial intelligence. Here, we focus on uncertainty quantification -- in the context of ante-hoc interpretability and counterfactual explainability -- showing how its adoption could address key challenges in the field. First, we posit that uncertainty and ante-hoc interpretability offer complementary views of the same underlying idea; second, we assert that uncertainty provides a principled unifying framework for counterfactual explainability. Consequently, inherently transparent models can benefit from human-centred explanatory insights -- like counterfactuals -- which are otherwise missing. At a higher level, integrating artificial intelligence fundamentals into transparency research promises to yield more reliable, robust and understandable predictive models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17020</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17020</id><created>2025-02-24</created><authors><author><keyname>Miller</keyname><forenames>Justin</forenames></author><author><keyname>Alexander</keyname><forenames>Tristram</forenames></author></authors><title>Moving Past Single Metrics: Exploring Short-Text Clustering Across   Multiple Resolutions</title><categories>cs.LG cs.AI stat.ML</categories><comments>11 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Cluster number is typically a parameter selected at the outset in clustering problems, and while impactful, the choice can often be difficult to justify. Inspired by bioinformatics, this study examines how the nature of clusters varies with cluster number, presenting a method for determining cluster robustness, and providing a systematic method for deciding on the cluster number. The study focuses specifically on short-text clustering, involving 30,000 political Twitter bios, where the sparse co-occurrence of words between texts makes finding meaningful clusters challenging. A metric of proportional stability is introduced to uncover the stability of specific clusters between cluster resolutions, and the results are visualised using Sankey diagrams to provide an interrogative tool for understanding the nature of the dataset. The visualisation provides an intuitive way to track cluster subdivision and reorganisation as cluster number increases, offering insights that static, single-resolution metrics cannot capture. The results show that instead of seeking a single 'optimal' solution, choosing a cluster number involves balancing informativeness and complexity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17022</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17022</id><created>2025-02-24</created><authors><author><keyname>Baer</keyname><forenames>Gregor</forenames></author><author><keyname>Grau</keyname><forenames>Isel</forenames></author><author><keyname>Zhang</keyname><forenames>Chao</forenames></author><author><keyname>Van Gorp</keyname><forenames>Pieter</forenames></author></authors><title>Class-Dependent Perturbation Effects in Evaluating Time Series   Attributions</title><categories>cs.LG cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As machine learning models become increasingly prevalent in time series applications, Explainable Artificial Intelligence (XAI) methods are essential for understanding their predictions. Within XAI, feature attribution methods aim to identify which input features contributed the most to a model's prediction, with their evaluation typically relying on perturbation-based metrics. Through empirical analysis across multiple datasets, model architectures, and perturbation strategies, we identify important class-dependent effects in these metrics: they show varying effectiveness across classes, achieving strong results for some while remaining less sensitive to others. In particular, we find that the most effective perturbation strategies often demonstrate the most pronounced class differences. Our analysis suggests that these effects arise from the learned biases of classifiers, indicating that perturbation-based evaluation may reflect specific model behaviors rather than intrinsic attribution quality. We propose an evaluation framework with a class-aware penalty term to help assess and account for these effects in evaluating feature attributions. Although our analysis focuses on time series classification, these class-dependent effects likely extend to other structured data domains where perturbation-based evaluation is common. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17023</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17023</id><created>2025-02-24</created><authors><author><keyname>Kriuk</keyname><forenames>Boris</forenames></author></authors><title>Advancing Eurasia Fire Understanding Through Machine Learning Techniques</title><categories>cs.LG stat.ML</categories><comments>13 pages, 7 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern fire management systems increasingly rely on satellite data and weather forecasting; however, access to comprehensive datasets remains limited due to proprietary restrictions. Despite the ecological significance of wildfires, large-scale, multi-regional research is constrained by data scarcity. Russian diverse ecosystems play a crucial role in shaping Eurasian fire dynamics, yet they remain underexplored. This study addresses existing gaps by introducing an open-access dataset that captures detailed fire incidents alongside corresponding meteorological conditions. We present one of the most extensive datasets available for wildfire analysis in Russia, covering 13 consecutive months of observations. Leveraging machine learning techniques, we conduct exploratory data analysis and develop predictive models to identify key fire behavior patterns across different fire categories and ecosystems. Our results highlight the critical influence of environmental factor patterns on fire occurrence and spread behavior. By improving the understanding of wildfire dynamics in Eurasia, this work contributes to more effective, data-driven approaches for proactive fire management in the face of evolving environmental conditions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17024</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17024</id><created>2025-02-24</created><authors><author><keyname>Gong</keyname><forenames>Zixuan</forenames></author><author><keyname>Hu</keyname><forenames>Xiaolin</forenames></author><author><keyname>Tang</keyname><forenames>Huayi</forenames></author><author><keyname>Liu</keyname><forenames>Yong</forenames></author></authors><title>Towards Auto-Regressive Next-Token Prediction: In-Context Learning   Emerges from Generalization</title><categories>cs.CL cs.LG stat.ML</categories><comments>Published at ICLR 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large language models (LLMs) have demonstrated remarkable in-context learning (ICL) abilities. However, existing theoretical analysis of ICL primarily exhibits two limitations: (a) Limited i.i.d. Setting. Most studies focus on supervised function learning tasks where prompts are constructed with i.i.d. input-label pairs. This i.i.d. assumption diverges significantly from real language learning scenarios where prompt tokens are interdependent. (b) Lack of Emergence Explanation. Most literature answers what ICL does from an implicit optimization perspective but falls short in elucidating how ICL emerges and the impact of pre-training phase on ICL. In our paper, to extend (a), we adopt a more practical paradigm, auto-regressive next-token prediction (AR-NTP), which closely aligns with the actual training of language models. Specifically, within AR-NTP, we emphasize prompt token-dependency, which involves predicting each subsequent token based on the preceding sequence. To address (b), we formalize a systematic pre-training and ICL framework, highlighting the layer-wise structure of sequences and topics, alongside a two-level expectation. In conclusion, we present data-dependent, topic-dependent and optimization-dependent PAC-Bayesian generalization bounds for pre-trained LLMs, investigating that ICL emerges from the generalization of sequences and topics. Our theory is supported by experiments on numerical linear dynamic systems, synthetic GINC and real-world language datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17030</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17030</id><created>2025-02-24</created><authors><author><keyname>Padh</keyname><forenames>Kirtan</forenames></author><author><keyname>Li</keyname><forenames>Zhufeng</forenames></author><author><keyname>Casolo</keyname><forenames>Cecilia</forenames></author><author><keyname>Kilbertus</keyname><forenames>Niki</forenames></author></authors><title>Your Assumed DAG is Wrong and Here's How To Deal With It</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Assuming a directed acyclic graph (DAG) that represents prior knowledge of causal relationships between variables is a common starting point for cause-effect estimation. Existing literature typically invokes hypothetical domain expert knowledge or causal discovery algorithms to justify this assumption. In practice, neither may propose a single DAG with high confidence. Domain experts are hesitant to rule out dependencies with certainty or have ongoing disputes about relationships; causal discovery often relies on untestable assumptions itself or only provides an equivalence class of DAGs and is commonly sensitive to hyperparameter and threshold choices. We propose an efficient, gradient-based optimization method that provides bounds for causal queries over a collection of causal graphs -- compatible with imperfect prior knowledge -- that may still be too large for exhaustive enumeration. Our bounds achieve good coverage and sharpness for causal queries such as average treatment effects in linear and non-linear synthetic settings as well as on real-world data. Our approach aims at providing an easy-to-use and widely applicable rebuttal to the valid critique of `What if your assumed DAG is wrong?'. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17058</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17058</id><created>2025-02-24</created><authors><author><keyname>Nishikawa</keyname><forenames>Hiromasa</forenames></author><author><keyname>Kawai</keyname><forenames>Tetsuya</forenames></author><author><keyname>Uchida</keyname><forenames>Masayuki</forenames></author></authors><title>Quasi-likelihood ratio test for jump-diffusion processes based on   adaptive maximum likelihood inference</title><categories>math.ST stat.TH</categories><comments>83 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider parameter estimation and quasi-likelihood ratio tests for multidimensional jump-diffusion processes defined by stochastic differential equations. In general, simultaneous estimation faces challenges such as an increase of computational time for optimization and instability of estimation accuracy as the dimensionality of parameters grows. To address these issues, we propose an adaptive quasi-log likelihood function based on the joint quasi-log likelihood function introduced by Shimizu and Yoshida (2003, 2006) and Ogihara and Yoshida (2011). We then show that the resulting adaptive estimators possess consistency and asymptotic normality. Furthermore, we extend the joint quasi-log likelihood function proposed by Shimizu and Yoshida (2003, 2006) and Ogihara and Yoshida (2011) and construct a test statistic using the proposed adaptive estimators. We prove that the proposed test statistic converges in distribution to a $\chi^2$-distribution under the null hypothesis and that the associated test is consistent. Finally, we conduct numerical simulations using a specific jump-diffusion process model to examine the asymptotic behavior of the proposed adaptive estimators and test statistics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17060</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17060</id><created>2025-02-24</created><authors><author><keyname>Loizou</keyname><forenames>Andreas</forenames></author><author><keyname>Tsoumakos</keyname><forenames>Dimitrios</forenames></author></authors><title>Data Analysis Prediction over Multiple Unseen Datasets: A Vector   Embedding Approach</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The massive increase in the data volume and dataset availability for analysts compels researchers to focus on data content and select high-quality datasets to enhance the performance of analytics operators. While selecting the highest quality data for analysis highly increases task accuracy and efficiency, it is still a hard task, especially when the number of available inputs is very large. To address this issue, we propose a novel methodology that infers the outcome of analytics operators by creating a model from datasets similar to the queried one. Dataset similarity is performed via projecting each dataset to a vector embedding representation. The vectorization process is performed using our proposed deep learning model NumTabData2Vec, which takes a whole dataset and projects it into a lower vector embedding representation space. Through experimental evaluation, we compare the prediction performance and the execution time of our framework to another state-of-the-art modelling operator framework, illustrating that our approach predicts analytics outcomes accurately. Furthermore, our vectorization model can project different real-world scenarios to a lower vector embedding representation and distinguish between them. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17061</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17061</id><created>2025-02-24</created><authors><author><keyname>Marco-Blanco</keyname><forenames>Jorge</forenames></author><author><keyname>Cuevas</keyname><forenames>Rub√©n</forenames></author></authors><title>Random Projections and Natural Sparsity in Time-Series Classification: A   Theoretical Analysis</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Time-series classification is essential across diverse domains, including medical diagnosis, industrial monitoring, financial forecasting, and human activity recognition. The Rocket algorithm has emerged as a simple yet powerful method, achieving state-of-the-art performance through random convolutional kernels applied to time-series data, followed by non-linear transformation. Its architecture approximates a one-hidden-layer convolutional neural network while eliminating parameter training, ensuring computational efficiency. Despite its empirical success, fundamental questions about its theoretical foundations remain unexplored. We bridge theory and practice by formalizing Rocket's random convolutional filters within the compressed sensing framework, proving that random projections preserve discriminative patterns in time-series data. This analysis reveals relationships between kernel parameters and input signal characteristics, enabling more principled approaches to algorithm configuration. Moreover, we demonstrate that its non-linearity, based on the proportion of positive values after convolutions, expresses the inherent sparsity of time-series data. Our theoretical investigation also proves that Rocket satisfies two critical conditions: translation invariance and noise robustness. These findings enhance interpretability and provide guidance for parameter optimization in extreme cases, advancing both theoretical understanding and practical application of time-series classification. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17077</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17077</id><created>2025-02-24</created><updated>2025-02-25</updated><authors><author><keyname>Wang</keyname><forenames>Jiayi</forenames></author><author><keyname>Alfaro</keyname><forenames>Juan C.</forenames></author><author><keyname>Bengs</keyname><forenames>Viktor</forenames></author></authors><title>A comparative analysis of rank aggregation methods for the partial label   ranking problem</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The label ranking problem is a supervised learning scenario in which the learner predicts a total order of the class labels for a given input instance. Recently, research has increasingly focused on the partial label ranking problem, a generalization of the label ranking problem that allows ties in the predicted orders. So far, most existing learning approaches for the partial label ranking problem rely on approximation algorithms for rank aggregation in the final prediction step. This paper explores several alternative aggregation methods for this critical step, including scoring-based and probabilistic-based rank aggregation approaches. To enhance their suitability for the more general partial label ranking problem, the investigated methods are extended to increase the likelihood of producing ties. Experimental evaluations on standard benchmarks demonstrate that scoring-based variants consistently outperform the current state-of-the-art method in handling incomplete information. In contrast, probabilistic-based variants fail to achieve competitive performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17137</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17137</id><created>2025-02-24</created><authors><author><keyname>Andreani</keyname><forenames>Mila</forenames></author></authors><title>On Quantile Regression Forests for Modelling Mixed-Frequency and   Longitudinal Data</title><categories>stat.ML cs.LG stat.ME</categories><comments>PhD Thesis</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The aim of this thesis is to extend the applications of the Quantile Regression Forest (QRF) algorithm to handle mixed-frequency and longitudinal data. To this end, standard statistical approaches have been exploited to build two novel algorithms: the Mixed- Frequency Quantile Regression Forest (MIDAS-QRF) and the Finite Mixture Quantile Regression Forest (FM-QRF). The MIDAS-QRF combines the flexibility of QRF with the Mixed Data Sampling (MIDAS) approach, enabling non-parametric quantile estimation with variables observed at different frequencies. FM-QRF, on the other hand, extends random effects machine learning algorithms to a QR framework, allowing for conditional quantile estimation in a longitudinal data setting. The contributions of this dissertation lie both methodologically and empirically. Methodologically, the MIDAS-QRF and the FM-QRF represent two novel approaches for handling mixed-frequency and longitudinal data in QR machine learning framework. Empirically, the application of the proposed models in financial risk management and climate-change impact evaluation demonstrates their validity as accurate and flexible models to be applied in complex empirical settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17142</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17142</id><created>2025-02-24</created><authors><author><keyname>Vassaux</keyname><forenames>Louis</forenames></author><author><keyname>Massouli√©</keyname><forenames>Laurent</forenames></author></authors><title>The feasibility of multi-graph alignment: a Bayesian approach</title><categories>math.ST math.PR stat.ML stat.TH</categories><comments>37 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We establish thresholds for the feasibility of random multi-graph alignment in two models. In the Gaussian model, we demonstrate an "all-or-nothing" phenomenon: above a critical threshold, exact alignment is achievable with high probability, while below it, even partial alignment is statistically impossible. In the sparse Erd\H{o}s-R\'enyi model, we rigorously identify a threshold below which no meaningful partial alignment is possible and conjecture that above this threshold, partial alignment can be achieved. To prove these results, we develop a general Bayesian estimation framework over metric spaces, which provides insight into a broader class of high-dimensional statistical problems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17150</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17150</id><created>2025-02-24</created><authors><author><keyname>Bertazzi</keyname><forenames>Andrea</forenames></author><author><keyname>Johnston</keyname><forenames>Tim</forenames></author><author><keyname>Roberts</keyname><forenames>Gareth O.</forenames></author><author><keyname>Durmus</keyname><forenames>Alain</forenames></author></authors><title>Differential privacy guarantees of Markov chain Monte Carlo algorithms</title><categories>stat.ML cs.LG stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper aims to provide differential privacy (DP) guarantees for Markov chain Monte Carlo (MCMC) algorithms. In a first part, we establish DP guarantees on samples output by MCMC algorithms as well as Monte Carlo estimators associated with these methods under assumptions on the convergence properties of the underlying Markov chain. In particular, our results highlight the critical condition of ensuring the target distribution is differentially private itself. In a second part, we specialise our analysis to the unadjusted Langevin algorithm and stochastic gradient Langevin dynamics and establish guarantees on their (R\'enyi) DP. To this end, we develop a novel methodology based on Girsanov's theorem combined with a perturbation trick to obtain bounds for an unbounded domain and in a non-convex setting. We establish: (i) uniform in $n$ privacy guarantees when the state of the chain after $n$ iterations is released, (ii) bounds on the privacy of the entire chain trajectory. These findings provide concrete guidelines for privacy-preserving MCMC. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17175</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17175</id><created>2025-02-24</created><authors><author><keyname>Zhang</keyname><forenames>Raymond</forenames></author><author><keyname>Hadiji</keyname><forenames>Hedi</forenames></author><author><keyname>Combes</keyname><forenames>Richard</forenames></author></authors><title>Linear Bandits on Ellipsoids: Minimax Optimal Algorithms</title><categories>stat.ML cs.LG</categories><comments>20 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider linear stochastic bandits where the set of actions is an ellipsoid. We provide the first known minimax optimal algorithm for this problem. We first derive a novel information-theoretic lower bound on the regret of any algorithm, which must be at least $\Omega(\min(d \sigma \sqrt{T} + d \|\theta\|_{A}, \|\theta\|_{A} T))$ where $d$ is the dimension, $T$ the time horizon, $\sigma^2$ the noise variance, $A$ a matrix defining the set of actions and $\theta$ the vector of unknown parameters. We then provide an algorithm whose regret matches this bound to a multiplicative universal constant. The algorithm is non-classical in the sense that it is not optimistic, and it is not a sampling algorithm. The main idea is to combine a novel sequential procedure to estimate $\|\theta\|$, followed by an explore-and-commit strategy informed by this estimate. The algorithm is highly computationally efficient, and a run requires only time $O(dT + d^2 \log(T/d) + d^3)$ and memory $O(d^2)$, in contrast with known optimistic algorithms, which are not implementable in polynomial time. We go beyond minimax optimality and show that our algorithm is locally asymptotically minimax optimal, a much stronger notion of optimality. We further provide numerical experiments to illustrate our theoretical findings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17214</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17214</id><created>2025-02-24</created><authors><author><keyname>Zhang</keyname><forenames>Boxuan</forenames></author><author><keyname>Zhang</keyname><forenames>Ruqi</forenames></author></authors><title>CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with   Chain-of-Thought</title><categories>cs.CL cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large language models (LLMs) excel in many tasks but struggle to accurately quantify uncertainty in their generated responses. This limitation makes it challenging to detect misinformation and ensure reliable decision-making. Existing uncertainty quantification (UQ) methods for LLMs are primarily prompt-wise rather than response-wise, often requiring multiple response samples, which incurs high computational costs. Moreover, LLMs have been shown to be overconfident, particularly when using reasoning steps to derive their answers. In this work, we propose CoT-UQ, a response-wise UQ framework that integrates LLMs' inherent reasoning capabilities through Chain-of-Thought (CoT) into the UQ process. CoT-UQ captures critical information during inference by extracting keywords from each reasoning step and assessing their importance to the final answer. This key reasoning information is then aggregated to produce a final uncertainty estimate. We conduct extensive experiments based on LLaMA Family with model sizes varying from 8B to 13B across logical and mathematical reasoning tasks. Experimental results demonstrate that CoT-UQ significantly outperforms existing UQ methods, achieving an average improvement of 5.9% AUROC compared to current UQ methods. The code is available at: https://github.com/ZBox1005/CoT-UQ. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17215</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17215</id><created>2025-02-24</created><authors><author><keyname>Saha</keyname><forenames>Shital</forenames></author><author><keyname>Kayal</keyname><forenames>Suchandan</forenames></author></authors><title>Multivariate R\'enyi inaccuracy measures based on copulas: properties   and application</title><categories>math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose R\'enyi inaccuracy measure based on multivariate copula and multivariate survival copula, respectively dubbed as multivariate cumulative copula R\'enyi inaccuracy measure and multivariate survival copula R\'enyi inaccuracy measure. Bounds of multivariate cumulative copula R\'enyi inaccuracy and multivariate survival copula R\'enyi inaccuracy measures have been obtained using Fr\'echet-Hoeffding bound. We discuss the comparison studies of the multivariate cumulative copula R\'enyi inaccuracy and multivariate survival copula R\'enyi inaccuracy measures based on lower orthant and upper orthant orders. We have also proposed multivariate co-copula R\'enyi inaccuracy and multivariate dual copula R\'enyi inaccuracy measures based on multivariate co-copula and dual copula. Similar properties have been explored. Further, we propose semiparametric estimator of multivariate cumulative copula R\'enyi inaccuracy measure. A simulation study is performed to compute standard deviation, absolute bias and mean squared error of the proposed estimator. Finally, a data set is considered to show that the multivariate cumulative copula R\'enyi inaccuracy measure can be applied as a model (copula) selection criteria. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17223</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17223</id><created>2025-02-24</created><authors><author><keyname>Learned-Miller</keyname><forenames>Erik</forenames></author></authors><title>On the admissibility of bounds on the mean of discrete, scalar   probability distributions from an iid sample</title><categories>math.ST stat.TH</categories><comments>24 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We address the problem of producing a lower bound for the mean of a discrete probability distribution, with known support over a finite set of real numbers, from an iid sample of that distribution. Up to a constant, this is equivalent to bounding the mean of a multinomial distribution (with known support) from a sample of that distribution. Our main contribution is to characterize the complete set of admissible bound functions for any sample space, and to show that certain previously published bounds are admissible. We prove that the solution to each one of a set of simple-to-state optimization problems yields such an admissible bound. Single examples of such bounds, such as the trinomial bound by Miratrix and Stark [2009] have been previously published, but without an analysis of admissibility, and without a discussion of the full set of alternative admissible bounds. In addition to a variety of results about admissible bounds, we prove the non-existence of optimal bounds for sample spaces with supports of size greater than 1 and samples sizes greater than 1. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17261</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17261</id><created>2025-02-24</created><authors><author><keyname>Wang</keyname><forenames>Ying-Ao</forenames></author><author><keyname>Zhang</keyname><forenames>Yunyi</forenames></author><author><keyname>Zhang</keyname><forenames>Ye</forenames></author></authors><title>On a class of high dimensional linear regression methods with debiasing   and thresholding</title><categories>math.ST math.PR stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a unified framework, inspired by classical regularization theory, for designing and analyzing a broad class of linear regression approaches. Our framework encompasses traditional methods like least squares regression and Ridge regression, as well as innovative techniques, including seven novel regression methods such as Landweber and Showalter regressions. Within this framework, we further propose a class of debiased and thresholded regression methods to promote feature selection, particularly in terms of sparsity. These methods may offer advantages over conventional regression techniques, including Lasso, due to their ease of computation via a closed-form expression. Theoretically, we establish consistency results and Gaussian approximation theorems for this new class of regularization methods. Extensive numerical simulations further demonstrate that the debiased and thresholded counterparts of linear regression methods exhibit favorable finite sample performance and may be preferable in certain settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17264</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17264</id><created>2025-02-24</created><authors><author><keyname>Bairaktari</keyname><forenames>Konstantina</forenames></author><author><keyname>Wu</keyname><forenames>Jiayun</forenames></author><author><keyname>Wu</keyname><forenames>Zhiwei Steven</forenames></author></authors><title>Kandinsky Conformal Prediction: Beyond Class- and Covariate-Conditional   Coverage</title><categories>cs.LG stat.ML</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Conformal prediction is a powerful distribution-free framework for constructing prediction sets with coverage guarantees. Classical methods, such as split conformal prediction, provide marginal coverage, ensuring that the prediction set contains the label of a random test point with a target probability. However, these guarantees may not hold uniformly across different subpopulations, leading to disparities in coverage. Prior work has explored coverage guarantees conditioned on events related to the covariates and label of the test point. We present Kandinsky conformal prediction, a framework that significantly expands the scope of conditional coverage guarantees. In contrast to Mondrian conformal prediction, which restricts its coverage guarantees to disjoint groups -- reminiscent of the rigid, structured grids of Piet Mondrian's art -- our framework flexibly handles overlapping and fractional group memberships defined jointly on covariates and labels, reflecting the layered, intersecting forms in Wassily Kandinsky's compositions. Our algorithm unifies and extends existing methods, encompassing covariate-based group conditional, class conditional, and Mondrian conformal prediction as special cases, while achieving a minimax-optimal high-probability conditional coverage bound. Finally, we demonstrate the practicality of our approach through empirical evaluation on real-world datasets. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17275</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17275</id><created>2025-02-24</created><authors><author><keyname>Zonta</keyname><forenames>Elia</forenames></author><author><keyname>Buha</keyname><forenames>Ivana Jovanovic</forenames></author><author><keyname>Spinola</keyname><forenames>Michele</forenames></author><author><keyname>Wei√üinger</keyname><forenames>Christoph</forenames></author><author><keyname>Bungartz</keyname><forenames>Hans-Joachim</forenames></author><author><keyname>Jossen</keyname><forenames>Andreas</forenames></author></authors><title>Time-dependent global sensitivity analysis of the Doyle-Fuller-Newman   model</title><categories>cond-mat.mtrl-sci stat.CO</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Doyle-Fuller-Newman model is arguably the most ubiquitous electrochemical model in lithium-ion battery research. Since it is a highly nonlinear model, its input-output relations are still poorly understood. Researchers therefore often employ sensitivity analyses to elucidate relative parametric importance for certain use cases. However, some methods are ill-suited for the complexity of the model and appropriate methods often face the downside of only being applicable to scalar quantities of interest. We implement a novel framework for global sensitivity analysis of time-dependent model outputs and apply it to a drive cycle simulation. We conduct a full and a subgroup sensitivity analysis to resolve lowly sensitive parameters and explore the model error when unimportant parameters are set to arbitrary values. Our findings suggest that the method identifies insensitive parameters whose variations cause only small deviations in the voltage response of the model. By providing the methodology, we hope research questions related to parametric sensitivity for time-dependent quantities of interest, such as voltage responses, can be addressed more easily and adequately in simulative battery research and beyond. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17292</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17292</id><created>2025-02-24</created><authors><author><keyname>Wen</keyname><forenames>Yuxiao</forenames></author><author><keyname>Han</keyname><forenames>Yanjun</forenames></author><author><keyname>Zhou</keyname><forenames>Zhengyuan</forenames></author></authors><title>Joint Value Estimation and Bidding in Repeated First-Price Auctions</title><categories>cs.LG cs.GT cs.IT math.IT stat.ME stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study regret minimization in repeated first-price auctions (FPAs), where a bidder observes only the realized outcome after each auction -- win or loss. This setup reflects practical scenarios in online display advertising where the actual value of an impression depends on the difference between two potential outcomes, such as clicks or conversion rates, when the auction is won versus lost. We analyze three outcome models: (1) adversarial outcomes without features, (2) linear potential outcomes with features, and (3) linear treatment effects in features. For each setting, we propose algorithms that jointly estimate private values and optimize bidding strategies, achieving near-optimal regret bounds. Notably, our framework enjoys a unique feature that the treatments are also actively chosen, and hence eliminates the need for the overlap condition commonly required in causal inference. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17323</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17323</id><created>2025-02-24</created><authors><author><keyname>Van Waerebeke</keyname><forenames>Martin</forenames></author><author><keyname>Lorenzi</keyname><forenames>Marco</forenames></author><author><keyname>Neglia</keyname><forenames>Giovanni</forenames></author><author><keyname>Scaman</keyname><forenames>Kevin</forenames></author></authors><title>When to Forget? Complexity Trade-offs in Machine Unlearning</title><categories>stat.ML cs.LG math.OC</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Machine Unlearning (MU) aims at removing the influence of specific data points from a trained model, striving to achieve this at a fraction of the cost of full model retraining. In this paper, we analyze the efficiency of unlearning methods and establish the first upper and lower bounds on minimax computation times for this problem, characterizing the performance of the most efficient algorithm against the most difficult objective function. Specifically, for strongly convex objective functions and under the assumption that the forget data is inaccessible to the unlearning method, we provide a phase diagram for the unlearning complexity ratio -- a novel metric that compares the computational cost of the best unlearning method to full model retraining. The phase diagram reveals three distinct regimes: one where unlearning at a reduced cost is infeasible, another where unlearning is trivial because adding noise suffices, and a third where unlearning achieves significant computational advantages over retraining. These findings highlight the critical role of factors such as data dimensionality, the number of samples to forget, and privacy constraints in determining the practical feasibility of unlearning. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17326</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17326</id><created>2025-02-24</created><authors><author><keyname>Jha</keyname><forenames>Sneha</forenames></author><author><keyname>Zhang</keyname><forenames>Yaguang</forenames></author><author><keyname>Krogmeier</keyname><forenames>J. V.</forenames></author><author><keyname>Buckmaster</keyname><forenames>D</forenames></author></authors><title>A Web-Based Application Leveraging Geospatial Information to Automate   On-Farm Trial Design</title><categories>stat.ME</categories><comments>This was presented at the ASABE 2023 AIM meeting with id: 2301158</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  On-farm sensor data have allowed farmers to implement field management techniques and intensively track the corresponding responses. These data combined with historical records open the door for real-time field management improvements with the help of current advancements in computing power. However, despite these advances, the statistical design of experiments is rarely used to evaluate the performance of field management techniques accurately. Traditionally, randomized block design is prevalent in statistical designs of field trials, but in practice it is limited in dealing with large variations in soil classes, management practices, and crop varieties. More specifically, although this experimental design is suited for most trial types, it is not the optimal choice when multiple factors are tested over multifarious natural variations in farms, due to the economic constraints caused by the sheer number of variables involved. Experimental refinement is required to better estimate the effects of the primary factor in the presence of auxiliary factors. In this way, farmers can better understand the characteristics and limitations of the primary factor. This work presents a framework for automating the analysis of local field variations by fusing soil classification data and lidar topography data with historical yield. This framework will be leveraged to automate the designing of field experiments based on multiple topographic features </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17367</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17367</id><created>2025-02-24</created><authors><author><keyname>Kimpton</keyname><forenames>Louise</forenames></author><author><keyname>Salter</keyname><forenames>James</forenames></author><author><keyname>Xiong</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Challenor</keyname><forenames>Peter</forenames></author></authors><title>Bayesian Hierarchical Emulators for Multi-Level Models: BayHEm</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision making often uses complex computer codes run at the exa-scale (10e18 flops). Such computer codes or models are often run in a hierarchy of different levels of fidelity ranging from the basic to the very sophisticated. The top levels in this hierarchy are expensive to run, limiting the number of possible runs. To make use of runs over all levels, and crucially improve emulation at the top level, we use multi-level Gaussian process emulators (GPs). We will present a new method of building GP emulators from hierarchies of models. In order to share information across the different levels, l=1,...,L, we define the form of the prior of the l+1th level to be the posterior of the lth level, hence building a Bayesian hierarchical structure for the top Lth level. This enables us to not only learn about the GP hyperparameters as we move up the multi-level hierarchy, but also allows us to limit the total number of parameters in the full model, whilst maintaining accuracy. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17370</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17370</id><created>2025-02-24</created><authors><author><keyname>Drago</keyname><forenames>Simone</forenames></author><author><keyname>Mussi</keyname><forenames>Marco</forenames></author><author><keyname>Metelli</keyname><forenames>Alberto Maria</forenames></author></authors><title>A Refined Analysis of UCBVI</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this work, we provide a refined analysis of the UCBVI algorithm (Azar et al., 2017), improving both the bonus terms and the regret analysis. Additionally, we compare our version of UCBVI with both its original version and the state-of-the-art MVP algorithm. Our empirical validation demonstrates that improving the multiplicative constants in the bounds has significant positive effects on the empirical performance of the algorithms. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17371</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17371</id><created>2025-02-24</created><authors><author><keyname>Seri</keyname><forenames>Emiliano</forenames></author><author><keyname>Petitta</keyname><forenames>Marcello</forenames></author><author><keyname>Cornaro</keyname><forenames>Cristina</forenames></author></authors><title>Sustainable Greenhouse Management: A Comparative Analysis of Recurrent   and Graph Neural Networks</title><categories>cs.LG stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The integration of photovoltaic (PV) systems into greenhouses not only optimizes land use but also enhances sustainable agricultural practices by enabling dual benefits of food production and renewable energy generation. However, accurate prediction of internal environmental conditions is crucial to ensure optimal crop growth while maximizing energy production. This study introduces a novel application of Spatio-Temporal Graph Neural Networks (STGNNs) to greenhouse microclimate modeling, comparing their performance with traditional Recurrent Neural Networks (RNNs). While RNNs excel at temporal pattern recognition, they cannot explicitly model the directional relationships between environmental variables. Our STGNN approach addresses this limitation by representing these relationships as directed graphs, enabling the model to capture both spatial dependencies and their directionality. Using high-frequency data collected at 15-minute intervals from a greenhouse in Volos, Greece, we demonstrate that RNNs achieve exceptional accuracy in winter conditions (R^2 = 0.985) but show limitations during summer cooling system operation. Though STGNNs currently show lower performance (winter R^2 = 0.947), their architecture offers greater potential for integrating additional variables such as PV generation and crop growth indicators. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17405</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17405</id><created>2025-02-24</created><authors><author><keyname>Milsom</keyname><forenames>Edward</forenames></author><author><keyname>Anson</keyname><forenames>Ben</forenames></author><author><keyname>Aitchison</keyname><forenames>Laurence</forenames></author></authors><title>Function-Space Learning Rates</title><categories>stat.ML cs.LG</categories><comments>19 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider layerwise function-space learning rates, which measure the magnitude of the change in a neural network's output function in response to an update to a parameter tensor. This contrasts with traditional learning rates, which describe the magnitude of changes in parameter space. We develop efficient methods to measure and set function-space learning rates in arbitrary neural networks, requiring only minimal computational overhead through a few additional backward passes that can be performed at the start of, or periodically during, training. We demonstrate two key applications: (1) analysing the dynamics of standard neural network optimisers in function space, rather than parameter space, and (2) introducing FLeRM (Function-space Learning Rate Matching), a novel approach to hyperparameter transfer across model scales. FLeRM records function-space learning rates while training a small, cheap base model, then automatically adjusts parameter-space layerwise learning rates when training larger models to maintain consistent function-space updates. FLeRM gives hyperparameter transfer across model width, depth, initialisation scale, and LoRA rank in various architectures including MLPs with residual connections and transformers with different layer normalisation schemes. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17412</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17412</id><created>2025-02-24</created><authors><author><keyname>Chowdhury</keyname><forenames>Mriganka Basu Roy</forenames></author><author><keyname>Ganguly</keyname><forenames>Shirshendu</forenames></author></authors><title>Invariance principle for the Gaussian Multiplicative Chaos via a high   dimensional CLT with low rank increments</title><categories>math.PR math-ph math.MP math.ST stat.TH</categories><comments>34 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian multiplicative chaos (GMC) is a canonical random fractal measure obtained by exponentiating log-correlated Gaussian processes, first constructed in the seminal work of Kahane (1985). Since then it has served as an important building block in constructions of quantum field theories and Liouville quantum gravity. However, in many natural settings, non-Gaussian log-correlated processes arise. In this paper, we investigate the universality of GMC through an invariance principle. We consider the model of a random Fourier series, a process known to be log-correlated. While the Gaussian Fourier series has been a classical object of study, recently, the non-Gaussian counterpart was investigated and the associated multiplicative chaos constructed by Junnila in 2016. We show that the Gaussian and non-Gaussian variables can be coupled so that the associated chaos measures are almost surely mutually absolutely continuous throughout the entire sub-critical regime. This solves the main open problem from Kim and Kriechbaum (2024) who had earlier established such a result for a part of the regime. The main ingredient is a new high dimensional CLT for a sum of independent (but not i.i.d.) random vectors belonging to rank one subspaces with error bounds involving the isotropic properties of the covariance matrix of the sum, which we expect will find other applications. The proof relies on a path-wise analysis of Skorokhod embeddings as well as a perturbative result about square roots of positive semi-definite matrices which, surprisingly, appears to be new. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17427</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17427</id><created>2025-02-24</created><authors><author><keyname>Noarov</keyname><forenames>Georgy</forenames></author><author><keyname>Fogliato</keyname><forenames>Riccardo</forenames></author><author><keyname>Bertran</keyname><forenames>Martin</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author></authors><title>Stronger Neyman Regret Guarantees for Adaptive Experimental Design</title><categories>stat.ME cs.LG math.ST stat.ML stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study the design of adaptive, sequential experiments for unbiased average treatment effect (ATE) estimation in the design-based potential outcomes setting. Our goal is to develop adaptive designs offering sublinear Neyman regret, meaning their efficiency must approach that of the hindsight-optimal nonadaptive design. Recent work [Dai et al, 2023] introduced ClipOGD, the first method achieving $\widetilde{O}(\sqrt{T})$ expected Neyman regret under mild conditions. In this work, we propose adaptive designs with substantially stronger Neyman regret guarantees. In particular, we modify ClipOGD to obtain anytime $\widetilde{O}(\log T)$ Neyman regret under natural boundedness assumptions. Further, in the setting where experimental units have pre-treatment covariates, we introduce and study a class of contextual "multigroup" Neyman regret guarantees: Given any set of possibly overlapping groups based on the covariates, the adaptive design outperforms each group's best non-adaptive designs. In particular, we develop a contextual adaptive design with $\widetilde{O}(\sqrt{T})$ anytime multigroup Neyman regret. We empirically validate the proposed designs through an array of experiments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17431</identifier><datestamp>2025-02-25</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17431</id><created>2025-02-20</created><authors><author><keyname>Basse-O'Connor</keyname><forenames>Andreas</forenames></author><author><keyname>Kramer-Bang</keyname><forenames>David</forenames></author></authors><title>Exponential dimensional dependence in high-dimensional Hermite method of   moments</title><categories>math.ST math.PR stat.TH</categories><msc-class>60H07, 60F05, 60G15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show exponential dimensional dependence for the Hermite method of moments as a statistical test for Gaussianity in the case of i.i.d. Gaussian variables, by constructing a lower bound for the the Kolmogorov-Smirnov distance and an upper bound for the convex distance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17453</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17453</id><created>2025-02-08</created><authors><author><keyname>Giri</keyname><forenames>Ritik Roshan</forenames></author><author><keyname>Kayal</keyname><forenames>Suchandan</forenames></author></authors><title>Permutation extropy: a time series complexity measure</title><categories>nlin.CD stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  On account of a greater need for understanding the complexity of time series like physiological time series, financial time series, and many more that enter into picture for their inculpation with real-world problems, several complexity parameters have already been proposed in the literature. Permutation entropy, Lyapunov exponents are such complexity parameters out of many. In this article, we introduce a new time series complexity parameter, that is, the permutation extropy. The failure of permutation entropy in correctly specifying complexity of some chaotic time series motivates us to come up with a better complexity parameter, hence we propose this permutation extropy measure. We try to combine the ideas behind the permutation entropy and extopy to construct this measure. We also validate our proposed measure using several chaotic maps like logistic map, Henon map and Burger map. We apply the proposed complexity parameter to study the complexity of financial time series of the stock market and time series constructed using WHO data, finding a better complexity specification than permutation entropy. The proposed measure is kind of robust, fast calculation and invariant with respect to monotonous nonlinear transformation like permutation entropy, but it gives us a better result in specifying complexity in some cases. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17474</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17474</id><created>2025-02-15</created><authors><author><keyname>Jurhar</keyname><forenames>Tin</forenames></author><author><keyname>Pellegrini</keyname><forenames>Franziska</forenames></author><author><keyname>del Toro</keyname><forenames>Ana I. Nu√±es</forenames></author><author><keyname>Stephani</keyname><forenames>Tilman</forenames></author><author><keyname>Nolte</keyname><forenames>Guido</forenames></author><author><keyname>Haufe</keyname><forenames>Stefan</forenames></author></authors><title>Estimating Time Delays between Signals under Mixed Noise Influence with   Novel Cross- and Bispectral Methods</title><categories>eess.SP math.ST stat.ME stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  A common problem to signal processing are biases introduced by correlated noise. In Time-Delay Estimation (TDE), which quantifies a time lag between two signals, noise mixing introduces a bias towards zero delay in conventional TDE protocols based on the cross- or bispectrum. Here we propose two novel TDE approaches that address these shortcomings: (1) A cross-spectrum based TDE protocol that relies on estimating the periodicity of the phase spectrum rather than its slope, and (2) a bispectrum based TDE analysis, bispectral antisymmetrization, which removes contributions from not just Gaussian but all independent sources. In a simulation study, we compare conventional and novel TDE protocols and resolve differences in performance with respect to noise Gaussianity and auto-correlation structure. As a proof-of-concept, we also perform TDE analysis on a neural stimulation dataset (n=3). We find that antisymmetrization consistently outperforms conventional bispectral TDE methods at low signal-to-noise ratios (SNR) and removes spurious zero-delay estimates in all mixed-noise environments. TDE based on phase periodicity also improves signal sensitivity compared to conventional cross-spectral methods. These observations are stable with respect to the magnitude of the delay and the statistical properties of the noise. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17491</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17491</id><created>2025-02-20</created><authors><author><keyname>Yanchenko</keyname><forenames>Eric</forenames></author></authors><title>Pseudo-R2D2 prior for high-dimensional ordinal regression</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ordinal regression with a high-dimensional covariate space has many important application areas including gene expression studies. The lack of an intrinsic numeric value associated with ordinal responses, however, makes methods based on continuous data, like linear regression, inappropriate. In this work, we extend the R2D2 prior framework to the high-dimensional ordinal setting. Since the $R^2$ definition used in the R2D2 prior relies on means and variances, it cannot be used for ordinal regression as these two quantities are not suitable for such data. Instead, by simulating data and using McFadden's coefficient-of-determination ($R^2_M$), we show that a generalized inverse Gaussian prior distribution on the global variance parameter approximately induces a beta prior distribution on $R^2_M$. The proposed prior can be implemented in $\texttt{Stan}$ and an $\texttt{R}$ package is also developed. Our method demonstrates excellent inference properties on simulated data, as well as yielding accurate predictions when applied to a liver tissue gene expression dataset. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17492</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17492</id><created>2025-02-20</created><authors><author><keyname>Edwards</keyname><forenames>Christopher</forenames></author><author><keyname>Smith</keyname><forenames>Ralph C</forenames></author></authors><title>Rapid Parameter Inference with Uncertainty Quantification for a   Radiological Plume Source Identification Problem</title><categories>cs.LG stat.AP</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the event of a nuclear accident, or the detonation of a radiological dispersal device, quickly locating the source of the accident or blast is important for emergency response and environmental decontamination. At a specified time after a simulated instantaneous release of an aerosolized radioactive contaminant, measurements are recorded downwind from an array of radiation sensors. Neural networks are employed to infer the source release parameters in an accurate and rapid manner using sensor and mean wind speed data. We consider two neural network constructions that quantify the uncertainty of the predicted values; a categorical classification neural network and a Bayesian neural network. With the categorical classification neural network, we partition the spatial domain and treat each partition as a separate class for which we estimate the probability that it contains the true source location. In a Bayesian neural network, the weights and biases have a distribution rather than a single optimal value. With each evaluation, these distributions are sampled, yielding a different prediction with each evaluation. The trained Bayesian neural network is thus evaluated to construct posterior densities for the release parameters. Results are compared to Markov chain Monte Carlo (MCMC) results found using the Delayed Rejection Adaptive Metropolis Algorithm. The Bayesian neural network approach is generally much cheaper computationally than the MCMC approach as it relies on the computational cost of the neural network evaluation to generate posterior densities as opposed to the MCMC approach which depends on the computational expense of the transport and radiation detection models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17495</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17495</id><created>2025-02-20</created><authors><author><keyname>Herrera</keyname><forenames>Mauricio</forenames></author><author><keyname>Kleisinger</keyname><forenames>Francisca</forenames></author><author><keyname>Wils√≥n</keyname><forenames>Andr√©s</forenames></author></authors><title>Spatiotemporal Forecasting in Climate Data Using EOFs and Machine   Learning Models: A Case Study in Chile</title><categories>cs.LG physics.ao-ph stat.AP stat.ML</categories><comments>25 pages, 6 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Effective resource management and environmental planning in regions with high climatic variability, such as Chile, demand advanced predictive tools. This study addresses this challenge by employing an innovative and computationally efficient hybrid methodology that integrates machine learning (ML) methods for time series forecasting with established statistical techniques. The spatiotemporal data undergo decomposition using time-dependent Empirical Orthogonal Functions (EOFs), denoted as \(\phi_{k}(t)\), and their corresponding spatial coefficients, \(\alpha_{k}(s)\), to reduce dimensionality. Wavelet analysis provides high-resolution time and frequency information from the \(\phi_{k}(t)\) functions, while neural networks forecast these functions within a medium-range horizon \(h\). By utilizing various ML models, particularly a Wavelet - ANN hybrid model, we forecast \(\phi_{k}(t+h)\) up to a time horizon \(h\), and subsequently reconstruct the spatiotemporal data using these extended EOFs. This methodology is applied to a grid of climate data covering the territory of Chile. It transitions from a high-dimensional multivariate spatiotemporal data forecasting problem to a low-dimensional univariate forecasting problem. Additionally, cluster analysis with Dynamic Time Warping for defining similarities between rainfall time series, along with spatial coherence and predictability assessments, has been instrumental in identifying geographic areas where model performance is enhanced. This approach also elucidates the reasons behind poor forecast performance in regions or clusters with low spatial coherence and predictability. By utilizing cluster medoids, the forecasting process becomes more practical and efficient. This compound approach significantly reduces computational complexity while generating forecasts of reasonable accuracy and utility. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17497</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17497</id><created>2025-02-21</created><authors><author><keyname>Zhang</keyname><forenames>Yushi</forenames></author><author><keyname>Su</keyname><forenames>Shuai</forenames></author><author><keyname>Wang</keyname><forenames>Yong</forenames></author><author><keyname>Yao</keyname><forenames>Yanzhong</forenames></author></authors><title>Hard constraint learning approaches with trainable influence functions   for evolutionary equations</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a novel deep learning approach for solving evolutionary equations, which integrates sequential learning strategies with an enhanced hard constraint strategy featuring trainable parameters, addressing the low computational accuracy of standard Physics-Informed Neural Networks (PINNs) in large temporal domains.Sequential learning strategies divide a large temporal domain into multiple subintervals and solve them one by one in a chronological order, which naturally respects the principle of causality and improves the stability of the PINN solution. The improved hard constraint strategy strictly ensures the continuity and smoothness of the PINN solution at time interval nodes, and at the same time passes the information from the previous interval to the next interval, which avoids the incorrect/trivial solution at the position far from the initial time. Furthermore, by investigating the requirements of different types of equations on hard constraints, we design a novel influence function with trainable parameters for hard constraints, which provides theoretical and technical support for the effective implementations of hard constraint strategies, and significantly improves the universality and computational accuracy of our method. In addition, an adaptive time-domain partitioning algorithm is proposed, which plays an important role in the application of the proposed method as well as in the improvement of computational efficiency and accuracy. Numerical experiments verify the performance of the method. The data and code accompanying this paper are available at https://github.com/zhizhi4452/HCS. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17518</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17518</id><created>2025-02-22</created><authors><author><keyname>Xiong</keyname><forenames>Zheli</forenames></author></authors><title>Ensemble RL through Classifier Models: Enhancing Risk-Return Trade-offs   in Trading Strategies</title><categories>cs.LG cs.AI q-fin.CP stat.ML</categories><comments>16 pages,5 figures, 1 table</comments><msc-class>68T42</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a comprehensive study on the use of ensemble Reinforcement Learning (RL) models in financial trading strategies, leveraging classifier models to enhance performance. By combining RL algorithms such as A2C, PPO, and SAC with traditional classifiers like Support Vector Machines (SVM), Decision Trees, and Logistic Regression, we investigate how different classifier groups can be integrated to improve risk-return trade-offs. The study evaluates the effectiveness of various ensemble methods, comparing them with individual RL models across key financial metrics, including Cumulative Returns, Sharpe Ratios (SR), Calmar Ratios, and Maximum Drawdown (MDD). Our results demonstrate that ensemble methods consistently outperform base models in terms of risk-adjusted returns, providing better management of drawdowns and overall stability. However, we identify the sensitivity of ensemble performance to the choice of variance threshold {\tau}, highlighting the importance of dynamic {\tau} adjustment to achieve optimal performance. This study emphasizes the value of combining RL with classifiers for adaptive decision-making, with implications for financial trading, robotics, and other dynamic environments. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17586</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17586</id><created>2025-02-24</created><authors><author><keyname>Geraldo</keyname><forenames>Issa Cherif</forenames></author><author><keyname>Katchekpele</keyname><forenames>Edoh</forenames></author><author><keyname>Kpanzou</keyname><forenames>Tchilabalo Abozou</forenames></author></authors><title>Theoretical analysis and improvements in cubic transmutations of   probability distributions</title><categories>stat.ME</categories><comments>26 pages, 7 figures</comments><msc-class>60E05, 62E15, 62F10, 62F99, 62P99</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In statistics, processed data are becoming increasingly complex, and classical probability distributions are limited in their ability to model them. This is why, to better model data, extensive work has been conducted on extending classical probability distributions. Generally, this extension is achieved by transforming the cumulative distribution function of a baseline distribution through the addition of one or more parameters to enhance its flexibility. Cubic transmutation (CT) is one of the most popular methods for such extensions. However, CT does not have a unique definition because different approaches for CT have been proposed in the literature but are yet to be compared. The main goal of this paper is to compare these different approaches from both theoretical and empirical viewpoints. We study the relationships between the different approaches and we propose modified versions based on the extension of parameter ranges. The results are illustrated using Pareto distribution as baseline distribution. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17613</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17613</id><created>2025-02-24</created><authors><author><keyname>Hellemans</keyname><forenames>Stig</forenames></author><author><keyname>Algaba</keyname><forenames>Andres</forenames></author><author><keyname>Verboven</keyname><forenames>Sam</forenames></author><author><keyname>Ginis</keyname><forenames>Vincent</forenames></author></authors><title>Flexible Counterfactual Explanations with Generative Models</title><categories>cs.LG cs.AI stat.ME</categories><comments>28 pages, 13 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Counterfactual explanations provide actionable insights to achieve desired outcomes by suggesting minimal changes to input features. However, existing methods rely on fixed sets of mutable features, which makes counterfactual explanations inflexible for users with heterogeneous real-world constraints. Here, we introduce Flexible Counterfactual Explanations, a framework incorporating counterfactual templates, which allows users to dynamically specify mutable features at inference time. In our implementation, we use Generative Adversarial Networks (FCEGAN), which align explanations with user-defined constraints without requiring model retraining or additional optimization. Furthermore, FCEGAN is designed for black-box scenarios, leveraging historical prediction datasets to generate explanations without direct access to model internals. Experiments across economic and healthcare datasets demonstrate that FCEGAN significantly improves counterfactual explanations' validity compared to traditional benchmark methods. By integrating user-driven flexibility and black-box compatibility, counterfactual templates support personalized explanations tailored to user constraints. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17620</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17620</id><created>2025-02-24</created><authors><author><keyname>Bodenschatz</keyname><forenames>John C.</forenames></author><author><keyname>Rowe</keyname><forenames>Daniel B.</forenames></author></authors><title>Simulation and Harmonic Analysis of k-Space Readout (SHAKER)</title><categories>stat.AP</categories><comments>20 pages, 12 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the realm of neuroimaging research, the demand for efficient and accurate simulation tools for functional magnetic resonance imaging (fMRI) data is ever increasing. We present SHAKER, a comprehensive MATLAB package for simulating complex-valued fMRI time series data that will advance understanding and implementation of the MR signal equation and related physics principles to fMRI simulation. The core objective of the package is to provide researchers with a user-friendly MATLAB graphical user interface (GUI) tool capable of generating complex-valued fMRI time series data. This tool will allow researchers to input various parameters related to the MRI scan and receive simulated k-space data with ease, facilitating a deeper understanding of the intricacies of the generation and interpretation of fMRI data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17636</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17636</id><created>2025-02-24</created><authors><author><keyname>Marinescu</keyname><forenames>Marius</forenames></author><author><keyname>Balcau</keyname><forenames>Costel</forenames></author></authors><title>On the use of Mutual Information for Testing Independence</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we use a well know method in statistics, the $\delta$-method, to provide an asymptotic distribution for the Mutual Information, and construct and independence test based on it. Interesting connections are found with the likelihood ratio test and the chi-square goodness of fit test. In general, the difference between the Mutual Information evaluated at the true probabilities and at the empirical distribution, can be approximated by the sum of a normal random variable and a linear combination of chi-squares random variables. This summands are not independent, however the normal terms vanishes when testing independence, making the test statistic being asymptotically a linear combination of chi-squares. The $\delta$-method gives a general framework for computing the asymptotic distribution of other information based measures. A common difficulty is calculating the first and second-order derivatives, which is already challenging in the case of Mutual Information. However, this difficulty can be circumvallated by using advance symbolic software such as Mathematica. Finally, we explore the underlying geometry of the Mutual Information and propose other statical measures which may give competing alternatives to classical tests. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17657</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17657</id><created>2025-02-24</created><authors><author><keyname>Song</keyname><forenames>Xinyi</forenames></author><author><keyname>Lee</keyname><forenames>Lina</forenames></author><author><keyname>Xie</keyname><forenames>Kexin</forenames></author><author><keyname>Liu</keyname><forenames>Xueying</forenames></author><author><keyname>Deng</keyname><forenames>Xinwei</forenames></author><author><keyname>Hong</keyname><forenames>Yili</forenames></author></authors><title>StatLLM: A Dataset for Evaluating the Performance of Large Language   Models in Statistical Analysis</title><categories>stat.AP cs.AI</categories><comments>25 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The coding capabilities of large language models (LLMs) have opened up new opportunities for automatic statistical analysis in machine learning and data science. However, before their widespread adoption, it is crucial to assess the accuracy of code generated by LLMs. A major challenge in this evaluation lies in the absence of a benchmark dataset for statistical code (e.g., SAS and R). To fill in this gap, this paper introduces StatLLM, an open-source dataset for evaluating the performance of LLMs in statistical analysis. The StatLLM dataset comprises three key components: statistical analysis tasks, LLM-generated SAS code, and human evaluation scores. The first component includes statistical analysis tasks spanning a variety of analyses and datasets, providing problem descriptions, dataset details, and human-verified SAS code. The second component features SAS code generated by ChatGPT 3.5, ChatGPT 4.0, and Llama 3.1 for those tasks. The third component contains evaluation scores from human experts in assessing the correctness, effectiveness, readability, executability, and output accuracy of the LLM-generated code. We also illustrate the unique potential of the established benchmark dataset for (1) evaluating and enhancing natural language processing metrics, (2) assessing and improving LLM performance in statistical coding, and (3) developing and testing of next-generation statistical software - advancements that are crucial for data science and machine learning research. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17671</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17671</id><created>2025-02-24</created><authors><author><keyname>DeVore</keyname><forenames>Ronald</forenames></author><author><keyname>Nowak</keyname><forenames>Robert D.</forenames></author><author><keyname>Parhi</keyname><forenames>Rahul</forenames></author><author><keyname>Petrova</keyname><forenames>Guergana</forenames></author><author><keyname>Siegel</keyname><forenames>Jonathan W.</forenames></author></authors><title>Optimal Recovery Meets Minimax Estimation</title><categories>math.ST cs.NA math.NA stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental problem in statistics and machine learning is to estimate a function $f$ from possibly noisy observations of its point samples. The goal is to design a numerical algorithm to construct an approximation $\hat f$ to $f$ in a prescribed norm that asymptotically achieves the best possible error (as a function of the number $m$ of observations and the variance $\sigma^2$ of the noise). This problem has received considerable attention in both nonparametric statistics (noisy observations) and optimal recovery (noiseless observations). Quantitative bounds require assumptions on $f$, known as model class assumptions. Classical results assume that $f$ is in the unit ball of a Besov space. In nonparametric statistics, the best possible performance of an algorithm for finding $\hat f$ is known as the minimax rate and has been studied in this setting under the assumption that the noise is Gaussian. In optimal recovery, the best possible performance of an algorithm is known as the optimal recovery rate and has also been determined in this setting. While one would expect that the minimax rate recovers the optimal recovery rate when the noise level $\sigma$ tends to zero, it turns out that the current results on minimax rates do not carefully determine the dependence on $\sigma$ and the limit cannot be taken. This paper handles this issue and determines the noise-level-aware (NLA) minimax rates for Besov classes when error is measured in an $L_q$-norm with matching upper and lower bounds. The end result is a reconciliation between minimax rates and optimal recovery rates. The NLA minimax rate continuously depends on the noise level and recovers the optimal recovery rate when $\sigma$ tends to zero. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17679</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17679</id><created>2025-02-24</created><authors><author><keyname>Zhang</keyname><forenames>Ruizhe</forenames></author><author><keyname>Kong</keyname><forenames>Jooyoung</forenames></author><author><keyname>Small</keyname><forenames>Dylan S.</forenames></author><author><keyname>Bekerman</keyname><forenames>William</forenames></author></authors><title>Protocol For An Observational Study On The Effects Of Combinations Of   Adverse Childhood Experiences On Adult Depression</title><categories>stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adverse childhood experiences (ACEs) have been linked to a wide range of negative health outcomes in adulthood. However, few studies have investigated what specific combinations of ACEs most substantially impact mental health. In this article, we provide the protocol for our observational study of the effects of combinations of ACEs on adult depression. We use data from the 2023 Behavioral Risk Factor Surveillance System (BRFSS) to assess these effects. We will evaluate the replicability of our findings by splitting the sample into two discrete subpopulations of individuals. We employ data turnover for this analysis, enabling a single team of statisticians and domain experts to collaboratively evaluate the strength of evidence, and also integrating both qualitative and quantitative insights from exploratory data analysis. We outline our analysis plan using this method and conclude with a brief discussion of several specifics for our study. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17684</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17684</id><created>2025-02-24</created><authors><author><keyname>Wang</keyname><forenames>Jiacheng</forenames></author><author><keyname>Gao</keyname><forenames>Xin</forenames></author></authors><title>High-Dimensional Covariate-Dependent Gaussian Graphical Models</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by dynamic biologic network analysis, we propose a covariate-dependent Gaussian graphical model (cdexGGM) for capturing network structure that varies with covariates through a novel parameterization. Utilizing a likelihood framework, our methodology jointly estimates all dynamic edge and vertex parameters. We further develop statistical inference procedures to test the dynamic nature of the underlying network. Concerning large-scale networks, we perform composite likelihood estimation with an $\ell_1$ penalty to discover sparse dynamic network structures. We establish the estimation error bound in $\ell_2$ norm and validate the sign consistency in the high-dimensional context. We apply our method to an influenza vaccine data set to model the dynamic gene network that evolves with time. We also investigate a Down syndrome data set to model the dynamic protein network which varies under a factorial experimental design. These applications demonstrate the applicability and effectiveness of the proposed model. The supplemental materials for this article are available online. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17708</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17708</id><created>2025-02-24</created><authors><author><keyname>Kim</keyname><forenames>ByungKoo</forenames></author><author><keyname>Kuzushima</keyname><forenames>Saki</forenames></author><author><keyname>Shiraito</keyname><forenames>Yuki</forenames></author></authors><title>A Unified Model of Text and Citations for Topic-Specific Citation   Networks</title><categories>stat.AP cs.DL</categories><msc-class>62P25, 91C20, 62F15</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Social scientists analyze citation networks to study how documents influence subsequent work across various domains such as judicial politics and international relations. However, conventional approaches that summarize document attributes in citation networks often overlook the diverse semantic contexts in which citations occur. This paper develops the paragraph-citation topic model (PCTM), which analyzes citation networks and document texts jointly. The PCTM extends conventional topic models by assigning topics to paragraphs of citing documents, allowing citations to share topics with their embedding paragraphs. Our empirical analysis of U.S. Supreme Court opinions in the privacy issue domain, which includes cases on reproductive rights, demonstrates that citations within individual documents frequently span multiple substantive areas, and citations to individual documents show considerable topical diversity. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17723</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17723</id><created>2025-02-24</created><authors><author><keyname>Jiang</keyname><forenames>Alex Ziyu</forenames></author><author><keyname>Rodriguez</keyname><forenames>Abel</forenames></author></authors><title>Semiparametric estimation for multivariate Hawkes processes using   dependent Dirichlet processes: An application to order flow data in financial   markets</title><categories>stat.ME</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The order flow in high-frequency financial markets has been of particular research interest in recent years, as it provides insights into trading and order execution strategies and leads to better understanding of the supply-demand interplay and price formation. In this work, we propose a semiparametric multivariate Hawkes process model that relies on (mixtures of) dependent Dirichlet processes to analyze order flow data. Such a formulation avoids the kind of strong parametric assumptions about the excitation functions of the Hawkes process that often accompany traditional models and which, as we show, are not justified in the case of order flow data. It also allows us to borrow information across dimensions, improving estimation of the individual excitation functions. To fit the model, we develop two algorithms, one using Markov chain Monte Carlo methods and one using a stochastic variational approximation. In the context of simulation studies, we show that our model outperforms benchmark methods in terms of lower estimation error for both algorithms. In the context of real order flow data, we show that our model can capture features of the excitation functions such as non-monotonicity that cannot be accommodated by standard parametric models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17733</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17733</id><created>2025-02-24</created><authors><author><keyname>Xiang</keyname><forenames>Nuannuan</forenames></author><author><keyname>Shiraito</keyname><forenames>Yuki</forenames></author></authors><title>A Dynamic Dirichlet Process Mixture Model for the Partisan Realignment   of Civil Rights Issues in the U.S. House of Representatives</title><categories>stat.AP</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Evolutionary societal changes often prompt a debate. The positions of the two major political parties in the United States on civil rights issues underwent a reversal in the 20th century. The conventional view holds that this shift was a structural break in the 1960s, driven by party elites, while recent studies argue that the change was a more gradual process that began as early as the 1930s, driven by local rank-and-file party members. Motivated by this controversy, this paper develops a nonparametric Bayesian model that incorporates a hidden Markov model into the Dirichlet process mixture model. A distinctive feature of the proposed approach is that it models a process in which multiple latent clusters emerge and diminish as a continuing process so that it uncovers any of steady, sudden, and repeated shifts in analysing longitudinal data. Our model estimates each party's positions on civil rights in each state based on the legislative activities of their Congressional members, identifying cross- and within-party coalitions over time. We find evidence of gradual racial realignment in the 20th century, with two periods of fast changes during the 1948 election and the Civil Rights Movement. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17738</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17738</id><created>2025-02-24</created><authors><author><keyname>Yao</keyname><forenames>Rentian</forenames></author><author><keyname>Nitanda</keyname><forenames>Atsushi</forenames></author><author><keyname>Chen</keyname><forenames>Xiaohui</forenames></author><author><keyname>Yang</keyname><forenames>Yun</forenames></author></authors><title>Learning Density Evolution from Snapshot Data</title><categories>math.ST stat.CO stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by learning dynamical structures from static snapshot data, this paper presents a distribution-on-scalar regression approach for estimating the density evolution of a stochastic process from its noisy temporal point clouds. We propose an entropy-regularized nonparametric maximum likelihood estimator (E-NPMLE), which leverages the entropic optimal transport as a smoothing regularizer for the density flow. We show that the E-NPMLE has almost dimension-free statistical rates of convergence to the ground truth distributions, which exhibit a striking phase transition phenomenon in terms of the number of snapshots and per-snapshot sample size. To efficiently compute the E-NPMLE, we design a novel particle-based and grid-free coordinate KL divergence gradient descent (CKLGD) algorithm and prove its polynomial iteration complexity. Moreover, we provide numerical evidence on synthetic data to support our theoretical findings. This work contributes to the theoretical understanding and practical computation of estimating density evolution from noisy observations in arbitrary dimensions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17739</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17739</id><created>2025-02-24</created><authors><author><keyname>Aboussalah</keyname><forenames>Amine Mohamed</forenames></author><author><keyname>Ed-dib</keyname><forenames>Abdessalam</forenames></author></authors><title>Are GNNs doomed by the topology of their input graph?</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Graph Neural Networks (GNNs) have demonstrated remarkable success in learning from graph-structured data. However, the influence of the input graph's topology on GNN behavior remains poorly understood. In this work, we explore whether GNNs are inherently limited by the structure of their input graphs, focusing on how local topological features interact with the message-passing scheme to produce global phenomena such as oversmoothing or expressive representations. We introduce the concept of $k$-hop similarity and investigate whether locally similar neighborhoods lead to consistent node representations. This interaction can result in either effective learning or inevitable oversmoothing, depending on the inherent properties of the graph. Our empirical experiments validate these insights, highlighting the practical implications of graph topology on GNN performance. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17741</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17741</id><created>2025-02-24</created><authors><author><keyname>Xu</keyname><forenames>Zichun</forenames></author><author><keyname>Witten</keyname><forenames>Daniela</forenames></author><author><keyname>Shojaie</keyname><forenames>Ali</forenames></author></authors><title>A Unified Framework for Semiparametrically Efficient Semi-Supervised   Learning</title><categories>math.ST stat.ME stat.TH</categories><comments>60 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider statistical inference under a semi-supervised setting where we have access to both a labeled dataset consisting of pairs $\{X_i, Y_i \}_{i=1}^n$ and an unlabeled dataset $\{ X_i \}_{i=n+1}^{n+N}$. We ask the question: under what circumstances, and by how much, can incorporating the unlabeled dataset improve upon inference using the labeled data? To answer this question, we investigate semi-supervised learning through the lens of semiparametric efficiency theory. We characterize the efficiency lower bound under the semi-supervised setting for an arbitrary inferential problem, and show that incorporating unlabeled data can potentially improve efficiency if the parameter is not well-specified. We then propose two types of semi-supervised estimators: a safe estimator that imposes minimal assumptions, is simple to compute, and is guaranteed to be at least as efficient as the initial supervised estimator; and an efficient estimator, which -- under stronger assumptions -- achieves the semiparametric efficiency bound. Our findings unify existing semiparametric efficiency results for particular special cases, and extend these results to a much more general class of problems. Moreover, we show that our estimators can flexibly incorporate predicted outcomes arising from ``black-box" machine learning models, and thereby achieve the same goal as prediction-powered inference (PPI), but with superior theoretical guarantees. We also provide a complete understanding of the theoretical basis for the existing set of PPI methods. Finally, we apply the theoretical framework developed to derive and analyze efficient semi-supervised estimators in a number of settings, including M-estimation, U-statistics, and average treatment effect estimation, and demonstrate the performance of the proposed estimators via simulations. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17744</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17744</id><created>2025-02-24</created><authors><author><keyname>Wang</keyname><forenames>Baozhen</forenames></author><author><keyname>Qiao</keyname><forenames>Xingye</forenames></author></authors><title>Conformal Prediction Under Generalized Covariate Shift with Posterior   Drift</title><categories>stat.ML cs.LG math.ST stat.TH</categories><comments>Accepted to AISTATS 2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many real applications of statistical learning, collecting sufficiently many training data is often expensive, time-consuming, or even unrealistic. In this case, a transfer learning approach, which aims to leverage knowledge from a related source domain to improve the learning performance in the target domain, is more beneficial. There have been many transfer learning methods developed under various distributional assumptions. In this article, we study a particular type of classification problem, called conformal prediction, under a new distributional assumption for transfer learning. Classifiers under the conformal prediction framework predict a set of plausible labels instead of one single label for each data instance, affording a more cautious and safer decision. We consider a generalization of the \textit{covariate shift with posterior drift} setting for transfer learning. Under this setting, we propose a weighted conformal classifier that leverages both the source and target samples, with a coverage guarantee in the target domain. Theoretical studies demonstrate favorable asymptotic properties. Numerical studies further illustrate the usefulness of the proposed method. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17761</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17761</id><created>2025-02-24</created><authors><author><keyname>Almagro-P√©rez</keyname><forenames>Cristina</forenames></author><author><keyname>Song</keyname><forenames>Andrew H.</forenames></author><author><keyname>Weishaupt</keyname><forenames>Luca</forenames></author><author><keyname>Kim</keyname><forenames>Ahrong</forenames></author><author><keyname>Jaume</keyname><forenames>Guillaume</forenames></author><author><keyname>Williamson</keyname><forenames>Drew F. K.</forenames></author><author><keyname>Hemker</keyname><forenames>Konstantin</forenames></author><author><keyname>Lu</keyname><forenames>Ming Y.</forenames></author><author><keyname>Singh</keyname><forenames>Kritika</forenames></author><author><keyname>Chen</keyname><forenames>Bowen</forenames></author><author><keyname>Le</keyname><forenames>Long Phi</forenames></author><author><keyname>Baras</keyname><forenames>Alexander S.</forenames></author><author><keyname>Jiang</keyname><forenames>Sizun</forenames></author><author><keyname>Bashashati</keyname><forenames>Ali</forenames></author><author><keyname>Liu</keyname><forenames>Jonathan T. C.</forenames></author><author><keyname>Mahmood</keyname><forenames>Faisal</forenames></author></authors><title>AI-driven 3D Spatial Transcriptomics</title><categories>cs.CV stat.AP</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  A comprehensive three-dimensional (3D) map of tissue architecture and gene expression is crucial for illuminating the complexity and heterogeneity of tissues across diverse biomedical applications. However, most spatial transcriptomics (ST) approaches remain limited to two-dimensional (2D) sections of tissue. Although current 3D ST methods hold promise, they typically require extensive tissue sectioning, are complex, are not compatible with non-destructive 3D tissue imaging technologies, and often lack scalability. Here, we present VOlumetrically Resolved Transcriptomics EXpression (VORTEX), an AI framework that leverages 3D tissue morphology and minimal 2D ST to predict volumetric 3D ST. By pretraining on diverse 3D morphology-transcriptomic pairs from heterogeneous tissue samples and then fine-tuning on minimal 2D ST data from a specific volume of interest, VORTEX learns both generic tissue-related and sample-specific morphological correlates of gene expression. This approach enables dense, high-throughput, and fast 3D ST, scaling seamlessly to large tissue volumes far beyond the reach of existing 3D ST techniques. By offering a cost-effective and minimally destructive route to obtaining volumetric molecular insights, we anticipate that VORTEX will accelerate biomarker discovery and our understanding of morphomolecular associations and cell states in complex tissues. Interactive 3D ST volumes can be viewed at https://vortex-demo.github.io/ </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17772</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17772</id><created>2025-02-24</created><authors><author><keyname>Liang</keyname><forenames>Hao</forenames></author><author><keyname>Zhang</keyname><forenames>Wanrong</forenames></author><author><keyname>He</keyname><forenames>Xinlei</forenames></author><author><keyname>He</keyname><forenames>Kaishun</forenames></author><author><keyname>Xing</keyname><forenames>Hong</forenames></author></authors><title>An Improved Privacy and Utility Analysis of Differentially Private SGD   with Bounded Domain and Smooth Losses</title><categories>cs.LG cs.CR stat.ML</categories><comments>18 pages, 2 figures, submitted for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differentially Private Stochastic Gradient Descent (DPSGD) is widely used to protect sensitive data during the training of machine learning models, but its privacy guarantees often come at the cost of model performance, largely due to the inherent challenge of accurately quantifying privacy loss. While recent efforts have strengthened privacy guarantees by focusing solely on the final output and bounded domain cases, they still impose restrictive assumptions, such as convexity and other parameter limitations, and often lack a thorough analysis of utility. In this paper, we provide rigorous privacy and utility characterization for DPSGD for smooth loss functions in both bounded and unbounded domains. We track the privacy loss over multiple iterations by exploiting the noisy smooth-reduction property and establish the utility analysis by leveraging the projection's non-expansiveness and clipped SGD properties. In particular, we show that for DPSGD with a bounded domain, (i) the privacy loss can still converge without the convexity assumption, and (ii) a smaller bounded diameter can improve both privacy and utility simultaneously under certain conditions. Numerical results validate our results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17773</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17773</id><created>2025-02-24</created><authors><author><keyname>Huang</keyname><forenames>Chengpiao</forenames></author><author><keyname>Wu</keyname><forenames>Yuhang</forenames></author><author><keyname>Wang</keyname><forenames>Kaizheng</forenames></author></authors><title>Uncertainty Quantification for LLM-Based Survey Simulations</title><categories>stat.ME cs.AI cs.LG</categories><comments>30 pages, 6 figures, 10 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the reliable use of simulated survey responses from large language models (LLMs) through the lens of uncertainty quantification. Our approach converts synthetic data into confidence sets for population parameters of human responses, addressing the distribution shift between the simulated and real populations. A key innovation lies in determining the optimal number of simulated responses: too many produce overly narrow confidence sets with poor coverage, while too few yield excessively loose estimates. To resolve this, our method adaptively selects the simulation sample size, ensuring valid average-case coverage guarantees. It is broadly applicable to any LLM, irrespective of its fidelity, and any procedure for constructing confidence sets. Additionally, the selected sample size quantifies the degree of misalignment between the LLM and the target human population. We illustrate our method on real datasets and LLMs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17814</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17814</id><created>2025-02-24</created><authors><author><keyname>Ji</keyname><forenames>Wenlong</forenames></author><author><keyname>Yuan</keyname><forenames>Weizhe</forenames></author><author><keyname>Getzen</keyname><forenames>Emily</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author><author><keyname>Mei</keyname><forenames>Song</forenames></author><author><keyname>Weston</keyname><forenames>Jason E</forenames></author><author><keyname>Su</keyname><forenames>Weijie J.</forenames></author><author><keyname>Xu</keyname><forenames>Jing</forenames></author><author><keyname>Zhang</keyname><forenames>Linjun</forenames></author></authors><title>An Overview of Large Language Models for Statisticians</title><categories>stat.ML cs.AI cs.CL cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Large Language Models (LLMs) have emerged as transformative tools in artificial intelligence (AI), exhibiting remarkable capabilities across diverse tasks such as text generation, reasoning, and decision-making. While their success has primarily been driven by advances in computational power and deep learning architectures, emerging problems -- in areas such as uncertainty quantification, decision-making, causal inference, and distribution shift -- require a deeper engagement with the field of statistics. This paper explores potential areas where statisticians can make important contributions to the development of LLMs, particularly those that aim to engender trustworthiness and transparency for human users. Thus, we focus on issues such as uncertainty quantification, interpretability, fairness, privacy, watermarking and model adaptation. We also consider possible roles for LLMs in statistical analysis. By bridging AI and statistics, we aim to foster a deeper collaboration that advances both the theoretical foundations and practical applications of LLMs, ultimately shaping their role in addressing complex societal challenges. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17827</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17827</id><created>2025-02-24</created><authors><author><keyname>Alam</keyname><forenames>Entejar</forenames></author><author><keyname>Rathouz</keyname><forenames>Paul J.</forenames></author><author><keyname>M√ºller</keyname><forenames>Peter</forenames></author></authors><title>DPGLM: A Semiparametric Bayesian GLM with Inhomogeneous Normalized   Random Measures</title><categories>stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a varying weight dependent Dirichlet process (DDP) model to implement a semi-parametric GLM. The model extends a recently developed semi-parametric generalized linear model (SPGLM) by adding a nonparametric Bayesian prior on the baseline distribution of the GLM. We show that the resulting model takes the form of an inhomogeneous completely random measure that arises from exponential tilting of a normalized completely random measure. Building on familiar posterior simulation methods for mixtures with respect to normalized random measures we introduce posterior simulation in the resulting semi-parametric GLM model. The proposed methodology is validated through a series of simulation studies and is illustrated using data from a speech intelligibility study. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17830</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17830</id><created>2025-02-24</created><authors><author><keyname>Andrews</keyname><forenames>Isaiah</forenames></author><author><keyname>Chen</keyname><forenames>Jiafeng</forenames></author></authors><title>Certified Decisions</title><categories>econ.EM econ.TH math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Hypothesis tests and confidence intervals are ubiquitous in empirical research, yet their connection to subsequent decision-making is often unclear. We develop a theory of certified decisions that pairs recommended decisions with inferential guarantees. Specifically, we attach P-certificates -- upper bounds on loss that hold with probability at least $1-\alpha$ -- to recommended actions. We show that such certificates allow "safe," risk-controlling adoption decisions for ambiguity-averse downstream decision-makers. We further prove that it is without loss to limit attention to P-certificates arising as minimax decisions over confidence sets, or what Manski (2021) terms "as-if decisions with a set estimate." A parallel argument applies to E-certified decisions obtained from e-values in settings with unbounded loss. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17934</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17934</id><created>2025-02-25</created><authors><author><keyname>Han</keyname><forenames>Zhuoye</forenames></author><author><keyname>Wang</keyname><forenames>Tiandong</forenames></author></authors><title>Measuring Interlayer Dependence of Large Degrees in Multilayer   Inhomogeneous Random Graphs</title><categories>stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurately capturing interlayer dependence is essential for understanding the structure of complex multilayer networks. We propose an upper tail dependence estimator specifically designed for multilayer networks, leveraging multilayer inhomogeneous random graphs and multivariate regular variation to model extremal dependence. We establish the consistency of the estimator and demonstrate its practical effectiveness through real-data analysis of Reddit. Our findings reveal how financial market dynamics influence user interactions in the BitcoinMarkets subreddit and how seasonal trends shape engagement in sports-related subreddits. This work provides a rigorous and practical tool for quantifying extremal dependence across network layers, offering valuable insights into risk propagation and interaction patterns in multilayer systems. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.17981</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.17981</id><created>2025-02-25</created><authors><author><keyname>Fakhar</keyname><forenames>Ali</forenames><affiliation>STATIFY, LJK</affiliation></author><author><keyname>Polisano</keyname><forenames>K√©vin</forenames><affiliation>SVH, LJK</affiliation></author><author><keyname>Gannaz</keyname><forenames>Ir√®ne</forenames><affiliation>G-SCOP\_GROG, G-SCOP</affiliation></author><author><keyname>Achard</keyname><forenames>Sophie</forenames><affiliation>STATIFY, LJK</affiliation></author></authors><title>Generating Correlation Matrices with Graph Structures Using Convex   Optimization</title><categories>eess.SP math.OC math.ST stat.ME stat.TH</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work deals with the generation of theoretical correlation matrices with specific sparsity patterns, associated to graph structures. We present a novel approach based on convex optimization, offering greater flexibility compared to existing techniques, notably by controlling the mean of the entry distribution in the generated correlation matrices. This allows for the generation of correlation matrices that better represent realistic data and can be used to benchmark statistical methods for graph inference. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.18038</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.18038</id><created>2025-02-25</created><authors><author><keyname>Heinrichs</keyname><forenames>Florian</forenames></author><author><keyname>Bastian</keyname><forenames>Patrick</forenames></author><author><keyname>Dette</keyname><forenames>Holger</forenames></author></authors><title>Sequential Outlier Detection in Non-Stationary Time Series</title><categories>math.ST stat.ME stat.TH</categories><comments>24 pages, 3 figures</comments><msc-class>62M10, 62G10, 62L10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel method for sequential outlier detection in non-stationary time series is proposed. The method tests the null hypothesis of ``no outlier'' at each time point, addressing the multiple testing problem by bounding the error probability of successive tests, using extreme value theory. The asymptotic properties of the test statistic are studied under the null hypothesis and alternative. The finite sample properties of the new detection scheme are investigated by means of a simulation study, and the method is compared with alternative procedures which have recently been proposed in the statistics and machine learning literature. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.18049</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.18049</id><created>2025-02-25</created><authors><author><keyname>He</keyname><forenames>Hengzhi</forenames></author><author><keyname>Xu</keyname><forenames>Shirong</forenames></author><author><keyname>Cheng</keyname><forenames>Guang</forenames></author></authors><title>Golden Ratio Mixing of Real and Synthetic Data for Stabilizing   Generative Model Training</title><categories>stat.ML cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recent studies identified an intriguing phenomenon in recursive generative model training known as model collapse, where models trained on data generated by previous models exhibit severe performance degradation. Addressing this issue and developing more effective training strategies have become central challenges in generative model research. In this paper, we investigate this phenomenon theoretically within a novel framework, where generative models are iteratively trained on a combination of newly collected real data and synthetic data from the previous training step. To develop an optimal training strategy for integrating real and synthetic data, we evaluate the performance of a weighted training scheme in various scenarios, including Gaussian distribution estimation and linear regression. We theoretically characterize the impact of the mixing proportion and weighting scheme of synthetic data on the final model's performance. Our key finding is that, across different settings, the optimal weighting scheme under different proportions of synthetic data asymptotically follows a unified expression, revealing a fundamental trade-off between leveraging synthetic data and generative model performance. Notably, in some cases, the optimal weight assigned to real data corresponds precisely to the reciprocal of the golden ratio. Finally, we validate our theoretical results on extensive simulated datasets and a real tabular dataset. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.18142</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.18142</id><created>2025-02-25</created><authors><author><keyname>Higham</keyname><forenames>Catherine F.</forenames></author><author><keyname>Henderson</keyname><forenames>Paul</forenames></author><author><keyname>Murray-Smith</keyname><forenames>Roderick</forenames></author></authors><title>Actively Inferring Optimal Measurement Sequences</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Measurement of a physical quantity such as light intensity is an integral part of many reconstruction and decision scenarios but can be costly in terms of acquisition time, invasion of or damage to the environment and storage. Data minimisation and compliance with data protection laws is also an important consideration. Where there are a range of measurements that can be made, some may be more informative and compliant with the overall measurement objective than others. We develop an active sequential inference algorithm that uses the low dimensional representational latent space from a variational autoencoder (VAE) to choose which measurement to make next. Our aim is to recover high dimensional data by making as few measurements as possible. We adapt the VAE encoder to map partial data measurements on to the latent space of the complete data. The algorithm draws samples from this latent space and uses the VAE decoder to generate data conditional on the partial measurements. Estimated measurements are made on the generated data and fed back through the partial VAE encoder to the latent space where they can be evaluated prior to making a measurement. Starting from no measurements and a normal prior on the latent space, we consider alternative strategies for choosing the next measurement and updating the predictive posterior prior for the next step. The algorithm is illustrated using the Fashion MNIST dataset and a novel convolutional Hadamard pattern measurement basis. We see that useful patterns are chosen within 10 steps, leading to the convergence of the guiding generative images. Compared with using stochastic variational inference to infer the parameters of the posterior distribution for each generated data point individually, the partial VAE framework can efficiently process batches of generated data and obtains superior results with minimal measurements. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.18167</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.18167</id><created>2025-02-25</created><authors><author><keyname>Shao</keyname><forenames>Xiao</forenames></author><author><keyname>Wu</keyname><forenames>Guoqiang</forenames></author></authors><title>Sharper Concentration Inequalities for Multi-Graph Dependent Variables</title><categories>cs.LG stat.ML</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multi-task learning (MTL) with each task involving graph-dependent data, generalization results of existing theoretical analyses yield a sub-optimal risk bound of $O(\frac{1}{\sqrt{n}})$, where $n$ is the number of training samples.This is attributed to the lack of a foundational sharper concentration inequality for multi-graph dependent random variables. To fill this gap, this paper proposes a new corresponding Bennett inequality, enabling the derivation of a sharper risk bound of $O(\frac{\log n}{n})$. Specifically, building on the proposed Bennett inequality, we propose a new corresponding Talagrand inequality for the empirical process and further develop an analytical framework of the local Rademacher complexity to enhance theoretical generalization analyses in MTL with multi-graph dependent data. Finally, we apply the theoretical advancements to applications such as Macro-AUC Optimization, demonstrating the superiority of our theoretical results over previous work, which is also corroborated by experimental results. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.18198</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.18198</id><created>2025-02-25</created><authors><author><keyname>Kim</keyname><forenames>Dangchan</forenames></author><author><keyname>Lim</keyname><forenames>Chae Young</forenames></author></authors><title>Differentially private synthesis of Spatial Point Processes</title><categories>stat.AP stat.ML</categories><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  This paper proposes a method to generate synthetic data for spatial point patterns within the differential privacy (DP) framework. Specifically, we define a differentially private Poisson point synthesizer (PPS) and Cox point synthesizer (CPS) to generate synthetic point patterns with the concept of the $\alpha$-neighborhood that relaxes the original definition of DP. We present three example models to construct a differentially private PPS and CPS, providing sufficient conditions on their parameters to ensure the DP given a specified privacy budget. In addition, we demonstrate that the synthesizers can be applied to point patterns on the linear network. Simulation experiments demonstrate that the proposed approaches effectively maintain the privacy and utility of synthetic data. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.18223</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.18223</id><created>2025-02-25</created><authors><author><keyname>Ye</keyname><forenames>Xiang</forenames></author><author><keyname>Van Niekerk</keyname><forenames>Janet</forenames></author><author><keyname>Rue</keyname><forenames>H√•vard</forenames></author></authors><title>Principled priors for Bayesian inference of circular models</title><categories>stat.ME</categories><comments>44 pages, 20 figures</comments><license>http://creativecommons.org/licenses/by-nc-nd/4.0/</license><abstract>  Advancements in computational power and methodologies have enabled research on massive datasets. However, tools for analyzing data with directional or periodic characteristics, such as wind directions and customers' arrival time in 24-hour clock, remain underdeveloped. While statisticians have proposed circular distributions for such analyses, significant challenges persist in constructing circular statistical models, particularly in the context of Bayesian methods. These challenges stem from limited theoretical development and a lack of historical studies on prior selection for circular distribution parameters.   In this article, we propose a principled, practical and systematic framework for selecting priors that effectively prevents overfitting in circular scenarios, especially when there is insufficient information to guide prior selection. We introduce well-examined Penalized Complexity (PC) priors for the most widely used circular distributions. Comprehensive comparisons with existing priors in the literature are conducted through simulation studies and a practical case study. Finally, we discuss the contributions and implications of our work, providing a foundation for further advancements in constructing Bayesian circular statistical models. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.18253</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.18253</id><created>2025-02-25</created><authors><author><keyname>Wang</keyname><forenames>Chen</forenames></author><author><keyname>Han</keyname><forenames>Shichao</forenames></author><author><keyname>Huang</keyname><forenames>Shan</forenames></author></authors><title>Enhancing External Validity of Experiments with Ongoing Sampling</title><categories>econ.GN q-fin.EC stat.AP</categories><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Participants in online experiments often enroll over time, which can compromise sample representativeness due to temporal shifts in covariates. This issue is particularly critical in A/B tests, online controlled experiments extensively used to evaluate product updates, since these tests are cost-sensitive and typically short in duration. We propose a novel framework that dynamically assesses sample representativeness by dividing the ongoing sampling process into three stages. We then develop stage-specific estimators for Population Average Treatment Effects (PATE), ensuring that experimental results remain generalizable across varying experiment durations. Leveraging survival analysis, we develop a heuristic function that identifies these stages without requiring prior knowledge of population or sample characteristics, thereby keeping implementation costs low. Our approach bridges the gap between experimental findings and real-world applicability, enabling product decisions to be based on evidence that accurately represents the broader target population. We validate the effectiveness of our framework on three levels: (1) through a real-world online experiment conducted on WeChat; (2) via a synthetic experiment; and (3) by applying it to 600 A/B tests on WeChat in a platform-wide application. Additionally, we provide practical guidelines for practitioners to implement our method in real-world settings. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.18279</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.18279</id><created>2025-02-25</created><authors><author><keyname>Wild</keyname><forenames>Veit</forenames></author><author><keyname>Wu</keyname><forenames>James</forenames></author><author><keyname>Sejdinovic</keyname><forenames>Dino</forenames></author><author><keyname>Knoblauch</keyname><forenames>Jeremias</forenames></author></authors><title>Near-Optimal Approximations for Bayesian Inference in Function Space</title><categories>stat.ML cs.LG stat.ME</categories><comments>59 pages (26 pages main paper + 33 pages appendices); 6 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We propose a scalable inference algorithm for Bayes posteriors defined on a reproducing kernel Hilbert space (RKHS). Given a likelihood function and a Gaussian random element representing the prior, the corresponding Bayes posterior measure $\Pi_{\text{B}}$ can be obtained as the stationary distribution of an RKHS-valued Langevin diffusion. We approximate the infinite-dimensional Langevin diffusion via a projection onto the first $M$ components of the Kosambi-Karhunen-Lo\`eve expansion. Exploiting the thus obtained approximate posterior for these $M$ components, we perform inference for $\Pi_{\text{B}}$ by relying on the law of total probability and a sufficiency assumption. The resulting method scales as $O(M^3+JM^2)$, where $J$ is the number of samples produced from the posterior measure $\Pi_{\text{B}}$. Interestingly, the algorithm recovers the posterior arising from the sparse variational Gaussian process (SVGP) (see Titsias, 2009) as a special case, owed to the fact that the sufficiency assumption underlies both methods. However, whereas the SVGP is parametrically constrained to be a Gaussian process, our method is based on a non-parametric variational family $\mathcal{P}(\mathbb{R}^M)$ consisting of all probability measures on $\mathbb{R}^M$. As a result, our method is provably close to the optimal $M$-dimensional variational approximation of the Bayes posterior $\Pi_{\text{B}}$ in $\mathcal{P}(\mathbb{R}^M)$ for convex and Lipschitz continuous negative log likelihoods, and coincides with SVGP for the special case of a Gaussian error likelihood. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.18284</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.18284</id><created>2025-02-25</created><authors><author><keyname>Chen</keyname><forenames>Zonghao</forenames></author><author><keyname>Naslidnyk</keyname><forenames>Masha</forenames></author><author><keyname>Briol</keyname><forenames>Fran√ßois-Xavier</forenames></author></authors><title>Nested Expectations with Kernel Quadrature</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the challenging computational task of estimating nested expectations. Existing algorithms, such as nested Monte Carlo or multilevel Monte Carlo, are known to be consistent but require a large number of samples at both inner and outer levels to converge. Instead, we propose a novel estimator consisting of nested kernel quadrature estimators and we prove that it has a faster convergence rate than all baseline methods when the integrands have sufficient smoothness. We then demonstrate empirically that our proposed method does indeed require fewer samples to estimate nested expectations on real-world applications including Bayesian optimisation, option pricing, and health economics. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.18298</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.18298</id><created>2025-02-25</created><authors><author><keyname>Pargo</keyname><forenames>Taha Ahmadi</forenames></author><author><keyname>Shirazi</keyname><forenames>Mohsen Akbarpour</forenames></author><author><keyname>Fadai</keyname><forenames>Dawud</forenames></author></authors><title>Smart and Efficient IoT-Based Irrigation System Design: Utilizing a   Hybrid Agent-Based and System Dynamics Approach</title><categories>cs.MA cs.AI cs.SY eess.SY stat.AP</categories><comments>50 pages, 22 figures</comments><msc-class>I.6.6, I.2.1, J.2</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regarding problems like reduced precipitation and an increase in population, water resource scarcity has become one of the most critical problems in modern-day societies, as a consequence, there is a shortage of available water resources for irrigation in arid and semi-arid countries. On the other hand, it is possible to utilize modern technologies to control irrigation and reduce water loss. One of these technologies is the Internet of Things (IoT). Despite the possibility of using the IoT in irrigation control systems, there are complexities in designing such systems. Considering this issue, it is possible to use agent-oriented software engineering (AOSE) methodologies to design complex cyber-physical systems such as IoT-based systems. In this research, a smart irrigation system is designed based on Prometheus AOSE methodology, to reduce water loss by maintaining soil moisture in a suitable interval. The designed system comprises sensors, a central agent, and irrigation nodes. These agents follow defined rules to maintain soil moisture at a desired level cooperatively. For system simulation, a hybrid agent-based and system dynamics model was designed. In this hybrid model, soil moisture dynamics were modeled based on the system dynamics approach. The proposed model, was implemented in AnyLogic computer simulation software. Utilizing the simulation model, irrigation rules were examined. The system's functionality in automatic irrigation mode was tested based on a 256-run, fractional factorial design, and the effects of important factors such as soil properties on total irrigated water and total operation time were analyzed. Based on the tests, the system consistently irrigated nearly optimal water amounts in all tests. Moreover, the results were also used to minimize the system's energy consumption by reducing the system's operational time. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.18300</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.18300</id><created>2025-02-25</created><authors><author><keyname>Chen</keyname><forenames>Wenlong</forenames></author><author><keyname>Li</keyname><forenames>Bolian</forenames></author><author><keyname>Zhang</keyname><forenames>Ruqi</forenames></author><author><keyname>Li</keyname><forenames>Yingzhen</forenames></author></authors><title>Bayesian Computation in Deep Learning</title><categories>cs.LG stat.ML</categories><comments>43 pages, 7 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This review paper is intended for the 2nd edition of the Handbook of Markov chain Monte Carlo.We provide an introduction to approximate inference techniques as Bayesian computation methods applied to deep learning models. We organize the chapter by presenting popular computational methods for (1) Bayesian neural networks and (2) deep generative models, explaining their unique challenges in posterior inference as well as the solutions. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.18325</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.18325</id><created>2025-02-25</created><authors><author><keyname>Szczecinski</keyname><forenames>Leszek</forenames></author><author><keyname>Benesty</keyname><forenames>Jacob</forenames></author><author><keyname>Kuhn</keyname><forenames>Eduardo Vinicius</forenames></author></authors><title>A Unified Bayesian Perspective for Conventional and Robust Adaptive   Filters</title><categories>cs.IR math.ST stat.TH</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this work, we present a new perspective on the origin and interpretation of adaptive filters. By applying Bayesian principles of recursive inference from the state-space model and using a series of simplifications regarding the structure of the solution, we can present, in a unified framework, derivations of many adaptive filters which depend on the probabilistic model of the observational noise. In particular, under a Gaussian model, we obtain solutions well-known in the literature (such as LMS, NLMS, or Kalman filter), while using non-Gaussian noise, we obtain new families of adaptive filter. Notably, under assumption of Laplacian noise, we obtain a family of robust filters of which the signed-error algorithm is a well-known member, while other algorithms, derived effortlessly in the proposed framework, are entirely new. Numerical examples are shown to illustrate the properties and provide a better insight into the performance of the derived adaptive filters. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.18332</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.18332</id><created>2025-02-25</created><authors><author><keyname>Csat√≥</keyname><forenames>L√°szl√≥</forenames></author><author><keyname>Petr√≥czy</keyname><forenames>D√≥ra Gr√©ta</forenames></author></authors><title>Attractiveness and equal treatment in a group draw</title><categories>math.OC physics.soc-ph stat.AP</categories><comments>16 pages, 3 figures, 4 tables</comments><msc-class>62-08, 90-10, 90B90, 91B14</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  National teams from different continents can play against each other only in afew sports competitions. Therefore, a reasonable aim is maximising the number of intercontinental games in world cups, as done in basketball and football, in contrast to handball and volleyball. However, this objective requires additional draw constraints that imply the violation of equal treatment. In addition, the standard draw mechanism is non-uniformly distributed on the set of valid assignments, which may lead to further distortions. Our paper analyses this novel trade-off between attractiveness and fairness through the example of the 2025 World Men's Handball Championship. We introduce a measure of inequality, which enables considering 32 sets of reasonable geographical restrictions to determine the Pareto frontier. The proposed methodology can be used by policy-makers to select the optimal set of draw constraints. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.18346</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.18346</id><created>2025-02-25</created><authors><author><keyname>Baguley</keyname><forenames>Samuel</forenames></author><author><keyname>G√∂bel</keyname><forenames>Andreas</forenames></author><author><keyname>Pappik</keyname><forenames>Marcus</forenames></author><author><keyname>Schiller</keyname><forenames>Leon</forenames></author></authors><title>Testing Thresholds and Spectral Properties of High-Dimensional Random   Toroidal Graphs via Edgeworth-Style Expansions</title><categories>math.ST math.PR stat.TH</categories><comments>91 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study high-dimensional random geometric graphs (RGGs) of edge-density $p$ with vertices uniformly distributed on the $d$-dimensional torus and edges inserted between sufficiently close vertices with respect to an $L_q$-norm. We focus on distinguishing an RGG from an Erd\H{o}s--R\'enyi (ER) graph if both models have edge probability $p$. So far, most results considered either spherical RGGs with $L_2$-distance or toroidal RGGs under $L_\infty$-distance. However, for general $L_q$-distances, many questions remain open, especially if $p$ is allowed to depend on $n$. The main reason for this is that RGGs under $L_q$-distances can not easily be represented as the logical AND of their 1-dimensional counterparts, as for $L_\infty$ geometries. To overcome this, we devise a novel technique for quantifying the dependence between edges based on modified Edgeworth expansions.   Our technique yields the first tight algorithmic upper bounds for distinguishing toroidal RGGs under general $L_q$ norms from ER-graphs for fixed $p$ and $q$. We achieve this by showing that signed triangles can distinguish the two models when $d\ll n^3p^3$ for the whole regime of $c/n&lt;p&lt;1$. Additionally, our technique yields an improved information-theoretic lower bound for this task, showing that the two distributions converge whenever $d=\tilde{\Omega}(n^3p^2)$, which is just as strong as the currently best known lower bound for spherical RGGs in case of general $p$ from Liu et al. [STOC'22]. Finally, our expansions allow us to tightly characterize the spectral properties of toroidal RGGs both under $L_q$-distances for fixed $1\le q&lt;\infty$, and $L_\infty$-distance. Our results partially resolve a conjecture of Bangachev and Bresler [COLT'24] and prove that the distance metric, rather than the underlying space, is responsible for the observed differences in the behavior of spherical and toroidal RGGs. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.18347</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.18347</id><created>2025-02-25</created><authors><author><keyname>Geadah</keyname><forenames>Victor</forenames></author><author><keyname>Nejatbakhsh</keyname><forenames>Amin</forenames></author><author><keyname>Lipshutz</keyname><forenames>David</forenames></author><author><keyname>Pillow</keyname><forenames>Jonathan W.</forenames></author><author><keyname>Williams</keyname><forenames>Alex H.</forenames></author></authors><title>Modeling Neural Activity with Conditionally Linear Dynamical Systems</title><categories>q-bio.NC stat.ML</categories><comments>18 pages, 6 figures. Associated code available at:   https://github.com/neurostatslab/clds</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Neural population activity exhibits complex, nonlinear dynamics, varying in time, over trials, and across experimental conditions. Here, we develop Conditionally Linear Dynamical System (CLDS) models as a general-purpose method to characterize these dynamics. These models use Gaussian Process (GP) priors to capture the nonlinear dependence of circuit dynamics on task and behavioral variables. Conditioned on these covariates, the data is modeled with linear dynamics. This allows for transparent interpretation and tractable Bayesian inference. We find that CLDS models can perform well even in severely data-limited regimes (e.g. one trial per condition) due to their Bayesian formulation and ability to share statistical power across nearby task conditions. In example applications, we apply CLDS to model thalamic neurons that nonlinearly encode heading direction and to model motor cortical neurons during a cued reaching task </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.18393</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.18393</id><created>2025-02-25</created><authors><author><keyname>Matsumoto</keyname><forenames>Namiko</forenames></author><author><keyname>Mazumdar</keyname><forenames>Arya</forenames></author></authors><title>Learning sparse generalized linear models with binary outcomes via   iterative hard thresholding</title><categories>math.ST cs.DS cs.IT cs.LG math.IT stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In statistics, generalized linear models (GLMs) are widely used for modeling data and can expressively capture potential nonlinear dependence of the model's outcomes on its covariates. Within the broad family of GLMs, those with binary outcomes, which include logistic and probit regressions, are motivated by common tasks such as binary classification with (possibly) non-separable data. In addition, in modern machine learning and statistics, data is often high-dimensional yet has a low intrinsic dimension, making sparsity constraints in models another reasonable consideration. In this work, we propose to use and analyze an iterative hard thresholding (projected gradient descent on the ReLU loss) algorithm, called binary iterative hard thresholding (BIHT), for parameter estimation in sparse GLMs with binary outcomes. We establish that BIHT is statistically efficient and converges to the correct solution for parameter estimation in a general class of sparse binary GLMs. Unlike many other methods for learning GLMs, including maximum likelihood estimation, generalized approximate message passing, and GLM-tron (Kakade et al. 2011; Bahmani et al. 2016), BIHT does not require knowledge of the GLM's link function, offering flexibility and generality in allowing the algorithm to learn arbitrary binary GLMs. As two applications, logistic and probit regression are additionally studied. In this regard, it is shown that in logistic regression, the algorithm is in fact statistically optimal in the sense that the order-wise sample complexity matches (up to logarithmic factors) the lower bound obtained previously. To the best of our knowledge, this is the first work achieving statistical optimality for logistic regression in all noise regimes with a computationally efficient algorithm. Moreover, for probit regression, our sample complexity is on the same order as that obtained for logistic regression. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.18428</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.18428</id><created>2025-02-25</created><authors><author><keyname>Guionnet</keyname><forenames>Alice</forenames></author><author><keyname>Piccolo</keyname><forenames>Vanessa</forenames></author></authors><title>Global law of conjugate kernel random matrices with heavy-tailed weights</title><categories>math.PR cs.LG stat.ML</categories><comments>45 pages, 1 figure</comments><msc-class>60B20, 15B52, 68T07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the asymptotic spectral behavior of the conjugate kernel random matrix $YY^\top$, where $Y= f(WX)$ arises from a two-layer neural network model. We consider the setting where $W$ and $X$ are both random rectangular matrices with i.i.d. entries, where the entries of $W$ follow a heavy-tailed distribution, while those of $X$ have light tails. Our assumptions on $W$ include a broad class of heavy-tailed distributions, such as symmetric $\alpha$-stable laws with $\alpha \in (0,2)$ and sparse matrices with $\mathcal{O}(1)$ nonzero entries per row. The activation function $f$, applied entrywise, is nonlinear, smooth, and odd. By computing the eigenvalue distribution of $YY^\top$ through its moments, we show that heavy-tailed weights induce strong correlations between the entries of $Y$, leading to richer and fundamentally different spectral behavior compared to models with light-tailed weights. </abstract></arXiv></metadata></record>
<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"><header><identifier>oai:arXiv.org:2502.18463</identifier><datestamp>2025-02-26</datestamp><setSpec>stat</setSpec></header><metadata><arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd"><id>2502.18463</id><created>2025-02-25</created><authors><author><keyname>Leme</keyname><forenames>Renato Purita Paes</forenames></author><author><keyname>Stein</keyname><forenames>Cliff</forenames></author><author><keyname>Teng</keyname><forenames>Yifeng</forenames></author><author><keyname>Worah</keyname><forenames>Pratik</forenames></author></authors><title>Allocating Variance to Maximize Expectation</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design efficient approximation algorithms for maximizing the expectation of the supremum of families of Gaussian random variables. In particular, let $\mathrm{OPT}:=\max_{\sigma_1,\cdots,\sigma_n}\mathbb{E}\left[\sum_{j=1}^{m}\max_{i\in S_j} X_i\right]$, where $X_i$ are Gaussian, $S_j\subset[n]$ and $\sum_i\sigma_i^2=1$, then our theoretical results include:   - We characterize the optimal variance allocation -- it concentrates on a small subset of variables as $|S_j|$ increases,   - A polynomial time approximation scheme (PTAS) for computing $\mathrm{OPT}$ when $m=1$, and   - An $O(\log n)$ approximation algorithm for computing $\mathrm{OPT}$ for general $m&gt;1$.   Such expectation maximization problems occur in diverse applications, ranging from utility maximization in auctions markets to learning mixture models in quantitative genetics. </abstract></arXiv></metadata></record>
